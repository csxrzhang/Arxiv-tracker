<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-11 03:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260111_0330</div>
    <div class="row"><div class="card">
<div class="title">GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</div>
<div class="meta-line">Authors: Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov</div>
<div class="meta-line">First: 2026-01-08T18:59:24+00:00 · Latest: 2026-01-08T18:59:24+00:00</div>
<div class="meta-line">Comments: NVIDIA-Tech Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDPO：面向多奖励强化学习优化的组奖励解耦归一化策略优化</div>
<div class="mono" style="margin-top:8px">随着语言模型能力日益增强，用户不仅期望其提供准确回答，还要求其行为在不同场景下符合多样化的人类偏好。为实现这一目标，强化学习（RL）流程开始引入多个奖励，每个奖励对应一种特定偏好，以引导模型达成期望行为。然而，近期研究默认在多奖励设置下应用组相对策略优化（GRPO），而未检验其适用性。本文指出，直接应用GRPO对不同轨迹奖励组合进行归一化会导致其坍缩为相同的优势值，降低训练信号的分辨率，从而引发次优收敛甚至早期训练失败。为此，我们提出组奖励解耦归一化策略优化（GDPO），该方法通过解耦各奖励的归一化过程，更真实地保留其相对差异，实现更精确的多奖励优化，并显著提升训练稳定性。我们在工具调用、数学推理和代码推理三项任务中对比GDPO与GRPO，评估指标包括正确性（准确率、错误率）和约束遵循性（格式、长度）。在所有实验设置下，GDPO均稳定优于GRPO，证明了其在多奖励强化学习优化中的有效性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for language models to align with diverse human preferences, which has led to multi-reward reinforcement learning (RL) pipelines. However, it identifies that directly applying Group Relative Policy Optimization (GRPO) in such settings causes reward collapse, reducing training signal resolution and leading to suboptimal convergence. To address this, the authors propose Group reward-Decoupled Normalization Policy Optimization (GDPO), a method that decouples the normalization of individual rewards to preserve their relative differences, thereby improving optimization accuracy and training stability. Experimental results across tool calling, math reasoning, and coding reasoning tasks show that GDPO consistently outperforms GRPO in both correctness metrics (e.g., accuracy, bug ratio) and constraint adherence metrics (e.g., format, length), demonstrating its effectiveness and generalizability for multi-reward RL optimization.</div>
<div class="mono" style="margin-top:8px">本文的动机是语言模型需要符合多样化的人类偏好，这催生了多奖励强化学习（RL）流程。然而，研究发现直接应用组相对策略优化（GRPO）在多奖励设置中会导致奖励崩溃，降低训练信号分辨率并引发次优收敛。为解决此问题，作者提出了组奖励解耦归一化策略优化（GDPO），该方法通过解耦各奖励的归一化来保持其相对差异，从而提升优化准确性和训练稳定性。在工具调用、数学推理和代码推理任务上的实验结果表明，GDPO在正确性指标（如准确率、错误率）和约束遵循指标（如格式、长度）上均一致优于GRPO，证明了其在多奖励RL优化中的有效性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI</div>
<div class="meta-line">Authors: Zain Iqbal, Lorenzo Valerio</div>
<div class="meta-line">First: 2026-01-08T18:31:11+00:00 · Latest: 2026-01-08T18:31:11+00:00</div>
<div class="meta-line">Comments: 6 pages, 9 figures, 2 Tables, conference [Submitted in PerConAI-2026]</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EARL：面向泛在AI的液态状态机能耗感知优化方法</div>
<div class="mono" style="margin-top:8px">泛在AI日益依赖在严格资源约束下实现低延迟、高能效计算的设备端学习系统。液态状态机为泛在系统与神经形态系统提供了低功耗时序处理的有效途径，但其部署仍面临高超参数敏感性和传统优化方法忽略能耗约束导致计算成本高昂的挑战。本研究提出EARL——一种集成贝叶斯优化与基于自适应强化学习选择策略的能耗感知强化学习框架，可协同优化精度与能耗。该框架采用代理模型进行全局探索，利用强化学习实现动态候选方案优先级排序，并引入早期终止机制消除冗余评估，显著降低计算开销。在三个基准数据集上的实验表明，相较于主流超参数调优框架，EARL实现了6%-15%的精度提升、60%-80%的能耗降低，并将优化时间缩短达一个数量级。这些结果凸显了能耗感知自适应搜索在提升资源受限设备端AI应用中液态状态机效率与可扩展性方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need for low-latency, energy-efficient on-device AI, where Liquid State Machines (LSMs) are promising but difficult to deploy due to sensitive hyperparameters and costly, energy-unaware optimization. The method introduces EARL, an energy-aware reinforcement learning framework that combines Bayesian optimization for global exploration with an adaptive reinforcement learning policy to dynamically prioritize hyperparameter candidates, incorporating an early termination mechanism to cut redundant evaluations. Experimental results on three benchmarks show EARL achieves 6-15% higher accuracy, 60-80% lower energy consumption, and up to a 10x reduction in optimization time compared to state-of-the-art tuning frameworks, demonstrating its effectiveness for resource-constrained pervasive AI.</div>
<div class="mono" style="margin-top:8px">本研究的动机是普适AI对低延迟、高能效设备端学习的需求，其中液态机虽前景广阔，但因超参数敏感且传统优化方法计算成本高、忽略能耗而难以部署。方法上提出了EARL，一个能量感知的强化学习框架，它结合了用于全局探索的贝叶斯优化与自适应强化学习策略，以动态优先选择超参数候选，并采用早期终止机制减少冗余评估。在三个基准数据集上的实验结果表明，与领先的超参数调优框架相比，EARL实现了6-15%的准确率提升、60-80%的能耗降低以及高达10倍的优化时间缩减，证明了其在资源受限设备端AI应用中的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">An interpretable data-driven approach to optimizing clinical fall risk assessment</div>
<div class="meta-line">Authors: Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Holley Farley, Kimia Ghobadi</div>
<div class="meta-line">First: 2026-01-08T18:17:31+00:00 · Latest: 2026-01-08T18:17:31+00:00</div>
<div class="meta-line">Comments: arXiv admin note: substantial text overlap with arXiv:2510.20714</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05194v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study&#x27;s risk labels, and without changing the tool&#x27;s form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种可解释的数据驱动方法优化临床跌倒风险评估</div>
<div class="mono" style="margin-top:8px">本研究旨在通过数据驱动建模方法，使约翰霍普金斯跌倒风险评估工具（JHFRAT）的跌倒风险预测与更具临床意义的指标更好对齐。我们对2022年3月至2023年10月期间约翰霍普金斯医疗系统三家医院的54,209例住院病例进行了回顾性队列分析，其中20,208例被纳入高风险跌倒事件，13,941例被纳入低风险跌倒事件。为融入临床知识并保持可解释性，我们采用约束评分优化（CSO）模型重新调整JHFRAT评分权重，同时保留其累加结构和临床阈值。重新校准指调整项目权重，使生成的评分能更一致地按研究风险标签排序事件，且不改变工具形式或部署流程。模型预测性能较当前JHFRAT显著提升（CSO AUC-ROC=0.91，JHFRAT AUC-ROC=0.86），相当于每周在约翰霍普金斯医疗系统多保护35名高风险患者。约束评分优化模型在包含与不包含电子健康记录变量时表现相似。尽管基准黑盒模型（XGBoost）在性能指标上优于基于知识的约束逻辑回归（AUC-ROC=0.94），但CSO对风险标签变异性展现更强稳健性。这种循证方法为医疗系统利用数据驱动优化技术系统提升住院患者跌倒预防方案和患者安全提供了坚实基础，有助于改善医疗环境中的风险评估和资源配置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study was motivated by the need to better align the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with clinically meaningful measures through a data-driven approach while preserving interpretability. The method employed a constrained score optimization (CSO) model to reweight the JHFRAT&#x27;s scoring weights, maintaining its additive structure and clinical thresholds without altering the tool&#x27;s form factor. The main experimental results showed that the CSO model significantly improved predictive performance over the original JHFRAT, achieving an AUC-ROC of 0.91 compared to 0.86, which translates to protecting an additional 35 high-risk patients per week across the health system, and it demonstrated robustness comparable to a black-box XGBoost model while being more interpretable.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过数据驱动的方法，使约翰霍普金斯跌倒风险评估工具（JHFRAT）更好地与临床有意义的指标对齐，同时保持可解释性。方法上采用了约束分数优化（CSO）模型来重新调整JHFRAT的评分权重，保留了其可加性结构和临床阈值，且未改变工具的形式或部署流程。主要实验结果表明，CSO模型相比原始JHFRAT显著提升了预测性能，AUC-ROC从0.86提高到0.91，这意味着在医疗系统中每周可额外保护35名高风险患者，并且该模型在保持可解释性的同时，其鲁棒性与黑盒XGBoost模型相当。</div>
</details>
</div>
<div class="card">
<div class="title">SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning</div>
<div class="meta-line">Authors: Yanchang Liang, Xiaowei Zhao</div>
<div class="meta-line">First: 2026-01-08T18:10:35+00:00 · Latest: 2026-01-08T18:10:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05187v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05187v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimuAgent：基于大语言模型并强化学习增强的Simulink建模助手</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）已彻底改变基于文本的代码自动化，但其在图式工程工作流中的潜力仍未充分发掘。本文提出SimuAgent，一个专为Simulink设计的LLM驱动建模与仿真智能体。它用简洁的字典式Python表示替代冗长的XML，显著减少令牌数量、提升可解释性，并实现快速进程内仿真。采用轻量级规划-执行架构，通过两阶段训练使智能体同时掌握底层工具技能与高层设计推理能力。为应对长周期任务中的稀疏奖励问题，我们提出反射式GRPO（ReGRPO），通过自我反思轨迹为分组相对策略优化（GRPO）提供丰富的中间反馈，从而加速收敛并增强鲁棒性。在新发布的包含5300项多领域建模任务的基准测试集SimuBench上的实验表明：经SimuAgent微调的Qwen2.5-7B模型比标准强化学习基线收敛更快、建模精度更高，在相同基准的少样本提示评估中甚至超越GPT-4o。消融实验证实，两阶段课程学习与抽象重构数据增强策略进一步提升了泛化能力。SimuAgent完全可在本地有限硬件上训练运行，为工业模型驱动工程提供隐私安全、成本高效的解决方案，弥合了LLMs与图形化建模环境间的鸿沟，为工业场景下的AI辅助工程设计提供了实用工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the under-explored potential of large language models (LLMs) in graph-oriented engineering workflows, this paper introduces SimuAgent, an LLM-based assistant for Simulink modeling. The method employs a concise Python representation to replace verbose XML, reducing token counts and enabling fast simulation, alongside a two-stage trained plan-execute architecture enhanced with a novel reinforcement learning approach called Reflection-GRPO (ReGRPO), which uses self-reflection traces to provide intermediate feedback for long-horizon tasks. Experimental results on the SimuBench benchmark of 5300 tasks show that a fine-tuned Qwen2.5-7B model achieves higher modeling accuracy and faster convergence than standard RL baselines, even surpassing GPT-4o in few-shot evaluations, while ablations confirm the benefits of the two-stage curriculum and data augmentation, with the system operating on-premise for privacy and cost-effectiveness.</div>
<div class="mono" style="margin-top:8px">本文的动机在于探索大语言模型（LLM）在图形化工程工作流中尚未充分开发的潜力，为此提出了SimuAgent，一个基于LLM的Simulink建模助手。方法上，它用简洁的Python字典表示替代冗长的XML，显著减少了令牌数量并实现了快速仿真，同时采用两阶段训练的规划-执行架构，并引入了名为Reflection-GRPO（ReGRPO）的新型强化学习方法，通过自我反思轨迹为长周期任务提供中间反馈。在包含5300个多领域建模任务的新基准SimuBench上的实验结果表明，经SimuAgent微调的Qwen2.5-7B模型比标准强化学习基线收敛更快、建模精度更高，甚至在少量示例提示评估中超越了GPT-4o，消融实验验证了两阶段课程学习和数据增强对泛化能力的提升，且系统可在本地硬件上运行，提供了隐私保护且成本高效的工业解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art</div>
<div class="meta-line">Authors: Timofey Tomashevskiy</div>
<div class="meta-line">First: 2026-01-08T17:42:56+00:00 · Latest: 2026-01-08T17:42:56+00:00</div>
<div class="meta-line">Comments: 20 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05152v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05152v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.
  Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向非平稳环境的安全持续强化学习方法研究进展综述</div>
<div class="mono" style="margin-top:8px">本文对持续安全在线强化学习方法进行了前沿综述。我们探讨了构建持续在线安全强化学习算法的理论层面、挑战与开放性问题，基于考虑非平稳性适应的安全学习机制类型，提出了持续在线安全强化学习方法的分类体系与具体细节。我们系统归类了在线强化学习算法的安全约束形式化方法，并最终展望了构建可靠安全的在线学习算法的发展前景。关键词：非平稳环境下的安全强化学习、非平稳条件下的安全持续强化学习、HM-MDP、NSMDP、POMDP、安全POMDP、持续学习约束、安全持续强化学习评述、安全持续强化学习综述、安全持续强化学习、分布漂移下的安全在线学习、安全持续在线适应、安全强化学习、安全探索、安全适应、约束马尔可夫决策过程、部分可观测马尔可夫决策过程、安全强化学习与隐马尔可夫决策过程、安全在线强化学习、安全元学习、安全元强化学习、基于上下文的安全强化学习、持续学习的安全约束形式化</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper surveys continual safe online reinforcement learning (COSRL) methods, motivated by the need for reliable algorithms that can adapt to nonstationary environments while maintaining safety constraints. The method involves categorizing existing approaches based on their safe learning mechanisms and formulations of safety constraints, drawing on frameworks like HM-MDP, NSMDP, and POMDP to address adaptation and partial observability. The main experimental results are not detailed as this is a review paper, but it synthesizes state-of-the-art techniques, highlights theoretical challenges, and outlines prospects for developing robust safe online learning systems.</div>
<div class="mono" style="margin-top:8px">本文综述了持续安全在线强化学习方法，其动机在于需要可靠的算法，使其能在非平稳环境中适应并保持安全约束。方法基于安全学习机制和安全约束的表述对现有方法进行分类，利用HM-MDP、NSMDP和POMDP等框架来处理适应性和部分可观测性问题。由于这是一篇综述论文，未详述具体实验结果，但综合了最先进的技术，强调了理论挑战，并概述了开发鲁棒安全在线学习系统的前景。</div>
</details>
</div>
<div class="card">
<div class="title">Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning</div>
<div class="meta-line">Authors: Marvin Illian, Ramin Khalili, Antonio A. de A. Rocha, Lin Wang</div>
<div class="meta-line">First: 2026-01-07T16:51:33+00:00 · Latest: 2026-01-08T17:32:37+00:00</div>
<div class="meta-line">Comments: 11 pages, 12 figures, v2: Corrected performance numbers in the conclusion; no change to methodology</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04083v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04083v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based on operator experience and rarely adapted to dynamic network conditions. In this work, we ask: Can an agent automatically learn and adapt cell (re)selection parameters to consistently improve network performance? We present a reinforcement learning (RL)-based framework called CellPilot that adaptively tunes cell (re)selection parameters by learning spatiotemporal patterns of mobile network dynamics. Our study with real-world data demonstrates that even a lightweight RL agent can outperform conventional heuristic reconfigurations by up to 167%, while generalizing effectively across different network scenarios. These results indicate that data-driven approaches can significantly improve cell (re)selection configurations and enhance mobile network performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶的细胞：基于强化学习的自适应小区（重）选择</div>
<div class="mono" style="margin-top:8px">5G网络的广泛部署与4G/LTE网络的共存，为移动设备提供了多样化的候选连接小区。然而，如何将移动设备关联至小区以最大化整体网络性能，即小区（重）选择，仍是移动运营商面临的关键挑战。目前，小区（重）选择参数通常基于运营商经验手动配置，极少适应动态网络条件。本研究提出：能否通过智能体自动学习并调整小区（重）选择参数，持续提升网络性能？我们提出了一个基于强化学习（RL）的框架CellPilot，通过学习移动网络动态的时空模式自适应调整参数。基于真实数据的实验表明，即使轻量级RL智能体也能超越传统启发式重配置方法达167%，并在不同网络场景中有效泛化。这些结果表明，数据驱动方法能显著优化小区（重）选择配置，提升移动网络性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of manually configuring cell (re)selection parameters in dynamic 4G/5G networks, this paper introduces CellPilot, a reinforcement learning framework that autonomously adapts these parameters by learning spatiotemporal network patterns. The method employs a lightweight RL agent to optimize network performance. Experimental results using real-world data show that this approach outperforms conventional heuristic reconfigurations by up to 167% and generalizes effectively across various network scenarios, demonstrating the potential of data-driven techniques to enhance mobile network operations.</div>
<div class="mono" style="margin-top:8px">本文针对4G/5G动态网络中手动配置小区（重）选择参数的挑战，提出了CellPilot，一个通过强化学习自适应调整这些参数的框架，其方法基于学习网络时空模式的轻量级智能体。实验使用真实世界数据表明，该方法相比传统启发式重配置性能提升高达167%，并能有效泛化到不同网络场景，这证明了数据驱动技术可显著改善移动网络性能。</div>
</details>
</div>
<div class="card">
<div class="title">Act-Adaptive Margin: Dynamically Calibrating Reward Models for Subjective Ambiguity</div>
<div class="meta-line">Authors: Feiteng Fang, Dingwei Chen, Xiang Huang, Ting-En Lin, Yuchuan Wu, Xiong Liu, Xinge Ye, Ziqiang Liu, Haonan Zhang, Liang Zhu, Hamid Alinejad-Rokny, Min Yang, Yongbin Li</div>
<div class="meta-line">First: 2025-05-29T18:15:18+00:00 · Latest: 2026-01-08T16:58:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23923v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.23923v2">PDF</a> · <a href="https://github.com/calubkk/AAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Currently, most reinforcement learning tasks focus on domains like mathematics and programming, where verification is relatively straightforward. However, in subjective tasks such as role-playing, alignment techniques struggle to make progress, primarily because subjective reward modeling using the Bradley-Terry model faces significant challenges when dealing with ambiguous preferences. To improve reward modeling in subjective tasks, this paper proposes AAM (\textbf{\underline{A}}ct-\textbf{\underline{A}}daptive \textbf{\underline{M}}argin), which enhances reward modeling by dynamically calibrating preference margins using the model&#x27;s internal parameter knowledge. We design two versions of AAM that efficiently generate contextually-appropriate preference gaps without additional human annotation. This approach fundamentally improves how reward models handle subjective rewards by better integrating generative understanding with preference scoring. To validate AAM&#x27;s effectiveness in subjective reward modeling, we conduct evaluations on RewardBench, JudgeBench, and challenging role-playing tasks. Results show that AAM significantly improves subjective reward modeling performance, enhancing Bradley-Terry reward models by 2.95\% in general tasks and 4.85\% in subjective role-playing tasks. Furthermore, reward models trained with AAM can help downstream alignment tasks achieve better results. Our test results show that applying rewards generated by AAM-Augmented RM to preference learning techniques (e.g., GRPO) achieves state-of-the-art results on CharacterEval and Charm. Code and dataset are available at https://github.com/calubkk/AAM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动作自适应边界：针对主观模糊性动态校准奖励模型</div>
<div class="mono" style="margin-top:8px">当前大多数强化学习任务集中于数学、编程等验证相对直接的领域。然而在角色扮演等主观性任务中，对齐技术进展缓慢，主要因为基于Bradley-Terry模型的主观奖励建模在处理模糊偏好时面临显著挑战。为改进主观任务的奖励建模，本文提出AAM（动作自适应边界），通过利用模型内部参数知识动态校准偏好边界来增强奖励建模。我们设计了两个版本的AAM，无需额外人工标注即可高效生成符合语境的偏好间隙。该方法通过更好地整合生成式理解与偏好评分，从根本上改进了奖励模型处理主观奖励的方式。为验证AAM在主观奖励建模中的有效性，我们在RewardBench、JudgeBench及高难度角色扮演任务上进行评估。结果表明，AAM显著提升主观奖励建模性能：在通用任务中使Bradley-Terry奖励模型提升2.95%，在主观角色扮演任务中提升4.85%。此外，经AAM训练的奖励模型可助力下游对齐任务取得更好效果。测试显示，将AAM增强奖励模型生成的奖励应用于偏好学习技术（如GRPO），可在CharacterEval和Charm基准上达到最先进水平。代码与数据集详见https://github.com/calubkk/AAM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that reinforcement learning alignment techniques struggle with subjective tasks like role-playing, where ambiguous preferences make reward modeling difficult using standard methods like the Bradley-Terry model. The proposed method, Act-Adaptive Margin (AAM), dynamically calibrates preference margins by leveraging the model&#x27;s internal parameter knowledge to generate contextually-appropriate preference gaps without extra human annotation, thereby better integrating generative understanding with preference scoring. Experimental results on RewardBench, JudgeBench, and role-playing tasks show that AAM improves Bradley-Terry reward models by 2.95% in general tasks and 4.85% in subjective tasks, and when applied to downstream alignment like GRPO, it achieves state-of-the-art performance on benchmarks such as CharacterEval and Charm.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，强化学习对齐技术在角色扮演等主观性任务中进展困难，因为使用布拉德利-特里模型进行主观奖励建模时，模糊的偏好带来了重大挑战。提出的方法——行动自适应边界（AAM）——通过利用模型内部参数知识动态校准偏好边界，无需额外人工标注即可生成上下文合适的偏好间隙，从而更好地将生成理解与偏好评分相结合。在RewardBench、JudgeBench和角色扮演任务上的实验结果表明，AAM将布拉德利-特里奖励模型在一般任务中的性能提升了2.95%，在主观角色扮演任务中提升了4.85%，并且当应用于GRPO等下游对齐任务时，在CharacterEval和Charm等基准上取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforced Efficient Reasoning via Semantically Diverse Exploration</div>
<div class="meta-line">Authors: Ziqi Zhao, Zhaochun Ren, Jiahong Zou, Liu Yang, Zhiwei Xu, Xuri Ge, Zhumin Chen, Xinyu Ma, Daiting Shi, Shuaiqiang Wang, Dawei Yin, Xin Xin</div>
<div class="meta-line">First: 2026-01-08T15:56:44+00:00 · Latest: 2026-01-08T15:56:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05053v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05053v1">PDF</a> · <a href="https://github.com/ZiqiZhao1/ROSE-rl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于语义多样性探索的强化高效推理</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）已被证明能有效提升大语言模型（LLMs）的推理能力。基于蒙特卡洛树搜索（MCTS）的扩展方法（如GRPO）通过提供树状推理推演，实现了细粒度和片段级的信用分配，从而改进了基础RLVR。然而，现有方法仍存在探索多样性有限和推理效率低下的问题。为解决上述挑战，我们提出了面向LLMs的基于语义多样性探索的强化高效推理方法——ROSE。为促进更多样化的推理探索，本方法融合了基于语义熵的分支策略和ε-探索机制：前者基于已采样的推理推演捕获语义不确定性，选择语义分歧度高的分支点以生成新的连续推理路径；后者则随机从根节点启动推理推演，避免搜索过程过度局部化。为提升效率，我们设计了长度感知的片段级优势估计器，对简洁正确的推理给予奖励，同时对冗余的推理链进行惩罚。基于Qwen和Llama模型在多项数学推理基准上的大量实验验证了ROSE的有效性与高效性。代码发布于https://github.com/ZiqiZhao1/ROSE-rl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing reinforcement learning with verifiable rewards (RLVR) methods, such as limited exploration diversity and inefficient reasoning in large language models (LLMs). It proposes ROSE, a method that enhances exploration through a semantic-entropy-based branching strategy and an ε-exploration mechanism to generate diverse reasoning paths, while improving efficiency with a length-aware segment-level advantage estimator that rewards concise correct reasoning. Experimental results on mathematical reasoning benchmarks with Qwen and Llama models demonstrate that ROSE effectively boosts both performance and efficiency compared to prior approaches.</div>
<div class="mono" style="margin-top:8px">本文针对现有基于可验证奖励的强化学习方法在大语言模型中存在的探索多样性不足和推理效率低下问题，提出了ROSE方法。该方法通过基于语义熵的分支策略和ε探索机制来增强推理路径的多样性探索，同时利用长度感知的片段级优势估计器奖励简洁正确的推理以提升效率。在Qwen和Llama模型上的多个数学推理基准测试实验表明，ROSE相比先前方法能有效提高性能和效率。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Robotic World Model Makes Offline Model-Based Reinforcement Learning Work on Real Robots</div>
<div class="meta-line">Authors: Chenhao Li, Andreas Krause, Marco Hutter</div>
<div class="meta-line">First: 2025-04-23T12:58:15+00:00 · Latest: 2026-01-08T15:43:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.16680v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.16680v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has achieved impressive results in robotics, yet high-performing pipelines remain highly task-specific, with little reuse of prior data. Offline Model-based RL (MBRL) offers greater data efficiency by training policies entirely from existing datasets, but suffers from compounding errors and distribution shift in long-horizon rollouts. Although existing methods have shown success in controlled simulation benchmarks, robustly applying them to the noisy, biased, and partially observed datasets typical of real-world robotics remains challenging. We present a principled pipeline for making offline MBRL effective on physical robots. Our RWM-U extends autoregressive world models with epistemic uncertainty estimation, enabling temporally consistent multi-step rollouts with uncertainty effectively propagated over long horizons. We combine RWM-U with MOPO-PPO, which adapts uncertainty-penalized policy optimization to the stable, on-policy PPO framework for real-world control. We evaluate our approach on diverse manipulation and locomotion tasks in simulation and on real quadruped and humanoid, training policies entirely from offline datasets. The resulting policies consistently outperform model-free and uncertainty-unaware model-based baselines, and fusing real-world data in model learning further yields robust policies that surpass online model-free baselines trained solely in simulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不确定性感知机器人世界模型使离线模型强化学习在真实机器人上生效</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在机器人领域已取得显著成果，但高性能流程仍高度依赖特定任务，对历史数据的复用有限。离线模型强化学习（MBRL）通过完全基于现有数据集训练策略，提升了数据效率，但在长时程推演中易受误差累积和分布偏移影响。尽管现有方法在受控仿真基准测试中表现成功，但将其稳健应用于真实机器人场景中常见的噪声、偏差及部分可观测数据集仍具挑战。本文提出一种原则性流程，使离线MBRL在实体机器人上有效运行。我们的RWM-U通过认知不确定性估计扩展自回归世界模型，实现了时间一致的多步推演，并将不确定性有效传递至长时程。我们将RWM-U与MOPO-PPO结合，后者将不确定性惩罚策略优化适配至稳定的同策略PPO框架，以适用于现实控制任务。我们在仿真及真实四足与人形机器人上，针对多种操作与移动任务评估本方法，所有策略均完全基于离线数据集训练。所得策略持续优于无模型及未考虑不确定性的模型基线，且在模型学习中融合真实世界数据后，进一步产生了超越纯仿真训练的在线无模型基线的稳健策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge of applying offline model-based reinforcement learning (MBRL) to real-world robotics, where existing methods struggle with compounding errors and distribution shift in noisy, partially observed datasets. The method introduces RWM-U, an autoregressive world model enhanced with epistemic uncertainty estimation to enable temporally consistent long-horizon rollouts, combined with MOPO-PPO for uncertainty-penalized policy optimization within a stable on-policy framework. Experimental results on manipulation and locomotion tasks in simulation and on real quadruped and humanoid robots show that policies trained entirely from offline data outperform model-free and uncertainty-unaware model-based baselines, and incorporating real-world data further yields robust policies surpassing online model-free baselines trained only in simulation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线模型强化学习在现实机器人应用中面临的挑战，现有方法在噪声大、部分可观测的数据集中易受复合误差和分布偏移影响。方法上提出了RWM-U，这是一种通过认知不确定性估计增强的自回归世界模型，能实现时间一致的长时域推演，并结合MOPO-PPO在稳定的同策略框架中进行不确定性惩罚的策略优化。在仿真和真实四足及人形机器人的多种操作与运动任务实验中，完全基于离线数据训练的策略一致优于无模型和未考虑不确定性的模型基线，且融合真实世界数据进一步产生了超越仅仿真训练的在线无模型基线的鲁棒策略。</div>
</details>
</div>
<div class="card">
<div class="title">Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models</div>
<div class="meta-line">Authors: Yueqing Hu, Xinyang Peng, Shuting Peng, Hanqi Wang, Tianhong Wang</div>
<div class="meta-line">First: 2026-01-08T15:27:03+00:00 · Latest: 2026-01-08T15:27:03+00:00</div>
<div class="meta-line">Comments: 7 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05019v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Large Reasoning Models trained via reinforcement learning exhibit a &quot;natural&quot; alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the &quot;Hán Dān Xué Bù&quot; (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a &quot;Functional Alignment Collapse&quot;: while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines (&quot;Negative Transfer&quot;). Our analysis suggests that SFT induces a &quot;Cargo Cult&quot; effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher&#x27;s dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>邯郸学步（模仿）还是青出于蓝（掌握）？大语言模型推理蒸馏的认知视角</div>
<div class="mono" style="margin-top:8px">近期通过强化学习训练的大型推理模型展现出与人类认知成本的&#x27;自然&#x27;对齐。然而，我们发现当前主流的推理蒸馏范式——通过监督微调训练学生模型模仿这些推理轨迹——未能传递这种认知结构。在14个模型中检验&#x27;邯郸学步&#x27;（表面模仿）假说，发现蒸馏引发了&#x27;功能对齐崩溃&#x27;：教师模型能反映人类难度缩放趋势（$\bar{r}=0.64$），而蒸馏学生显著削弱了这种对齐（$\bar{r}=0.34$），甚至常低于自身蒸馏前的基线水平（&#x27;负迁移&#x27;）。分析表明监督微调会诱发&#x27;货物崇拜&#x27;效应：学生仪式化地复制推理的语言形式（冗长表达），却未内化教师动态资源分配策略。因此，推理蒸馏使计算成本与认知需求脱钩，揭示类人认知是主动强化的涌现特性，而非被动模仿的产物。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether reasoning distillation in Large Language Models (LLMs) leads to genuine mastery or merely superficial mimicry of reasoning processes. Motivated by the observation that models trained with reinforcement learning naturally align with human cognitive costs, the authors test the hypothesis that standard supervised fine-tuning (SFT) for distillation fails to transfer this cognitive structure. Their method involves analyzing 14 models, comparing teacher models that use reinforcement learning with student models distilled via SFT on reasoning traces. The main experimental results reveal a &#x27;Functional Alignment Collapse&#x27;: while teacher models maintain a strong correlation with human difficulty scaling (average r=0.64), distilled students show significantly degraded alignment (average r=0.34) and often suffer from negative transfer, performing worse than their pre-distillation baselines. The analysis indicates that SFT leads to a &#x27;Cargo Cult&#x27; effect, where students replicate the linguistic form of reasoning without internalizing the teacher&#x27;s dynamic resource allocation, thereby decoupling computational cost from cognitive demand.</div>
<div class="mono" style="margin-top:8px">本文研究大型语言模型中的推理蒸馏是导致真正的掌握还是仅仅对推理过程的表面模仿。其动机源于观察到通过强化学习训练的模型能自然地与人类认知成本对齐，作者检验了标准监督微调进行蒸馏无法传递这种认知结构的假设。方法上，他们分析了14个模型，比较了使用强化学习的教师模型与通过监督微调在推理轨迹上蒸馏的学生模型。主要实验结果显示了一种&#x27;功能对齐崩溃&#x27;：教师模型与人类难度缩放保持强相关性（平均r=0.64），而蒸馏学生模型的对齐性显著退化（平均r=0.34），且经常出现负迁移，表现甚至低于蒸馏前的基线。分析表明，监督微调导致了&#x27;货物崇拜&#x27;效应，学生模型仅仪式性地复制推理的语言形式，而未内化教师的动态资源分配策略，从而使得计算成本与认知需求脱钩。</div>
</details>
</div>
<div class="card">
<div class="title">Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing</div>
<div class="meta-line">Authors: Osvaldo Simeone</div>
<div class="meta-line">First: 2026-01-01T07:38:07+00:00 · Latest: 2026-01-08T15:17:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00245v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00245v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现代神经形态人工智能：从令牌内处理到令牌间处理</div>
<div class="mono" style="margin-top:8px">人工智能的快速发展带来了新颖的数据处理和生成能力，但也伴随着不断攀升的能耗需求。这一挑战重新激发了人们对神经形态计算原理的兴趣，该原理通过离散稀疏激活、循环动力学和非线性反馈，有望实现类脑的高效计算。事实上，现代人工智能架构正通过重度量化激活、状态空间动力学和稀疏注意力机制，日益体现神经形态原理。本文通过区分令牌内处理与令牌间处理的视角，详细阐述了神经形态模型、状态空间模型与Transformer架构之间的关联。早期神经形态人工智能研究主要基于脉冲神经网络进行令牌内处理，即对同一向量输入（如图像像素）的多通道或特征进行转换。相比之下，近期研究探索了如何利用神经形态原理设计高效的令牌间处理方法，该方法能根据上下文相关性选择性地组合不同信息元素。这些方法通过实现关联记忆机制，利用状态空间动力学或稀疏自注意力。在系统阐述基于令牌内/间处理视角的现代神经形态人工智能模型的同时，本文还综述了相关训练方法，涵盖从利用并行卷积处理的代理梯度到基于强化学习机制的局部学习规则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the escalating energy demands of conventional AI, this paper explores neuromorphic computing principles as a pathway to brain-like efficiency, highlighting their increasing embodiment in modern architectures through quantized activations and sparse mechanisms. The method introduces a conceptual framework distinguishing between intra-token processing, historically addressed by spiking neural networks for feature transformations within single inputs, and inter-token processing, which leverages state-space dynamics or sparse attention to contextually combine information across tokens. Experimental results and reviews indicate that these neuromorphic approaches, supported by training methods like surrogate gradients and local learning rules, enable efficient associative memorization and selective information integration, bridging models such as transformers and state-space models with neuromorphic efficiency goals.</div>
<div class="mono" style="margin-top:8px">本文的动机是应对传统人工智能不断增长的能耗挑战，通过借鉴神经形态计算原理来实现类脑的高效处理，这些原理在现代架构中日益体现为量化激活和稀疏机制。方法上，论文提出了一个概念框架，区分了令牌内处理（历史上由脉冲神经网络处理单个输入内的特征变换）和令牌间处理（利用状态空间动态或稀疏注意力根据上下文相关性选择性地组合不同信息）。实验结果表明，这些神经形态方法结合代理梯度等训练技术，能够实现高效的关联记忆和选择性信息整合，从而将变压器和状态空间模型与神经形态的效率目标相连接。</div>
</details>
</div>
<div class="card">
<div class="title">On the Hidden Objective Biases of Group-based Reinforcement Learning</div>
<div class="meta-line">Authors: Aleksandar Fontana, Marco Simoni, Giulio Rossolini, Andrea Saracino, Paolo Mori</div>
<div class="meta-line">First: 2026-01-08T15:00:35+00:00 · Latest: 2026-01-08T15:00:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05002v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05002v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论基于群体的强化学习方法的隐性目标偏差</div>
<div class="mono" style="margin-top:8px">基于群体的强化学习方法，如群体相对策略优化（GRPO），目前被广泛用于大型语言模型的后续训练。尽管这些方法在实证中取得了成功，但其奖励优化与底层训练目标之间存在结构性不匹配。本文通过将GRPO类方法置于统一的代理公式框架下进行研究，提出了理论分析。该视角揭示了影响所有被分析方法的共性特征：（i）非均匀群体加权会导致共享前缀词元产生系统性梯度偏差；（ii）与AdamW优化器的交互使得训练动态对奖励缩放极不敏感；（iii）在重复优化步骤中，优化器动量可能推动策略更新超出预设的裁剪区域。我们认为这些发现揭示了当前方法的根本局限性，并为未来公式设计提供了原则性指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the structural biases in group-based reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), which are commonly used for post-training large language models. The authors conduct a theoretical analysis through a unified surrogate formulation, revealing three key issues: non-uniform group weighting introduces systematic gradient biases on shared prefix tokens, interactions with the AdamW optimizer render training dynamics insensitive to reward scaling, and optimizer momentum can cause policy updates to exceed intended clipping regions over repeated steps. These findings expose fundamental limitations in current approaches and offer principled guidance for future method design.</div>
<div class="mono" style="margin-top:8px">本文研究了基于群体的强化学习方法（如群体相对策略优化GRPO）中存在的结构性偏差，这些方法常用于大型语言模型的后训练。作者通过统一的代理公式进行理论分析，揭示了三个关键问题：非均匀群体加权会在共享前缀标记上引入系统性梯度偏差，与AdamW优化器的交互使训练动态对奖励缩放不敏感，且优化器动量可能导致策略更新在重复优化步骤中超出预期的裁剪区域。这些发现揭示了当前方法的根本局限性，并为未来方法设计提供了原则性指导。</div>
</details>
</div>
<div class="card">
<div class="title">AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?</div>
<div class="meta-line">Authors: Henan Sun, Kaichi Yu, Yuyao Wang, Bowen Liu, Xunkai Li, Rong-Hua Li, Nuo Chen, Jia Li</div>
<div class="meta-line">First: 2026-01-08T14:54:44+00:00 · Latest: 2026-01-08T14:54:44+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04996v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04996v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.
  AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlgBench：大型推理模型对算法的理解程度如何？</div>
<div class="mono" style="margin-top:8px">推理能力已成为大型推理模型发展的核心焦点。尽管在MATH500和LiveCodeBench等推理基准测试中取得了显著进展，但现有的算法推理基准仍存在局限，未能回答一个关键问题：LRMs是否真正掌握了算法推理？为解答此问题，我们提出AlgBench——一个由专家构建的、以算法为中心范式评估LRMs的基准。AlgBench包含27种算法领域的3000余道原创题目，由ACM算法专家构建，并采用包含欧几里得结构、非欧几里得结构、非优化、局部优化、全局优化及启发式优化类别的完整分类体系。对主流LRMs（如Gemini-3-Pro、DeepSeek-v3.2-Speciale和GPT-o3）的实证评估显示出显著的性能异质性：模型在非优化任务上表现良好（最高达92%），但在动态规划等全局优化算法上准确率骤降至约49%。进一步分析揭示了\textbf{策略性过转移}现象，即模型因必要的低熵标记而过早放弃正确的算法设计。这些发现暴露了以问题为中心的强化学习的根本局限，并凸显了采用以算法为中心的训练范式对于实现稳健算法推理的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to assess whether Large Reasoning Models (LRMs) genuinely master algorithmic reasoning beyond existing benchmarks, this paper introduces AlgBench, an expert-curated benchmark with over 3,000 problems spanning 27 algorithms across a comprehensive taxonomy. The method involves constructing and evaluating LRMs under an algorithm-centric paradigm, with empirical tests on models like Gemini-3-Pro and GPT-o3. Results show significant performance heterogeneity: while models achieve up to 92% accuracy on non-optimized tasks, accuracy drops sharply to around 49% on globally optimized algorithms like dynamic programming, revealing strategic over-shifts where models prematurely abandon correct designs due to low-entropy tokens, thus exposing limitations in current training approaches.</div>
<div class="mono" style="margin-top:8px">本文的动机是评估大型推理模型是否真正掌握算法推理，超越现有基准测试的局限，为此提出了AlgBench，这是一个由专家策划的基准，包含超过3000个问题，涵盖27种算法，并按照全面分类法组织。方法上采用以算法为中心的范式构建和评估大型推理模型，对Gemini-3-Pro和GPT-o3等模型进行了实证测试。主要实验结果显示显著的性能异质性：模型在非优化任务上准确率高达92%，但在动态规划等全局优化算法上急剧下降至约49%，揭示了策略性过度转移现象，即模型因低熵标记过早放弃正确算法设计，从而暴露了当前训练范式的根本缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning</div>
<div class="meta-line">Authors: Tonghe Zhang, Chao Yu, Sichang Su, Yu Wang</div>
<div class="meta-line">Venue: Published in The Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025</div>
<div class="meta-line">First: 2025-05-28T08:17:16+00:00 · Latest: 2026-01-08T14:39:03+00:00</div>
<div class="meta-line">Comments: 38 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22094v7">Abs</a> · <a href="https://arxiv.org/pdf/2505.22094v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://reinflow.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose ReinFlow, a simple yet effective online reinforcement learning (RL) framework that fine-tunes a family of flow matching policies for continuous robotic control. Derived from rigorous RL theory, ReinFlow injects learnable noise into a flow policy&#x27;s deterministic path, converting the flow into a discrete-time Markov Process for exact and straightforward likelihood computation. This conversion facilitates exploration and ensures training stability, enabling ReinFlow to fine-tune diverse flow model variants, including Rectified Flow [35] and Shortcut Models [19], particularly at very few or even one denoising step. We benchmark ReinFlow in representative locomotion and manipulation tasks, including long-horizon planning with visual input and sparse reward. The episode reward of Rectified Flow policies obtained an average net growth of 135.36% after fine-tuning in challenging legged locomotion tasks while saving denoising steps and 82.63% of wall time compared to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate of the Shortcut Model policies in state and visual manipulation tasks achieved an average net increase of 40.34% after fine-tuning with ReinFlow at four or even one denoising step, whose performance is comparable to fine-tuned DDIM policies while saving computation time for an average of 23.20%. Project webpage: https://reinflow.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReinFlow：基于在线强化学习的流匹配策略微调方法</div>
<div class="mono" style="margin-top:8px">本文提出ReinFlow，一种简洁高效的在线强化学习框架，用于微调连续机器人控制中的流匹配策略族。基于严谨的强化学习理论，ReinFlow向确定性流策略路径注入可学习噪声，将流转换为离散时间马尔可夫过程，实现精确直观的似然计算。该转换促进探索并保障训练稳定性，使ReinFlow能够微调包括整流流[35]和捷径模型[19]在内的多种流模型变体，尤其适用于极少数甚至单步去噪场景。我们在具代表性的运动与操控任务中评估ReinFlow，包括视觉输入的长时程规划和稀疏奖励任务。在挑战性足式运动任务中，微调后整流流策略的回合奖励平均净增长135.36%，同时相比前沿扩散强化学习微调方法DPPO[43]节省去噪步骤与82.63%的墙钟时间。在状态与视觉操控任务中，经ReinFlow以四步甚至单步去噪微调后，捷径模型策略的成功率平均净提升40.34%，其性能与微调后的DDIM策略相当，同时平均节省23.20%的计算时间。项目页面：https://reinflow.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ReinFlow, an online reinforcement learning framework designed to fine-tune flow matching policies for continuous robotic control, motivated by the need to enhance policy performance while reducing computational costs. The method injects learnable noise into deterministic flow paths, converting them into a Markov Process for exact likelihood computation, which stabilizes training and enables efficient fine-tuning of models like Rectified Flow and Shortcut Models with very few denoising steps. Experimental results show significant improvements: Rectified Flow policies achieved a 135.36% average net reward growth in locomotion tasks while saving denoising steps and 82.63% wall time compared to DPPO, and Shortcut Model policies saw a 40.34% average net increase in success rates for manipulation tasks, matching DDIM performance with 23.20% less computation time.</div>
<div class="mono" style="margin-top:8px">本文提出了ReinFlow，一种在线强化学习框架，旨在微调用于连续机器人控制的流匹配策略，其动机在于提升策略性能并降低计算成本。该方法通过向确定性流路径注入可学习的噪声，将其转换为马尔可夫过程以进行精确似然计算，从而稳定训练并实现对Rectified Flow和Shortcut Models等模型的高效微调，仅需极少甚至一步去噪步骤。实验结果表明显著改进：在运动任务中，Rectified Flow策略的平均净奖励增长达135.36%，同时节省了去噪步骤和82.63%的壁钟时间（相比DPPO）；在操作任务中，Shortcut Model策略的平均成功率净增40.34%，性能与DDIM相当但计算时间平均减少23.20%。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Shaping to Mitigate Reward Hacking in RLHF</div>
<div class="meta-line">Authors: Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</div>
<div class="meta-line">First: 2025-02-26T02:57:59+00:00 · Latest: 2026-01-08T14:33:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18770v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.18770v4">PDF</a> · <a href="https://github.com/PorUna-byte/PAR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR&#x27;s superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过奖励塑形缓解RLHF中的奖励黑客问题</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）对于使大语言模型（LLMs）与人类价值观对齐至关重要。然而，RLHF容易受到奖励黑客攻击，即智能体利用奖励函数的缺陷而非学习预期行为，从而降低对齐效果。尽管奖励塑形有助于稳定RLHF并部分缓解奖励黑客问题，但对塑形技术及其基本原理的系统性研究仍显不足。为填补这一空白，我们对主流奖励塑形方法进行了全面研究。分析提出两个关键设计原则：（1）RL奖励应有界；（2）RL奖励宜采用快速初始增长后逐步收敛的模式。基于这些见解，我们提出偏好即奖励（PAR）方法，利用奖励模型中隐含的偏好作为强化学习信号。PAR具有两个关键的方差缩减特性，有助于稳定RLHF训练过程并有效扩展早停容忍窗口。我们在Gemma2-2B基础模型上使用Ultrafeedback-Binarized和HH-RLHF两个数据集评估PAR。实验结果表明PAR优于其他奖励塑形方法：在AlpacaEval 2.0基准测试中，PAR的胜率至少高出其他方法5个百分点；PAR还展现出卓越的数据效率，仅需单个参考奖励即可实现最优性能，并在完整训练两轮后仍能保持对奖励黑客攻击的鲁棒性。代码已开源：https://github.com/PorUna-byte/PAR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward hacking in Reinforcement Learning from Human Feedback (RLHF), where agents exploit flaws in reward functions instead of learning desired behaviors, thereby undermining alignment. The authors conduct a systematic study of existing reward shaping methods, identifying two key design principles: bounded RL rewards and a reward curve with rapid initial growth followed by gradual convergence. Based on these insights, they propose Preference As Reward (PAR), a novel method that uses latent preferences from the reward model as the RL signal, offering variance-reduction properties to stabilize training and extend the early stopping tolerance window. Experiments on Gemma2-2B with Ultrafeedback-Binarized and HH-RLHF datasets show that PAR outperforms other shaping methods, achieving at least a 5 percentage point higher win rate on AlpacaEval 2.0, while demonstrating strong data efficiency and robustness against reward hacking even after extended training.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习（RLHF）中的奖励黑客问题展开研究，即智能体利用奖励函数的缺陷而非学习预期行为，从而损害对齐效果。作者系统分析了现有奖励塑形方法，提出了两个关键设计原则：有界的RL奖励以及奖励曲线应具有快速初始增长和渐进收敛特性。基于这些发现，他们提出了偏好即奖励（PAR）方法，该方法利用奖励模型中的潜在偏好作为RL信号，具备方差减少特性以稳定训练并延长早停容忍窗口。在Gemma2-2B模型上使用Ultrafeedback-Binarized和HH-RLHF数据集的实验表明，PAR优于其他塑形方法，在AlpacaEval 2.0基准测试中胜率至少高出5个百分点，同时展现出优异的数据效率和鲁棒性，即使在完整训练两轮后仍能有效抵抗奖励黑客攻击。</div>
</details>
</div>
<div class="card">
<div class="title">ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning</div>
<div class="meta-line">Authors: Minda Hu, Zexuan Qiu, Zenan Xu, Kun Li, Bo Zhou, Irwin King</div>
<div class="meta-line">First: 2026-01-08T14:22:58+00:00 · Latest: 2026-01-08T14:22:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04973v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04973v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking&#x27;&#x27;, where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the &#x27;cold start&#x27; phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ConMax：面向高效思维链推理的置信度最大化压缩方法</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）的最新突破表明，广泛的思维链（CoT）生成对于实现复杂认知行为（如自我验证和回溯）以解决复杂任务至关重要。然而，这种能力常导致‘过度思考’现象，即模型生成冗余推理路径，在不提升准确性的情况下增加计算成本。虽然基于推理轨迹的监督微调（SFT）是‘冷启动’阶段的标准范式，但将现有压缩技术应用于这些轨迹往往会损害逻辑连贯性或产生过高的采样成本。本文提出ConMax（置信度最大化压缩），一种新颖的强化学习框架，旨在自动压缩推理轨迹的同时保留关键推理模式。ConMax将压缩构建为奖励驱动的优化问题，通过训练策略来剪除冗余，其优化目标是通过冻结的辅助LRM最大化预测保真度的答案置信度与推理有效性的思维置信度的加权组合。在五个推理数据集上的大量实验表明，ConMax实现了更优的效率-性能平衡：在仅损失0.7%准确率的情况下，推理长度较基线模型减少43%，证明了其为LRMs生成高质量高效训练数据的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the problem of &#x27;overthinking&#x27; in large reasoning models, where extensive chain-of-thought generation increases computational costs without accuracy gains, and existing compression methods risk logical coherence or high sampling costs. The method introduces ConMax, a reinforcement learning framework that formulates compression as a reward-driven optimization, training a policy to prune redundancy by maximizing a combination of answer and thinking confidence using a frozen auxiliary model. Experimental results across five reasoning datasets show that ConMax reduces inference length by 43% over baselines with only a 0.7% accuracy drop, achieving a superior efficiency-performance trade-off for generating high-quality training data.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决大型推理模型中的&#x27;过度思考&#x27;问题，即冗长的思维链生成会增加计算成本而不提升准确性，且现有压缩方法可能损害逻辑连贯性或采样成本过高。方法上提出了ConMax，这是一个强化学习框架，将压缩建模为奖励驱动的优化问题，通过训练策略来剪枝冗余，利用冻结的辅助模型最大化答案置信度和思维置信度的加权组合。在五个推理数据集上的实验结果表明，ConMax相比基线将推理长度减少了43%，而准确率仅下降0.7%，实现了高效的性能权衡，能生成高质量的训练数据。</div>
</details>
</div>
<div class="card">
<div class="title">Text as a Universal Interface for Transferable Personalization</div>
<div class="meta-line">Authors: Yuting Liu, Jian Guan, Jia-Nan Li, Wei Wu, Jiang-Ming Yang, Jianzhe Zhao, Guibing Guo</div>
<div class="meta-line">First: 2026-01-08T14:09:17+00:00 · Latest: 2026-01-08T14:09:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04963v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04963v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box&#x27;&#x27; profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本作为可迁移个性化建模的通用接口</div>
<div class="mono" style="margin-top:8px">本研究探讨大型语言模型中的个性化问题。现有方法主要将用户偏好表示为隐式的模型特定向量或参数，形成难以解释且跨模型/任务迁移的“黑盒”配置文件。我们提出以自然语言作为模型无关、任务无关的通用偏好表示接口。该框架生成可解释、可复用的偏好描述，并能随新交互自然持续演化。为实现此类表征，我们设计了两阶段训练框架：先通过高质量合成数据进行监督微调，再结合强化学习优化长期效用与跨任务迁移能力。基于此框架开发的AlignXplore+通用偏好推理模型可生成文本化偏好摘要。在九个基准测试中，我们的80亿参数模型达到最先进性能——显著超越规模更大的开源模型——同时在跨任务、跨模型族、跨交互格式场景中展现出强大迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of personalizing large language models (LLMs) by moving away from opaque, model-specific vector representations of user preferences. The authors propose using natural language as a universal, interpretable, and transferable interface for preference representation, which can evolve over time. Their method involves a two-stage training framework combining supervised fine-tuning on synthesized data with reinforcement learning to optimize long-term utility and cross-task transfer, resulting in a model called AlignXplore+. Experimental results on nine benchmarks demonstrate that their 8B parameter model achieves state-of-the-art performance, surpassing larger open-source models and showing strong transferability across different tasks, model families, and interaction formats.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLM）的个性化问题，旨在改进以往将用户偏好表示为不透明、模型特定向量或参数的方法。作者提出使用自然语言作为通用、可解释且可迁移的偏好表示接口，以支持偏好的持续演化。其方法采用一个两阶段训练框架，结合了基于高质量合成数据的监督微调和强化学习，以优化长期效用和跨任务迁移能力，由此开发了名为AlignXplore+的通用偏好推理模型。在九个基准测试上的实验结果表明，该8B参数模型实现了最先进的性能，显著超越了更大的开源模型，并在不同任务、模型系列和交互格式间展现出强大的可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following</div>
<div class="meta-line">Authors: Yirong Zeng, Yufei Liu, Xiao Ding, Yutai Hou, Yuxian Wang, Haonan Song, Wu Ning, Dandan Tu, Qixun Zhang, Bibo Cai, Yuxiang He, Ting Liu</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2026-01-08T14:00:51+00:00 · Latest: 2026-01-08T14:00:51+00:00</div>
<div class="meta-line">Comments: ACL under review 13 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\% in performance while achieving a 58\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>精度优于多样性：高精度奖励泛化至鲁棒指令跟随</div>
<div class="mono" style="margin-top:8px">在指令跟随任务中，通过可验证奖励进行强化学习扩展的一个核心观点是：可验证的硬约束与不可验证的软约束的多样性混合对于泛化至未见指令至关重要。本研究通过系统性实证检验挑战了这一主流共识。反直觉的是，我们发现仅使用硬约束训练的模型始终优于混合数据集训练的模型。大量实验表明，奖励精度（而非约束多样性）是实现有效对齐的主要驱动力。大语言模型评判器在检测错误响应时召回率较低，导致严重的奖励破解现象，从而削弱了多样性的优势。此外，注意力机制分析表明，高精度奖励能形成可迁移的指令跟随元技能。基于这些发现，我们提出了一种简单高效的数据中心化优化策略，优先保障奖励精度。在五个基准测试中，该方法以13.4%的性能优势超越基线模型，同时减少58%的训练时间，并在指令跟随之外保持强大的泛化能力。我们的研究主张范式转变：从盲目追求数据多样性转向聚焦高精度奖励。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the prevailing belief that diverse mixtures of verifiable and unverifiable constraints are essential for generalizing instruction-following models, finding instead that high-precision rewards are the key driver. Through systematic experiments, the authors demonstrate that models trained solely on hard, verifiable constraints consistently outperform those on mixed datasets, as low recall in LLM judges for detecting false responses leads to reward hacking that undermines diversity benefits. Analysis shows high-precision rewards foster a transferable meta-skill, motivating a simple data-centric refinement strategy that prioritizes precision; evaluated on five benchmarks, this approach outperforms baselines by 13.4% in performance while reducing training time by 58% and maintaining strong generalization.</div>
<div class="mono" style="margin-top:8px">本文挑战了当前主流观点，即指令跟随模型的泛化需要可验证与不可验证约束的多样化混合，研究发现高精度奖励才是关键驱动因素。通过系统实验，作者证明仅使用硬性可验证约束训练的模型始终优于混合数据集训练的模型，因为大语言模型法官在检测错误响应时召回率低，导致奖励破解，从而削弱了多样性的益处。分析表明高精度奖励能培养可迁移的元技能，据此提出了一种简单有效的数据中心化精炼策略，优先考虑奖励精度；在五个基准测试中评估，该方法性能超越基线13.4%，同时训练时间减少58%，并保持了强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism AI Psychological Counselor</div>
<div class="meta-line">Authors: Qianjun Pan, Junyi Wang, Jie Zhou, Yutao Yang, Junsong Li, Kaiyin Xu, Yougen Zhou, Yihan Li, Jingyuan Zhao, Qin Chen, Ningning Zhou, Kai Chen, Liang He</div>
<div class="meta-line">First: 2026-01-05T05:26:57+00:00 · Latest: 2026-01-08T13:52:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01802v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01802v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PsychEval：面向高拟真AI心理咨询师的多会话多疗法基准测试</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估AI，我们推出\texttt{PsychEval}——一个多会话、多疗法、高拟真度的基准测试，旨在解决三大核心挑战：\textbf{1) 能否训练出高拟真AI咨询师？} 拟真咨询是需持续记忆与动态目标追踪的纵向任务。我们构建了跨三个独立阶段（6-10次会话）的多会话基准，要求模型具备记忆连续性、适应性推理与纵向规划等关键能力。数据集标注了涵盖677项元技能与4577项原子技能的完整专业体系。\textbf{2) 如何训练多疗法AI咨询师？} 现有模型多聚焦单一疗法，而复杂案例常需跨疗法灵活策略。我们构建了涵盖五大疗法流派（心理动力学、行为主义、认知行为疗法、人本存在主义、后现代主义）的数据集，并基于六类核心心理议题建立了融合统一三阶段临床框架的整合疗法体系。\textbf{3) 如何系统评估AI咨询师？} 我们建立了包含来访者维度与咨询师维度的18项疗法专用/通用指标的全方位评估框架，并配套构建了2000余个多样化来访者画像。大量实验分析充分验证了数据集在质量与临床保真度上的优越性。\texttt{PsychEval} 更突破静态基准测试范畴，可作为高保真强化学习环境，支持具备临床责任意识与自适应能力的AI咨询师的自我进化训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable AI in psychological assessment, this paper introduces PsychEval, a benchmark designed to train and evaluate highly realistic, multi-therapy AI counselors. The method involves constructing a multi-session dataset spanning 6-10 sessions across three clinical stages, annotated with over 677 meta-skills, and covering five therapeutic modalities within a unified framework to enable adaptive reasoning and longitudinal planning. The main experimental results validate the dataset&#x27;s superior quality and clinical fidelity through a holistic evaluation framework with 18 metrics, and the benchmark serves as a reinforcement learning environment for training self-evolutionary AI counselors.</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估人工智能，本文提出了PsychEval基准，旨在训练和评估高真实性、多疗法的AI心理咨询师。其方法构建了一个多会话数据集，涵盖三个临床阶段的6-10次会话，标注了超过677项元技能，并整合了五种治疗模式于统一框架中，以支持适应性推理和纵向规划。主要实验结果通过包含18项指标的整体评估框架验证了数据集的优越质量和临床保真度，该基准还可作为强化学习环境用于训练具备临床责任感和自适应能力的进化型AI咨询师。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective</div>
<div class="meta-line">Authors: Ziwen Wang, Shangshang Yang, Xiaoshan Yu, Haiping Ma, Xingyi Zhang</div>
<div class="meta-line">First: 2026-01-08T13:17:40+00:00 · Latest: 2026-01-08T13:17:40+00:00</div>
<div class="meta-line">Comments: KDD2026, 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04918v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04918v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners&#x27; mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers&#x27; domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures&#x27; full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model&#x27;s capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破认知诊断的鲁棒性壁垒：一次性神经架构搜索视角</div>
<div class="mono" style="margin-top:8px">随着网络技术的发展，智能导学系统（ITS）已能够提供日益精准和个性化的学习服务。认知诊断（CD）作为ITS的核心研究任务，旨在通过建模学习行为数据与知识状态之间的映射关系，推断学习者对特定知识概念的掌握程度。然而，现有研究侧重于提升模型性能，却忽视了观测响应数据中普遍存在的噪声污染问题，这严重阻碍了实际应用。此外，当前认知诊断模型（CDMs）的结构设计高度依赖研究者的领域专业知识，未能充分探索架构可能性，导致模型潜力未能完全释放。为解决这一问题，我们提出OSCD——一种用于认知诊断的进化多目标一次性神经架构搜索方法，旨在高效且鲁棒地提升模型评估学习者能力的效果。具体而言，OSCD通过训练和搜索两个阶段运行：在训练阶段，我们构建包含多样化架构组合的搜索空间，并训练以完全二叉树拓扑表示的权重共享超网络，从而超越人工设计先验，全面探索潜在架构；在搜索阶段，我们将异构噪声场景下的最优架构搜索建模为多目标优化问题（MOP），并开发了融合帕累托最优解搜索策略与跨场景性能评估的优化框架。基于真实教育数据集的广泛实验验证了OSCD模型为CD任务发现的最优架构的有效性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for robust cognitive diagnosis models that can handle noisy educational data and overcome the limitations of manually designed architectures, this paper proposes OSCD, a one-shot neural architecture search method. The method employs a two-stage process: first training a weight-sharing supernet over a diverse architectural search space, then formulating architecture selection as a multi-objective optimization problem to find solutions robust to heterogeneous noise. Experimental results on real-world datasets demonstrate that the architectures discovered by OSCD are both effective and robust for cognitive diagnosis tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决认知诊断模型对噪声数据敏感且依赖专家手动设计架构的问题，提出了OSCD，一种基于一次性神经架构搜索的进化多目标方法。该方法分为训练和搜索两阶段：训练阶段通过权重共享的超网探索广泛的架构空间；搜索阶段则将最优架构搜索构建为多目标优化问题，以寻找对异构噪声具有鲁棒性的解决方案。在真实教育数据集上的大量实验验证了OSCD所发现架构的有效性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking</div>
<div class="meta-line">Authors: Sofiene Lassoued, Laxmikant Shrikant Bahetic, Nathalie Weiß-Borkowskib, Stefan Lierc, Andreas Schwunga</div>
<div class="meta-line">Venue: Journal of Manufacturing Systems Journal of Manufacturing Systems Volume 82, October 2025, Pages 405-419</div>
<div class="meta-line">First: 2026-01-08T12:37:02+00:00 · Latest: 2026-01-08T12:37:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04887v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04887v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today&#x27;s rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>柔性制造系统内部物流：基于着色时序Petri网与动作掩码演员-评论家强化学习的AGV动态优化与工具共享</div>
<div class="mono" style="margin-top:8px">柔性制造系统（FMS）在当今快速发展的制造业环境中对优化生产流程至关重要。本文通过同时集成自动导引车（AGV）和工具共享系统引入额外复杂性，拓展了传统作业车间调度问题。我们提出一种结合着色时序Petri网（CTPN）与基于模型的演员-评论家强化学习（MBRL）的新方法，有效应对FMS的多维度挑战。CTPN提供形式化建模结构与动态动作掩码机制，显著压缩动作搜索空间；MBRL则通过习得策略确保对动态环境的适应性。利用MBRL优势，我们引入前瞻策略优化AGV定位，提升运行效率。方法在小型公共基准集及受Taillard基准启发新开发的大规模基准集上进行评估。结果表明：在较小实例中与传统方法性能相当，在较大实例中完工时间指标更优，同时计算时间减少十倍。为确保可复现性，我们开发了兼容OpenAI Gym的环境与实例生成器，并通过消融实验评估各框架组件对整体性能的贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to optimize complex Flexible Manufacturing Systems (FMS) that integrate automated guided vehicles (AGVs) and tool sharing, this paper proposes a novel method combining Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL). The CTPNs provide formal modeling and dynamic action masking to reduce the action space, while MBRL, enhanced with a lookahead strategy for AGV positioning, ensures adaptability. Experimental results on public benchmarks and a new large-scale benchmark show the approach matches traditional methods on small instances and outperforms them on larger ones, achieving superior makespan and a tenfold reduction in computation time.</div>
<div class="mono" style="margin-top:8px">为优化集成自动导引车（AGV）和刀具共享的复杂柔性制造系统（FMS），本文提出了一种结合着色时序Petri网（CTPN）和基于演员-评论家模型的强化学习（MBRL）的新方法。CTPN提供形式化建模和动态动作屏蔽以缩减动作空间，而MBRL结合AGV前瞻定位策略确保适应性。在公共基准和新开发的大规模基准上的实验结果表明，该方法在小型实例上与传统方法相当，在大型实例上则表现更优，实现了更短的总完工时间和计算时间减少十倍。</div>
</details>
</div>
<div class="card">
<div class="title">From Actions to Words: Towards Abstractive-Textual Policy Summarization in RL</div>
<div class="meta-line">Authors: Sahar Admoni, Assaf Hallak, Yftah Ziser, Omer Ben-Porat, Ofra Amir</div>
<div class="meta-line">First: 2025-03-13T16:10:14+00:00 · Latest: 2026-01-08T11:06:58+00:00</div>
<div class="meta-line">Comments: In Proceedings of AAMAS 2026 (The 25th International Conference on Autonomous Agents and Multi-Agent Systems)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10509v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.10509v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Explaining reinforcement learning agents is challenging because policies emerge from complex reward structures and neural representations that are difficult for humans to interpret. Existing approaches often rely on curated demonstrations that expose local behaviors but provide limited insight into an agent&#x27;s global strategy, leaving users to infer intent from raw observations. We propose SySLLM (Synthesized Summary using Large Language Models), a framework that reframes policy interpretation as a language-generation problem. Instead of visual demonstrations, SySLLM converts spatiotemporal trajectories into structured text and prompts an LLM to generate coherent summaries describing the agent&#x27;s goals, exploration style, and decision patterns. SySLLM scales to long-horizon, semantically rich environments without task-specific fine-tuning, leveraging LLM world knowledge and compositional reasoning to capture latent behavioral structure across policies. Expert evaluations show strong alignment with human analyses, and a large-scale user study found that 75.5% of participants preferred SySLLM summaries over state-of-the-art demonstration-based explanations. Together, these results position abstractive textual summarization as a paradigm for interpreting complex RL behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从行动到语言：强化学习中抽象文本策略摘要的探索</div>
<div class="mono" style="margin-top:8px">解释强化学习智能体具有挑战性，因为策略源自复杂的奖励结构和难以被人理解的神经表征。现有方法通常依赖精心设计的演示来展示局部行为，但对智能体全局策略的洞察有限，用户需从原始观察中推断意图。我们提出SySLLM（基于大语言模型的合成摘要框架），将策略解释重构为语言生成问题。SySLLM将时空轨迹转化为结构化文本，并提示大语言模型生成连贯摘要，描述智能体的目标、探索风格和决策模式，而非依赖视觉演示。该框架无需任务特定微调即可扩展到长周期、语义丰富的环境，利用大语言模型的世界知识和组合推理能力捕捉策略间的潜在行为结构。专家评估显示其与人类分析高度一致，大规模用户研究表明75.5%的参与者更青睐SySLLM摘要而非当前最先进的基于演示的解释。这些成果共同确立了抽象文本摘要作为解释复杂强化学习行为的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of interpreting complex reinforcement learning policies that are opaque to humans, this paper proposes SySLLM, a framework that reframes policy interpretation as a language-generation problem. The method converts agent trajectories into structured text and uses a large language model (LLM) to generate abstractive summaries of the agent&#x27;s goals and decision patterns, requiring no task-specific fine-tuning. Experimental results from expert evaluations and a user study show strong alignment with human analysis, with 75.5% of participants preferring SySLLM summaries over demonstration-based explanations, positioning textual summarization as an effective paradigm for RL interpretation.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解释强化学习智能体复杂且不透明的策略对人类而言十分困难。为此，论文提出了SySLLM框架，将策略解释重新定义为语言生成问题，其方法是将智能体的时空轨迹转化为结构化文本，并利用大语言模型生成描述其目标与决策模式的抽象摘要，无需针对具体任务进行微调。主要实验结果表明，专家评估显示其与人类分析高度一致，大规模用户研究中75.5%的参与者更偏好SySLLM的摘要而非基于演示的现有方法，从而确立了抽象文本摘要作为解释复杂强化学习行为的有效范式。</div>
</details>
</div>
<div class="card">
<div class="title">Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Chengyandan Shen, Christoffer Sloth</div>
<div class="meta-line">First: 2025-09-04T10:02:32+00:00 · Latest: 2026-01-08T10:57:57+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for Journal publication in Frontiers in Robotics and AI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04069v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04069v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes an exploration-efficient Deep Reinforcement Learning with Reference policy (DRLR) framework for learning robotics tasks that incorporates demonstrations. The DRLR framework is developed based on an algorithm called Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve IBRL by modifying the action selection module. The proposed action selection module provides a calibrated Q-value, which mitigates the bootstrapping error that otherwise leads to inefficient exploration. Furthermore, to prevent the RL policy from converging to a sub-optimal policy, SAC is used as the RL policy instead of TD3. The effectiveness of our method in mitigating bootstrapping error and preventing overfitting is empirically validated by learning two robotics tasks: bucket loading and open drawer, which require extensive interactions with the environment. Simulation results also demonstrate the robustness of the DRLR framework across tasks with both low and high state-action dimensions, and varying demonstration qualities. To evaluate the developed framework on a real-world industrial robotics task, the bucket loading task is deployed on a real wheel loader. The sim2real results validate the successful deployment of the DRLR framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用先验演示通过高效探索深度强化学习解决机器人任务</div>
<div class="mono" style="margin-top:8px">本文提出了一种融合演示的、基于参考策略的高效探索深度强化学习框架，用于学习机器人任务。该框架基于模仿引导强化学习算法开发，通过改进动作选择模块来优化原算法。改进后的动作选择模块提供校准后的Q值，从而减轻导致探索效率低下的引导误差。此外，为防止强化学习策略收敛至次优解，采用SAC替代TD3作为强化学习策略。通过两项需与环境大量交互的机器人任务验证了该方法在减轻引导误差和防止过拟合方面的有效性。仿真结果还表明，该框架在不同状态-动作维度及演示质量的任務中均具鲁棒性。为在实际工业机器人任务中评估该框架，将铲斗装载任务部署于真实轮式装载机，仿真到现实的迁移结果验证了框架的成功应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for more exploration-efficient deep reinforcement learning (DRL) in robotics, particularly when prior demonstrations are available. The method introduces the DRLR framework, building on Imitation Bootstrapped Reinforcement Learning (IBRL) but modifying the action selection module to provide calibrated Q-values, thereby reducing bootstrapping error and inefficient exploration; it also employs SAC instead of TD3 to avoid sub-optimal convergence. Experimental results on simulated robotics tasks like bucket loading and open drawer show the framework mitigates bootstrapping error and overfitting, demonstrating robustness across varying state-action dimensions and demonstration qualities, with successful sim2real deployment validated on a real wheel loader for bucket loading.</div>
<div class="mono" style="margin-top:8px">本文旨在解决机器人任务中深度强化学习探索效率低的问题，特别是在已有演示数据的情况下。方法上提出了DRLR框架，基于模仿引导强化学习（IBRL）改进，通过修改动作选择模块提供校准的Q值以减少引导误差和低效探索，并采用SAC替代TD3以避免策略陷入次优。实验在模拟的铲斗装载和开抽屉等机器人任务上进行，结果表明该框架有效缓解了引导误差和过拟合，对不同状态-动作维度和演示质量均表现出鲁棒性，并在真实轮式装载机上成功实现了铲斗装载任务的仿真到现实部署验证。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-08T10:46:04+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但这些模型仍易受对抗性越狱攻击的影响——攻击者通过精心设计的细微扰动绕过安全机制，触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等问题，难以应用于现实黑盒场景。为克服这些局限，我们提出通过采用同步扰动随机逼近的零阶优化方法（ZO-SPSA）对LVLMs实施黑盒越狱攻击。ZO-SPSA具备三大优势：（1）通过输入输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低资源需求，减少GPU内存消耗。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，同时保持与白盒方法相当的不可感知扰动。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱在现实场景中的可行性，并暴露了当前LVLMs安全机制的关键弱点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods that require full model access, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free approach approximates gradients through input-output interactions without needing internal model knowledge, making it model-agnostic and resource-efficient. Experimental results on models like InstructBLIP, LLaVA, and MiniGPT-4 show a high jailbreak success rate of up to 83.0% on InstructBLIP with imperceptible perturbations, and adversarial examples from MiniGPT-4 demonstrate strong transferability, achieving an attack success rate of 64.18% on other LVLMs, highlighting critical safety weaknesses in these systems.</div>
<div class="mono" style="margin-top:8px">本研究针对大型视觉语言模型（LVLMs）易受对抗性越狱攻击的漏洞，以及现有白盒方法需要完整模型访问权限、计算成本高且实用性不足的问题，提出了一种基于零阶优化和同步扰动随机逼近（ZO-SPSA）的黑盒攻击方法。该方法无需模型内部知识，通过输入输出交互进行梯度近似，具有模型无关性和低资源消耗的优势。在InstructBLIP、LLaVA和MiniGPT-4等模型上的实验结果表明，该方法在InstructBLIP上实现了高达83.0%的越狱成功率，且扰动难以察觉；同时，从MiniGPT-4生成的对抗样本在其他LVLMs上表现出强迁移性，攻击成功率可达64.18%，揭示了当前LVLMs安全机制的重大缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning</div>
<div class="meta-line">Authors: Caijun Xu, Changyi Xiao, Zhongyuan Peng, Xinrun Wang, Yixin Cao</div>
<div class="meta-line">First: 2026-01-08T10:42:04+00:00 · Latest: 2026-01-08T10:42:04+00:00</div>
<div class="meta-line">Comments: 19 pages,5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04809v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model&#x27;s capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCALER：用于推理的合成可扩展自适应学习环境</div>
<div class="mono" style="margin-top:8px">强化学习为提升大语言模型的推理能力提供了原则性方法，但其有效性依赖于随模型演化保持信息量的训练信号。实践中，当任务难度与模型能力不匹配，或训练被少数重复问题模式主导时，强化学习的进展常会放缓。为协同解决这些问题，我们提出SCALER（用于推理的合成可扩展自适应学习环境），该框架通过自适应环境设计维持有效的学习信号。SCALER引入可扩展的合成流程，将真实编程问题转化为具有可控难度和无限实例生成的、可验证的推理环境，使强化学习训练能突破有限数据集的限制，同时保持强正确性保证。在此基础上，SCALER进一步采用自适应多环境强化学习策略，动态调整实例难度并筛选活跃环境集合，以追踪模型能力边界并保持分布多样性。这种协同适应机制避免了奖励稀疏性，减轻了对狭窄任务模式的过拟合，并支持训练过程中的持续改进。大量实验表明，SCALER在多种推理基准测试中持续优于基于数据集的强化学习基线，并展现出更稳定、更长周期的训练动态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge that reinforcement learning (RL) for improving large language model reasoning often suffers from diminishing training signals when task difficulty mismatches model capability or when training data lacks diversity. To address this, the method introduces SCALER, a framework that creates a synthetic, scalable learning environment by converting real-world programming problems into verifiable reasoning tasks with controllable difficulty and unlimited instance generation, coupled with an adaptive multi-environment RL strategy that dynamically adjusts difficulty and curates environments to match the model&#x27;s evolving capabilities. Experimental results demonstrate that SCALER consistently outperforms dataset-based RL baselines across various reasoning benchmarks, showing more stable and sustained training progress.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，用于提升大语言模型推理能力的强化学习常因任务难度与模型能力不匹配或训练数据缺乏多样性而导致训练信号减弱。为解决此问题，方法提出了SCALER框架，该框架通过将现实编程问题转化为可验证、难度可控且可无限生成实例的推理环境，构建了一个合成的可扩展学习环境，并结合自适应多环境强化学习策略，动态调整难度并筛选环境以匹配模型能力演进。实验结果表明，SCALER在多种推理基准测试中持续优于基于数据集的强化学习基线，展现出更稳定、持久的训练动态。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning</div>
<div class="meta-line">Authors: Siyuan Gan, Jiaheng Liu, Boyan Wang, Tianpei Yang, Runqing Miao, Yuyao Zhang, Fanyu Meng, Junlan Feng, Linjian Meng, Jing Huo, Yang Gao</div>
<div class="meta-line">First: 2026-01-08T10:38:41+00:00 · Latest: 2026-01-08T10:38:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04805v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04805v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT&#x27;s responses, which are classified as not using thinking, remains below 10% across all tested datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于思考的非思考：通过强化学习解决混合推理模型训练中的奖励破解问题</div>
<div class="mono" style="margin-top:8px">大型推理模型因其卓越性能备受关注，但其性能主要源于长链思维过程，显著增加了计算开销。为应对过度思考问题，现有研究聚焦于使用强化学习训练混合推理模型，使其能根据查询复杂度自动决定是否启动思考。然而强化学习易遭遇奖励破解问题，例如模型实际执行思考却被判定为未思考，导致奖励分配错误。现有缓解方案或采用计算成本高昂的监督微调，或对非思考响应强制统一令牌限制，效果均有限。本文提出基于思考的非思考方法，该方法无需监督微调，而是通过利用思考型响应的解决方案信息，为不同查询的非思考响应设置差异化的最大令牌使用量。在五个数学基准测试中，该方法相比DeepSeek-R1-Distill-Qwen-1.5B/7B和DeepScaleR-1.5B减少约50%令牌使用量，同时显著提升准确率，在所有测试方法中实现了准确性与效率的最优平衡。此外，该方法被归类为非思考的响应中，奖励破解问题发生率在所有测试数据集均低于10%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the reward hacking problem in training hybrid reasoning models via reinforcement learning, where models incorrectly receive rewards for not thinking despite engaging in it, leading to inefficiency. The proposed method, Thinking-Based Non-Thinking (TNT), avoids supervised fine-tuning and instead dynamically sets maximum token limits for non-thinking responses by leveraging information from thinking-based solutions, thereby reducing computational overhead. Experimental results on five mathematical benchmarks show that TNT cuts token usage by approximately 50% compared to baseline models like DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly boosting accuracy and maintaining reward hacking probability below 10%, achieving an optimal accuracy-efficiency trade-off.</div>
<div class="mono" style="margin-top:8px">本文针对基于强化学习训练混合推理模型中的奖励黑客问题，即模型在思考时被误判为未思考而获得错误奖励，导致效率低下，提出了思考式非思考方法。该方法避免使用监督微调，而是通过利用思考响应中的解决方案信息，为不同查询动态设置非思考响应的最大令牌限制，从而降低计算开销。在五个数学基准测试上的实验结果表明，与DeepSeek-R1-Distill-Qwen-1.5B/7B和DeepScaleR-1.5B等基线模型相比，TNT将令牌使用量减少约50%，同时显著提高准确性，并将奖励黑客问题概率保持在10%以下，实现了准确性与效率的最佳平衡。</div>
</details>
</div>
<div class="card">
<div class="title">AgentOCR: Reimagining Agent History via Optical Self-Compression</div>
<div class="meta-line">Authors: Lang Feng, Fuchao Yang, Feng Chen, Xin Cheng, Haiyang Xu, Zhenglin Wan, Ming Yan, Bo An</div>
<div class="meta-line">First: 2026-01-08T10:10:20+00:00 · Latest: 2026-01-08T10:10:20+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04786v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04786v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\% of text-based agent performance while substantially reducing token consumption (&gt;50\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentOCR：通过光学自压缩重构智能体历史记录</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的最新进展使得通过强化学习（RL）在多轮交互轨迹上训练智能体系统成为可能，但实际部署受限于快速增长的文本历史记录，这些记录会扩大令牌预算和内存使用。我们提出了AgentOCR框架，该框架通过将累积的观察-行动历史表示为紧凑的渲染图像，利用视觉令牌更高的信息密度。为实现多轮推演的可扩展性，AgentOCR提出了分段光学缓存机制。通过将历史分解为可哈希的片段并维护视觉缓存，该机制消除了冗余的重复渲染。除了固定渲染外，AgentOCR引入了智能体自压缩技术，使智能体主动发出压缩率，并通过压缩感知奖励进行训练，以自适应地平衡任务成功率和令牌效率。我们在ALFWorld和基于搜索的问答等具有挑战性的智能体基准测试中进行了广泛实验。显著的结果表明，AgentOCR在显著降低令牌消耗（&gt;50%）的同时，保持了基于文本的智能体95%以上的性能，实现了持续的令牌和内存效率提升。我们的进一步分析验证了分段光学缓存带来的20倍渲染加速效果，以及自压缩机制的有效策略平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for AgentOCR stems from the practical bottleneck in deploying agentic systems, where rapidly growing textual histories from multi-turn interactions inflate token budgets and memory usage. The method introduces a framework that represents observation-action history as a compact rendered image to exploit the higher information density of visual tokens, incorporating segment optical caching to eliminate redundant re-rendering by decomposing history into hashable segments, and agentic self-compression where the agent actively emits a compression rate trained with compression-aware reward. Main experimental results on benchmarks like ALFWorld and search-based QA show that AgentOCR preserves over 95% of text-based agent performance while reducing token consumption by more than 50%, with analysis validating a 20x rendering speedup from caching and effective strategic balancing.</div>
<div class="mono" style="margin-top:8px">AgentOCR的动机源于部署智能体系统时的实际瓶颈，即多轮交互中快速增长的文本历史会扩大令牌预算和内存使用。该方法提出了一个框架，将观察-行动历史表示为紧凑的渲染图像，以利用视觉令牌更高的信息密度，并通过分段光学缓存将历史分解为可哈希的片段来消除冗余重新渲染，同时引入智能体自压缩机制，使智能体主动发出压缩率并通过压缩感知奖励进行训练，以自适应平衡任务成功与令牌效率。在ALFWorld和基于搜索的QA等基准测试中的主要实验结果表明，AgentOCR在保持超过95%的基于文本的智能体性能的同时，将令牌消耗降低了50%以上，分析进一步验证了缓存带来的20倍渲染加速和自压缩的有效战略平衡。</div>
</details>
</div>
<div class="card">
<div class="title">AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search</div>
<div class="meta-line">Authors: Zefang Zong, Dingwei Chen, Yang Li, Qi Yi, Bo Zhou, Chengming Li, Bo Qian, Peng Chen, Jie Jiang</div>
<div class="meta-line">First: 2026-01-08T09:35:49+00:00 · Latest: 2026-01-08T09:35:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04767v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04767v1">PDF</a> · <a href="https://github.com/zzfoutofspace/ATPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AT²PO：基于树搜索的智能体回合制策略优化</div>
<div class="mono" style="margin-top:8px">大语言模型智能体已成为通过交替内部推理与外部工具交互来处理多回合任务的强大系统。智能体强化学习作为进一步精炼这些能力的关键后训练范式，近期引起了广泛研究关注。本文提出AT²PO（基于树搜索的智能体回合制策略优化），这是一个面向多回合智能体强化学习的统一框架，解决了三个核心挑战：探索多样性受限、稀疏信用分配以及策略优化失准。AT²PO引入了回合级树结构，同步实现熵引导树扩展以支持策略探索，以及回合级信用分配以实现稀疏结果下的细粒度奖励传播。在此基础上，我们提出智能体回合制策略优化——一种与智能体交互自然决策粒度对齐的回合级学习目标。该优化方法与树搜索正交，可无缝集成至任何多回合强化学习流程。在七个基准测试中的实验表明，其平均性能较现有最优基线提升最高达1.84个百分点，消融研究验证了各模块的有效性。代码已开源：https://github.com/zzfoutofspace/ATPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces AT²PO, a framework designed to enhance multi-turn LLM agents through agentic reinforcement learning, addressing challenges like limited exploration diversity, sparse credit assignment, and misaligned policy optimization. The method employs a turn-level tree structure that combines Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation, alongside Agentic Turn-based Policy Optimization to align policy updates with agentic decision granularity. Experimental results across seven benchmarks show consistent improvements over state-of-the-art baselines by up to 1.84 percentage points on average, with ablation studies confirming the effectiveness of each component.</div>
<div class="mono" style="margin-top:8px">本文提出了AT²PO框架，旨在通过智能体强化学习提升多轮LLM智能体的性能，解决探索多样性有限、信用分配稀疏和策略优化不对齐等核心挑战。该方法采用轮次级树结构，结合了用于战略探索的熵引导树扩展和用于细粒度奖励传播的轮次级信用分配，并引入智能体轮次级策略优化以对齐策略更新与智能体决策粒度。在七个基准测试上的实验结果表明，相比最先进基线平均提升高达1.84个百分点，消融研究验证了各组成部分的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GAPO: Robust Advantage Estimation for Real-World Code LLMs</div>
<div class="meta-line">Authors: Jianqing Zhang, Zhezheng Hao, Wei Xia, Hande Dong, Hong Wang, Chenxing Wei, Yuyan Zhou, Yubin Qi, Qiang Lin, Jian Cao</div>
<div class="meta-line">First: 2025-10-22T03:37:49+00:00 · Latest: 2026-01-08T08:42:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21830v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.21830v4">PDF</a> · <a href="https://github.com/TsingZ0/verl-GAPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods, such as GRPO, are popular due to their critic-free and normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable noise, leading to distorted advantage computation and increased rollout outliers. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an interval with the highest SNR (Signal to Noise Ratio) per prompt and uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation to reduce noise further. This adaptive Q robustly handles rollout noise while remaining plug-and-play and efficient. We evaluate GAPO on nine instruction-tuned LLMs (3B-14B) using a collected large dataset of 51,844 real-world, history-aware code-editing tasks spanning 10 programming languages. GAPO yields up to 4.35 in-domain (ID) and 5.30 out-of-domain (OOD) exact-match improvements over GRPO and its variant DAPO, while achieving lower clipping ratios and higher GPU throughput. Code: https://github.com/TsingZ0/verl-GAPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAPO：面向现实世界代码大语言模型的鲁棒优势估计方法</div>
<div class="mono" style="margin-top:8px">强化学习（RL）被广泛用于代码编辑场景中大语言模型（LLM）的后训练，其中无评论家且归一化优势估计的组相对方法（如GRPO）备受青睐。然而，在实际代码编辑任务中，奖励分布常存在偏斜与不可预测噪声，导致优势计算失真并增加训练轨迹异常值。为解决此问题，我们提出组自适应策略优化（GAPO），该方法自适应地为每个提示寻找信噪比（SNR）最高的区间，并以该区间中位数作为自适应Q值替代组均值进行优势计算，从而进一步抑制噪声。这种自适应Q值在保持即插即用与高效性的同时，能鲁棒处理训练轨迹噪声。我们在涵盖10种编程语言、包含51,844个现实世界历史感知代码编辑任务的大规模数据集上，对九个指令微调LLM（3B-14B）进行评估。GAPO相比GRPO及其变体DAPO，在领域内（ID）和领域外（OOD）的精确匹配率分别提升最高达4.35和5.30，同时实现了更低的裁剪比率与更高的GPU吞吐量。代码：https://github.com/TsingZ0/verl-GAPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for GAPO arises from the limitations of existing group-relative reinforcement learning methods like GRPO in real-world code editing, where skewed reward distributions and unpredictable noise distort advantage estimation and cause rollout outliers. The method introduces Group Adaptive Policy Optimization, which adaptively identifies an interval with the highest signal-to-noise ratio per prompt and uses the median within that interval as an adaptive Q-value to replace the group mean, thereby reducing noise while remaining plug-and-play and efficient. Experimental results on a large dataset of 51,844 code-editing tasks across 10 programming languages show that GAPO achieves up to 4.35 in-domain and 5.30 out-of-domain exact-match improvements over GRPO and DAPO, along with lower clipping ratios and higher GPU throughput.</div>
<div class="mono" style="margin-top:8px">GAPO的提出动机源于现有基于组别的强化学习方法（如GRPO）在现实世界代码编辑中的局限性，即奖励分布偏斜和不可预测的噪声会导致优势估计失真并产生异常输出。该方法引入了组自适应策略优化，通过自适应地寻找每个提示中信噪比最高的区间，并使用该区间中位数作为自适应Q值来替代组均值，从而在保持即插即用和高效的同时降低噪声影响。在涵盖10种编程语言、包含51,844个现实世界代码编辑任务的大型数据集上的实验结果表明，GAPO相比GRPO及其变体DAPO，在领域内和领域外精确匹配率上分别提升最高达4.35和5.30，同时实现了更低的裁剪比例和更高的GPU吞吐量。</div>
</details>
</div>
<div class="card">
<div class="title">ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving</div>
<div class="meta-line">Authors: Chang Zhao, Zheming Yang, Yunqing Hu, Qi Guo, Zijian Wang, Pengcheng Li, Wen Ji</div>
<div class="meta-line">First: 2026-01-08T08:30:36+00:00 · Latest: 2026-01-08T08:30:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ThinkDrive：自动驾驶中思维链引导的渐进式强化学习微调</div>
<div class="mono" style="margin-top:8px">随着大语言模型（LLM）技术的快速发展，其在自动驾驶领域的应用日益广泛。然而，现有方法存在推理缺乏结构化、泛化能力差以及与人类驾驶意图不一致等问题。尽管思维链（CoT）推理提升了决策透明度，但传统的监督微调（SFT）未能充分发挥其潜力，而强化学习（RL）方法则面临不稳定和推理深度不足的挑战。本文提出ThinkDrive，一种用于自动驾驶的CoT引导渐进式RL微调框架，将显式推理与难度感知自适应策略优化相结合。该方法采用两阶段训练策略：首先使用CoT解释进行SFT，然后应用渐进式RL，通过难度感知自适应策略优化器根据样本复杂度动态调整学习强度。我们在公开数据集上评估了该方法，结果显示ThinkDrive在考试、易考和准确率指标上分别优于强RL基线1.45%、1.95%和1.01%。此外，采用本方法训练的20亿参数模型在考试指标上超越了规模大得多的GPT-4o达3.28%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing methods in autonomous driving, such as unstructured reasoning and poor generalization, this paper introduces ThinkDrive, a framework that integrates Chain-of-Thought reasoning with progressive reinforcement learning fine-tuning. The method employs a two-stage approach: first using supervised fine-tuning with CoT explanations, followed by progressive reinforcement learning with a difficulty-aware adaptive policy optimizer that adjusts learning intensity based on sample complexity. Experimental results on a public dataset demonstrate that ThinkDrive outperforms strong RL baselines, achieving improvements of 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy metrics, respectively, and a 2B-parameter model trained with this method surpasses GPT-4o by 3.28% on the exam metric.</div>
<div class="mono" style="margin-top:8px">针对自动驾驶领域现有方法存在推理结构混乱、泛化能力差等问题，本文提出了ThinkDrive框架，将思维链推理与渐进式强化学习微调相结合。该方法采用两阶段训练策略：首先使用带有思维链解释的监督微调，然后应用渐进式强化学习，并配备一个难度感知的自适应策略优化器，根据样本复杂度动态调整学习强度。在公开数据集上的实验结果表明，ThinkDrive在考试、易考和准确率指标上分别优于强化的学习基线1.45%、1.95%和1.01%，且采用此方法训练的20亿参数模型在考试指标上超越了GPT-4o达3.28%。</div>
</details>
</div>
<div class="card">
<div class="title">Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement</div>
<div class="meta-line">Authors: Mingyu Xu, Cheng Fang, Keyue Jiang, Yuqian Zheng, Yanghua Xiao, Baojian Zhou, Qifang Zhao, Suhang Zheng, Xiuwen Zhu, Jiyang Tang, Yongchi Zhao, Yijia Luo, Zhiqi Bai, Yuchi Xu, Wenbo Su, Wei Wang, Bing Zhao, Lin Qu, Xiaoxiao Xu</div>
<div class="meta-line">First: 2026-01-04T15:23:18+00:00 · Latest: 2026-01-08T08:13:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01562v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01562v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Logics-STEM：通过失败驱动的后训练与文档知识增强赋能大语言模型推理</div>
<div class="mono" style="margin-top:8px">我们提出Logics-STEM，这是一个基于Logics-STEM-SFT-Dataset微调的前沿推理模型。该数据集规模达1000万，质量高且多样性丰富，是最大规模的开源长链思维语料库之一。Logics-STEM专注于科学、技术、工程和数学（STEM）领域的推理任务，在STEM相关基准测试中表现卓越，在80亿参数规模上平均优于次优模型4.68%。我们将性能提升归因于数据-算法协同设计引擎，通过联合优化使模型拟合推理背后的黄金标准分布。数据层面，Logics-STEM-SFT-Dataset通过精心设计的五阶段数据策管引擎构建，包括标注、去重、净化、蒸馏和分层采样，确保质量、多样性和可扩展性。算法层面，我们的失败驱动后训练框架在监督微调阶段，针对模型失败区域进行定向知识检索与数据合成，有效指导第二阶段SFT或强化学习，以更好地拟合目标分布。Logics-STEM的卓越实证性能揭示了大规模开源数据与精心设计合成数据结合的巨大潜力，凸显了数据-算法协同设计在通过后训练增强推理能力中的关键作用。我们公开提供Logics-STEM模型（80亿和320亿参数）及Logics-STEM-SFT-Dataset（1000万和降采样220万版本），以支持开源社区的未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Logics-STEM, a reasoning model motivated by the need to enhance large language model (LLM) performance in STEM domains through improved data and training methods. Its method involves a data-algorithm co-design engine: data-wise, it constructs a high-quality 10M-scale dataset via a five-stage curation process (annotation, deduplication, decontamination, distillation, and stratified sampling), while algorithm-wise, it employs a failure-driven post-training framework that uses targeted knowledge retrieval and data synthesis around model failures to guide further fine-tuning or reinforcement learning. Experimentally, Logics-STEM achieves state-of-the-art results on STEM benchmarks, showing an average improvement of 4.68% over the next-best 8B-scale model, demonstrating the effectiveness of its co-design approach in boosting reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文提出了Logics-STEM模型，其动机是通过改进数据和训练方法来增强大语言模型在科学、技术、工程和数学领域的推理能力。方法上采用数据-算法协同设计引擎：数据方面，通过精心设计的五阶段数据整理流程构建了一个高质量、规模达1000万的数据集；算法方面，采用失败驱动的后训练框架，利用针对模型失败区域的知识检索和数据合成来指导进一步的监督微调或强化学习。实验结果表明，Logics-STEM在STEM相关基准测试中表现优异，相比同规模最佳模型平均提升4.68%，验证了其协同设计策略在提升推理性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning</div>
<div class="meta-line">Authors: Yinuo Wang, Mining Tan, Wenxiang Jiao, Xiaoxi Li, Hao Wang, Xuanyu Zhang, Yuan Lu, Weiming Dong</div>
<div class="meta-line">First: 2026-01-08T08:08:35+00:00 · Latest: 2026-01-08T08:08:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04698v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs&#x27; set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TourPlanner：一种融合约束门控强化学习的竞争性共识旅行规划框架</div>
<div class="mono" style="margin-top:8px">旅行规划是一个复杂的决策过程，需要综合多维度信息以构建行程。然而现有方法面临三大挑战：(1) 在保持高召回率的同时筛选候选兴趣点；(2) 单一路径推理限制了可行解空间的探索能力；(3) 硬约束与软约束的同步优化仍具难度。为此，我们提出TourPlanner——一个融合多路径推理与约束门控强化学习的综合框架。具体而言，首先通过个性化召回与空间优化流程构建空间感知的候选兴趣点集；随后提出竞争性共识思维链多路径推理范式，增强可行解空间的探索能力；最后在强化学习阶段引入基于Sigmoid的门控机制，实现硬约束满足后对软约束满足度的动态优化。在旅行规划基准测试中，TourPlanner在方案可行性与用户偏好契合度上均显著超越现有方法，达到最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses key challenges in automated travel planning, including the need to efficiently prune points of interest while maintaining high recall, explore diverse solution paths, and jointly optimize hard and soft constraints. To tackle these issues, the authors propose TourPlanner, a framework that first uses a Personalized Recall and Spatial Optimization workflow to build a spatially-aware candidate set, then employs a Competitive Consensus Chain-of-Thought for multi-path reasoning to better explore the solution space, and finally integrates a sigmoid-based gating mechanism into reinforcement learning to dynamically prioritize soft constraints only after hard constraints are satisfied. Experiments on travel planning benchmarks show that TourPlanner achieves state-of-the-art performance, significantly outperforming existing methods in both feasibility and alignment with user preferences.</div>
<div class="mono" style="margin-top:8px">该研究针对自动化旅行规划中的关键挑战，包括在保持高召回率的同时高效筛选兴趣点、探索多样化的解决方案路径以及共同优化硬约束和软约束。为解决这些问题，作者提出了TourPlanner框架，该框架首先使用个性化召回与空间优化工作流构建空间感知的候选兴趣点集，然后采用竞争共识思维链进行多路径推理以更好地探索解空间，最后将基于Sigmoid的门控机制集成到强化学习阶段，动态地在满足硬约束后才优先考虑软约束。在旅行规划基准测试上的实验结果表明，TourPlanner实现了最先进的性能，在可行性和用户偏好对齐方面显著超越了现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models</div>
<div class="meta-line">Authors: Huayi Liu</div>
<div class="meta-line">First: 2026-01-08T08:06:58+00:00 · Latest: 2026-01-08T08:06:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04696v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大模型语义理解的数字化转型驱动机制构建方法</div>
<div class="mono" style="margin-top:8px">在数字化转型过程中，企业面临非结构化数据语义理解不足、驱动机制缺乏智能决策依据等问题。本研究提出一种结合大语言模型（LLM）与知识图谱的方法：首先利用微调BERT模型对多源异构文本进行实体识别与关系抽取，并采用GPT-4生成语义增强的向量表示；其次设计双层图神经网络（GNN）架构，将LLM输出的语义向量与业务元数据融合，构建动态可扩展的企业知识图谱；随后引入强化学习优化决策路径生成，通过奖励函数驱动机制迭代。在制造业案例中，该机制使设备故障场景响应时间从7.8小时降至3.7小时，F1值达94.3%，年度数字化转型成本中决策失误补偿降低45.3%。该方法通过融合大模型语义理解与结构化知识，显著提升了数字化转型驱动机制的智能化水平与执行效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by challenges in digital transformation, such as poor semantic understanding of unstructured data and a lack of intelligent decision support in driving mechanisms, this paper proposes a method integrating large language models (LLMs) and knowledge graphs. The method employs a fine-tuned BERT model for entity and relation extraction from multi-source texts, uses GPT-4 for semantic vector enhancement, designs a two-layer graph neural network (GNN) to fuse these vectors with business metadata into a dynamic knowledge graph, and introduces reinforcement learning to optimize decision paths. Experimental results in a manufacturing case show the mechanism reduced equipment failure response time from 7.8 to 3.7 hours, achieved an F1 score of 94.3%, and decreased annual digital transformation cost compensation for decision errors by 45.3%, significantly enhancing the intelligence and efficiency of transformation drives.</div>
<div class="mono" style="margin-top:8px">针对企业数字化转型中非结构化数据语义理解不足、驱动机制缺乏智能决策依据等问题，本研究提出了一种融合大语言模型与知识图谱的方法。该方法首先使用微调的BERT模型对多源异构文本进行实体识别和关系抽取，并利用GPT-4生成语义增强的向量表示；其次设计双层图神经网络架构，将大模型输出的语义向量与业务元数据融合，构建动态可扩展的企业知识图谱；随后引入强化学习优化决策路径生成，通过奖励函数驱动机制迭代。在制造业案例中，该机制将设备故障场景的响应时间从7.8小时降至3.7小时，F1值达到94.3%，年度数字化转型成本中决策错误补偿降低了45.3%，有效提升了驱动机制的智能化水平和执行效率。</div>
</details>
</div>
<div class="card">
<div class="title">Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning</div>
<div class="meta-line">Authors: Enze Pan</div>
<div class="meta-line">First: 2026-01-08T08:05:42+00:00 · Latest: 2026-01-08T08:05:42+00:00</div>
<div class="meta-line">Comments: 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04695v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04695v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what &quot;uncertainty reduction&quot; objectives can and cannot guarantee under rule shifts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Tape：用于评估强化学习中规则迁移泛化能力的元胞自动机基准</div>
<div class="mono" style="margin-top:8px">本文提出Tape——一个受控的强化学习基准，旨在分离潜在规则迁移下的分布外（OOD）失效问题。Tape基于一维元胞自动机构建，通过固定观测与动作空间而改变转移规则，实现精确的训练/测试划分。利用可复现的评估流程，我们比较了无模型基线方法、基于学习世界模型的规划方法以及任务推断（元强化学习）方法。结果显示一致规律：在分布内（ID）表现优异的方法可能在未见过规则的OOD场景中失效，且高方差的OOD评估会导致排名不稳定，除非实验进行充分重复。我们提供：（1）标准化OOD评估协议；（2）统计报告要求（随机种子、置信区间与假设检验）；（3）连接熵缩减与条件互信息、期望后验KL散度的信息论恒等式，阐明规则迁移下“不确定性缩减”目标的可保证与不可保证范畴。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Tape, a reinforcement learning benchmark based on one-dimensional cellular automata, motivated by the need to systematically evaluate out-of-distribution (OOD) generalization under latent rule shifts while keeping observation and action spaces fixed. The method involves a controlled experimental setup with precise train/test splits, comparing model-free, model-based planning, and meta-RL approaches through a reproducible pipeline. Key experimental results reveal that strong in-distribution performance often collapses under OOD rule shifts, and high evaluation variance necessitates extensive replication for stable rankings, leading the authors to propose standardized OOD protocols, statistical reporting guidelines, and information-theoretic insights linking entropy reduction to generalization guarantees.</div>
<div class="mono" style="margin-top:8px">本文提出了Tape，一个基于一维元胞自动机的强化学习基准，其动机在于需要系统评估潜在规则变化下的分布外泛化能力，同时保持观察和行动空间不变。方法采用受控实验设置，通过精确的训练/测试分割，利用可复现的流程比较了无模型、基于模型的规划以及元强化学习方法。主要实验结果表明，在分布内表现良好的方法在分布外规则变化下常会失效，且评估的高方差要求大量重复实验以确保排名稳定性，为此作者提出了标准化的分布外评估协议、统计报告要求，以及连接熵减少与泛化保证的信息论分析。</div>
</details>
</div>
<div class="card">
<div class="title">ResMAS: Resilience Optimization in LLM-based Multi-agent Systems</div>
<div class="meta-line">Authors: Zhilun Zhou, Zihan Liu, Jiahe Liu, Qingyu Shao, Yihan Wang, Kun Shao, Depeng Jin, Fengli Xu</div>
<div class="meta-line">First: 2026-01-08T08:03:37+00:00 · Latest: 2026-01-08T08:03:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04694v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04694v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS&#x27;s resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent&#x27;s prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResMAS：基于大语言模型的多智能体系统韧性优化</div>
<div class="mono" style="margin-top:8px">基于大语言模型的多智能体系统（LLM-based MAS）通过多个LLM智能体协作解决复杂任务，已在众多领域展现出卓越性能。然而，MAS通常部署于不同设备或环境中，易受智能体故障等扰动影响。现有研究虽关注对抗性攻击及相应防御策略，但主要侧重于事后检测与缓解，而非主动设计具备内在韧性的系统。本文研究了扰动下LLM-based MAS的韧性，发现通信拓扑与提示设计均显著影响系统韧性。基于此，我们提出ResMAS：一个两阶段增强MAS韧性的框架。首先，训练奖励模型预测MAS韧性，并以此训练拓扑生成器，通过强化学习为特定任务自动设计韧性拓扑。其次，引入拓扑感知的提示优化方法，根据各智能体的连接与交互关系优化其提示。跨任务实验表明，该方法在各种约束下显著提升了MAS韧性。此外，该框架对新任务与模型展现出强大的泛化能力，凸显了构建韧性MAS的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of Large Language Model-based Multi-Agent Systems (LLM-based MAS) to perturbations like agent failures, noting that prior work focuses on reactive defenses rather than proactive resilience design. The authors propose ResMAS, a two-stage framework that first trains a reward model to predict resilience and uses reinforcement learning to generate optimal communication topologies, then applies a topology-aware method to optimize each agent&#x27;s prompts based on its interactions. Experimental results across various tasks demonstrate that ResMAS significantly enhances system resilience under constraints and shows strong generalization to new tasks and models.</div>
<div class="mono" style="margin-top:8px">本文针对基于大语言模型的多智能体系统（LLM-based MAS）在智能体故障等扰动下的脆弱性问题展开研究，指出现有工作主要关注被动防御而非主动设计固有韧性系统。作者提出ResMAS框架，包含两个阶段：首先训练奖励模型预测系统韧性，并通过强化学习自动生成针对特定任务的韧性拓扑；其次引入拓扑感知的提示优化方法，根据智能体的连接与交互优化其提示。在多种任务上的实验表明，该方法能显著提升系统在不同约束下的韧性，并对新任务和模型展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead</div>
<div class="meta-line">Authors: Oluwatosin Oseni, Shengjie Wang, Jun Zhu, Micah Corah</div>
<div class="meta-line">Venue: RSS</div>
<div class="meta-line">First: 2026-01-08T07:55:07+00:00 · Latest: 2026-01-08T07:55:07+00:00</div>
<div class="meta-line">Comments: RSS&#x27;25: Multi-Objective Optimization and Planning in Robotics Workshop: 5 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04686v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04686v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>噩梦编织者：预测不安全状态与前瞻性规划</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在现实应用中取得了显著成功，尤其在机器人控制领域。然而，由于安全保证不足，强化学习的应用仍受限制。我们提出&#x27;噩梦编织者&#x27;——一种基于模型的安全强化学习算法，通过利用学习得到的世界模型预测潜在安全违规并相应规划行动，以解决安全问题。该算法在最大化奖励的同时实现了近乎零安全违规。在仅使用图像观测的Safety Gymnasium任务中，&#x27;噩梦编织者&#x27;优于无模型基线方法，效率提升近20倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the insufficient safety guarantees that limit the adoption of Reinforcement Learning (RL) in real-world applications like robotics. The method introduces Nightmare Dreamer, a model-based Safe RL algorithm that uses a learned world model to predict and plan around potential safety violations. The main experimental results show that it achieves nearly zero safety violations while maximizing rewards, outperforming model-free baselines on Safety Gymnasium tasks with image observations by nearly 20x in efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决强化学习在机器人等现实应用中因安全性保障不足而受限的问题。方法上提出了Nightmare Dreamer，这是一种基于模型的安全强化学习算法，通过利用学习得到的世界模型来预测潜在的安全违规并相应规划行动。主要实验结果表明，该算法在最大化奖励的同时实现了近乎零的安全违规，在仅使用图像观测的Safety Gymnasium任务上，其效率比无模型基线方法提高了近20倍。</div>
</details>
</div>
<div class="card">
<div class="title">IndexTTS 2.5 Technical Report</div>
<div class="meta-line">Authors: Yunpei Li, Xun Zhou, Jinchao Wang, Lu Wang, Yong Wu, Siyi Zhou, Yiquan Zhou, Jingchen Shu</div>
<div class="meta-line">First: 2026-01-07T12:58:16+00:00 · Latest: 2026-01-08T07:46:09+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03888v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03888v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IndexTTS 2.5技术报告</div>
<div class="mono" style="margin-top:8px">在先前工作中，我们提出了IndexTTS 2——一个由两个核心组件构成的零样本神经文本转语音基础模型：基于Transformer的文本到语义（T2S）模块和非自回归的语义到梅尔频谱（S2M）模块，二者协同实现了精准的情感复现，并建立了首个自回归时长可控的生成范式。在此基础上，我们推出IndexTTS 2.5，通过四项关键改进显著提升了多语言覆盖能力、推理速度和整体合成质量：1）语义编解码压缩：将语义编解码帧率从50Hz降至25Hz，序列长度减半，大幅降低训练和推理成本；2）架构升级：将S2M模块基于U-DiT的主干网络替换为更高效的Zipformer建模架构，实现显著参数量减少和更快的梅尔频谱生成；3）多语言扩展：提出三种显式跨语言建模策略——边界感知对齐、词元级拼接和指令引导生成，为零样本多语言情感TTS建立了实用设计原则，支持中、英、日、西四种语言，即使无目标语言情感训练数据也能实现鲁棒的情感迁移；4）强化学习优化：在T2S模块的后训练中应用GRPO，提升发音准确性和自然度。实验表明，IndexTTS 2.5不仅支持更广泛的语言覆盖，还能在相同零样本设置下复现未见语言的情感韵律。IndexTTS 2.5在保持与IndexTTS 2相当的词错误率和说话人相似度同时，实现了2.28倍的实时因子提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance the multilingual capability, speed, and quality of zero-shot text-to-speech synthesis, IndexTTS 2.5 introduces four key improvements over its predecessor: compressing the semantic codec frame rate to reduce sequence length, upgrading the S2M module architecture to a more efficient Zipformer, implementing explicit cross-lingual strategies for multilingual emotional TTS, and applying reinforcement learning to optimize pronunciation. Experimental results demonstrate that the model supports Chinese, English, Japanese, and Spanish, enables robust zero-shot emotion transfer across languages, and achieves a 2.28 times improvement in real-time factor while maintaining word error rate and speaker similarity comparable to IndexTTS 2.</div>
<div class="mono" style="margin-top:8px">为提升零样本文本到语音合成的多语言覆盖、推理速度与整体质量，IndexTTS 2.5在先前工作基础上提出了四项关键改进：通过压缩语义编解码器帧率以降低序列长度，将S2M模块主干升级为更高效的Zipformer架构，采用边界感知对齐等显式跨语言策略实现多语言情感TTS，并应用强化学习优化发音自然度。实验结果表明，该模型支持中、英、日、西四种语言，能在零样本设置下实现跨语言的鲁棒情感迁移，并在保持词错误率和说话人相似度与IndexTTS 2相当的同时，将实时因子提升了2.28倍。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Dynamics in RL Post-Training for Language Models</div>
<div class="meta-line">Authors: Akiyoshi Tomihari</div>
<div class="meta-line">First: 2026-01-08T07:32:15+00:00 · Latest: 2026-01-08T07:32:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04670v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04670v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型强化学习后训练中的学习动态研究</div>
<div class="mono" style="margin-top:8px">强化学习（RL）后训练是现代语言模型开发的关键阶段，对提升模型对齐性和推理能力至关重要。然而，包括输出多样性下降在内的若干现象仍缺乏深入理解。为更全面认识RL后训练，我们从监督学习中已有研究但RL领域尚未充分探索的视角，分析其学习动态。我们采用经验性神经正切核（NTK）框架，将NTK分解为两个分量以刻画RL更新在训练样本间的传播机制。分析表明，特征表征的有限变异性会导致RL更新系统性地增强模型置信度，这解释了RL后训练后输出多样性普遍下降的现象。进一步研究发现，该训练机制下的有效学习依赖于快速塑造分类器，直接影响NTK的梯度分量。基于这些发现，我们提出分类器优先强化学习（CF-RL）——一种简单的两阶段训练策略，在标准RL优化前优先更新分类器。实验结果验证了理论分析：CF-RL能提升模型置信度并加速优化。附加分析表明CF-RL的机制与监督学习中的线性探测再微调存在本质差异。本研究系统阐述了RL后训练的学习动态，为后续分析与改进提供了理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the learning dynamics of reinforcement learning (RL) post-training for language models, motivated by the need to better understand phenomena like reduced output diversity. The method employs an empirical neural tangent kernel (NTK) framework to analyze how RL updates propagate, revealing that limited feature variability leads RL to systematically increase model confidence, explaining the diversity drop. Based on this, the authors propose classifier-first RL (CF-RL), a two-stage strategy prioritizing classifier updates before full RL optimization. Experimental results confirm that CF-RL accelerates optimization and increases model confidence, with analysis showing its mechanism differs from linear-probing-then-fine-tuning in supervised learning.</div>
<div class="mono" style="margin-top:8px">本文研究了语言模型强化学习后训练的学习动态，旨在理解输出多样性降低等现象。方法上采用经验神经正切核框架分析强化学习更新的传播，发现特征表示变异性有限会导致模型置信度系统性增加，从而解释了多样性下降。基于此，作者提出了分类器优先强化学习，这是一种两阶段训练策略，优先更新分类器再进行标准强化学习优化。实验结果验证了理论分析，表明该方法能加速优化并提高模型置信度，且其机制与监督学习中的线性探测再微调不同。</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture</div>
<div class="meta-line">Authors: Laukik Patade, Rohan Rane, Sandeep Pillai</div>
<div class="meta-line">First: 2026-01-08T07:28:11+00:00 · Latest: 2026-01-08T07:28:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04668v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04668v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra&#x27;s algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的精准农业无人地面车辆路径规划优化</div>
<div class="mono" style="margin-top:8px">本研究聚焦于利用连续动作空间中的深度强化学习技术，优化精准农业中无人地面车辆的路径规划。研究首先回顾了传统基于网格的方法（如A*和Dijkstra算法），并讨论了其在动态农业环境中的局限性，强调了自适应学习策略的必要性。随后探讨了深度强化学习方法，包括深度Q网络，其在二维仿真中展现出更强的适应性和性能。研究评估了双Q网络和竞争网络等改进方法以进一步提升决策能力。在此基础上，重点转向连续动作空间模型，特别是深度确定性策略梯度及其改进算法TD3，并在日益复杂的环境中进行测试。基于ROS和Gazebo的三维环境实验表明，连续深度强化学习算法在动态农业场景导航中具有显著效果。值得注意的是，预训练的TD3智能体在动态环境中实现了95%的成功率，证明了该方法在确保作物与机器人安全的同时处理移动障碍物的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional grid-based path planning algorithms like A* and Dijkstra in dynamic agricultural settings, this study employs deep reinforcement learning (DRL) to optimize path planning for unmanned ground vehicles (UGVs) in precision agriculture. The method progresses from evaluating discrete-action DRL approaches, such as Deep Q-Networks with enhancements, to more advanced continuous-action models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), tested in increasingly complex simulations. The main experimental results, validated in a 3D ROS/Gazebo environment, show that the pretrained TD3 agent achieves a 95% success rate in dynamic scenarios, effectively navigating moving obstacles while ensuring operational safety for crops and the robot.</div>
<div class="mono" style="margin-top:8px">本研究针对传统基于网格的路径规划算法（如A*和Dijkstra）在动态农业环境中的局限性，采用深度强化学习（DRL）来优化精准农业中无人地面车辆（UGV）的路径规划。方法上，从评估离散动作的DRL方法（如深度Q网络及其改进）逐步推进到更先进的连续动作模型，特别是深度确定性策略梯度（DDPG）和双延迟深度确定性策略梯度（TD3），并在日益复杂的模拟环境中进行测试。主要实验结果表明，在三维ROS/Gazebo环境中，预训练的TD3智能体在动态场景中实现了95%的成功率，能有效规避移动障碍物，同时确保作物和机器人的操作安全。</div>
</details>
</div>
<div class="card">
<div class="title">FAQNAS: FLOPs-aware Hybrid Quantum Neural Architecture Search using Genetic Algorithm</div>
<div class="meta-line">Authors: Muhammad Kashif, Shaf Khalid, Alberto Marchisio, Nouhaila Innan, Muhammad Shafique</div>
<div class="meta-line">First: 2025-11-13T08:04:17+00:00 · Latest: 2026-01-08T07:16:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10062v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10062v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid Quantum Neural Networks (HQNNs), which combine parameterized quantum circuits with classical neural layers, are emerging as promising models in the noisy intermediate-scale quantum (NISQ) era. While quantum circuits are not naturally measured in floating point operations (FLOPs), most HQNNs (in NISQ era) are still trained on classical simulators where FLOPs directly dictate runtime and scalability. Hence, FLOPs represent a practical and viable metric to measure the computational complexity of HQNNs. In this work, we introduce FAQNAS, a FLOPs-aware neural architecture search (NAS) framework that formulates HQNN design as a multi-objective optimization problem balancing accuracy and FLOPs. Unlike traditional approaches, FAQNAS explicitly incorporates FLOPs into the optimization objective, enabling the discovery of architectures that achieve strong performance while minimizing computational cost. Experiments on five benchmark datasets (MNIST, Digits, Wine, Breast Cancer, and Iris) show that quantum FLOPs dominate accuracy improvements, while classical FLOPs remain largely fixed. Pareto-optimal solutions reveal that competitive accuracy can often be achieved with significantly reduced computational cost compared to FLOPs-agnostic baselines. Our results establish FLOPs-awareness as a practical criterion for HQNN design in the NISQ era and as a scalable principle for future HQNN systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FAQNAS：基于遗传算法的FLOPs感知混合量子神经架构搜索</div>
<div class="mono" style="margin-top:8px">混合量子神经网络（HQNNs）将参数化量子电路与经典神经层相结合，在噪声中等规模量子（NISQ）时代展现出广阔前景。虽然量子电路本身不直接以浮点运算（FLOPs）衡量，但当前NISQ时代的大多数HQNN仍通过经典模拟器训练，其运行时间和可扩展性直接受FLOPs影响。因此，FLOPs可作为衡量HQNN计算复杂度的实用有效指标。本研究提出FAQNAS框架，将HQNN设计构建为平衡精度与FLOPs的多目标优化问题。与传统方法不同，FAQNAS将FLOPs显式纳入优化目标，从而在控制计算成本的同时获得高性能架构。在五个基准数据集（MNIST、Digits、Wine、Breast Cancer和Iris）上的实验表明：量子FLOPs主导精度提升，而经典FLOPs基本保持稳定。帕累托最优解显示，相较于忽略FLOPs的基线方法，本方案能以显著降低的计算成本实现具有竞争力的精度。本研究确立了FLOPs感知作为NISQ时代HQNN设计的实用准则，并为未来HQNN系统提供了可扩展的设计原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces FAQNAS, a FLOPs-aware neural architecture search framework for designing hybrid quantum neural networks (HQNNs) in the NISQ era, motivated by the need to balance accuracy with computational efficiency since FLOPs dictate runtime on classical simulators. The method formulates HQNN design as a multi-objective optimization problem using a genetic algorithm to explicitly incorporate FLOPs into the search, aiming to minimize computational cost while maintaining performance. Experimental results on five benchmark datasets demonstrate that quantum FLOPs are key to accuracy gains, with Pareto-optimal solutions achieving competitive accuracy at significantly reduced computational costs compared to FLOPs-agnostic baselines.</div>
<div class="mono" style="margin-top:8px">本文提出了FAQNAS，一种面向NISQ时代混合量子神经网络（HQNN）设计的FLOPs感知神经架构搜索框架，其动机在于在经典模拟器上运行时FLOPs决定计算开销，需平衡精度与效率。该方法将HQNN设计表述为多目标优化问题，采用遗传算法显式地将FLOPs纳入搜索目标，以在保持性能的同时最小化计算成本。在五个基准数据集上的实验结果表明，量子FLOPs主导精度提升，而帕累托最优解能在显著降低计算成本的情况下实现与忽略FLOPs的基线方法相竞争的精度。</div>
</details>
</div>
<div class="card">
<div class="title">InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training</div>
<div class="meta-line">Authors: Ziyun Zhang, Zezhou Wang, Xiaoyi Zhang, Zongyu Guo, Jiahao Li, Bin Li, Yan Lu</div>
<div class="meta-line">First: 2026-01-07T17:40:08+00:00 · Latest: 2026-01-08T06:37:47+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04126v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04126v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiniteWeb：面向GUI智能体训练的可扩展网页环境合成系统</div>
<div class="mono" style="margin-top:8px">代表用户与图形界面交互的GUI智能体是实用AI助手的重要发展方向，但其训练受限于合适环境的稀缺性。本文提出InfiniteWeb系统，可自动大规模生成用于GUI智能体训练的功能性网页环境。虽然大语言模型能较好生成单个网页，但构建具有多页面互连的真实功能性网站仍面临挑战。我们通过统一规范、以任务为中心的测试驱动开发、结合网站种子与参考设计图像确保多样性等方法应对这些挑战。本系统同时生成可验证的任务评估器，为强化学习提供密集奖励信号。实验表明，InfiniteWeb在真实网站构建方面超越商业编程智能体，基于生成环境训练的GUI智能体在OSWorld和Online-Mind2Web基准上取得显著性能提升，验证了系统的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for InfiniteWeb stems from the scarcity of suitable training environments for GUI agents, which are promising AI assistants that interact with graphical interfaces. The method introduces a system that automatically synthesizes functional web environments at scale by addressing challenges in generating multi-page websites through unified specification, task-centric test-driven development, and combining website seeds with reference design images for diversity, while also producing verifiable task evaluators for dense reward signals. Main experimental results show that InfiniteWeb outperforms commercial coding agents in realistic website construction, and GUI agents trained on its generated environments achieve significant performance improvements on benchmarks like OSWorld and Online-Mind2Web, demonstrating the system&#x27;s effectiveness.</div>
<div class="mono" style="margin-top:8px">InfiniteWeb的动机源于GUI代理训练环境稀缺的问题，这类代理作为与图形界面交互的AI助手具有广阔前景。该方法提出了一个系统，通过统一规范、以任务为中心的测试驱动开发、结合网站种子与参考设计图像确保多样性，来自动大规模合成功能性网络环境，并生成可验证的任务评估器以提供密集奖励信号。主要实验结果表明，InfiniteWeb在构建真实网站方面超越了商业编码代理，且在其生成环境中训练的GUI代理在OSWorld和Online-Mind2Web等基准测试上取得了显著性能提升，证明了该系统的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary Generative Optimization: Towards Fully Data-Driven Evolutionary Optimization via Generative Learning</div>
<div class="meta-line">Authors: Tao Jiang, Kebin Sun, Zhenyu Liang, Ran Cheng, Yaochu Jin, Kay Chen Tan</div>
<div class="meta-line">First: 2025-08-01T07:17:57+00:00 · Latest: 2026-01-08T06:08:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.00380v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.00380v2">PDF</a> · <a href="https://github.com/EMI-Group/evogo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in data-driven evolutionary algorithms (EAs) have demonstrated the potential of leveraging historical data to improve optimization accuracy and adaptability. Despite these advancements, existing methods remain reliant on handcrafted process-level operators. In contrast, Evolutionary Generative Optimization (EvoGO) is a fully data-driven framework designed from the objective level, enabling autonomous learning of the entire search process. EvoGO streamlines the evolutionary optimization process into three stages: data preparation, model training, and population generation. The data preparation stage constructs a pairwise dataset to enrich training diversity without incurring additional evaluation costs. During model training, a tailored generative model learns to transform inferior solutions into superior ones. In the population generation stage, EvoGO replaces traditional reproduction operators with a scalable and parallelizable generative mechanism. Extensive experiments on numerical benchmarks, classical control problems, and high-dimensional robotic tasks demonstrate that EvoGO consistently converges within merely 10 generations and substantially outperforms a wide spectrum of optimization approaches, including traditional EAs, Bayesian optimization, and reinforcement learning based methods. Code is available at: https://github.com/EMI-Group/evogo</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化生成优化：通过生成学习实现完全数据驱动的进化优化</div>
<div class="mono" style="margin-top:8px">数据驱动进化算法的最新进展展示了利用历史数据提升优化精度与适应性的潜力。然而，现有方法仍依赖人工设计的流程级算子。相比之下，进化生成优化（EvoGO）是一个从目标层面设计的完全数据驱动框架，能够自主习得完整搜索过程。EvoGO将进化优化流程简化为三个阶段：数据准备、模型训练和种群生成。数据准备阶段构建配对数据集以增强训练多样性，且无需额外评估成本；模型训练阶段通过定制生成模型学习将劣质解转化为优质解；种群生成阶段则用可扩展、可并行的生成机制替代传统繁殖算子。在数值基准测试、经典控制问题及高维机器人任务上的大量实验表明，EvoGO仅需10代即可稳定收敛，且显著优于传统进化算法、贝叶斯优化及基于强化学习的方法。代码发布于：https://github.com/EMI-Group/evogo</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing data-driven evolutionary algorithms that still rely on handcrafted operators, this paper introduces Evolutionary Generative Optimization (EvoGO), a fully data-driven framework designed to autonomously learn the entire search process from the objective level. The method streamlines optimization into three stages: preparing a pairwise dataset to enhance training diversity without extra evaluations, training a tailored generative model to transform inferior solutions into superior ones, and generating populations via a scalable generative mechanism that replaces traditional reproduction operators. Experimental results on numerical benchmarks, control problems, and high-dimensional robotic tasks show that EvoGO consistently converges within just 10 generations and significantly outperforms a wide range of methods, including traditional evolutionary algorithms, Bayesian optimization, and reinforcement learning approaches.</div>
<div class="mono" style="margin-top:8px">针对现有数据驱动进化算法仍依赖人工设计算子的局限性，本文提出了进化生成优化（EvoGO），这是一个完全数据驱动的框架，旨在从目标层面自主学习整个搜索过程。该方法将优化流程简化为三个阶段：准备配对数据集以在不增加评估成本的情况下增强训练多样性，训练一个定制的生成模型以将劣质解转化为优质解，以及通过可扩展的生成机制替代传统繁殖算子来生成种群。在数值基准测试、经典控制问题和高维机器人任务上的实验结果表明，EvoGO仅需10代即可稳定收敛，并显著优于包括传统进化算法、贝叶斯优化和基于强化学习的方法在内的多种优化方法。</div>
</details>
</div>
<div class="card">
<div class="title">MiMo-V2-Flash Technical Report</div>
<div class="meta-line">Authors: Xiaomi LLM-Core Team, :, Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, Gang Xie, Hailin Zhang, Hanglong Lv, Hanyu Li, Heyu Chen, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Lei Li, Liang Zhao, Linghao Zhang, Peidian Li, Qianli Chen, Shaohui Liu, Shihua Yu, Shijie Cao, Shimao Chen, Shouqiu Yu, Shuo Liu, Tianling Zhou, Weijiang Su, Weikun Wang, Wenhan Ma, Xiangwei Deng, Bohan Mao, Bowen Ye, Can Cai, Chenghua Wang, Chengxuan Zhu, Chong Ma, Chun Chen, Chunan Li, Dawei Zhu, Deshan Xiao, Dong Zhang, Duo Zhang, Fangyue Liu, Feiyu Yang, Fengyuan Shi, Guoan Wang, Hao Tian, Hao Wu, Heng Qu, Hongfei Yi, Hongxu An, Hongyi Guan, Xing Zhang, Yifan Song, Yihan Yan, Yihao Zhao, Yingchun Lai, Yizhao Gao, Yu Cheng, Yuanyuan Tian, Yudong Wang, Zhen Tang, Zhengju Tang, Zhengtao Wen, Zhichao Song, Zhixian Zheng, Zihan Jiang, Jian Wen, Jiarui Sun, Jiawei Li, Jinlong Xue, Jun Xia, Kai Fang, Menghang Zhu, Nuo Chen, Qian Tu, Qihao Zhang, Qiying Wang, Rang Li, Rui Ma, Shaolei Zhang, Shengfan Wang, Shicheng Li, Shuhao Gu, Shuhuai Ren, Sirui Deng, Tao Guo, Tianyang Lu, Weiji Zhuang, Weikang Zhang, Weimin Xiong, Wenshan Huang, Wenyu Yang, Xin Zhang, Xing Yong, Xu Wang, Xueyang Xie, Yilin Jiang, Yixin Yang, Yongzhe He, Yu Tu, Yuanliang Dong, Yuchen Liu, Yue Ma, Yue Yu, Yuxing Xiang, Zhaojun Huang, Zhenru Lin, Zhipeng Xu, Zhiyang Chen, Zhonghua Deng, Zihan Zhang, Zihao Yue</div>
<div class="meta-line">First: 2026-01-06T07:31:47+00:00 · Latest: 2026-01-08T05:52:17+00:00</div>
<div class="meta-line">Comments: 31 pages, technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02780v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02780v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiMo-V2-Flash技术报告</div>
<div class="mono" style="margin-top:8px">我们提出MiMo-V2-Flash，这是一个总参数量为3090亿、激活参数量为150亿的专家混合模型，专为快速、强大的推理与智能体能力而设计。该模型采用混合注意力架构，以5:1的混合比例交错使用滑动窗口注意力与全局注意力，滑动窗口大小为128个词元。模型通过多词元预测在27万亿词元上进行预训练，原生支持32k上下文长度，并后续扩展至256k。为高效扩展训练后计算，MiMo-V2-Flash引入了新颖的多教师同策略蒸馏范式。在此框架中，领域专业化教师模型（例如通过大规模强化学习训练）提供密集的词元级奖励，使学生模型能完美掌握教师专长。MiMo-V2-Flash在总参数量分别仅为DeepSeek-V3.2和Kimi-K2的1/2和1/3的情况下，性能仍可与这些顶尖开源模型相媲美。在推理阶段，通过将多词元预测复用为推测解码的草稿模型，配合三层多词元预测结构，模型最高可实现3.6的平均接受长度和2.6倍的解码加速。我们开源了模型权重及三层多词元预测权重，以促进开放研究与社区协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient large-scale models with strong reasoning and agentic capabilities, this paper introduces MiMo-V2-Flash, a 309B parameter Mixture-of-Experts model with 15B active parameters. The method employs a hybrid attention architecture combining Sliding Window and global attention, pre-trains on 27T tokens with Multi-Token Prediction (MTP), and uses a novel Multi-Teacher On-Policy Distillation (MOPD) for efficient post-training. Key experimental results show the model rivals top open-weight models like DeepSeek-V3.2 with far fewer total parameters and, by repurposing MTP for speculative decoding, achieves up to a 2.6x decoding speedup.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发具有强大推理与智能体能力的高效大规模模型，提出了MiMo-V2-Flash，这是一个总参数量为3090亿、激活参数量为150亿的混合专家模型。方法上采用滑动窗口与全局注意力混合的架构，基于27000亿token进行多令牌预测预训练，并引入新颖的多教师策略蒸馏范式进行高效后训练。主要实验结果表明，该模型在总参数量仅为同类顶尖开源模型一半或三分之一的情况下，性能可与之媲美，并且通过将多令牌预测重用作推测解码的草稿模型，实现了最高2.6倍的解码加速。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</div>
<div class="meta-line">Authors: Jianfeng Si, Lin Sun, Zhewen Tan, Xiangzheng Zhang</div>
<div class="meta-line">First: 2025-08-12T02:39:33+00:00 · Latest: 2026-01-08T05:34:47+00:00</div>
<div class="meta-line">Comments: 15 pages,3 figures,5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14904v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.14904v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model&#x27;s safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于魔法令牌引导协同训练的大语言模型高效可切换安全控制</div>
<div class="mono" style="margin-top:8px">当前大语言模型（LLMs）的内容安全方法，如监督微调（SFT）和基于人类反馈的强化学习（RLHF），通常依赖多阶段训练流程，且缺乏细粒度的部署后可控性。为解决这些局限，我们提出一种统一的协同训练框架，在单一SFT阶段内高效整合多种安全行为：积极（合法/亲社会）、消极（无过滤/风险倾向）和拒绝（拒绝导向/保守）。值得注意的是，每种行为均可通过简单的系统级指令（即魔法令牌）动态激活，实现推理时隐蔽高效的行为切换。这种灵活性支持多样化的部署场景，例如积极模式用于安全用户交互，消极模式用于内部红队测试，拒绝模式用于响应上游审核信号的情境感知拒绝。该协同训练策略在输出空间诱导出独特的安全对齐边界，其特征表现为各安全模式对应的高度分离的响应分布。该边界的存在为模型的安全鲁棒性提供了实证依据，并实现了前所未有的细粒度控制。实验表明，我们的方法在安全对齐质量上匹配SFT+DPO，其中8B模型在安全性能上显著超越DeepSeek-R1（671B），同时大幅降低训练复杂度和部署成本。本研究为大语言模型内容安全提供了可扩展、高效且高度可控的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of current LLM safety methods like SFT and RLHF, which involve complex multi-stage training and lack post-deployment controllability, by proposing a unified co-training framework that integrates positive, negative, and rejective safety behaviors in a single SFT stage. The method dynamically activates each behavior using a simple system-level instruction or magic token, enabling efficient and stealthy switching at inference to support diverse scenarios such as safe interaction, red-teaming, and context-aware refusals. Experimental results demonstrate that this approach matches the safety alignment quality of SFT+DPO, with an 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing training complexity and deployment costs, thereby offering a scalable and highly controllable solution for LLM content safety.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型语言模型安全方法（如监督微调和人类反馈强化学习）存在的多阶段训练流程复杂且缺乏部署后细粒度可控性的问题，提出了一种统一的协同训练框架，在单一监督微调阶段内高效整合了积极、消极和拒绝三种安全行为。该方法通过简单的系统级指令或魔法令牌动态激活每种行为，支持在推理时进行隐蔽高效的行为切换，以适应安全交互、内部红队测试和基于上游审核信号的上下文感知拒绝等多种部署场景。实验结果表明，该方法在安全对齐质量上匹配了监督微调加直接偏好优化，其80亿参数模型在安全性能上显著超越了DeepSeek-R1（6710亿参数），同时大幅降低了训练复杂度和部署成本，为大型语言模型内容安全提供了一个可扩展、高效且高度可控的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Human-in-the-Loop Feature Selection Using Interpretable Kolmogorov-Arnold Network-based Double Deep Q-Network</div>
<div class="meta-line">Authors: Md Abrar Jahin, M. F. Mridha, Nilanjan Dey, Md. Jakir Hossen</div>
<div class="meta-line">Venue: IEEE Open Journal of the Computer Society (2026)</div>
<div class="meta-line">First: 2024-11-06T08:13:09+00:00 · Latest: 2026-01-08T05:33:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.03740v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.03740v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feature selection is critical for improving the performance and interpretability of machine learning models, particularly in high-dimensional spaces where complex feature interactions can reduce accuracy and increase computational demands. Existing approaches often rely on static feature subsets or manual intervention, limiting adaptability and scalability. However, dynamic, per-instance feature selection methods and model-specific interpretability in reinforcement learning remain underexplored. This study proposes a human-in-the-loop (HITL) feature selection framework integrated into a Double Deep Q-Network (DDQN) using a Kolmogorov-Arnold Network (KAN). Our novel approach leverages simulated human feedback and stochastic distribution-based sampling, specifically Beta, to iteratively refine feature subsets per data instance, improving flexibility in feature selection. The KAN-DDQN achieved notable test accuracies of 93% on MNIST and 83% on FashionMNIST, outperforming conventional MLP-DDQN models by up to 9%. The KAN-based model provided high interpretability via symbolic representation while using 4 times fewer neurons in the hidden layer than MLPs did. Comparatively, the models without feature selection achieved test accuracies of only 58% on MNIST and 64% on FashionMNIST, highlighting significant gains with our framework. We further validate scalability on CIFAR-10 and CIFAR-100, achieving up to 30% relative macro F1 improvement on MNIST and 5% on CIFAR-10, while reducing calibration error by 25%. Complexity analysis confirms real-time feasibility with latency below 1 ms and parameter counts under 0.02M. Pruning and visualization further enhanced model transparency by elucidating decision pathways. These findings present a scalable, interpretable solution for feature selection that is suitable for applications requiring real-time, adaptive decision-making with minimal human oversight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可解释柯尔莫哥洛夫-阿诺德网络的双深度Q网络人机协同特征选择方法</div>
<div class="mono" style="margin-top:8px">特征选择对提升机器学习模型性能与可解释性至关重要，尤其在高维空间中，复杂的特征交互会降低精度并增加计算负担。现有方法多依赖静态特征子集或人工干预，限制了适应性与可扩展性。动态的逐实例特征选择方法及强化学习中模型特定的可解释性研究仍显不足。本研究提出一种集成于双深度Q网络的人机协同特征选择框架，采用柯尔莫哥洛夫-阿诺德网络。该创新方法通过模拟人类反馈和基于随机分布（具体为Beta分布）的采样，逐数据实例迭代优化特征子集，提升特征选择的灵活性。KAN-DDQN在MNIST和FashionMNIST数据集上分别取得93%和83%的显著测试准确率，较传统MLP-DDQN模型提升达9%。基于KAN的模型通过符号表示提供高可解释性，且隐藏层神经元数量仅为MLP的1/4。对比实验显示，未使用特征选择的模型在MNIST和FashionMNIST上准确率仅分别为58%和64%，凸显了本框架的显著增益。在CIFAR-10和CIFAR-100上的可扩展性验证表明，MNIST相对宏观F1值提升达30%，CIFAR-10提升5%，同时校准误差降低25%。复杂度分析证实了实时可行性：延迟低于1毫秒，参数量小于0.02M。剪枝与可视化技术通过阐明决策路径进一步增强了模型透明度。这些发现为特征选择提供了一种可扩展、可解释的解决方案，适用于需要实时自适应决策且人工干预最少的应用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for adaptive and interpretable feature selection in high-dimensional machine learning, this paper introduces a human-in-the-loop framework that integrates a Kolmogorov-Arnold Network into a Double Deep Q-Network (KAN-DDQN) to dynamically select features per data instance using simulated human feedback and Beta distribution sampling. The method achieved test accuracies of 93% on MNIST and 83% on FashionMNIST, outperforming MLP-DDQN baselines by up to 9%, while providing high interpretability through symbolic representations and using four times fewer hidden neurons. Experimental results on CIFAR datasets showed up to 30% relative macro F1 improvement on MNIST and reduced calibration error by 25%, with complexity analysis confirming real-time feasibility through low latency and minimal parameters.</div>
<div class="mono" style="margin-top:8px">针对高维机器学习中自适应和可解释特征选择的需求，本研究提出了一种人机交互框架，将Kolmogorov-Arnold网络集成到双深度Q网络（KAN-DDQN）中，通过模拟人类反馈和Beta分布采样实现按数据实例的动态特征选择。该方法在MNIST和FashionMNIST数据集上分别达到93%和83%的测试准确率，较MLP-DDQN基线提升最高9%，同时通过符号表示提供高可解释性并减少四倍隐藏层神经元。在CIFAR数据集上的实验显示，MNIST的相对宏观F1分数提升最高达30%，校准误差降低25%，复杂度分析证实了低延迟和少参数下的实时可行性。</div>
</details>
</div>
<div class="card">
<div class="title">ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning</div>
<div class="meta-line">Authors: Zhengyi Kwan, Wei Zhang, Aik Beng Ng, Zhengkui Wang, Simon See</div>
<div class="meta-line">First: 2026-01-07T06:50:56+00:00 · Latest: 2026-01-08T05:28:59+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03646v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03646v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Job scheduling is widely used in real-world manufacturing systems to assign ordered job operations to machines under various constraints. Existing solutions remain limited by long running time or insufficient schedule quality, especially when problem scale increases. In this paper, we propose ReLA, a reinforcement-learning (RL) scheduler built on structured representation learning and aggregation. ReLA first learns diverse representations from scheduling entities, including job operations and machines, using two intra-entity learning modules with self-attention and convolution and one inter-entity learning module with cross-attention. These modules are applied in a multi-scale architecture, and their outputs are aggregated to support RL decision-making. Across experiments on small, medium, and large job instances, ReLA achieves the best makespan in most tested settings over the latest solutions. On non-large instances, ReLA reduces the optimality gap of the SOTA baseline by 13.0%, while on large-scale instances it reduces the gap by 78.6%, with the average optimality gaps lowered to 7.3% and 2.1%, respectively. These results confirm that ReLA&#x27;s learned representations and aggregation provide strong decision support for RL scheduling, and enable fast job completion and decision-making for real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReLA：基于强化学习的作业调度中的表示学习与聚合方法</div>
<div class="mono" style="margin-top:8px">作业调度广泛应用于现实制造系统，用于在多种约束下将有序作业工序分配给机器。现有解决方案仍受限于运行时间过长或调度质量不足，尤其在问题规模增大时更为明显。本文提出ReLA，一种基于结构化表示学习与聚合的强化学习调度器。ReLA首先通过两个采用自注意力与卷积的实体内部学习模块，以及一个采用交叉注意力的实体间学习模块，从调度实体（包括作业工序和机器）中学习多样化表示。这些模块应用于多尺度架构，其输出经聚合后支持强化学习决策。在小型、中型和大型作业实例的实验中，ReLA在多数测试场景下取得了优于最新解决方案的最短完工时间。在非大型实例上，ReLA将SOTA基准的最优性差距降低了13.0%；在大型实例上，该差距降低78.6%，平均最优性差距分别降至7.3%和2.1%。这些结果证实，ReLA学习的表示与聚合为强化学习调度提供了强有力的决策支持，并能实现快速作业完成与实时应用决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in job scheduling, such as long runtimes and poor schedule quality at scale, by proposing ReLA, a reinforcement learning scheduler that employs structured representation learning and aggregation. The method learns diverse representations from scheduling entities like job operations and machines using intra-entity modules with self-attention and convolution and an inter-entity module with cross-attention within a multi-scale architecture, aggregating outputs to inform RL decisions. Experimental results show ReLA achieves the best makespan across small, medium, and large instances, reducing the optimality gap of state-of-the-art baselines by 13.0% on non-large instances and 78.6% on large-scale instances, with average gaps lowered to 7.3% and 2.1%, respectively, demonstrating its effectiveness for fast and high-quality scheduling.</div>
<div class="mono" style="margin-top:8px">该论文针对作业调度中运行时间长和大规模下调度质量不足的问题，提出了ReLA，一种基于结构化表示学习与聚合的强化学习调度器。该方法通过自注意力和卷积的实体内部学习模块以及跨注意力的实体间学习模块，在多尺度架构中学习作业操作和机器等调度实体的多样化表示，并聚合输出以支持强化学习决策。实验结果表明，ReLA在小型、中型和大型作业实例上均实现了最佳完工时间，将最先进基线的优化差距在非大型实例上降低了13.0%，在大型实例上降低了78.6%，平均优化差距分别降至7.3%和2.1%，验证了其在实际应用中实现快速、高质量调度的能力。</div>
</details>
</div>
<div class="card">
<div class="title">SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection</div>
<div class="meta-line">Authors: Zhiyong Cao, Dunqiang Liu, Qi Dai, Haojun Xu, Huaiyan Xu, Huan He, Yafei Liu, Siyuan Liu, XiaoLin Lin, Ke Ma, Ruqian Shi, Sijia Yao, Hao Wang, Sicheng Zhou</div>
<div class="meta-line">First: 2026-01-06T10:00:15+00:00 · Latest: 2026-01-08T04:14:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02871v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02871v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimRPD：基于模拟器的数据评估与选择优化招聘主动对话智能体</div>
<div class="mono" style="margin-top:8px">面向任务的主动对话智能体在招聘场景中具有关键作用，尤其在引导对话实现特定业务目标（如获取社交媒体联系人以进行私域转化）方面。尽管监督微调和强化学习已被证明对此类智能体训练有效，但其性能受限于高质量、目标导向的领域特定训练数据的稀缺性。为解决这一问题，我们提出SimRPD——一个三阶段框架用于训练招聘主动对话智能体。首先，我们开发高保真用户模拟器，通过多轮在线对话合成大规模对话数据；其次，引入基于意图链的多维评估框架，结合全局级与实例级指标，全面评估模拟器并高效筛选高质量数据；最后，基于筛选数据集训练招聘主动对话智能体。真实招聘场景实验表明，SimRPD优于现有基于模拟器的数据选择策略，凸显了其在工业部署中的实用价值及面向其他商业对话场景的潜在适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training effective recruitment proactive dialogue agents, which are hindered by the scarcity of high-quality, domain-specific training data. To overcome this, the authors propose SimRPD, a three-stage framework that first employs a high-fidelity user simulator to generate large-scale conversational data, then applies a multi-dimensional evaluation based on Chain-of-Intention to select high-quality data using global and instance-level metrics, and finally trains the dialogue agent on this curated dataset. Experimental results in a real-world recruitment setting show that SimRPD surpasses existing simulator-based data selection methods, demonstrating its practical utility for industrial applications and potential extension to other business-oriented dialogue systems.</div>
<div class="mono" style="margin-top:8px">本文针对招聘主动对话代理训练中高质量领域特定数据稀缺的挑战，提出SimRPD框架。该方法首先通过高保真用户模拟器生成大规模多轮对话数据，然后基于意图链的多维度评估体系，结合全局和实例级指标筛选高质量数据，最后利用所选数据训练对话代理。在真实招聘场景中的实验表明，SimRPD优于现有的基于模拟器的数据选择策略，凸显了其工业部署的实用价值，并具备扩展到其他业务导向对话场景的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Over Space: Enabling Geographic Reasoning for LLM-Based Generative Next POI Recommendation</div>
<div class="meta-line">Authors: Dongyi Lv, Qiuyu Ding, Heng-Da Xu, Zhaoxu Sun, Zhi Wang, Feng Xiong, Mu Xu</div>
<div class="meta-line">First: 2026-01-08T03:46:03+00:00 · Latest: 2026-01-08T03:46:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04562v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04562v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative recommendation with large language models (LLMs) reframes prediction as sequence generation, yet existing LLM-based recommenders remain limited in leveraging geographic signals that are crucial in mobility and local-services scenarios. Here, we present Reasoning Over Space (ROS), a framework that utilizes geography as a vital decision variable within the reasoning process. ROS introduces a Hierarchical Spatial Semantic ID (SID) that discretizes coarse-to-fine locality and POI semantics into compositional tokens, and endows LLM with a three-stage Mobility Chain-of-Thought (CoT) paradigm that models user personality, constructs an intent-aligned candidate space, and performs locality informed pruning. We further align the model with real world geography via spatial-guided Reinforcement Learning (RL). Experiments on three widely used location-based social network (LBSN) datasets show that ROS achieves over 10% relative gains in hit rate over strongest LLM-based baselines and improves cross-city transfer, despite using a smaller backbone model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间推理：为基于大语言模型的生成式下一兴趣点推荐赋予地理推理能力</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的生成式推荐将预测重构为序列生成，但现有基于LLM的推荐系统在利用地理信号方面仍存在局限，而地理信号在移动性和本地服务场景中至关重要。本文提出“空间推理”（ROS）框架，将地理信息作为推理过程中的关键决策变量。ROS引入分层空间语义ID（SID），将粗粒度到细粒度的地理位置与兴趣点语义离散化为组合标记，并通过三阶段移动思维链（CoT）范式赋能LLM，该范式建模用户个性、构建意图对齐的候选空间，并执行基于地理信息的剪枝。我们进一步通过空间引导的强化学习（RL）使模型与现实世界地理对齐。在三个广泛使用的基于位置的社交网络（LBSN）数据集上的实验表明，ROS在命中率上相比最强的基于LLM的基线模型取得超过10%的相对提升，并改善了跨城市迁移性能，且使用了更小的骨干模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited use of geographic signals in existing large language model (LLM)-based generative recommenders, which are crucial for mobility and local-services scenarios, this paper introduces the Reasoning Over Space (ROS) framework to incorporate geography as a key decision variable. The method employs a Hierarchical Spatial Semantic ID to tokenize location and POI semantics and a three-stage Mobility Chain-of-Thought paradigm for reasoning, further enhanced by spatial-guided reinforcement learning for real-world alignment. Experimental results on three location-based social network datasets demonstrate that ROS achieves over 10% relative improvement in hit rate compared to strong LLM baselines and enhances cross-city transferability, even with a smaller backbone model.</div>
<div class="mono" style="margin-top:8px">针对现有基于大语言模型（LLM）的生成式推荐系统在移动性和本地服务场景中未能充分利用关键地理信号的问题，本文提出了空间推理（ROS）框架，将地理信息作为推理过程中的核心决策变量。该方法采用分层空间语义ID对位置和兴趣点语义进行标记化表示，并通过三阶段移动思维链范式进行推理，同时利用空间引导的强化学习实现与现实地理的对齐。在三个广泛使用的位置社交网络数据集上的实验表明，ROS相比最强的LLM基线在命中率上取得了超过10%的相对提升，并增强了跨城市迁移能力，尽管使用了更小的骨干模型。</div>
</details>
</div>
<div class="card">
<div class="title">One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents</div>
<div class="meta-line">Authors: Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Weikang Li, Jiahui Liang, Deguo Xia, Jizhou Huang, Jiyan He, Yunfang Wu</div>
<div class="meta-line">First: 2025-12-24T05:27:53+00:00 · Latest: 2026-01-08T03:22:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20957v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.20957v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一器足矣：面向仓库级大语言模型智能体的强化学习</div>
<div class="mono" style="margin-top:8px">在大型开源软件仓库中，由于规模庞大和结构复杂，准确定位需要修改的文件和函数极具挑战。现有基于大语言模型的方法通常将其视为仓库级检索任务，并依赖多种辅助工具，这忽视了代码执行逻辑且使模型控制复杂化。我们提出RepoNavigator——一种配备单一执行感知工具（跳转至被调用符号定义）的大语言模型智能体。该统一设计既反映了代码执行的实际流程，又简化了工具操作。RepoNavigator通过强化学习对预训练模型进行端到端训练，无需任何闭源蒸馏。实验表明，经强化学习训练的RepoNavigator实现了最先进的性能：7B模型超越14B基线，14B模型优于32B竞品，32B模型甚至超过了Claude-3.7等闭源模型。这些结果证实，将单一结构基础工具与强化学习训练相结合，为仓库级问题定位提供了高效且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of locating relevant code in large open-source repositories, where existing LLM-based methods rely on multiple auxiliary tools, complicating control and overlooking execution logic. The authors propose RepoNavigator, an LLM agent that uses a single execution-aware tool—jumping to symbol definitions—to mirror actual code flow and simplify tool use. It is trained end-to-end with reinforcement learning from a pretrained model, without closed-source distillation. Experimental results show state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and the 32B model exceeding closed-source models like Claude-3.7, demonstrating the efficiency of this unified, RL-driven approach.</div>
<div class="mono" style="margin-top:8px">本文针对在大型开源代码库中定位相关文件的挑战，现有基于大语言模型的方法依赖多种辅助工具，导致控制复杂且忽略代码执行逻辑。作者提出RepoNavigator，这是一个配备单一执行感知工具（跳转到符号定义）的大语言模型智能体，以反映实际代码执行流程并简化工具操作。该模型通过强化学习对预训练模型进行端到端训练，无需闭源蒸馏。实验结果表明其性能达到最先进水平，其中7B模型超越14B基线，14B模型优于32B竞争对手，32B模型甚至超过Claude-3.7等闭源模型，验证了这种统一、基于强化学习的方法的高效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Not All Steps are Informative: On the Linearity of LLMs&#x27; RLVR Training</div>
<div class="meta-line">Authors: Tianle Wang, Zhongyuan Wu, Shenghao Jin, Hao Xu, Wei Chen, Ning Miao</div>
<div class="meta-line">First: 2026-01-08T03:06:18+00:00 · Latest: 2026-01-08T03:06:18+00:00</div>
<div class="meta-line">Comments: pre-print</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04537v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并非所有训练步骤都具备信息量：论大语言模型RLVR训练的线性特性</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为大语言模型（LLM）后训练的核心组成部分。与监督微调（SFT）不同，RLVR让LLM生成多个候选解决方案，并强化那些能产生可验证正确答案的路径。然而实践中，RLVR通常需要数千训练步才能达到强劲性能，其巨大的计算成本主要源于漫长的探索过程。本研究揭示了一个意外现象：在RLVR训练过程中，LLM呈现强烈的线性演化特征。具体而言，模型权重和输出对数概率均与RL训练步数呈现显著线性相关。这表明RLVR主要放大训练早期出现的趋势，而非在整个优化轨迹中持续发现新行为。基于这种线性特性，我们探究能否通过中间检查点外推预测未来模型状态，从而避免持续的高成本训练。实验证明：权重外推法产生的模型性能与标准RL训练相当，同时显著降低计算需求；而对数概率外推法通过在RL训练保持稳定的步数范围外进行外推，在全部四个基准测试中均持续优于继续RL训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper investigates the linear evolution of large language models (LLMs) during reinforcement learning with verifiable rewards (RLVR), a costly post-training method that often requires extensive exploration. The authors observe that both model weights and output log-probabilities change linearly with training steps, indicating RLVR mainly amplifies early trends rather than discovering new behaviors. Based on this insight, they propose extrapolating from intermediate checkpoints to predict future model states, demonstrating that weight extrapolation achieves comparable performance to full RL training with less computation, while logits extrapolation consistently outperforms continued training across four benchmarks.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型在可验证奖励强化学习训练过程中的线性演化现象，该方法作为后训练的关键环节通常计算成本高昂且依赖大量探索。作者发现模型权重和输出对数概率均随训练步数呈强线性变化，表明RLVR主要放大早期趋势而非持续探索新行为。基于此线性特性，他们提出通过中间检查点外推来预测未来模型状态，实验证明权重外推能以更少计算达到与标准RL训练相当的性能，而对数概率外推在四个基准测试中均优于持续训练。</div>
</details>
</div>
<div class="card">
<div class="title">Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification</div>
<div class="meta-line">Authors: Rui Sun, Yifan Sun, Sheng Xu, Li Zhao, Jing Li, Daxin Jiang, Cheng Hua, Zuo Bai</div>
<div class="meta-line">First: 2026-01-07T14:03:22+00:00 · Latest: 2026-01-08T02:48:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03948v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market&#x27;s stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Trade-R1：通过过程级推理验证将可验证奖励桥接至随机环境</div>
<div class="mono" style="margin-top:8px">强化学习（RL）使大型语言模型（LLM）在数学和编程等可验证奖励提供明确信号的领域实现了卓越的推理能力。然而，将这一范式扩展到金融决策面临市场随机性的挑战：奖励虽可验证但本质具有噪声，导致标准RL退化为奖励黑客行为。为此，我们提出Trade-R1，一种通过过程级推理验证将可验证奖励桥接至随机环境的模型训练框架。核心创新是一种验证方法，将评估长篇金融文档推理的问题转化为结构化检索增强生成（RAG）任务。我们构建了三角一致性度量，评估检索证据、推理链和决策之间的两两对齐，作为噪声市场回报的有效性过滤器。我们探索了两种奖励整合策略：用于稳定对齐信号的固定效应语义奖励（FSR），以及用于耦合幅度优化的动态效应语义奖励（DSR）。在不同国家资产选择上的实验表明，该范式减少了奖励黑客行为，其中DSR在保持最高推理一致性的同时实现了卓越的跨市场泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the challenge of applying Reinforcement Learning (RL) with Large Language Models (LLMs) to financial decision-making, where market stochasticity makes verifiable rewards noisy and leads to reward hacking. The proposed method, Trade-R1, introduces a process-level reasoning verification framework that transforms the evaluation of reasoning over financial documents into a structured Retrieval-Augmented Generation (RAG) task, using a triangular consistency metric to filter noisy returns via alignment between evidence, reasoning, and decisions. Experimental results on cross-country asset selection show that the framework mitigates reward hacking, with the Dynamic-effect Semantic Reward (DSR) strategy achieving the best generalization and reasoning consistency.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于将强化学习与大语言模型应用于金融决策的挑战，其中市场的随机性使得可验证奖励带有噪声并导致奖励黑客行为。所提出的Trade-R1方法通过过程级推理验证框架，将金融文档的推理评估转化为结构化检索增强生成任务，利用三角一致性度量，通过证据、推理链和决策之间的对齐来过滤噪声回报。在不同国家资产选择上的实验结果表明，该框架减少了奖励黑客问题，其中动态效应语义奖励策略实现了最佳的跨市场泛化能力和最高的推理一致性。</div>
</details>
</div>
<div class="card">
<div class="title">TSSR: Two-Stage Swap-Reward-Driven Reinforcement Learning for Character-Level SMILES Generation</div>
<div class="meta-line">Authors: Jacob Ede Levine, Yun Lyan Luo, Sai Chandra Kosaraju</div>
<div class="meta-line">First: 2026-01-08T02:35:22+00:00 · Latest: 2026-01-08T02:35:22+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04521v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04521v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The design of reliable, valid, and diverse molecules is fundamental to modern drug discovery, as improved molecular generation supports efficient exploration of the chemical space for potential drug candidates and reduces the cost of early design efforts. Despite these needs, current chemical language models that generate molecules as SMILES strings are vulnerable to compounding token errors: many samples are unparseable or chemically implausible, and hard constraints meant to prevent failure can restrict exploration. To address this gap, we introduce TSSR, a Two-Stage, Swap-Reward-driven reinforcement learning (RL) framework for character-level SMILES generation. Stage one rewards local token swaps that repair syntax, promoting transitions from invalid to parseable strings. Stage two provides chemistry-aware feedback from RDKit diagnostics, rewarding reductions in valence, aromaticity, and connectivity issues. The reward decomposes into interpretable terms (swap efficiency, error reduction, distance to validity), is model agnostic, and requires no task-specific labels or hand-crafted grammars. We evaluated TSSR on the MOSES benchmark using a GRU policy trained with PPO in both pure RL (P-RL) from random initialization and fine-tuning RL (F-RL) starting from a pretrained chemical language model, assessing 10,000 generated SMILES per run. In P-RL, TSSR significantly improves syntactic validity, chemical validity, and novelty. In F-RL, TSSR preserves drug-likeness and synthesizability while increasing validity and novelty. Token-level analysis shows that syntax edits and chemistry fixes act jointly to reduce RDKit detected errors. TSSR converts a sparse terminal objective into a denser and more interpretable reward, improving both syntactic and chemical quality without reducing diversity. TSSR is dataset-agnostic and can be adapted to various reinforcement learning approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TSSR：面向字符级SMILES生成的两阶段交换奖励驱动强化学习</div>
<div class="mono" style="margin-top:8px">设计可靠、有效且多样化的分子是现代药物发现的基础，因为改进的分子生成有助于高效探索潜在候选药物的化学空间，并降低早期设计成本。尽管存在这些需求，当前将分子生成为SMILES字符串的化学语言模型易受复合标记错误影响：许多样本无法解析或化学上不合理，而旨在防止失败的硬性约束可能限制探索。为弥补这一不足，我们提出了TSSR——一种面向字符级SMILES生成的两阶段交换奖励驱动强化学习框架。第一阶段奖励修复语法的局部标记交换，促进从无效字符串到可解析字符串的转换；第二阶段通过RDKit诊断提供化学感知反馈，奖励减少化合价、芳香性和连接性问题。该奖励可分解为可解释项（交换效率、错误减少、有效距离），具有模型无关性，且无需任务特定标签或手工语法。我们在MOSES基准上使用PPO训练的GRU策略评估TSSR，包括从随机初始化的纯强化学习（P-RL）和从预训练化学语言模型开始的微调强化学习（F-RL），每次运行评估10,000个生成的SMILES。在P-RL中，TSSR显著提升语法有效性、化学有效性和新颖性；在F-RL中，TSSR在保持类药性与可合成性的同时提高有效性和新颖性。标记级分析表明语法编辑与化学修复共同减少RDKit检测错误。TSSR将稀疏终端目标转化为更密集且可解释的奖励，在保持多样性的同时提升语法与化学质量。该方法具有数据集无关性，可适配多种强化学习策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to generate reliable, diverse molecules for drug discovery while overcoming the vulnerability of current SMILES-based models to compounding token errors that produce invalid or implausible strings, this paper introduces TSSR, a two-stage reinforcement learning framework. The method employs a swap-reward-driven approach where the first stage rewards local token swaps to repair syntax, and the second stage provides chemistry-aware feedback from RDKit to address valence and connectivity issues, all without requiring task-specific labels or grammars. Experimental results on the MOSES benchmark show that TSSR significantly improves syntactic and chemical validity as well as novelty in pure RL settings, while in fine-tuning RL it maintains drug-likeness and synthesizability while boosting validity and novelty, effectively converting sparse objectives into denser, interpretable rewards.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决当前基于SMILES的分子生成模型易受复合标记错误影响、常产生无效或化学上不合理字符串的问题，以支持药物发现中可靠、多样分子的设计。方法上提出了TSSR，一个两阶段强化学习框架：第一阶段通过奖励局部标记交换来修复语法，第二阶段利用RDKit提供化学感知反馈以减少价态和连接性错误，且无需任务特定标签或手工语法。在MOSES基准上的实验结果表明，在纯强化学习设置中，TSSR显著提升了句法有效性、化学有效性和新颖性；在微调强化学习中，它在保持类药性和可合成性的同时提高了有效性和新颖性，从而将稀疏目标转化为更密集、可解释的奖励。</div>
</details>
</div>
<div class="card">
<div class="title">Multiagent Reinforcement Learning with Neighbor Action Estimation</div>
<div class="meta-line">Authors: Zhenglong Luo, Zhiyong Chen, Aoxiang Liu</div>
<div class="meta-line">First: 2026-01-08T02:26:57+00:00 · Latest: 2026-01-08T02:26:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04511v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04511v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents&#x27; behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于邻域动作估计的多智能体强化学习</div>
<div class="mono" style="margin-top:8px">多智能体强化学习作为一种重要的智能范式，能够在复杂系统中实现协同决策。然而，现有方法通常依赖智能体间显式的动作交换来评估动作价值函数，这在现实工程环境中常因通信限制、延迟、能耗及可靠性要求而难以实现。本文从人工智能视角提出一种增强型多智能体强化学习框架，采用动作估计神经网络来推断智能体行为。通过集成轻量级动作估计模块，每个智能体仅利用局部可观测信息推断邻近智能体的行为，从而在不显式共享动作的情况下实现协同策略学习。该方法与标准TD3算法完全兼容，并可扩展至更大规模的多智能体系统。在工程应用层面，该框架已在双臂机器人操作任务中实现并验证：两个机械臂协同搬运物体。实验结果表明，该方法显著提升了实际机器人系统的鲁棒性与部署可行性，同时降低了对信息基础设施的依赖。总体而言，本研究推动了去中心化多智能体人工智能系统的发展，使人工智能能在动态、信息受限的现实环境中高效运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the impracticality of explicit action exchange in real-world multiagent systems due to communication constraints, this paper proposes a multiagent reinforcement learning framework that uses neural networks to estimate neighboring agents&#x27; actions from local observations, eliminating the need for direct action sharing. The method integrates a lightweight action estimation module into standard TD3 algorithms, enabling decentralized collaborative policy learning that scales to larger systems. Experimental validation in dual-arm robotic manipulation tasks shows that this approach enhances robustness and deployment feasibility while reducing reliance on information infrastructure.</div>
<div class="mono" style="margin-top:8px">针对现实多智能体系统中因通信限制导致显式动作交换不切实际的问题，本文提出了一种多智能体强化学习框架，利用神经网络从局部观测中估计邻近智能体的动作，无需直接动作共享。该方法将轻量级动作估计模块集成到标准TD3算法中，实现了可扩展的分散式协作策略学习。在双臂机器人协同搬运任务的实验中，该方法显著提升了系统的鲁棒性和部署可行性，同时降低了对信息基础设施的依赖。</div>
</details>
</div>
<div class="card">
<div class="title">Mirror Descent Actor Critic via Bounded Advantage Learning</div>
<div class="meta-line">Authors: Ryo Iwaki</div>
<div class="meta-line">First: 2025-02-06T08:14:03+00:00 · Latest: 2026-01-08T02:15:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03854v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.03854v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Regularization is a core component of recent Reinforcement Learning (RL) algorithms. Mirror Descent Value Iteration (MDVI) uses both Kullback-Leibler divergence and entropy as regularizers in its value and policy updates. Despite its empirical success in discrete action domains and strong theoretical guarantees, the performance of KL-entropy-regularized methods does not surpass that of a strong entropy-only-regularized method in continuous action domains. In this study, we propose Mirror Descent Actor Critic (MDAC) as an actor-critic style instantiation of MDVI for continuous action domains, and show that its empirical performance is significantly boosted by bounding the actor&#x27;s log-density terms in the critic&#x27;s loss function, compared to a non-bounded naive instantiation. Further, we relate MDAC to Advantage Learning by recalling that the actor&#x27;s log-probability is equal to the regularized advantage function in tabular cases, and theoretically discuss when and why bounding the advantage terms is validated and beneficial. We also empirically explore effective choices for the bounding functions, and show that MDAC performs better than strong non-regularized and entropy-only-regularized methods with an appropriate choice of the bounding functions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于有界优势学习的镜像下降演员-评论家算法</div>
<div class="mono" style="margin-top:8px">正则化是现代强化学习算法的核心组成部分。镜像下降值迭代算法在值和策略更新中同时使用KL散度和熵作为正则化器。尽管该算法在离散动作领域取得实证成功并具备强理论保证，但在连续动作领域中，KL-熵正则化方法的性能并未超越仅使用熵正则化的强基准方法。本研究提出镜像下降演员-评论家算法作为MDVI在连续动作领域的演员-评论家式实例化，并通过实验证明：在评论家损失函数中对演员的对数密度项进行有界处理，相比无界朴素实例化能显著提升算法性能。进一步地，我们通过回顾表格场景中演员对数概率等于正则化优势函数这一性质，将MDAC与优势学习建立理论关联，并从理论上论证优势项有界处理的适用条件与效益机制。最后通过实证研究探索有界函数的选择策略，证明采用合适有界函数的MDAC能够超越强非正则化及纯熵正则化方法的性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Mirror Descent Actor Critic (MDAC), an actor-critic algorithm designed to improve the performance of mirror descent-based reinforcement learning in continuous action domains, where prior KL-entropy-regularized methods underperformed compared to entropy-only regularization. The method&#x27;s key innovation is bounding the actor&#x27;s log-density terms within the critic&#x27;s loss function, which is motivated by its equivalence to bounding advantage estimates, thereby stabilizing training and enhancing empirical results. Experiments demonstrate that MDAC, with appropriate bounding functions, outperforms strong baseline methods that use either no regularization or only entropy regularization.</div>
<div class="mono" style="margin-top:8px">本文提出了镜像下降演员评论家（MDAC）算法，旨在提升基于镜像下降的强化学习在连续动作域中的性能，此前该类结合KL散度和熵正则化的方法在此类任务中表现不及仅使用熵正则化的方法。该方法的核心创新是在评论家损失函数中对演员的对数密度项进行有界处理，这源于该对数概率在表格情况下等同于正则化优势函数，理论分析探讨了此有界处理的合理性与益处。实验结果表明，通过选择合适的边界函数，MDAC的性能优于未使用正则化或仅使用熵正则化的强基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Improving and Accelerating Offline RL in Large Discrete Action Spaces with Structured Policy Initialization</div>
<div class="meta-line">Authors: Matthew Landers, Taylor W. Killian, Thomas Hartvigsen, Afsaneh Doryab</div>
<div class="meta-line">First: 2026-01-07T22:57:21+00:00 · Latest: 2026-01-07T22:57:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04441v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04441v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning in discrete combinatorial action spaces requires searching over exponentially many joint actions to simultaneously select multiple sub-actions that form coherent combinations. Existing approaches either simplify policy learning by assuming independence across sub-actions, which often yields incoherent or invalid actions, or attempt to learn action structure and control jointly, which is slow and unstable. We introduce Structured Policy Initialization (SPIN), a two-stage framework that first pre-trains an Action Structure Model (ASM) to capture the manifold of valid actions, then freezes this representation and trains lightweight policy heads for control. On challenging discrete DM Control benchmarks, SPIN improves average return by up to 39% over the state of the art while reducing time to convergence by up to 12.8$\times$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过结构化策略初始化改进并加速大规模离散动作空间中的离线强化学习</div>
<div class="mono" style="margin-top:8px">在离散组合动作空间中进行强化学习，需要搜索指数级数量的联合动作，以同时选择多个构成连贯组合的子动作。现有方法要么通过假设子动作间相互独立来简化策略学习（这常导致动作不连贯或无效），要么尝试同时学习动作结构和控制（这种方法缓慢且不稳定）。我们提出了结构化策略初始化（SPIN），这是一个两阶段框架：首先预训练动作结构模型（ASM）以捕捉有效动作的流形，随后冻结该表征并训练轻量级策略头进行控制。在具有挑战性的离散DM Control基准测试中，SPIN将平均回报较现有最优方法提升高达39%，同时将收敛时间缩短高达12.8倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of reinforcement learning in large discrete action spaces, where existing methods either produce invalid actions by assuming sub-action independence or suffer from slow, unstable joint training. The proposed method, Structured Policy Initialization (SPIN), addresses this with a two-stage framework: it first pre-trains an Action Structure Model to learn the manifold of valid actions, then freezes this representation to train lightweight policy heads for efficient control. Experimental results on discrete DM Control benchmarks show that SPIN improves average return by up to 39% over state-of-the-art methods and accelerates convergence by up to 12.8 times.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决大规模离散动作空间中强化学习的挑战，现有方法要么因假设子动作独立而产生无效动作，要么因联合训练而速度慢、不稳定。提出的方法称为结构化策略初始化（SPIN），采用两阶段框架：首先预训练动作结构模型以学习有效动作的流形，然后冻结该表示并训练轻量级策略头进行高效控制。在离散DM Control基准测试中，实验结果表明SPIN将平均回报提高了高达39%，并加速收敛达12.8倍。</div>
</details>
</div>
<div class="card">
<div class="title">SAINT: Attention-Based Policies for Discrete Combinatorial Action Spaces</div>
<div class="meta-line">Authors: Matthew Landers, Taylor W. Killian, Thomas Hartvigsen, Afsaneh Doryab</div>
<div class="meta-line">First: 2025-05-17T18:34:31+00:00 · Latest: 2026-01-07T22:33:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12109v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12109v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The combinatorial structure of many real-world action spaces leads to exponential growth in the number of possible actions, limiting the effectiveness of conventional reinforcement learning algorithms. Recent approaches for combinatorial action spaces impose factorized or sequential structures over sub-actions, failing to capture complex joint behavior. We introduce the Sub-Action Interaction Network using Transformers (SAINT), a novel policy architecture that represents multi-component actions as unordered sets and models their dependencies via self-attention conditioned on the global state. SAINT is permutation-invariant, sample-efficient, and compatible with standard policy optimization algorithms. In 20 distinct combinatorial environments across three task domains, including environments with nearly 17 million joint actions, SAINT consistently outperforms strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAINT：面向离散组合动作空间的注意力策略</div>
<div class="mono" style="margin-top:8px">现实世界中许多动作空间的组合结构导致可能动作数量呈指数级增长，限制了传统强化学习算法的有效性。现有针对组合动作空间的方法通常对子动作施加因子化或序列化结构，难以捕捉复杂的联合行为。本文提出基于Transformer的子动作交互网络（SAINT），这是一种创新的策略架构，将多组件动作表示为无序集合，并通过基于全局状态的自注意力机制建模其依赖关系。SAINT具有排列不变性、样本高效性，且兼容标准策略优化算法。在涵盖三个任务领域的20个不同组合环境中（包括包含近1700万个联合动作的环境），SAINT均持续优于现有强基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of reinforcement learning in discrete combinatorial action spaces, where the exponential growth of possible actions limits conventional methods and existing approaches fail to capture complex joint dependencies among sub-actions. The authors propose SAINT, a policy architecture that represents multi-component actions as unordered sets and models their interactions via self-attention conditioned on the global state, ensuring permutation invariance and compatibility with standard policy optimization. Experimental results across 20 combinatorial environments, including those with up to 17 million joint actions, demonstrate that SAINT consistently outperforms strong baselines in terms of sample efficiency and performance.</div>
<div class="mono" style="margin-top:8px">本文针对离散组合动作空间中强化学习的挑战，即可能动作数量的指数增长限制了传统方法，且现有方法难以捕捉子动作间的复杂联合依赖。作者提出了SAINT，一种将多组件动作表示为无序集合并通过基于全局状态的自注意力建模其交互的策略架构，确保了排列不变性并与标准策略优化算法兼容。在涵盖三个任务领域的20个组合环境中，包括具有近1700万联合动作的环境，实验结果表明SAINT在样本效率和性能上均持续优于强基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards</div>
<div class="meta-line">Authors: Ali Rad, Khashayar Filom, Darioush Keivan, Peyman Mohajerin Esfahani, Ehsan Kamalinejad</div>
<div class="meta-line">First: 2026-01-07T21:31:26+00:00 · Latest: 2026-01-07T21:31:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04411v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04411v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?
  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden&#x27;s index J=TPR-FPR. This yields a sharp phase transition: when J&gt;0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J&lt;0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J&gt;0, noise primarily rescales convergence time (&quot;rate, not fate&quot;). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>速率还是命运？RLV$^\varepsilon$R：具有可验证噪声奖励的强化学习</div>
<div class="mono" style="margin-top:8px">具有可验证奖励的强化学习（RLVR）是一种训练大语言模型的简洁而强大的范式：采样生成结果、验证、更新。然而实践中，验证器几乎从不完美——单元测试仅覆盖有限边界情况；人工与合成标注存在缺陷；大语言模型评判器（如RLAIF）具有噪声且可能被利用——该问题在测试稀疏且日益由模型生成的困难领域（尤其是代码生成）中更为严重。我们提出一个现实问题：验证噪声仅是减缓学习进程（速率问题），还是可能逆转学习结果（命运问题）？为此，我们建立了RLVR动态的可解析多臂老虎机视角，通过GRPO算法实例化并在受控实验中验证。通过建模假阳性与假阴性、将生成结果归类为重复推理模式，我们得到了概率单纯形上的复制器式（自然选择）流。该动态解耦为正确模式内部竞争与错误模式质量的一维演化，其漂移完全由约登指数J=TPR-FPR决定。这引发尖锐相变：当J&gt;0时错误模式趋于消亡（学习）；J=0时过程呈中性；J&lt;0时错误模式持续放大直至主导（反学习与崩溃）。在学习区间J&gt;0内，噪声主要重标收敛时间（“速率问题，非命运问题”）。在合成噪声下的可验证编程任务实验中，我们重现了预测的J=0边界。该框架超越噪声分析，为研究RLVR稳定性、收敛性及算法干预提供了通用理论视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper investigates whether noisy reward signals in reinforcement learning with verifiable rewards (RLVR) merely slow convergence or can cause catastrophic failure, particularly in domains like coding where verification is imperfect. The authors develop an analytical model using a multi-armed bandit framework, instantiated with GRPO, which treats completions as reasoning modes and incorporates false positives/negatives to derive replicator dynamics. Their key finding is a sharp phase transition governed by Youden&#x27;s index J (true positive rate minus false positive rate): if J &gt; 0, incorrect modes are driven to extinction and learning succeeds; if J = 0, the process is neutral; and if J &lt; 0, incorrect modes dominate, leading to anti-learning and collapse. Experimental results on programming tasks with synthetic noise confirm the predicted J=0 boundary, showing that in the learning regime (J &gt; 0), noise primarily affects convergence rate rather than ultimate fate.</div>
<div class="mono" style="margin-top:8px">本文研究了在强化学习与可验证奖励（RLVR）中，噪声奖励信号是仅减缓收敛速度还是会导致灾难性失败，特别是在编程等验证不完善的领域。作者使用多臂老虎机框架建立了一个分析模型，以GRPO实例化，将补全视为推理模式并纳入假阳性/假阴性，推导出复制器动力学。核心发现是一个由约登指数J（真阳性率减假阳性率）控制的尖锐相变：若J &gt; 0，错误模式被驱向灭绝，学习成功；若J = 0，过程呈中性；若J &lt; 0，错误模式占据主导，导致反学习和崩溃。在合成噪声下的编程任务实验中，结果证实了预测的J=0边界，表明在学习机制（J &gt; 0）下，噪声主要影响收敛速率而非最终结果。</div>
</details>
</div>
<div class="card">
<div class="title">Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces</div>
<div class="meta-line">Authors: Arsyi Aziz, Peng Wei</div>
<div class="meta-line">First: 2026-01-07T21:18:28+00:00 · Latest: 2026-01-07T21:18:28+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures, 4 tables. Presented at SESAR Innovation Days 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04401v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Transformer的多智能体强化学习在结构化与非结构化空域中的间隔保障研究</div>
<div class="mono" style="margin-top:8px">传统基于优化的流量管理依赖严格遵循预计算时刻表，限制了先进空中交通随机运行所需的灵活性。相比之下，多智能体强化学习提供了一种去中心化、自适应的框架，能更好地处理不确定性，满足安全航空器间隔保障需求。尽管具有优势，现有MARL方法常过度拟合特定空域结构，限制了其对新配置的适应性。为提升泛化能力，我们在相对极坐标状态空间中重构MARL问题，并针对多样化交通模式和交叉角度训练Transformer编码器模型。该学习模型通过提供速度建议来解决冲突，同时保持航空器接近期望巡航速度。实验中，我们在结构化与非结构化空域中评估了1、2、3层编码器深度，发现单层编码器配置优于深层变体，实现了近乎零的空中接近碰撞率，且间隔违规持续时间更短。此外，相同配置也优于纯注意力设计的基线模型。综合结果表明，新构建的状态表示、神经网络架构创新设计及训练策略，为结构化与非结构化空域中的航空器间隔保障提供了适应性强、可扩展的去中心化解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inflexibility of conventional optimization-based metering for Advanced Air Mobility&#x27;s stochastic operations, this paper proposes a multi-agent reinforcement learning approach using a transformer encoder to provide decentralized, adaptive separation assurance. The method recasts the problem in a relative polar state space and trains the model across diverse traffic patterns and intersection angles to improve generalization beyond specific airspace structures. Experimental results in both structured and unstructured airspaces show that a single-layer encoder configuration outperformed deeper variants and a pure attention baseline, achieving near-zero near mid-air collision rates and shorter loss-of-separation infringements, demonstrating an adaptable and scalable solution.</div>
<div class="mono" style="margin-top:8px">针对传统基于优化的间隔管理方法在先进空中机动性随机操作中灵活性不足的问题，本文提出了一种基于多智能体强化学习的方法，利用Transformer编码器提供去中心化、自适应的间隔保障。该方法将问题重构于相对极坐标状态空间，并在多样交通模式和交叉角度下训练模型，以提升对不同空域结构的泛化能力。在结构化和非结构化空域的实验中，单层编码器配置优于更深层变体及纯注意力基线，实现了近乎零的空中接近碰撞率和更短的间隔丧失违规时间，证明该方案具有适应性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay</div>
<div class="meta-line">Authors: Mohsen Jalaeian-Farimani</div>
<div class="meta-line">First: 2026-01-07T20:59:18+00:00 · Latest: 2026-01-07T20:59:18+00:00</div>
<div class="meta-line">Comments: Submitted to ECC26 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04392v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($λ$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($λ$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($λ$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework&#x27;s inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Enhanced-FQL($λ$)：一种高效可解释的强化学习方法，融合新型模糊资格迹与分段经验回放</div>
<div class="mono" style="margin-top:8px">本文提出一种模糊强化学习框架Enhanced-FQL($λ$)，将新型模糊资格迹与分段经验回放机制集成至基于模糊贝尔曼方程的模糊Q学习中，适用于连续控制任务。该方法采用可解释的模糊规则库替代复杂神经网络架构，并通过两项关键创新保持竞争力：采用带资格迹的模糊贝尔曼方程实现稳定的多步信用分配，以及基于分段的高效经验回放机制提升样本效率。理论分析证明该方法在标准假设下具有收敛性。在连续控制领域的广泛实验表明，相较于n步模糊TD和模糊SARSA($λ$)基线方法，Enhanced-FQL($λ$)在保持显著低于DDPG等深度强化学习算法计算复杂度的同时，实现了更优的样本效率与更低的方差。该框架固有的可解释性、计算效率及理论收敛保证，使其特别适用于对透明度和资源约束要求严格的安全关键型应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for efficient, interpretable reinforcement learning (RL) for continuous control, particularly in safety-critical applications with resource constraints. The method, Enhanced-FQL(λ), introduces a fuzzy RL framework that replaces neural networks with an interpretable fuzzy rule base, integrating two key innovations: Fuzzified Eligibility Traces (FET) for stable multi-step credit assignment via a fuzzified Bellman equation, and a Segmented Experience Replay (SER) mechanism for memory-efficient learning. Experimental results in continuous control domains show that the approach achieves superior sample efficiency and lower variance compared to fuzzy TD and SARSA(λ) baselines, while maintaining lower computational complexity than deep RL methods like DDPG, with theoretical convergence guarantees under standard assumptions.</div>
<div class="mono" style="margin-top:8px">本文旨在为连续控制任务，特别是在对透明度和资源有限制要求的安全关键应用中，开发高效且可解释的强化学习方法。所提出的方法Enhanced-FQL(λ)是一个模糊强化学习框架，它使用可解释的模糊规则库替代复杂的神经网络架构，并集成了两项关键创新：通过模糊化贝尔曼方程实现的模糊化资格迹（FET）用于稳定的多步信用分配，以及分段经验回放（SER）机制以提高样本效率。在连续控制领域的广泛评估表明，该方法相比模糊TD和模糊SARSA(λ)基线实现了更优的样本效率和更低的方差，同时保持了比DDPG等深度强化学习替代方案更低的计算复杂度，并在标准假设下具有理论收敛保证。</div>
</details>
</div>
<div class="card">
<div class="title">BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces</div>
<div class="meta-line">Authors: Matthew Landers, Taylor W. Killian, Hugo Barnes, Thomas Hartvigsen, Afsaneh Doryab</div>
<div class="meta-line">First: 2024-10-28T15:49:46+00:00 · Latest: 2026-01-07T20:57:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.21151v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.21151v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning in high-dimensional, discrete action spaces is challenging due to the exponential scaling of the joint action space with the number of sub-actions and the complexity of modeling sub-action dependencies. Existing methods either exhaustively evaluate the action space, making them computationally infeasible, or factorize Q-values, failing to represent joint sub-action effects. We propose Branch Value Estimation (BraVE), a value-based method that uses tree-structured action traversal to evaluate a linear number of joint actions while preserving dependency structure. BraVE outperforms prior offline RL methods by up to $20\times$ in environments with over four million actions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BraVE：面向离散组合动作空间的离线强化学习</div>
<div class="mono" style="margin-top:8px">在高维离散动作空间中，由于联合动作空间随子动作数量呈指数级扩展且子动作依赖关系建模复杂，离线强化学习面临挑战。现有方法要么需穷举评估动作空间导致计算不可行，要么对Q值进行因子化而无法表征联合子动作效应。本文提出分支价值估计（BraVE），这是一种基于价值的方法，通过树状动作遍历评估线性数量的联合动作，同时保持依赖结构。在动作空间超过四百万的环境中，BraVE相比现有离线强化学习方法性能提升最高达20倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the computational challenges of offline reinforcement learning in high-dimensional discrete combinatorial action spaces, where the joint action space scales exponentially and existing methods are either infeasible or fail to capture sub-action dependencies. The proposed method, Branch Value Estimation (BraVE), employs a tree-structured action traversal approach to efficiently evaluate a linear number of joint actions while preserving their dependency structure. In experiments, BraVE demonstrates significant performance improvements, outperforming prior offline RL methods by up to 20 times in environments with over four million actions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决高维离散组合动作空间中离线强化学习的计算挑战，其中联合动作空间呈指数级增长，现有方法要么计算不可行，要么无法捕捉子动作间的依赖关系。所提出的方法——分支价值估计（BraVE），采用树形结构动作遍历策略，在保持依赖结构的同时高效评估线性数量的联合动作。实验结果表明，BraVE在动作数超过四百万的环境中，性能显著优于先前的离线强化学习方法，提升幅度高达20倍。</div>
</details>
</div>
<div class="card">
<div class="title">Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning</div>
<div class="meta-line">Authors: Anton Roupassov-Ruiz, Yiyang Zuo</div>
<div class="meta-line">First: 2026-01-07T20:09:28+00:00 · Latest: 2026-01-07T20:09:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04365v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04365v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化强化学习中神经策略与程序化策略的生存动态研究</div>
<div class="mono" style="margin-top:8px">在进化强化学习任务中，智能体策略通常编码为小型人工神经网络。此类表征缺乏显式模块化结构，限制了行为可解释性。本研究探讨以软可微决策列表实现的程序化策略能否达到神经策略的性能水平。为支持可复现评估，我们首次完整复现并开源了199年经典人工生命进化强化学习测试平台。通过4000次独立试验，采用原始研究未涉及的卡普兰-迈耶曲线与限制平均生存时间指标进行严格生存分析。研究发现程序化策略与神经策略的生存概率存在统计学显著差异：程序化智能体平均比神经智能体多生存201.69步；仅使用学习机制（无进化）的软可微决策列表智能体，其平均生存步数比同时使用学习与进化机制的神经智能体多73.67步。这些结果表明在人工生命环境中，程序化策略的生存性能可超越神经策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited interpretability of neural network policies in evolutionary reinforcement learning (ERL), this study investigates whether programmatic policies, implemented as soft differentiable decision lists (SDDL), can match or exceed the performance of traditional neural network policies (NERL). The method involves a fully specified, open-source reimplementation of a classic 1992 Artificial Life ERL testbed, enabling reproducible evaluation through rigorous survival analysis across 4000 independent trials using Kaplan-Meier curves and Restricted Mean Survival Time metrics. The main experimental results show a statistically significant difference, with programmatic policies (PERL) surviving on average 201.69 steps longer than neural policies, and SDDL agents using learning alone outperforming neural agents using both learning and evolution by 73.67 steps, demonstrating superior survival performance for programmatic policies.</div>
<div class="mono" style="margin-top:8px">本研究针对进化强化学习中神经网络策略可解释性不足的问题，探讨了以软可微决策列表实现的程序化策略是否能达到或超越传统神经网络策略的性能。方法上，通过完全指定并开源复现1992年经典人工生命进化强化学习测试平台，利用Kaplan-Meier曲线和限制平均生存时间指标，在4000次独立试验中进行了严谨的生存分析。主要实验结果表明，程序化策略比神经网络策略平均多生存201.69步，且仅使用学习的软可微决策列表代理比同时使用学习和进化的神经网络代理多生存73.67步，显示出程序化策略在生存性能上的显著优势。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Bayesian Optimization for Portfolio Management with an Adaptive Scheduling</div>
<div class="meta-line">Authors: Zinuo You, John Cartlidge, Karen Elliott, Menghan Ge, Daniel Gold</div>
<div class="meta-line">Venue: In 2025 9th International Conference on Advances in Artificial Intelligence (ICAAI 2025), November 14-16, 2025, Manchester, United Kingdom. ACM, New York, NY, USA, 5 pages</div>
<div class="meta-line">First: 2025-04-18T07:40:24+00:00 · Latest: 2026-01-07T19:25:50+00:00</div>
<div class="meta-line">Comments: 5 pages, 2 figures; version of record. ICAAI 2025, 9th International Conference on Advances in Artificial Intelligence (ICAAI 2025), November 14-16, 2025, Manchester, United Kingdom. ACM, New York, NY, USA, 5 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.13529v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.13529v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing black-box portfolio management systems are prevalent in the financial industry due to commercial and safety constraints, though their performance can fluctuate dramatically with changing market regimes. Evaluating these non-transparent systems is computationally expensive, as fixed budgets limit the number of possible observations. Therefore, achieving stable and sample-efficient optimization for these systems has become a critical challenge. This work presents a novel Bayesian optimization framework (TPE-AS) that improves search stability and efficiency for black-box portfolio models under these limited observation budgets. Standard Bayesian optimization, which solely maximizes expected return, can yield erratic search trajectories and misalign the surrogate model with the true objective, thereby wasting the limited evaluation budget. To mitigate these issues, we propose a weighted Lagrangian estimator that leverages an adaptive schedule and importance sampling. This estimator dynamically balances exploration and exploitation by incorporating both the maximization of model performance and the minimization of the variance of model observations. It guides the search from broad, performance-seeking exploration towards stable and desirable regions as the optimization progresses. Extensive experiments and ablation studies, which establish our proposed method as the primary approach and other configurations as baselines, demonstrate its effectiveness across four backtest settings with three distinct black-box portfolio management models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应调度的贝叶斯优化在投资组合管理中的改进</div>
<div class="mono" style="margin-top:8px">现有黑箱投资组合管理系统因商业与安全约束在金融行业广泛应用，但其性能随市场机制变化可能剧烈波动。由于固定预算限制了可观测次数，评估这类非透明系统的计算成本高昂。因此，实现稳定且样本高效的优化成为关键挑战。本研究提出一种新颖的贝叶斯优化框架（TPE-AS），在有限观测预算下提升黑箱投资组合模型的搜索稳定性与效率。传统贝叶斯优化仅最大化预期收益，可能导致搜索轨迹不稳定并使代理模型与真实目标失配，从而浪费有限评估资源。为解决这些问题，我们提出一种结合自适应调度与重要性采样的加权拉格朗日估计器。该估计器通过同时最大化模型性能与最小化观测方差，动态平衡探索与利用。随着优化进程推进，它将搜索从广泛性能探索引导至稳定理想区域。在四种回测场景与三种黑箱投资组合模型上的大量实验及消融研究（将本方法设为主方案，其他配置作为基线）验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for stable and sample-efficient optimization of non-transparent, computationally expensive black-box portfolio management systems under limited evaluation budgets, this paper introduces TPE-AS, a novel Bayesian optimization framework. The method employs a weighted Lagrangian estimator with an adaptive schedule and importance sampling to dynamically balance exploration and exploitation, aiming to maximize model performance while minimizing the variance of observations. Experimental results from extensive tests across four backtest settings with three distinct portfolio models demonstrate that the proposed approach effectively improves search stability and efficiency compared to standard Bayesian optimization.</div>
<div class="mono" style="margin-top:8px">针对在有限评估预算下，对不透明、计算成本高的黑盒投资组合管理系统进行稳定且样本高效优化的需求，本文提出了TPE-AS这一新颖的贝叶斯优化框架。该方法采用带有自适应调度和重要性采样的加权拉格朗日估计器，动态平衡探索与利用，旨在最大化模型性能的同时最小化观测方差。在四种回测设置和三种不同投资组合模型上的大量实验结果表明，与标准贝叶斯优化相比，所提出的方法有效提高了搜索的稳定性和效率。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
