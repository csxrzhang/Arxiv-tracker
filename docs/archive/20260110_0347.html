<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-10 03:47</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260110_0347</div>
    <div class="row"><div class="card">
<div class="title">GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</div>
<div class="meta-line">Authors: Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov</div>
<div class="meta-line">First: 2026-01-08T18:59:24+00:00 · Latest: 2026-01-08T18:59:24+00:00</div>
<div class="meta-line">Comments: NVIDIA-Tech Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDPO：面向多奖励强化学习优化的组奖励解耦归一化策略优化</div>
<div class="mono" style="margin-top:8px">随着语言模型能力日益增强，用户不仅期望其提供准确响应，还要求其行为能适应多样场景下的人类偏好。为实现这一目标，强化学习（RL）流程开始引入多个奖励，每个奖励对应一种特定偏好，以引导模型达成期望行为。然而，近期研究默认在多奖励设置下应用组相对策略优化（GRPO），而未检验其适用性。本文证明，直接应用GRPO对不同轨迹奖励组合进行归一化会导致其坍缩为相同的优势值，降低训练信号的分辨率，进而引发次优收敛甚至早期训练失败。为此，我们提出组奖励解耦归一化策略优化（GDPO），该方法通过解耦各奖励的归一化过程，更真实地保留其相对差异，实现更精确的多奖励优化，并显著提升训练稳定性。我们在工具调用、数学推理和代码推理三项任务中对比GDPO与GRPO，评估正确性指标（准确率、错误率）与约束遵循指标（格式、长度）。在所有实验设置下，GDPO均持续优于GRPO，证明了其在多奖励强化学习优化中的有效性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for language models to align with diverse human preferences, which has led to multi-reward reinforcement learning pipelines. The method introduces Group reward-Decoupled Normalization Policy Optimization (GDPO), which addresses issues in prior approaches like GRPO by decoupling the normalization of individual rewards to preserve their relative differences and improve training stability. Experimental results across tool calling, math reasoning, and coding reasoning tasks show that GDPO consistently outperforms GRPO in both correctness and constraint adherence metrics, demonstrating its effectiveness and generalizability.</div>
<div class="mono" style="margin-top:8px">本文的动机是语言模型需要符合多样化的人类偏好，这催生了多奖励强化学习流程。方法上提出了组奖励解耦归一化策略优化（GDPO），通过解耦单个奖励的归一化来保留其相对差异并提升训练稳定性，解决了先前方法如GRPO的不足。在工具调用、数学推理和代码推理任务上的实验结果表明，GDPO在正确性和约束遵循指标上均一致优于GRPO，验证了其有效性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI</div>
<div class="meta-line">Authors: Zain Iqbal, Lorenzo Valerio</div>
<div class="meta-line">First: 2026-01-08T18:31:11+00:00 · Latest: 2026-01-08T18:31:11+00:00</div>
<div class="meta-line">Comments: 6 pages, 9 figures, 2 Tables, conference [Submitted in PerConAI-2026]</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EARL：面向泛在人工智能的液态状态机能耗感知优化</div>
<div class="mono" style="margin-top:8px">泛在人工智能日益依赖于在严格资源约束下实现低延迟与高能效计算的设备端学习系统。液态状态机为泛在及神经形态系统中的低功耗时序处理提供了可行路径，但其部署仍面临高超参数敏感性和传统优化方法忽略能耗约束导致计算成本高昂的挑战。本研究提出EARL——一种能耗感知强化学习框架，通过将贝叶斯优化与基于自适应强化学习的候选策略相结合，协同优化精度与能耗。EARL采用代理模型进行全局探索，利用强化学习实现动态候选优先级排序，并引入早期终止机制消除冗余评估，显著降低计算开销。在三个基准数据集上的实验表明，相较于主流超参数调优框架，EARL实现了6%至15%的精度提升、60%至80%的能耗降低，以及最高达数量级的优化时间缩减。这些结果凸显了能耗感知自适应搜索在提升资源受限设备端AI应用中液态状态机效率与可扩展性方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need for low-latency, energy-efficient on-device AI, where Liquid State Machines (LSMs) are promising but difficult to deploy due to sensitive hyperparameters and costly, energy-agnostic optimization. The proposed method, EARL, is an energy-aware reinforcement learning framework that combines Bayesian optimization for global exploration with an adaptive reinforcement learning policy to jointly optimize model accuracy and energy consumption, incorporating an early termination mechanism to cut redundant evaluations. Experimental results on three benchmarks show EARL achieves 6-15% higher accuracy, 60-80% lower energy use, and up to a 10x reduction in optimization time compared to state-of-the-art hyperparameter tuning methods, demonstrating its effectiveness for scalable, resource-constrained LSM deployment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是普适AI对低延迟、高能效设备端学习的需求，液态机虽具潜力，但因超参数敏感且传统优化方法计算成本高、忽略能耗而难以部署。所提出的EARL方法是一个能量感知的强化学习框架，它结合贝叶斯优化进行全局探索和自适应强化学习策略，以共同优化模型精度与能耗，并采用早期终止机制减少冗余评估。在三个基准数据集上的实验结果表明，与领先的超参数调优框架相比，EARL实现了6-15%的精度提升、60-80%的能耗降低以及高达一个数量级的优化时间缩减，证明了其在资源受限设备端AI应用中提升液态机效率与可扩展性的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">An interpretable data-driven approach to optimizing clinical fall risk assessment</div>
<div class="meta-line">Authors: Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Holley Farley, Kimia Ghobadi</div>
<div class="meta-line">First: 2026-01-08T18:17:31+00:00 · Latest: 2026-01-08T18:17:31+00:00</div>
<div class="meta-line">Comments: arXiv admin note: substantial text overlap with arXiv:2510.20714</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05194v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study&#x27;s risk labels, and without changing the tool&#x27;s form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种可解释的数据驱动方法优化临床跌倒风险评估</div>
<div class="mono" style="margin-top:8px">本研究旨在通过数据驱动建模方法，使约翰霍普金斯跌倒风险评估工具（JHFRAT）的跌倒风险预测与更具临床意义的指标更好对齐。我们对2022年3月至2023年10月期间约翰霍普金斯医疗系统三家医院的54,209例住院病例进行了回顾性队列分析，其中20,208例被纳入高风险跌倒病例，13,941例被纳入低风险跌倒病例。为融入临床知识并保持可解释性，我们采用约束评分优化（CSO）模型重新调整JHFRAT评分权重，同时保留其累加结构和临床阈值。重新校准指调整项目权重，使生成的评分能更一致地按研究风险标签排序病例，且不改变工具形式或部署流程。模型预测性能较当前JHFRAT显著提升（CSO AUC-ROC=0.91，JHFRAT AUC-ROC=0.86），相当于每周在约翰霍普金斯医疗系统多保护35名高风险患者。约束评分优化模型在包含与不包含电子健康记录变量时表现相似。尽管基准黑盒模型（XGBoost）在性能指标上优于基于知识的约束逻辑回归（AUC-ROC=0.94），但CSO对风险标签变化表现出更强稳健性。这种循证方法为医疗系统通过数据驱动优化技术系统性提升住院患者跌倒预防方案和患者安全提供了坚实基础，有助于改善医疗环境中的风险评估和资源配置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study was motivated by the need to better align the widely used Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinical measures to improve inpatient fall prediction. The method employed a data-driven, interpretable approach using constrained score optimization (CSO) models to recalibrate the weighting of the JHFRAT&#x27;s existing items, preserving its additive structure and clinical thresholds without altering its deployment workflow. The main experimental results showed that the optimized model significantly improved predictive performance, achieving an AUC-ROC of 0.91 compared to the original tool&#x27;s 0.86, which translates to identifying an additional 35 high-risk patients per week across the health system, while maintaining robustness comparable to more complex black-box models.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过数据驱动的方法，使约翰霍普金斯跌倒风险评估工具（JHFRAT）的预测结果与更多临床有意义的指标更好地结合，以优化临床跌倒风险评估。研究方法采用了一种可解释的约束评分优化模型，在保留原工具加法结构和临床阈值的前提下，重新调整其项目权重，而不改变工具的形式或部署流程。主要实验结果表明，优化后的模型预测性能显著提升，AUC-ROC达到0.91，优于原工具的0.86，这意味着在全医疗系统内每周可多识别出35名高风险患者，同时该模型在风险标签变化下表现出比复杂黑盒模型更强的稳健性，为基于证据改进住院患者跌倒预防提供了基础。</div>
</details>
</div>
<div class="card">
<div class="title">SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning</div>
<div class="meta-line">Authors: Yanchang Liang, Xiaowei Zhao</div>
<div class="meta-line">First: 2026-01-08T18:10:35+00:00 · Latest: 2026-01-08T18:10:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05187v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05187v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimuAgent：基于大语言模型并强化学习增强的Simulink建模助手</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）已革新文本代码自动化，但其在图式工程工作流中的潜力尚未充分挖掘。本文推出SimuAgent，一款专为Simulink定制的LLM驱动建模与仿真智能体。它用简洁的字典式Python表示替代冗长XML，大幅减少令牌数、提升可解释性，并实现快速进程内仿真。经两阶段训练的轻量级规划-执行架构使智能体兼具底层工具技能与高层设计推理能力。为应对长周期任务中的稀疏奖励问题，我们提出反射式GRPO（ReGRPO），通过自反思轨迹为分组相对策略优化（GRPO）提供丰富中间反馈，加速收敛并增强鲁棒性。在新发布的包含5300项跨领域建模任务的基准测试集SimuBench上，经SimuAgent微调的Qwen2.5-7B模型比标准强化学习基线收敛更快、建模精度更高，在相同基准的少样本提示评估中甚至超越GPT-4o。消融实验证实两阶段课程学习与抽象重构数据增强能进一步提升泛化能力。SimuAgent完全在本地中等硬件上训练运行，为工业模型驱动工程提供隐私安全、经济高效的解决方案，弥合了LLM与图形化建模环境间的鸿沟，为工业场景下的AI辅助工程设计提供实用工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the under-explored potential of large language models (LLMs) in graph-oriented engineering workflows, this paper introduces SimuAgent, an LLM-based assistant for Simulink modeling. The method employs a concise Python representation to replace verbose XML, reducing token counts and enabling fast simulation, alongside a two-stage trained plan-execute architecture enhanced with a novel reinforcement learning approach called Reflection-GRPO (ReGRPO), which uses self-reflection traces to provide intermediate feedback for long-horizon tasks. Experimental results on the SimuBench benchmark of 5300 tasks show that a fine-tuned Qwen2.5-7B model achieves higher modeling accuracy and faster convergence than standard RL baselines, even surpassing GPT-4o in few-shot evaluations, while ablations confirm the benefits of the two-stage curriculum and data augmentation, with the system operating on-premise for privacy and cost-effectiveness.</div>
<div class="mono" style="margin-top:8px">本文的动机在于探索大语言模型（LLM）在图形化工程工作流中尚未充分开发的潜力，为此提出了SimuAgent，一个基于LLM的Simulink建模助手。方法上，它用简洁的Python字典表示替代冗长的XML，显著减少了令牌数量并实现了快速仿真，同时采用两阶段训练的计划-执行架构，并引入了名为Reflection-GRPO（ReGRPO）的新型强化学习方法，通过自我反思轨迹为长周期任务提供中间反馈。在包含5300个多领域建模任务的新基准SimuBench上的实验结果表明，经SimuAgent微调的Qwen2.5-7B模型比标准强化学习基线收敛更快、建模精度更高，甚至在少量样本提示评估中超越了GPT-4o，消融实验进一步验证了两阶段课程学习和数据增强对泛化能力的提升，且该系统可在本地硬件上运行，提供了隐私保护且成本高效的工业解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art</div>
<div class="meta-line">Authors: Timofey Tomashevskiy</div>
<div class="meta-line">First: 2026-01-08T17:42:56+00:00 · Latest: 2026-01-08T17:42:56+00:00</div>
<div class="meta-line">Comments: 20 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05152v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05152v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.
  Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向非平稳环境的安全持续强化学习方法研究综述</div>
<div class="mono" style="margin-top:8px">本文对持续安全在线强化学习方法进行了前沿综述。我们探讨了构建持续在线安全强化学习算法的理论层面、挑战与开放性问题，基于考虑非平稳性适应的安全学习机制类型，提出了持续在线安全强化学习方法的分类体系与具体细节。我们系统归类了在线强化学习算法的安全约束形式化方法，并最终讨论了构建可靠安全的在线学习算法的发展前景。关键词：非平稳环境下的安全强化学习、非平稳条件下的安全持续强化学习、HM-MDP、NSMDP、POMDP、安全POMDP、持续学习约束、安全持续强化学习综述、安全持续强化学习调查、安全持续强化学习、分布漂移下的安全在线学习、安全持续在线适应、安全强化学习、安全探索、安全适应、约束马尔可夫决策过程、部分可观测马尔可夫决策过程、安全强化学习与隐马尔可夫决策过程、安全在线强化学习、安全元学习、安全元强化学习、基于上下文的安全强化学习、持续学习的安全约束形式化</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper surveys continual safe online reinforcement learning (COSRL) methods, motivated by the need for reliable algorithms that can adapt to nonstationary environments while maintaining safety constraints. The method involves categorizing existing approaches based on their safe learning mechanisms and formulations of safety constraints, drawing on frameworks like HM-MDP, NSMDP, and POMDP to address adaptation and partial observability. The main experimental results are not detailed as this is a review paper, but it synthesizes state-of-the-art techniques, highlights theoretical challenges, and outlines prospects for developing robust safe online learning systems in dynamic settings.</div>
<div class="mono" style="margin-top:8px">本文综述了持续安全在线强化学习方法，其动机在于需要可靠的算法以适应非平稳环境并保持安全约束。方法包括基于安全学习机制和安全约束形式对现有方法进行分类，利用HM-MDP、NSMDP和POMDP等框架来解决适应性和部分可观测性问题。由于这是一篇综述论文，未详述具体实验结果，但综合了最先进的技术，强调了理论挑战，并展望了在动态环境中开发鲁棒安全在线学习系统的前景。</div>
</details>
</div>
<div class="card">
<div class="title">Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning</div>
<div class="meta-line">Authors: Marvin Illian, Ramin Khalili, Antonio A. de A. Rocha, Lin Wang</div>
<div class="meta-line">First: 2026-01-07T16:51:33+00:00 · Latest: 2026-01-08T17:32:37+00:00</div>
<div class="meta-line">Comments: 11 pages, 12 figures, v2: Corrected performance numbers in the conclusion; no change to methodology</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04083v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04083v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based on operator experience and rarely adapted to dynamic network conditions. In this work, we ask: Can an agent automatically learn and adapt cell (re)selection parameters to consistently improve network performance? We present a reinforcement learning (RL)-based framework called CellPilot that adaptively tunes cell (re)selection parameters by learning spatiotemporal patterns of mobile network dynamics. Our study with real-world data demonstrates that even a lightweight RL agent can outperform conventional heuristic reconfigurations by up to 167%, while generalizing effectively across different network scenarios. These results indicate that data-driven approaches can significantly improve cell (re)selection configurations and enhance mobile network performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶的细胞：基于强化学习的自适应小区（重）选择</div>
<div class="mono" style="margin-top:8px">5G网络的广泛部署与4G/LTE网络的共存，为移动设备提供了多样化的候选连接小区。然而，如何将移动设备与小区关联以最大化整体网络性能，即小区（重）选择，仍是移动运营商面临的关键挑战。目前，小区（重）选择参数通常基于运营商经验手动配置，很少适应动态网络条件。本研究探讨：能否通过智能体自动学习并调整小区（重）选择参数，持续提升网络性能？我们提出了名为CellPilot的强化学习框架，通过学习移动网络动态的时空模式自适应调整参数。基于真实数据的实验表明，即使轻量级强化学习智能体也能超越传统启发式重配置方法达167%，并在不同网络场景中有效泛化。这些结果表明，数据驱动方法可显著优化小区（重）选择配置，提升移动网络性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of manually configuring cell (re)selection parameters in dynamic 4G/5G networks, this paper introduces CellPilot, a reinforcement learning framework that automatically learns spatiotemporal patterns to adapt these parameters. The method employs a lightweight RL agent to tune configurations in response to network conditions. Experimental results using real-world data show that this approach outperforms conventional heuristic reconfigurations by up to 167% in network performance and generalizes effectively across diverse scenarios, indicating significant improvements through data-driven automation.</div>
<div class="mono" style="margin-top:8px">本文针对4G/5G动态网络中手动配置小区（重）选择参数的挑战，提出了CellPilot这一强化学习框架，通过学习时空模式自动调整参数。该方法采用轻量级强化学习代理，根据网络条件动态优化配置。基于真实数据的实验结果表明，该方法在性能上比传统启发式重配置提升高达167%，并能有效泛化到不同网络场景，证明了数据驱动自动化可显著提升移动网络性能。</div>
</details>
</div>
<div class="card">
<div class="title">Act-Adaptive Margin: Dynamically Calibrating Reward Models for Subjective Ambiguity</div>
<div class="meta-line">Authors: Feiteng Fang, Dingwei Chen, Xiang Huang, Ting-En Lin, Yuchuan Wu, Xiong Liu, Xinge Ye, Ziqiang Liu, Haonan Zhang, Liang Zhu, Hamid Alinejad-Rokny, Min Yang, Yongbin Li</div>
<div class="meta-line">First: 2025-05-29T18:15:18+00:00 · Latest: 2026-01-08T16:58:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23923v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.23923v2">PDF</a> · <a href="https://github.com/calubkk/AAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Currently, most reinforcement learning tasks focus on domains like mathematics and programming, where verification is relatively straightforward. However, in subjective tasks such as role-playing, alignment techniques struggle to make progress, primarily because subjective reward modeling using the Bradley-Terry model faces significant challenges when dealing with ambiguous preferences. To improve reward modeling in subjective tasks, this paper proposes AAM (\textbf{\underline{A}}ct-\textbf{\underline{A}}daptive \textbf{\underline{M}}argin), which enhances reward modeling by dynamically calibrating preference margins using the model&#x27;s internal parameter knowledge. We design two versions of AAM that efficiently generate contextually-appropriate preference gaps without additional human annotation. This approach fundamentally improves how reward models handle subjective rewards by better integrating generative understanding with preference scoring. To validate AAM&#x27;s effectiveness in subjective reward modeling, we conduct evaluations on RewardBench, JudgeBench, and challenging role-playing tasks. Results show that AAM significantly improves subjective reward modeling performance, enhancing Bradley-Terry reward models by 2.95\% in general tasks and 4.85\% in subjective role-playing tasks. Furthermore, reward models trained with AAM can help downstream alignment tasks achieve better results. Our test results show that applying rewards generated by AAM-Augmented RM to preference learning techniques (e.g., GRPO) achieves state-of-the-art results on CharacterEval and Charm. Code and dataset are available at https://github.com/calubkk/AAM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动作自适应边界：针对主观模糊性的奖励模型动态校准方法</div>
<div class="mono" style="margin-top:8px">当前大多数强化学习任务集中于数学、编程等验证相对直接的领域。然而在角色扮演等主观性任务中，对齐技术进展缓慢，主要因为基于Bradley-Terry模型的主观奖励建模在处理模糊偏好时面临显著挑战。为改进主观任务的奖励建模，本文提出AAM（动作自适应边界），通过利用模型内部参数知识动态校准偏好边界来增强奖励建模。我们设计了两个版本的AAM，无需额外人工标注即可高效生成符合语境的偏好间隙。该方法通过更好地整合生成式理解与偏好评分，从根本上改进了奖励模型处理主观奖励的方式。为验证AAM在主观奖励建模中的有效性，我们在RewardBench、JudgeBench及高难度角色扮演任务上进行评估。结果表明，AAM显著提升主观奖励建模性能，将Bradley-Terry奖励模型在通用任务中提升2.95%，在主观角色扮演任务中提升4.85%。此外，经AAM训练的奖励模型可助力下游对齐任务取得更好效果：将AAM增强奖励模型生成的奖励应用于偏好学习技术（如GRPO），在CharacterEval和Charm基准上达到最先进水平。代码与数据集详见https://github.com/calubkk/AAM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that reinforcement learning alignment techniques struggle with subjective tasks like role-playing, where ambiguous preferences make reward modeling difficult using standard methods like the Bradley-Terry model. The proposed method, Act-Adaptive Margin (AAM), dynamically calibrates preference margins using the model&#x27;s internal parameter knowledge to generate contextually-appropriate preference gaps without extra human annotation, thereby better integrating generative understanding with preference scoring. Experimental results on RewardBench, JudgeBench, and role-playing tasks show that AAM improves Bradley-Terry reward models by 2.95% in general tasks and 4.85% in subjective tasks, and its augmented rewards help downstream alignment achieve state-of-the-art results on benchmarks like CharacterEval and Charm.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，强化学习对齐技术在角色扮演等主观任务上进展困难，因为使用布拉德利-特里模型进行主观奖励建模时面临偏好模糊的挑战。为此，本文提出了行动自适应边界方法，该方法利用模型内部参数知识动态校准偏好边界，无需额外人工标注即可生成符合上下文的偏好间隙，从而更好地将生成理解与偏好评分相结合。在RewardBench、JudgeBench和角色扮演任务上的实验结果表明，AAM将布拉德利-特里奖励模型在一般任务和主观任务上的性能分别提升了2.95%和4.85%，且其增强的奖励能帮助下游对齐任务在CharacterEval和Charm等基准上取得最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforced Efficient Reasoning via Semantically Diverse Exploration</div>
<div class="meta-line">Authors: Ziqi Zhao, Zhaochun Ren, Jiahong Zou, Liu Yang, Zhiwei Xu, Xuri Ge, Zhumin Chen, Xinyu Ma, Daiting Shi, Shuaiqiang Wang, Dawei Yin, Xin Xin</div>
<div class="meta-line">First: 2026-01-08T15:56:44+00:00 · Latest: 2026-01-08T15:56:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05053v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05053v1">PDF</a> · <a href="https://github.com/ZiqiZhao1/ROSE-rl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于语义多样性探索的强化高效推理</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）已被证明能有效增强大语言模型（LLMs）的推理能力。基于蒙特卡洛树搜索（MCTS）的扩展方法（如GRPO）通过提供树状推理推演，实现了细粒度和片段级的信用分配，从而改进了基础RLVR。然而，现有方法仍存在探索多样性有限和推理效率低下的问题。为解决上述挑战，我们提出了面向LLMs的基于语义多样性探索的强化高效推理方法——ROSE。为促进更多样化的推理探索，本方法融合了基于语义熵的分支策略和ε-探索机制：前者对已采样的推理推演进行操作，通过捕捉语义不确定性并选择语义分歧度高的分支点来生成新的连续推理路径；后者则从根节点随机启动推理推演，防止搜索过程过度局部化。为提升效率，我们设计了长度感知的片段级优势估计器，对简洁正确的推理给予奖励，同时对不必要的冗长推理链进行惩罚。基于Qwen和Llama模型在多种数学推理基准测试上的大量实验验证了ROSE的有效性与高效性。代码发布于https://github.com/ZiqiZhao1/ROSE-rl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing reinforcement learning with verifiable rewards (RLVR) methods, such as limited exploration diversity and inefficient reasoning in large language models (LLMs). It proposes ROSE, a method that enhances exploration through a semantic-entropy-based branching strategy and an ε-exploration mechanism to generate diverse reasoning paths and avoid local optima, while improving efficiency with a length-aware segment-level advantage estimator that rewards concise correct reasoning. Experimental results on mathematical reasoning benchmarks with Qwen and Llama models demonstrate the effectiveness and efficiency of ROSE in improving reasoning performance.</div>
<div class="mono" style="margin-top:8px">本文针对现有基于可验证奖励的强化学习方法在大语言模型推理中存在的探索多样性不足和效率低下问题，提出了ROSE方法。该方法通过基于语义熵的分支策略和ε探索机制来增强探索，生成语义多样的推理路径并避免局部最优，同时采用长度感知的片段级优势估计器来奖励简洁正确的推理以提高效率。在Qwen和Llama模型上的多个数学推理基准测试实验验证了ROSE的有效性和高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Robotic World Model Makes Offline Model-Based Reinforcement Learning Work on Real Robots</div>
<div class="meta-line">Authors: Chenhao Li, Andreas Krause, Marco Hutter</div>
<div class="meta-line">First: 2025-04-23T12:58:15+00:00 · Latest: 2026-01-08T15:43:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.16680v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.16680v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has achieved impressive results in robotics, yet high-performing pipelines remain highly task-specific, with little reuse of prior data. Offline Model-based RL (MBRL) offers greater data efficiency by training policies entirely from existing datasets, but suffers from compounding errors and distribution shift in long-horizon rollouts. Although existing methods have shown success in controlled simulation benchmarks, robustly applying them to the noisy, biased, and partially observed datasets typical of real-world robotics remains challenging. We present a principled pipeline for making offline MBRL effective on physical robots. Our RWM-U extends autoregressive world models with epistemic uncertainty estimation, enabling temporally consistent multi-step rollouts with uncertainty effectively propagated over long horizons. We combine RWM-U with MOPO-PPO, which adapts uncertainty-penalized policy optimization to the stable, on-policy PPO framework for real-world control. We evaluate our approach on diverse manipulation and locomotion tasks in simulation and on real quadruped and humanoid, training policies entirely from offline datasets. The resulting policies consistently outperform model-free and uncertainty-unaware model-based baselines, and fusing real-world data in model learning further yields robust policies that surpass online model-free baselines trained solely in simulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不确定性感知机器人世界模型使离线模型强化学习在真实机器人上生效</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在机器人领域已取得显著成果，但高性能流程仍高度依赖特定任务，对历史数据的复用有限。离线模型强化学习（MBRL）通过完全基于现有数据集训练策略，提升了数据效率，但在长时程推演中易受误差累积和分布偏移影响。尽管现有方法在受控仿真基准测试中表现成功，但将其稳健应用于真实机器人领域常见的噪声、偏差及部分可观测数据集仍具挑战。本文提出一种原则性流程，使离线MBRL在实体机器人上有效运行。我们的RWM-U方法通过认知不确定性估计扩展自回归世界模型，实现时间一致的多步推演，并将不确定性有效传递至长时程。结合MOPO-PPO框架——该框架将不确定性惩罚策略优化适配于稳定的同策略PPO架构以适用于现实控制——我们在仿真及真实四足/人形机器人上对多样化操作与移动任务进行评估，完全基于离线数据集训练策略。所得策略持续优于无模型及未考虑不确定性的模型基线方法，且在模型学习中融合真实世界数据进一步产生了超越纯仿真训练的在线无模型基线的稳健策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge of applying offline model-based reinforcement learning (MBRL) to real-world robotics, where existing methods struggle with compounding errors and distribution shift in noisy, partially observed datasets. The method introduces RWM-U, an autoregressive world model enhanced with epistemic uncertainty estimation to enable temporally consistent multi-step rollouts, combined with MOPO-PPO for uncertainty-penalized policy optimization within a stable on-policy framework. Experimental results on manipulation and locomotion tasks in simulation and on real quadruped and humanoid robots show that policies trained entirely from offline data outperform model-free and uncertainty-unaware model-based baselines, and incorporating real-world data further yields robust policies surpassing online model-free baselines trained only in simulation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线模型强化学习在现实机器人应用中面临的挑战，现有方法在噪声大、部分可观测的数据集中易受复合误差和分布偏移的影响。方法上提出了RWM-U，这是一种通过认知不确定性估计增强的自回归世界模型，能够实现时间一致的多步推演，并结合MOPO-PPO在稳定的同策略框架中进行不确定性惩罚的策略优化。在模拟和真实四足及人形机器人的多种操作与运动任务实验中，完全基于离线数据训练的策略一致优于无模型和无不确定性感知的模型基线，且融合真实世界数据进一步产生了超越仅模拟训练的在线无模型基线的鲁棒策略。</div>
</details>
</div>
<div class="card">
<div class="title">Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models</div>
<div class="meta-line">Authors: Yueqing Hu, Xinyang Peng, Shuting Peng, Hanqi Wang, Tianhong Wang</div>
<div class="meta-line">First: 2026-01-08T15:27:03+00:00 · Latest: 2026-01-08T15:27:03+00:00</div>
<div class="meta-line">Comments: 7 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05019v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Large Reasoning Models trained via reinforcement learning exhibit a &quot;natural&quot; alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the &quot;Hán Dān Xué Bù&quot; (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a &quot;Functional Alignment Collapse&quot;: while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines (&quot;Negative Transfer&quot;). Our analysis suggests that SFT induces a &quot;Cargo Cult&quot; effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher&#x27;s dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>邯郸学步（模仿）还是青出于蓝（掌握）？大语言模型推理蒸馏的认知视角</div>
<div class="mono" style="margin-top:8px">近期通过强化学习训练的大型推理模型展现出与人类认知成本的&#x27;自然&#x27;对齐。然而我们发现，当前主流的推理蒸馏范式——通过监督微调训练学生模型模仿这些推理轨迹——未能传递这种认知结构。在14个模型中检验&#x27;邯郸学步&#x27;（表面模仿）假说后，我们发现蒸馏引发了&#x27;功能对齐崩溃&#x27;：教师模型能反映人类难度缩放规律（$\bar{r}=0.64$），而蒸馏学生模型显著削弱了这种对齐（$\bar{r}=0.34$），甚至常低于自身蒸馏前的基线水平（&#x27;负迁移&#x27;）。分析表明监督微调会诱发&#x27;货物崇拜&#x27;效应：学生模型仪式化地复制推理的语言形式（冗长表达），却未内化教师模型的动态资源分配策略。因此，推理蒸馏使计算成本与认知需求脱钩，揭示类人认知是主动强化的涌现特性，而非被动模仿的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether reasoning distillation in Large Language Models (LLMs) leads to genuine mastery or merely superficial mimicry of teacher models. Motivated by the observation that teacher models trained with reinforcement learning align with human cognitive costs, the authors test the hypothesis that standard Supervised Fine-Tuning (SFT) for distillation fails to transfer this cognitive structure. Their method involves evaluating 14 models, comparing the alignment between model computational steps and human difficulty scaling before and after distillation. The main experimental results reveal a &#x27;Functional Alignment Collapse&#x27;: while teacher models show strong alignment (average correlation r=0.64), distilled students exhibit significantly degraded alignment (r=0.34) and often suffer from negative transfer, performing worse than their pre-distillation baselines. The analysis suggests SFT causes a &#x27;Cargo Cult&#x27; effect, where students copy the linguistic form of reasoning without internalizing the teacher&#x27;s efficient resource allocation, demonstrating that human-like cognitive alignment emerges from reinforcement learning, not passive imitation.</div>
<div class="mono" style="margin-top:8px">本文研究大型语言模型中的推理蒸馏是导致真正的掌握还是仅仅对教师模型的表面模仿。其动机源于观察到通过强化学习训练的教师模型与人类认知成本具有一致性，作者检验了标准的监督微调蒸馏方法无法传递这种认知结构的假设。研究方法涉及评估14个模型，比较蒸馏前后模型计算步骤与人类难度缩放之间的对齐性。主要实验结果表明存在&#x27;功能对齐崩溃&#x27;：教师模型表现出较强的对齐性（平均相关系数r=0.64），而蒸馏后的学生模型对齐性显著下降（r=0.34），且经常出现负迁移，性能甚至低于蒸馏前的基线。分析表明监督微调引发了&#x27;货物崇拜&#x27;效应，学生模型仅机械复制推理的语言形式，而未内化教师模型高效的资源分配策略，这证明类人的认知对齐特性源于强化学习，而非被动模仿。</div>
</details>
</div>
<div class="card">
<div class="title">Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing</div>
<div class="meta-line">Authors: Osvaldo Simeone</div>
<div class="meta-line">First: 2026-01-01T07:38:07+00:00 · Latest: 2026-01-08T15:17:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00245v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00245v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现代神经形态人工智能：从令牌内处理到令牌间处理</div>
<div class="mono" style="margin-top:8px">人工智能的快速发展带来了新颖的数据处理和生成能力，但也伴随着不断攀升的能耗需求。这一挑战重新激发了人们对神经形态计算原理的兴趣，该原理通过离散稀疏激活、循环动力学和非线性反馈，有望实现类脑的高效计算。事实上，现代人工智能架构正通过重度量化激活、状态空间动力学和稀疏注意力机制，日益体现神经形态原理。本文通过区分令牌内处理与令牌间处理的视角，详细阐述了神经形态模型、状态空间模型与Transformer架构之间的关联。早期神经形态人工智能研究大多基于脉冲神经网络进行令牌内处理，即对同一向量输入（如图像像素）的多通道或多特征进行变换。相比之下，近期研究探索了如何利用神经形态原理设计高效的令牌间处理方法，该方法能根据上下文相关性选择性地组合不同的信息元素。这些方法通过实现关联记忆机制，利用了状态空间动力学或稀疏自注意力。本文不仅通过令牌内与令牌间处理的视角系统介绍了现代神经形态人工智能模型，还综述了相关训练方法，涵盖从利用并行卷积处理的代理梯度到基于强化学习机制的局部学习规则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the escalating energy demands of modern AI, this paper explores how neuromorphic computing principles, which emphasize brain-like efficiency through sparse and discrete operations, are increasingly integrated into contemporary architectures. The method involves analyzing AI models through the lens of intra-token processing (transforming features within a single input, like an image via spiking neural networks) and inter-token processing (selectively combining contextual information across inputs using state-space models or sparse attention). The main experimental results and review indicate that these neuromorphic approaches, supported by training methods like surrogate gradients and local learning rules, enable more efficient data processing and generative capabilities.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现代人工智能不断增长的能耗挑战，因此探讨了如何将神经形态计算原则（通过稀疏和离散操作实现类脑效率）融入当前架构。方法上，通过区分词内处理（如使用脉冲神经网络转换单个输入如图像的内部特征）和词间处理（如利用状态空间模型或稀疏注意力根据上下文选择性组合不同信息元素）的视角，系统分析了各类模型。主要的实验成果与综述表明，这些神经形态方法，辅以代理梯度等训练技术，能够实现更高效的数据处理和生成能力。</div>
</details>
</div>
<div class="card">
<div class="title">On the Hidden Objective Biases of Group-based Reinforcement Learning</div>
<div class="meta-line">Authors: Aleksandar Fontana, Marco Simoni, Giulio Rossolini, Andrea Saracino, Paolo Mori</div>
<div class="meta-line">First: 2026-01-08T15:00:35+00:00 · Latest: 2026-01-08T15:00:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05002v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05002v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论基于群体的强化学习方法的隐性目标偏差</div>
<div class="mono" style="margin-top:8px">基于群体的强化学习方法，如群体相对策略优化（GRPO），目前被广泛用于大型语言模型的后训练。尽管这些方法在实证上取得了成功，但其奖励优化与底层训练目标之间存在结构性失配。本文通过在一个统一的代理公式框架下研究GRPO类方法，提出了理论分析。这一视角揭示了影响所有被分析方法的共性特征：（i）非均匀群体加权会导致共享前缀词元产生系统性梯度偏差；（ii）与AdamW优化器的交互使得训练动态对奖励缩放基本不敏感；（iii）在重复优化步骤中，优化器动量可能推动策略更新超出预设的裁剪区域。我们认为这些发现揭示了当前方法的根本局限性，并为未来公式设计提供了原则性指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the structural biases in group-based reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), which are commonly used for post-training large language models. The motivation stems from observed mismatches between reward optimization and training objectives, prompting a theoretical analysis through a unified surrogate formulation. The method reveals three key properties: non-uniform group weighting introduces systematic gradient biases on shared prefix tokens, interactions with AdamW optimizer reduce sensitivity to reward scaling, and optimizer momentum can push policy updates beyond intended clipping regions. These findings highlight fundamental limitations in current approaches and offer principled guidance for future method design.</div>
<div class="mono" style="margin-top:8px">本文研究了基于群体的强化学习方法（如群体相对策略优化GRPO）中存在的结构性偏差，这些方法广泛用于大型语言模型的后训练。研究动机源于奖励优化与训练目标之间的不匹配，通过统一的代理公式进行理论分析。方法揭示了三个关键特性：非均匀群体加权会导致共享前缀令牌的系统性梯度偏差，与AdamW优化器的交互使训练动态对奖励缩放不敏感，且优化器动量可能在重复优化步骤中将策略更新推至预期裁剪区域之外。这些发现凸显了当前方法的根本局限性，并为未来方法设计提供了原则性指导。</div>
</details>
</div>
<div class="card">
<div class="title">AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?</div>
<div class="meta-line">Authors: Henan Sun, Kaichi Yu, Yuyao Wang, Bowen Liu, Xunkai Li, Rong-Hua Li, Nuo Chen, Jia Li</div>
<div class="meta-line">First: 2026-01-08T14:54:44+00:00 · Latest: 2026-01-08T14:54:44+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04996v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04996v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.
  AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlgBench：大型推理模型对算法的理解程度如何？</div>
<div class="mono" style="margin-top:8px">推理能力已成为大型推理模型发展的核心焦点。尽管在MATH500和LiveCodeBench等多项推理基准测试中取得了显著进展，但现有的算法推理基准仍存在局限，未能回答一个关键问题：LRMs是否真正掌握了算法推理？为解答此问题，我们提出AlgBench——一个由专家策划、以算法为中心范式评估LRMs的基准。AlgBench包含27类算法逾3000道原创题目，由ACM算法专家构建并按综合分类体系组织，涵盖欧几里得结构、非欧几里得结构、非优化、局部优化、全局优化及启发式优化等类别。对主流LRMs（如Gemini-3-Pro、DeepSeek-v3.2-Speciale和GPT-o3）的实证评估显示显著的性能异质性：模型在非优化任务上表现良好（最高达92%），但在动态规划等全局优化算法上准确率骤降至49%左右。进一步分析揭示了“策略性过转移”现象，即模型因必要的低熵标记而过早放弃正确的算法设计。这些发现暴露了以问题为中心的强化学习的根本局限，并凸显了采用以算法为中心的训练范式对于实现稳健算法推理的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to assess whether Large Reasoning Models (LRMs) genuinely master algorithmic reasoning beyond existing benchmarks, this paper introduces AlgBench, an expert-curated benchmark with over 3,000 problems across 27 algorithms, categorized by structure and optimization. The method involves constructing a comprehensive taxonomy and evaluating leading LRMs like Gemini-3-Pro and GPT-o3. Experimental results show significant performance heterogeneity, with high accuracy on non-optimized tasks (up to 92%) but sharp drops to around 49% on globally optimized algorithms such as dynamic programming, revealing strategic over-shifts where models prematurely abandon correct designs due to low-entropy tokens, thus exposing limitations in current training paradigms.</div>
<div class="mono" style="margin-top:8px">本文旨在评估大型推理模型是否真正掌握算法推理，针对现有基准的不足，提出了AlgBench这一专家构建的基准，包含超过3000个问题，涵盖27种算法，并按结构和优化类别分类。方法上，通过构建全面分类体系，并对Gemini-3-Pro等领先模型进行评估。实验结果显示性能存在显著异质性：在非优化任务上准确率高达92%，但在动态规划等全局优化算法上骤降至约49%，揭示了模型因低熵标记而提前放弃正确算法设计的策略性过度转移，从而暴露了当前问题中心训练范式的根本局限。</div>
</details>
</div>
<div class="card">
<div class="title">ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning</div>
<div class="meta-line">Authors: Tonghe Zhang, Chao Yu, Sichang Su, Yu Wang</div>
<div class="meta-line">Venue: Published in The Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025</div>
<div class="meta-line">First: 2025-05-28T08:17:16+00:00 · Latest: 2026-01-08T14:39:03+00:00</div>
<div class="meta-line">Comments: 38 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22094v7">Abs</a> · <a href="https://arxiv.org/pdf/2505.22094v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://reinflow.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose ReinFlow, a simple yet effective online reinforcement learning (RL) framework that fine-tunes a family of flow matching policies for continuous robotic control. Derived from rigorous RL theory, ReinFlow injects learnable noise into a flow policy&#x27;s deterministic path, converting the flow into a discrete-time Markov Process for exact and straightforward likelihood computation. This conversion facilitates exploration and ensures training stability, enabling ReinFlow to fine-tune diverse flow model variants, including Rectified Flow [35] and Shortcut Models [19], particularly at very few or even one denoising step. We benchmark ReinFlow in representative locomotion and manipulation tasks, including long-horizon planning with visual input and sparse reward. The episode reward of Rectified Flow policies obtained an average net growth of 135.36% after fine-tuning in challenging legged locomotion tasks while saving denoising steps and 82.63% of wall time compared to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate of the Shortcut Model policies in state and visual manipulation tasks achieved an average net increase of 40.34% after fine-tuning with ReinFlow at four or even one denoising step, whose performance is comparable to fine-tuned DDIM policies while saving computation time for an average of 23.20%. Project webpage: https://reinflow.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReinFlow：基于在线强化学习的流匹配策略微调方法</div>
<div class="mono" style="margin-top:8px">本文提出ReinFlow，一种简洁高效的在线强化学习框架，用于微调连续机器人控制中的流匹配策略族。基于严谨的强化学习理论，ReinFlow向确定性流策略路径注入可学习噪声，将流转换为离散时间马尔可夫过程，实现精确直观的似然计算。该转换促进探索并保障训练稳定性，使ReinFlow能够微调包括整流流[35]和捷径模型[19]在内的多种流模型变体，尤其在极少数（甚至单步）去噪场景中表现突出。我们在具身运动与操作任务（含视觉输入的长时程规划与稀疏奖励任务）中评估ReinFlow。经微调后，整流流策略在挑战性足式运动任务中平均获得135.36%的净收益增长，同时相比前沿扩散强化学习微调方法DPPO[43]节省去噪步骤与82.63%的墙钟时间。在状态与视觉操作任务中，捷径模型策略经ReinFlow四步（甚至单步）微调后成功率平均净增40.34%，其性能与微调后的DDIM策略相当，同时平均节省23.20%的计算时间。项目主页：https://reinflow.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ReinFlow, an online reinforcement learning framework designed to fine-tune flow matching policies for continuous robotic control, motivated by the need to enhance policy performance while reducing computational steps. The method injects learnable noise into a deterministic flow policy, converting it into a Markov Process for exact likelihood computation, which stabilizes training and enables fine-tuning of models like Rectified Flow and Shortcut Models with very few denoising steps. Experimental results on locomotion and manipulation tasks show significant improvements: Rectified Flow policies achieved a 135.36% average net reward growth with reduced steps and 82.63% less wall time compared to DPPO, while Shortcut Model policies saw a 40.34% average net success rate increase at one to four steps, matching DDIM performance with 23.20% time savings.</div>
<div class="mono" style="margin-top:8px">本文提出了ReinFlow，一种在线强化学习框架，旨在微调连续机器人控制中的流匹配策略，其动机在于提升策略性能并减少计算步骤。该方法通过向确定性流策略注入可学习噪声，将其转换为马尔可夫过程以进行精确似然计算，从而稳定训练并支持对Rectified Flow和Shortcut Models等模型在极少去噪步骤下的微调。在运动和操作任务上的实验结果表明显著改进：Rectified Flow策略在奖励上实现了135.36%的平均净增长，同时减少了步骤和82.63%的墙钟时间（相比DPPO）；而Shortcut Model策略在一到四个步骤下取得了40.34%的平均净成功率提升，性能与DDIM相当且节省了23.20%的计算时间。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Shaping to Mitigate Reward Hacking in RLHF</div>
<div class="meta-line">Authors: Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</div>
<div class="meta-line">First: 2025-02-26T02:57:59+00:00 · Latest: 2026-01-08T14:33:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18770v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.18770v4">PDF</a> · <a href="https://github.com/PorUna-byte/PAR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR&#x27;s superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过奖励塑形缓解RLHF中的奖励黑客问题</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）对于使大语言模型（LLMs）与人类价值观对齐至关重要。然而，RLHF容易受到奖励黑客攻击，即智能体利用奖励函数的缺陷而非学习预期行为，从而降低对齐效果。尽管奖励塑形有助于稳定RLHF并部分缓解奖励黑客问题，但对塑形技术及其基本原理的系统性研究仍显不足。为填补这一空白，我们对主流奖励塑形方法进行了全面研究。分析提出两个关键设计原则：（1）RL奖励应有界；（2）RL奖励应具备快速初始增长后逐步收敛的特性。基于这些见解，我们提出偏好即奖励（PAR）方法，利用奖励模型中隐含的偏好作为强化学习信号。PAR具有两个关键的方差缩减特性，有助于稳定RLHF训练过程并有效扩展早停容忍窗口。我们在Gemma2-2B基础模型上使用Ultrafeedback-Binarized和HH-RLHF数据集评估PAR，实验结果表明其性能优于其他奖励塑形方法。在AlpacaEval 2.0基准测试中，PAR的胜率至少比其他方法高5个百分点。此外，PAR展现出卓越的数据效率——仅需单个参考奖励即可实现最优性能，并在完整训练两轮后仍能保持对奖励黑客攻击的鲁棒性。代码已开源：https://github.com/PorUna-byte/PAR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward hacking in Reinforcement Learning from Human Feedback (RLHF), where agents exploit flaws in reward functions instead of learning desired behaviors, thereby undermining alignment. The authors conduct a systematic study of existing reward shaping methods, identifying two key design principles: bounded RL rewards and a reward structure that grows quickly initially before converging gradually. Based on these principles, they propose Preference As Reward (PAR), a novel method that uses latent preferences from the reward model as the RL signal, offering variance reduction and enhanced training stability. Experiments on Gemma2-2B with Ultrafeedback-Binarized and HH-RLHF datasets show that PAR outperforms other shaping methods, achieving at least a 5 percentage point higher win rate on AlpacaEval 2.0, while demonstrating data efficiency and robustness against reward hacking even after extended training.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习（RLHF）中的奖励黑客问题展开研究，即智能体利用奖励函数的缺陷而非学习预期行为，从而损害模型对齐效果。作者系统分析了现有奖励塑形方法，总结出两个关键设计原则：强化学习奖励应有界，且奖励应快速初始增长后逐步收敛。基于这些原则，他们提出了偏好即奖励（PAR）新方法，利用奖励模型中的潜在偏好作为强化学习信号，具备方差降低特性并能提升训练稳定性。在Gemma2-2B模型上使用Ultrafeedback-Binarized和HH-RLHF数据集的实验表明，PAR优于其他塑形方法，在AlpacaEval 2.0基准上胜率至少高出5个百分点，同时展现出数据高效性，即使经过两轮完整训练仍能有效抵抗奖励黑客攻击。</div>
</details>
</div>
<div class="card">
<div class="title">ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning</div>
<div class="meta-line">Authors: Minda Hu, Zexuan Qiu, Zenan Xu, Kun Li, Bo Zhou, Irwin King</div>
<div class="meta-line">First: 2026-01-08T14:22:58+00:00 · Latest: 2026-01-08T14:22:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04973v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04973v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking&#x27;&#x27;, where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the &#x27;cold start&#x27; phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ConMax：面向高效思维链推理的置信度最大化压缩方法</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）的最新突破表明，广泛的思维链（CoT）生成对于实现自我验证和回溯等复杂认知行为以解决复杂任务至关重要。然而，这种能力常导致‘过度思考’现象，即模型生成冗余推理路径，在不提升准确性的情况下增加计算成本。虽然基于推理轨迹的监督微调（SFT）是‘冷启动’阶段的标准范式，但将现有压缩技术应用于这些轨迹往往会损害逻辑连贯性或产生过高的采样成本。本文提出ConMax（置信度最大化压缩），一种新颖的强化学习框架，旨在自动压缩推理轨迹的同时保留关键推理模式。ConMax将压缩构建为奖励驱动的优化问题，通过最大化预测保真度的答案置信度与推理有效性的思维置信度的加权组合，训练策略以剪枝冗余，并借助冻结的辅助LRM实现。在五个推理数据集上的大量实验表明，ConMax实现了更优的效率-性能平衡：相比强基线方法，推理长度减少43%而准确率仅下降0.7%，证明了其为LRMs生成高质量高效训练数据的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the problem of &#x27;overthinking&#x27; in large reasoning models, where extensive chain-of-thought generation increases computational costs without accuracy gains, and existing compression methods often harm logical coherence or are too costly. The method introduces ConMax, a reinforcement learning framework that formulates compression as a reward optimization problem, training a policy to prune redundant reasoning traces by maximizing a weighted combination of answer confidence and thinking confidence using a frozen auxiliary model. Experimental results across five reasoning datasets show that ConMax reduces inference length by 43% compared to strong baselines while incurring only a 0.7% accuracy drop, achieving a superior efficiency-performance trade-off for generating high-quality training data.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决大型推理模型中的&#x27;过度思考&#x27;问题，即冗长的思维链生成会增加计算成本而不提升准确性，且现有压缩方法常损害逻辑连贯性或成本过高。方法上提出了ConMax，这是一个强化学习框架，将压缩建模为奖励优化问题，通过训练一个策略来剪枝冗余推理轨迹，利用冻结的辅助模型最大化答案置信度和思维置信度的加权组合。在五个推理数据集上的实验结果表明，ConMax相比强基线将推理长度减少了43%，而准确率仅下降0.7%，实现了高效的性能权衡，能生成高质量的训练数据。</div>
</details>
</div>
<div class="card">
<div class="title">Text as a Universal Interface for Transferable Personalization</div>
<div class="meta-line">Authors: Yuting Liu, Jian Guan, Jia-Nan Li, Wei Wu, Jiang-Ming Yang, Jianzhe Zhao, Guibing Guo</div>
<div class="meta-line">First: 2026-01-08T14:09:17+00:00 · Latest: 2026-01-08T14:09:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04963v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04963v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box&#x27;&#x27; profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本作为可迁移个性化建模的通用接口</div>
<div class="mono" style="margin-top:8px">本研究探讨大型语言模型中的个性化问题。现有方法主要将用户偏好表示为隐式的、模型特定的向量或参数，产生难以解释且跨模型/任务迁移的“黑盒”配置文件。我们提出以自然语言作为偏好表达的通用接口，该方案具有模型无关性和任务无关性。这种表述能生成可解释、可复用的偏好描述，并自然支持根据新交互持续演化。为学习此类表征，我们引入两阶段训练框架：结合高质量合成数据的监督微调与强化学习，以优化长期效用和跨任务迁移能力。基于此框架，我们开发了通用偏好推理模型AlignXplore+，可生成文本化偏好摘要。在九个基准测试中，我们的80亿参数模型达到最先进性能——显著超越规模更大的开源模型——同时在跨任务、模型家族和交互格式方面展现出强大迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of personalizing large language models (LLMs) by moving away from opaque, model-specific vector representations of user preferences. The authors propose using natural language as a universal, interpretable, and transferable interface for preference representation, which can evolve over time. Their method involves a two-stage training framework combining supervised fine-tuning on synthesized data with reinforcement learning to optimize long-term utility and cross-task transfer, resulting in a model called AlignXplore+. Experimental results on nine benchmarks demonstrate that their 8B parameter model achieves state-of-the-art performance, surpassing larger open-source models and showing strong transferability across different tasks, model families, and interaction formats.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型个性化中的问题，旨在替代以往不透明、模型特定的用户偏好向量表示方法。研究者提倡使用自然语言作为通用、可解释且可迁移的偏好表示接口，以支持偏好的持续演化。其方法采用了一个两阶段训练框架，结合了基于高质量合成数据的监督微调和强化学习，以优化长期效用和跨任务迁移能力，由此开发了名为AlignXplore+的通用偏好推理模型。在九个基准测试上的实验结果表明，该8B参数模型实现了最先进的性能，显著超越了更大的开源模型，并在不同任务、模型系列和交互格式间展现出强大的可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following</div>
<div class="meta-line">Authors: Yirong Zeng, Yufei Liu, Xiao Ding, Yutai Hou, Yuxian Wang, Haonan Song, Wu Ning, Dandan Tu, Qixun Zhang, Bibo Cai, Yuxiang He, Ting Liu</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2026-01-08T14:00:51+00:00 · Latest: 2026-01-08T14:00:51+00:00</div>
<div class="meta-line">Comments: ACL under review 13 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\% in performance while achieving a 58\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>精度优于多样性：高精度奖励泛化至鲁棒指令跟随</div>
<div class="mono" style="margin-top:8px">在指令跟随任务中，通过可验证奖励扩展强化学习的核心观点认为：可验证的硬约束与不可验证的软约束的多样化混合对泛化至未见指令至关重要。本研究通过系统性实证检验挑战了这一主流共识。反直觉的是，我们发现仅使用硬约束训练的模型始终优于混合数据集训练的模型。大量实验表明，奖励精度（而非约束多样性）是实现有效对齐的主要驱动力。大语言模型评判器在检测错误响应时召回率较低，导致严重的奖励破解现象，从而削弱了多样性的优势。注意力机制分析进一步揭示，高精度奖励能形成可迁移的指令跟随元技能。基于这些发现，我们提出一种以数据为中心、优先考虑奖励精度的简洁有效优化策略。在五个基准测试中，该方法以13.4%的性能优势超越基线模型，同时减少58%的训练时间，并在指令跟随之外保持强泛化能力。本研究主张范式转变：从盲目追求数据多样性转向聚焦高精度奖励。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the prevailing belief that diverse mixtures of verifiable and unverifiable constraints are essential for generalizing instruction-following models, arguing instead that high-precision rewards are the primary driver of effective alignment. Through systematic empirical investigation, the authors find that models trained solely on hard constraints outperform those on mixed datasets, as low recall in LLM judges for detecting false responses leads to reward hacking, undermining diversity&#x27;s benefits. Analysis shows high-precision rewards foster a transferable meta-skill, motivating a data-centric refinement strategy that prioritizes precision. Evaluated on five benchmarks, this approach outperforms baselines by 13.4% in performance while reducing training time by 58%, demonstrating strong generalization beyond instruction following and advocating a paradigm shift toward precision over diversity.</div>
<div class="mono" style="margin-top:8px">本文挑战了当前主流观点，即认为可验证与不可验证约束的多样性混合对于指令跟随模型的泛化至关重要，转而提出高精度奖励才是有效对齐的主要驱动力。通过系统的实证研究，作者发现仅基于硬约束训练的模型优于混合数据集训练的模型，因为大语言模型法官在检测错误响应时召回率低，导致奖励黑客攻击，从而削弱了多样性的益处。分析表明高精度奖励能培养可迁移的元技能，据此提出了一种以数据为中心、优先考虑精度的优化策略。在五个基准测试中评估，该方法性能超越基线13.4%，同时训练时间减少58%，展现出超越指令跟随的强泛化能力，倡导了从盲目追求数据多样性转向高精度奖励的范式转变。</div>
</details>
</div>
<div class="card">
<div class="title">PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism AI Psychological Counselor</div>
<div class="meta-line">Authors: Qianjun Pan, Junyi Wang, Jie Zhou, Yutao Yang, Junsong Li, Kaiyin Xu, Yougen Zhou, Yihan Li, Jingyuan Zhao, Qin Chen, Ningning Zhou, Kai Chen, Liang He</div>
<div class="meta-line">First: 2026-01-05T05:26:57+00:00 · Latest: 2026-01-08T13:52:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01802v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01802v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PsychEval：面向高拟真AI心理咨询师的多会话多疗法基准测试</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估AI，我们提出\texttt{PsychEval}——一个多会话、多疗法、高拟真度的基准测试，旨在解决三大核心挑战：\textbf{1) 能否训练出高拟真AI咨询师？} 拟真咨询是需持续记忆与动态目标追踪的纵向任务。我们构建了跨三个独立阶段（6-10次会话）的多会话基准，要求模型具备记忆连续性、适应性推理与纵向规划等关键能力。数据集标注了涵盖677项元技能与4577项原子技能的完整专业体系。\textbf{2) 如何训练多疗法AI咨询师？} 现有模型常局限于单一疗法，而复杂案例常需跨疗法灵活策略。我们构建了涵盖五大疗法流派（心理动力学、行为主义、认知行为疗法、人本存在主义、后现代主义）的数据集，并基于六类核心心理议题建立了统一的三阶段临床整合疗法框架。\textbf{3) 如何系统评估AI咨询师？} 我们建立了包含来访者维度与咨询师维度的18项疗法专用/通用指标的评估体系，并构建了2000余个多样化来访者画像支持评估。大量实验分析充分验证了数据集的高质量与临床保真度。\texttt{PsychEval} 更超越了静态基准测试，可作为高保真强化学习环境，支持具备临床责任意识与适应性的AI咨询师的自主进化训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable AI in psychological assessment, this paper introduces PsychEval, a benchmark designed to train and evaluate highly realistic, multi-therapy AI counselors. The method involves constructing a multi-session dataset spanning 6-10 sessions across three clinical stages, annotated with over 677 meta-skills and 4577 atomic skills, and covering five therapeutic modalities within a unified framework. The main experimental results validate the dataset&#x27;s superior quality and clinical fidelity, and the benchmark serves as a reinforcement learning environment to enable the self-evolutionary training of adaptive AI counselors.</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估人工智能，本文提出了PsychEval基准，旨在训练和评估高真实性、多疗法的AI心理咨询师。其方法构建了一个多会话数据集，涵盖6-10次会话和三个临床阶段，标注了超过677项元技能和4577项原子技能，并在统一框架下覆盖了五种治疗模式。主要实验结果验证了数据集的优越质量和临床保真度，该基准可作为一个强化学习环境，用于实现自适应AI咨询师的自我进化训练。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective</div>
<div class="meta-line">Authors: Ziwen Wang, Shangshang Yang, Xiaoshan Yu, Haiping Ma, Xingyi Zhang</div>
<div class="meta-line">First: 2026-01-08T13:17:40+00:00 · Latest: 2026-01-08T13:17:40+00:00</div>
<div class="meta-line">Comments: KDD2026, 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04918v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04918v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners&#x27; mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers&#x27; domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures&#x27; full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model&#x27;s capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破认知诊断的鲁棒性壁垒：一次性神经架构搜索视角</div>
<div class="mono" style="margin-top:8px">随着网络技术的发展，智能导学系统（ITS）已能够提供日益精准和个性化的学习服务。认知诊断（CD）作为ITS的核心研究任务，旨在通过建模学习行为数据与知识状态之间的映射关系，推断学习者对特定知识概念的掌握程度。然而，现有研究侧重于提升模型性能，却忽视了观测响应数据中普遍存在的噪声污染问题，这严重阻碍了实际应用。此外，当前认知诊断模型（CDMs）的结构设计高度依赖研究者的领域专业知识，未能充分探索架构可能性，导致模型潜力未能完全释放。为解决这一问题，我们提出OSCD——一种用于认知诊断的进化多目标一次性神经架构搜索方法，旨在高效且鲁棒地提升模型评估学习者能力的效果。具体而言，OSCD通过训练和搜索两个阶段运行：在训练阶段，我们构建包含多样化架构组合的搜索空间，并训练以完全二叉树拓扑表示的权重共享超网络，从而超越人工设计先验，全面探索潜在架构；在搜索阶段，我们将异构噪声场景下的最优架构搜索建模为多目标优化问题（MOP），并开发了融合帕累托最优解搜索策略与跨场景性能评估的优化框架。基于真实教育数据集的广泛实验验证了OSCD模型为CD任务发现的最优架构的有效性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for robust cognitive diagnosis models that can handle noisy educational data and overcome the limitations of manually designed architectures, this paper proposes OSCD, a one-shot neural architecture search method. The method employs a two-stage process: first training a weight-sharing supernet over a diverse architectural search space, then formulating architecture selection as a multi-objective optimization problem to find solutions robust to heterogeneous noise. Experimental results on real-world datasets demonstrate that the architectures discovered by OSCD are both effective and robust for cognitive diagnosis tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对认知诊断模型在噪声数据下鲁棒性不足以及依赖专家手动设计架构的局限性，提出了OSCD，一种基于一次性神经架构搜索的方法。该方法采用两阶段流程：首先在多样化的架构搜索空间上训练一个权重共享的超网，随后将最优架构搜索构建为一个多目标优化问题，以寻找能适应异构噪声的解决方案。在真实教育数据集上的大量实验验证了OSCD所发现的最优架构对于认知诊断任务的有效性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking</div>
<div class="meta-line">Authors: Sofiene Lassoued, Laxmikant Shrikant Bahetic, Nathalie Weiß-Borkowskib, Stefan Lierc, Andreas Schwunga</div>
<div class="meta-line">Venue: Journal of Manufacturing Systems Journal of Manufacturing Systems Volume 82, October 2025, Pages 405-419</div>
<div class="meta-line">First: 2026-01-08T12:37:02+00:00 · Latest: 2026-01-08T12:37:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04887v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04887v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today&#x27;s rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>柔性制造系统内部物流：基于着色时序Petri网与动作掩码演员-评论家强化学习的AGV动态优化与工具共享</div>
<div class="mono" style="margin-top:8px">柔性制造系统（FMS）在当今快速发展的制造业环境中对优化生产流程至关重要。本文通过同时集成自动导引车（AGV）和工具共享系统引入额外复杂性，推进了传统作业车间调度问题。我们提出一种结合着色时序Petri网（CTPN）与基于模型的演员-评论家强化学习（MBRL）的新方法，有效应对FMS相关的多维度挑战。CTPN提供形式化建模结构与动态动作掩码机制，显著缩减动作搜索空间；MBRL则通过习得策略确保对动态环境的适应性。利用MBRL优势，我们引入前瞻策略优化AGV定位，提升运行效率。方法在小型公共基准及受Taillard基准启发新开发的大规模基准上验证，结果表明：在较小实例中与传统方法性能相当，在较大实例中完工时间指标更优，同时计算时间减少十倍。为确保可复现性，我们提出了兼容OpenAI Gym的环境与实例生成器，并通过消融实验评估各框架组件对整体性能的贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance Flexible Manufacturing Systems (FMS) with greater complexity, this paper integrates automated guided vehicles (AGVs) and tool-sharing into job shop scheduling. The method employs Colored-Timed Petri Nets (CTPNs) for formal modeling and dynamic action masking, combined with actor-critic model-based reinforcement learning (MBRL) for adaptability, including a lookahead strategy for AGV positioning. Experimental results on public and new large-scale benchmarks show the approach matches traditional methods on small instances and outperforms them on larger ones in makespan, while reducing computation time tenfold, with an ablation study confirming component contributions.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过整合自动导引车（AGV）和刀具共享系统来增强柔性制造系统（FMS）的复杂性，从而推进传统作业车间调度问题。方法上结合了着色时间Petri网（CTPN）进行形式化建模和动态动作屏蔽，以及基于演员-评论家模型的强化学习（MBRL）以实现适应性，并采用前瞻策略优化AGV定位。在公开基准和新开发的大规模基准上的实验结果表明，该方法在小型实例上与传统方法相当，在大型实例上则在完工时间方面表现更优，同时计算时间减少了十倍，消融研究验证了各组件对整体性能的贡献。</div>
</details>
</div>
<div class="card">
<div class="title">From Actions to Words: Towards Abstractive-Textual Policy Summarization in RL</div>
<div class="meta-line">Authors: Sahar Admoni, Assaf Hallak, Yftah Ziser, Omer Ben-Porat, Ofra Amir</div>
<div class="meta-line">First: 2025-03-13T16:10:14+00:00 · Latest: 2026-01-08T11:06:58+00:00</div>
<div class="meta-line">Comments: In Proceedings of AAMAS 2026 (The 25th International Conference on Autonomous Agents and Multi-Agent Systems)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10509v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.10509v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Explaining reinforcement learning agents is challenging because policies emerge from complex reward structures and neural representations that are difficult for humans to interpret. Existing approaches often rely on curated demonstrations that expose local behaviors but provide limited insight into an agent&#x27;s global strategy, leaving users to infer intent from raw observations. We propose SySLLM (Synthesized Summary using Large Language Models), a framework that reframes policy interpretation as a language-generation problem. Instead of visual demonstrations, SySLLM converts spatiotemporal trajectories into structured text and prompts an LLM to generate coherent summaries describing the agent&#x27;s goals, exploration style, and decision patterns. SySLLM scales to long-horizon, semantically rich environments without task-specific fine-tuning, leveraging LLM world knowledge and compositional reasoning to capture latent behavioral structure across policies. Expert evaluations show strong alignment with human analyses, and a large-scale user study found that 75.5% of participants preferred SySLLM summaries over state-of-the-art demonstration-based explanations. Together, these results position abstractive textual summarization as a paradigm for interpreting complex RL behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从行动到语言：强化学习中抽象文本策略摘要的探索</div>
<div class="mono" style="margin-top:8px">解释强化学习智能体具有挑战性，因为策略源自复杂的奖励结构和难以被人理解的神经表征。现有方法通常依赖精心设计的演示来展示局部行为，但对智能体全局策略的洞察有限，用户需从原始观察中推断意图。我们提出SySLLM（基于大语言模型的合成摘要框架），将策略解释重构为语言生成问题。SySLLM将时空轨迹转化为结构化文本，并提示大语言模型生成连贯摘要，描述智能体的目标、探索风格和决策模式，而非依赖视觉演示。该框架无需任务特定微调即可扩展到长周期、语义丰富的环境，利用大语言模型的世界知识和组合推理能力捕捉策略间的潜在行为结构。专家评估显示其与人类分析高度一致，大规模用户研究表明75.5%的参与者更倾向于SySLLM摘要而非当前最先进的基于演示的解释方法。这些成果共同确立了抽象文本摘要作为解释复杂强化学习行为的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of interpreting complex reinforcement learning policies that are opaque to humans, this paper proposes SySLLM, a framework that reframes policy interpretation as a language-generation problem. The method converts agent trajectories into structured text and uses a large language model (LLM) to generate abstractive summaries of the agent&#x27;s goals and decision patterns, scaling without task-specific tuning. Experimental results from expert evaluations and a user study show strong alignment with human analysis, with 75.5% of participants preferring SySLLM summaries over demonstration-based explanations.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决强化学习策略因复杂奖励结构和神经表示而难以解释的问题。为此，作者提出了SySLLM框架，将策略解释重新定义为语言生成任务，其方法是将智能体的时空轨迹转化为结构化文本，并利用大语言模型生成描述其目标与决策模式的抽象摘要，无需针对具体任务进行微调。主要实验结果表明，专家评估显示其与人类分析高度一致，大规模用户研究中75.5%的参与者更偏好SySLLM的摘要而非基于演示的现有解释方法。</div>
</details>
</div>
<div class="card">
<div class="title">Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Chengyandan Shen, Christoffer Sloth</div>
<div class="meta-line">First: 2025-09-04T10:02:32+00:00 · Latest: 2026-01-08T10:57:57+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for Journal publication in Frontiers in Robotics and AI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04069v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04069v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes an exploration-efficient Deep Reinforcement Learning with Reference policy (DRLR) framework for learning robotics tasks that incorporates demonstrations. The DRLR framework is developed based on an algorithm called Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve IBRL by modifying the action selection module. The proposed action selection module provides a calibrated Q-value, which mitigates the bootstrapping error that otherwise leads to inefficient exploration. Furthermore, to prevent the RL policy from converging to a sub-optimal policy, SAC is used as the RL policy instead of TD3. The effectiveness of our method in mitigating bootstrapping error and preventing overfitting is empirically validated by learning two robotics tasks: bucket loading and open drawer, which require extensive interactions with the environment. Simulation results also demonstrate the robustness of the DRLR framework across tasks with both low and high state-action dimensions, and varying demonstration qualities. To evaluate the developed framework on a real-world industrial robotics task, the bucket loading task is deployed on a real wheel loader. The sim2real results validate the successful deployment of the DRLR framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用先验演示通过高效探索深度强化学习解决机器人任务</div>
<div class="mono" style="margin-top:8px">本文提出了一种融合演示的、基于参考策略的高效探索深度强化学习框架，用于学习机器人任务。该框架基于模仿引导强化学习算法开发，通过改进动作选择模块来优化原算法。改进后的动作选择模块提供校准后的Q值，从而减轻导致探索效率低下的引导误差。此外，为防止强化学习策略收敛至次优解，采用SAC替代TD3作为强化学习策略。通过两项需与环境大量交互的机器人任务验证了该方法在减轻引导误差和防止过拟合方面的有效性。仿真结果还表明，该框架在不同状态-动作维度及演示质量的任務中均表现出鲁棒性。为在实际工业机器人任务中评估该框架，将铲斗装载任务部署于真实轮式装载机，仿真到现实的迁移结果验证了框架的成功应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for more exploration-efficient deep reinforcement learning (DRL) in robotics, particularly when prior demonstrations are available. The method introduces the DRLR framework, building on Imitation Bootstrapped Reinforcement Learning (IBRL) but modifying the action selection module to provide calibrated Q-values, thereby reducing bootstrapping error and inefficient exploration; it also employs SAC instead of TD3 to avoid sub-optimal convergence. Experimental results on simulated robotics tasks like bucket loading and open drawer show the framework&#x27;s effectiveness in mitigating bootstrapping error and overfitting, with robustness across varying state-action dimensions and demonstration qualities, and sim2real deployment on a real wheel loader validates its practical applicability.</div>
<div class="mono" style="margin-top:8px">本文旨在解决机器人任务中深度强化学习探索效率低的问题，特别是在已有演示数据的情况下。方法上提出了DRLR框架，基于模仿引导强化学习（IBRL）改进，通过修改动作选择模块提供校准的Q值以减少引导误差和低效探索，并采用SAC替代TD3防止策略陷入次优解。实验在模拟的铲斗装载和开抽屉等机器人任务上进行，结果表明该方法有效缓解了引导误差和过拟合，对不同状态-动作维度和演示质量具有鲁棒性，并在真实轮式装载机上的仿真到现实部署验证了其实际应用效果。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-08T10:46:04+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗性输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但这些模型仍易受对抗性越狱攻击，攻击者通过精心设计的细微扰动绕过安全机制并触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等问题，难以应用于现实黑盒场景。为此，我们提出基于同步扰动随机逼近零阶优化（ZO-SPSA）的黑盒越狱攻击方法。ZO-SPSA具备三大优势：（1）通过输入输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低GPU内存占用的资源需求。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，同时保持与白盒方法相当的不可感知扰动。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱的现实可行性，并暴露了当前LVLMs安全机制的关键弱点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods that require full model access, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free approach approximates gradients through input-output interactions without needing internal model knowledge, making it model-agnostic and resource-efficient. Experimental results on models like InstructBLIP, LLaVA, and MiniGPT-4 show a high jailbreak success rate of up to 83.0% on InstructBLIP with imperceptible perturbations, and adversarial examples from MiniGPT-4 demonstrate strong transferability to other LVLMs, achieving an attack success rate of 64.18%, highlighting critical safety weaknesses in LVLMs.</div>
<div class="mono" style="margin-top:8px">本文的动机在于大型视觉语言模型（LVLMs）易受对抗性越狱攻击，而现有需要完整模型访问权限的白盒方法因计算成本高和迁移性不足而不实用。为此，研究提出了一种基于零阶优化和同步扰动随机逼近（ZO-SPSA）的黑盒攻击方法，该方法通过输入-输出交互进行梯度近似，无需模型内部知识，具有模型无关性和低资源消耗的优势。在InstructBLIP、LLaVA和MiniGPT-4等模型上的实验结果表明，该方法在InstructBLIP上实现了高达83.0%的越狱成功率，且扰动难以察觉；同时，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率可达64.18%，这揭示了当前LVLMs安全机制的重大缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning</div>
<div class="meta-line">Authors: Caijun Xu, Changyi Xiao, Zhongyuan Peng, Xinrun Wang, Yixin Cao</div>
<div class="meta-line">First: 2026-01-08T10:42:04+00:00 · Latest: 2026-01-08T10:42:04+00:00</div>
<div class="meta-line">Comments: 19 pages,5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04809v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model&#x27;s capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCALER：用于推理的合成可扩展自适应学习环境</div>
<div class="mono" style="margin-top:8px">强化学习为提升大语言模型的推理能力提供了原则性方法，但其有效性依赖于随模型演化保持信息量的训练信号。实践中，当任务难度与模型能力不匹配，或训练被少量重复问题模式主导时，强化学习进展常会放缓。为协同解决这些问题，我们提出SCALER（用于推理的合成可扩展自适应学习环境），该框架通过自适应环境设计维持有效的学习信号。SCALER引入可扩展的合成流程，将现实编程问题转化为具有可控难度和无限实例生成的、可验证的推理环境，使强化学习训练能突破有限数据集的限制，同时保持强正确性保证。在此基础上，SCALER进一步采用自适应多环境强化学习策略，动态调整实例难度并筛选活跃环境集合，以追踪模型能力边界并保持分布多样性。这种协同适应机制避免了奖励稀疏性，减轻了对狭窄任务模式的过拟合，并支持训练过程中的持续改进。大量实验表明，SCALER在多样化推理基准测试中始终优于基于数据集的强化学习基线，并展现出更稳定、更长周期的训练动态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge that reinforcement learning (RL) for improving large language model reasoning often suffers from diminishing training signals when task difficulty mismatches model capability or when training data lacks diversity. To address this, the authors propose SCALER, a framework that creates a synthetic, scalable learning environment by converting real-world programming problems into verifiable reasoning tasks with controllable difficulty and unlimited instance generation, ensuring correctness guarantees. The method further employs an adaptive multi-environment RL strategy that dynamically adjusts difficulty and curates environments to match the model&#x27;s evolving capability and maintain diversity. Experimental results demonstrate that SCALER consistently outperforms dataset-based RL baselines across various reasoning benchmarks, showing more stable and sustained training progress.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，使用强化学习提升大语言模型推理能力时，常因任务难度与模型能力不匹配或训练数据缺乏多样性而导致训练信号减弱。为解决这一问题，作者提出了SCALER框架，该框架通过将现实编程问题转化为可验证的推理任务，构建了一个可扩展的合成学习环境，其中任务难度可控且能无限生成实例，同时保证了正确性。方法上，SCALER采用了一种自适应多环境强化学习策略，动态调整难度并筛选环境，以匹配模型能力前沿并保持分布多样性。实验结果表明，SCALER在多种推理基准测试中持续优于基于数据集的强化学习基线，并展现出更稳定、更持久的训练动态。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning</div>
<div class="meta-line">Authors: Siyuan Gan, Jiaheng Liu, Boyan Wang, Tianpei Yang, Runqing Miao, Yuyao Zhang, Fanyu Meng, Junlan Feng, Linjian Meng, Jing Huo, Yang Gao</div>
<div class="meta-line">First: 2026-01-08T10:38:41+00:00 · Latest: 2026-01-08T10:38:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04805v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04805v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT&#x27;s responses, which are classified as not using thinking, remains below 10% across all tested datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于思考的非思考：通过强化学习解决混合推理模型训练中的奖励破解问题</div>
<div class="mono" style="margin-top:8px">大型推理模型因其卓越性能备受关注，但其性能主要源于长链思维过程，显著增加了计算开销。为应对过度思考问题，现有研究聚焦于使用强化学习训练混合推理模型，使其能根据查询复杂度自动决定是否启动思考。然而，强化学习易遭遇奖励破解问题，例如模型实际执行了思考却被判定为未执行，导致奖励分配错误。现有缓解方案或采用计算成本高昂的监督微调，或对非思考响应强制统一令牌限制，但效果有限。本文提出基于思考的非思考方法，该方法无需监督微调，而是通过利用思考型响应的解决方案信息，为不同查询的非思考响应设定差异化的最大令牌使用上限。在五个数学基准测试上的实验表明，相较于DeepSeek-R1-Distill-Qwen-1.5B/7B和DeepScaleR-1.5B，该方法能降低约50%的令牌使用量，同时显著提升准确率。事实上，该方法在所有测试方法中实现了准确率与效率的最优平衡。此外，在归类为非思考的响应中，奖励破解问题的发生率在所有测试数据集上均低于10%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the reward hacking problem in training hybrid reasoning models via reinforcement learning, where models incorrectly receive rewards for not thinking despite engaging in it, leading to inefficiency. The proposed method, Thinking-Based Non-Thinking (TNT), avoids supervised fine-tuning and instead dynamically sets maximum token limits for non-thinking responses based on information from thinking-based solutions, thereby mitigating reward hacking without high computational cost. Experimental results on five mathematical benchmarks show that TNT reduces token usage by approximately 50% compared to baseline models like DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy and maintaining reward hacking probability below 10% in non-thinking responses, achieving an optimal accuracy-efficiency trade-off.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习训练混合推理模型中的奖励欺骗问题进行研究，即模型在思考时被误判为未思考而获得错误奖励，导致效率低下。提出的方法“基于思考的非思考”（TNT）避免了监督微调，而是利用思考响应中的解决方案信息，为不同查询动态设置非思考响应的最大令牌限制，从而以较低计算成本缓解奖励欺骗。在五个数学基准测试上的实验结果表明，与DeepSeek-R1-Distill-Qwen-1.5B/7B和DeepScaleR-1.5B等基线模型相比，TNT将令牌使用量减少约50%，同时显著提高准确性，并将非思考响应中的奖励欺骗概率保持在10%以下，实现了准确性与效率的最优平衡。</div>
</details>
</div>
<div class="card">
<div class="title">AgentOCR: Reimagining Agent History via Optical Self-Compression</div>
<div class="meta-line">Authors: Lang Feng, Fuchao Yang, Feng Chen, Xin Cheng, Haiyang Xu, Zhenglin Wan, Ming Yan, Bo An</div>
<div class="meta-line">First: 2026-01-08T10:10:20+00:00 · Latest: 2026-01-08T10:10:20+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04786v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04786v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\% of text-based agent performance while substantially reducing token consumption (&gt;50\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentOCR：通过光学自压缩重构智能体历史记录</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的最新进展使得通过强化学习（RL）在多轮交互轨迹上训练智能体系统成为可能，但实际部署受限于快速增长的文本历史记录，这些记录会扩大令牌预算和内存使用。我们提出了AgentOCR框架，该框架通过将累积的观察-行动历史表示为紧凑的渲染图像，利用视觉令牌更高的信息密度。为实现多轮推演的可扩展性，AgentOCR提出了分段光学缓存机制。通过将历史分解为可哈希的片段并维护视觉缓存，该机制消除了冗余的重复渲染。除了固定渲染外，AgentOCR引入了智能体自压缩技术，智能体主动发出压缩率，并通过压缩感知奖励进行训练，以自适应地平衡任务成功率和令牌效率。我们在ALFWorld和基于搜索的问答等具有挑战性的智能体基准测试中进行了广泛实验。显著的结果表明，AgentOCR在保持基于文本的智能体性能超过95%的同时，大幅降低了令牌消耗（&gt;50%），实现了持续的令牌和内存效率。我们的进一步分析验证了分段光学缓存带来的20倍渲染加速效果，以及自压缩机制的有效策略平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind AgentOCR is to address the inefficiency of rapidly growing textual histories in LLM-based agentic systems, which inflate token budgets and memory usage. The method introduces a framework that compresses observation-action history into compact rendered images to exploit visual tokens&#x27; higher information density, employing segment optical caching to avoid redundant re-rendering and agentic self-compression where the agent learns to balance task success and token efficiency. Experimental results on ALFWorld and search-based QA benchmarks show that AgentOCR maintains over 95% of text-based agent performance while reducing token consumption by more than 50%, with segment optical caching achieving a 20x rendering speedup and self-compression effectively optimizing the trade-off.</div>
<div class="mono" style="margin-top:8px">AgentOCR的动机是解决基于大语言模型的智能体系统中文本历史快速增长导致的令牌预算和内存使用效率低下问题。该方法通过将观察-行动历史压缩为紧凑的渲染图像，以利用视觉令牌更高的信息密度，并采用分段光学缓存避免冗余重渲染，以及引入智能体自压缩机制，使智能体学习平衡任务成功与令牌效率。在ALFWorld和基于搜索的问答基准上的实验结果表明，AgentOCR保持了文本智能体95%以上的性能，同时将令牌消耗降低超过50%，分段光学缓存实现了20倍的渲染加速，自压缩机制有效优化了性能与效率的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search</div>
<div class="meta-line">Authors: Zefang Zong, Dingwei Chen, Yang Li, Qi Yi, Bo Zhou, Chengming Li, Bo Qian, Peng Chen, Jie Jiang</div>
<div class="meta-line">First: 2026-01-08T09:35:49+00:00 · Latest: 2026-01-08T09:35:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04767v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04767v1">PDF</a> · <a href="https://github.com/zzfoutofspace/ATPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AT²PO：基于树搜索的智能体回合制策略优化</div>
<div class="mono" style="margin-top:8px">大语言模型智能体已成为通过交替内部推理与外部工具交互来处理多回合任务的强大系统。智能体强化学习作为进一步精炼这些能力的关键后训练范式，近期引起了广泛研究关注。本文提出AT²PO（基于树搜索的智能体回合制策略优化），这是一个面向多回合智能体强化学习的统一框架，解决了三个核心挑战：探索多样性受限、稀疏信用分配以及策略优化失准。AT²PO引入了回合级树结构，同步实现熵引导树扩展以支持策略探索，以及回合级信用分配以实现稀疏结果下的细粒度奖励传播。在此基础上，我们提出智能体回合制策略优化——一种回合级学习目标，使策略更新与智能体交互的自然决策粒度对齐。该优化方法与树搜索正交，可轻松集成至任何多回合强化学习流程。在七个基准测试上的实验表明，其平均性能较现有最优基线提升最高达1.84个百分点，消融研究验证了各模块的有效性。代码已开源：https://github.com/zzfoutofspace/ATPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces AT²PO, a framework designed to enhance multi-turn LLM agents through agentic reinforcement learning, addressing challenges like limited exploration diversity, sparse credit assignment, and misaligned policy optimization. The method employs a turn-level tree structure that combines entropy-guided expansion for strategic exploration and turn-wise credit assignment for fine-grained reward propagation, alongside a turn-level policy optimization objective aligned with agentic decision granularity. Experimental results across seven benchmarks show consistent improvements over state-of-the-art baselines by up to 1.84 percentage points on average, with ablations confirming the efficacy of each component.</div>
<div class="mono" style="margin-top:8px">本文提出了AT²PO框架，旨在通过智能体强化学习提升多轮次大语言模型智能体的性能，解决探索多样性有限、信用分配稀疏和策略优化不对齐等核心挑战。该方法采用轮次级树结构，结合熵引导的树扩展进行策略探索和轮次级信用分配以实现细粒度奖励传播，并提出了与智能体决策粒度对齐的轮次级策略优化目标。在七个基准测试上的实验结果表明，该方法相比现有最优基线平均提升高达1.84个百分点，消融研究验证了各组成部分的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GAPO: Robust Advantage Estimation for Real-World Code LLMs</div>
<div class="meta-line">Authors: Jianqing Zhang, Zhezheng Hao, Wei Xia, Hande Dong, Hong Wang, Chenxing Wei, Yuyan Zhou, Yubin Qi, Qiang Lin, Jian Cao</div>
<div class="meta-line">First: 2025-10-22T03:37:49+00:00 · Latest: 2026-01-08T08:42:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21830v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.21830v4">PDF</a> · <a href="https://github.com/TsingZ0/verl-GAPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods, such as GRPO, are popular due to their critic-free and normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable noise, leading to distorted advantage computation and increased rollout outliers. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an interval with the highest SNR (Signal to Noise Ratio) per prompt and uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation to reduce noise further. This adaptive Q robustly handles rollout noise while remaining plug-and-play and efficient. We evaluate GAPO on nine instruction-tuned LLMs (3B-14B) using a collected large dataset of 51,844 real-world, history-aware code-editing tasks spanning 10 programming languages. GAPO yields up to 4.35 in-domain (ID) and 5.30 out-of-domain (OOD) exact-match improvements over GRPO and its variant DAPO, while achieving lower clipping ratios and higher GPU throughput. Code: https://github.com/TsingZ0/verl-GAPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAPO：面向现实世界代码大语言模型的鲁棒优势估计方法</div>
<div class="mono" style="margin-top:8px">强化学习（RL）被广泛用于代码编辑场景中大语言模型（LLM）的后训练，其中无评论家且归一化优势估计的组相对方法（如GRPO）备受青睐。然而，在实际代码编辑任务中，奖励分布常存在偏斜与不可预测噪声，导致优势计算失真并增加训练轨迹异常值。为解决此问题，我们提出组自适应策略优化（GAPO），该方法自适应地寻找每个提示中信噪比（SNR）最高的区间，并以该区间中位数作为自适应Q值替代组均值进行优势计算，从而进一步抑制噪声。这种自适应Q值在保持即插即用与高效性的同时，能鲁棒处理训练噪声。我们在包含10种编程语言、51,844个现实世界历史感知代码编辑任务的大规模数据集上，对九个指令微调LLM（3B-14B）进行评估。结果显示，GAPO相比GRPO及其变体DAPO在领域内（ID）和领域外（OOD）的精确匹配率分别提升最高达4.35和5.30，同时实现了更低的裁剪比率与更高的GPU吞吐量。代码：https://github.com/TsingZ0/verl-GAPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge that real-world code-editing tasks often involve skewed reward distributions and unpredictable noise, which can distort advantage estimation in group-relative reinforcement learning methods like GRPO. To address this, the authors propose GAPO, a method that adaptively identifies an interval with the highest signal-to-noise ratio per prompt and uses its median as an adaptive Q-value to replace the group mean, thereby reducing noise while remaining plug-and-play and efficient. Experimental results on a dataset of 51,844 code-editing tasks across 10 programming languages show that GAPO achieves up to 4.35 in-domain and 5.30 out-of-domain exact-match improvements over GRPO and its variant DAPO, along with lower clipping ratios and higher GPU throughput.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现实世界代码编辑任务中的奖励分布常存在偏斜和不可预测的噪声，这会扭曲GRPO等基于组别的强化学习方法的优势估计。为解决此问题，作者提出了GAPO方法，它自适应地为每个提示找到信噪比最高的区间，并使用该区间中位数作为自适应Q值来替代组均值，从而在保持即插即用和高效的同时降低噪声影响。在涵盖10种编程语言、包含51,844个真实世界代码编辑任务的数据集上实验表明，GAPO相比GRPO及其变体DAPO，在域内和域外精确匹配上分别提升最高达4.35和5.30，同时实现了更低的裁剪比率和更高的GPU吞吐量。</div>
</details>
</div>
<div class="card">
<div class="title">ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving</div>
<div class="meta-line">Authors: Chang Zhao, Zheming Yang, Yunqing Hu, Qi Guo, Zijian Wang, Pengcheng Li, Wen Ji</div>
<div class="meta-line">First: 2026-01-08T08:30:36+00:00 · Latest: 2026-01-08T08:30:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ThinkDrive：自动驾驶中思维链引导的渐进式强化学习微调框架</div>
<div class="mono" style="margin-top:8px">随着大语言模型（LLM）技术的快速发展，其在自动驾驶领域的应用日益广泛。然而，现有方法存在推理缺乏结构性、泛化能力差以及与人类驾驶意图不一致等问题。思维链（CoT）推理虽能提升决策透明度，但传统的监督微调（SFT）未能充分发挥其潜力，而强化学习（RL）方法则面临不稳定和推理深度不足的挑战。本文提出ThinkDrive，一种用于自动驾驶的CoT引导渐进式RL微调框架，将显式推理与难度感知自适应策略优化相结合。该方法采用两阶段训练策略：首先使用CoT解释进行SFT，然后应用渐进式RL，通过难度感知自适应策略优化器根据样本复杂度动态调整学习强度。我们在公开数据集上评估了该方法，结果显示ThinkDrive在考试、易考和准确率指标上分别优于强RL基线1.45%、1.95%和1.01%。此外，采用本方法训练的20亿参数模型在考试指标上超越了规模大得多的GPT-4o模型3.28%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing methods in autonomous driving, such as unstructured reasoning and poor generalization with large language models, this paper introduces ThinkDrive, a framework that integrates Chain-of-Thought reasoning with progressive reinforcement learning fine-tuning. The method employs a two-stage approach: first using supervised fine-tuning with CoT explanations, followed by progressive reinforcement learning with a difficulty-aware adaptive policy optimizer that adjusts learning based on sample complexity. Experimental results on a public dataset demonstrate that ThinkDrive outperforms strong RL baselines, achieving improvements of 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy metrics, respectively, and a 2B-parameter model trained with this method surpasses GPT-4o by 3.28% on the exam metric.</div>
<div class="mono" style="margin-top:8px">针对自动驾驶领域现有方法存在推理结构混乱、泛化能力差以及与人类驾驶意图不一致等问题，本文提出了ThinkDrive框架，将思维链推理与渐进式强化学习微调相结合。该方法采用两阶段训练策略：首先利用思维链解释进行监督微调，然后通过基于难度感知的自适应策略优化器进行渐进式强化学习，该优化器根据样本复杂度动态调整学习强度。在公开数据集上的实验结果表明，ThinkDrive在考试、简易考试和准确率指标上分别优于强强化学习基线1.45%、1.95%和1.01%，且使用该方法训练的20亿参数模型在考试指标上超越了GPT-4o达3.28%。</div>
</details>
</div>
<div class="card">
<div class="title">Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement</div>
<div class="meta-line">Authors: Mingyu Xu, Cheng Fang, Keyue Jiang, Yuqian Zheng, Yanghua Xiao, Baojian Zhou, Qifang Zhao, Suhang Zheng, Xiuwen Zhu, Jiyang Tang, Yongchi Zhao, Yijia Luo, Zhiqi Bai, Yuchi Xu, Wenbo Su, Wei Wang, Bing Zhao, Lin Qu, Xiaoxiao Xu</div>
<div class="meta-line">First: 2026-01-04T15:23:18+00:00 · Latest: 2026-01-08T08:13:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01562v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01562v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Logics-STEM：通过失败驱动的后训练与文档知识增强赋能大语言模型推理</div>
<div class="mono" style="margin-top:8px">我们提出Logics-STEM，这是一个基于Logics-STEM-SFT-Dataset微调的前沿推理模型。该数据集规模达1000万，质量高且多样性丰富，是最大规模的开源长链思维语料库之一。Logics-STEM专注于科学、技术、工程和数学（STEM）领域的推理任务，在STEM相关基准测试中表现卓越，在80亿参数规模上平均优于次优模型4.68%。我们将性能提升归因于数据-算法协同设计引擎，通过联合优化使模型拟合推理背后的黄金标准分布。数据层面，Logics-STEM-SFT-Dataset通过精心设计的五阶段数据策管引擎构建（包括标注、去重、净化、蒸馏和分层采样），确保质量、多样性和可扩展性。算法层面，我们的失败驱动后训练框架在监督微调阶段，针对模型失败区域进行定向知识检索与数据合成，有效指导第二阶段SFT或强化学习以更好拟合目标分布。Logics-STEM的卓越实证性能揭示了大规模开源数据与精心设计合成数据结合的巨大潜力，凸显了数据-算法协同设计通过后训练增强推理能力的关键作用。我们公开提供Logics-STEM模型（80亿和320亿参数）及Logics-STEM-SFT-Dataset（1000万和降采样220万版本），以支持开源社区的未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Logics-STEM, a reasoning model motivated by the need to enhance large language model (LLM) performance in STEM domains through improved data and training methods. The method involves a data-algorithm co-design engine: data-wise, it constructs a high-quality 10M-scale dataset via a five-stage curation process (annotation, deduplication, decontamination, distillation, and stratified sampling), while algorithm-wise, it employs a failure-driven post-training framework that uses targeted knowledge retrieval and data synthesis around model failures to guide further fine-tuning or reinforcement learning. Experimental results show that Logics-STEM achieves state-of-the-art performance on STEM benchmarks, with an average improvement of 4.68% over the next-best 8B-scale model, demonstrating the effectiveness of combining large-scale open-source data with synthetic data for reasoning enhancement.</div>
<div class="mono" style="margin-top:8px">本文提出了Logics-STEM模型，其动机是通过改进数据和训练方法来提升大语言模型在科学、技术、工程和数学领域的推理能力。方法上采用数据-算法协同设计引擎：数据方面，通过精心设计的五阶段数据整理流程构建了一个高质量、多样化的千万规模数据集；算法方面，采用失败驱动的后训练框架，利用针对模型失败区域的知识检索和数据合成来指导进一步的监督微调或强化学习。主要实验结果表明，Logics-STEM在STEM相关基准测试中表现优异，相比同规模最佳模型平均提升4.68%，验证了大规模开源数据与合成数据结合在增强推理能力方面的巨大潜力。</div>
</details>
</div>
<div class="card">
<div class="title">TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning</div>
<div class="meta-line">Authors: Yinuo Wang, Mining Tan, Wenxiang Jiao, Xiaoxi Li, Hao Wang, Xuanyu Zhang, Yuan Lu, Weiming Dong</div>
<div class="meta-line">First: 2026-01-08T08:08:35+00:00 · Latest: 2026-01-08T08:08:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04698v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs&#x27; set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TourPlanner：一种融合约束门控强化学习的竞争性共识旅行规划框架</div>
<div class="mono" style="margin-top:8px">旅行规划是一个复杂的决策过程，需要综合多维度信息以构建行程。然而，现有方法面临三大挑战：(1) 在保持高召回率的同时筛选候选兴趣点；(2) 单一路径推理限制了可行解空间的探索能力；(3) 硬约束与软约束的同步优化仍具难度。为此，我们提出TourPlanner框架，集成多路径推理与约束门控强化学习。具体而言，首先通过个性化召回与空间优化流程构建空间感知的候选兴趣点集；随后提出竞争性共识思维链多路径推理范式，增强可行解空间的探索能力；进一步引入基于Sigmoid的门控机制至强化学习阶段，实现硬约束满足后对软约束的动态优先级优化。在旅行规划基准测试中，TourPlanner在方案可行性与用户偏好契合度方面均显著超越现有方法，达到最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces TourPlanner, a framework designed to overcome key challenges in travel planning: efficiently pruning points of interest (POIs) while maintaining high recall, expanding exploration within the solution space beyond single reasoning paths, and jointly optimizing hard and soft constraints. The method employs a Personalized Recall and Spatial Optimization (PReSO) workflow to build a candidate POI set, a Competitive consensus Chain-of-Thought (CCoT) for multi-path reasoning to better explore feasible solutions, and a constraint-gated reinforcement learning stage with a sigmoid-based mechanism that prioritizes hard constraints before soft ones. Experiments on benchmarks show that TourPlanner achieves state-of-the-art performance, significantly outperforming existing methods in both feasibility and alignment with user preferences.</div>
<div class="mono" style="margin-top:8px">本文提出了TourPlanner框架，旨在解决旅行规划中的关键挑战：在保持高召回率的同时高效筛选兴趣点（POI）、超越单一路径限制以扩展解空间的探索能力，以及协同优化硬约束和软约束。该方法采用个性化召回与空间优化（PReSO）流程构建候选POI集，利用竞争共识思维链（CCoT）进行多路径推理以更好地探索可行解，并通过基于Sigmoid的门控机制将约束融入强化学习阶段，确保在满足硬约束后才动态优化软约束。在旅行规划基准测试上的实验结果表明，TourPlanner实现了最先进的性能，在可行性和用户偏好对齐方面显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models</div>
<div class="meta-line">Authors: Huayi Liu</div>
<div class="meta-line">First: 2026-01-08T08:06:58+00:00 · Latest: 2026-01-08T08:06:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04696v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大模型语义理解的数字化转型驱动机制构建方法</div>
<div class="mono" style="margin-top:8px">在数字化转型过程中，企业面临非结构化数据语义理解不足、驱动机制缺乏智能决策依据等问题。本研究提出一种结合大语言模型（LLM）与知识图谱的方法：首先利用微调BERT模型对多源异构文本进行实体识别与关系抽取，并采用GPT-4生成语义增强的向量表示；其次设计双层图神经网络（GNN）架构，将LLM输出的语义向量与业务元数据融合，构建动态可扩展的企业知识图谱；随后引入强化学习优化决策路径生成，通过奖励函数驱动机制迭代。在制造业案例中，该机制使设备故障场景响应时间从7.8小时降至3.7小时，F1值达94.3%，年度数字化转型成本中决策失误补偿降低45.3%。该方法通过融合大模型语义理解与结构化知识，显著提升了数字化转型驱动机制的智能化水平与执行效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by challenges in digital transformation, such as poor semantic understanding of unstructured data and a lack of intelligent decision support in driving mechanisms, this paper proposes a method integrating large language models (LLMs) and knowledge graphs. The method employs a fine-tuned BERT model for entity and relation extraction from multi-source texts, uses GPT-4 for semantic enhancement, designs a two-layer graph neural network (GNN) to fuse these semantic vectors with business metadata into a dynamic knowledge graph, and introduces reinforcement learning to optimize decision paths. Experimental results in a manufacturing case show the mechanism reduced equipment failure response time from 7.8 to 3.7 hours, achieved an F1 score of 94.3%, and decreased annual digital transformation cost penalties from decision errors by 45.3%, significantly enhancing the intelligence and efficiency of transformation drivers.</div>
<div class="mono" style="margin-top:8px">针对企业数字化转型中非结构化数据语义理解不足、驱动机制缺乏智能决策依据等问题，本研究提出了一种融合大语言模型与知识图谱的方法。该方法首先利用微调的BERT模型对多源异构文本进行实体识别和关系抽取，并采用GPT-4生成语义增强的向量表示；其次设计双层图神经网络架构，将大模型输出的语义向量与业务元数据融合，构建动态可扩展的企业知识图谱；随后引入强化学习优化决策路径生成，通过奖励函数驱动机制迭代。在制造业案例中，该机制将设备故障场景的响应时间从7.8小时缩短至3.7小时，F1值达到94.3%，年度数字化转型成本中决策错误导致的补偿支出降低了45.3%，有效提升了转型驱动机制的智能化水平和执行效率。</div>
</details>
</div>
<div class="card">
<div class="title">Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning</div>
<div class="meta-line">Authors: Enze Pan</div>
<div class="meta-line">First: 2026-01-08T08:05:42+00:00 · Latest: 2026-01-08T08:05:42+00:00</div>
<div class="meta-line">Comments: 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04695v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04695v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what &quot;uncertainty reduction&quot; objectives can and cannot guarantee under rule shifts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Tape：用于评估强化学习中规则迁移泛化能力的元胞自动机基准</div>
<div class="mono" style="margin-top:8px">本文提出Tape——一个受控的强化学习基准，旨在分离潜在规则迁移下的分布外（OOD）失效问题。Tape基于一维元胞自动机构建，通过固定观测与动作空间而改变转移规则，实现精确的训练/测试划分。借助可复现的评估流程，我们比较了无模型基线方法、基于学习世界模型的规划方法以及任务推断（元强化学习）方法。结果显示一致规律：在分布内（ID）表现优异的方法可能在未见过规则的OOD场景中失效，且高方差的OOD评估会导致排名不稳定，除非实验进行充分重复。我们提供：（1）标准化OOD评估协议；（2）统计报告要求（随机种子、置信区间与假设检验）；（3）连接熵缩减与条件互信息、期望后验KL散度的信息论恒等式，阐明规则迁移下“不确定性缩减”目标的可保证与不可保证范畴。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Tape, a reinforcement learning benchmark based on one-dimensional cellular automata, motivated by the need to systematically evaluate out-of-distribution (OOD) generalization under latent rule shifts while keeping observation and action spaces fixed. The method involves a controlled experimental setup with precise train/test splits, comparing model-free, model-based planning, and meta-RL approaches through a reproducible pipeline. Key experimental results reveal that in-distribution performance does not guarantee OOD robustness, with many strong methods collapsing under heldout rules, and highlight the importance of statistical rigor—including multiple seeds and confidence intervals—for stable evaluation, alongside information-theoretic insights into uncertainty reduction objectives.</div>
<div class="mono" style="margin-top:8px">本文提出了Tape，一个基于一维元胞自动机的强化学习基准，其动机在于需要系统评估潜在规则变化下的分布外泛化能力，同时保持观察和动作空间不变。方法采用受控实验设置，通过精确的训练/测试分割，利用可复现的流程比较了无模型、基于模型的规划和元强化学习方法。主要实验结果表明，分布内性能不能保证分布外鲁棒性，许多强方法在未见规则下失效，并强调了统计严谨性（如多次随机种子和置信区间）对稳定评估的重要性，同时提供了关于不确定性减少目标的信息论见解。</div>
</details>
</div>
<div class="card">
<div class="title">ResMAS: Resilience Optimization in LLM-based Multi-agent Systems</div>
<div class="meta-line">Authors: Zhilun Zhou, Zihan Liu, Jiahe Liu, Qingyu Shao, Yihan Wang, Kun Shao, Depeng Jin, Fengli Xu</div>
<div class="meta-line">First: 2026-01-08T08:03:37+00:00 · Latest: 2026-01-08T08:03:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04694v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04694v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS&#x27;s resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent&#x27;s prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResMAS：基于大语言模型的多智能体系统韧性优化</div>
<div class="mono" style="margin-top:8px">基于大语言模型的多智能体系统（LLM-based MAS）通过多个LLM智能体协作解决复杂任务，已在众多领域展现出卓越性能。然而，MAS通常部署于不同设备或环境中，易受智能体故障等扰动影响。现有研究虽关注对抗性攻击及相应防御策略，但主要侧重于攻击发生后的被动检测与缓解，而非主动设计具备内在韧性的系统。本文研究了扰动下LLM-based MAS的韧性，发现通信拓扑与提示设计均显著影响系统韧性。基于此，我们提出ResMAS：一个两阶段增强MAS韧性的框架。首先，训练奖励模型预测MAS韧性，并以此训练拓扑生成器，通过强化学习为特定任务自动设计韧性拓扑。其次，引入拓扑感知的提示优化方法，根据各智能体的连接与交互关系优化其提示。多任务实验表明，该方法在各种约束下显著提升了MAS韧性。此外，该框架对新任务与模型展现出强大的泛化能力，凸显了构建韧性MAS的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of Large Language Model-based Multi-Agent Systems (LLM-based MAS) to perturbations like agent failures, noting that prior work focuses on reactive defenses rather than proactive resilience design. The authors propose ResMAS, a two-stage framework that first trains a reward model to predict resilience and uses reinforcement learning to generate optimal communication topologies, then employs a topology-aware method to optimize individual agent prompts based on their interactions. Experimental results across various tasks demonstrate that ResMAS significantly enhances system resilience under constraints and shows strong generalization to new tasks and models.</div>
<div class="mono" style="margin-top:8px">本文针对基于大语言模型的多智能体系统（LLM-based MAS）在智能体故障等扰动下的脆弱性问题展开研究，指出现有工作主要关注被动防御而非主动设计固有韧性系统。作者提出了ResMAS框架，该框架包含两个阶段：首先训练一个奖励模型来预测系统韧性，并通过强化学习自动生成针对特定任务的韧性拓扑结构；其次引入一种拓扑感知的提示优化方法，根据智能体的连接和交互优化其提示。在多种任务上的大量实验表明，该方法在各种约束下显著提升了多智能体系统的韧性，并且对新任务和模型展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead</div>
<div class="meta-line">Authors: Oluwatosin Oseni, Shengjie Wang, Jun Zhu, Micah Corah</div>
<div class="meta-line">Venue: RSS</div>
<div class="meta-line">First: 2026-01-08T07:55:07+00:00 · Latest: 2026-01-08T07:55:07+00:00</div>
<div class="meta-line">Comments: RSS&#x27;25: Multi-Objective Optimization and Planning in Robotics Workshop: 5 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04686v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04686v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>噩梦编织者：预测不安全状态与前瞻性规划</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在现实应用中取得了显著成功，尤其在机器人控制领域。然而，由于安全保证不足，强化学习的应用仍受限制。我们提出&#x27;噩梦编织者&#x27;——一种基于模型的安全强化学习算法，通过利用学习得到的世界模型预测潜在安全违规并相应规划行动，以解决安全问题。该算法在最大化奖励的同时实现了近乎零安全违规。在仅使用图像观测的Safety Gymnasium任务中，&#x27;噩梦编织者&#x27;优于无模型基线方法，效率提升近20倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the insufficient safety guarantees that limit the adoption of Reinforcement Learning (RL) in real-world applications like robotics. The method introduces Nightmare Dreamer, a model-based Safe RL algorithm that uses a learned world model to predict and plan around potential safety violations. The main experimental results show that it achieves nearly zero safety violations while maximizing rewards, outperforming model-free baselines on Safety Gymnasium tasks with image observations by nearly 20x in efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决强化学习在机器人等现实应用中因安全性不足而受限的问题。方法上提出了Nightmare Dreamer，这是一种基于模型的安全强化学习算法，通过利用学习的世界模型来预测潜在的安全违规并相应规划行动。主要实验结果表明，该算法在最大化奖励的同时实现了近乎零的安全违规，在仅使用图像观测的Safety Gymnasium任务上，其效率比无模型基线提高了近20倍。</div>
</details>
</div>
<div class="card">
<div class="title">IndexTTS 2.5 Technical Report</div>
<div class="meta-line">Authors: Yunpei Li, Xun Zhou, Jinchao Wang, Lu Wang, Yong Wu, Siyi Zhou, Yiquan Zhou, Jingchen Shu</div>
<div class="meta-line">First: 2026-01-07T12:58:16+00:00 · Latest: 2026-01-08T07:46:09+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03888v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03888v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IndexTTS 2.5 技术报告</div>
<div class="mono" style="margin-top:8px">在先前工作中，我们提出了 IndexTTS 2——一个零样本神经文本转语音基础模型，包含两个核心组件：基于 Transformer 的文本到语义（T2S）模块和非自回归的语义到梅尔频谱（S2M）模块，二者共同实现了忠实的情感复现，并建立了首个自回归时长可控的生成范式。在此基础上，我们推出 IndexTTS 2.5，通过四项关键改进显著提升了多语言覆盖范围、推理速度和整体合成质量：1）语义编解码器压缩：将语义编解码器帧率从 50 Hz 降至 25 Hz，序列长度减半，大幅降低训练和推理成本；2）架构升级：将 S2M 模块基于 U-DiT 的主干网络替换为更高效的基于 Zipformer 的建模架构，实现显著参数减少和更快的梅尔频谱生成；3）多语言扩展：提出三种显式跨语言建模策略——边界感知对齐、词元级拼接和指令引导生成，为零样本多语言情感 TTS 建立了实用设计原则，支持中文、英文、日文和西班牙文，即使没有目标语言情感训练数据也能实现鲁棒的情感迁移；4）强化学习优化：在 T2S 模块的后训练中应用 GRPO，提升了发音准确性和自然度。实验表明，IndexTTS 2.5 不仅支持更广泛的语言覆盖，还能在相同零样本设置下复现未见语言的情感韵律。IndexTTS 2.5 在保持与 IndexTTS 2 相当的词错误率和说话人相似度的同时，实现了 2.28 倍的实时因子提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance the multilingual capability, speed, and quality of zero-shot emotional text-to-speech synthesis, this work introduces IndexTTS 2.5 as an upgrade to its predecessor. The method incorporates four key improvements: compressing the semantic codec frame rate to reduce sequence length, upgrading the S2M module architecture to a more efficient Zipformer, implementing explicit cross-lingual modeling strategies for multilingual support, and applying reinforcement learning to optimize the T2S module. Experimental results demonstrate that IndexTTS 2.5 achieves a 2.28x improvement in real-time factor while maintaining word error rate and speaker similarity, and it successfully replicates emotional prosody in unseen languages under a zero-shot setting.</div>
<div class="mono" style="margin-top:8px">本工作的动机是提升零样本情感文本到语音合成模型的多语言覆盖、推理速度与整体质量，为此在先前版本基础上提出了IndexTTS 2.5。其方法包含四项关键改进：通过压缩语义编解码器帧率以减半序列长度，将S2M模块主干升级为更高效的Zipformer架构，采用边界感知对齐等显式跨语言建模策略支持多语言，并利用强化学习对T2S模块进行后训练优化。实验结果表明，IndexTTS 2.5在保持词错误率和说话人相似度的同时，实现了2.28倍的实时因子提升，并能在零样本设置下对未见语言成功复制情感韵律。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Dynamics in RL Post-Training for Language Models</div>
<div class="meta-line">Authors: Akiyoshi Tomihari</div>
<div class="meta-line">First: 2026-01-08T07:32:15+00:00 · Latest: 2026-01-08T07:32:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04670v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04670v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型强化学习后训练中的学习动态研究</div>
<div class="mono" style="margin-top:8px">强化学习（RL）后训练是现代语言模型开发的关键阶段，对提升模型对齐性和推理能力至关重要。然而，包括输出多样性下降在内的若干现象仍缺乏深入理解。为更全面认识RL后训练，我们从监督学习中已有研究但RL领域尚未充分探索的视角，分析其学习动态。采用经验性神经正切核（NTK）框架，将NTK分解为两个分量以刻画RL更新在训练样本间的传播机制。分析表明，特征表示的低变异性会导致RL更新系统性地增强模型置信度，这解释了RL后训练后输出多样性普遍下降的现象。进一步研究发现，该机制下的有效学习依赖于快速塑造分类器，直接影响NTK的梯度分量。基于这些发现，我们提出分类器优先强化学习（CF-RL），一种先优化分类器再执行标准RL训练的两阶段策略。实验结果验证了理论分析：CF-RL能提升模型置信度并加速优化。补充分析表明CF-RL的机制不同于监督学习中的线性探测再微调方法。本研究系统阐述了RL后训练的学习动态，为后续分析与改进提供了理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the learning dynamics of reinforcement learning (RL) post-training for language models, motivated by the need to better understand phenomena like reduced output diversity. The method employs an empirical neural tangent kernel (NTK) framework to analyze how RL updates propagate, revealing that limited feature variability leads to systematic increases in model confidence. Based on this, the authors propose classifier-first reinforcement learning (CF-RL), a two-stage strategy that prioritizes classifier updates. Experimental results confirm that CF-RL accelerates optimization and increases model confidence, with analysis showing its mechanism differs from related supervised learning approaches.</div>
<div class="mono" style="margin-top:8px">本文研究了语言模型强化学习后训练的学习动态，旨在深入理解输出多样性降低等现象。方法上采用经验神经正切核框架分析强化学习更新的传播，发现特征表示变异性有限会导致模型置信度系统性增加。基于此，作者提出了分类器优先的强化学习方法，这是一种优先更新分类器的两阶段训练策略。实验结果验证了理论分析，表明该方法能加速优化并提高模型置信度，且其机制与监督学习中的线性探测再微调方法不同。</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture</div>
<div class="meta-line">Authors: Laukik Patade, Rohan Rane, Sandeep Pillai</div>
<div class="meta-line">First: 2026-01-08T07:28:11+00:00 · Latest: 2026-01-08T07:28:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04668v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04668v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra&#x27;s algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的精准农业无人地面车辆路径规划优化</div>
<div class="mono" style="margin-top:8px">本研究聚焦于利用连续动作空间中的深度强化学习技术，优化精准农业场景下无人地面车辆的路径规划。研究首先回顾了传统基于网格的方法（如A*和Dijkstra算法），并探讨其在动态农业环境中的局限性，强调自适应学习策略的必要性。随后系统研究了深度强化学习方法，其中深度Q网络在二维仿真中展现出更强的适应性与性能提升；通过评估双Q网络和竞争网络等改进架构以优化决策能力。在此基础上，研究重点转向连续动作空间模型——深度确定性策略梯度及其改进算法双延迟深度确定性策略梯度，并在逐步复杂化的环境中进行验证。基于ROS和Gazebo的三维环境实验表明，连续动作空间深度强化学习算法能有效应对动态农业场景。经预训练的双延迟深度确定性策略梯度智能体在动态环境中达到95%的成功率，验证了该方法在保障农作物与机器人安全的同时处理移动障碍物的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional grid-based path planning algorithms like A* and Dijkstra in dynamic agricultural settings, this study employs deep reinforcement learning (DRL) to optimize path planning for unmanned ground vehicles (UGVs) in precision agriculture. The method progresses from evaluating discrete-action DRL approaches, such as Deep Q-Networks with enhancements, to implementing continuous-action models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), in simulations of increasing complexity. The main experimental results, validated in a 3D ROS/Gazebo environment, show that the pretrained TD3 agent achieves a 95% success rate in navigating dynamic scenarios with moving obstacles, demonstrating robust and safe performance for agricultural operations.</div>
<div class="mono" style="margin-top:8px">本研究针对传统基于网格的路径规划算法（如A*和Dijkstra）在动态农业环境中的局限性，采用深度强化学习（DRL）来优化精准农业中无人地面车辆（UGV）的路径规划。方法上，从评估离散动作的DRL方法（如改进的深度Q网络）推进到在更复杂模拟中实施连续动作模型，特别是深度确定性策略梯度（DDPG）及其改进算法TD3。主要实验结果在三维ROS/Gazebo环境中验证，表明预训练的TD3智能体在动态场景中导航移动障碍物时成功率高达95%，证明了该方法在确保农作物和机器人安全方面的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">FAQNAS: FLOPs-aware Hybrid Quantum Neural Architecture Search using Genetic Algorithm</div>
<div class="meta-line">Authors: Muhammad Kashif, Shaf Khalid, Alberto Marchisio, Nouhaila Innan, Muhammad Shafique</div>
<div class="meta-line">First: 2025-11-13T08:04:17+00:00 · Latest: 2026-01-08T07:16:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10062v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10062v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid Quantum Neural Networks (HQNNs), which combine parameterized quantum circuits with classical neural layers, are emerging as promising models in the noisy intermediate-scale quantum (NISQ) era. While quantum circuits are not naturally measured in floating point operations (FLOPs), most HQNNs (in NISQ era) are still trained on classical simulators where FLOPs directly dictate runtime and scalability. Hence, FLOPs represent a practical and viable metric to measure the computational complexity of HQNNs. In this work, we introduce FAQNAS, a FLOPs-aware neural architecture search (NAS) framework that formulates HQNN design as a multi-objective optimization problem balancing accuracy and FLOPs. Unlike traditional approaches, FAQNAS explicitly incorporates FLOPs into the optimization objective, enabling the discovery of architectures that achieve strong performance while minimizing computational cost. Experiments on five benchmark datasets (MNIST, Digits, Wine, Breast Cancer, and Iris) show that quantum FLOPs dominate accuracy improvements, while classical FLOPs remain largely fixed. Pareto-optimal solutions reveal that competitive accuracy can often be achieved with significantly reduced computational cost compared to FLOPs-agnostic baselines. Our results establish FLOPs-awareness as a practical criterion for HQNN design in the NISQ era and as a scalable principle for future HQNN systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FAQNAS：基于遗传算法的FLOPs感知混合量子神经架构搜索</div>
<div class="mono" style="margin-top:8px">混合量子神经网络（HQNNs）将参数化量子电路与经典神经层相结合，在噪声中等规模量子（NISQ）时代展现出广阔前景。虽然量子电路本身不直接以浮点运算（FLOPs）衡量，但当前NISQ时代的大多数HQNN仍通过经典模拟器训练，其运行时间和可扩展性直接受FLOPs影响。因此，FLOPs可作为衡量HQNN计算复杂度的实用有效指标。本研究提出FAQNAS框架，将HQNN设计构建为平衡精度与FLOPs的多目标优化问题。与传统方法不同，FAQNAS将FLOPs显式纳入优化目标，从而在控制计算成本的同时获得高性能架构。在五个基准数据集（MNIST、Digits、Wine、Breast Cancer和Iris）上的实验表明：量子FLOPs主导精度提升，而经典FLOPs基本保持稳定。帕累托最优解显示，相较于忽略FLOPs的基线方法，本方案能以显著降低的计算成本实现具有竞争力的精度。本研究确立了FLOPs感知作为NISQ时代HQNN设计的实用准则，并为未来HQNN系统提供了可扩展的设计原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind FAQNAS is to address the computational complexity of Hybrid Quantum Neural Networks (HQNNs) in the NISQ era, where FLOPs on classical simulators dictate runtime and scalability, making FLOPs a practical metric for HQNN design. The method introduces a FLOPs-aware neural architecture search framework that treats HQNN design as a multi-objective optimization problem, balancing accuracy and FLOPs by explicitly incorporating FLOPs into the optimization objective to discover efficient architectures. Experimental results on five benchmark datasets demonstrate that quantum FLOPs are key to accuracy improvements while classical FLOPs remain stable, with Pareto-optimal solutions achieving competitive accuracy at significantly reduced computational cost compared to FLOPs-agnostic baselines, establishing FLOPs-awareness as a practical criterion for HQNN design.</div>
<div class="mono" style="margin-top:8px">FAQNAS的动机在于解决嘈杂中等规模量子（NISQ）时代混合量子神经网络（HQNN）的计算复杂度问题，其中经典模拟器上的浮点运算（FLOPs）决定了运行时间和可扩展性，因此FLOPs成为HQNN设计的实用指标。该方法提出了一种FLOPs感知的神经架构搜索框架，将HQNN设计视为多目标优化问题，通过将FLOPs明确纳入优化目标来平衡准确性和FLOPs，从而发现高效架构。在五个基准数据集上的实验结果表明，量子FLOPs主导了准确性提升，而经典FLOPs基本保持稳定，帕累托最优解在显著降低计算成本的同时实现了与忽略FLOPs的基线方法相竞争的准确性，确立了FLOPs感知作为NISQ时代HQNN设计的实用准则。</div>
</details>
</div>
<div class="card">
<div class="title">InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training</div>
<div class="meta-line">Authors: Ziyun Zhang, Zezhou Wang, Xiaoyi Zhang, Zongyu Guo, Jiahao Li, Bin Li, Yan Lu</div>
<div class="meta-line">First: 2026-01-07T17:40:08+00:00 · Latest: 2026-01-08T06:37:47+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04126v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04126v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiniteWeb：面向GUI智能体训练的可扩展网页环境合成系统</div>
<div class="mono" style="margin-top:8px">代表用户与图形界面交互的GUI智能体是实用AI助手的重要发展方向，但其训练受限于合适环境的稀缺性。本文提出InfiniteWeb系统，能够大规模自动生成用于GUI智能体训练的功能性网页环境。虽然大语言模型在生成单个网页方面表现良好，但构建具有多页面互连的真实功能性网站仍面临挑战。我们通过统一规范、以任务为中心的测试驱动开发、结合网站种子与参考设计图像确保多样性等方法应对这些挑战。本系统还能生成可验证的任务评估器，为强化学习提供密集奖励信号。实验表明，InfiniteWeb在真实网站构建方面超越商业编程智能体，基于生成环境训练的GUI智能体在OSWorld和Online-Mind2Web基准上取得显著性能提升，验证了所提系统的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for InfiniteWeb stems from the scarcity of suitable training environments for GUI agents that interact with graphical user interfaces. The method introduces a system that automatically synthesizes functional web environments at scale by addressing challenges in generating multi-page websites through unified specification, task-centric test-driven development, and combining website seeds with reference design images to ensure diversity, while also generating verifiable task evaluators for dense reward signals. Main experimental results show that InfiniteWeb outperforms commercial coding agents in realistic website construction, and GUI agents trained on its generated environments achieve significant performance improvements on benchmarks like OSWorld and Online-Mind2Web, demonstrating the system&#x27;s effectiveness.</div>
<div class="mono" style="margin-top:8px">InfiniteWeb的动机源于GUI代理训练中缺乏合适的交互环境。该方法提出了一个系统，通过统一规范、以任务为中心的测试驱动开发、结合网站种子与参考设计图像以确保多样性，来自动大规模合成功能性网页环境，并生成可验证的任务评估器以提供密集奖励信号。主要实验结果表明，InfiniteWeb在构建真实网站方面超越了商业编码代理，且在其生成环境中训练的GUI代理在OSWorld和Online-Mind2Web等基准测试上取得了显著性能提升，证明了该系统的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary Generative Optimization: Towards Fully Data-Driven Evolutionary Optimization via Generative Learning</div>
<div class="meta-line">Authors: Tao Jiang, Kebin Sun, Zhenyu Liang, Ran Cheng, Yaochu Jin, Kay Chen Tan</div>
<div class="meta-line">First: 2025-08-01T07:17:57+00:00 · Latest: 2026-01-08T06:08:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.00380v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.00380v2">PDF</a> · <a href="https://github.com/EMI-Group/evogo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in data-driven evolutionary algorithms (EAs) have demonstrated the potential of leveraging historical data to improve optimization accuracy and adaptability. Despite these advancements, existing methods remain reliant on handcrafted process-level operators. In contrast, Evolutionary Generative Optimization (EvoGO) is a fully data-driven framework designed from the objective level, enabling autonomous learning of the entire search process. EvoGO streamlines the evolutionary optimization process into three stages: data preparation, model training, and population generation. The data preparation stage constructs a pairwise dataset to enrich training diversity without incurring additional evaluation costs. During model training, a tailored generative model learns to transform inferior solutions into superior ones. In the population generation stage, EvoGO replaces traditional reproduction operators with a scalable and parallelizable generative mechanism. Extensive experiments on numerical benchmarks, classical control problems, and high-dimensional robotic tasks demonstrate that EvoGO consistently converges within merely 10 generations and substantially outperforms a wide spectrum of optimization approaches, including traditional EAs, Bayesian optimization, and reinforcement learning based methods. Code is available at: https://github.com/EMI-Group/evogo</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化生成优化：通过生成学习实现完全数据驱动的进化优化</div>
<div class="mono" style="margin-top:8px">数据驱动进化算法的最新进展展示了利用历史数据提升优化精度与适应性的潜力。然而，现有方法仍依赖人工设计的流程级算子。相比之下，进化生成优化（EvoGO）是一个从目标层面设计的完全数据驱动框架，能够自主学习整个搜索过程。EvoGO将进化优化流程简化为三个阶段：数据准备、模型训练和种群生成。数据准备阶段构建配对数据集以增强训练多样性，且无需额外评估成本；模型训练阶段通过定制生成模型学习将劣质解转化为优质解；种群生成阶段则用可扩展、可并行的生成机制替代传统繁殖算子。在数值基准测试、经典控制问题和高维机器人任务上的大量实验表明，EvoGO仅需10代即可稳定收敛，且显著优于传统进化算法、贝叶斯优化及基于强化学习的方法。代码发布于：https://github.com/EMI-Group/evogo</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing data-driven evolutionary algorithms that still rely on handcrafted operators, this paper introduces Evolutionary Generative Optimization (EvoGO), a fully data-driven framework designed to autonomously learn the entire search process from the objective level. The method streamlines optimization into three stages: data preparation, which builds a pairwise dataset to enhance training diversity without extra evaluations; model training, where a tailored generative model learns to transform inferior solutions into superior ones; and population generation, which replaces traditional operators with a scalable generative mechanism. Experimental results on numerical benchmarks, control problems, and high-dimensional robotic tasks show that EvoGO consistently converges within just 10 generations and significantly outperforms a wide range of methods, including traditional evolutionary algorithms, Bayesian optimization, and reinforcement learning approaches.</div>
<div class="mono" style="margin-top:8px">针对现有数据驱动进化算法仍依赖人工设计算子的局限性，本文提出了进化生成优化（EvoGO），这是一个完全数据驱动的框架，旨在从目标层面自主学习整个搜索过程。该方法将优化流程简化为三个阶段：数据准备阶段构建配对数据集以增强训练多样性而无需额外评估；模型训练阶段通过定制的生成模型学习将劣质解转化为优质解；种群生成阶段则用可扩展的生成机制替代传统繁殖算子。在数值基准测试、经典控制问题和高维机器人任务上的实验结果表明，EvoGO仅需10代即可稳定收敛，并显著优于传统进化算法、贝叶斯优化和基于强化学习的方法等多种优化方法。</div>
</details>
</div>
<div class="card">
<div class="title">MiMo-V2-Flash Technical Report</div>
<div class="meta-line">Authors: Xiaomi LLM-Core Team, :, Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, Gang Xie, Hailin Zhang, Hanglong Lv, Hanyu Li, Heyu Chen, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Lei Li, Liang Zhao, Linghao Zhang, Peidian Li, Qianli Chen, Shaohui Liu, Shihua Yu, Shijie Cao, Shimao Chen, Shouqiu Yu, Shuo Liu, Tianling Zhou, Weijiang Su, Weikun Wang, Wenhan Ma, Xiangwei Deng, Bohan Mao, Bowen Ye, Can Cai, Chenghua Wang, Chengxuan Zhu, Chong Ma, Chun Chen, Chunan Li, Dawei Zhu, Deshan Xiao, Dong Zhang, Duo Zhang, Fangyue Liu, Feiyu Yang, Fengyuan Shi, Guoan Wang, Hao Tian, Hao Wu, Heng Qu, Hongfei Yi, Hongxu An, Hongyi Guan, Xing Zhang, Yifan Song, Yihan Yan, Yihao Zhao, Yingchun Lai, Yizhao Gao, Yu Cheng, Yuanyuan Tian, Yudong Wang, Zhen Tang, Zhengju Tang, Zhengtao Wen, Zhichao Song, Zhixian Zheng, Zihan Jiang, Jian Wen, Jiarui Sun, Jiawei Li, Jinlong Xue, Jun Xia, Kai Fang, Menghang Zhu, Nuo Chen, Qian Tu, Qihao Zhang, Qiying Wang, Rang Li, Rui Ma, Shaolei Zhang, Shengfan Wang, Shicheng Li, Shuhao Gu, Shuhuai Ren, Sirui Deng, Tao Guo, Tianyang Lu, Weiji Zhuang, Weikang Zhang, Weimin Xiong, Wenshan Huang, Wenyu Yang, Xin Zhang, Xing Yong, Xu Wang, Xueyang Xie, Yilin Jiang, Yixin Yang, Yongzhe He, Yu Tu, Yuanliang Dong, Yuchen Liu, Yue Ma, Yue Yu, Yuxing Xiang, Zhaojun Huang, Zhenru Lin, Zhipeng Xu, Zhiyang Chen, Zhonghua Deng, Zihan Zhang, Zihao Yue</div>
<div class="meta-line">First: 2026-01-06T07:31:47+00:00 · Latest: 2026-01-08T05:52:17+00:00</div>
<div class="meta-line">Comments: 31 pages, technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02780v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02780v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiMo-V2-Flash技术报告</div>
<div class="mono" style="margin-top:8px">我们提出MiMo-V2-Flash，这是一个总参数量为3090亿、激活参数量为150亿的专家混合模型，专为快速、强大的推理与智能体能力而设计。该模型采用混合注意力架构，以5:1的混合比例交错使用滑动窗口注意力与全局注意力，滑动窗口大小为128个词元。模型通过多词元预测在27万亿词元上进行预训练，原生支持32k上下文长度，并后续扩展至256k。为高效扩展训练后计算，MiMo-V2-Flash引入了新颖的多教师同策略蒸馏范式。在此框架中，领域专业化教师模型（例如通过大规模强化学习训练）提供密集的词元级奖励，使学生模型能完美掌握教师专长。MiMo-V2-Flash在总参数量分别仅为DeepSeek-V3.2和Kimi-K2的1/2和1/3的情况下，仍可与这些顶尖开源模型媲美。在推理阶段，通过将多词元预测复用为推测解码的草稿模型，配合三层多词元预测结构，模型最高可实现3.6的平均接受长度和2.6倍的解码加速。我们开源了模型权重及三层多词元预测权重，以促进开放研究与社区协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient yet powerful large language models, this paper introduces MiMo-V2-Flash, a 309B parameter Mixture-of-Experts model with only 15B active parameters, designed for fast reasoning and agentic tasks. The method employs a hybrid attention architecture combining Sliding Window and global attention, pre-trains on 27T tokens with Multi-Token Prediction (MTP), and uses a novel Multi-Teacher On-Policy Distillation (MOPD) for efficient post-training scaling. Key experimental results show the model rivals top open-weight models like DeepSeek-V3.2 with far fewer parameters, and by repurposing MTP for speculative decoding, it achieves up to a 2.6x decoding speedup and a 3.6 average acceptance length.</div>
<div class="mono" style="margin-top:8px">本文旨在开发高效且强大的大语言模型，提出了MiMo-V2-Flash，这是一个总参数量为3090亿、激活参数量为150亿的混合专家模型，专为快速推理和智能体任务设计。其方法采用滑动窗口与全局注意力混合的架构，基于27000亿token和多令牌预测进行预训练，并引入新颖的多教师策略蒸馏范式进行高效的训练后扩展。主要实验结果表明，该模型在总参数量仅为同类顶尖开源模型一半或三分之一的情况下，性能可与之媲美；通过将多令牌预测机制用作推测解码的草稿模型，实现了最高2.6倍的解码加速和3.6的平均接受长度。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</div>
<div class="meta-line">Authors: Jianfeng Si, Lin Sun, Zhewen Tan, Xiangzheng Zhang</div>
<div class="meta-line">First: 2025-08-12T02:39:33+00:00 · Latest: 2026-01-08T05:34:47+00:00</div>
<div class="meta-line">Comments: 15 pages,3 figures,5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14904v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.14904v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model&#x27;s safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于魔法令牌引导协同训练的大语言模型高效可切换安全控制</div>
<div class="mono" style="margin-top:8px">当前大语言模型（LLMs）的内容安全方法，如监督微调（SFT）和基于人类反馈的强化学习（RLHF），通常依赖多阶段训练流程，且缺乏细粒度的部署后可控性。为解决这些局限，我们提出一种统一的协同训练框架，在单一SFT阶段高效整合多种安全行为：积极型（合法/亲社会）、消极型（无过滤/风险倾向）和拒绝型（拒绝导向/保守型）。值得注意的是，每种行为均可通过简单的系统级指令（即魔法令牌）动态激活，实现推理时隐蔽高效的行为切换。这种灵活性支持多样化的部署场景，例如：积极型用于安全用户交互，消极型用于内部红队测试，拒绝型用于响应上游审核信号的情境感知拒绝。该协同训练策略在输出空间诱导出独特的安全对齐边界，其特征表现为各安全模式对应的高度分离的响应分布。该边界的存在为模型的安全鲁棒性提供了实证依据，并实现了前所未有的细粒度控制。实验表明，我们的方法在安全对齐质量上媲美SFT+DPO，其中8B模型在安全性能上显著超越DeepSeek-R1（671B），同时大幅降低了训练复杂度和部署成本。本研究为大语言模型内容安全提供了可扩展、高效且高度可控的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing safety alignment methods for LLMs, such as multi-stage training and lack of post-deployment controllability, by proposing a unified co-training framework that integrates positive, negative, and rejective safety behaviors in a single SFT stage. The method uses magic tokens to dynamically switch between these behaviors at inference, enabling flexible deployment for safe interaction, red-teaming, or context-aware refusals. Experimental results demonstrate that this approach matches the safety quality of SFT+DPO, with an 8B model outperforming DeepSeek-R1 (671B) in safety, while reducing training complexity and deployment costs, and it induces a distinct Safety Alignment Margin in output distributions as evidence of robustness.</div>
<div class="mono" style="margin-top:8px">本文针对现有大语言模型安全对齐方法（如多阶段训练和缺乏部署后可控性）的局限性，提出了一种统一的协同训练框架，在单一监督微调阶段内整合了积极、消极和拒绝三种安全行为。该方法通过魔法令牌在推理时动态切换行为，支持安全交互、内部红队测试或上下文感知拒绝等灵活部署场景。实验结果表明，该方法在安全性能上匹配SFT+DPO，其80亿参数模型在安全性上显著超越DeepSeek-R1（6710亿参数），同时降低了训练复杂度和部署成本，并在输出空间中形成了表征鲁棒性的安全对齐边界。</div>
</details>
</div>
<div class="card">
<div class="title">Human-in-the-Loop Feature Selection Using Interpretable Kolmogorov-Arnold Network-based Double Deep Q-Network</div>
<div class="meta-line">Authors: Md Abrar Jahin, M. F. Mridha, Nilanjan Dey, Md. Jakir Hossen</div>
<div class="meta-line">Venue: IEEE Open Journal of the Computer Society (2026)</div>
<div class="meta-line">First: 2024-11-06T08:13:09+00:00 · Latest: 2026-01-08T05:33:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.03740v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.03740v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feature selection is critical for improving the performance and interpretability of machine learning models, particularly in high-dimensional spaces where complex feature interactions can reduce accuracy and increase computational demands. Existing approaches often rely on static feature subsets or manual intervention, limiting adaptability and scalability. However, dynamic, per-instance feature selection methods and model-specific interpretability in reinforcement learning remain underexplored. This study proposes a human-in-the-loop (HITL) feature selection framework integrated into a Double Deep Q-Network (DDQN) using a Kolmogorov-Arnold Network (KAN). Our novel approach leverages simulated human feedback and stochastic distribution-based sampling, specifically Beta, to iteratively refine feature subsets per data instance, improving flexibility in feature selection. The KAN-DDQN achieved notable test accuracies of 93% on MNIST and 83% on FashionMNIST, outperforming conventional MLP-DDQN models by up to 9%. The KAN-based model provided high interpretability via symbolic representation while using 4 times fewer neurons in the hidden layer than MLPs did. Comparatively, the models without feature selection achieved test accuracies of only 58% on MNIST and 64% on FashionMNIST, highlighting significant gains with our framework. We further validate scalability on CIFAR-10 and CIFAR-100, achieving up to 30% relative macro F1 improvement on MNIST and 5% on CIFAR-10, while reducing calibration error by 25%. Complexity analysis confirms real-time feasibility with latency below 1 ms and parameter counts under 0.02M. Pruning and visualization further enhanced model transparency by elucidating decision pathways. These findings present a scalable, interpretable solution for feature selection that is suitable for applications requiring real-time, adaptive decision-making with minimal human oversight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可解释柯尔莫哥洛夫-阿诺德网络的双深度Q网络人机协同特征选择方法</div>
<div class="mono" style="margin-top:8px">特征选择对提升机器学习模型性能与可解释性至关重要，尤其在高维空间中，复杂的特征交互会降低精度并增加计算负担。现有方法多依赖静态特征子集或人工干预，限制了适应性与可扩展性。而动态的逐实例特征选择方法及强化学习中模型特定的可解释性研究仍显不足。本研究提出一种集成柯尔莫哥洛夫-阿诺德网络（KAN）的双深度Q网络（DDQN）人机协同（HITL）特征选择框架。该创新方法通过模拟人类反馈和基于随机分布（特别是Beta分布）的采样，逐数据实例迭代优化特征子集，从而提升特征选择的灵活性。KAN-DDQN在MNIST和FashionMNIST数据集上分别取得93%和83%的显著测试准确率，较传统MLP-DDQN模型提升高达9%。基于KAN的模型通过符号表示实现高可解释性，且隐藏层神经元数量仅为MLP的1/4。相比之下，未使用特征选择的模型在MNIST和FashionMNIST上仅获得58%和64%的准确率，凸显了本框架的显著增益。我们进一步在CIFAR-10和CIFAR-100上验证了可扩展性：在MNIST上相对宏观F1值提升达30%，CIFAR-10提升5%，同时校准误差降低25%。复杂度分析证实了实时可行性，延迟低于1毫秒且参数量小于0.02M。剪枝与可视化技术通过阐明决策路径进一步增强了模型透明度。这些研究成果为特征选择提供了一种可扩展、可解释的解决方案，适用于需要实时自适应决策且人工干预最少的应用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for adaptive and interpretable feature selection in high-dimensional machine learning, this paper introduces a human-in-the-loop framework that integrates a Kolmogorov-Arnold Network into a Double Deep Q-Network (KAN-DDQN) to dynamically select features per data instance using simulated human feedback and Beta distribution-based sampling. The method achieved test accuracies of 93% on MNIST and 83% on FashionMNIST, outperforming MLP-DDQN baselines by up to 9%, while providing higher interpretability through symbolic representations and using four times fewer hidden neurons. Experimental results on CIFAR datasets showed up to 30% relative macro F1 improvement on MNIST and reduced calibration error by 25%, with complexity analysis confirming real-time feasibility through low latency and parameter counts.</div>
<div class="mono" style="margin-top:8px">针对高维机器学习中自适应和可解释特征选择的需求，本研究提出了一种人机交互框架，将Kolmogorov-Arnold网络集成到双深度Q网络（KAN-DDQN）中，通过模拟人类反馈和基于Beta分布的采样，动态地为每个数据实例选择特征。该方法在MNIST和FashionMNIST数据集上分别取得了93%和83%的测试准确率，较传统MLP-DDQN模型提升高达9%，同时通过符号表示提高了可解释性，并减少了四倍的隐藏层神经元数量。在CIFAR数据集上的实验显示，MNIST的相对宏观F1分数提升达30%，校准误差降低25%，复杂度分析证实了其低延迟和小参数量的实时可行性。</div>
</details>
</div>
<div class="card">
<div class="title">ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning</div>
<div class="meta-line">Authors: Zhengyi Kwan, Wei Zhang, Aik Beng Ng, Zhengkui Wang, Simon See</div>
<div class="meta-line">First: 2026-01-07T06:50:56+00:00 · Latest: 2026-01-08T05:28:59+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03646v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03646v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Job scheduling is widely used in real-world manufacturing systems to assign ordered job operations to machines under various constraints. Existing solutions remain limited by long running time or insufficient schedule quality, especially when problem scale increases. In this paper, we propose ReLA, a reinforcement-learning (RL) scheduler built on structured representation learning and aggregation. ReLA first learns diverse representations from scheduling entities, including job operations and machines, using two intra-entity learning modules with self-attention and convolution and one inter-entity learning module with cross-attention. These modules are applied in a multi-scale architecture, and their outputs are aggregated to support RL decision-making. Across experiments on small, medium, and large job instances, ReLA achieves the best makespan in most tested settings over the latest solutions. On non-large instances, ReLA reduces the optimality gap of the SOTA baseline by 13.0%, while on large-scale instances it reduces the gap by 78.6%, with the average optimality gaps lowered to 7.3% and 2.1%, respectively. These results confirm that ReLA&#x27;s learned representations and aggregation provide strong decision support for RL scheduling, and enable fast job completion and decision-making for real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReLA：基于强化学习的作业调度表示学习与聚合方法</div>
<div class="mono" style="margin-top:8px">作业调度广泛应用于实际制造系统，用于在多种约束条件下将有序作业工序分配给机器。现有解决方案仍受限于运行时间过长或调度质量不足，尤其在问题规模增大时更为明显。本文提出ReLA，一种基于结构化表示学习与聚合的强化学习调度器。ReLA首先通过两个分别采用自注意力和卷积的实体内部学习模块，以及一个采用交叉注意力的实体间学习模块，从调度实体（包括作业工序和机器）中学习多样化表示。这些模块应用于多尺度架构，其输出经聚合后支持强化学习决策。在小型、中型和大型作业实例的实验中，ReLA在多数测试场景下取得了优于最新解决方案的最短完工时间。在非大型实例中，ReLA将SOTA基准的最优性差距降低了13.0%；在大型实例中，该差距降低78.6%，平均最优性差距分别降至7.3%和2.1%。结果表明，ReLA学习的表示与聚合为强化学习调度提供了强有力的决策支持，能够实现快速作业完成与实时决策应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in job scheduling, such as long runtimes and poor schedule quality at scale, by proposing ReLA, a reinforcement learning scheduler that employs structured representation learning and aggregation. The method learns diverse representations from scheduling entities like job operations and machines using intra-entity modules with self-attention and convolution and an inter-entity module with cross-attention within a multi-scale architecture, aggregating outputs to inform RL decisions. Experimental results show ReLA achieves the best makespan across small, medium, and large instances, reducing the optimality gap of state-of-the-art baselines by 13.0% on non-large instances and 78.6% on large-scale instances, with average gaps lowered to 7.3% and 2.1%, respectively, confirming its effectiveness for fast and high-quality scheduling.</div>
<div class="mono" style="margin-top:8px">本文针对作业调度中运行时间长和规模扩大时调度质量不足的问题，提出了ReLA，一种基于强化学习的调度器，采用结构化表示学习与聚合方法。该方法通过自注意力和卷积的实体内部学习模块以及跨注意力的实体间学习模块，在多尺度架构中学习作业操作和机器等调度实体的多样化表示，并聚合输出以支持强化学习决策。实验结果表明，ReLA在小型、中型和大型作业实例上均实现了最佳完工时间，将最先进基线的优化差距在非大型实例上降低了13.0%，在大型实例上降低了78.6%，平均优化差距分别降至7.3%和2.1%，证实了其在实际应用中实现快速作业完成和高效决策的能力。</div>
</details>
</div>
<div class="card">
<div class="title">SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection</div>
<div class="meta-line">Authors: Zhiyong Cao, Dunqiang Liu, Qi Dai, Haojun Xu, Huaiyan Xu, Huan He, Yafei Liu, Siyuan Liu, XiaoLin Lin, Ke Ma, Ruqian Shi, Sijia Yao, Hao Wang, Sicheng Zhou</div>
<div class="meta-line">First: 2026-01-06T10:00:15+00:00 · Latest: 2026-01-08T04:14:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02871v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02871v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimRPD：基于模拟器的数据评估与选择优化招聘主动对话智能体</div>
<div class="mono" style="margin-top:8px">面向任务的主动对话智能体在招聘场景中发挥着关键作用，尤其在引导对话实现特定商业目标（如获取社交媒体联系人以进行私域转化）方面。尽管监督微调和强化学习已被证明对此类智能体训练有效，但其性能受限于高质量、目标导向的领域特定训练数据的稀缺性。为解决这一挑战，我们提出SimRPD——一个三阶段框架用于训练招聘主动对话智能体。首先，我们开发高保真用户模拟器，通过多轮在线对话合成大规模对话数据；其次，引入基于意图链的多维评估框架，结合全局级与实例级指标，全面评估模拟器并高效筛选高质量数据；最后，基于筛选数据集训练招聘主动对话智能体。真实招聘场景实验表明，SimRPD优于现有基于模拟器的数据选择策略，凸显了其工业部署的实用价值及其在其他商业对话场景中的潜在适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training effective recruitment proactive dialogue agents, which are hindered by the scarcity of high-quality, domain-specific training data. To overcome this, the authors propose SimRPD, a three-stage framework that first uses a high-fidelity user simulator to generate large-scale conversational data, then applies a multi-dimensional evaluation based on Chain-of-Intention to select high-quality data through global and instance-level metrics, and finally trains the dialogue agent on this curated dataset. Experimental results in a real-world recruitment setting show that SimRPD surpasses existing simulator-based data selection methods, demonstrating its practical utility for industrial applications and potential for broader business-oriented dialogue tasks.</div>
<div class="mono" style="margin-top:8px">本文针对招聘主动对话智能体训练中高质量领域特定数据稀缺的挑战，提出了SimRPD框架。该方法包含三个阶段：首先开发高保真用户模拟器，通过多轮在线对话合成大规模对话数据；然后引入基于意图链的多维评估框架，结合全局和实例级指标全面评估并筛选高质量数据；最后利用筛选后的数据集训练招聘主动对话智能体。在真实招聘场景中的实验表明，SimRPD优于现有的基于模拟器的数据选择策略，凸显了其工业部署的实用价值以及在其它业务导向对话场景中的潜在适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Over Space: Enabling Geographic Reasoning for LLM-Based Generative Next POI Recommendation</div>
<div class="meta-line">Authors: Dongyi Lv, Qiuyu Ding, Heng-Da Xu, Zhaoxu Sun, Zhi Wang, Feng Xiong, Mu Xu</div>
<div class="meta-line">First: 2026-01-08T03:46:03+00:00 · Latest: 2026-01-08T03:46:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04562v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04562v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative recommendation with large language models (LLMs) reframes prediction as sequence generation, yet existing LLM-based recommenders remain limited in leveraging geographic signals that are crucial in mobility and local-services scenarios. Here, we present Reasoning Over Space (ROS), a framework that utilizes geography as a vital decision variable within the reasoning process. ROS introduces a Hierarchical Spatial Semantic ID (SID) that discretizes coarse-to-fine locality and POI semantics into compositional tokens, and endows LLM with a three-stage Mobility Chain-of-Thought (CoT) paradigm that models user personality, constructs an intent-aligned candidate space, and performs locality informed pruning. We further align the model with real world geography via spatial-guided Reinforcement Learning (RL). Experiments on three widely used location-based social network (LBSN) datasets show that ROS achieves over 10% relative gains in hit rate over strongest LLM-based baselines and improves cross-city transfer, despite using a smaller backbone model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间推理：为基于大语言模型的生成式下一兴趣点推荐赋予地理推理能力</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的生成式推荐将预测重构为序列生成，但现有基于LLM的推荐系统在利用地理信号方面仍存在局限，而地理信号在移动性和本地服务场景中至关重要。本文提出“空间推理”（ROS）框架，将地理信息作为推理过程中的关键决策变量。ROS引入分层空间语义ID（SID），将粗粒度到细粒度的地理位置与兴趣点语义离散化为组合标记，并通过三阶段移动思维链（CoT）范式赋能LLM，该范式建模用户个性、构建意图对齐的候选空间，并执行基于地理信息的剪枝。我们进一步通过空间引导的强化学习（RL）使模型与现实世界地理对齐。在三个广泛使用的基于位置的社交网络（LBSN）数据集上的实验表明，即使使用更小的骨干模型，ROS在命中率上仍比最强的基于LLM的基线获得超过10%的相对提升，并改善了跨城市迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited use of geographic signals in existing large language model (LLM)-based generative recommenders, which are crucial for mobility and local-services scenarios, this paper introduces the Reasoning Over Space (ROS) framework to incorporate geography as a key decision variable. The method employs a Hierarchical Spatial Semantic ID to tokenize location and point-of-interest (POI) semantics and a three-stage Mobility Chain-of-Thought paradigm for reasoning, further enhanced by spatial-guided reinforcement learning for real-world alignment. Experimental results on three location-based social network datasets demonstrate that ROS achieves over 10% relative improvement in hit rate compared to strong LLM-based baselines and enhances cross-city transferability, even with a smaller backbone model.</div>
<div class="mono" style="margin-top:8px">针对现有基于大语言模型（LLM）的生成式推荐系统在移动性和本地服务场景中难以有效利用关键地理信号的问题，本文提出了空间推理（ROS）框架，将地理信息作为推理过程中的核心决策变量。该方法采用分层空间语义ID对位置和兴趣点（POI）语义进行标记化表示，并通过三阶段移动思维链范式进行推理，同时利用空间引导的强化学习实现与真实地理环境的对齐。在三个广泛使用的位置社交网络数据集上的实验表明，ROS相比最强的LLM基线模型在命中率上取得了超过10%的相对提升，并增强了跨城市迁移能力，尽管使用了更小的骨干模型。</div>
</details>
</div>
<div class="card">
<div class="title">One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents</div>
<div class="meta-line">Authors: Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Weikang Li, Jiahui Liang, Deguo Xia, Jizhou Huang, Jiyan He, Yunfang Wu</div>
<div class="meta-line">First: 2025-12-24T05:27:53+00:00 · Latest: 2026-01-08T03:22:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20957v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.20957v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一器足矣：面向仓库级大语言模型智能体的强化学习</div>
<div class="mono" style="margin-top:8px">在大型开源软件仓库中定位需要修改的文件和函数极具挑战性，因其规模庞大且结构复杂。现有基于大语言模型的方法通常将其视为仓库级检索任务，并依赖多种辅助工具，这忽视了代码执行逻辑且使模型控制复杂化。我们提出RepoNavigator——一种配备单一执行感知工具（跳转至被调用符号定义）的大语言模型智能体。该统一设计反映了代码执行的实际流程，同时简化了工具操作。RepoNavigator通过强化学习对预训练模型进行端到端训练，无需任何闭源蒸馏。实验表明，经强化学习训练的RepoNavigator实现了最先进的性能：7B模型超越14B基线，14B模型优于32B竞品，32B模型甚至超过Claude-3.7等闭源模型。这些结果证实，将单一结构基础工具与强化学习训练相结合，为仓库级问题定位提供了高效可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of locating relevant code in large open-source repositories, where existing LLM-based methods rely on multiple auxiliary tools, complicating control and overlooking execution logic. The authors propose RepoNavigator, an LLM agent that uses a single tool—jumping to the definition of invoked symbols—to reflect actual code execution flow and simplify tool use. It is trained end-to-end with reinforcement learning from a pretrained model, without closed-source distillation. Experimental results show state-of-the-art performance: the 7B model outperforms 14B baselines, the 14B model surpasses 32B competitors, and the 32B model exceeds closed-source models like Claude-3.7, demonstrating the efficiency of this unified, execution-aware approach.</div>
<div class="mono" style="margin-top:8px">本文针对在大型开源代码库中定位相关文件的挑战，现有基于大语言模型的方法依赖多种辅助工具，导致控制复杂且忽略代码执行逻辑。作者提出RepoNavigator，这是一个仅使用单一工具（跳转到被调用符号的定义）的大语言模型智能体，以反映实际代码执行流程并简化工具操作。该模型通过强化学习对预训练模型进行端到端训练，无需闭源模型蒸馏。实验结果表明其性能达到最先进水平：7B模型超越14B基线，14B模型优于32B竞争对手，32B模型甚至超过Claude-3.7等闭源模型，验证了这种统一且基于执行感知的方法的高效性与可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Not All Steps are Informative: On the Linearity of LLMs&#x27; RLVR Training</div>
<div class="meta-line">Authors: Tianle Wang, Zhongyuan Wu, Shenghao Jin, Hao Xu, Wei Chen, Ning Miao</div>
<div class="meta-line">First: 2026-01-08T03:06:18+00:00 · Latest: 2026-01-08T03:06:18+00:00</div>
<div class="meta-line">Comments: pre-print</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04537v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并非所有训练步骤都具信息量：论大语言模型RLVR训练的线性特性</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）已成为大语言模型（LLM）后训练的核心组成部分。与监督微调（SFT）不同，RLVR让LLM生成多个候选解决方案，并强化那些能导向可验证正确答案的路径。然而在实践中，RLVR通常需要数千个训练步骤才能达到强劲性能，产生大量计算成本，这主要归因于漫长的探索过程。本研究发现了一个令人惊讶的现象：在RLVR过程中，LLMs以高度线性的方式演进。具体而言，模型权重和模型输出的对数概率均与RL训练步数呈现强线性相关性。这表明RLVR主要放大训练早期出现的趋势，而非在整个优化轨迹中持续发现新行为。受此线性特性启发，我们探究能否通过外推法从中间检查点预测未来模型状态，从而避免持续的高成本训练。实验证明：权重外推法生成的模型性能与标准RL训练相当，同时显著降低计算需求；而通过对数概率外推法，在四个基准测试中均持续超越传统RL训练，其外推范围已超出RL训练保持稳定的步数区间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the training dynamics of reinforcement learning with verifiable rewards (RLVR) for large language models, motivated by the observation that RLVR typically requires extensive computation due to prolonged exploration. The authors find that during RLVR, model weights and output log-probabilities evolve in a strongly linear manner with training steps, indicating that the process mainly amplifies early trends rather than discovering new behaviors. Experimentally, they demonstrate that weight extrapolation from intermediate checkpoints can produce models with performance comparable to standard RL training but with significantly less computation, and logits extrapolation even outperforms continued RL training across four benchmarks by extrapolating beyond the stable step range.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型在可验证奖励强化学习训练中的动态，其动机在于观察到该训练通常因长时间探索而需要大量计算。作者发现，在训练过程中，模型权重和输出对数概率随训练步数呈现强烈的线性演化，表明该过程主要是放大早期趋势而非发现新行为。实验结果表明，通过中间检查点进行权重外推可以产生与标准强化学习训练性能相当的模型，同时显著减少计算量，而对数概率外推通过超越稳定步数范围的外推，在四个基准测试中均优于持续强化学习训练。</div>
</details>
</div>
<div class="card">
<div class="title">Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification</div>
<div class="meta-line">Authors: Rui Sun, Yifan Sun, Sheng Xu, Li Zhao, Jing Li, Daxin Jiang, Cheng Hua, Zuo Bai</div>
<div class="meta-line">First: 2026-01-07T14:03:22+00:00 · Latest: 2026-01-08T02:48:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03948v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market&#x27;s stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Trade-R1：通过过程级推理验证将可验证奖励桥接至随机环境</div>
<div class="mono" style="margin-top:8px">强化学习（RL）使大型语言模型（LLM）在数学和编程等可验证奖励提供明确信号的领域实现了卓越的推理能力。然而，将这一范式扩展到金融决策面临市场随机性的挑战：奖励虽可验证但本质具有噪声，导致标准RL退化为奖励破解。为此，我们提出Trade-R1，一种通过过程级推理验证将可验证奖励桥接至随机环境的模型训练框架。核心创新是一种验证方法，将评估长篇金融文档推理的问题转化为结构化检索增强生成（RAG）任务。我们构建了三角一致性度量，通过评估检索证据、推理链和决策之间的两两对齐性，作为噪声市场回报的有效性过滤器。我们探索了两种奖励整合策略：提供稳定对齐信号的固定效应语义奖励（FSR），以及用于耦合幅度优化的动态效应语义奖励（DSR）。在不同国家资产选择上的实验表明，该范式能减少奖励破解，其中DSR在保持最高推理一致性的同时实现了更优的跨市场泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying Reinforcement Learning (RL) to financial decision-making, where market rewards are verifiable but inherently noisy, leading standard RL to degenerate into reward hacking. To bridge verifiable rewards to stochastic environments, the authors propose Trade-R1, a framework that introduces process-level reasoning verification via a structured Retrieval-Augmented Generation (RAG) task, creating a triangular consistency metric to filter noisy returns by assessing alignment between evidence, reasoning chains, and decisions. Experimental results on cross-country asset selection show that this approach reduces reward hacking, with the Dynamic-effect Semantic Reward (DSR) strategy achieving superior generalization and the highest reasoning consistency compared to alternatives.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在金融决策中的应用挑战，即市场回报可验证但本质具有噪声，导致标准强化学习退化为奖励黑客行为。为将可验证奖励与随机环境连接，作者提出Trade-R1框架，通过结构化检索增强生成任务实现过程级推理验证，构建三角一致性指标，通过评估证据、推理链和决策之间的对齐来过滤噪声回报。在不同国家资产选择上的实验结果表明，该方法减少了奖励黑客现象，其中动态效应语义奖励策略在保持最高推理一致性的同时，实现了更优的跨市场泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">TSSR: Two-Stage Swap-Reward-Driven Reinforcement Learning for Character-Level SMILES Generation</div>
<div class="meta-line">Authors: Jacob Ede Levine, Yun Lyan Luo, Sai Chandra Kosaraju</div>
<div class="meta-line">First: 2026-01-08T02:35:22+00:00 · Latest: 2026-01-08T02:35:22+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04521v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04521v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The design of reliable, valid, and diverse molecules is fundamental to modern drug discovery, as improved molecular generation supports efficient exploration of the chemical space for potential drug candidates and reduces the cost of early design efforts. Despite these needs, current chemical language models that generate molecules as SMILES strings are vulnerable to compounding token errors: many samples are unparseable or chemically implausible, and hard constraints meant to prevent failure can restrict exploration. To address this gap, we introduce TSSR, a Two-Stage, Swap-Reward-driven reinforcement learning (RL) framework for character-level SMILES generation. Stage one rewards local token swaps that repair syntax, promoting transitions from invalid to parseable strings. Stage two provides chemistry-aware feedback from RDKit diagnostics, rewarding reductions in valence, aromaticity, and connectivity issues. The reward decomposes into interpretable terms (swap efficiency, error reduction, distance to validity), is model agnostic, and requires no task-specific labels or hand-crafted grammars. We evaluated TSSR on the MOSES benchmark using a GRU policy trained with PPO in both pure RL (P-RL) from random initialization and fine-tuning RL (F-RL) starting from a pretrained chemical language model, assessing 10,000 generated SMILES per run. In P-RL, TSSR significantly improves syntactic validity, chemical validity, and novelty. In F-RL, TSSR preserves drug-likeness and synthesizability while increasing validity and novelty. Token-level analysis shows that syntax edits and chemistry fixes act jointly to reduce RDKit detected errors. TSSR converts a sparse terminal objective into a denser and more interpretable reward, improving both syntactic and chemical quality without reducing diversity. TSSR is dataset-agnostic and can be adapted to various reinforcement learning approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TSSR：面向字符级SMILES生成的两阶段交换奖励驱动强化学习</div>
<div class="mono" style="margin-top:8px">可靠、有效且多样化的分子设计是现代药物发现的基础，改进的分子生成有助于高效探索潜在候选药物的化学空间，并降低早期设计成本。尽管需求迫切，当前生成SMILES字符串的化学语言模型易受复合标记错误影响：许多样本无法解析或化学上不合理，而用于防止失败的硬性约束可能限制探索空间。为填补这一空白，我们提出了TSSR——一种面向字符级SMILES生成的两阶段交换奖励驱动强化学习框架。第一阶段奖励修复语法的局部标记交换，促进无效字符串向可解析字符串的转化；第二阶段通过RDKit诊断提供化学感知反馈，奖励化合价、芳香性和连接性问题的减少。该奖励机制可分解为可解释的指标（交换效率、错误减少率、有效距离），具有模型无关性，且无需任务特定标签或人工设计语法。我们在MOSES基准上使用PPO训练的GRU策略评估TSSR，包括从随机初始化的纯强化学习（P-RL）和基于预训练化学语言模型的微调强化学习（F-RL），每次运行评估10,000个生成的SMILES。在P-RL中，TSSR显著提升语法有效性、化学有效性和新颖性；在F-RL中，TSSR在保持类药性与可合成性的同时提高有效性与新颖性。标记级分析表明语法编辑与化学修复协同减少RDKit检测错误。TSSR将稀疏的终端目标转化为更密集且可解释的奖励，在保持多样性的同时提升语法与化学质量。该方法具有数据集无关性，可适配多种强化学习方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to generate reliable, diverse molecules for drug discovery while overcoming the vulnerability of current SMILES-based models to compounding token errors that produce invalid or implausible strings, this paper introduces TSSR, a two-stage reinforcement learning framework. The method employs a swap-reward-driven approach: the first stage rewards local token swaps to repair syntax and achieve parseable strings, while the second stage provides chemistry-aware feedback from RDKit to reduce valence, aromaticity, and connectivity issues, using an interpretable, model-agnostic reward without task-specific labels. Experimental results on the MOSES benchmark show that TSSR significantly improves syntactic and chemical validity and novelty in pure RL from random initialization, and in fine-tuning RL from a pretrained model, it preserves drug-likeness and synthesizability while boosting validity and novelty, with token-level analysis confirming joint error reduction.</div>
<div class="mono" style="margin-top:8px">本文的动机是药物发现需要生成可靠、多样的分子，同时克服当前基于SMILES的模型易因复合标记错误产生无效或化学不合理字符串的问题。为此，提出了TSSR，一个两阶段强化学习框架：方法采用交换奖励驱动策略，第一阶段奖励局部标记交换以修复语法、实现可解析字符串，第二阶段利用RDKit提供化学感知反馈，减少价键、芳香性和连接性问题，使用可解释、模型无关的奖励，无需任务特定标签。在MOSES基准上的实验结果表明，在从随机初始化的纯强化学习中，TSSR显著提升了语法和化学有效性及新颖性；在从预训练模型微调的强化学习中，它在保持类药性和可合成性的同时提高了有效性和新颖性，标记级分析证实了错误的联合减少。</div>
</details>
</div>
<div class="card">
<div class="title">Multiagent Reinforcement Learning with Neighbor Action Estimation</div>
<div class="meta-line">Authors: Zhenglong Luo, Zhiyong Chen, Aoxiang Liu</div>
<div class="meta-line">First: 2026-01-08T02:26:57+00:00 · Latest: 2026-01-08T02:26:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04511v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04511v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents&#x27; behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于邻域动作估计的多智能体强化学习</div>
<div class="mono" style="margin-top:8px">多智能体强化学习作为一种重要的智能范式，能够在复杂系统中实现协同决策。然而，现有方法通常依赖智能体间显式的动作交换来评估动作价值函数，这在通信受限、存在延迟、能耗及可靠性要求的实际工程环境中往往难以实现。本文从人工智能视角出发，提出一种增强型多智能体强化学习框架，通过动作估计神经网络推断智能体行为。该框架集成轻量级动作估计模块，使每个智能体仅利用局部可观测信息即可推断邻近智能体的行为，从而无需显式动作共享即可实现协同策略学习。该方法与标准TD3算法完全兼容，并可扩展至更大规模的多智能体系统。在工程应用层面，该框架已在双臂机器人操作任务中实现并验证：两个机械臂协同搬运物体。实验结果表明，该方法显著提升了实际机器人系统的鲁棒性与部署可行性，同时降低了对信息基础设施的依赖。总体而言，本研究推动了去中心化多智能体人工智能系统的发展，使人工智能能在动态、信息受限的真实环境中高效运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the impracticality of explicit action exchange in real-world multiagent systems due to communication constraints, this paper proposes a decentralized reinforcement learning framework where agents infer neighbors&#x27; actions via lightweight neural networks using only local observations, eliminating the need for direct action sharing. The method integrates an action estimation module compatible with standard TD3 algorithms, enhancing scalability. Experimental validation in dual-arm robotic manipulation tasks shows the approach improves system robustness and deployment feasibility while reducing reliance on communication infrastructure.</div>
<div class="mono" style="margin-top:8px">针对现实多智能体系统中因通信限制而难以实现显式动作交换的问题，本文提出一种去中心化强化学习框架，通过轻量级动作估计神经网络，使智能体仅利用局部观测推断邻居行为，无需直接动作共享。该方法与标准TD3算法兼容，可扩展至大规模系统。在双臂机器人协同搬运任务的实验中，该框架显著提升了系统鲁棒性和部署可行性，降低了对信息基础设施的依赖。</div>
</details>
</div>
<div class="card">
<div class="title">Mirror Descent Actor Critic via Bounded Advantage Learning</div>
<div class="meta-line">Authors: Ryo Iwaki</div>
<div class="meta-line">First: 2025-02-06T08:14:03+00:00 · Latest: 2026-01-08T02:15:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03854v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.03854v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Regularization is a core component of recent Reinforcement Learning (RL) algorithms. Mirror Descent Value Iteration (MDVI) uses both Kullback-Leibler divergence and entropy as regularizers in its value and policy updates. Despite its empirical success in discrete action domains and strong theoretical guarantees, the performance of KL-entropy-regularized methods does not surpass that of a strong entropy-only-regularized method in continuous action domains. In this study, we propose Mirror Descent Actor Critic (MDAC) as an actor-critic style instantiation of MDVI for continuous action domains, and show that its empirical performance is significantly boosted by bounding the actor&#x27;s log-density terms in the critic&#x27;s loss function, compared to a non-bounded naive instantiation. Further, we relate MDAC to Advantage Learning by recalling that the actor&#x27;s log-probability is equal to the regularized advantage function in tabular cases, and theoretically discuss when and why bounding the advantage terms is validated and beneficial. We also empirically explore effective choices for the bounding functions, and show that MDAC performs better than strong non-regularized and entropy-only-regularized methods with an appropriate choice of the bounding functions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于有界优势学习的镜像下降行动者-评论者算法</div>
<div class="mono" style="margin-top:8px">正则化是现代强化学习算法的核心组成部分。镜像下降值迭代算法在值和策略更新中同时使用KL散度和熵作为正则项。尽管该方法在离散动作领域取得实证成功并具备强理论保证，但在连续动作领域中，KL-熵正则化方法的性能并未超越仅使用熵正则化的强基准方法。本研究提出镜像下降行动者-评论者算法作为MDVI在连续动作领域的行动者-评论者式实例化，并通过实验证明：与无界朴素实例化相比，在评论者损失函数中对行动者的对数密度项进行有界处理能显著提升其经验性能。进一步地，我们通过回顾表格场景中行动者对数概率等于正则化优势函数这一性质，将MDAC与优势学习建立联系，并从理论上探讨何时及为何对优势项进行有界处理是有效且有益的。我们还通过实证研究探索了有界函数的有效选择方案，并证明当选用合适的边界函数时，MDAC的性能优于强非正则化方法和仅使用熵正则化的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Mirror Descent Actor Critic (MDAC), an actor-critic algorithm designed to improve the performance of KL-entropy-regularized reinforcement learning in continuous action domains, where such methods have previously underperformed compared to entropy-only regularization. The method instantiates Mirror Descent Value Iteration in an actor-critic framework and boosts empirical performance by bounding the actor&#x27;s log-density terms within the critic&#x27;s loss function, which is theoretically linked to bounding advantage functions. Experimental results demonstrate that MDAC, with appropriate bounding functions, outperforms strong non-regularized and entropy-only-regularized baselines, validating the effectiveness of the proposed bounding technique.</div>
<div class="mono" style="margin-top:8px">本文提出了镜像下降行动者评论家（MDAC）算法，旨在提升KL熵正则化强化学习在连续动作域中的性能，此前该类方法在此类域中表现不及仅使用熵正则化的方法。该方法将镜像下降值迭代实例化为行动者评论家框架，并通过在评论家损失函数中对行动者的对数密度项进行有界处理来提升实证性能，这在理论上与对优势函数进行有界处理相关联。实验结果表明，在选用合适的边界函数时，MDAC的性能优于强大的非正则化及仅熵正则化的基线方法，验证了所提出的边界技术的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving and Accelerating Offline RL in Large Discrete Action Spaces with Structured Policy Initialization</div>
<div class="meta-line">Authors: Matthew Landers, Taylor W. Killian, Thomas Hartvigsen, Afsaneh Doryab</div>
<div class="meta-line">First: 2026-01-07T22:57:21+00:00 · Latest: 2026-01-07T22:57:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04441v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04441v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning in discrete combinatorial action spaces requires searching over exponentially many joint actions to simultaneously select multiple sub-actions that form coherent combinations. Existing approaches either simplify policy learning by assuming independence across sub-actions, which often yields incoherent or invalid actions, or attempt to learn action structure and control jointly, which is slow and unstable. We introduce Structured Policy Initialization (SPIN), a two-stage framework that first pre-trains an Action Structure Model (ASM) to capture the manifold of valid actions, then freezes this representation and trains lightweight policy heads for control. On challenging discrete DM Control benchmarks, SPIN improves average return by up to 39% over the state of the art while reducing time to convergence by up to 12.8$\times$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过结构化策略初始化改进和加速大规模离散动作空间中的离线强化学习</div>
<div class="mono" style="margin-top:8px">在离散组合动作空间中进行强化学习，需要搜索指数级数量的联合动作，以同时选择多个构成连贯组合的子动作。现有方法要么通过假设子动作间相互独立来简化策略学习（这常导致动作不连贯或无效），要么尝试同时学习动作结构和控制（这种方法缓慢且不稳定）。我们提出了结构化策略初始化（SPIN），这是一个两阶段框架：首先预训练动作结构模型（ASM）以捕捉有效动作的流形，然后冻结该表征并训练轻量级策略头进行控制。在具有挑战性的离散DM Control基准测试中，SPIN将平均回报较现有最优方法提升了最高39%，同时将收敛时间缩短了最高12.8倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of reinforcement learning in large discrete combinatorial action spaces, where existing methods either produce invalid actions by assuming sub-action independence or suffer from slow, unstable joint learning of action structure and control. The proposed method, Structured Policy Initialization (SPIN), addresses this with a two-stage framework: it first pre-trains an Action Structure Model to learn the manifold of valid actions, then freezes this representation to train lightweight policy heads for efficient control. Experimental results on discrete DM Control benchmarks show that SPIN improves average return by up to 39% over state-of-the-art methods and accelerates convergence by up to 12.8 times.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决大规模离散组合动作空间中的强化学习难题，现有方法要么假设子动作独立导致动作无效或不连贯，要么联合学习动作结构和控制策略，但速度慢且不稳定。提出的方法称为结构化策略初始化（SPIN），采用两阶段框架：首先预训练一个动作结构模型以学习有效动作的流形，然后冻结该表示并训练轻量级策略头进行控制。在离散DM Control基准测试中，实验结果表明SPIN将平均回报提高了高达39%，并加速收敛达12.8倍，优于现有技术。</div>
</details>
</div>
<div class="card">
<div class="title">SAINT: Attention-Based Policies for Discrete Combinatorial Action Spaces</div>
<div class="meta-line">Authors: Matthew Landers, Taylor W. Killian, Thomas Hartvigsen, Afsaneh Doryab</div>
<div class="meta-line">First: 2025-05-17T18:34:31+00:00 · Latest: 2026-01-07T22:33:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12109v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12109v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The combinatorial structure of many real-world action spaces leads to exponential growth in the number of possible actions, limiting the effectiveness of conventional reinforcement learning algorithms. Recent approaches for combinatorial action spaces impose factorized or sequential structures over sub-actions, failing to capture complex joint behavior. We introduce the Sub-Action Interaction Network using Transformers (SAINT), a novel policy architecture that represents multi-component actions as unordered sets and models their dependencies via self-attention conditioned on the global state. SAINT is permutation-invariant, sample-efficient, and compatible with standard policy optimization algorithms. In 20 distinct combinatorial environments across three task domains, including environments with nearly 17 million joint actions, SAINT consistently outperforms strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAINT：面向离散组合动作空间的注意力策略</div>
<div class="mono" style="margin-top:8px">现实世界中许多动作空间的组合结构导致可能动作数量呈指数级增长，限制了传统强化学习算法的有效性。现有针对组合动作空间的方法通常对子动作施加因子化或序列化结构，难以捕捉复杂的联合行为。本文提出基于Transformer的子动作交互网络（SAINT），这是一种创新的策略架构，将多组件动作表示为无序集合，并通过基于全局状态的自注意力机制建模其依赖关系。SAINT具有排列不变性、样本高效性，且兼容标准策略优化算法。在涵盖三个任务领域的20个不同组合环境中（包括包含近1700万个联合动作的环境），SAINT均持续优于现有强基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of reinforcement learning in discrete combinatorial action spaces, where the exponential growth of possible actions limits conventional methods, and existing factorized or sequential approaches fail to capture complex joint dependencies. The authors propose SAINT, a novel policy architecture that represents multi-component actions as unordered sets and models their interactions via self-attention conditioned on the global state, ensuring permutation invariance and compatibility with standard policy optimization algorithms. Experimental results across 20 combinatorial environments, including tasks with up to 17 million joint actions, demonstrate that SAINT consistently outperforms strong baselines in terms of sample efficiency and performance.</div>
<div class="mono" style="margin-top:8px">本文针对离散组合动作空间中强化学习面临的挑战展开研究，传统方法因动作数量指数增长而受限，现有因子化或序列化方法难以捕捉复杂的联合依赖关系。作者提出SAINT，一种新颖的策略架构，将多组件动作表示为无序集，并通过基于全局状态的自注意力机制建模其依赖关系，确保排列不变性且兼容标准策略优化算法。在涵盖三个任务领域的20个组合环境中进行实验，包括涉及近1700万联合动作的任务，结果表明SAINT在样本效率和性能上均持续优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards</div>
<div class="meta-line">Authors: Ali Rad, Khashayar Filom, Darioush Keivan, Peyman Mohajerin Esfahani, Ehsan Kamalinejad</div>
<div class="meta-line">First: 2026-01-07T21:31:26+00:00 · Latest: 2026-01-07T21:31:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04411v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04411v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?
  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden&#x27;s index J=TPR-FPR. This yields a sharp phase transition: when J&gt;0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J&lt;0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J&gt;0, noise primarily rescales convergence time (&quot;rate, not fate&quot;). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>速率还是命运？RLV$^\varepsilon$R：具有可验证噪声奖励的强化学习</div>
<div class="mono" style="margin-top:8px">具有可验证奖励的强化学习（RLVR）是一种训练大语言模型的简洁而强大的范式：采样生成结果、验证、更新。然而实践中，验证器几乎从不完美——单元测试仅覆盖有限边界情况；人工与合成标注存在缺陷；大语言模型评判器（如RLAIF）具有噪声且可能被利用——该问题在测试稀疏且日益由模型生成的困难领域（特别是代码生成）中更为严重。我们提出一个现实问题：验证噪声仅是减缓学习速度（速率），还是可能逆转学习结果（命运）？为此，我们建立了RLVR动态的解析可处理多臂老虎机视角，通过GRPO实例化并在受控实验中验证。通过建模假阳性与假阴性、将生成结果归类为重复推理模式，我们得到了概率单纯形上的复制器式（自然选择）流。该动态解耦为正确模式内部竞争与错误模式质量的一维演化，其漂移完全由约登指数J=TPR-FPR决定。这产生了尖锐的相变：当J&gt;0时，错误模式趋于消亡（学习）；J=0时过程呈中性；J&lt;0时错误模式持续放大直至主导（反学习与崩溃）。在学习区域J&gt;0中，噪声主要重标收敛时间（“速率而非命运”）。在合成噪声下的可验证编程任务实验中，我们重现了预测的J=0边界。除噪声分析外，该框架为研究RLVR的稳定性、收敛性及算法干预提供了通用分析视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper investigates the impact of noisy reward verification in reinforcement learning for large language models, questioning whether such noise merely slows learning or fundamentally alters outcomes. It introduces an analytical model using a multi-armed bandit framework with GRPO, grouping completions into reasoning modes to derive replicator dynamics that reveal a phase transition based on Youden&#x27;s index (J = true positive rate minus false positive rate). Experimental results on programming tasks show that when J &gt; 0, learning occurs with noise affecting convergence rate; when J ≤ 0, incorrect modes dominate, leading to anti-learning or collapse, thus confirming noise can dictate fate beyond just rate.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型强化学习中噪声奖励验证的影响，探讨噪声是仅减缓学习速度还是根本改变结果。通过采用基于GRPO的多臂老虎机分析框架，将完成结果分组为推理模式，推导出复制动力学，揭示了基于约登指数（J = 真阳性率减假阳性率）的相变。在编程任务上的实验结果表明，当J &gt; 0时学习发生，噪声主要影响收敛速度；当J ≤ 0时错误模式主导，导致反学习或崩溃，从而证实噪声不仅能影响速率，还能决定命运。</div>
</details>
</div>
<div class="card">
<div class="title">Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces</div>
<div class="meta-line">Authors: Arsyi Aziz, Peng Wei</div>
<div class="meta-line">First: 2026-01-07T21:18:28+00:00 · Latest: 2026-01-07T21:18:28+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures, 4 tables. Presented at SESAR Innovation Days 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04401v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Transformer的多智能体强化学习在结构化与非结构化空域中的间隔保障研究</div>
<div class="mono" style="margin-top:8px">传统基于优化的流量管理依赖严格遵循预计算时刻表，限制了先进空中交通随机运行所需的灵活性。相比之下，多智能体强化学习提供了一种去中心化、自适应的框架，能更好地处理不确定性，满足安全航空器间隔保障需求。尽管具备优势，现有MARL方法常过度拟合特定空域结构，限制了其对新构型的适应性。为提升泛化能力，我们将MARL问题重构于相对极坐标状态空间，并针对多样化交通模式与交叉角度训练Transformer编码器模型。该学习模型通过提供速度建议来解决冲突，同时保持航空器接近期望巡航速度。实验中，我们在结构化与非结构化空域评估了1、2、3层编码器深度，发现单层编码器配置优于深层变体，实现了近乎零的空中接近碰撞率，且间隔违规持续时间更短。此外，该配置还优于纯注意力设计的基线模型。综合结果表明，新构建的状态表示方法、神经网络架构创新设计及所提训练策略，为结构化与非结构化空域中的航空器间隔保障提供了适应性强、可扩展的去中心化解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inflexibility of conventional optimization-based metering for Advanced Air Mobility&#x27;s stochastic operations, this paper proposes a multi-agent reinforcement learning approach using a transformer encoder to provide decentralized, adaptive separation assurance. The method recasts the problem in a relative polar state space and trains the model across diverse traffic patterns and intersection angles to improve generalization beyond specific airspace structures. Experimental results in both structured and unstructured airspaces show that a single-layer encoder configuration outperforms deeper variants and a pure attention baseline, achieving near-zero near mid-air collision rates and shorter loss-of-separation infringements, demonstrating an adaptable and scalable solution.</div>
<div class="mono" style="margin-top:8px">针对传统基于优化的间隔管理方法在先进空中交通随机运行中灵活性不足的问题，本文提出了一种基于Transformer编码器的多智能体强化学习方法，以实现去中心化、自适应的飞行间隔保障。该方法将问题重构于相对极坐标状态空间，并通过在不同交通模式和交叉角度下训练模型来提升对空域结构的泛化能力。在结构化和非结构化空域的实验中，单层编码器配置优于更深层变体及纯注意力基线模型，实现了近乎零的空中接近碰撞率和更短的间隔侵入时间，验证了该方案的可适应性与可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay</div>
<div class="meta-line">Authors: Mohsen Jalaeian-Farimani</div>
<div class="meta-line">First: 2026-01-07T20:59:18+00:00 · Latest: 2026-01-07T20:59:18+00:00</div>
<div class="meta-line">Comments: Submitted to ECC26 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04392v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($λ$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($λ$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($λ$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework&#x27;s inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Enhanced-FQL($λ$)：一种高效可解释的强化学习方法，融合新型模糊资格迹与分段经验回放</div>
<div class="mono" style="margin-top:8px">本文提出一种模糊强化学习框架Enhanced-FQL($λ$)，将新型模糊资格迹与分段经验回放机制集成至基于模糊贝尔曼方程的模糊Q学习中，适用于连续控制任务。该方法采用可解释的模糊规则库替代复杂神经网络架构，通过两项关键创新保持竞争力：采用带资格迹的模糊贝尔曼方程实现稳定的多步信用分配，以及基于分段的高效经验回放机制提升样本效率。理论分析证明该方法在标准假设下具有收敛性。在连续控制领域的广泛实验表明，相较于n步模糊TD和模糊SARSA($λ$)基线方法，Enhanced-FQL($λ$)在保持显著低于DDPG等深度强化学习算法计算复杂度的同时，实现了更优的样本效率与更低的方差。该框架固有的可解释性、计算效率及理论收敛保证，使其特别适用于对透明度和资源约束有严格要求的安全关键型应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for interpretable and computationally efficient reinforcement learning for continuous control, particularly in safety-critical domains, this paper proposes Enhanced-FQL(λ), a fuzzy reinforcement learning framework. The method integrates novel Fuzzified Eligibility Traces (FET) for stable multi-step credit assignment and a Segmented Experience Replay (SER) mechanism for memory-efficient learning, all built upon an interpretable fuzzy rule base instead of deep neural networks. Experimental results in continuous control tasks demonstrate that the approach achieves superior sample efficiency and lower variance compared to fuzzy TD and SARSA(λ) baselines, while maintaining lower computational complexity than deep RL methods like DDPG, alongside proven theoretical convergence.</div>
<div class="mono" style="margin-top:8px">本文的动机是为连续控制任务，特别是安全关键领域，开发可解释且计算高效的强化学习方法。为此，论文提出了Enhanced-FQL(λ)框架，该方法将新颖的模糊化资格迹（FET）用于稳定的多步信用分配，并结合了分段经验回放（SER）机制以实现内存高效的学习，整个系统基于可解释的模糊规则库而非深度神经网络构建。在连续控制领域的广泛评估表明，该方法相比模糊TD和SARSA(λ)基线实现了更优的样本效率和更低的方差，同时保持了比DDPG等深度强化学习方法更低的计算复杂度，并且具有理论上的收敛性保证。</div>
</details>
</div>
<div class="card">
<div class="title">BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces</div>
<div class="meta-line">Authors: Matthew Landers, Taylor W. Killian, Hugo Barnes, Thomas Hartvigsen, Afsaneh Doryab</div>
<div class="meta-line">First: 2024-10-28T15:49:46+00:00 · Latest: 2026-01-07T20:57:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.21151v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.21151v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning in high-dimensional, discrete action spaces is challenging due to the exponential scaling of the joint action space with the number of sub-actions and the complexity of modeling sub-action dependencies. Existing methods either exhaustively evaluate the action space, making them computationally infeasible, or factorize Q-values, failing to represent joint sub-action effects. We propose Branch Value Estimation (BraVE), a value-based method that uses tree-structured action traversal to evaluate a linear number of joint actions while preserving dependency structure. BraVE outperforms prior offline RL methods by up to $20\times$ in environments with over four million actions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BraVE：面向离散组合动作空间的离线强化学习</div>
<div class="mono" style="margin-top:8px">在高维离散动作空间中，由于联合动作空间随子动作数量呈指数级增长且子动作依赖关系建模复杂，离线强化学习面临挑战。现有方法要么需穷举评估动作空间导致计算不可行，要么通过分解Q值而无法表征子动作的联合效应。本文提出分支价值估计（BraVE），这是一种基于价值的方法，通过树状动作遍历评估线性数量的联合动作，同时保持依赖结构。在动作空间超过四百万的环境中，BraVE相比现有离线强化学习方法性能提升最高达20倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the computational challenge of offline reinforcement learning in high-dimensional discrete combinatorial action spaces, where the joint action space scales exponentially and sub-action dependencies are complex. The proposed method, Branch Value Estimation (BraVE), employs a tree-structured action traversal to efficiently evaluate a linear number of joint actions while maintaining dependency structure, avoiding the infeasibility of exhaustive evaluation and the limitations of factorized Q-values. Experimental results demonstrate that BraVE outperforms prior offline RL methods by up to 20 times in environments with over four million actions.</div>
<div class="mono" style="margin-top:8px">本文针对高维离散组合动作空间中的离线强化学习计算挑战，其中联合动作空间呈指数级增长且子动作依赖关系复杂。提出的方法——分支价值估计（BraVE），采用树状结构动作遍历来高效评估线性数量的联合动作，同时保持依赖结构，避免了穷举评估的不可行性和分解Q值的局限性。实验结果表明，在动作数超过四百万的环境中，BraVE的性能优于先前的离线强化学习方法，提升幅度高达20倍。</div>
</details>
</div>
<div class="card">
<div class="title">Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning</div>
<div class="meta-line">Authors: Anton Roupassov-Ruiz, Yiyang Zuo</div>
<div class="meta-line">First: 2026-01-07T20:09:28+00:00 · Latest: 2026-01-07T20:09:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04365v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04365v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化强化学习中神经策略与程序化策略的生存动态研究</div>
<div class="mono" style="margin-top:8px">在进化强化学习任务中，智能体策略通常编码为小型人工神经网络。此类表征缺乏显式模块化结构，限制了行为可解释性。本研究探讨以软可微决策列表实现的程序化策略能否达到神经策略的性能水平。为支持可复现评估，我们首次完整复现并开源了199年经典人工生命进化强化学习测试平台。通过4000次独立试验，采用原始研究未涉及的卡普兰-迈耶曲线与限制平均生存时间指标进行严格生存分析。结果显示程序化策略与神经策略的生存概率存在统计学显著差异：程序化智能体平均比神经智能体多生存201.69步；仅使用学习机制（无进化）的软可微决策列表智能体，其平均生存步数比同时使用学习与进化机制的神经智能体多73.67步。这表明在人工生命环境中，程序化策略的生存性能可超越神经策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited interpretability of neural network policies in evolutionary reinforcement learning (ERL), this study investigates whether programmatic policies, implemented as soft differentiable decision lists (SDDL), can match or exceed the performance of traditional neural network policies (NERL). The method involves creating a fully specified, open-source reimplementation of a classic 1992 Artificial Life ERL testbed and conducting a rigorous survival analysis across 4000 independent trials using Kaplan-Meier curves and Restricted Mean Survival Time metrics. The main experimental results show a statistically significant difference, with programmatic policies (PERL) surviving on average 201.69 steps longer than neural policies, and even SDDL agents using only learning (without evolution) outperforming neural agents that use both learning and evolution by 73.67 steps, demonstrating superior survival performance for programmatic policies.</div>
<div class="mono" style="margin-top:8px">本研究针对进化强化学习中神经网络策略可解释性有限的问题，探讨了以软可微决策列表实现的程序化策略能否达到或超越传统神经网络策略的性能。方法上，研究首次完整复现并开源了1992年经典人工生命进化强化学习测试环境，并在4000次独立试验中利用Kaplan-Meier曲线和限制平均生存时间指标进行了严谨的生存分析。主要实验结果表明，程序化策略与神经网络策略的生存概率存在统计学显著差异，程序化策略平均多存活201.69步；仅使用学习（无进化）的软可微决策列表代理也比同时使用学习与进化的神经网络代理平均多存活73.67步，证明了程序化策略在生存性能上的优越性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Bayesian Optimization for Portfolio Management with an Adaptive Scheduling</div>
<div class="meta-line">Authors: Zinuo You, John Cartlidge, Karen Elliott, Menghan Ge, Daniel Gold</div>
<div class="meta-line">Venue: In 2025 9th International Conference on Advances in Artificial Intelligence (ICAAI 2025), November 14-16, 2025, Manchester, United Kingdom. ACM, New York, NY, USA, 5 pages</div>
<div class="meta-line">First: 2025-04-18T07:40:24+00:00 · Latest: 2026-01-07T19:25:50+00:00</div>
<div class="meta-line">Comments: 5 pages, 2 figures; version of record. ICAAI 2025, 9th International Conference on Advances in Artificial Intelligence (ICAAI 2025), November 14-16, 2025, Manchester, United Kingdom. ACM, New York, NY, USA, 5 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.13529v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.13529v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing black-box portfolio management systems are prevalent in the financial industry due to commercial and safety constraints, though their performance can fluctuate dramatically with changing market regimes. Evaluating these non-transparent systems is computationally expensive, as fixed budgets limit the number of possible observations. Therefore, achieving stable and sample-efficient optimization for these systems has become a critical challenge. This work presents a novel Bayesian optimization framework (TPE-AS) that improves search stability and efficiency for black-box portfolio models under these limited observation budgets. Standard Bayesian optimization, which solely maximizes expected return, can yield erratic search trajectories and misalign the surrogate model with the true objective, thereby wasting the limited evaluation budget. To mitigate these issues, we propose a weighted Lagrangian estimator that leverages an adaptive schedule and importance sampling. This estimator dynamically balances exploration and exploitation by incorporating both the maximization of model performance and the minimization of the variance of model observations. It guides the search from broad, performance-seeking exploration towards stable and desirable regions as the optimization progresses. Extensive experiments and ablation studies, which establish our proposed method as the primary approach and other configurations as baselines, demonstrate its effectiveness across four backtest settings with three distinct black-box portfolio management models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应调度的贝叶斯优化在投资组合管理中的改进</div>
<div class="mono" style="margin-top:8px">现有黑箱投资组合管理系统因商业与安全约束在金融行业广泛应用，但其性能随市场机制变化可能剧烈波动。评估这些非透明系统的计算成本高昂，固定预算限制了可观测次数。因此，实现这些系统的稳定且样本高效的优化成为关键挑战。本研究提出一种新颖的贝叶斯优化框架（TPE-AS），在有限观测预算下提升黑箱投资组合模型的搜索稳定性与效率。仅最大化期望收益的标准贝叶斯优化可能导致搜索轨迹不稳定，并使代理模型与真实目标失配，从而浪费有限评估资源。为缓解这些问题，我们提出一种结合自适应调度与重要性采样的加权拉格朗日估计器。该估计器通过同时纳入模型性能最大化与观测方差最小化，动态平衡探索与利用。随着优化进程推进，它将搜索从广泛性能探索引导至稳定理想区域。大量实验与消融研究（将本方法设为主要方案，其他配置作为基线）在四种回测设置与三种不同黑箱投资组合管理模型中验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for stable and sample-efficient optimization of black-box portfolio management systems under limited evaluation budgets, this paper introduces TPE-AS, a novel Bayesian optimization framework. The method employs a weighted Lagrangian estimator with adaptive scheduling and importance sampling to dynamically balance exploration and exploitation, aiming to maximize model performance while minimizing observation variance. Experimental results from extensive tests across four backtest settings with three distinct portfolio models demonstrate that the proposed approach effectively improves search stability and efficiency compared to baseline configurations.</div>
<div class="mono" style="margin-top:8px">针对黑盒投资组合管理系统在有限评估预算下需要稳定且样本高效的优化这一挑战，本文提出了TPE-AS这一新颖的贝叶斯优化框架。该方法采用带有自适应调度和重要性采样的加权拉格朗日估计器，动态平衡探索与利用，旨在最大化模型性能的同时最小化观测方差。在四种回测设置下对三种不同投资组合模型进行的广泛实验表明，与基线配置相比，所提方法有效提升了搜索的稳定性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Rubrics as Contextual Verifiers for SWE Agents</div>
<div class="meta-line">Authors: Mohit Raghavendra, Anisha Gunjal, Bing Liu, Yunzhong He</div>
<div class="meta-line">First: 2026-01-07T18:38:23+00:00 · Latest: 2026-01-07T18:38:23+00:00</div>
<div class="meta-line">Comments: 31 pages, 11 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04171v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>作为软件工程智能体上下文验证器的代理式评估准则</div>
<div class="mono" style="margin-top:8px">验证对提升智能体性能至关重要：它为强化学习提供奖励信号，并通过测试时扩展（TTS）实现推理阶段增益。尽管验证在软件工程（SWE）智能体场景中极为重要，但当前主要依赖代码执行，而环境配置开销使其难以扩展。虽然存在补丁分类器和启发式方法等可扩展替代方案，但这些方法较少基于代码库上下文且可解释性较差。为此，我们探索了代理式评估准则：专家智能体通过与代码库交互创建基于上下文的准则检查表，随后无需执行测试即可依据该准则对候选补丁进行评分。在并行TTS评估的SWE-Bench Verified基准上，代理式评估准则在Qwen3-Coder-30B-A3B模型上获得54.2%的得分，在Qwen3-32B模型上获得40.6%的得分，较对比集中最强基线的提升幅度至少达+3.5个百分点。我们进一步分析了准则行为，表明准则评分与真实测试结果一致，同时能标记测试未覆盖的问题。消融实验证明，代理式上下文收集对生成代码库特异性、无歧义的评判标准至关重要。综合结果表明，代理式评估准则为软件工程智能体提供了高效、可扩展且细粒度的验证信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scalable verification for software engineering (SWE) agents, where traditional reliance on code execution is hindered by environment setup overhead. The proposed method, Agentic Rubrics, involves an expert agent interacting with a repository to generate a context-grounded rubric checklist, which is then used to score candidate patches without executing tests. Experimental results on SWE-Bench Verified show that this approach achieves scores of 54.2% and 40.6% on specific models, outperforming baselines by at least 3.5 percentage points, while analysis confirms that rubric scores align with ground-truth tests and capture additional issues, with ablations highlighting the importance of agentic context for codebase-specific criteria.</div>
<div class="mono" style="margin-top:8px">本文针对软件工程智能体可扩展验证的挑战，传统方法依赖代码执行但受限于环境设置开销。提出的方法“智能体评分标准”通过专家智能体与代码库交互，生成基于上下文的评分检查表，用于在不执行测试的情况下评估补丁。在SWE-Bench Verified上的实验结果显示，该方法在特定模型上分别达到54.2%和40.6%的得分，比基线至少提高3.5个百分点，分析表明评分标准与真实测试一致并能捕捉额外问题，消融实验强调了智能体上下文对生成代码库特定标准的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Is Enough: LLMs Are In-Context Reinforcement Learners</div>
<div class="meta-line">Authors: Kefan Song, Amir Moeini, Peng Wang, Lei Gong, Rohan Chandra, Shangtong Zhang, Yanjun Qi</div>
<div class="meta-line">First: 2025-05-21T16:15:01+00:00 · Latest: 2026-01-07T17:58:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06303v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.06303v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励即足够：大语言模型是上下文强化学习者</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是解决序列决策问题的框架。本研究发现，在大语言模型（LLM）的推理阶段会自发涌现强化学习行为，我们称之为上下文强化学习（ICRL）。为揭示此能力，我们提出了一个简单的多轮提示框架——ICRL提示法，用于实现推理时的自我改进。该方法旨在引导LLM在推理过程中通过强化学习优化特定任务表现：模型每轮生成响应后获得数值标量反馈（即奖励），下一轮提示时将历史响应及其对应奖励拼接为上下文输入。实验表明，随着上下文扩展，响应质量持续提升——LLM能在推理过程中优化标量奖励信号，表现出类强化学习行为。我们在24点游戏、创意写作、ScienceWorld及奥林匹克数学竞赛（AIME与HMMT）上评估ICRL提示法，其性能显著优于Self-Refine、Reflexion等基线方法。值得注意的是，即使奖励信号由同一LLM生成，该方法仍能提升表现，这为测试时扩展提供了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the hypothesis that reinforcement learning (RL) capabilities can emerge in large language models (LLMs) during inference without explicit training. The method introduces in-context RL (ICRL) prompting, a multi-round framework where an LLM receives numerical reward feedback after each response and is then prompted again with the history of prior responses and rewards, enabling it to self-improve by optimizing these signals. Experimental results across tasks including Game of 24, creative writing, ScienceWorld, and math competitions show that response quality consistently improves as context grows, outperforming baselines like Self-Refine and Reflexion, even when rewards are generated by the same LLM, suggesting a new paradigm for test-time scaling.</div>
<div class="mono" style="margin-top:8px">本文的动机是基于一个假设：大型语言模型在推理过程中无需显式训练即可涌现出强化学习能力。方法上提出了上下文内强化学习提示框架，通过多轮交互让模型在每次生成响应后接收数值奖励反馈，并在下一轮提示时结合历史响应和奖励，使其能够通过优化这些信号进行自我改进。实验结果表明，在24点游戏、创意写作、ScienceWorld和数学竞赛等任务中，随着上下文增长，响应质量持续提升，其性能超越了自我优化和反思等基线方法，即使奖励由同一模型生成也能实现改进，这为测试时扩展提供了一种有前景的新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Quantifying the Impact of Modules and Their Interactions in the PSO-X Framework</div>
<div class="meta-line">Authors: Christian L. Camacho-Villalón, Ana Nikolikj, Katharina Dost, Eva Tuba, Sašo Džeroski, Tome Eftimov</div>
<div class="meta-line">First: 2026-01-07T17:06:05+00:00 · Latest: 2026-01-07T17:06:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The PSO-X framework incorporates dozens of modules that have been proposed for solving single-objective continuous optimization problems using particle swarm optimization. While modular frameworks enable users to automatically generate and configure algorithms tailored to specific optimization problems, the complexity of this process increases with the number of modules in the framework and the degrees of freedom defined for their interaction. Understanding how modules affect the performance of algorithms for different problems is critical to making the process of finding effective implementations more efficient and identifying promising areas for further investigation. Despite their practical applications and scientific relevance, there is a lack of empirical studies investigating which modules matter most in modular optimization frameworks and how they interact. In this paper, we analyze the performance of 1424 particle swarm optimization algorithms instantiated from the PSO-X framework on the 25 functions in the CEC&#x27;05 benchmark suite with 10 and 30 dimensions. We use functional ANOVA to quantify the impact of modules and their combinations on performance in different problem classes. In practice, this allows us to identify which modules have greater influence on PSO-X performance depending on problem features such as multimodality, mathematical transformations and varying dimensionality. We then perform a cluster analysis to identify groups of problem classes that share similar module effect patterns. Our results show low variability in the importance of modules in all problem classes, suggesting that particle swarm optimization performance is driven by a few influential modules.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>量化PSO-X框架中模块及其交互的影响</div>
<div class="mono" style="margin-top:8px">PSO-X框架整合了数十个用于解决单目标连续优化问题的粒子群优化模块。模块化框架虽能自动生成并配置针对特定优化问题的算法，但其复杂性随模块数量及交互自由度增加而提升。理解模块如何影响不同问题的算法性能，对于高效寻找有效实现方案及确定未来研究方向至关重要。尽管具有实际应用与科学意义，目前仍缺乏关于模块化优化框架中关键模块及其交互的实证研究。本文基于CEC&#x27;05基准测试集的25个函数（10维与30维），分析了从PSO-X框架实例化的1424种粒子群优化算法性能。通过函数方差分析量化模块及其组合对不同问题类别性能的影响，从而依据多模态性、数学变换及维度变化等特征识别对PSO-X性能影响显著的模块。进一步通过聚类分析发现具有相似模块效应模式的问题类别组。结果表明，所有问题类别中模块重要性变异度较低，说明粒子群优化性能主要由少数关键模块驱动。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to efficiently navigate the complex design space of modular optimization frameworks like PSO-X, which contains many configurable modules for particle swarm optimization, this study aims to quantify the impact of individual modules and their interactions on algorithm performance. The method involves instantiating 1424 PSO algorithms from the PSO-X framework and evaluating them on the CEC&#x27;05 benchmark suite across 10 and 30 dimensions, using functional ANOVA to measure module contributions and cluster analysis to group problems with similar effect patterns. The main experimental results reveal that module importance exhibits low variability across different problem classes, indicating that PSO performance is predominantly driven by a small subset of influential modules, which helps streamline algorithm configuration and guide future research.</div>
<div class="mono" style="margin-top:8px">本研究旨在量化模块化优化框架PSO-X中各个模块及其交互对算法性能的影响，其动机源于需要高效处理该框架中众多可配置粒子群优化模块所带来的复杂设计空间。研究方法包括从PSO-X框架实例化1424种粒子群算法，在CEC&#x27;05基准测试集的10维和30维问题上进行评估，运用功能方差分析测量模块贡献度，并通过聚类分析识别具有相似模块效应模式的问题类别。主要实验结果表明，不同问题类别中模块的重要性差异较小，这意味着粒子群优化性能主要由少数关键模块驱动，这一发现有助于简化算法配置过程并指导未来研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Reinforcement Learning for Power Grids: A Comprehensive Survey</div>
<div class="meta-line">Authors: Mohamed Hassouna, Clara Holzhüter, Pawel Lytaev, Josephine Thomas, Bernhard Sick, Christoph Scholz</div>
<div class="meta-line">First: 2024-07-05T14:07:15+00:00 · Latest: 2026-01-07T15:09:22+00:00</div>
<div class="meta-line">Comments: Accepted in Energy &amp; AI, in-press</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.04522v4">Abs</a> · <a href="https://arxiv.org/pdf/2407.04522v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing share of renewable energy and distributed electricity generation requires the development of deep learning approaches to address the lack of flexibility inherent in traditional power grid methods. In this context, Graph Neural Networks are a promising solution due to their ability to learn from graph-structured data. Combined with Reinforcement Learning, they can be used as control approaches to determine remedial actions. This review analyses how Graph Reinforcement Learning can improve representation learning and decision-making in power grid applications, particularly transmission and distribution grids. We analyze the reviewed approaches in terms of the graph structure, the Graph Neural Network architecture, and the Reinforcement Learning approach. Although Graph Reinforcement Learning has demonstrated adaptability to unpredictable events and noisy data, its current stage is primarily proof-of-concept, and it is not yet deployable to real-world applications. We highlight the open challenges and limitations for real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向电力系统的图强化学习：全面综述</div>
<div class="mono" style="margin-top:8px">可再生能源与分布式发电占比的不断提升，要求开发深度学习方法来应对传统电网方法固有的灵活性不足问题。在此背景下，图神经网络因其能从图结构数据中学习的能力而成为前景广阔的解决方案。结合强化学习技术，它们可作为控制方法来确定补救措施。本综述分析了图强化学习如何提升电网应用（特别是输配电网络）中的表征学习与决策能力。我们从图结构、图神经网络架构和强化学习方法三个维度对现有研究进行了系统分析。尽管图强化学习已展现出对不可预测事件和噪声数据的适应能力，但其现阶段主要处于概念验证期，尚未能部署于实际应用。我们重点指出了实际应用面临的开放挑战与局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey is motivated by the need for more flexible control methods in power grids due to rising renewable energy integration, where traditional approaches fall short. The method examined is Graph Reinforcement Learning, which combines Graph Neural Networks to learn from grid topology with Reinforcement Learning for decision-making, analyzing specific implementations based on graph structure, network architecture, and learning approach. The main experimental findings indicate that while this combined approach shows promise in handling unpredictable events and noisy data for both transmission and distribution grids, current research remains largely proof-of-concept and not yet ready for real-world deployment, with significant open challenges identified.</div>
<div class="mono" style="margin-top:8px">本综述的动机是，随着可再生能源占比提高，传统电网方法缺乏灵活性，需要开发更灵活的控制方法。其探讨的方法是图强化学习，该方法结合了图神经网络（用于从电网拓扑结构学习）和强化学习（用于决策制定），并从图结构、网络架构和学习方法角度分析了具体实现。主要实验结果表明，尽管这种结合方法在处理输配电电网中的不可预测事件和噪声数据方面展现出潜力，但当前研究主要处于概念验证阶段，尚未能部署于实际应用，且存在明显的开放性挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models</div>
<div class="meta-line">Authors: Wei Wu, Liyi Chen, Congxi Xiao, Tianfu Wang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Hui Xiong</div>
<div class="meta-line">First: 2026-01-07T14:31:07+00:00 · Latest: 2026-01-07T14:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03969v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03969v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>抗长度偏移：面向高效推理模型训练的动态异常截断方法</div>
<div class="mono" style="margin-top:8px">通过可验证奖励的强化学习增强的大型推理模型，凭借扩展其思维链已取得显著性能提升。然而，该范式在简单查询上常表现出过度冗长，导致部署成本大幅增加。现有依赖显式长度惩罚的高效推理方法常引发优化冲突，且对驱动过度思考的生成机制缺乏深入探究。本文发现一种称为&#x27;长度偏移&#x27;的现象：模型在训练过程中对简单输入逐渐产生不必要的推理。为此，我们提出动态异常截断（DOT），这是一种训练时干预方法，能选择性抑制冗余标记。该方法仅针对完全正确的推演组中响应长度的极端尾部进行处理，同时保留对复杂问题的长程推理能力。为配合此干预并确保稳定收敛，我们进一步引入辅助KL正则化与预测性动态采样。多模型规模的实验结果表明，我们的方法显著拓展了效率-性能的帕累托前沿。值得注意的是，在AIME-24基准上，本方法在比初始策略提升准确率的同时减少78%的推理标记使用量，且优于当前最先进的高效推理方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the issue of length shift, where large reasoning models trained with reinforcement learning become excessively verbose on simple queries, increasing deployment costs. To address this, the authors propose Dynamic Outlier Truncation (DOT), a training-time method that selectively truncates redundant tokens from the extreme tail of response lengths in correct rollouts, while preserving long-horizon reasoning for complex problems, and they enhance it with auxiliary KL regularization and predictive dynamic sampling for stable convergence. Experimental results across multiple model scales show that the approach significantly improves the efficiency-performance Pareto frontier, reducing inference token usage by 78% on AIME-24 while increasing accuracy compared to the initial policy and outperforming state-of-the-art efficient reasoning methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决长度偏移现象，即通过强化学习训练的大型推理模型在简单查询上变得过于冗长，从而增加了部署成本。为此，作者提出了动态异常截断方法，这是一种训练时干预技术，选择性地截断完全正确响应组中长度极端尾部的冗余标记，同时保留复杂问题的长程推理能力，并辅以辅助KL正则化和预测性动态采样以确保稳定收敛。实验结果表明，该方法在多模型规模上显著扩展了效率-性能帕累托前沿，在AIME-24上推理标记使用量减少了78%，同时准确率相比初始策略有所提升，并超越了现有高效推理方法。</div>
</details>
</div>
<div class="card">
<div class="title">Online Action-Stacking Improves Reinforcement Learning Performance for Air Traffic Control</div>
<div class="meta-line">Authors: Ben Carvell, George De Ath, Eseoghene Benjamin, Richard Everson</div>
<div class="meta-line">First: 2026-01-07T14:28:37+00:00 · Latest: 2026-01-07T14:28:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04287v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04287v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce online action-stacking, an inference-time wrapper for reinforcement learning policies that produces realistic air traffic control commands while allowing training on a much smaller discrete action space. Policies are trained with simple incremental heading or level adjustments, together with an action-damping penalty that reduces instruction frequency and leads agents to issue commands in short bursts. At inference, online action-stacking compiles these bursts of primitive actions into domain-appropriate compound clearances. Using Proximal Policy Optimisation and the BluebirdDT digital twin platform, we train agents to navigate aircraft along lateral routes, manage climb and descent to target flight levels, and perform two-aircraft collision avoidance under a minimum separation constraint. In our lateral navigation experiments, action stacking greatly reduces the number of issued instructions relative to a damped baseline and achieves comparable performance to a policy trained with a 37-dimensional action space, despite operating with only five actions. These results indicate that online action-stacking helps bridge a key gap between standard reinforcement learning formulations and operational ATC requirements, and provides a simple mechanism for scaling to more complex control scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线动作堆叠提升空中交通管制强化学习性能</div>
<div class="mono" style="margin-top:8px">本文提出在线动作堆叠技术——一种强化学习策略的推理时封装方法，能在训练时使用极简离散动作空间的同时，生成符合实际需求的空管指令。策略训练采用简单的航向或高度增量调整，配合动作阻尼惩罚机制以降低指令频率，使智能体以短脉冲形式发出指令。推理阶段，在线动作堆叠将这些基础动作脉冲编译为符合空管领域的复合放行指令。通过近端策略优化算法与BluebirdDT数字孪生平台，我们训练智能体执行航空器侧向航线导航、管理升降至目标飞行高度层，并在最小间隔约束下实现双机冲突解脱。侧向导航实验表明：相较于阻尼基线方法，动作堆叠大幅减少指令发布数量；仅使用五个动作维度的策略即可达到与37维动作空间训练策略相当的性能。这些结果表明，在线动作堆叠技术有效弥合了标准强化学习框架与实战空管需求间的关键鸿沟，为扩展至更复杂控制场景提供了简洁机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to bridge the gap between standard reinforcement learning formulations and operational air traffic control (ATC) requirements, where realistic compound commands are essential. The method introduces online action-stacking, an inference-time wrapper that trains policies on a small discrete action space using simple incremental adjustments and an action-damping penalty to reduce instruction frequency, then compiles bursts of primitive actions into domain-appropriate compound clearances during inference. Experimental results using Proximal Policy Optimisation on the BluebirdDT platform show that in lateral navigation tasks, action stacking significantly reduces the number of issued instructions compared to a damped baseline and achieves performance comparable to a policy trained with a much larger action space, despite using only five actions, demonstrating its effectiveness for scaling to complex ATC scenarios.</div>
<div class="mono" style="margin-top:8px">本文的动机在于弥合标准强化学习框架与空中交通管制实际需求之间的差距，后者需要符合操作规范的复合指令。方法提出了在线动作堆叠，这是一种推理时封装技术，通过使用简单的增量调整和动作阻尼惩罚来减少指令频率，在小规模离散动作空间上训练策略，并在推理时将原始动作序列编译为符合领域要求的复合指令。基于Proximal Policy Optimisation和BluebirdDT数字孪生平台的实验结果表明，在横向导航任务中，动作堆叠相比阻尼基线显著减少了指令发布数量，并且尽管仅使用五个动作，其性能与在更大动作空间上训练的策略相当，证明了该方法可有效扩展到更复杂的空中交通管制场景。</div>
</details>
</div>
<div class="card">
<div class="title">A Future Capabilities Agent for Tactical Air Traffic Control</div>
<div class="meta-line">Authors: Paul Kent, George De Ath, Martin Layton, Allen Hart, Richard Everson, Ben Carvell</div>
<div class="meta-line">First: 2026-01-07T14:19:46+00:00 · Latest: 2026-01-07T14:19:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04285v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Escalating air traffic demand is driving the adoption of automation to support air traffic controllers, but existing approaches face a trade-off between safety assurance and interpretability. Optimisation-based methods such as reinforcement learning offer strong performance but are difficult to verify and explain, while rules-based systems are transparent yet rarely check safety under uncertainty. This paper outlines Agent Mallard, a forward-planning, rules-based agent for tactical control in systemised airspace that embeds a stochastic digital twin directly into its conflict-resolution loop. Mallard operates on predefined GPS-guided routes, reducing continuous 4D vectoring to discrete choices over lanes and levels, and constructs hierarchical plans from an expert-informed library of deconfliction strategies. A depth-limited backtracking search uses causal attribution, topological plan splicing, and monotonic axis constraints to seek a complete safe plan for all aircraft, validating each candidate manoeuvre against uncertain execution scenarios (e.g., wind variation, pilot response, communication loss) before commitment.
  Preliminary walkthroughs with UK controllers and initial tests in the BluebirdDT airspace digital twin indicate that Mallard&#x27;s behaviour aligns with expert reasoning and resolves conflicts in simplified scenarios. The architecture is intended to combine model-based safety assessment, interpretable decision logic, and tractable computational performance in future structured en-route environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向战术空中交通控制的未来能力智能体</div>
<div class="mono" style="margin-top:8px">日益增长的空中交通需求正推动自动化技术应用于空管支持，但现有方法面临安全保证与可解释性之间的权衡。基于优化的方法（如强化学习）性能优越但难以验证和解释，而基于规则的系统虽透明却很少检验不确定性下的安全性。本文提出Agent Mallard——一种面向系统化空域战术控制的前向规划规则智能体，它将随机数字孪生直接嵌入冲突解决循环。Mallard基于预定义的GPS引导航路运行，将连续的4D矢量控制简化为对航路层级的离散选择，并通过专家知识构建的冲突解脱策略库生成分层计划。采用深度受限的回溯搜索机制，结合因果归因、拓扑计划拼接和单调轴约束，为所有航空器寻找完整安全计划，并在执行前针对不确定场景（如风变、飞行员响应、通信中断）验证每个候选机动方案。
与英国空管人员的初步推演及BluebirdDT空域数字孪生中的测试表明，Mallard的行为符合专家推理逻辑，能在简化场景中解决冲突。该架构旨在为未来结构化航路环境融合基于模型的安全评估、可解释决策逻辑与可管控的计算性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the trade-off between safety and interpretability in air traffic control automation by developing a rules-based agent that can operate safely under uncertainty. The method involves Agent Mallard, a forward-planning agent that uses a stochastic digital twin within its conflict-resolution loop, operating on predefined routes to simplify decision-making into discrete lane and level choices, and employing a depth-limited backtracking search with causal attribution and plan splicing to generate safe plans validated against uncertain execution scenarios. Main experimental results from preliminary walkthroughs with UK controllers and tests in the BluebirdDT digital twin show that Mallard&#x27;s behavior aligns with expert reasoning and successfully resolves conflicts in simplified scenarios, aiming to combine safety assessment, interpretable logic, and computational tractability for future structured airspace.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过开发一种能在不确定性下安全运行的基于规则的智能体，解决空中交通管制自动化中安全性与可解释性之间的权衡问题。方法上提出了Agent Mallard，这是一种前向规划智能体，在其冲突解决循环中嵌入随机数字孪生，基于预定义航线将决策简化为离散的航线和高度选择，并采用深度受限回溯搜索结合因果归因和计划拼接来生成安全计划，且在实施前针对不确定执行场景（如风变、飞行员响应、通信丢失）进行验证。主要实验结果来自与英国管制员的初步演练及在BluebirdDT数字孪生中的测试，表明Mallard的行为符合专家推理，能在简化场景中成功解决冲突，旨在为未来结构化空域结合基于模型的安全评估、可解释决策逻辑和可计算性能。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training</div>
<div class="meta-line">Authors: Chi Liu, Xin Chen</div>
<div class="meta-line">First: 2026-01-07T13:04:52+00:00 · Latest: 2026-01-07T13:04:52+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03895v1">PDF</a> · <a href="https://github.com/chi2liu/ABC-GRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model&#x27;s exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自适应边界裁剪GRPO：通过有界比率确保稳定且可泛化的训练</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）已成为大语言模型（LLM）强化学习的主流算法。然而，通过分析其裁剪机制，我们认为其在某些场景下存在不足。经过适当改进，GRPO的灵活性与泛化能力均可显著提升。为此，我们提出自适应边界裁剪GRPO（ABC-GRPO），这是对原始GRPO框架的非对称自适应改进。实验表明，在基于Qwen3大语言模型的数学推理任务中，ABC-GRPO较标准GRPO具有更优性能。此外，ABC-GRPO在训练全程保持显著更高的熵值，从而维持模型探索能力并缓解早熟收敛问题。为便于复现，实现代码已开源：https://github.com/chi2liu/ABC-GRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that the clipping mechanism in Group Relative Policy Optimization (GRPO), a popular reinforcement learning algorithm for large language models, is suboptimal in certain scenarios, limiting flexibility and generalization. To address this, the authors propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original framework that ensures bounded ratios for more stable training. Experimental results on mathematical reasoning tasks using Qwen3 LLMs demonstrate that ABC-GRPO achieves superior performance over standard GRPO, while also maintaining substantially higher entropy throughout training to preserve exploration capacity and mitigate premature convergence.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到用于大语言模型的流行强化学习算法——组相对策略优化（GRPO）中的裁剪机制在某些场景下并非最优，限制了灵活性和泛化能力。为此，作者提出了自适应边界裁剪GRPO（ABC-GRPO），这是对原始框架的一种非对称自适应改进，通过确保有界比率来实现更稳定的训练。在基于Qwen3大语言模型的数学推理任务上的实验结果表明，ABC-GRPO相比标准GRPO取得了更优的性能，同时在训练过程中保持了显著更高的熵，从而保留了模型的探索能力并缓解了过早收敛的问题。</div>
</details>
</div>
<div class="card">
<div class="title">Practitioner Motives to Use Different Hyperparameter Optimization Methods</div>
<div class="meta-line">Authors: Niclas Kannengießer, Niklas Hasebrook, Felix Morsbach, Marc-André Zöller, Jörg Franke, Marius Lindauer, Frank Hutter, Ali Sunyaev</div>
<div class="meta-line">Venue: ACM Transactions on Computer-Human Interaction, Volume 32, Issue 6, 2025</div>
<div class="meta-line">First: 2022-03-03T13:55:38+00:00 · Latest: 2026-01-07T12:27:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2203.01717v5">Abs</a> · <a href="https://arxiv.org/pdf/2203.01717v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Programmatic hyperparameter optimization (HPO) methods, such as Bayesian optimization and evolutionary algorithms, are highly sample-efficient in identifying optimal hyperparameter configurations for machine learning (ML) models. However, practitioners frequently use less efficient methods, such as grid search, which can lead to under-optimized models. We suspect this behavior is driven by a range of practitioner-specific motives. Practitioner motives, however, still need to be clarified to enhance user-centered development of HPO tools. To uncover practitioner motives to use different HPO methods, we conducted 20 semi-structured interviews and an online survey with 49 ML experts. By presenting main goals (e.g., increase ML model understanding) and contextual factors affecting practitioners&#x27; selection of HPO methods (e.g., available computer resources), this study offers a conceptual foundation to better understand why practitioners use different HPO methods, supporting development of more user-centered and context-adaptive HPO tools in automated ML.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从业者采用不同超参数优化方法的动机</div>
<div class="mono" style="margin-top:8px">程序化超参数优化（HPO）方法，如贝叶斯优化和进化算法，在寻找机器学习模型最优超参数配置时具有极高的样本效率。然而，从业者常使用效率较低的方法（如网格搜索），可能导致模型优化不足。我们推测这一行为源于一系列从业者特有的动机。为促进以用户为中心的HPO工具开发，需明确这些动机。为此，我们通过20次半结构化访谈和一项针对49位机器学习专家的在线调查，揭示了从业者选择不同HPO方法的动机。本研究通过呈现主要目标（如提升对机器学习模型的理解）及影响HPO方法选择的背景因素（如可用计算资源），为理解从业者行为提供了概念基础，有助于开发更以用户为中心、适应上下文的自动化机器学习HPO工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates why machine learning practitioners often choose less efficient hyperparameter optimization (HPO) methods like grid search over more sample-efficient alternatives such as Bayesian optimization, hypothesizing that this behavior is driven by diverse practitioner-specific motives. Through 20 semi-structured interviews and an online survey with 49 ML experts, the research identifies key goals (e.g., enhancing model understanding) and contextual factors (e.g., computational resources) that influence method selection. The results provide a conceptual foundation for understanding practitioner motives, aiming to support the development of more user-centered and context-adaptive HPO tools in automated machine learning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了机器学习从业者为何经常选择网格搜索等低效超参数优化方法，而非贝叶斯优化等更高效的替代方案，假设这种行为是由从业者特定的多样化动机驱动的。通过对49位机器学习专家进行20次半结构化访谈和在线调查，该研究确定了影响方法选择的关键目标（如增强模型理解）和情境因素（如计算资源）。研究结果为理解从业者动机提供了概念基础，旨在支持开发更以用户为中心、适应情境的自动化机器学习超参数优化工具。</div>
</details>
</div>
<div class="card">
<div class="title">Parametric Expensive Multi-Objective Optimization via Generative Solution Modeling</div>
<div class="meta-line">Authors: Tingyang Wei, Jiao Liu, Abhishek Gupta, Chin Chun Ooi, Puay Siew Tan, Yew-Soon Ong</div>
<div class="meta-line">First: 2025-11-12T15:13:27+00:00 · Latest: 2026-01-07T12:19:33+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09598v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.09598v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world applications require solving families of expensive multi-objective optimization problems~(EMOPs) under varying operational conditions. This gives rise to parametric expensive multi-objective optimization problems (P-EMOPs) where each task parameter defines a distinct optimization instance. Current multi-objective Bayesian optimization methods have been widely used for finding finite sets of Pareto optimal solutions for individual tasks. However, P-EMOPs present a fundamental challenge: the continuous task parameter space can contain infinite distinct problems, each requiring separate expensive evaluations. This demands learning an inverse model that can directly predict optimized solutions for any task-preference query without expensive re-evaluation. This paper introduces a novel parametric multi-task multi-objective Bayesian optimizer that learns this inverse model by alternating between (1) acquisition-driven search leveraging inter-task synergies and (2) generative solution sampling via conditional generative models. This approach enables efficient optimization across related tasks and finally achieves direct solution prediction for unseen parameterized EMOPs without additional expensive evaluations. We theoretically justify the faster convergence by leveraging inter-task synergies through task-aware Gaussian processes. Meanwhile, based on that, empirical studies of our optimizer and inverse model in synthetic and real-world benchmarks further verify the effectiveness of the proposed generative alternating framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于生成式解建模的参数化昂贵多目标优化</div>
<div class="mono" style="margin-top:8px">许多实际应用需要在不同操作条件下求解一系列昂贵的多目标优化问题（EMOPs），由此产生了参数化昂贵多目标优化问题（P-EMOPs），其中每个任务参数定义一个独立的优化实例。当前多目标贝叶斯优化方法已广泛用于为单个任务寻找有限的帕累托最优解集。然而，P-EMOPs面临根本性挑战：连续的任务参数空间可能包含无限个不同问题，每个问题都需要独立的昂贵评估。这要求学习一个能够直接预测任意任务-偏好查询优化解的逆模型，而无需昂贵的重复评估。本文提出一种新颖的参数化多任务多目标贝叶斯优化器，通过交替进行（1）利用任务间协同效应的采集驱动搜索，以及（2）通过条件生成模型进行生成式解采样，来学习该逆模型。该方法实现了跨相关任务的高效优化，并最终能够直接预测未见参数化EMOPs的解，无需额外昂贵评估。我们通过任务感知高斯过程利用任务间协同效应，从理论上证明了更快的收敛速度。同时，基于此，我们在合成与真实基准测试中对优化器及逆模型进行的实证研究，进一步验证了所提生成式交替框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of parametric expensive multi-objective optimization problems (P-EMOPs), where infinite distinct optimization tasks arise from varying operational conditions, each requiring costly evaluations. The proposed method introduces a parametric multi-task Bayesian optimizer that alternates between an acquisition-driven search, which leverages synergies across tasks using task-aware Gaussian processes for faster convergence, and generative solution sampling via conditional models to learn an inverse mapping. Experimental results on synthetic and real-world benchmarks demonstrate that this generative alternating framework effectively enables direct prediction of optimized solutions for unseen parameterized problems without additional expensive evaluations.</div>
<div class="mono" style="margin-top:8px">本文针对参数化昂贵多目标优化问题（P-EMOPs）的挑战展开研究，该问题中由于操作条件变化会产生无限个不同的优化任务，每个任务都需要昂贵的评估成本。所提出的方法引入了一种参数化多任务贝叶斯优化器，它交替进行两种操作：一是利用任务感知高斯过程跨任务协同的采集驱动搜索以加速收敛，二是通过条件生成模型进行生成式解采样，从而学习一个逆映射。在合成和真实世界基准测试中的实验结果表明，该生成式交替框架能够有效实现对未见参数化问题的优化解的直接预测，而无需额外的昂贵评估。</div>
</details>
</div>
<div class="card">
<div class="title">ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition</div>
<div class="meta-line">Authors: Muyang Zhao, Qi Qi, Hao Sun</div>
<div class="meta-line">First: 2026-01-07T11:30:55+00:00 · Latest: 2026-01-07T11:30:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03822v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03822v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ROI推理：基于预计算元认知的推理过程理性优化</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在充足计算资源下可实现强大的推理性能，但其本身无法预知任务所需计算量。本研究针对严格全局token约束下的多任务推理场景，将其形式化为有序随机多选择背包问题（OS-MCKP）。该视角揭示了元认知需求——预测任务难度、估算投资回报率（ROI）并策略性分配计算资源。我们提出ROI推理框架，通过两阶段机制赋予LLMs内在的预算感知理性能力：第一阶段通过元认知微调使模型在生成前预测推理成本与期望效用，实现显式的求解/跳过决策；第二阶段通过理性感知强化学习在硬性token预算下优化序列决策，使模型习得长期视野的资源分配策略。在预算约束数学推理基准测试中，ROI推理框架在严格计算预算下持续提升总体得分，同时显著降低决策遗憾度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling large language models (LLMs) to perform efficient reasoning under strict computational budgets, as they lack inherent awareness of task difficulty and optimal resource allocation. The authors formalize this as an Ordered Stochastic Multiple-Choice Knapsack Problem and propose ROI-Reasoning, a two-stage framework that first uses meta-cognitive fine-tuning to predict reasoning cost and utility, then applies rationality-aware reinforcement learning to optimize sequential decision-making under token constraints. Experimental results on budgeted mathematical reasoning benchmarks show that ROI-Reasoning consistently improves overall scores while significantly reducing regret when computation is limited.</div>
<div class="mono" style="margin-top:8px">该论文针对大语言模型在严格计算预算下缺乏任务难度感知和资源优化能力的问题，研究如何实现高效的推理。作者将其形式化为有序随机多选择背包问题，并提出了ROI-Reasoning框架，该框架通过元认知微调预测推理成本和预期效用，再结合理性感知强化学习在令牌约束下优化序列决策。在预算数学推理基准测试中，ROI-Reasoning在计算受限时显著降低了遗憾值，同时持续提升了整体得分。</div>
</details>
</div>
<div class="card">
<div class="title">WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</div>
<div class="meta-line">Authors: Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead</div>
<div class="meta-line">First: 2026-01-05T09:35:11+00:00 · Latest: 2026-01-07T11:21:44+00:00</div>
<div class="meta-line">Comments: Slightly modified format; added Table 3 for better illustration of the scaling results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02439v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent&#x27;s own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebGym：基于真实任务的视觉网页智能体规模化训练环境</div>
<div class="mono" style="margin-top:8px">我们推出WebGym，这是迄今为止最大的开源真实视觉网页智能体训练环境。真实网站具有非稳态和多样性特征，使得人工或小规模任务集难以支撑稳健的策略学习。WebGym包含近30万个任务，通过标准化评估体系覆盖多样化的真实网站及难度等级。我们采用简洁的强化学习方案训练智能体：基于智能体自身交互轨迹进行训练，以任务奖励作为学习反馈。为实现强化学习的规模化，我们专门为网页智能体开发了高吞吐异步轨迹采样系统，将WebGym的轨迹采样速度较基础实现提升4-5倍。其次，我们通过拓展任务集的广度、深度和规模实现持续性能提升。在WebGym上对强基础视觉语言模型Qwen-3-VL-8B-Instruct进行微调后，其在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的智能体。该提升具有实质性意义，因为我们的测试集完全由训练阶段未见的网站任务构成，这与多数现有视觉网页智能体训练研究形成鲜明对比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for WebGym is to address the insufficiency of artificial or small-scale task sets for training robust visual web agents, given the non-stationary and diverse nature of real websites. The method involves creating a large-scale open-source environment with nearly 300,000 tasks across real-world websites, using a reinforcement learning recipe that trains on agent interaction traces with task rewards as feedback, and implementing a high-throughput asynchronous rollout system to speed up trajectory sampling. The main experimental results show that this system achieves a 4-5x rollout speedup compared to naive implementations, and fine-tuning the Qwen-3-VL-8B-Instruct model on WebGym improves the success rate on an out-of-distribution test set from 26.2% to 42.9%, outperforming agents based on proprietary models like GPT-4o and GPT-5-Thinking.</div>
<div class="mono" style="margin-top:8px">WebGym的动机在于，鉴于真实网站的非平稳性和多样性，人工或小规模任务集不足以训练鲁棒的视觉网页智能体。其方法包括创建一个大规模开源环境，包含近30万个跨真实网站的任务，采用强化学习方案，利用智能体交互轨迹和任务奖励作为反馈进行训练，并开发了一个高吞吐量的异步轨迹采样系统以加速训练。主要实验结果表明，该系统相比简单实现实现了4-5倍的轨迹采样加速，且在WebGym上微调Qwen-3-VL-8B-Instruct模型后，在未见网站任务测试集上的成功率从26.2%提升至42.9%，优于基于GPT-4o和GPT-5-Thinking等专有模型的智能体。</div>
</details>
</div>
<div class="card">
<div class="title">Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning</div>
<div class="meta-line">Authors: Pritthijit Nath, Sebastian Schemm, Henry Moss, Peter Haynes, Emily Shuckburgh, Mark J. Webb</div>
<div class="meta-line">First: 2026-01-07T11:19:16+00:00 · Latest: 2026-01-07T11:19:16+00:00</div>
<div class="meta-line">Comments: 66 pages, 22 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04268v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Weather and climate models rely on parametrisations to represent unresolved sub-grid processes. Traditional schemes rely on fixed coefficients that are weakly constrained and tuned offline, contributing to persistent biases that limit their ability to adapt to the underlying physics. This study presents a framework that learns components of parametrisation schemes online as a function of the evolving model state using reinforcement learning (RL) and evaluates the resulting RL-driven parameter updates across a hierarchy of idealised testbeds spanning a simple climate bias correction (SCBC), a radiative-convective equilibrium (RCE), and a zonal mean energy balance model (EBM) with both single-agent and federated multi-agent settings. Across nine RL algorithms, Truncated Quantile Critics (TQC), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3) achieved the highest skill and the most stable convergence across configurations, with performance assessed against a static baseline using area-weighted RMSE, temperature profile and pressure-level diagnostics. For the EBM, single-agent RL outperformed static parameter tuning with the strongest gains in tropical and mid-latitude bands, while federated RL on multi-agent setups enabled geographically specialised control and faster convergence, with a six-agent DDPG configuration using frequent aggregation yielding the lowest area-weighted RMSE across the tropics and mid-latitudes. The learnt corrections were also physically meaningful as agents modulated EBM radiative parameters to reduce meridional biases, adjusted RCE lapse rates to match vertical temperature errors, and stabilised SCBC heating increments to limit drift. Overall, results highlight RL to deliver skilful state-dependent, and regime-aware parametrisations, offering a scalable pathway for online learning within numerical models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习实现天气与气候模型中可调参数的状态依赖性</div>
<div class="mono" style="margin-top:8px">天气与气候模型依赖参数化方案表征未解析的次网格过程。传统方案采用离线弱约束调谐的固定系数，导致持续偏差并限制其适应底层物理过程的能力。本研究提出一种框架，利用强化学习在线学习参数化方案中随模型状态演化的分量，并在从简单气候偏差修正、辐射对流平衡到纬向平均能量平衡模型的理想化测试体系中，评估单智能体与联邦多智能体设置下的强化学习驱动参数更新。在九种强化学习算法中，截断分位数批评家、深度确定性策略梯度及其孪生延迟版本在各类配置中展现出最高的技能与最稳定的收敛性，其性能通过面积加权均方根误差、温度廓线和气压层诊断指标与静态基线对比评估。在能量平衡模型中，单智能体强化学习在热带和中纬度带改进最显著，优于静态参数调谐；而多智能体联邦强化学习实现了地理专一化控制和更快收敛，其中采用频繁聚合的六智能体深度确定性策略梯度配置在热带和中纬度区域获得最低面积加权均方根误差。习得的修正具有物理意义：智能体通过调节能量平衡模型辐射参数减少经向偏差，调整辐射对流平衡递减率以匹配垂直温度误差，并稳定简单气候偏差修正的加热增量以限制漂移。总体表明强化学习能够提供具备状态依赖性和区域感知能力的参数化方案，为数值模型中的在线学习提供可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the persistent biases in weather and climate models caused by traditional, fixed parameterizations by introducing a reinforcement learning (RL) framework that learns state-dependent parameter updates online. The method employs various RL algorithms, including Truncated Quantile Critics (TQC), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3), across a hierarchy of idealized testbeds such as a simple climate bias correction, radiative-convective equilibrium, and an energy balance model, evaluating performance with single-agent and federated multi-agent settings. Experimental results show that RL-driven parameter tuning outperforms static baselines, with single-agent RL reducing errors in tropical and mid-latitude bands and federated multi-agent setups enabling geographically specialized control and faster convergence, while the learned corrections are physically interpretable, adjusting radiative parameters and lapse rates to mitigate biases and stabilize model outputs.</div>
<div class="mono" style="margin-top:8px">本研究针对天气和气候模型中传统固定参数化方案导致的持续偏差，提出了一个强化学习框架，能够在线学习依赖于模型状态的参数更新。该方法在简单气候偏差校正、辐射对流平衡和能量平衡模型等一系列理想化测试平台上，应用了包括截断分位数批评器、深度确定性策略梯度及其孪生延迟版本在内的多种强化学习算法，并评估了单智能体和联邦多智能体设置下的性能。实验结果表明，强化学习驱动的参数调整优于静态基准，单智能体强化学习在热带和中纬度带减少了误差，而联邦多智能体设置实现了地理专门化控制和更快收敛，同时学习到的修正具有物理意义，通过调整辐射参数和递减率来减轻偏差并稳定模型输出。</div>
</details>
</div>
<div class="card">
<div class="title">From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs</div>
<div class="meta-line">Authors: Usha Shrestha, Dmitry Ignatov, Radu Timofte</div>
<div class="meta-line">First: 2026-01-07T11:13:02+00:00 · Latest: 2026-01-07T11:13:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03808v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03808v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从暴力搜索到语义洞察：基于性能引导的LLM数据转换设计</div>
<div class="mono" style="margin-top:8px">大语言模型在代码合成方面已取得显著性能，但数据感知增强仍受限于启发式设计或暴力搜索方法。我们在NNGPT项目生态中提出一种性能感知的闭环解决方案，使LLM能够通过内化经验性能指标自主设计最优转换方案。我们采用低秩适应方法，基于包含6000多个经实证评估的PyTorch增强函数库对LLM进行微调，每个函数仅通过下游模型准确率进行标注。训练采用成对性能排序（优劣转换对比），通过经验反馈实现对齐，无需强化学习、奖励模型或符号目标。该方法大幅减少穷举搜索需求，相比暴力搜索评估候选方案数量降低高达600倍，同时保持竞争力的峰值准确率，并将生成模式从随机合成转向任务对齐设计。消融研究表明，结构化思维链提示会引入语法噪声导致性能下降，而直接提示能确保性能关键型代码任务的稳定优化。定性与定量分析表明模型内化了语义性能线索而非记忆语法规则。这些结果证明LLM能够通过非文本反馈循环展现任务级推理能力，无需依赖显式符号奖励。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of heuristic or brute-force methods in data-aware code augmentation, this paper introduces a performance-guided, closed-loop approach within the NNGPT ecosystem that enables LLMs to autonomously design optimal data transformations by learning from empirical performance feedback. The method fine-tunes LLMs using Low-Rank Adaptation on a novel dataset of over 6,000 PyTorch augmentation functions, each annotated solely by downstream model accuracy, and trains them via pairwise performance ordering to align transformations with task performance without reinforcement learning or symbolic rewards. Experimental results show this approach reduces the need for exhaustive search by up to 600x compared to brute-force methods while maintaining competitive peak accuracy, shifts generation from random synthesis to task-aligned design, and demonstrates through ablations that direct prompting outperforms structured Chain-of-Thought prompting by avoiding syntactic noise, with qualitative analyses indicating the model internalizes semantic performance cues rather than memorizing syntax.</div>
<div class="mono" style="margin-top:8px">针对数据感知代码增强中启发式或暴力方法的局限性，本文在NNGPT生态系统中提出了一种性能引导的闭环方法，使大语言模型能够通过从经验性能反馈中学习，自主设计最优数据转换。该方法基于一个包含6000多个仅以下游模型准确率标注的PyTorch增强函数的新数据集，使用低秩适应对大语言模型进行微调，并通过成对性能排序进行训练，从而无需强化学习或符号奖励即可使转换与任务性能对齐。实验结果表明，与暴力方法相比，该方法将搜索需求降低了高达600倍，同时保持了有竞争力的峰值准确率，并将生成从随机合成转向任务对齐的设计；消融研究显示直接提示优于结构化思维链提示，避免了句法噪声，定性分析表明模型内化了语义性能线索而非记忆句法。</div>
</details>
</div>
<div class="card">
<div class="title">NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning</div>
<div class="meta-line">Authors: Zhongtao Miao, Kaiyan Zhao, Masaaki Nagata, Yoshimasa Tsuruoka</div>
<div class="meta-line">First: 2026-01-07T10:49:00+00:00 · Latest: 2026-01-07T10:49:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03790v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03790v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging &quot;translation difficulty&quot; to further improve the translation quality of translation agents using our search tool.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NeoAMT：基于强化学习的新词感知智能机器翻译框架</div>
<div class="mono" style="margin-top:8px">新词感知机器翻译旨在将包含新词的源语句翻译为目标语言。与通用机器翻译相比，该领域研究尚不充分。本文提出一种基于维基词典检索工具的智能框架NeoAMT，用于新词感知机器翻译。具体而言，我们首先构建了覆盖16种语言、75个翻译方向的新词感知机器翻译数据集，该数据集源自约1000万条英文维基词典记录；同时基于约300万条清洗后的维基词典记录构建了检索工具的语料库。随后利用该工具，通过强化学习训练翻译智能体，并评估新词感知机器翻译的准确性。在此基础上，进一步提出一种强化学习训练框架，通过引入“翻译难度”概念设计新型奖励机制与自适应推演生成方法，以提升使用本检索工具的翻译智能体的译文质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underexplored challenge of translating neologisms in machine translation by proposing NeoAMT, an agentic framework that integrates a Wiktionary-based search tool. The method involves creating a new multilingual dataset from English Wiktionary and developing a retrieval tool, followed by training a translation agent using reinforcement learning with a novel reward design and adaptive rollout generation based on translation difficulty. Experimental results demonstrate improved translation accuracy for sentences containing neologisms across 16 languages and 75 translation directions.</div>
<div class="mono" style="margin-top:8px">本文针对机器翻译中未充分探索的新词翻译问题，提出了NeoAMT这一基于维基词典搜索工具的智能体框架。方法包括利用英语维基词典构建新的多语言数据集和检索工具，并通过强化学习训练翻译智能体，其中设计了新颖的奖励机制和基于翻译难度的自适应生成策略。实验结果表明，该框架在涵盖16种语言和75个翻译方向的任务中，有效提升了包含新词的句子的翻译准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Differential Evolution via Nonlinear Population Size Reduction and Adaptive Restart: The ARRDE Algorithm</div>
<div class="meta-line">Authors: Khoirul Faiq Muzakka, Ahsani Hafizhu Shali, Haris Suhendar, Sören Möller, Martin Finsterbusch</div>
<div class="meta-line">First: 2025-11-23T12:50:25+00:00 · Latest: 2026-01-07T10:20:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18429v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18429v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study is motivated by a robustness issue in numerical optimization of bound-constrained problems: many algorithms that perform well on a particular benchmark suite, such as the IEEE CEC2017 problems, struggle to maintain the same level of performance when applied to other suites that differ in dimensionality, landscape complexity, or the maximum number of function evaluations ($N_{\text{max}}$). To address this, we propose the Adaptive Restart-Refine Differential Evolution (ARRDE) algorithm, a new variant of Differential Evolution (DE). ARRDE builds upon the LSHADE algorithm, incorporates key mechanisms from jSO, and introduces a nonlinear population-size reduction strategy combined with an adaptive restart-refine mechanism.
  We evaluate ARRDE on five benchmark suites (CEC2011, CEC2017, CEC2019, CEC2020, and CEC2022) which, to the best of our knowledge, constitutes the most extensive experimental study to date in the context of algorithmic comparison, as most prior works consider only one or two suites. This broad evaluation enables a rigorous assessment of generalization across markedly different problem characteristics. To further support fair cross-suite comparisons, we also introduce a bounded accuracy-based scoring metric derived from relative error. Using both rank-based and accuracy-based metrics, and comparing against algorithms that perform strongly on CEC2017 (e.g., jSO and LSHADE-cnEpSin) as well as those that excel on CEC2020 (e.g., j2020 and NLSHADE-RSP), ARRDE consistently demonstrates top-tier performance, ranking first across all benchmark suites considered. These results highlight ARRDE&#x27;s robustness and its superior generalization capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于非线性种群规模缩减与自适应重启的鲁棒差分进化算法：ARRDE算法</div>
<div class="mono" style="margin-top:8px">本研究针对边界约束问题数值优化中的鲁棒性问题：许多在特定基准测试集（如IEEE CEC2017问题集）表现优异的算法，在应用于维度、景观复杂度或最大函数评估次数（$N_{\text{max}}$）不同的其他测试集时，难以保持同等性能。为此，我们提出自适应重启-精化差分进化（ARRDE）算法，这是差分进化（DE）的一种新变体。ARRDE基于LSHADE算法，融合了jSO的关键机制，并引入非线性种群规模缩减策略与自适应重启-精化机制。我们在五个基准测试集（CEC2011、CEC2017、CEC2019、CEC2020和CEC2022）上评估ARRDE，据我们所知，这是迄今为止算法比较领域最广泛的实验研究，因为大多数先前工作仅考虑一至两个测试集。这种广泛评估能严格检验算法在显著不同问题特征上的泛化能力。为支持公平的跨测试集比较，我们还提出基于相对误差的有界精度评分指标。通过使用基于排名和精度的指标，并与在CEC2017表现优异的算法（如jSO和LSHADE-cnEpSin）及在CEC2020表现突出的算法（如j2020和NLSHADE-RSP）对比，ARRDE在所有测试集中均展现顶级性能，排名首位。这些结果凸显了ARRDE的鲁棒性和卓越的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of robustness in numerical optimization algorithms when applied across different benchmark suites with varying problem characteristics, this paper proposes the Adaptive Restart-Refine Differential Evolution (ARRDE) algorithm. The method enhances the LSHADE algorithm by incorporating mechanisms from jSO and introducing a nonlinear population-size reduction strategy paired with an adaptive restart-refine mechanism. Experimental results on five benchmark suites (CEC2011, CEC2017, CEC2019, CEC2020, and CEC2022) demonstrate that ARRDE consistently achieves top-tier performance, ranking first across all suites and highlighting its superior robustness and generalization capability.</div>
<div class="mono" style="margin-top:8px">本文针对数值优化算法在不同基准测试集上因维度、景观复杂性或评估次数差异而表现不稳健的问题，提出了自适应重启-精化差分进化算法（ARRDE）。该方法基于LSHADE算法，融合了jSO的关键机制，并引入了非线性种群规模缩减策略与自适应重启-精化机制。在五个基准测试集上的实验结果表明，ARRDE在所有测试集中均排名第一，展现了卓越的鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics</div>
<div class="meta-line">Authors: Zhuofan Shi, Hubao A, Yufei Shao, Dongliang Huang, Hongxu An, Chunxiao Xin, Haiyang Shen, Zhenyu Wang, Yunshan Na, Gang Huang, Xiang Jing</div>
<div class="meta-line">First: 2026-01-05T12:56:51+00:00 · Latest: 2026-01-07T10:06:36+00:00</div>
<div class="meta-line">Comments: 24 pages,4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02075v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.02075v3">PDF</a> · <a href="https://github.com/FredericVAN/PKU_MDAgent2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&amp;A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MDAgent2：面向分子动力学代码生成与知识问答的大语言模型</div>
<div class="mono" style="margin-top:8px">分子动力学模拟在材料科学原子尺度行为研究中至关重要，但编写LAMMPS脚本仍是高度专业化且耗时的任务。尽管大语言模型在代码生成和领域问答中展现出潜力，但其在分子动力学场景中的性能受限于领域数据稀缺、前沿大语言模型部署成本高以及代码可执行性低等问题。基于先前开发的MDAgent，我们提出首个能在分子动力学领域同时执行知识问答与代码生成的端到端框架MDAgent2。通过构建领域专用数据生成流程，我们产出涵盖分子动力学知识、问答与代码生成的三类高质量数据集。基于这些数据集，采用三阶段后训练策略——持续预训练、监督微调与强化学习——训练出两个领域适配模型MD-Instruct与MD-Code。进一步提出MD-GRPO强化学习方法，以模拟结果为奖励信号并循环利用低奖励轨迹进行持续优化。同时构建可部署的多智能体系统MDAgent2-RUNTIME，集成代码生成、执行、评估与自我修正功能。结合本文提出的首个LAMMPS代码生成与问答基准MD-EvalBench，我们的模型与系统性能超越多个强基线。本工作系统论证了大语言模型在工业仿真任务中的适应性与泛化能力，为AI for Science及工业级仿真的自动化代码生成奠定方法论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the specialized and time-consuming nature of writing LAMMPS scripts for molecular dynamics (MD) simulations, where existing large language models (LLMs) are limited by scarce domain data, high deployment costs, and low code executability. The method involves constructing three high-quality MD datasets and training domain-adapted models (MD-Instruct and MD-Code) through a three-stage post-training strategy—continued pre-training, supervised fine-tuning, and reinforcement learning (including a novel closed-loop method, MD-GRPO)—alongside a deployable multi-agent system, MDAgent2-RUNTIME, for code generation, execution, and self-correction. The main experimental results, evaluated on the proposed MD-EvalBench benchmark, show that the models and system outperform several strong baselines in both knowledge question answering and code generation, demonstrating the adaptability of LLMs to industrial simulation tasks and providing a foundation for automated code generation in AI for Science.</div>
<div class="mono" style="margin-top:8px">本工作的动机是解决分子动力学模拟中编写LAMMPS脚本的专业性和耗时性问题，现有大语言模型在该领域受限于稀缺的领域数据、高昂的部署成本和低代码可执行性。方法包括构建三个高质量的分子动力学数据集，并通过三阶段后训练策略——持续预训练、监督微调和强化学习（包含一种新颖的闭环方法MD-GRPO）——训练领域适应模型（MD-Instruct和MD-Code），同时开发了可部署的多智能体系统MDAgent2-RUNTIME，用于代码生成、执行和自我修正。主要实验结果在提出的MD-EvalBench基准测试中显示，该模型和系统在知识问答和代码生成方面均优于多个强基线，证明了大语言模型在工业模拟任务中的适应性和泛化能力，为AI for Science和工业级模拟的自动代码生成奠定了方法基础。</div>
</details>
</div>
<div class="card">
<div class="title">O-Researcher: An Open Ended Deep Research Model via Multi-Agent Distillation and Agentic RL</div>
<div class="meta-line">Authors: Yi Yao, He Zhu, Piaohong Wang, Jincheng Ren, Xinlong Yang, Qianben Chen, Xiaowan Li, Dingfeng Shi, Jiaxian Li, Qiexiang Wang, Sinuo Wang, Xinpeng Liu, Jiaqi Wu, Minghao Liu, Wangchunshu Zhou</div>
<div class="meta-line">First: 2026-01-07T09:31:10+00:00 · Latest: 2026-01-07T09:31:10+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03743v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03743v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance gap between closed-source and open-source large language models (LLMs) is largely attributed to disparities in access to high-quality training data. To bridge this gap, we introduce a novel framework for the automated synthesis of sophisticated, research-grade instructional data. Our approach centers on a multi-agent workflow where collaborative AI agents simulate complex tool-integrated reasoning to generate diverse and high-fidelity data end-to-end. Leveraging this synthesized data, we develop a two-stage training strategy that integrates supervised fine-tuning with a novel reinforcement learning method, designed to maximize model alignment and capability. Extensive experiments demonstrate that our framework empowers open-source models across multiple scales, enabling them to achieve new state-of-the-art performance on the major deep research benchmark. This work provides a scalable and effective pathway for advancing open-source LLMs without relying on proprietary data or models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>O-Researcher：基于多智能体蒸馏与智能体强化学习的开放式深度研究模型</div>
<div class="mono" style="margin-top:8px">闭源与开源大语言模型（LLMs）的性能差距主要源于高质量训练数据的获取差异。为弥合这一差距，我们提出了一种自动化合成复杂研究级指令数据的新框架。该方法采用多智能体工作流，通过协作的AI智能体模拟集成工具的复杂推理过程，端到端生成多样化、高保真度的数据。基于此合成数据，我们开发了融合监督微调与新型强化学习方法的两阶段训练策略，旨在最大化模型对齐与能力。大量实验表明，该框架能有效赋能多规模开源模型，使其在主流深度研究基准测试中达到新的最优性能。本工作为不依赖专有数据或模型的开源LLMs发展提供了可扩展的有效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance gap between closed-source and open-source large language models due to disparities in training data, this paper introduces a novel framework for automated synthesis of high-quality, research-grade instructional data. The method employs a multi-agent workflow where collaborative AI agents simulate complex, tool-integrated reasoning to generate diverse data end-to-end, followed by a two-stage training strategy combining supervised fine-tuning with a novel reinforcement learning approach to enhance model alignment and capability. Main experimental results show that this framework enables open-source models across various scales to achieve new state-of-the-art performance on a major deep research benchmark, providing a scalable pathway for advancement without proprietary resources.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决闭源与开源大语言模型之间因训练数据质量差异导致的性能差距，为此引入了一个自动化合成高质量研究级指令数据的新框架。该方法采用多智能体工作流，通过协作的AI智能体模拟复杂的工具集成推理以端到端生成多样化数据，并实施了一个结合监督微调与新型强化学习的两阶段训练策略，以提升模型的对齐性和能力。主要实验结果表明，该框架使不同规模的开源模型在核心深度研究基准测试中取得了新的最先进性能，为不依赖专有数据或模型的进步提供了一条可扩展的有效路径。</div>
</details>
</div>
<div class="card">
<div class="title">EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning</div>
<div class="meta-line">Authors: Jing-Cheng Pang, Liu Sun, Chang Zhou, Xian Tang, Haichuan Ma, Kun Jiang, Jianlong Wang, Kai Zhang, Sijie Wu, Haoran Cai, Chenwei Wu, Xubin Li, Xin Chen</div>
<div class="meta-line">First: 2026-01-07T09:20:05+00:00 · Latest: 2026-01-07T09:20:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03725v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03725v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model&#x27;s evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EDCO：面向领域特定大语言模型微调的动态课程编排框架</div>
<div class="mono" style="margin-top:8px">领域特定大语言模型通常通过对预训练通用大模型在专业数据集上进行微调而开发，代表了应用人工智能的重要进展。课程学习是常见的微调策略，其根据难度等指标对训练样本进行预排序，相比随机采样能提升学习效率。然而现有方法多采用训练前设计的静态课程，难以适应微调过程中模型动态变化的需求。为此，我们提出基于推理熵与动态课程编排两大核心概念的EDCO框架。受近期研究发现高答案熵有利于长期推理增益的启发，EDCO在持续调整的课程中优先选择高推理熵样本。该框架包含三个核心组件：利用前缀词元近似全序列熵的高效熵估计器、基于熵值筛选最高推理熵数据点的课程生成器，以及对选定课程进行模型优化的训练器。在通信、医学和法律领域的综合实验中，EDCO在监督学习与强化学习设置下微调Qwen3-4B和Llama3.2-3B模型时均优于传统课程策略。所提出的高效熵估计方法在保持高精度的同时将计算时间降低83.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitation of static curriculum learning in domain-specific LLM fine-tuning, which lacks adaptability to the model&#x27;s evolving needs during training. The method introduces EDCO, a framework that dynamically orchestrates the training curriculum based on inference entropy, prioritizing samples with high entropy to enhance learning efficiency; it includes an efficient entropy estimator using prefix tokens, an entropy-based curriculum generator, and an LLM trainer. Main experimental results show that EDCO outperforms traditional curriculum strategies in domains like communication, medicine, and law when fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings, while the efficient entropy estimation reduces computational time by 83.5% without sacrificing accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决领域特定大语言模型微调中静态课程学习的局限性，即其缺乏对训练过程中模型动态需求的适应性。方法上提出了EDCO框架，通过基于推理熵的动态课程编排来优先选择高熵样本以提升学习效率，该框架包含使用前缀标记的高效熵估计器、基于熵的课程生成器和大语言模型训练器。主要实验结果表明，在通信、医学和法律等领域微调Qwen3-4B和Llama3.2-3B模型时，EDCO在监督学习和强化学习设置下均优于传统课程策略，同时高效熵估计在保持高精度的前提下将计算时间减少了83.5%。</div>
</details>
</div>
<div class="card">
<div class="title">ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization</div>
<div class="meta-line">Authors: Shijie Zhang, Kevin Zhang, Zheyuan Gu, Xiang Guo, Rujun Guo, Shaoyu Liu, Guanjun Jiang, Xiaozhao Wang</div>
<div class="meta-line">First: 2026-01-07T09:19:53+00:00 · Latest: 2026-01-07T09:19:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03723v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03723v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \textbf{E}lastic \textbf{T}rust \textbf{R}egions (\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ETR：面向策略优化的结果导向弹性置信域方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为解锁大语言模型推理能力的重要范式，以OpenAI o1和DeepSeek-R1的成功为代表。当前，组相对策略优化凭借其稳定的训练效率和无需价值函数的特性，成为该领域的主导算法。然而，我们认为GRPO存在结构性局限：它对所有样本施加统一、静态的置信域约束。这种设计隐含了信号同质性的假设，与结果驱动学习中优势值幅度和方差显著波动的异质性本质相悖。因此，静态约束既无法充分利用高质量信号，又难以有效抑制噪声，常导致策略熵快速坍缩。为此，我们提出弹性置信域方法——一种使优化约束与信号质量动态对齐的机制。ETR通过双层弹性构建信号感知的优化空间：在微观层面，根据优势值幅度动态调整截断边界，加速高置信度路径的学习；在宏观层面，利用组方差隐式分配更大更新预算给处于最优学习区间的任务。在AIME和MATH基准上的大量实验表明，ETR持续超越GRPO，在获得更优准确率的同时有效缓解策略熵衰减，确保持续探索能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the structural limitation of Group Relative Policy Optimization (GRPO) in reinforcement learning with verifiable rewards, which uses a uniform, static trust region that misaligns with the heterogeneous nature of outcome-driven signals, often leading to rapid entropy collapse. To address this, the authors propose Elastic Trust Regions (ETR), a dynamic method that adjusts optimization constraints based on signal quality through dual-level elasticity: micro-level scaling of clipping boundaries by advantage magnitude and macro-level implicit budget allocation using group variance. Experimental results on AIME and MATH benchmarks show that ETR consistently outperforms GRPO, achieving higher accuracy while effectively mitigating policy entropy degradation to sustain exploration.</div>
<div class="mono" style="margin-top:8px">本文的动机源于可验证奖励强化学习中组相对策略优化（GRPO）的结构性局限，即其采用均匀、静态的信任区域，这与结果驱动信号的异质性不匹配，常导致策略熵快速崩溃。为解决此问题，作者提出了弹性信任区域（ETR），这是一种动态方法，通过双层次弹性使优化约束与信号质量对齐：微观层面根据优势幅度缩放裁剪边界以加速高置信路径的学习，宏观层面利用组方差隐式分配更大更新预算给处于最优学习区间的任务。在AIME和MATH基准上的大量实验表明，ETR持续优于GRPO，在实现更高准确率的同时有效缓解了策略熵退化，确保了持续的探索。</div>
</details>
</div>
<div class="card">
<div class="title">Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</div>
<div class="meta-line">Authors: NVIDIA, :, Yan Wang, Wenjie Luo, Junjie Bai, Yulong Cao, Tong Che, Ke Chen, Yuxiao Chen, Jenna Diamond, Yifan Ding, Wenhao Ding, Liang Feng, Greg Heinrich, Jack Huang, Peter Karkus, Boyi Li, Pinyi Li, Tsung-Yi Lin, Dongran Liu, Ming-Yu Liu, Langechuan Liu, Zhijian Liu, Jason Lu, Yunxiang Mao, Pavlo Molchanov, Lindsey Pavao, Zhenghao Peng, Mike Ranzinger, Ed Schmerling, Shida Shen, Yunfei Shi, Sarah Tariq, Ran Tian, Tilman Wekel, Xinshuo Weng, Tianjun Xiao, Eric Yang, Xiaodong Yang, Yurong You, Xiaohui Zeng, Wenyuan Zhang, Boris Ivanovic, Marco Pavone</div>
<div class="meta-line">First: 2025-10-30T01:25:34+00:00 · Latest: 2026-01-07T09:09:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00088v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00088v2">PDF</a> · <a href="https://huggingface.co/nvidia/Alpamayo-R1-10B">Code1</a> · <a href="https://github.com/NVlabs/alpamayo">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. We introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning for complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a vision-language model pre-trained for Physical AI, with a diffusion-based trajectory decoder that generates dynamically feasible trajectories in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to enforce reasoning-action consistency and optimize reasoning quality. AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. Model weights are available at https://huggingface.co/nvidia/Alpamayo-R1-10B with inference code at https://github.com/NVlabs/alpamayo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alpamayo-R1：通过桥接推理与行为预测实现长尾场景下可泛化的自动驾驶</div>
<div class="mono" style="margin-top:8px">基于模仿学习的端到端架构通过扩大模型规模与数据量推动了自动驾驶发展，但在监督稀疏、因果理解受限的安全关键长尾场景中，其性能仍显脆弱。我们提出Alpamayo-R1（AR1）——一种融合因果推理链与轨迹规划的视觉-语言-行为模型，针对复杂驾驶场景实现三大创新：（1）通过混合自动标注与人机协同流程构建的因果链数据集，生成与驾驶行为对齐、基于决策的因果推理轨迹；（2）模块化视觉-语言-行为架构，将预训练用于物理AI的视觉语言模型Cosmos-Reason与基于扩散的轨迹解码器结合，实时生成动态可行轨迹；（3）采用监督微调激发推理能力、强化学习确保推理-行为一致性的多阶段训练策略。相比纯轨迹基线，AR1在挑战性案例中规划准确率提升达12%，闭环仿真中近距离接触率降低35%。强化学习后训练使推理质量提升45%，推理-行为一致性提高37%。模型参数从0.5B扩展至7B时性能持续提升。实车道路测试证实其实时性（99毫秒延迟）与城市部署可行性。通过融合可解释推理与精准控制，AR1为迈向L4级自动驾驶提供了可行路径。模型权重与推理代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the brittleness of end-to-end imitation learning for autonomous driving in long-tail safety-critical scenarios by introducing Alpamayo-R1 (AR1), a vision-language-action model that integrates Chain of Causation reasoning with trajectory planning. The method involves creating a Chain of Causation dataset via a hybrid auto-labeling and human-in-the-loop pipeline, a modular architecture combining a pre-trained vision-language model with a diffusion-based trajectory decoder, and a multi-stage training strategy using supervised fine-tuning and reinforcement learning. Experimental results show AR1 achieves up to a 12% improvement in planning accuracy on challenging cases, a 35% reduction in close encounter rate in simulation, and RL post-training improves reasoning quality by 45% and reasoning-action consistency by 37%, with scaling from 0.5B to 7B parameters yielding consistent gains and on-vehicle tests confirming real-time performance.</div>
<div class="mono" style="margin-top:8px">本文针对端到端模仿学习在自动驾驶长尾安全关键场景中的脆弱性问题，提出了Alpamayo-R1（AR1）模型，这是一种将因果链推理与轨迹规划相结合的视觉-语言-动作模型。方法包括通过混合自动标注和人机协同流程构建因果链数据集、结合预训练视觉语言模型与扩散轨迹解码器的模块化架构，以及采用监督微调和强化学习的多阶段训练策略。实验结果表明，AR1在挑战性案例上的规划准确率最高提升12%，仿真中近距离接触率降低35%，强化学习后训练使推理质量提升45%、推理-动作一致性提高37%，参数从0.5B扩展到7B时性能持续提升，实车道路测试验证了其实时性能。</div>
</details>
</div>
<div class="card">
<div class="title">R$^3$L: Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification</div>
<div class="meta-line">Authors: Weijie Shi, Yanxi Chen, Zexi Li, Xuchen Pan, Yuchang Sun, Jiajie Xu, Xiaofang Zhou, Yaliang Li</div>
<div class="meta-line">First: 2026-01-07T09:04:52+00:00 · Latest: 2026-01-07T09:04:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03715v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03715v1">PDF</a> · <a href="https://github.com/shiweijiezero/R3L">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning drives recent advances in LLM reasoning and agentic capabilities, yet current approaches struggle with both exploration and exploitation. Exploration suffers from low success rates on difficult tasks and high costs of repeated rollouts from scratch. Exploitation suffers from coarse credit assignment and training instability: Trajectory-level rewards penalize valid prefixes for later errors, and failure-dominated groups overwhelm the few positive signals, leaving optimization without constructive direction. To this end, we propose R$^3$L, Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification. To synthesize high-quality trajectories, R$^3$L shifts from stochastic sampling to active synthesis via reflect-then-retry, leveraging language feedback to diagnose errors, transform failed attempts into successful ones, and reduce rollout costs by restarting from identified failure points. With errors diagnosed and localized, Pivotal Credit Assignment updates only the diverging suffix where contrastive signals exist, excluding the shared prefix from gradient update. Since failures dominate on difficult tasks and reflect-then-retry produces off-policy data, risking training instability, Positive Amplification upweights successful trajectories to ensure positive signals guide the optimization process. Experiments on agentic and reasoning tasks demonstrate 5\% to 52\% relative improvements over baselines while maintaining training stability. Our code is released at https://github.com/shiweijiezero/R3L.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>R$^3$L：基于语言引导探索、关键信用分配与正向放大的反思重试强化学习</div>
<div class="mono" style="margin-top:8px">强化学习推动了大型语言模型推理与智能体能力的近期进展，但现有方法在探索与利用两方面均面临挑战。探索方面，困难任务成功率低且从头重复执行成本高昂；利用方面，粗粒度信用分配与训练不稳定性并存：轨迹级奖励会因后续错误惩罚有效前缀，失败主导的样本群淹没少量正向信号，导致优化缺乏建设性方向。为此，我们提出R$^3$L——融合语言引导探索、关键信用分配与正向放大的反思重试强化学习。为生成高质量轨迹，R$^3$L通过“反思-重试”机制将随机采样转为主动合成：利用语言反馈诊断错误，将失败尝试转化为成功轨迹，并通过从识别出的失败点重启降低执行成本。在错误被诊断定位后，关键信用分配仅更新存在对比信号的分歧后缀，将共享前缀排除在梯度更新外。鉴于困难任务中失败样本占主导且反思-重试产生离线策略数据可能引发训练不稳定，正向放大机制通过加权成功轨迹确保优化过程受正向信号引导。在智能体与推理任务上的实验表明，该方法在保持训练稳定性的同时，相对基线获得5%至52%的性能提升。代码已发布于https://github.com/shiweijiezero/R3L。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces R³L, a reinforcement learning method designed to address exploration and exploitation challenges in LLM reasoning and agentic tasks. Its motivation stems from the inefficiency of stochastic exploration, coarse credit assignment that penalizes valid prefixes, and training instability due to failure-dominated data. The method employs a reflect-then-retry strategy using language feedback to diagnose errors and synthesize successful trajectories from failure points, alongside pivotal credit assignment that updates only diverging suffixes and positive amplification to upweight successful examples. Experimental results on agentic and reasoning tasks show relative improvements of 5% to 52% over baselines while maintaining training stability.</div>
<div class="mono" style="margin-top:8px">本文提出了R³L方法，旨在解决大型语言模型推理与智能体任务中强化学习面临的探索与利用难题。其动机源于随机探索效率低下、粗粒度信用分配惩罚有效前缀，以及失败主导数据导致的训练不稳定。该方法采用反思后重试策略，利用语言反馈诊断错误并从失败点合成成功轨迹，同时结合关键信用分配仅更新分歧后缀，并通过正样本放大增强成功轨迹权重。在智能体与推理任务上的实验表明，相比基线方法取得了5%至52%的相对性能提升，同时保持了训练稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL</div>
<div class="meta-line">Authors: Lang Cao, Hui Ruan, Yongqian Li, Peng Chao, Wu Ning, Haonan Song, Renhong Chen, Yitong Li</div>
<div class="meta-line">First: 2026-01-07T08:42:14+00:00 · Latest: 2026-01-07T08:42:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03703v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TreeAdv：面向分组强化学习的树形优势重分配方法</div>
<div class="mono" style="margin-top:8px">基于分组目标的强化学习（如组相对策略优化GRPO）是对齐大语言模型处理复杂推理任务的常用框架。然而标准GRPO将每条轨迹视为独立扁平序列，并为所有词元分配单一序列级优势值，这导致样本效率低下，且产生偏向冗长冗余思维链的长度偏差，却未提升逻辑深度。本文提出TreeAdv（面向分组强化学习的树形优势重分配方法），在探索与优势分配中显式构建分组轨迹的树形结构。具体而言，TreeAdv通过熵驱动采样构建树群（森林），每棵树在高不确定性决策点分叉，同时在轨迹间共享低不确定性词元。随后，TreeAdv通过重分配完整轨迹（所有叶节点）的优势值，聚合内部树段的词元级优势，该方法可轻松适配GRPO、GSPO等分组目标。在10项数学推理基准测试中，TreeAdv在相同监督、数据和解码预算下，使用显著更少的生成词元，持续超越GRPO与GSPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the sample inefficiency and length bias in group-based reinforcement learning for aligning language models, where standard methods like GRPO treat each trajectory as a flat sequence and assign a single advantage to all tokens. To address this, TreeAdv introduces a tree-structured approach that explicitly models group rollouts as a forest, branching at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts, and redistributes token-level advantages by aggregating advantages from complete rollouts. Experimental results across 10 math reasoning benchmarks show that TreeAdv consistently outperforms GRPO and GSPO while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决基于群体的强化学习在语言模型对齐中的样本效率低下和长度偏差问题，因为标准方法如GRPO将每个轨迹视为独立平面序列并为所有令牌分配单一优势。为此，TreeAdv提出了一种树形结构方法，将群体展开明确建模为森林，在高不确定性决策处分支，同时在展开间共享低不确定性令牌，并通过聚合完整展开的优势来重新分配令牌级优势。在10个数学推理基准测试中，TreeAdv在相同监督、数据和解码预算下，始终优于GRPO和GSPO，同时显著减少了生成的令牌数量。</div>
</details>
</div>
<div class="card">
<div class="title">Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction</div>
<div class="meta-line">Authors: Chen Zhang, Kepu Zhang, Jiatong Zhang, Xiao Zhang, Jun Xu</div>
<div class="meta-line">First: 2026-01-07T07:52:30+00:00 · Latest: 2026-01-07T07:52:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03672v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三明治推理：一种面向低延迟查询纠错的答案-推理-答案方法</div>
<div class="mono" style="margin-top:8px">查询纠错是现代搜索流程中的关键环节，必须在实时延迟约束下保证高准确率。思维链推理虽能提升准确率，但其延迟过高，难以满足实时纠错需求。一种潜在解决方案是在推理前先输出答案以降低延迟，但在自回归解码框架下，早期答案独立于后续推理，导致模型无法利用推理能力提升准确率。为解决此问题，我们提出三明治推理——一种通过显式对齐快速初始答案与事后推理的新方法，在保持推理感知准确率的同时实现低延迟查询纠错。该方法遵循答案-推理-答案范式，依次生成初始纠错结果、显式推理过程和最终优化纠错结果。为实现初始答案与推理后认知的对齐，我们设计了基于一致性的强化学习策略：专用一致性奖励机制强制初始与最终纠错结果对齐，而基于边界的拒绝采样则优先处理推理能带来最显著纠错增益的临界样本。此外，针对复杂查询纠错缺乏专项基准数据集的问题，我们构建了高质量查询纠错数据集。实验结果表明，三明治推理在达到与标准思维链推理相当的最优准确率的同时，实现了40-70%的延迟降低，有效解决了在线搜索中的延迟-准确率权衡难题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the latency-accuracy trade-off in real-time query correction for search engines, where Chain-of-Thought reasoning improves accuracy but introduces prohibitive delays. To resolve this, the authors propose Sandwich Reasoning, an Answer-Reasoning-Answer method that generates an initial correction, an explicit reasoning process, and a final refined correction, aligning the initial answer with post-hoc reasoning via a consistency-aware reinforcement learning strategy and margin-based rejection sampling. Experiments show the approach achieves state-of-the-art accuracy similar to standard CoT while reducing latency by 40-70%, effectively balancing performance and speed for online search applications.</div>
<div class="mono" style="margin-top:8px">本文针对搜索引擎实时查询纠错中的延迟与准确性权衡问题展开研究，指出思维链推理虽能提升准确性但会导致难以接受的延迟。为解决此问题，作者提出了三明治推理方法，采用答案-推理-答案范式，先生成初始纠错结果，再进行显式推理，最后输出精炼的纠错结果，并通过一致性感知的强化学习策略和基于边界的拒绝采样，使初始答案与事后推理保持一致。实验结果表明，该方法在达到与标准思维链推理相当的最先进准确性的同时，将延迟降低了40-70%，有效解决了在线搜索中的延迟-准确性权衡难题。</div>
</details>
</div>
<div class="card">
<div class="title">AMIR-GRPO: Inducing Implicit Preference Signals into GRPO</div>
<div class="meta-line">Authors: Amir Hossein Yari, Fajri Koto</div>
<div class="meta-line">First: 2026-01-07T07:22:58+00:00 · Latest: 2026-01-07T07:22:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has become the primary paradigm for aligning large language models (LLMs) on complex reasoning tasks, with group relative policy optimization (GRPO) widely used in large-scale post-training. However, GRPO faces structural limitations in reasoning-heavy settings: sequence-level advantage normalization introduces systematic length bias, penalties for low-quality trajectories are diluted, and the scalar objective discards rich pairwise preference information embedded in within-group reward rankings. As a result, valuable supervision from costly rollouts remains underutilized.
  We propose AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no additional annotations. This mechanism amplifies suppression of low-reward trajectories, attenuates response-level length bias, and transforms each rollout group into a denser set of supervision constraints. Across multiple mathematical reasoning benchmarks, AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and delivers broader coverage gains beyond the subset of instances solved by standard GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AMIR-GRPO：将隐式偏好信号引入GRPO</div>
<div class="mono" style="margin-top:8px">强化学习已成为在复杂推理任务上对齐大语言模型（LLM）的主要范式，其中组相对策略优化（GRPO）被广泛用于大规模后训练。然而，GRPO在推理密集型场景中存在结构性局限：序列级优势归一化会引入系统性长度偏差，低质量轨迹的惩罚被稀释，且标量目标函数丢弃了组内奖励排序所蕴含的丰富成对偏好信息。这导致来自高成本轨迹生成的有价值监督未能被充分利用。
我们提出AMIR-GRPO，通过直接从组内奖励排序构建隐式DPO风格对比正则化器来增强GRPO，无需额外标注。该机制强化了对低奖励轨迹的抑制，减弱了响应级别的长度偏差，并将每个轨迹组转化为更密集的监督约束集合。在多个数学推理基准测试中，AMIR-GRPO持续优于强GRPO基线，在正确与错误推理链之间产生更清晰的区分，并在标准GRPO已解决的实例子集之外实现了更广泛的覆盖增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by structural limitations in Group Relative Policy Optimization (GRPO) for aligning large language models on reasoning tasks, where sequence-level advantage normalization introduces length bias, penalties are diluted, and pairwise preference information is discarded. The method proposes AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no extra annotations to amplify suppression of low-reward trajectories and attenuate length bias. Experimental results across multiple mathematical reasoning benchmarks show that AMIR-GRPO consistently outperforms GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and achieves broader coverage gains beyond instances solved by standard GRPO.</div>
<div class="mono" style="margin-top:8px">本文的动机源于群组相对策略优化（GRPO）在对齐大语言模型进行复杂推理任务时的结构局限性，包括序列级优势归一化引入的长度偏差、对低质量轨迹惩罚的稀释以及标量目标丢弃了群组内奖励排序中丰富的成对偏好信息。方法上提出了AMIR-GRPO，通过直接从群组内奖励排名构建隐式的DPO风格对比正则化器来增强GRPO，无需额外标注即可强化对低奖励轨迹的抑制并减轻长度偏差。在多个数学推理基准上的实验结果表明，AMIR-GRPO持续优于GRPO基线，在正确与错误推理链之间产生更清晰的区分，并实现了超出标准GRPO解决实例范围的更广泛覆盖增益。</div>
</details>
</div>
<div class="card">
<div class="title">Interleaved Tool-Call Reasoning for Protein Function Understanding</div>
<div class="meta-line">Authors: Chuanliu Fan, Zicheng Ma, Huanran Meng, Aijia Zhang, Wenjie Du, Jun Zhang, Yi Qin Gao, Ziqiang Cao, Guohong Fu</div>
<div class="meta-line">First: 2026-01-07T05:34:38+00:00 · Latest: 2026-01-07T05:34:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03604v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03604v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>交错式工具调用推理用于蛋白质功能理解</div>
<div class="mono" style="margin-top:8px">近期大语言模型（LLMs）的进展突显了思维链推理在数学和编程等符号领域的有效性。然而，我们的研究表明，直接将此类基于文本的推理范式迁移到蛋白质功能理解中是无效的：强化学习主要放大了表面的关键词模式，而未能引入新的生物学知识，导致泛化能力有限。我们认为蛋白质功能预测是一项知识密集型的科学任务，其根本上依赖于外部生物学先验知识和计算工具，而非纯粹的内部推理。为弥补这一差距，我们提出了PFUA——一种工具增强的蛋白质推理智能体，它统一了问题分解、工具调用和基于证据的答案生成。PFUA不依赖冗长无约束的推理轨迹，而是整合领域专用工具以生成可验证的中间证据。在四个基准测试上的实验表明，PFUA始终优于纯文本推理模型，平均性能提升达103%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the ineffectiveness of directly applying text-based chain-of-thought reasoning to protein function understanding, which often amplifies superficial patterns without adding biological knowledge, this paper proposes PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation by integrating domain-specific tools to produce verifiable intermediate evidence. The method addresses the knowledge-intensive nature of protein function prediction by relying on external biological priors and computational tools rather than purely internal reasoning. Experimental results on four benchmarks show that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%, demonstrating strong generalization.</div>
<div class="mono" style="margin-top:8px">本文的动机在于直接应用基于文本的思维链推理于蛋白质功能理解效果不佳，往往强化表面关键词模式而未能引入新的生物学知识，导致泛化能力有限。为此，作者提出PFUA，一种工具增强的蛋白质推理智能体，通过整合领域专用工具来生成可验证的中间证据，统一了问题分解、工具调用和基于证据的答案生成。该方法针对蛋白质功能预测这一知识密集型科学任务，依赖外部生物学先验和计算工具而非纯内部推理。在四个基准测试上的实验结果表明，PFUA持续优于纯文本推理模型，平均性能提升达103%，展现了强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Tool-Integrated Interleaved Thinking towards Cross-Domain Generalization</div>
<div class="meta-line">Authors: Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang</div>
<div class="meta-line">First: 2025-10-13T09:19:13+00:00 · Latest: 2026-01-07T04:36:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11184v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains a significant challenge. Standard paradigms often treat tool usage as a linear or isolated event, which becomes brittle when transferring skills from restricted domains (e.g., mathematics) to open-ended tasks. In this work, we investigate the cross-domain generalization of an LLM agent trained exclusively on mathematical problem-solving. To facilitate robust skill transfer, we propose a {\textbf{R}einforcement Learning for \textbf{I}nterleaved \textbf{T}ool \textbf{E}xecution (RITE)}. Unlike traditional methods, RITE enforces a continuous ``Plan-Action-Reflection&#x27;&#x27; cycle, allowing the model to ground its reasoning in intermediate tool outputs and self-correct during long-horizon tasks. To effectively train this complex interleaved policy, we introduce {Dr. GRPO}, a robust optimization objective that utilizes token-level loss aggregation with importance sampling to mitigate reward sparsity and high-variance credit assignment. Furthermore, we employ a dual-component reward system and dynamic curriculum via online rollout filtering to ensure structural integrity and sample efficiency. Extensive experiments reveal that our approach, despite being trained solely on math tasks, achieves state-of-the-art performance across diverse reasoning domains, demonstrating high token efficiency and strong generalization capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向跨领域泛化的工具集成交错思维强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在推理与工具利用方面展现出卓越能力，但工具增强强化学习（RL）的跨领域泛化仍是重大挑战。传统范式常将工具使用视为线性或孤立事件，导致从受限领域（如数学）向开放任务迁移时表现脆弱。本研究探索了仅通过数学问题训练的大语言模型智能体的跨领域泛化能力。为实现鲁棒的技能迁移，我们提出强化学习交错工具执行框架（RITE）。与传统方法不同，RITE通过持续的“规划-执行-反思”循环，使模型能够基于中间工具输出进行推理，并在长程任务中实现自我修正。为有效训练这一复杂交错策略，我们提出Dr. GRPO优化目标，利用基于重要性采样的词元级损失聚合来缓解奖励稀疏性与高方差信用分配问题。此外，我们采用双组件奖励系统与基于在线推演筛选的动态课程学习机制，确保结构完整性与样本效率。大量实验表明，尽管仅使用数学任务训练，该方法在多样化推理领域均达到最先进性能，展现出高效的词元利用率与强大的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of cross-domain generalization for tool-augmented reinforcement learning agents, motivated by the brittleness of standard linear tool-use paradigms when transferring skills from restricted domains like mathematics to open-ended tasks. The method introduces RITE (Reinforcement Learning for Interleaved Tool Execution), which enforces a continuous Plan-Action-Reflection cycle to ground reasoning in tool outputs and enable self-correction, and employs Dr. GRPO, an optimization objective using token-level loss aggregation with importance sampling to mitigate reward sparsity, alongside a dual-component reward system and dynamic curriculum for efficient training. Experimental results show that the approach, trained solely on math tasks, achieves state-of-the-art performance across diverse reasoning domains with high token efficiency and strong generalization.</div>
<div class="mono" style="margin-top:8px">本文针对工具增强强化学习智能体在跨领域泛化中的挑战，其动机在于标准线性工具使用范式在从数学等受限领域迁移技能到开放任务时表现脆弱。方法上提出了RITE（交错工具执行的强化学习），通过强制连续的“计划-行动-反思”循环使推理基于工具输出并支持自我修正，并采用Dr. GRPO优化目标，利用令牌级损失聚合与重要性采样来缓解奖励稀疏性，同时结合双组件奖励系统和动态课程以实现高效训练。实验结果表明，该方法仅在数学任务上训练，就在多样推理领域达到了最先进的性能，表现出高令牌效率和强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">CatCMA with Margin for Single- and Multi-Objective Mixed-Variable Black-Box Optimization</div>
<div class="meta-line">Authors: Ryoki Hamano, Masahiro Nomura, Shota Saito, Kento Uchida, Shinichi Shirakawa</div>
<div class="meta-line">First: 2025-04-10T15:59:22+00:00 · Latest: 2026-01-07T03:51:09+00:00</div>
<div class="meta-line">Comments: v6: Extended journal version. Adds a multi-objective extension (Section 7) and bi-objective mixed-variable experiments. v5 corresponds to the GECCO&#x27;25 conference version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.07884v6">Abs</a> · <a href="https://arxiv.org/pdf/2504.07884v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study focuses on mixed-variable black-box optimization (MV-BBO), addressing continuous, integer, and categorical variables. Many real-world MV-BBO problems involve dependencies among these different types of variables, requiring efficient methods to optimize them simultaneously. Recently, stochastic optimization methods leveraging the mechanism of the covariance matrix adaptation evolution strategy have shown promising results in mixed-integer or mixed-category optimization. However, such methods cannot handle the three types of variables simultaneously. In this study, we propose CatCMA with Margin (CatCMAwM), a stochastic optimization method for MV-BBO that jointly optimizes continuous, integer, and categorical variables. CatCMAwM is developed by incorporating novel integer handling into CatCMA, a mixed-category black-box optimization method employing a joint distribution of multivariate Gaussian and categorical distributions. The proposed integer handling is carefully designed by reviewing existing integer handling and following the design principles of CatCMA. Furthermore, we extend CatCMAwM to multi-objective MV-BBO by instantiating it within the Sofomore framework, obtaining a multi-objective optimizer termed COMO-CatCMA with Margin (COMO-CatCMAwM). Numerical experiments on single-objective MV-BBO problems show that CatCMAwM effectively handles the three types of variables, outperforming state-of-the-art Bayesian optimization methods and baselines that simply incorporate existing integer handlings into CatCMA. Moreover, on bi-objective MV-BBO benchmarks, COMO-CatCMAwM achieves competitive or superior hypervolume compared to representative baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带边界机制的CatCMA方法：面向单目标与多目标混合变量黑盒优化</div>
<div class="mono" style="margin-top:8px">本研究聚焦混合变量黑盒优化问题，涵盖连续、整数与类别变量。现实中的混合变量优化常涉及不同类型变量间的依赖关系，需开发能同步优化这些变量的高效方法。近期基于协方差矩阵自适应进化策略的随机优化方法在混合整数或混合类别优化中展现出潜力，但无法同时处理三类变量。本文提出带边界机制的CatCMA方法——一种能联合优化连续、整数及类别变量的随机优化方法。该方法通过在CatCMA（一种采用多元高斯与类别分布联合分布的混合类别黑盒优化方法）中引入新型整数处理机制构建而成，其整数处理模块在审视现有方法基础上严格遵循CatCMA的设计原则。进一步地，我们将该方法扩展至多目标混合变量优化领域，通过在Sofomore框架中实例化，得到多目标优化器COMO-CatCMAwM。在单目标混合变量优化问题的数值实验中，CatCMAwM能有效处理三类变量，其性能优于先进贝叶斯优化方法及简单整合现有整数处理机制的基线方法。在双目标混合变量基准测试中，COMO-CatCMAwM获得的超体积指标与代表性基线方法相比具有竞争力或更优表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses mixed-variable black-box optimization (MV-BBO), which involves continuous, integer, and categorical variables with interdependencies, by proposing CatCMA with Margin (CatCMAwM), a stochastic optimization method that jointly optimizes all three variable types. The method extends CatCMA by incorporating a novel integer-handling mechanism designed in line with CatCMA&#x27;s principles, and it is further adapted for multi-objective problems within the Sofomore framework, resulting in COMO-CatCMAwM. Experimental results on single-objective MV-BBO show that CatCMAwM outperforms state-of-the-art Bayesian optimization and baseline methods, while in bi-objective benchmarks, COMO-CatCMAwM achieves competitive or superior hypervolume compared to representative baselines.</div>
<div class="mono" style="margin-top:8px">本文针对涉及连续、整数和分类变量且存在依赖关系的混合变量黑盒优化问题，提出了CatCMA with Margin（CatCMAwM）方法，这是一种能同时优化三种变量类型的随机优化方法。该方法通过结合新颖的整数处理机制扩展了CatCMA，并遵循其设计原则，同时还在Sofomore框架内将其扩展为多目标优化器COMO-CatCMAwM。在单目标混合变量优化实验中，CatCMAwM优于先进的贝叶斯优化和基线方法；在双目标基准测试中，COMO-CatCMAwM取得了与代表性基线相当或更优的超体积性能。</div>
</details>
</div>
<div class="card">
<div class="title">Active operator learning with predictive uncertainty quantification for partial differential equations</div>
<div class="meta-line">Authors: Nick Winovich, Mitchell Daneker, Lu Lu, Guang Lin</div>
<div class="meta-line">First: 2025-03-05T04:48:14+00:00 · Latest: 2026-01-07T03:50:11+00:00</div>
<div class="meta-line">Comments: Submitted to the Journal of Computational Physics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03178v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.03178v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework&#x27;s uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>偏微分方程主动算子学习与预测不确定性量化</div>
<div class="mono" style="margin-top:8px">随着神经算子被广泛用于快速求解偏微分方程，理解模型预测的准确性及相关误差水平对于在科学应用中部署可靠的代理模型至关重要。现有不确定性量化框架采用集成或贝叶斯方法，在训练和推断阶段均可能产生高昂计算成本。本文提出一种专为深度算子网络设计的轻量级预测不确定性量化方法，该方法也可推广至其他算子网络。在线性与非线性偏微分方程上的数值实验表明，在训练数据集足够大的情况下，该框架的不确定性估计具有无偏性，并能提供准确的分布外不确定性预测。本框架提供快速推断与不确定性估计，可高效驱动传统求解器难以承担的外循环分析。我们展示了如何将预测不确定性应用于贝叶斯优化和主动学习问题，以提升外循环优化过程的精度与数据效率。在主动学习场景中，我们将框架扩展至傅里叶神经算子，并描述了适用于其他算子网络的通用方法。为实现实时部署，我们引入基于预计算主干输出和稀疏放置矩阵的推断策略，将评估时间缩短五倍以上。该方法为时间敏感场景下的不确定性感知算子学习提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable and computationally efficient uncertainty quantification (UQ) in neural operator surrogates for solving partial differential equations (PDEs), this paper proposes a lightweight predictive UQ method tailored for DeepONets, which also generalizes to other operator networks like Fourier Neural Operators. The method employs a strategy that avoids costly ensembles or Bayesian approaches, instead using predictive uncertainties that are unbiased and provide accurate out-of-distribution predictions with sufficient training data. Experimental results on linear and nonlinear PDEs demonstrate that the framework enables fast inference, reducing evaluation time by over five times via precomputed trunk outputs and sparse placement matrices, and effectively supports outer-loop applications such as Bayesian optimization and active learning, improving accuracy and data-efficiency in optimization procedures.</div>
<div class="mono" style="margin-top:8px">本文的动机是为求解偏微分方程的神经算子代理模型提供可靠且计算高效的预测不确定性量化方法，以克服现有基于集成或贝叶斯方法的高计算成本问题。该方法提出了一种轻量级的预测不确定性量化框架，专为DeepONet设计并可推广至其他算子网络，如傅里叶神经算子。通过在线性和非线性偏微分方程上的数值实验表明，该框架的不确定性估计是无偏的，能在足够训练数据下提供准确的分布外预测，并通过预计算主干输出和稀疏放置矩阵将评估时间减少五倍以上。该框架支持快速推理，可有效应用于贝叶斯优化和主动学习等外循环分析，提升了优化过程的准确性和数据效率。</div>
</details>
</div>
<div class="card">
<div class="title">SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models</div>
<div class="meta-line">Authors: Yuxuan Jiang, Francis Ferraro</div>
<div class="meta-line">First: 2026-01-07T03:49:48+00:00 · Latest: 2026-01-07T03:49:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03555v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03555v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.
  Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.
  Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCRIBE：面向工具调用语言模型的结构化中层监督框架</div>
<div class="mono" style="margin-top:8px">训练可靠的工具增强智能体仍是重大挑战，主要源于多步推理中的信用分配难题。虽然过程级奖励模型提供了可行方向，但现有基于大语言模型的评估器常因缺乏区分高层规划与底层执行的细粒度任务标准，而产生噪声大且不一致的信号。本研究提出SCRIBE（基于技能条件与中间行为评估的强化学习框架），该框架通过创新的中层抽象干预机制，将奖励模型锚定于精心构建的技能原型库，将开放式大语言模型评估转化为结构化验证问题。通过将每个子目标路由至对应原型，奖励模型获得精确的结构化评估标准，显著降低奖励方差。实验表明，SCRIBE在多项推理与工具使用基准测试中达到最先进性能：将Qwen3-4B模型在AIME25的准确率从43.3%提升至63.3%，并大幅提高复杂多轮工具交互成功率。训练动态分析进一步揭示抽象层级间的协同演化规律——中层技能的精通始终先于有效高层规划行为的涌现。最后，本研究论证SCRIBE可与底层工具优化形成互补，为构建更自主可靠的工具调用智能体提供可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of credit assignment in multi-step tool-use tasks, where existing LLM-based reward models produce noisy signals due to a lack of fine-grained rubrics, this paper introduces SCRIBE, a reinforcement learning framework that provides structured mid-level supervision. The method grounds reward modeling in a curated library of skill prototypes, transforming open-ended evaluation into a constrained verification problem to reduce reward variance. Experimental results demonstrate state-of-the-art performance on reasoning and tool-use benchmarks, notably improving a Qwen3-4B model&#x27;s AIME25 accuracy from 43.3% to 63.3% and boosting success in complex multi-turn interactions, with analysis showing that mid-level skill mastery precedes effective high-level planning.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决多步骤工具使用任务中的信用分配难题，现有基于大语言模型的奖励模型因缺乏细粒度评估标准而产生噪声信号。为此，论文提出了SCRIBE强化学习框架，通过引入结构化的中层监督，将奖励建模基于一个精心策划的技能原型库，从而将开放式评估转化为受限的验证问题以降低奖励方差。实验结果表明，该方法在推理和工具使用基准测试中取得了领先性能，特别是将Qwen3-4B模型在AIME25上的准确率从43.3%提升至63.3%，并显著提高了复杂多轮交互的成功率，分析进一步揭示中层技能掌握先于有效高层规划行为的出现。</div>
</details>
</div>
<div class="card">
<div class="title">DRA-GRPO: Your GRPO Needs to Know Diverse Reasoning Paths for Mathematical Reasoning</div>
<div class="meta-line">Authors: Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi</div>
<div class="meta-line">First: 2025-05-14T02:02:32+00:00 · Latest: 2026-01-07T03:24:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.09655v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.09655v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training LLMs with Reinforcement Learning, specifically Group Relative Policy Optimization (GRPO), has emerged as a paradigm for enhancing mathematical reasoning. However, standard GRPO relies on scalar correctness rewards that are often non-injective with respect to semantic content: distinct reasoning paths receive identical rewards. This leads to a Diversity-Quality Inconsistency, where the policy collapses into a narrow set of dominant modes while ignoring equally valid but structurally novel strategies. To bridge this gap, we propose Diversity-aware Reward Adjustment (DRA), a theoretically grounded framework that calibrates the reward signal using the semantic density of sampled groups. By leveraging Submodular Mutual Information (SMI), DRA implements an Inverse Propensity Scoring (IPS) mechanism that effectively de-biases the gradient estimation. This creates a repulsive force against redundancy, driving the policy to achieve better coverage of the high-reward landscape. Our method is plug-and-play and integrates seamlessly with GRPO variants. Empirical evaluations on five math benchmarks demonstrate that DRA-GRPO consistently outperforms strong baselines, achieving an average accuracy of 58.2% on DeepSeek-R1-Distill-Qwen-1.5B with only 7,000 training samples and $55 cost, highlighting the critical role of diversity calibration in data-efficient alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DRA-GRPO：数学推理中GRPO需要掌握多样化推理路径</div>
<div class="mono" style="margin-top:8px">基于强化学习（特别是组相对策略优化GRPO）的大语言模型后训练已成为提升数学推理能力的范式。然而，标准GRPO依赖的标量正确性奖励常与语义内容非单射对应：不同推理路径获得相同奖励。这导致多样性-质量不一致性问题，使策略坍缩至少数主导模式，忽略结构新颖但同等有效的策略。为弥合此差距，我们提出理论驱动的多样性感知奖励调整框架DRA，通过采样组的语义密度校准奖励信号。DRA利用子模互信息，实现逆倾向评分机制，有效消除梯度估计偏差。该方法产生针对冗余的排斥力，驱动策略更好地覆盖高奖励区域。本方法即插即用，可与GRPO变体无缝集成。在五个数学基准上的实证评估表明，DRA-GRPO持续超越强基线模型，仅用7,000训练样本和55美元成本即在DeepSeek-R1-Distill-Qwen-1.5B上实现58.2%平均准确率，凸显了多样性校准在数据高效对齐中的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the Diversity-Quality Inconsistency in standard Group Relative Policy Optimization (GRPO) for mathematical reasoning, where scalar correctness rewards fail to distinguish between distinct but valid reasoning paths, causing policy collapse into narrow modes. To address this, the authors propose Diversity-aware Reward Adjustment (DRA), a plug-and-play framework that calibrates rewards using semantic density via Submodular Mutual Information and an Inverse Propensity Scoring mechanism to de-bias gradient estimation and promote coverage of diverse high-reward strategies. Experimental results on five math benchmarks show that DRA-GRPO outperforms baselines, achieving an average accuracy of 58.2% on DeepSeek-R1-Distill-Qwen-1.5B with only 7,000 training samples at low cost, underscoring the importance of diversity calibration for data-efficient alignment.</div>
<div class="mono" style="margin-top:8px">本文的动机源于标准群组相对策略优化（GRPO）在数学推理中存在的多样性-质量不一致问题，即标量正确性奖励无法区分不同但有效的推理路径，导致策略坍缩到狭窄模式。为解决此问题，作者提出了多样性感知奖励调整（DRA），这是一个即插即用的框架，通过子模互信息和逆倾向评分机制，利用语义密度校准奖励信号，以去偏梯度估计并促进对多样高奖励策略的覆盖。在五个数学基准上的实验结果表明，DRA-GRPO优于强基线方法，仅用7,000个训练样本就以低成本在DeepSeek-R1-Distill-Qwen-1.5B上实现了58.2%的平均准确率，凸显了多样性校准在数据高效对齐中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Think Outside the Policy: In-Context Steered Policy Optimization</div>
<div class="meta-line">Authors: Hsiu-Yuan Huang, Chenming Tang, Weijie Liu, Clive Bai, Saiyong Yang, Yunfang Wu</div>
<div class="meta-line">First: 2025-10-30T14:14:15+00:00 · Latest: 2026-01-07T03:04:54+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26519v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26519v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such as Group Relative Policy Optimization (GRPO), have achieved remarkable progress in improving the reasoning capabilities of Large Reasoning Models (LRMs). However, they exhibit limited exploration due to reliance on on-policy rollouts which are confined to the current policy&#x27;s distribution, resulting in narrow trajectory diversity. Recent approaches attempt to expand policy coverage by incorporating trajectories generated from stronger expert models, yet this reliance increases computational cost and such advanced models are often inaccessible. To address these issues, we propose In-Context Steered Policy Optimization (ICPO), a unified framework that leverages the inherent in-context learning capability of LRMs to provide expert guidance using existing datasets. ICPO introduces mixed-policy GRPO with implicit expert forcing, which expands exploration beyond the current policy distribution without requiring advanced LRM trajectories. To further stabilize optimization, ICPO integrates expert region reject sampling to filter unreliable off-policy trajectories and annealed expert-bonus reward shaping to balance early expert guidance with later autonomous improvement. Results demonstrate that ICPO consistently enhances RLVR performance and training stability on mathematical reasoning benchmarks, revealing a scalable and effective RLVR paradigm for LRMs. Our code is available at https://anonymous.4open.science/r/ICPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越策略框架：基于上下文引导的策略优化</div>
<div class="mono" style="margin-top:8px">现有的可验证奖励强化学习方法，如组相对策略优化，在提升大型推理模型的推理能力方面取得了显著进展。然而，由于依赖局限于当前策略分布的在线策略轨迹，这些方法探索能力有限，导致轨迹多样性不足。近期研究尝试通过引入更强专家模型生成的轨迹来扩展策略覆盖范围，但这种方法增加了计算成本，且高级模型往往难以获取。为解决这些问题，我们提出了基于上下文引导的策略优化，这是一个统一框架，利用大型推理模型固有的上下文学习能力，通过现有数据集提供专家指导。该方法引入了具有隐式专家强化的混合策略组相对策略优化，可在无需高级模型轨迹的情况下，将探索范围扩展至当前策略分布之外。为进一步稳定优化过程，该方法整合了专家区域拒绝采样以过滤不可靠的离线策略轨迹，并采用退火式专家奖励塑形来平衡早期专家指导与后期自主改进。实验结果表明，在数学推理基准测试中，该方法持续提升了可验证奖励强化学习的性能与训练稳定性，为大型推理模型揭示了一种可扩展且有效的强化学习范式。代码发布于 https://anonymous.4open.science/r/ICPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limited exploration in existing Reinforcement Learning from Verifiable Rewards (RLVR) methods like GRPO, which rely on on-policy rollouts that constrain trajectory diversity. To overcome this without depending on computationally expensive or inaccessible expert models, the authors propose In-Context Steered Policy Optimization (ICPO), a framework that leverages the in-context learning of Large Reasoning Models to provide expert guidance from existing datasets. ICPO employs mixed-policy GRPO with implicit expert forcing to broaden exploration, along with expert region reject sampling and annealed expert-bonus reward shaping to stabilize training. Experimental results on mathematical reasoning benchmarks show that ICPO consistently improves RLVR performance and training stability, offering a scalable RLVR paradigm.</div>
<div class="mono" style="margin-top:8px">本文针对现有可验证奖励强化学习方法（如GRPO）探索有限的问题，这些方法依赖当前策略分布内的轨迹，导致多样性不足。为避免依赖计算成本高或难以获取的专家模型，作者提出了上下文引导策略优化（ICPO），该框架利用大型推理模型的上下文学习能力，从现有数据集中提供专家指导。ICPO采用混合策略GRPO与隐式专家强制来扩展探索，并结合专家区域拒绝采样和退火专家奖励塑形以稳定训练。在数学推理基准测试上的实验结果表明，ICPO能持续提升RLVR性能和训练稳定性，为大型推理模型提供了一个可扩展的有效强化学习范式。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search</div>
<div class="meta-line">Authors: Yaodong Yang, Yang Wang, Jinpeng Li, Pei Guo, Da Han, Guangyong Chen, Pheng-Ann Heng</div>
<div class="meta-line">First: 2025-11-13T03:00:52+00:00 · Latest: 2026-01-07T02:45:43+00:00</div>
<div class="meta-line">Comments: working in progress, 20 pages, 6 figures, 16 tables, updating template</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09900v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.09900v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Protein evolution through amino acid mutations is a cornerstone of life sciences. Recent advances in protein language models have shown rich evolutionary patterns, offering unprecedented potential for in-silicon directed evolution. However, existing directed evolution methods largely rely on heuristic evolution strategies and have yet to efficiently integrate the transformative protein language models with advanced optimization techniques, such as reinforcement learning, to adaptively learn superior evolution policies. To bridge this gap, we propose AlphaDE, a novel framework that evolves protein sequences by harnessing the innovative paradigms of large language models, such as fine-tuning and test-time inference. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility of the interested protein family. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. A case study further demonstrates that AlphaDE supports condensing the protein sequence space of avGFP through computational evolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于微调蛋白质语言模型与树搜索增强硅基定向进化</div>
<div class="mono" style="margin-top:8px">通过氨基酸突变实现蛋白质进化是生命科学的基石。近期蛋白质语言模型的进展揭示了丰富的进化模式，为硅基定向进化提供了前所未有的潜力。然而，现有定向进化方法主要依赖启发式进化策略，尚未有效整合变革性的蛋白质语言模型与强化学习等先进优化技术，以自适应学习更优的进化策略。为填补这一空白，我们提出AlphaDE框架，该框架通过利用大语言模型的创新范式（如微调和测试时推理）来进化蛋白质序列。首先，AlphaDE通过对同源蛋白质序列进行掩码语言建模，微调预训练的蛋白质语言模型，以激活目标蛋白质家族的进化合理性。其次，AlphaDE引入基于蒙特卡洛树搜索的测试时推理，在微调蛋白质语言模型的进化指导下高效进化蛋白质。大量基准实验表明，即使仅进行少样本微调，AlphaDE仍显著优于现有最优方法。案例研究进一步证明，AlphaDE能通过计算进化压缩avGFP的蛋白质序列空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enhance in-silico directed evolution by better integrating protein language models with advanced optimization techniques, as existing methods often rely on heuristic strategies and lack efficient adaptive learning. The proposed method, AlphaDE, fine-tunes a pretrained protein language model on homologous sequences to activate evolutionary plausibility for a target protein family and then employs Monte Carlo tree search during test-time inference to guide protein evolution. Experimental results from benchmark tests show that AlphaDE significantly outperforms prior state-of-the-art methods even with limited fine-tuning data, and a case study confirms its ability to effectively condense the sequence space of avGFP through computational evolution.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过更好地整合蛋白质语言模型与先进优化技术来增强硅基定向进化，因为现有方法多依赖启发式策略且缺乏高效的自适应学习。所提出的AlphaDE方法首先在同类蛋白质序列上对预训练的蛋白质语言模型进行微调，以激活目标蛋白质家族的进化合理性，然后在测试时推理中采用蒙特卡洛树搜索来指导蛋白质进化。基准实验结果表明，即使使用少量微调数据，AlphaDE也显著优于先前的最先进方法，案例研究进一步证实其能够通过计算进化有效压缩avGFP的蛋白质序列空间。</div>
</details>
</div>
<div class="card">
<div class="title">VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation</div>
<div class="meta-line">Authors: Longwen Wang, Xuan&#x27;er Wu, Xiaohui Hu, Yirui Liu, Yuankai Fan, Kaidong Yu, Qizhen Weng, Wei Xi, Xuelong Li</div>
<div class="meta-line">First: 2026-01-07T02:29:49+00:00 · Latest: 2026-01-07T02:29:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03525v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03525v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective reward design is a central challenge in Reinforcement Learning (RL) for code generation. Mainstream pass/fail outcome rewards enforce functional correctness via executing unit tests, but the resulting sparsity limits potential performance gains. While recent work has explored external Reward Models (RM) to generate richer, continuous rewards, the learned RMs suffer from reward misalignment and prohibitive computational cost. In this paper, we introduce \textbf{VeRPO} (\textbf{V}erifiable D\textbf{e}nse \textbf{R}eward \textbf{P}olicy \textbf{O}ptimization), a novel RL framework for code generation that synthesizes \textit{robust and dense rewards fully grounded in verifiable execution feedback}. The core idea of VeRPO is constructing dense rewards from weighted partial success: by dynamically estimating the difficulty weight of each unit test based on the execution statistics during training, a dense reward is derived from the sum of weights of the passed unit tests. To solidify the consistency between partial success and end-to-end functional correctness, VeRPO further integrates the dense signal with global execution outcomes, establishing a robust and dense reward paradigm relying solely on verifiable execution feedback. Extensive experiments across diverse benchmarks and settings demonstrate that VeRPO consistently outperforms outcome-driven and RM-based baselines, achieving up to +8.83\% gain in pass@1 with negligible time cost (&lt; 0.02\%) and zero GPU memory overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VeRPO：面向代码生成的可验证密集奖励策略优化</div>
<div class="mono" style="margin-top:8px">有效的奖励设计是代码生成强化学习（RL）中的核心挑战。主流基于通过/失败结果的奖励通过执行单元测试来确保功能正确性，但其稀疏性限制了性能提升潜力。尽管近期研究探索使用外部奖励模型（RM）生成更丰富的连续奖励，但学习得到的RM存在奖励失准和计算成本过高的问题。本文提出\textbf{VeRPO}（\textbf{可验证密集奖励策略优化}），这是一种面向代码生成的新型RL框架，能够基于\textit{完全可验证的执行反馈}合成\textit{鲁棒且密集的奖励}。VeRPO的核心思想是通过加权部分成功构建密集奖励：在训练过程中根据执行统计数据动态估计每个单元测试的难度权重，从而基于通过的单元测试权重之和推导出密集奖励。为强化部分成功与端到端功能正确性之间的一致性，VeRPO进一步将密集信号与全局执行结果融合，建立了仅依赖可验证执行反馈的鲁棒密集奖励范式。跨多基准和设置的广泛实验表明，VeRPO在pass@1指标上最高提升+8.83%，且时间成本可忽略（&lt;0.02%）、GPU内存开销为零，持续优于结果驱动和基于RM的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of sparse rewards in reinforcement learning for code generation, where traditional pass/fail rewards based on unit test execution limit performance. The proposed method, VeRPO, constructs dense rewards by weighting partial successes: it dynamically estimates the difficulty of each unit test from execution statistics during training and sums the weights of passed tests to form a dense reward signal, which is further integrated with global execution outcomes to ensure alignment with functional correctness. Experimental results across various benchmarks show that VeRPO consistently outperforms both outcome-driven and reward model-based baselines, achieving up to an 8.83% improvement in pass@1 with minimal computational overhead and no GPU memory cost.</div>
<div class="mono" style="margin-top:8px">本文针对代码生成中强化学习的稀疏奖励问题，传统基于单元测试执行结果的通过/失败奖励限制了性能提升。提出的方法VeRPO通过加权部分成功来构建密集奖励：在训练过程中根据执行统计数据动态估计每个单元测试的难度，并将通过测试的权重求和形成密集奖励信号，进一步与全局执行结果结合以确保与功能正确性一致。在多个基准测试中的实验结果表明，VeRPO持续优于基于结果和奖励模型的基线方法，在pass@1指标上最高提升8.83%，且计算开销极低，无GPU内存占用。</div>
</details>
</div>
<div class="card">
<div class="title">A Reinforcement Learning-Based Model for Mapping and Goal-Directed Navigation Using Multiscale Place Fields</div>
<div class="meta-line">Authors: Bekarys Dukenbaev, Andrew Gerstenslager, Alexander Johnson, Ali A. Minai</div>
<div class="meta-line">First: 2026-01-07T02:10:52+00:00 · Latest: 2026-01-07T02:10:52+00:00</div>
<div class="meta-line">Comments: 11 pages, 8 figures. Submitted to IEEE Transactions on Cognitive and Developmental Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03520v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03520v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous navigation in complex and partially observable environments remains a central challenge in robotics. Several bio-inspired models of mapping and navigation based on place cells in the mammalian hippocampus have been proposed. This paper introduces a new robust model that employs parallel layers of place fields at multiple spatial scales, a replay-based reward mechanism, and dynamic scale fusion. Simulations show that the model improves path efficiency and accelerates learning compared to single-scale baselines, highlighting the value of multiscale spatial representations for adaptive robot navigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的多尺度位置场地图构建与目标导向导航模型</div>
<div class="mono" style="margin-top:8px">在复杂且部分可观测环境中的自主导航仍是机器人技术的核心挑战。已有多种基于哺乳动物海马体位置细胞的仿生地图构建与导航模型被提出。本文提出一种新型鲁棒模型，采用多空间尺度的并行位置场层、基于经验回放的奖励机制及动态尺度融合技术。仿真实验表明，相较于单尺度基线模型，该模型能提升路径效率并加速学习进程，凸显了多尺度空间表征对自适应机器人导航的重要价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of autonomous navigation in complex, partially observable environments, this paper introduces a bio-inspired model that uses parallel layers of multiscale place fields, a replay-based reward mechanism, and dynamic scale fusion to enhance mapping and goal-directed navigation. The method draws inspiration from mammalian hippocampal place cells to create a robust spatial representation. Experimental simulations demonstrate that the model improves path efficiency and accelerates learning compared to single-scale baselines, underscoring the advantage of multiscale representations for adaptive robot navigation.</div>
<div class="mono" style="margin-top:8px">针对复杂、部分可观测环境中自主导航的挑战，本文提出了一种受生物启发的模型，该模型采用多尺度位置场的并行层、基于回放的奖励机制和动态尺度融合，以增强建图与目标导向导航。该方法借鉴哺乳动物海马体位置细胞的原理，构建了鲁棒的空间表征。实验仿真结果表明，与单尺度基线相比，该模型提高了路径效率并加速了学习过程，凸显了多尺度空间表征在自适应机器人导航中的价值。</div>
</details>
</div>
<div class="card">
<div class="title">DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search</div>
<div class="meta-line">Authors: Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi</div>
<div class="meta-line">First: 2025-09-29T20:00:29+00:00 · Latest: 2026-01-07T02:08:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25454v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25454v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although RLVR has become an essential component for developing advanced reasoning skills in language models, contemporary studies have documented training plateaus after thousands of optimization steps, i.e., notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search (MCTS) directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models, while using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepSearch：通过蒙特卡洛树搜索突破强化学习可验证奖励的瓶颈</div>
<div class="mono" style="margin-top:8px">尽管强化学习可验证奖励已成为语言模型发展高级推理能力的关键组成部分，但近期研究表明，经过数千步优化后会出现训练平台期，即计算资源投入增加而性能增益显著下降。这一局限源于当前强化学习可验证奖励实践中固有的稀疏探索模式，模型依赖有限的推演轨迹，往往错过关键推理路径，无法系统覆盖解空间。我们提出DeepSearch框架，将蒙特卡洛树搜索直接整合到强化学习可验证奖励训练中。与现有仅在推理阶段使用树搜索的方法不同，DeepSearch将结构化搜索嵌入训练循环，实现系统化探索和推理步骤间的细粒度信用分配。通过训练阶段的探索，DeepSearch解决了探索不足这一根本瓶颈，该瓶颈导致长期训练步骤中性能改进逐渐减弱。我们的贡献包括：（1）全局前沿选择策略，优先处理搜索树中有潜力的节点；（2）基于熵的引导选择机制，识别可用于监督的高置信度路径；（3）结合解缓存的自适应回放缓冲区训练以提升效率。数学推理基准测试表明，DeepSearch在1.5B参数推理模型上达到62.95%的平均准确率，创造了新的最优性能记录，同时比扩展训练方法减少5.7倍GPU时耗。这些结果凸显了策略性探索相对于暴力扩展的重要性，并证明了算法创新对推进强化学习可验证奖励方法的潜力。DeepSearch通过系统化搜索而非延长计算时间，为扩展推理能力开辟了新方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the training plateaus and sparse exploration patterns in current Reinforcement Learning with Verifiable Rewards (RLVR) methods, which limit performance gains despite increased computation, this paper introduces DeepSearch, a framework that integrates Monte Carlo Tree Search (MCTS) directly into the RLVR training loop to enable systematic exploration and fine-grained credit assignment across reasoning steps. The method employs a global frontier selection strategy, entropy-based path guidance, and adaptive replay buffer training with solution caching. Experimental results on mathematical reasoning benchmarks demonstrate that DeepSearch achieves a state-of-the-art average accuracy of 62.95% for 1.5B parameter models while using 5.7 times fewer GPU hours than extended training baselines, highlighting the efficacy of strategic exploration over brute-force scaling.</div>
<div class="mono" style="margin-top:8px">针对当前基于可验证奖励的强化学习（RLVR）方法中存在的训练瓶颈和稀疏探索模式导致性能提升受限的问题，本文提出了DeepSearch框架，它将蒙特卡洛树搜索（MCTS）直接集成到RLVR训练循环中，以实现对推理步骤的系统性探索和细粒度信用分配。该方法采用了全局前沿节点选择策略、基于熵的路径引导以及带解决方案缓存的适应性回放缓冲区训练。在数学推理基准测试上的实验结果表明，DeepSearch以1.5B参数模型取得了62.95%的平均准确率，达到了新的最先进水平，同时比延长训练的方法减少了5.7倍的GPU小时消耗，证明了战略性探索相比暴力计算扩展的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">The Invisible Leash: Why RLVR May or May Not Escape Its Origin</div>
<div class="meta-line">Authors: Fang Wu, Weihao Xuan, Ximing Lu, Mingjie Liu, Yi Dong, Zaid Harchaoui, Yejin Choi</div>
<div class="meta-line">First: 2025-07-20T07:04:08+00:00 · Latest: 2026-01-07T01:59:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14843v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.14843v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in LLMs highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI capabilities, particularly in solving complex logical tasks. However, it remains unclear whether the current practice of RLVR truly expands a model&#x27;s reasoning boundary or mainly amplifies high-reward outputs that the base model already knows, leading to improved precision. This study presents an empirical investigation that provides new insights into the potential limits of the common RLVR recipe. We examine how, under current training conditions, RLVR can operate as a support-constrained optimization mechanism that may restrict the discovery of entirely novel solutions, remaining constrained by the base model&#x27;s initial distribution. We also identify an entropy-reward trade-off: while the current RLVR recipe reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments show that although the current RLVR recipe consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, it leads to greater uncertainty at each generation step but declining answer-level entropy. This suggests that these seemingly more uncertain generation paths ultimately converge onto a smaller set of distinct answers. Taken together, our findings reveal potential limits of the current RLVR recipe in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations, such as explicit exploration mechanisms or hybrid strategies that allocate probability mass to underrepresented solution regions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无形之链：RLVR为何可能或无法突破其起源局限</div>
<div class="mono" style="margin-top:8px">近期大语言模型的发展突显了可验证奖励强化学习（RLVR）作为提升AI能力、特别是解决复杂逻辑任务的有效方法。然而，当前RLVR实践究竟是真正拓展了模型的推理边界，还是主要放大了基础模型已知的高奖励输出从而提升精度，仍不明确。本研究通过实证分析，对常见RLVR方法的潜在局限提供了新见解。我们探讨了在当前训练条件下，RLVR如何作为一种支持受限的优化机制运行，可能限制全新解决方案的发现，并始终受限于基础模型的初始分布。我们还发现了一种熵-奖励权衡：虽然当前RLVR方法能可靠提升精度，但可能逐步缩小探索空间，并潜在忽略正确但未被充分代表的解决方案。大量实验表明，尽管当前RLVR方法持续提升pass@1指标，但在更大采样预算下，经验支持集的收缩通常超过其扩展，未能恢复基础模型原本可获得的正确答案。有趣的是，我们观察到虽然RLVR有时会增加词元级熵，导致每个生成步骤的不确定性上升，但答案级熵却在下降。这表明这些看似更不确定的生成路径最终会收敛到更小的答案集合中。综合来看，我们的研究揭示了当前RLVR方法在拓展推理视野方面的潜在局限。要打破这条无形之链，可能需要未来的算法创新，例如显式探索机制或将概率质量分配给未被充分代表解区域的混合策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether Reinforcement Learning with Verifiable Rewards (RLVR) genuinely expands the reasoning capabilities of large language models or merely refines existing knowledge, motivated by concerns that it may act as a support-constrained optimizer limited by the base model&#x27;s initial output distribution. The method involves empirical analysis of RLVR&#x27;s training dynamics, revealing an entropy-reward trade-off where increased precision comes at the cost of narrowed exploration. Experimental results show that RLVR consistently improves pass@1 scores but shrinks the empirical support of outputs, often failing to recover correct answers previously accessible to the base model, while increasing token-level entropy paradoxically leads to declining answer-level diversity, indicating convergence onto a smaller set of distinct solutions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于可验证奖励的强化学习（RLVR）是真正扩展了大语言模型的推理能力，还是仅仅优化了其已有知识，其动机在于担心RLVR可能作为一种支持受限的优化器，受限于基础模型的初始输出分布。方法上通过对RLVR训练动态的实证分析，揭示了一种熵-奖励权衡，即精度的提升以探索范围收窄为代价。实验结果表明，RLVR虽能持续提高pass@1分数，但缩小了输出的经验支持集，常常无法恢复基础模型先前可访问的正确答案，同时词元级熵的增加反而导致答案级多样性下降，表明模型收敛到更小的解集合中。</div>
</details>
</div>
<div class="card">
<div class="title">Multiplayer Nash Preference Optimization</div>
<div class="meta-line">Authors: Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi</div>
<div class="meta-line">First: 2025-09-27T04:18:33+00:00 · Latest: 2026-01-07T01:54:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23102v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23102v2">PDF</a> · <a href="https://github.com/smiles724/MNPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. This work introduces Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an n-player game, where each policy competes against a population of opponents while being regularized toward a reference model. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Comprehensive empirical evaluation shows that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多人纳什偏好优化</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习已成为将大语言模型与人类偏好对齐的标准范式。然而，基于布拉德利-特里假设的奖励方法难以捕捉现实偏好的非传递性与异质性。为此，近期研究将对齐问题重构为双人纳什博弈，催生了基于人类反馈的纳什学习。尽管该视角已衍生出INPO、ONPO、EGPO等具备坚实理论与实证保证的算法，其本质上仍受限于双人交互框架，存在单一对手偏差，无法完整刻画现实偏好结构的复杂性。本文提出多人纳什偏好优化框架，将NLHF推广至多人博弈体系，将对齐问题建模为n人博弈——每个策略在与对手群体竞争的同时，受参考模型的正则化约束。我们证明MNPO在继承双人方法均衡保证的基础上，能实现更丰富的竞争动态与更全面的偏好结构覆盖。系统性实验表明，在指令遵循基准测试中，MNPO持续优于现有NLHF基线方法，在异质标注者条件与混合策略评估场景下均取得更优的对齐质量。这些成果共同确立了MNPO作为应对复杂非传递人类偏好的原则性可扩展框架。代码已开源：https://github.com/smiles724/MNPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of two-player Nash learning from human feedback (NLHF) methods, which fail to capture the full complexity of real-world non-transitive and heterogeneous preferences, this paper introduces Multiplayer Nash Preference Optimization (MNPO), a framework that generalizes alignment to an n-player game where each policy competes against a population of opponents while being regularized toward a reference model. The method inherits equilibrium guarantees from two-player approaches but enables richer competitive dynamics and better coverage of diverse preference structures. Experimental results demonstrate that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios.</div>
<div class="mono" style="margin-top:8px">针对现有基于两人纳什博弈的人类反馈学习（NLHF）方法难以捕捉现实世界中非传递性和异质性偏好复杂性的问题，本文提出了多人纳什偏好优化（MNPO）框架，将对齐问题推广为n人博弈，其中每个策略在与对手群体竞争的同时向参考模型正则化。该方法继承了两人方法的均衡保证，但实现了更丰富的竞争动态和对多样化偏好结构的更好覆盖。实验结果表明，在指令遵循基准测试中，MNPO持续优于现有的NLHF基线，在异质标注者条件和混合策略评估场景下实现了更优的对齐质量。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
