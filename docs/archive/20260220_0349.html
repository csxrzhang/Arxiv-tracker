<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-20 03:49</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260220_0349</div>
    <div class="row"><div class="card">
<div class="title">Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment</div>
<div class="meta-line">Authors: Shuta Kikuchi, Shu Tanaka</div>
<div class="meta-line">First: 2026-02-18T17:32:55+00:00 · Latest: 2026-02-18T17:32:55+00:00</div>
<div class="meta-line">Comments: 17 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16643v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16643v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于二次优化退火的因子分解机在RNA逆折叠中的应用及二进制整数编码与核苷酸分配评估</div>
<div class="mono" style="margin-top:8px">RNA逆折叠问题旨在识别优先采用给定目标二级结构的核苷酸序列。尽管已有多种启发式和基于机器学习的方法被提出，但许多方法需要大量序列评估，这在实验验证成本高昂时限制了其适用性。本研究提出一种使用二次优化退火因子分解机（FMQA）解决该问题的方法。FMQA是一种离散黑盒优化方法，据报道能以有限评估次数获得高质量解。将FMQA应用于此问题需将核苷酸转换为二进制变量，但整数-核苷酸分配和二进制整数编码对FMQA性能的影响尚未得到深入研究——这些选择决定了代理模型的结构和搜索空间，从而直接影响解的质量。因此，本研究旨在建立新的RNA逆折叠FMQA框架，并分析这些分配与编码方法的影响。我们评估了四种核苷酸与有序整数（0-3）之间所有24种可能的分配组合，并结合四种二进制整数编码方法。结果表明，在归一化整体缺陷值方面，独热编码和畴壁编码优于二进制编码和一元编码。在畴壁编码中，分配给边界整数（0和3）的核苷酸出现频率更高。在RNA逆折叠问题中，将鸟嘌呤和胞嘧啶分配给这些边界整数可促进其在茎区的富集，从而获得比独热编码热力学稳定性更高的二级结构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the RNA inverse folding problem, which seeks nucleotide sequences that fold into a target secondary structure, motivated by the need for efficient methods that minimize costly experimental evaluations. The authors propose a novel framework using a factorization machine with quadratic-optimization annealing (FMQA), a discrete black-box optimization technique designed to yield high-quality solutions with limited evaluations, and systematically investigate the impact of binary-integer encoding schemes and nucleotide-to-integer assignments on performance. Experimental results show that one-hot and domain-wall encodings outperform binary and unary encodings in terms of normalized ensemble defect, with domain-wall encoding particularly favoring assignments of guanine and cytosine to boundary integers, thereby enriching stem regions and enhancing thermodynamic stability compared to one-hot encoding.</div>
<div class="mono" style="margin-top:8px">本研究针对RNA逆折叠问题，旨在寻找能折叠成目标二级结构的核苷酸序列，其动机在于需要开发高效方法以减少昂贵的实验验证成本。作者提出了一种新颖的框架，采用因子分解机与二次优化退火（FMQA）相结合，这是一种离散黑盒优化技术，旨在通过有限评估获得高质量解，并系统研究了二进制整数编码方案和核苷酸到整数分配对性能的影响。实验结果表明，在归一化集成缺陷值方面，独热编码和畴壁编码优于二进制和一元编码，其中畴壁编码特别倾向于将鸟嘌呤和胞嘧啶分配给边界整数，从而富集茎区结构，相比独热编码能增强热力学稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes</div>
<div class="meta-line">Authors: Ethan Blaser, Jiuqi Wang, Shangtong Zhang</div>
<div class="meta-line">First: 2026-02-18T17:24:27+00:00 · Latest: 2026-02-18T17:24:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16629v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16629v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平均奖励马尔可夫决策过程中差分时序差分学习的几乎必然收敛性</div>
<div class="mono" style="margin-top:8px">平均奖励是强化学习中关注智能体长期性能的基本度量指标。差分时序差分学习算法是平均奖励强化学习的重要进展，为同策略和异策略场景下学习与平均奖励相关的价值函数提供了高效的在线方法。然而，现有收敛性证明要求学习率与状态访问次数绑定的局部时钟机制，这在实际应用中未被采用且无法推广至表格化设定之外。我们通过证明任意n步同策略差分时序差分算法在使用标准递减学习率（无需局部时钟）时具有几乎必然收敛性，解决了这一局限。随后推导出三个充分条件，证明异策略n步差分时序差分算法在无局部时钟时同样收敛。这些成果强化了差分时序差分的理论基础，使其收敛性分析更贴近实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to strengthen the theoretical foundations of differential temporal difference (TD) learning for average reward Markov decision processes, as existing convergence guarantees rely on impractical local clock learning rates that do not extend beyond tabular settings. The method involves proving the almost sure convergence of on-policy n-step differential TD using standard diminishing learning rates without a local clock, and deriving three sufficient conditions for the convergence of off-policy n-step differential TD under the same practical learning rate scheme. The main experimental results demonstrate that both on-policy and off-policy algorithms converge almost surely under these more realistic conditions, thereby aligning theoretical analysis closer to practical implementations in reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于加强平均奖励马尔可夫决策过程中差分时序差分学习的理论基础，因为现有的收敛保证依赖于不切实际的局部时钟学习率，且无法扩展到表格设置之外。方法包括证明在无需局部时钟的标准递减学习率下，同策略n步差分时序差分算法几乎必然收敛，并推导出三个充分条件以确保异策略n步差分时序差分算法在相同实用学习率方案下的收敛性。主要实验结果表明，在这些更现实的条件下，同策略和异策略算法均几乎必然收敛，从而将理论分析与强化学习中的实际应用更紧密地结合起来。</div>
</details>
</div>
<div class="card">
<div class="title">ReaCritic: Reasoning Transformer-based DRL Critic-model Scaling For Wireless Networks</div>
<div class="meta-line">Authors: Feiran You, Hongyang Du</div>
<div class="meta-line">First: 2025-05-16T08:42:08+00:00 · Latest: 2026-02-18T16:45:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.10992v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.10992v2">PDF</a> · <a href="https://github.com/NICE-HKU/ReaCritic">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Heterogeneous Networks (HetNets) pose critical challenges for intelligent management due to the diverse user requirements and time-varying wireless conditions. These factors introduce significant decision complexity, which limits the adaptability of existing Deep Reinforcement Learning (DRL) methods. In many DRL algorithms, especially those involving value-based or actor-critic structures, the critic component plays a key role in guiding policy learning by estimating value functions. However, conventional critic models often use shallow architectures that map observations directly to scalar estimates, limiting their ability to handle multi-task complexity. In contrast, recent progress in inference-time scaling of Large Language Models (LLMs) has shown that generating intermediate reasoning steps can significantly improve decision quality. Motivated by this, we propose ReaCritic, a reasoning transformer-based critic-model scaling scheme that brings reasoning-like ability into DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with a broad range of value-based and actor-critic DRL algorithms and enhances generalization in dynamic wireless environments. Extensive experiments demonstrate that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks. The code of ReaCritic is available at https://github.com/NICE-HKU/ReaCritic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReaCritic：基于推理Transformer的无线网络DRL评论家模型扩展方法</div>
<div class="mono" style="margin-top:8px">异构网络因用户需求多样性与无线环境时变性，给智能管理带来严峻挑战，这些因素导致决策复杂度显著增加，限制了现有深度强化学习方法的自适应能力。在众多DRL算法（尤其是基于价值或执行者-评论家架构的算法）中，评论家组件通过估计价值函数对策略学习起关键指导作用。然而传统评论家模型常采用浅层架构，直接将观测映射为标量估计，限制了其处理多任务复杂性的能力。相比之下，近期大语言模型在推理时扩展方面的进展表明，生成中间推理步骤能显著提升决策质量。受此启发，我们提出ReaCritic——一种基于推理Transformer的评论家模型扩展方案，将类推理能力引入DRL。ReaCritic通过并行状态-动作输入进行横向推理，并借助深度Transformer堆栈实现纵向推理。该方案兼容广泛的基于价值及执行者-评论家DRL算法，能增强动态无线环境中的泛化能力。大量实验表明，ReaCritic在多种异构网络场景和标准OpenAI Gym控制任务中均能提升收敛速度与最终性能。代码已开源：https://github.com/NICE-HKU/ReaCritic。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of conventional Deep Reinforcement Learning (DRL) critic models, which use shallow architectures that struggle with the decision complexity in dynamic Heterogeneous Networks (HetNets). The authors propose ReaCritic, a novel critic-model scaling scheme that integrates reasoning capabilities by employing a transformer architecture to perform horizontal reasoning over parallel state-action inputs and vertical reasoning through deep stacks, enhancing its compatibility with various value-based and actor-critic DRL algorithms. Experimental results across HetNet settings and standard control tasks show that ReaCritic improves both convergence speed and final performance compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对传统深度强化学习评论家模型在动态异构网络中因架构浅显而难以处理决策复杂性的问题，提出了一种名为ReaCritic的新型评论家模型扩展方案。该方法通过引入基于Transformer的架构，对并行状态-动作输入进行横向推理，并通过深层堆栈进行纵向推理，从而增强了与多种基于值和演员-评论家算法的兼容性。在异构网络环境和标准控制任务上的广泛实验表明，ReaCritic相比现有方法在收敛速度和最终性能上均有显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">A Scalable Approach to Solving Simulation-Based Network Security Games</div>
<div class="meta-line">Authors: Michael Lanier, Yevgeniy Vorobeychik</div>
<div class="meta-line">First: 2026-02-18T16:07:01+00:00 · Latest: 2026-02-18T16:07:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16564v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16564v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种可扩展的基于仿真的网络安全博弈求解方法</div>
<div class="mono" style="margin-top:8px">本文提出MetaDOAR，一种轻量级元控制器，通过引入基于学习的、分区感知的过滤层与Q值缓存机制，扩展了双Oracle/PSRO范式，从而支持在超大规模网络环境中进行可扩展的多智能体强化学习。MetaDOAR从节点结构嵌入中学习紧凑的状态投影，快速评分并选择一小部分设备（top-k分区），由传统底层执行器结合评论家智能体进行聚焦束搜索。候选动作通过批量评论家前向传播评估，并存储在以量化状态投影和局部动作标识为键的LRU缓存中，通过保守的k跳缓存失效策略，在保持决策质量的同时显著减少冗余的评论家计算。实验表明，在大型网络拓扑中，MetaDOAR在内存使用和训练时间未出现显著扩展问题的前提下，获得了优于现有先进基线的玩家收益。该研究为大规模网络化决策问题的高效分层策略学习提供了兼具理论依据与实践可行性的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for scalable solutions in large-scale cyber-network security games, this paper introduces MetaDOAR, a meta-controller that enhances the Double Oracle/PSRO framework with a learned partition-aware filter and Q-value caching. The method projects network states into compact embeddings to select a critical subset of devices, enabling a low-level actor to perform focused beam search guided by a critic, while caching mechanisms with invalidation reduce redundant computations. Experimental results demonstrate that MetaDOAR achieves higher player payoffs than state-of-the-art baselines on large topologies, without significant increases in memory or training time, offering an efficient hierarchical approach for networked decision-making.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大规模网络攻防博弈中的可扩展性问题，提出了MetaDOAR这一元控制器，它通过引入基于分区的学习过滤层和Q值缓存来增强双Oracle/PSRO框架。该方法将网络状态投影为紧凑嵌入，以快速筛选关键设备子集，使底层执行器能在评论家智能体的引导下进行聚焦束搜索，同时利用缓存及失效机制减少冗余计算。实验结果表明，在大型网络拓扑上，MetaDOAR相比现有先进基线获得了更高的玩家收益，且未显著增加内存或训练时间开销，为大规模网络化决策问题提供了高效的分层策略学习路径。</div>
</details>
</div>
<div class="card">
<div class="title">RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion</div>
<div class="meta-line">Authors: Tianmeng Hu, Yongzheng Cui, Biao Luo, Ke Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-18T15:52:26+00:00 · Latest: 2026-02-18T15:52:26+00:00</div>
<div class="meta-line">Comments: Accepted as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16548v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RIDER：基于强化学习引导扩散的RNA三维结构逆向设计</div>
<div class="mono" style="margin-top:8px">RNA三维结构的逆向设计对于合成生物学和治疗学中的功能RNA工程至关重要。尽管近期的深度学习方法推动了该领域发展，但这些方法通常以天然序列恢复率为优化和评估指标，这作为结构保真度的替代指标存在局限——不同序列可能折叠为相似的三维结构，且高恢复率未必意味着正确折叠。为解决此问题，我们提出RIDER框架，通过强化学习直接优化三维结构相似性。首先，我们开发并预训练了基于图神经网络的生成扩散模型，该模型以目标三维结构为条件，在天然序列恢复率上较现有最优方法提升9%。随后，我们采用改进的策略梯度算法，基于四项三维自洽度量指标构建任务特定奖励函数，对模型进行微调。实验结果表明，RIDER在所有指标上均将结构相似性提升超100%，并能发现与天然序列不同的设计方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of using native sequence recovery as a proxy for structural fidelity in RNA 3D inverse design, proposing RIDER to directly optimize for 3D structural similarity. The method involves pre-training a GNN-based diffusion model conditioned on target structures, which improves native sequence recovery by 9%, followed by fine-tuning with a policy gradient algorithm using 3D self-consistency rewards. Experimental results demonstrate that RIDER enhances structural similarity by over 100% across metrics and generates designs distinct from native sequences.</div>
<div class="mono" style="margin-top:8px">该论文针对RNA三维逆向设计中以原生序列恢复作为结构保真度代理的局限性，提出了RIDER框架以直接优化三维结构相似性。方法包括预训练一个基于目标结构条件的图神经网络扩散模型，将原生序列恢复率提升了9%，随后使用基于三维自洽度量的奖励函数通过策略梯度算法进行微调。实验结果表明，RIDER在所有指标上均将结构相似性提高了100%以上，并生成了不同于原生序列的设计。</div>
</details>
</div>
<div class="card">
<div class="title">Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation</div>
<div class="meta-line">Authors: Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie</div>
<div class="meta-line">First: 2025-10-21T09:57:44+00:00 · Latest: 2026-02-18T15:47:54+00:00</div>
<div class="meta-line">Comments: Accepted into AAMAS &#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18478v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.18478v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring the safe exploration of reinforcement learning (RL) agents is critical for deployment in real-world systems. Yet existing approaches struggle to strike the right balance: methods that tightly enforce safety often cripple task performance, while those that prioritize reward leave safety constraints frequently violated, producing diffuse cost landscapes that flatten gradients and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a novel approach that integrates uncertainty-aware modulation and refinement into critic training. By concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas, USC enables policies to achieve effective reward-safety trade-offs. Extensive experiments show that USC reduces safety violations by approximately 40% while maintaining competitive or higher rewards, and reduces the error between predicted and true cost gradients by approximately 83%, breaking the prevailing trade-off between safety and performance and paving the way for scalable safe RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全而不遗憾：通过不确定性感知调制降低安全评判器的过度保守性</div>
<div class="mono" style="margin-top:8px">确保强化学习（RL）智能体的安全探索对于实际系统部署至关重要。然而现有方法难以取得恰当平衡：严格保障安全的方法常严重损害任务性能，而优先追求奖励的方法则频繁违反安全约束，产生使梯度平坦化并阻碍策略改进的弥散代价景观。我们提出不确定性安全评判器（USC），这是一种将不确定性感知调制与精细化训练融入评判器训练的新方法。通过将保守性集中在不确定且高代价区域，同时在安全区域保持锐利梯度，USC使策略能实现有效的奖励-安全权衡。大量实验表明，USC在保持竞争力或更高奖励的同时，将安全违规减少约40%，并将预测与真实代价梯度间的误差降低约83%，打破了安全与性能间的普遍权衡，为可扩展的安全RL开辟了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of balancing safety and performance in reinforcement learning, where existing methods either overly constrain task performance or frequently violate safety. The authors propose the Uncertain Safety Critic (USC), which incorporates uncertainty-aware modulation to focus conservatism on uncertain, high-cost regions while maintaining sharp gradients in safe areas, enabling better trade-offs. Experimental results demonstrate that USC reduces safety violations by about 40% while preserving or improving rewards, and cuts the error between predicted and true cost gradients by approximately 83%, effectively breaking the traditional safety-performance trade-off.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中安全性与性能难以平衡的问题，现有方法往往过度约束任务性能或频繁违反安全限制。作者提出了不确定性安全评论家（USC），通过不确定性感知调制将保守性集中在不确定的高成本区域，同时在安全区域保持锐利梯度，以实现更好的权衡。实验结果表明，USC将安全违规减少了约40%，同时保持或提高了奖励，并将预测与真实成本梯度之间的误差降低了约83%，有效打破了传统安全与性能之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Jialiang Fan, Shixiong Jiang, Mengyu Liu, Fanxin Kong</div>
<div class="meta-line">First: 2026-02-18T15:43:36+00:00 · Latest: 2026-02-18T15:43:36+00:00</div>
<div class="meta-line">Comments: 12 pages, 6 figures, supplementary material included</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16543v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16543v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy&#x27;s gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy&#x27;s internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于逆向约束强化学习的强化学习安全性漏洞分析</div>
<div class="mono" style="margin-top:8px">安全强化学习旨在确保策略性能的同时满足安全约束。然而，现有方法多假设环境友好，使其易受现实场景中常见对抗性扰动的影响。此外，基于梯度的对抗攻击通常需获取策略梯度信息，这在实际中往往难以实现。为此，我们提出一种对抗攻击框架以揭示安全强化学习策略的脆弱性。通过专家示范和黑盒环境交互，本框架学习约束模型与代理策略，可在无需受害者策略内部梯度或真实安全约束的情况下实现梯度攻击优化。我们进一步通过理论分析论证可行性并推导扰动边界。在多个安全强化学习基准上的实验验证了本方法在有限权限访问下的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of Safe Reinforcement Learning (Safe RL) policies, which are typically designed for benign environments, to adversarial perturbations in real-world settings. The authors propose an adversarial attack framework that uses expert demonstrations and black-box environment interactions to learn a constraint model and a surrogate policy, enabling gradient-based attack optimization without needing the victim policy&#x27;s internal gradients or ground-truth safety constraints. Experimental results on multiple Safe RL benchmarks demonstrate the effectiveness of this approach in revealing policy vulnerabilities under limited access, supported by theoretical analysis on feasibility and perturbation bounds.</div>
<div class="mono" style="margin-top:8px">本文针对安全强化学习策略在现实对抗环境中易受攻击的脆弱性问题展开研究，这些策略通常假设环境是良性的。作者提出了一种对抗攻击框架，利用专家演示和黑盒环境交互学习约束模型和代理策略，从而在不依赖受害者策略内部梯度或真实安全约束的情况下实现基于梯度的攻击优化。在多个安全强化学习基准测试上的实验结果表明，该方法在有限访问权限下能有效揭示策略漏洞，并辅以关于可行性和扰动界限的理论分析支持。</div>
</details>
</div>
<div class="card">
<div class="title">Capacity-constrained demand response in smart grids using deep reinforcement learning</div>
<div class="meta-line">Authors: Shafagh Abband Pashaki, Sepehr Maleki, Amir Badiee</div>
<div class="meta-line">First: 2026-02-18T15:13:07+00:00 · Latest: 2026-02-18T15:13:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16525v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16525v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a capacity-constrained incentive-based demand response approach for residential smart grids. It aims to maintain electricity grid capacity limits and prevent congestion by financially incentivising end users to reduce or shift their energy consumption. The proposed framework adopts a hierarchical architecture in which a service provider adjusts hourly incentive rates based on wholesale electricity prices and aggregated residential load. The financial interests of both the service provider and end users are explicitly considered. A deep reinforcement learning approach is employed to learn optimal real-time incentive rates under explicit capacity constraints. Heterogeneous user preferences are modelled through appliance-level home energy management systems and dissatisfaction costs. Using real-world residential electricity consumption and price data from three households, simulation results show that the proposed approach effectively reduces peak demand and smooths the aggregated load profile. This leads to an approximately 22.82% reduction in the peak-to-average ratio compared to the no-demand-response case.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的智能电网容量约束需求响应</div>
<div class="mono" style="margin-top:8px">本文提出一种面向居民智能电网的容量约束激励型需求响应方法，旨在通过经济激励引导终端用户减少或转移用电，以维持电网容量限制并预防拥堵。该框架采用分层架构，服务商根据批发电价和居民总负荷动态调整小时级激励费率，并明确兼顾服务商与终端用户的经济利益。通过深度强化学习在显式容量约束下优化实时激励费率，并借助设备级家庭能源管理系统与用户不满成本建模异质性用电偏好。基于三户家庭的实际用电与电价数据仿真表明，该方法能有效降低峰值需求、平滑总负荷曲线，相比无需求响应情景，峰均比降低约22.82%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to maintain grid capacity and prevent congestion in residential smart grids, this paper proposes a capacity-constrained, incentive-based demand response framework. The method employs a hierarchical architecture where a service provider sets hourly incentives, using deep reinforcement learning to optimize these rates under explicit capacity constraints while modeling user preferences via appliance-level management and dissatisfaction costs. Experimental simulations with real-world data from three households demonstrate that the approach effectively reduces peak demand and smooths the aggregated load, achieving an approximately 22.82% reduction in the peak-to-average ratio compared to scenarios without demand response.</div>
<div class="mono" style="margin-top:8px">本文旨在维持住宅智能电网的容量限制并防止拥塞，提出了一种基于容量约束的激励型需求响应方法。该方法采用分层架构，服务提供商根据批发电价和总住宅负荷调整每小时激励费率，并利用深度强化学习在明确容量约束下学习最优实时激励率，同时通过设备级家庭能源管理系统和用户不满意成本来建模异构用户偏好。基于三个家庭的实际用电和价格数据进行仿真，结果表明该方法有效降低了峰值需求并平滑了总负荷曲线，与无需求响应的情况相比，峰均比降低了约22.82%。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study</div>
<div class="meta-line">Authors: Gerhard Stenzel, Isabella Debelic, Michael Kölle, Tobias Rohe, Leo Sünkel, Julian Hager, Claudia Linnhoff-Popien</div>
<div class="meta-line">First: 2026-02-18T15:10:43+00:00 · Latest: 2026-02-18T15:10:43+00:00</div>
<div class="meta-line">Comments: Extended version of a short paper to be published at ICAART 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16523v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16523v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \(R_x\), \(R_y\), and \(R_z\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \(λ\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \(5\times10^{-4}\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \(10^{-4}\)). Both approaches reliably reconstruct computational basis states (between 83\% and 99\% success) and Bell states (between 61\% and 77\% success). However, scalability saturates for \(λ\) of approximately three to four and does not extend to ten-qubit targets even at \(λ=2\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习在参数化量子态制备中的比较研究</div>
<div class="mono" style="margin-top:8px">我们将强化学习从纯离散门选择扩展到包含连续单量子比特旋转（\(R_x\)、\(R_y\)、\(R_z\)）的参数化量子态制备，以增强定向量子电路合成（DQCS）。我们比较了两种训练模式：一种是单阶段智能体，联合选择门类型、作用量子比特及旋转角度；另一种是两阶段变体，首先生成离散电路，随后利用参数偏移梯度通过Adam优化旋转角度。基于Gymnasium和PennyLane平台，我们在2至10量子比特系统及复杂度递增（\(λ\)取值1至5）的目标态上评估了近端策略优化（PPO）和优势演员-评论家（A2C）算法。A2C在此场景中未能习得有效策略，而PPO在稳定超参数下取得成功（单阶段：学习率约\(5\times10^{-4}\)，自保真误差阈值为0.01；两阶段：学习率约\(10^{-4}\)）。两种方法均能可靠重构计算基态（成功率83%至99%）和贝尔态（成功率61%至77%）。但可扩展性在\(λ\)约3至4时趋于饱和，即使\(λ=2\)也无法扩展至10量子比特目标。两阶段方法仅带来边际精度提升，却需约三倍运行时间。基于固定计算资源的实用性考量，我们推荐采用单阶段PPO策略，提供显式合成电路示例，并通过与经典变分基线的对比，为提升可扩展性指明改进方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study extends directed quantum circuit synthesis with reinforcement learning to handle continuous rotation parameters for quantum state preparation, comparing a one-stage agent that jointly selects gates and angles with a two-stage method that first proposes a discrete circuit then optimizes angles. Using Proximal Policy Optimization and Advantage Actor–Critic on systems of two to ten qubits, the authors found that PPO succeeded with stable hyperparameters while A2C failed, achieving high success rates for computational basis states (83–99%) and Bell states (61–77%), but scalability plateaued for complexity levels around λ=3–4 and did not extend to ten-qubit targets. The two-stage approach offered only slight accuracy improvements at triple the runtime, leading to a recommendation for the one-stage PPO policy for practical use under fixed compute budgets.</div>
<div class="mono" style="margin-top:8px">本研究将强化学习应用于参数化量子态制备，扩展了定向量子电路合成方法，以处理连续旋转参数，比较了联合选择门和角度的一阶段代理与先提出离散电路再优化角度的两阶段方法。使用近端策略优化和优势演员-评论家算法在二至十个量子比特系统上进行评估，作者发现PPO在稳定超参数下成功而A2C失败，对于计算基态（83–99%）和贝尔态（61–77%）实现了高成功率，但可扩展性在复杂度λ约3–4时达到饱和，且无法扩展到十个量子比特目标。两阶段方法仅带来轻微精度提升但运行时增加三倍，因此建议在实际计算预算下采用一阶段PPO策略。</div>
</details>
</div>
<div class="card">
<div class="title">Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Guanning Zeng, Zhaoyi Zhou, Daman Arora, Andrea Zanette</div>
<div class="meta-line">First: 2025-11-05T18:43:15+00:00 · Latest: 2026-02-18T15:08:01+00:00</div>
<div class="meta-line">Comments: Preprint. Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.03710v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.03710v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for post-training large reasoning models (LRMs) using policy-gradient methods such as GRPO. To stabilize training, these methods typically center trajectory rewards by subtracting the empirical mean reward for each prompt. Statistically, this centering acts as a control variate (baseline), reducing the variance of the policy-gradient estimator. In practice, the mean reward is estimated using per-prompt empirical averages computed from the generations for each prompt in a batch. Motivated by Stein&#x27;s paradox, we propose shrinkage estimators that combine per-prompt and across-prompt means to improve per-prompt mean estimation accuracy, especially in the low-generation regime typical of RLVR. Theoretically, we construct a shrinkage-based baseline that provably yields lower-variance policy-gradient estimators across algorithms. Our baseline is a drop-in replacement for standard per-prompt mean baselines and requires no additional hyperparameters or computation. Empirically, shrinkage baselines consistently outperform empirical-mean baselines, producing lower-variance gradient updates and improved training stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缩小方差：基于可验证奖励的强化学习收缩基线方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为使用GRPO等策略梯度方法对大型推理模型进行后训练的强大范式。为稳定训练，这些方法通常通过减去每个提示的实证平均奖励来对轨迹奖励进行中心化处理。从统计学角度看，这种中心化相当于控制变量（基线），能降低策略梯度估计量的方差。实践中，平均奖励是通过批次中每个提示生成结果的逐提示经验平均值进行估计的。受斯坦悖论启发，我们提出收缩估计量，通过结合逐提示与跨提示均值来提升逐提示均值估计精度，尤其适用于RLVR典型的低生成量场景。理论上，我们构建了基于收缩的基线，可证明在不同算法中产生更低方差的策略梯度估计量。该基线可直接替代标准的逐提示均值基线，无需额外超参数或计算开销。实证表明，收缩基线始终优于经验均值基线，能产生更低方差的梯度更新并提升训练稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high variance in policy-gradient estimators used in Reinforcement Learning with Verifiable Rewards (RLVR) for training large reasoning models. Motivated by Stein&#x27;s paradox, the authors propose a novel baseline method that uses shrinkage estimators to combine per-prompt and across-prompt mean rewards, improving the accuracy of mean reward estimation, particularly when few generations are available per prompt. Theoretically, this shrinkage-based baseline provably reduces the variance of the gradient estimator. Empirically, it outperforms standard empirical-mean baselines, leading to lower-variance gradient updates and enhanced training stability, all without requiring extra hyperparameters or computational cost.</div>
<div class="mono" style="margin-top:8px">本文针对使用可验证奖励的强化学习训练大型推理模型时策略梯度估计器方差高的问题展开研究。受斯坦悖论启发，作者提出了一种新的基线方法，采用收缩估计器来结合每个提示内部和跨提示的平均奖励，从而提高了平均奖励估计的准确性，尤其在每个提示生成样本较少的典型场景下效果显著。理论上，这种基于收缩的基线被证明能降低梯度估计器的方差。实验结果表明，该基线 consistently 优于标准的经验均值基线，实现了更低方差的梯度更新和更好的训练稳定性，且无需额外的超参数或计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens</div>
<div class="meta-line">Authors: Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng, Jiang Wu, Yinuo Wang, Yang Guan, Kehua Sheng, Bo Zhang, Keqiang Li, Jingliang Duan, Shengbo Eben Li</div>
<div class="meta-line">First: 2026-02-17T14:46:48+00:00 · Latest: 2026-02-18T14:13:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15620v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15620v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% ($ρ_{\mathrm{T}}$=1.0, top-p=1.0) and 3.69\% ($ρ_{\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STAPO：通过抑制罕见伪标记实现大语言模型强化学习的稳定化</div>
<div class="mono" style="margin-top:8px">强化学习显著提升了大语言模型的推理能力，但现有RL微调方法严重依赖熵正则化与权重调整等启发式技术以维持稳定性，实践中常出现后期性能崩溃，导致推理质量下降与训练失稳。分析表明，RL中逐标记策略梯度的幅度与标记概率及局部策略熵呈负相关。我们发现训练失稳可能由约0.01%的极少数标记引发，这类标记被称为伪标记。当伪标记出现在正确响应中时，其对推理结果的贡献微乎其微，却继承了完整的序列级奖励，导致梯度更新异常放大。为缓解此问题，我们设计了S2T机制，通过低概率、低熵与正优势值的特征信号高效识别伪标记，并在优化过程中抑制其梯度扰动。将该机制融入分组目标函数，我们提出了伪标记感知策略优化方法STAPO，可促进大规模模型稳定有效的精调。在基于Qwen 1.7B、8B和14B基础模型的六个数学推理基准测试中，STAPO始终展现出更优的熵稳定性，相比GRPO、20-Entropy和JustRL方法，平均性能分别提升7.13%（$ρ_{\mathrm{T}}$=1.0，top-p=1.0）与3.69%（$ρ_{\mathrm{T}}$=0.7，top-p=0.9）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the instability and performance collapse in existing reinforcement learning fine-tuning methods for large language models, this paper identifies that a minuscule fraction of spurious tokens with low probability and low entropy can cause abnormally amplified gradient updates, degrading reasoning quality. The method introduces a Spurious-Token-Aware Policy Optimization (STAPO) framework, which incorporates a silencing mechanism (S2T) to detect and suppress gradient perturbations from these rare tokens during optimization. Experimental results across six mathematical reasoning benchmarks with Qwen models show that STAPO achieves superior training stability and average performance improvements of 7.13% and 3.69% over baseline methods like GRPO.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型强化学习微调方法中存在的训练不稳定和后期性能崩溃问题，本文发现极少数的伪标记（约占0.01%）因其低概率和低熵特性会导致梯度异常放大，从而损害推理质量。方法上提出了伪标记感知策略优化（STAPO），通过引入静默机制（S2T）来高效识别并抑制这些伪标记在优化过程中的梯度扰动。在六个数学推理基准测试中使用Qwen系列模型的实验结果表明，STAPO显著提升了训练稳定性，相比GRPO等基线方法平均性能提高了7.13%和3.69%。</div>
</details>
</div>
<div class="card">
<div class="title">Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Arun Vignesh Malarkkan, Wangyang Ying, Yanjie Fu</div>
<div class="meta-line">First: 2026-02-18T13:12:11+00:00 · Latest: 2026-02-18T13:12:11+00:00</div>
<div class="meta-line">Comments: 11 Pages, References and Appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16435v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16435v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于因果引导的多智能体强化学习自动化特征工程</div>
<div class="mono" style="margin-top:8px">自动化特征工程（AFE）使AI系统能够从原始表格数据中自主构建高效用的表征。然而，现有AFE方法依赖统计启发式，产生的特征在分布偏移下表现脆弱。我们提出CAFE框架，将AFE重新定义为因果引导的序列决策过程，将因果发现与强化学习驱动的特征构建相结合。第一阶段通过特征与目标变量学习稀疏有向无环图，获取软因果先验，根据特征对目标的因果影响将其分组为直接、间接或其他类型。第二阶段采用级联多智能体深度Q学习架构选择因果组和变换算子，通过分层奖励塑造和因果组级探索策略，优先选择因果合理的变换并控制特征复杂度。在15个公开基准测试（分类任务采用宏F1；回归任务采用逆相对绝对误差）中，CAFE较主流AFE基线提升达7%，减少收敛所需回合数，并实现具有竞争力的目标达成时间。在受控协变量偏移下，CAFE相比非因果多智能体基线将性能下降降低约4倍，并生成更紧凑的特征集与更稳定的后验归因。这些发现表明，将因果结构作为软归纳先验而非刚性约束，可显著提升自动化特征工程的鲁棒性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the brittleness of existing automated feature engineering (AFE) methods under distribution shift, this paper introduces CAFE, a framework that reformulates AFE as a causally-guided sequential decision process. The method first learns a sparse directed acyclic graph to obtain soft causal priors, grouping features by their causal influence, and then employs a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators with hierarchical reward shaping and exploration strategies favoring causally plausible transformations. Experimental results on 15 public benchmarks show CAFE achieves up to 7% improvement over strong AFE baselines, reduces convergence episodes, and under covariate shift reduces performance drop by approximately fourfold compared to a non-causal multi-agent baseline, while producing more compact and stable feature sets.</div>
<div class="mono" style="margin-top:8px">针对现有自动化特征工程方法在分布偏移下表现脆弱的问题，本文提出了CAFE框架，将自动化特征工程重新定义为因果引导的序列决策过程。该方法首先学习稀疏有向无环图以获得软因果先验，根据特征与目标的因果影响将其分组，随后采用级联多智能体深度Q学习架构，通过分层奖励塑造和偏向因果合理转换的探索策略来选择因果组和转换算子。在15个公开基准测试上的实验结果表明，CAFE相比强大的自动化特征工程基线性能提升最高达7%，减少了收敛所需回合数，在协变量偏移下性能下降比非因果多智能体基线降低约四倍，并能生成更紧凑、特征归因更稳定的特征集。</div>
</details>
</div>
<div class="card">
<div class="title">Model-Agnostic Dynamic Feature Selection with Uncertainty Quantification</div>
<div class="meta-line">Authors: Javier Fumanal-Idocin, Raquel Fernandez-Peralta, Javier Andreu-Perez</div>
<div class="meta-line">First: 2025-08-04T16:21:43+00:00 · Latest: 2026-02-18T13:04:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.02566v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.02566v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic feature selection (DFS) addresses budget constraints in decision-making by sequentially acquiring features for each instance, making it appealing for resource-limited scenarios. However, existing DFS methods require models specifically designed for the sequential acquisition setting, limiting compatibility with models already deployed in practice. Furthermore, they provide limited uncertainty quantification, undermining trust in high-stakes decisions. In this work, we show that DFS introduces new uncertainty sources compared to the static setting. We formalise how model adaptation to feature subsets induces epistemic uncertainty, how standard imputation strategies bias aleatoric uncertainty estimation, and why predictive confidence fails to discriminate between good and bad selection policies. We also propose a model-agnostic DFS framework compatible with pre-trained classifiers, including interpretable-by-design models, through efficient subset reparametrization strategies. Empirical evaluation on tabular and image datasets demonstrates competitive accuracy against state-of-the-art greedy and reinforcement learning-based DFS methods with both neural and rule-based classifiers. We further show that the identified uncertainty sources persist across most existing approaches, highlighting the need for uncertainty-aware DFS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模型无关的动态特征选择与不确定性量化</div>
<div class="mono" style="margin-top:8px">动态特征选择（DFS）通过为每个实例顺序获取特征来解决决策中的预算约束，适用于资源受限场景。然而，现有DFS方法需要专门为顺序获取设置设计的模型，限制了与实践中已部署模型的兼容性。此外，其不确定性量化能力有限，削弱了高风险决策的可信度。本研究表明，与静态设置相比，DFS引入了新的不确定性来源：我们形式化分析了模型适应特征子集如何引发认知不确定性，标准插补策略如何偏置偶然不确定性估计，以及预测置信度为何无法区分优劣选择策略。同时，我们提出一种模型无关的DFS框架，通过高效子集重参数化策略兼容预训练分类器（包括可解释设计模型）。在表格和图像数据集上的实证评估表明，该框架在使用神经网络和基于规则的分类器时，相较于基于贪心算法和强化学习的前沿DFS方法具有竞争力。我们进一步揭示，所识别的不确定性来源在多数现有方法中持续存在，凸显了不确定性感知DFS的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing dynamic feature selection (DFS) methods, which are often model-specific and lack robust uncertainty quantification, hindering their trustworthiness in high-stakes applications. The authors propose a model-agnostic DFS framework that is compatible with pre-trained classifiers, including interpretable models, by employing efficient subset reparameterization strategies. Experimental results on tabular and image datasets show that the method achieves competitive accuracy compared to state-of-the-art greedy and reinforcement learning-based DFS approaches, while also demonstrating that common uncertainty sources in DFS persist across most existing methods, underscoring the need for uncertainty-aware solutions.</div>
<div class="mono" style="margin-top:8px">本文针对现有动态特征选择（DFS）方法的局限性展开研究，这些方法通常依赖于特定模型且缺乏可靠的不确定性量化，影响了在高风险决策中的可信度。作者提出了一种模型无关的DFS框架，通过高效的子集重参数化策略，使其能够兼容预训练的分类器，包括可解释模型。在表格和图像数据集上的实验结果表明，该方法与基于贪心或强化学习的最先进DFS方法相比具有竞争力的准确率，同时揭示了DFS中常见的不确定性来源在多数现有方法中普遍存在，强调了发展不确定性感知DFS的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Reinforcement Learning: Fast and Scalable Quantum Circuit Synthesis</div>
<div class="meta-line">Authors: Lukas Theißinger, Thore Gerlach, David Berghaus, Christian Bauckhage</div>
<div class="meta-line">First: 2026-02-16T19:43:43+00:00 · Latest: 2026-02-18T09:41:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15146v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15146v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum unitary synthesis addresses the problem of translating abstract quantum algorithms into sequences of hardware-executable quantum gates. Solving this task exactly is infeasible in general due to the exponential growth of the underlying combinatorial search space. Existing approaches suffer from misaligned optimization objectives, substantial training costs and limited generalization across different qubit counts. We mitigate these limitations by using supervised learning to approximate the minimum description length of residual unitaries and combining this estimate with stochastic beam search to identify near optimal gate sequences. Our method relies on a lightweight model with zero-shot generalization, substantially reducing training overhead compared to prior baselines. Across multiple benchmarks, we achieve faster wall-clock synthesis times while exceeding state-of-the-art methods in terms of success rate for complex circuits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越强化学习：快速可扩展的量子电路合成方法</div>
<div class="mono" style="margin-top:8px">量子幺正合成旨在将抽象量子算法转化为硬件可执行的量子门序列。由于底层组合搜索空间呈指数级增长，精确求解该任务通常不可行。现有方法存在优化目标失准、训练成本高昂及对不同量子比特数泛化能力有限等问题。我们通过监督学习逼近剩余幺正算符的最小描述长度，并将该估计与随机束搜索相结合以识别近似最优的门序列，从而缓解这些局限。该方法依赖具备零样本泛化能力的轻量模型，相比现有基线显著降低训练开销。在多项基准测试中，我们实现了更快的实际合成速度，同时在复杂电路成功率方面超越现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of quantum unitary synthesis, where the exponential search space makes exact solutions infeasible and existing methods suffer from misaligned objectives, high training costs, and poor generalization. The authors propose a method that uses supervised learning to approximate the minimum description length of residual unitaries and combines this with stochastic beam search to find near-optimal gate sequences. Experimental results across multiple benchmarks show that their lightweight, zero-shot generalization model achieves faster synthesis times and higher success rates for complex circuits compared to state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本文针对量子幺正合成问题展开研究，该问题由于指数级搜索空间而难以精确求解，现有方法存在优化目标不匹配、训练成本高和泛化能力差等局限。作者提出一种方法，利用监督学习近似残差幺正矩阵的最小描述长度，并结合随机束搜索来寻找接近最优的门序列。实验结果表明，在多个基准测试中，他们轻量级的零样本泛化模型相比现有最优方法，实现了更快的合成速度和更复杂的电路成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-agent cooperation through in-context co-player inference</div>
<div class="meta-line">Authors: Marissa A. Weis, Maciej Wołczyk, Rajai Nasser, Rif A. Saurous, Blaise Agüera y Arcas, João Sacramento, Alexander Meulemans</div>
<div class="meta-line">First: 2026-02-18T09:31:43+00:00 · Latest: 2026-02-18T09:31:43+00:00</div>
<div class="meta-line">Comments: 26 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16301v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16301v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between &quot;learning-aware&quot; agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between &quot;naive learners&quot; updating on fast timescales and &quot;meta-learners&quot; observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent&#x27;s in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过情境共玩家推断实现多智能体协作</div>
<div class="mono" style="margin-top:8px">在自利智能体间实现协作仍是多智能体强化学习的核心挑战。近期研究表明，通过考虑并塑造共玩家学习动态的“学习感知”智能体可诱导相互协作。然而，现有方法通常依赖对共玩家学习规则的硬编码假设（常存在不一致性），或强制划分“快速更新的朴素学习者”与“观察这些更新的元学习者”。本文证明，序列模型的情境学习能力可在无需硬编码假设或显式时间尺度分离的情况下实现共玩家学习感知。通过让序列模型智能体与多样化共玩家分布进行对抗训练，可自然诱导出情境最优响应策略，在快速情节内时间尺度上发挥学习算法功能。研究发现，先前工作中识别的协作机制——易受勒索的脆弱性驱动相互塑造——在此设定中自然涌现：情境适应使智能体易受勒索，而由此产生的相互施压塑造对手的情境学习动态，最终演化为协作行为的学习。结果表明，序列模型的标准分散式强化学习结合共玩家多样性，为学习协作行为提供了可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fostering cooperation among self-interested agents in multi-agent reinforcement learning, moving beyond prior methods that rely on hardcoded assumptions about opponent learning rules or strict timescale separation. The proposed method leverages the in-context learning capabilities of sequence models, training agents against a diverse distribution of co-players to naturally induce in-context best-response strategies that function as fast-timescale learning algorithms. Experimental results show that this approach naturally replicates a known cooperative mechanism where vulnerability to extortion drives mutual shaping, ultimately leading to the emergence of cooperative behavior without explicit meta-learning architectures, suggesting a scalable path via decentralized training and co-player diversity.</div>
<div class="mono" style="margin-top:8px">本文旨在解决多智能体强化学习中自利智能体间合作的根本挑战，摒弃了以往依赖对对手学习规则的硬编码假设或严格时间尺度分离的方法。该方法利用序列模型的上下文学习能力，通过让智能体与多样化的对手分布进行训练，自然地诱导出作为快速时间尺度学习算法的上下文最佳响应策略。实验结果表明，该方法自然地复现了一种已知的合作机制，即对勒索的脆弱性驱动相互塑造，最终在没有显式元学习架构的情况下促成了合作行为的涌现，为通过分散式训练和对手多样性实现可扩展的合作学习提供了路径。</div>
</details>
</div>
<div class="card">
<div class="title">VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models</div>
<div class="meta-line">Authors: Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, Jun Xiao, Yueting Zhuang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-21T17:54:43+00:00 · Latest: 2026-02-18T08:45:15+00:00</div>
<div class="meta-line">Comments: ICLR 2026: https://openreview.net/forum?id=JfsjGmuFxz Project Page: https://zju-real.github.io/VerifyBench Dataset: https://huggingface.co/datasets/ZJU-REAL/VerifyBench Code: https://github.com/ZJU-REAL/VerifyBench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.15801v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.15801v4">PDF</a> · <a href="https://huggingface.co/datasets/ZJU-REAL/VerifyBench">Code1</a> · <a href="https://github.com/ZJU-REAL/VerifyBench">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a> · <a href="https://zju-real.github.io/VerifyBench">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models such as OpenAI o1 and DeepSeek-R1 have demonstrated remarkable performance in complex reasoning tasks. A critical component of their training is the incorporation of reference-based reward systems within reinforcement learning (RL), where model outputs are evaluated against ground truth references. However, existing reward benchmarks focus on preference comparisons between responses rather than evaluating verification against ground truth references, leaving a critical gap in our ability to evaluate verification systems used in reasoning model training. In this paper, we introduce VerifyBench and its challenging variant VerifyBench-Hard, two benchmarks specifically designed to assess reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Our comprehensive evaluation reveals that while larger model-based verifiers show promise on standard cases, all current systems demonstrate substantial room for improvement on challenging instances. Through systematic analysis of performance patterns across reasoning tasks and error categories, we provide insights for advancing reference-based reward systems. These benchmarks establish a standardized framework for improving verification accuracy, ultimately enhancing reasoning capabilities in models trained via RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VerifyBench：面向大语言模型的基于参考的奖励系统基准测试</div>
<div class="mono" style="margin-top:8px">OpenAI o1 和 DeepSeek-R1 等大型推理模型在复杂推理任务中展现出卓越性能。其训练的关键环节是在强化学习（RL）中引入基于参考的奖励系统，即依据真实参考答案评估模型输出。然而，现有奖励基准多关注回答间的偏好比较，而非基于真实参考的验证评估，导致推理模型训练中验证系统的评估存在关键空白。本文提出 VerifyBench 及其挑战性变体 VerifyBench-Hard，这两个基准专门用于评估基于参考的奖励系统。通过精细的数据收集、整理及人工标注确保数据高质量。综合评估表明：基于更大模型的验证器在标准案例中表现良好，但所有现有系统在挑战性实例上仍有显著改进空间。通过对推理任务和错误类别的系统性绩效模式分析，本研究为推进基于参考的奖励系统提供了洞见。这些基准建立了提升验证准确度的标准化框架，最终将增强基于强化学习训练的模型的推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the lack of benchmarks for evaluating reference-based reward systems, which are crucial for training advanced reasoning models like OpenAI o1, as existing benchmarks focus on preference comparisons instead of verification against ground truth. The method involves introducing VerifyBench and VerifyBench-Hard, two benchmarks constructed through meticulous data collection, curation, and human annotation to assess these systems. The main experimental results show that while larger model-based verifiers perform reasonably on standard cases, all current systems have significant room for improvement on challenging instances, with systematic analysis providing insights for advancing such reward systems to enhance reasoning capabilities in RL-trained models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，现有基准主要关注响应间的偏好比较，而缺乏用于评估基于参考的奖励系统的基准，这类系统对于训练如OpenAI o1等高级推理模型至关重要。方法上，研究引入了VerifyBench及其挑战性变体VerifyBench-Hard，这两个基准通过细致的数据收集、整理和人工标注构建，专门用于评估此类系统。主要实验结果表明，尽管基于更大模型的验证器在标准案例上表现尚可，但所有现有系统在挑战性实例上均有显著改进空间，通过系统分析性能模式和错误类别，为推进基于参考的奖励系统提供了见解，以增强通过强化学习训练的模型的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</div>
<div class="meta-line">Authors: Zhonghao Yan, Muxi Diao, Yuxuan Yang, Ruoyan Jing, Jiayuan Xu, Kaizhou Zhang, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma</div>
<div class="meta-line">First: 2025-08-11T16:59:06+00:00 · Latest: 2026-02-18T08:36:10+00:00</div>
<div class="meta-line">Comments: AAAI2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08177v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.08177v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedReasoner：强化学习驱动从临床思维到像素级精度的推理定位</div>
<div class="mono" style="margin-top:8px">在医学影像中，准确定位感兴趣区域（ROI）对诊断和治疗规划至关重要。尽管多模态大语言模型（MLLM）融合了视觉感知与自然语言，现有医学定位流程仍依赖带有显式空间提示的监督微调，难以处理临床实践中常见的隐式查询。本研究作出三项核心贡献：首先定义统一医学推理定位（UMRG）这一需要临床推理与像素级定位的新颖视觉-语言任务；其次发布U-MRG-14K数据集，包含1.4万个样本，涵盖10种模态、15个超类与108个具体类别，每个样本均配有像素级掩码、隐式临床查询及推理轨迹；最后提出MedReasoner模块化框架，明确分离推理与分割环节——通过强化学习优化MLLM推理器，同时利用冻结的分割专家将空间提示转换为掩码，并通过格式与精度奖励实现对齐。MedReasoner在U-MRG-14K上达到最先进性能，且对未见临床查询展现出强大泛化能力，彰显强化学习在可解释医学定位领域的重大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of accurately grounding regions of interest in medical imaging based on implicit clinical queries, which current supervised methods struggle with. The authors propose a novel Unified Medical Reasoning Grounding (UMRG) task, introduce the U-MRG-14K dataset with pixel-level masks and reasoning traces, and develop MedReasoner, a modular framework that separates reasoning via a reinforcement learning-optimized MLLM from segmentation via a frozen expert. Experimental results show that MedReasoner achieves state-of-the-art performance on U-MRG-14K and generalizes well to unseen queries, highlighting the promise of reinforcement learning for interpretable medical grounding.</div>
<div class="mono" style="margin-top:8px">本文针对医学影像中基于隐含临床查询精确定位感兴趣区域的挑战，现有监督方法对此处理不佳。作者提出了统一医学推理定位（UMRG）新任务，发布了包含像素级掩码和推理轨迹的U-MRG-14K数据集，并开发了MedReasoner模块化框架，该框架通过强化学习优化的多模态大语言模型进行推理，与冻结的分割专家协同工作。实验结果表明，MedReasoner在U-MRG-14K上取得了最先进的性能，并对未见过的查询表现出强大的泛化能力，凸显了强化学习在可解释医学定位中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability</div>
<div class="meta-line">Authors: Aaditya Vikram Prasad, Connor Watts, Jack Merullo, Dhruvil Gala, Owen Lewis, Thomas McGrath, Ekdeep Singh Lubana</div>
<div class="meta-line">First: 2026-02-10T18:33:45+00:00 · Latest: 2026-02-18T07:19:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10067v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.10067v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model (when run in tandem with our probing harness), while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>特征作为奖励：通过可解释性为开放式任务提供可扩展监督</div>
<div class="mono" style="margin-top:8px">在大规模数据集上训练的语言模型已被证明能够学习编码抽象概念（如事实性或意图）的特征。此类特征传统上用于测试时监控或引导。我们提出一种替代性功能：将特征作为开放式任务的可扩展监督。我们以减少幻觉这一理想但开放的行为为例，设计了一个强化学习（RL）流程，称为RLFR（基于特征奖励的强化学习），该流程将特征用作奖励函数。基于一个识别候选幻觉主张的新型探测框架，我们的流程教导模型在其对生成内容的事实性不确定时进行干预和修正。此外，该流程通过我们的奖励特征引导，实现了可扩展的测试时计算。在Gemma-3-12B-IT模型上实施的这一端到端流程，产生了一个策略，与原始模型相比（在与我们的探测框架并行运行时），其产生幻觉的可能性降低了58%，同时保持了在标准基准测试上的性能。综上所述，通过将监督建立在特征语言的基础上，本文为利用可解释性学习开放式任务引入了一种新颖范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of providing scalable supervision for open-ended tasks like reducing hallucinations in language models, where traditional reward specification is difficult. The method, Reinforcement Learning from Feature Rewards (RLFR), leverages interpretable features—such as those encoding factuality—as reward functions within a reinforcement learning pipeline; this includes a probing framework to identify potential hallucinations and guide model intervention. Experimental results on Gemma-3-12B-IT show that the resulting policy reduces hallucination likelihood by 58% compared to the original model when used with the probing harness, while maintaining performance on standard benchmarks, demonstrating a novel paradigm for using interpretability in learning.</div>
<div class="mono" style="margin-top:8px">本文的动机是为减少语言模型幻觉等开放式任务提供可扩展的监督，因为传统奖励设定在此类任务中较为困难。方法上，提出了基于特征奖励的强化学习（RLFR），利用可解释性特征（如事实性特征）作为奖励函数，并结合一个探测框架来识别潜在幻觉并引导模型干预。在Gemma-3-12B-IT模型上的实验结果表明，与原始模型相比，所得策略在使用探测工具时幻觉可能性降低了58%，同时在标准基准测试上保持了性能，这为利用可解释性学习开放式任务引入了一种新范式。</div>
</details>
</div>
<div class="card">
<div class="title">CaveAgent: Transforming LLMs into Stateful Runtime Operators</div>
<div class="meta-line">Authors: Maohao Ran, Zhenglin Wan, Cooper Lin, Yanting Zhang, Hongyu Xin, Hongwei Fan, Yibo Xu, Beier Luo, Yaxin Zhou, Wangbo Zhao, Lijie Yang, Lang Feng, Fuchao Yang, Jingxuan Wu, Yiqiao Huang, Chendong Ma, Dailing Jiang, Jianbo Deng, Sihui Han, Yang You, Bo An, Yike Guo, Jun Song</div>
<div class="meta-line">First: 2026-01-04T15:32:47+00:00 · Latest: 2026-02-18T06:16:21+00:00</div>
<div class="meta-line">Comments: version 2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01569v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01569v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms that struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. We present CaveAgent, a framework that shifts tool use from ``LLM-as-Text-Generator&#x27;&#x27; to ``LLM-as-Runtime-Operator.&#x27;&#x27; CaveAgent introduces a dual-stream architecture that inverts the conventional paradigm: rather than treating the LLM&#x27;s text context as the primary workspace with tools as auxiliary, CaveAgent elevates the persistent Python runtime as the central locus of state, with a lightweight semantic stream serving as its orchestrator. Beyond leveraging code generation to resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, CaveAgent introduces \textit{Stateful Runtime Management}: it injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns, unlike existing code-based approaches that remain text-bound. CaveAgent further provides a runtime-integrated skill management system that extends the Agent Skills open standard, enabling ecosystem interoperability through executable skill injections. This persistence mechanism serves as a high-fidelity external memory that reduces context drift in multi-turn interactions and preserves processed data for downstream applications without information loss. Evaluations show consistent improvement across challenging benchmarks, enabling CaveAgent to handle data scales that cause context overflow in both JSON-based and code-based agents. The accessible runtime state further provides programmatically verifiable feedback, enabling automated evaluation and reward signal generation without human annotation and establishing a structural foundation for future research in Reinforcement Learning with Verifiable Rewards (RLVR).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CaveAgent：将大语言模型转化为有状态运行时算子</div>
<div class="mono" style="margin-top:8px">基于大语言模型的智能体执行复杂任务的能力日益增强，但现有智能体系统仍受限于以文本为中心的范式，在多轮依赖关系脆弱和语境漂移的影响下难以处理长周期任务。本文提出CaveAgent框架，将工具使用范式从“大语言模型作为文本生成器”转变为“大语言模型作为运行时算子”。该框架采用双流架构颠覆传统范式：不再以大语言模型的文本语境作为主要工作空间、工具作为辅助，而是将持久化Python运行时提升为核心状态载体，由轻量级语义流担任编排器。除通过代码生成单步解决相互依赖的子任务（如循环、条件判断）外，CaveAgent引入\textit{有状态运行时管理}机制：注入、操作和检索跨轮次持久化的复杂Python对象（如DataFrames、数据库连接），突破现有基于代码方法仍受文本束缚的限制。框架进一步提供运行时集成技能管理系统，扩展Agent Skills开放标准，通过可执行技能注入实现生态互操作性。这种持久化机制作为高保真外部记忆，可减少多轮交互中的语境漂移，完整保留处理后数据供下游应用使用。实验表明，该框架在多项挑战性基准测试中持续提升性能，能处理导致基于JSON和基于代码的智能体语境溢出的数据规模。其可访问的运行时状态进一步提供可编程验证的反馈，无需人工标注即可实现自动评估与奖励信号生成，为未来可验证奖励强化学习研究奠定结构基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of text-centric LLM agents in handling long-horizon tasks due to fragile multi-turn dependencies and context drift, this paper introduces CaveAgent, a framework that repositions LLMs as stateful runtime operators. The method employs a dual-stream architecture that elevates a persistent Python runtime as the central state locus, orchestrated by a lightweight semantic stream, enabling direct injection and manipulation of complex Python objects across turns and integrating a runtime skill management system. Experimental results demonstrate consistent improvements on challenging benchmarks, allowing CaveAgent to manage data scales that cause context overflow in existing agents and providing programmatically verifiable feedback for automated evaluation.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决以文本为中心的大语言模型智能体在长程任务中因多轮依赖脆弱和上下文漂移而受限的问题，提出了CaveAgent框架，将大语言模型转变为有状态的运行时操作符。该方法采用双流架构，将持久的Python运行时提升为核心状态载体，由轻量语义流进行编排，支持跨轮次直接注入和操作复杂的Python对象，并集成了运行时技能管理系统。主要实验结果表明，该框架在多个挑战性基准测试上取得了一致的性能提升，能够处理导致现有智能体上下文溢出的数据规模，并提供可编程验证的反馈以支持自动评估。</div>
</details>
</div>
<div class="card">
<div class="title">Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Emile Anand, Richard Hoffmann, Sarah Liaw, Adam Wierman</div>
<div class="meta-line">First: 2026-02-18T05:34:07+00:00 · Latest: 2026-02-18T05:34:07+00:00</div>
<div class="meta-line">Comments: 43 pages, 5 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16196v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\texttt{GMFS}$, a $\textbf{G}$raphon $\textbf{M}$ean-$\textbf{F}$ield $\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\mathrm{poly}(κ)$ and optimality gap $O(1/\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\texttt{GMFS}$ achieves near-optimal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图子均值场子采样用于协作异构多智能体强化学习</div>
<div class="mono" style="margin-top:8px">协调大规模交互智能体群体是多智能体强化学习（MARL）的核心挑战，其联合状态-动作空间规模随智能体数量呈指数级增长。均值场方法通过聚合智能体交互来缓解这一负担，但这些方法假设交互是同质的。基于图子的最新框架虽能捕捉异质性，但随着智能体数量增加，计算成本高昂。为此，我们提出$\texttt{GMFS}$——一种面向可扩展协作MARL的$\textbf{图子均值场子采样}$框架，用于处理异构智能体交互。通过根据交互强度对$κ$个智能体进行子采样，我们近似图子加权均值场，并以$\mathrm{poly}(κ)$的样本复杂度和$O(1/\sqrtκ)$的最优性差距学习策略。我们通过机器人协同的数值模拟验证理论，表明$\texttt{GMFS}$能实现接近最优的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scaling cooperative multi-agent reinforcement learning (MARL) to large, heterogeneous populations, where traditional mean-field methods assume homogeneous interactions and graphon-based approaches become computationally expensive. The proposed method, Graphon Mean-Field Subsampling (GMFS), introduces a scalable framework by subsampling a subset of agents based on interaction strength to approximate the graphon-weighted mean-field interactions, thereby reducing complexity. Experimental results in robotic coordination simulations demonstrate that GMFS achieves near-optimal performance with a sample complexity that scales polynomially with the subsample size and an optimality gap that diminishes as the subsample size increases.</div>
<div class="mono" style="margin-top:8px">本文针对大规模异构智能体群体中协同多智能体强化学习（MARL）的可扩展性挑战，传统均值场方法假设交互同质，而基于图论的方法计算成本高昂。提出的方法——图论均值场子采样（GMFS），通过根据交互强度对智能体子集进行采样，以近似图论加权的均值场交互，从而降低计算复杂度。在机器人协同模拟中的实验结果表明，GMFS实现了接近最优的性能，其样本复杂度随子采样规模多项式增长，且最优性差距随子采样规模增大而减小。</div>
</details>
</div>
<div class="card">
<div class="title">EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments</div>
<div class="meta-line">Authors: Sushant Mehta, Logan Ritchie, Suhaas Garre, Nick Heiner, Edwin Chen</div>
<div class="meta-line">First: 2026-02-18T04:35:46+00:00 · Latest: 2026-02-18T04:35:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16179v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16179v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \corecraft{}, the first environment in \textsc{EnterpriseGym}, Surge AI&#x27;s suite of agentic RL environments. \corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\% to 36.76\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\% on BFCL Parallel, +7.4\% on $τ^2$-Bench Retail, and +6.8\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EnterpriseGym Corecraft：在高保真强化学习环境中训练可泛化智能体</div>
<div class="mono" style="margin-top:8px">我们证明，在高保真强化学习环境中训练AI智能体，能产生超越训练分布之外的泛化能力。我们推出\corecraft{}，这是Surge AI智能体强化学习环境套件\textsc{EnterpriseGym}中的首个环境。\corecraft{}是一个完全可运行的客户支持组织企业模拟，包含14种实体类型、超过2500个实体及23种独特工具，旨在衡量AI智能体能否执行真实工作所需的多步骤、领域特定任务。当必须满足所有专家制定的评估标准时，GPT-5.2和Claude Opus 4.6等前沿模型的任务通过率不足30%。利用该环境，我们采用组相对策略优化（GRPO）和自适应剪裁技术训练GLM~4.6。经过单轮训练后，模型在保留评估任务上的通过率从25.37%提升至36.76%。更重要的是，这些提升能迁移至分布外基准测试：BFCL Parallel提升4.5%、$τ^2$-Bench Retail提升7.4%、Toolathlon（Pass@1）提升6.8%。我们认为以下三个环境特性与观察到的迁移效果相符：以任务为中心的世界构建（优化多样性与挑战性）、专家制定的可量化奖励评估标准，以及反映真实专业模式的企业工作流。结果表明，环境质量、多样性和真实性是形成可泛化智能体能力的关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to develop AI agents that can generalize beyond their training distribution by leveraging high-fidelity reinforcement learning environments. The method introduces Corecraft, a detailed enterprise simulation of a customer support organization within the EnterpriseGym suite, featuring over 2,500 entities and 23 tools to evaluate multi-step, domain-specific tasks. Experimental results show that frontier models like GPT-5.2 and Claude Opus 4.6 achieve under 30% task pass rates, while training GLM 4.6 with Group Relative Policy Optimization and adaptive clipping improves its pass rate from 25.37% to 36.76% on held-out tasks and demonstrates transfer gains of up to 7.4% on out-of-distribution benchmarks, highlighting the importance of environment quality and realism for generalization.</div>
<div class="mono" style="margin-top:8px">该研究的动机是通过利用高保真强化学习环境，开发能够超越训练分布进行泛化的AI智能体。方法上引入了Corecraft，这是EnterpriseGym套件中一个详细的企业客户支持组织模拟环境，包含超过2,500个实体和23种工具，用于评估多步骤、领域特定的任务。实验结果表明，像GPT-5.2和Claude Opus 4.6这样的前沿模型任务通过率低于30%，而使用组相对策略优化和自适应裁剪训练GLM 4.6后，其在保留任务上的通过率从25.37%提升至36.76%，并在分布外基准测试中实现了高达7.4%的转移增益，强调了环境质量、多样性和真实性对智能体泛化能力的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation</div>
<div class="meta-line">Authors: Fatih Temiz, Shavbo Salehi, Melike Erol-Kantarci</div>
<div class="meta-line">First: 2026-02-18T04:19:57+00:00 · Latest: 2026-02-18T04:19:57+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures, Accepted paper at IEEE International Conference on Communications (ICC) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data. Federated learning (FL) is a promising solution, and can be combined with reinforcement learning (RL) to develop generalized policies across MEC-servers. However, conventional FL incurs transmitting the full model parameters across the MEC-servers and the cloud, and suffer performance degradation due to naive global aggregation, especially in heterogeneous multi-radio access technology environments. To address these challenges, this paper proposes Federated Split Decision Transformer (FSDT), an offline RL framework where the transformer model is partitioned between MEC servers and the cloud. Agent-specific components (e.g., MEC-based embedding and prediction layers) enable local adaptability, while shared global layers in the cloud facilitate cooperative training across MEC servers. Experimental results demonstrate that FSDT enhances QoE for up to 10% in heterogeneous environments compared to baselines, while offloadingnearly 98% of the transformer model parameters to the cloud, thereby reducing the computational burden on MEC servers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向元宇宙资源分配的联邦分割决策Transformer边缘学习</div>
<div class="mono" style="margin-top:8px">基于移动边缘计算（MEC）的无线元宇宙服务为用户提供无束缚的沉浸式体验，其卓越体验质量（QoE）需在严格时延约束与视觉质量要求下实现。为此，面向虚拟现实用户的MEC智能资源分配需通过跨MEC服务器协同以利用分布式数据。联邦学习（FL）是前景广阔的解决方案，可与强化学习（RL）结合构建跨MEC服务器的通用策略。然而传统FL需在MEC服务器与云间传输完整模型参数，且因简单的全局聚合导致性能下降，在异构多无线接入技术环境中尤为明显。为应对这些挑战，本文提出联邦分割决策Transformer（FSDT）——一种将Transformer模型分割部署于MEC服务器与云端的离线RL框架。其中代理专用组件（如基于MEC的嵌入层与预测层）实现本地自适应，云端共享全局层则促进跨MEC服务器协同训练。实验表明：在异构环境中FSDT较基线方法提升QoE达10%，同时将近98%的Transformer模型参数卸载至云端，显著减轻MEC服务器计算负担。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for efficient resource allocation in mobile edge computing (MEC) for wireless metaverse services, which require high quality of experience under strict latency and visual quality constraints. The proposed method, Federated Split Decision Transformer (FSDT), is an offline reinforcement learning framework that partitions a transformer model between MEC servers and the cloud, keeping agent-specific layers locally for adaptability while sharing global layers in the cloud for cooperative training across servers. Experimental results show that FSDT improves quality of experience by up to 10% in heterogeneous environments compared to baselines and offloads nearly 98% of model parameters to the cloud, significantly reducing computational burden on MEC servers.</div>
<div class="mono" style="margin-top:8px">本文的动机是为无线元宇宙服务中的移动边缘计算资源分配提供高效解决方案，以满足严格延迟和视觉质量要求下的优质体验需求。所提出的方法，即联邦分割决策变换器，是一种离线强化学习框架，它将变换器模型分割部署在移动边缘服务器和云端，其中代理特定层保留在本地以实现适应性，而共享的全局层在云端促进服务器间的协同训练。实验结果表明，在异构环境中，该方法相比基线将体验质量提升了高达10%，并将近98%的模型参数卸载到云端，显著减轻了移动边缘服务器的计算负担。</div>
</details>
</div>
<div class="card">
<div class="title">HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents</div>
<div class="meta-line">Authors: Jiangweizhi Peng, Yuanxin Liu, Ruida Zhou, Charles Fleming, Zhaoran Wang, Alfredo Garcia, Mingyi Hong</div>
<div class="meta-line">First: 2026-02-18T03:31:34+00:00 · Latest: 2026-02-18T03:31:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16165v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.
  We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.
  Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\% success on ALFWorld and 83.3\% on WebShop with Qwen2.5-7B-Instruct (+6.6\% and +8.3\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiPER：面向大语言模型智能体的显式信用分配分层强化学习</div>
<div class="mono" style="margin-top:8px">将大语言模型训练为多轮决策交互智能体仍具挑战性，尤其在奖励稀疏且延迟的长周期任务中，智能体需执行长序列动作才能获得有效反馈。现有强化学习方法多将大语言模型智能体建模为单一时间尺度的扁平策略，每轮仅选择一个动作。在稀疏奖励场景下，此类扁平策略需在整个轨迹中传播信用而缺乏显式时间抽象，常导致优化不稳定与信用分配低效。
我们提出HiPER——一种显式分离高层规划与底层执行的新型分层规划-执行强化学习框架。HiPER将策略分解为提出子目标的高层规划器与多步执行子目标的底层执行器。为实现优化目标与结构对齐，我们引入分层优势估计关键技术，在规划与执行层面精准分配信用。通过聚合每个子目标执行期间的回报并协调两级更新，该估计器提供无偏梯度估计，较扁平广义优势估计可证明降低方差。
实验表明，HiPER在交互基准测试中取得最优性能：基于Qwen2.5-7B-Instruct模型在ALFWorld达到97.4%成功率，在WebShop达到83.3%（较先前最佳方法分别提升6.6%与8.3%），在需处理多依赖子任务的长周期任务中优势尤为显著。这些结果凸显了显式分层分解对多轮大语言模型智能体可扩展强化学习训练的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training large language models as interactive agents for long-horizon tasks with sparse rewards, where flat reinforcement learning policies struggle with inefficient credit assignment. It introduces HiPER, a hierarchical reinforcement learning framework that separates high-level planning for subgoal generation from low-level execution over multiple steps, employing hierarchical advantage estimation to assign credit explicitly across both levels and reduce variance. Experimental results show state-of-the-art performance, achieving 97.4% success on ALFWorld and 83.3% on WebShop with a Qwen2.5-7B-Instruct model, with significant improvements in long-horizon tasks requiring dependent subtasks.</div>
<div class="mono" style="margin-top:8px">该论文针对稀疏奖励的长时程任务中训练大型语言模型作为交互智能体的挑战，其中扁平强化学习策略在信用分配上效率低下。提出了HiPER，一种分层强化学习框架，将高层规划用于子目标生成与低层多步执行分离，并采用分层优势估计技术，在两层间显式分配信用以降低方差。实验结果表明，在ALFWorld和WebShop基准上，使用Qwen2.5-7B-Instruct模型分别达到97.4%和83.3%的成功率，尤其在需要多个依赖子任务的长时程任务上取得显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution</div>
<div class="meta-line">Authors: Nithin Sivakumaran, Shoubin Yu, Hyunji Lee, Yue Zhang, Ali Payani, Mohit Bansal, Elias Stengel-Eskin</div>
<div class="meta-line">First: 2026-02-18T02:55:55+00:00 · Latest: 2026-02-18T02:55:55+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/nsivaku/remul</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16154v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16154v1">PDF</a> · <a href="https://github.com/nsivaku/remul">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who &quot;execute&quot; the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多听众软执行平衡推理的忠实性与性能</div>
<div class="mono" style="margin-top:8px">思维链推理有时无法忠实反映大语言模型的真实计算过程，这削弱了其在解释模型如何得出答案方面的效用。此外，为提升推理的忠实性和可解释性进行优化往往会降低任务性能。为解决这一权衡问题并提升思维链的忠实性，我们提出了多听众推理执行方法——一种基于多方强化学习的方法。该方法基于一个假设：能被其他方遵循的推理轨迹将更具忠实性。发言者模型生成推理轨迹，该轨迹被截断后传递给一组听众模型，由听众“执行”该轨迹并延续至得出答案。发言者因生成听众能清晰理解的推理而获得奖励，同时通过掩码监督微调进行额外的正确性正则化，以抵消忠实性与性能之间的权衡。在多个推理基准测试中，该方法在三个忠实性指标上均取得显著提升，同时提高了准确率。分析表明，这些收益在不同训练领域均表现稳健，能转化为可读性提升，且与更简短直接的思维链相关。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the trade-off between faithfulness and performance in chain-of-thought reasoning, where current methods often produce reasoning traces that do not accurately reflect the model&#x27;s true computation, thereby limiting interpretability. The authors propose REMUL, a multi-party reinforcement learning method that improves faithfulness by having a speaker model generate a reasoning trace, which is then truncated and executed by a pool of listener models to produce an answer; speakers are rewarded for clarity to listeners, with masked supervised fine-tuning used to maintain task performance. Experimental results on multiple reasoning benchmarks show that REMUL consistently enhances three faithfulness metrics—hint attribution, early answering AOC, and mistake injection AOC—while also improving accuracy, with analysis indicating these gains are robust across domains and associated with shorter, more direct reasoning traces.</div>
<div class="mono" style="margin-top:8px">本文针对思维链推理中忠实性与性能之间的权衡问题，现有方法产生的推理轨迹常无法准确反映模型真实计算过程，从而限制了可解释性。作者提出REMUL，一种多方强化学习方法，通过让说话者模型生成推理轨迹，随后截断并由一组听者模型执行以得出答案，从而提升忠实性；说话者因对听者清晰而被奖励，并采用掩码监督微调以保持任务性能。在多个推理基准测试上的实验结果表明，REMUL持续提升了三个忠实性指标——提示归因、早期回答AOC和错误注入AOC——同时提高了准确性，分析显示这些增益在不同领域具有鲁棒性，且与更简短、更直接的推理轨迹相关。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Up the Instruction Ladder for Controllable Language Models</div>
<div class="meta-line">Authors: Zishuo Zheng, Vidhisha Balachandran, Chan Young Park, Faeze Brahman, Sachin Kumar</div>
<div class="meta-line">First: 2025-10-30T22:13:31+00:00 · Latest: 2026-02-18T02:51:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04694v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.04694v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first &quot;think&quot; about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises ~7K aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks, achieving roughly a 20% improvement on the IHEval conflict setup. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks, providing up to a 20% reduction in attack success rate (ASR). These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>沿指令阶梯推理实现可控语言模型</div>
<div class="mono" style="margin-top:8px">随着基于大语言模型（LLM）的系统在现实世界决策中承担高风险角色，它们必须在单一提示上下文中协调来自多个来源（如模型开发者、用户和工具）的竞争性指令。因此，在LLM中实施指令层级（IH）——即高级指令覆盖低优先级请求——对LLM的可靠性和可控性至关重要。本研究将指令层级解析重构为推理任务：模型在生成响应前需先“思考”给定用户提示与高优先级（系统）指令间的关系。为通过训练实现此能力，我们构建了VerIH——一个包含可验证答案的约束遵循任务的指令层级数据集，涵盖约7千条对齐及冲突的系统-用户指令。研究表明，基于VerIH的轻量级强化学习能有效将模型的通用推理能力迁移至指令优先级处理。经微调的模型在指令遵循和指令层级基准测试中取得持续改进，在IHEval冲突设置上提升约20%。该推理能力还可泛化至训练分布外的安全关键场景：通过将安全问题视为对抗性用户输入与预定义高优先级策略间的冲突解析，训练后的模型增强了对越狱和提示注入攻击的鲁棒性，攻击成功率（ASR）降低达20%。这些结果表明，对指令层级的推理为构建可靠LLM提供了可行路径，系统提示的更新能带来可控且鲁棒的模型行为改变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for large language models (LLMs) to reliably prioritize instructions from different sources, such as system policies versus user requests, by framing instruction hierarchy resolution as a reasoning task. The method involves constructing VerIH, a dataset of approximately 7,000 aligned and conflicting system-user instructions with verifiable answers, and using lightweight reinforcement learning to train models to reason about and prioritize higher-level directives before responding. Experimental results show that the finetuned models improve instruction following and hierarchy benchmarks by about 20% on conflict setups, and this reasoning ability generalizes to enhance robustness against jailbreak and prompt injection attacks, reducing attack success rates by up to 20%.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型需要可靠处理来自不同来源指令优先级的问题，将指令层级解析重构为推理任务。方法上，构建了VerIH数据集，包含约7000条可验证的对齐与冲突系统-用户指令，并通过轻量级强化学习训练模型在响应前推理并优先执行高级指令。实验结果表明，微调后的模型在指令遵循和层级基准测试上，冲突设置性能提升约20%，且该推理能力可泛化至安全关键场景，增强对越狱和提示注入攻击的鲁棒性，攻击成功率降低达20%。</div>
</details>
</div>
<div class="card">
<div class="title">Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs</div>
<div class="meta-line">Authors: Xiaoke Huang, Ningsen Wang, Hui Liu, Xianfeng Tang, Yuyin Zhou</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2025-10-29T18:10:44+00:00 · Latest: 2026-02-18T01:32:31+00:00</div>
<div class="meta-line">Comments: Project page, code, data, and models: https://ucsc-vlaa.github.io/MedVLSynther/ ; Accepted by ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25867v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25867v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ucsc-vlaa.github.io/MedVLSynther/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于生成器-验证器大语言模型从医学文献合成高质量视觉问答数据</div>
<div class="mono" style="margin-top:8px">大语言模型在需要图像与文本联合推理的医学问题解答方面能力日益增强，但通用医学视觉问答系统的训练因缺乏大规模、可公开使用的高质量语料库而受阻。我们提出MedVLSynther——一个基于规则指导的生成器-验证器框架，通过结合图表、标题及文内参考文献，直接从开放生物医学文献中合成高质量多项选择视觉问答条目。生成器按照机器可校验的JSON格式生成自包含题干及并行互斥选项；多阶段验证器在接收前执行核心校验（自包含性、单一正确答案、临床有效性、图文一致性），授予细粒度正向评分，并对常见错误模式进行扣分。将该流程应用于PubMed Central数据库得到MedSynVQA：包含13,087个已审核问题，覆盖14,803张图像，涉及13种影像模态和28个解剖区域。使用可验证奖励通过强化学习训练开放权重大语言模型，在六项医学视觉问答基准测试中准确率全面提升，3B和7B模型平均分别达到55.85和58.15，其中VQA-RAD最高达77.57，PathVQA达67.76，优于现有医学大语言模型。消融实验验证生成与验证环节均不可或缺，更多验证数据持续带来增益；定向污染分析未检测到评估集泄露。通过完全基于开放文献与开放权重模型，MedVLSynther为可扩展医学视觉问答训练数据提供了可审计、可复现且保护隐私的生成路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the scarcity of high-quality, open medical visual question answering (VQA) datasets by introducing MedVLSynther, a generator-verifier framework that synthesizes multiple-choice VQA items from biomedical literature using figures, captions, and text references. The method employs a generator to create question stems and options under a structured schema, followed by a multi-stage verifier that enforces criteria like clinical validity and image-text consistency to ensure quality. Experimental results show that applying this pipeline to PubMed Central produced MedSynVQA, a dataset of over 13,000 audited questions, and training open-weight large multimodal models with reinforcement learning using verifiable rewards improved accuracy across six medical VQA benchmarks, outperforming existing medical LMMs with scores up to 77.57 on VQA-RAD.</div>
<div class="mono" style="margin-top:8px">该研究针对高质量、开放的医学视觉问答数据集匮乏的问题，提出了MedVLSynther框架，通过生成器-验证器结构从生物医学文献中合成多项选择题，利用图像、标题和文本参考。方法中，生成器按照结构化模式生成问题主干和选项，多阶段验证器则强制执行临床有效性、图像-文本一致性等标准以保证质量。实验结果表明，将该流程应用于PubMed Central生成了包含超过13,000个审核问题的MedSynVQA数据集，并使用可验证奖励通过强化学习训练开放权重大型多模态模型，在六个医学VQA基准测试中提升了准确性，在VQA-RAD上最高达到77.57分，优于现有医学LMMs。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</div>
<div class="meta-line">Authors: Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu</div>
<div class="meta-line">First: 2025-09-18T17:50:04+00:00 · Latest: 2026-02-18T00:51:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15194v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.15194v3">PDF</a> · <a href="https://github.com/YujunZhou/EVOL-RL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing self-improvement approaches primarily rely on self-confirmation signals (e.g., confidence, entropy, or consistency) to generate rewards. This reliance drives models toward over-confident, majority-favored solutions, causing an entropy collapse that degrades pass@n and reasoning complexity. To address this, we propose EVOL-RL, a label-free framework that mirrors the evolutionary principle of balancing selection with variation. Concretely, EVOL-RL retains the majority-voted answer as an anchor for stability, but adds a novelty-aware reward that scores each sampled solution by how different its reasoning is from other concurrently generated responses. This majority-for-stability + novelty-for-exploration rule mirrors the variation-selection principle: selection prevents drift, while novelty prevents collapse. Evaluation results show that EVOL-RL consistently outperforms the majority-only baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline&#x27;s 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents in-domain diversity collapse but also improves out-of-domain generalization (from math reasoning to broader tasks, e.g., MMLU-Pro and BBEH). The code is available at: https://github.com/YujunZhou/EVOL-RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需标签的语言模型进化：多数驱动选择，新颖性促进变异</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）越来越多地通过可验证奖励的强化学习（RLVR）进行训练，但实际部署需要模型能在无标签或外部评判的情况下自我改进。现有的自我改进方法主要依赖自我确认信号（如置信度、熵或一致性）来生成奖励，这种依赖导致模型倾向于过度自信、多数偏好的解，引发熵崩溃，从而降低 pass@n 和推理复杂度。为此，我们提出 EVOL-RL，一种无标签框架，模拟了平衡选择与变异的进化原则。具体而言，EVOL-RL 保留多数投票答案作为稳定性锚点，但增加了一个新颖性感知奖励，该奖励根据每个采样解的推理过程与同时生成的其他响应的差异程度进行评分。这种“多数求稳定 + 新颖促探索”的规则体现了变异-选择原则：选择防止漂移，新颖性防止崩溃。评估结果显示，EVOL-RL 持续优于仅使用多数的基线；例如，在无标签的 AIME24 上训练，将 Qwen3-4B-Base 的 AIME25 pass@1 从基线的 4.6% 提升至 16.4%，pass@16 从 18.5% 提升至 37.9%。EVOL-RL 不仅防止了领域内多样性崩溃，还提升了领域外泛化能力（从数学推理到更广泛任务，如 MMLU-Pro 和 BBEH）。代码发布于：https://github.com/YujunZhou/EVOL-RL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of large language models (LLMs) degrading in diversity and reasoning complexity when self-improving via reinforcement learning without external labels, as existing methods rely on self-confirmation signals that lead to entropy collapse. To overcome this, the authors propose EVOL-RL, a label-free framework inspired by evolutionary principles, which combines a majority-voted answer for stability with a novelty-aware reward that encourages variation by scoring solutions based on reasoning differences from other generated responses. Experimental results demonstrate that EVOL-RL significantly outperforms majority-only baselines, improving AIME25 pass@1 from 4.6% to 16.4% and pass@16 from 18.5% to 37.9% for the Qwen3-4B-Base model, while also enhancing out-of-domain generalization on tasks like MMLU-Pro and BBEH.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在无外部标签情况下通过强化学习进行自我改进时，因依赖自我确认信号而导致多样性下降和推理复杂度降低的问题。为解决此问题，作者提出了EVOL-RL，一个受进化原理启发的无标签框架，它结合多数投票答案以保持稳定性，并引入新颖性奖励来鼓励变异，通过评估解决方案与其他生成响应的推理差异来评分。实验结果表明，EVOL-RL显著优于仅基于多数的基线方法，将Qwen3-4B-Base模型在AIME25上的pass@1从4.6%提升至16.4%，pass@16从18.5%提升至37.9%，同时还在MMLU-Pro和BBEH等任务上增强了跨领域泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Refined Bayesian Optimization for Efficient Beam Alignment in Intelligent Indoor Wireless Environments</div>
<div class="meta-line">Authors: Parth Ashokbhai Shiroya, Amod Ashtekar, Swarnagowri Shashidhar, Mohammed E. Eltayeb</div>
<div class="meta-line">First: 2025-11-12T22:46:39+00:00 · Latest: 2026-02-17T23:23:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00036v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00036v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Future intelligent indoor wireless environments require fast and reliable beam alignment to sustain high-throughput links under mobility and blockage. Exhaustive beam training achieves optimal performance but is prohibitively costly. In indoor settings, dense scatterers and transceiver hardware imperfections introduce multipath and sidelobe leakage, producing measurable power across multiple angles and reducing the effectiveness of outdoor-oriented alignment algorithms. This paper presents a Refined Bayesian Optimization (R-BO) framework that exploits the inherent structure of mmWave transceiver patterns, where received power gradually increases as the transmit and receive beams converge toward the optimum. R-BO integrates a Gaussian Process (GP) surrogate with a Matern kernel and an Expected Improvement (EI) acquisition function, followed by a localized refinement around the predicted optimum. The GP hyperparameters are re-optimized online to adapt to irregular variations in the measured angular power field caused by reflections and sidelobe leakage. Experiments across 43 receiver positions in an indoor laboratory demonstrate 97.7% beam-alignment accuracy within 10 degrees, less than 0.3 dB average loss, and an 88% reduction in probing overhead compared to exhaustive search. These results establish R-BO as an efficient and adaptive beam-alignment solution for real-time intelligent indoor wireless environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向智能室内无线环境高效波束对准的精细化贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">未来智能室内无线环境需通过快速可靠的波束对准以维持移动与遮挡下的高吞吐量链路。穷举波束训练虽能实现最优性能，但成本过高。室内环境中，密集散射体与收发硬件缺陷会引发多径和旁瓣泄漏，导致多角度可测功率分布，降低面向室外场景的对准算法效能。本文提出一种精细化贝叶斯优化框架，该框架利用毫米波收发模式的固有结构——当收发波束向最优方向收敛时接收功率逐渐增强。该方法集成采用马特恩核的高斯过程代理模型与期望提升采集函数，并在预测最优值附近进行局部精细化。高斯过程超参数通过在线重优化以适应由反射和旁瓣泄漏引起的角度功率场不规则变化。在室内实验室43个接收位置的实验表明：该方法在10度误差范围内实现97.7%的波束对准精度，平均损耗低于0.3分贝，且探测开销较穷举搜索降低88%。这些结果确立了精细化贝叶斯优化作为实时智能室内无线环境高效自适应波束对准解决方案的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for fast and reliable beam alignment in intelligent indoor wireless environments, where exhaustive beam training is too costly and outdoor-oriented algorithms are ineffective due to multipath and hardware imperfections, this paper proposes a Refined Bayesian Optimization (R-BO) framework. The method leverages the gradual power increase near optimal beams by using a Gaussian Process surrogate with a Matern kernel and Expected Improvement acquisition, followed by localized refinement, with online hyperparameter re-optimization to adapt to irregular power variations. Experimental results from 43 indoor receiver positions show 97.7% alignment accuracy within 10 degrees, less than 0.3 dB average loss, and an 88% reduction in probing overhead compared to exhaustive search, establishing R-BO as an efficient adaptive solution.</div>
<div class="mono" style="margin-top:8px">本文针对智能室内无线环境中快速可靠的波束对准需求而展开，由于多径效应和硬件缺陷使得穷举波束训练成本过高且室外导向算法失效，因此提出了一种精细化贝叶斯优化（R-BO）框架。该方法利用接近最优波束时功率逐渐增加的特性，采用带有Matern核的高斯过程代理和期望提升采集函数，并进行局部细化，同时在线重新优化超参数以适应不规则功率变化。在室内实验室43个接收器位置的实验结果表明，该方法在10度内实现了97.7%的对准精度，平均损耗小于0.3 dB，且探测开销相比穷举搜索降低了88%，从而确立了R-BO作为一种高效自适应波束对准解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen</div>
<div class="meta-line">First: 2026-02-14T22:31:49+00:00 · Latest: 2026-02-17T22:24:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13912v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13912v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs&#x27; limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从像素到策略：强化语言模型的空间推理能力以实现内容感知的版面设计</div>
<div class="mono" style="margin-top:8px">我们提出LaySPA——一种强化学习框架，通过显式且可解释的空间推理能力赋能大语言模型（LLMs），实现内容感知的图形版面设计。LaySPA解决两大核心挑战：LLMs有限的空间推理能力与设计决策过程的不透明性。该方法将版面设计重构为结构化文本空间环境中的策略学习问题（而非像素级操作），该环境显式编码画布几何结构、元素属性及元素间关系。LaySPA生成包含可解释推理轨迹与结构化版面规范的双层输出，实现透明可控的设计决策。版面设计策略通过多目标空间评估进行优化——将版面质量分解为几何有效性、关系协调性与美学一致性，并采用相对群体优化训练以稳定开放设计空间中的学习过程。实验表明，LaySPA在提升结构有效性与视觉质量方面，不仅超越规模更大的专有LLMs，其性能更可媲美专业SOTA版面生成器，同时所需标注样本更少、延迟更低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by large language models&#x27; limited spatial reasoning and opaque design decisions in layout generation, this paper introduces LaySPA, a reinforcement learning framework that reformulates layout design as policy learning over a structured textual spatial environment encoding canvas geometry, element attributes, and relationships. The method produces interpretable reasoning traces and structured layout specifications, optimized via a multi-objective spatial critique for geometric validity, relational coherence, and aesthetic consistency, using relative group optimization for stable training. Experimental results show LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and matching specialized state-of-the-art layout generators with fewer annotated samples and lower latency.</div>
<div class="mono" style="margin-top:8px">针对大语言模型在布局生成中空间推理能力有限且设计决策不透明的问题，本文提出了LaySPA强化学习框架，将布局设计重新定义为在编码画布几何、元素属性和关系的结构化文本空间环境上的策略学习。该方法生成可解释的推理轨迹和结构化布局规范，通过多目标空间评判优化几何有效性、关系一致性和美学一致性，并采用相对群体优化实现稳定训练。实验结果表明，LaySPA提升了结构有效性和视觉质量，优于更大的专有大语言模型，性能媲美专用最先进布局生成器，同时需要更少的标注样本和更低的延迟。</div>
</details>
</div>
<div class="card">
<div class="title">MARLEM: A Multi-Agent Reinforcement Learning Simulation Framework for Implicit Cooperation in Decentralized Local Energy Markets</div>
<div class="meta-line">Authors: Nelson Salazar-Pena, Alejandra Tabares, Andres Gonzalez-Mancera</div>
<div class="meta-line">First: 2026-02-17T22:22:45+00:00 · Latest: 2026-02-17T22:22:45+00:00</div>
<div class="meta-line">Comments: 32 pages, 7 figures, 1 table, 1 algorithm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16063v1">PDF</a> · <a href="https://github.com/salazarna/marlem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a novel, open-source MARL simulation framework for studying implicit cooperation in LEMs, modeled as a decentralized partially observable Markov decision process and implemented as a Gymnasium environment for MARL. Our framework features a modular market platform with plug-and-play clearing mechanisms, physically constrained agent models (including battery storage), a realistic grid network, and a comprehensive analytics suite to evaluate emergent coordination. The main contribution is a novel method to foster implicit cooperation, where agents&#x27; observations and rewards are enhanced with system-level key performance indicators to enable them to independently learn strategies that benefit the entire system and aim for collectively beneficial outcomes without explicit communication. Through representative case studies (available in a dedicated GitHub repository in https://github.com/salazarna/marlem, we show the framework&#x27;s ability to analyze how different market configurations (such as varying storage deployment) impact system performance. This illustrates its potential to facilitate emergent coordination, improve market efficiency, and strengthen grid stability. The proposed simulation framework is a flexible, extensible, and reproducible tool for researchers and practitioners to design, test, and validate strategies for future intelligent, decentralized energy systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARLEM：一种用于去中心化本地能源市场中隐性合作的多智能体强化学习仿真框架</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的开源多智能体强化学习仿真框架，用于研究本地能源市场中的隐性合作。该框架建模为去中心化部分可观测马尔可夫决策过程，并实现为适用于多智能体强化学习的Gymnasium环境。其特点包括：采用即插即用出清机制的模块化市场平台、物理约束的智能体模型（含电池储能）、真实的电网网络，以及用于评估涌现协调的综合分析套件。主要贡献在于提出了一种促进隐性合作的新方法——通过系统级关键性能指标增强智能体的观测与奖励，使其能独立学习有益于整体系统的策略，在无需显式通信的情况下实现集体效益。通过代表性案例研究（详见GitHub仓库https://github.com/salazarna/marlem），我们展示了该框架分析不同市场配置（如储能部署方案）如何影响系统性能的能力，印证了其在促进涌现协调、提升市场效率、增强电网稳定性方面的潜力。该仿真框架为研究人员和从业者设计、测试和验证未来智能去中心化能源系统策略提供了灵活、可扩展且可复现的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to study implicit cooperation in decentralized local energy markets (LEMs) without explicit communication, this paper introduces MARLEM, a novel open-source multi-agent reinforcement learning (MARL) simulation framework. The method models LEMs as a decentralized partially observable Markov decision process, implemented as a Gymnasium environment, and features a modular platform with plug-and-play market clearing, physically constrained agents, a realistic grid, and an analytics suite; its key innovation is enhancing agents&#x27; observations and rewards with system-level key performance indicators to foster independently learned, collectively beneficial strategies. Experimental results from case studies demonstrate the framework&#x27;s ability to analyze how factors like storage deployment impact system performance, illustrating its potential to facilitate emergent coordination, improve market efficiency, and strengthen grid stability, providing a flexible tool for designing future decentralized energy systems.</div>
<div class="mono" style="margin-top:8px">本文旨在研究无需显式通信的去中心化本地能源市场中的隐性合作，提出了MARLEM这一新颖的开源多智能体强化学习仿真框架。该方法将本地能源市场建模为去中心化部分可观测马尔可夫决策过程，并以Gymnasium环境实现，其核心模块化平台支持即插即用的市场出清机制、物理约束的智能体模型、真实电网网络和综合分析套件；关键创新在于通过系统级关键绩效指标增强智能体的观测和奖励，以促使其独立学习对整体系统有益的策略。代表性案例研究表明，该框架能够分析不同市场配置（如储能部署变化）对系统性能的影响，验证了其在促进协同涌现、提升市场效率和增强电网稳定性方面的潜力，为未来智能去中心化能源系统的策略设计与验证提供了灵活可扩展的工具。</div>
</details>
</div>
<div class="card">
<div class="title">Harnessing Implicit Cooperation: A Multi-Agent Reinforcement Learning Approach Towards Decentralized Local Energy Markets</div>
<div class="meta-line">Authors: Nelson Salazar-Pena, Alejandra Tabares, Andres Gonzalez-Mancera</div>
<div class="meta-line">First: 2026-02-17T22:22:32+00:00 · Latest: 2026-02-17T22:22:32+00:00</div>
<div class="meta-line">Comments: 42 pages, 7 figures, 10 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16062v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes implicit cooperation, a framework enabling decentralized agents to approximate optimal coordination in local energy markets without explicit peer-to-peer communication. We formulate the problem as a decentralized partially observable Markov decision problem that is solved through a multi-agent reinforcement learning task in which agents use stigmergic signals (key performance indicators at the system level) to infer and react to global states. Through a 3x3 factorial design on an IEEE 34-node topology, we evaluated three training paradigms (CTCE, CTDE, DTDE) and three algorithms (PPO, APPO, SAC). Results identify APPO-DTDE as the optimal configuration, achieving a coordination score of 91.7% relative to the theoretical centralized benchmark (CTCE). However, a critical trade-off emerges between efficiency and stability: while the centralized benchmark maximizes allocative efficiency with a peer-to-peer trade ratio of 0.6, the fully decentralized approach (DTDE) demonstrates superior physical stability. Specifically, DTDE reduces the variance of grid balance by 31% compared to hybrid architectures, establishing a highly predictable, import-biased load profile that simplifies grid regulation. Furthermore, topological analysis reveals emergent spatial clustering, where decentralized agents self-organize into stable trading communities to minimize congestion penalties. While SAC excelled in hybrid settings, it failed in decentralized environments due to entropy-driven instability. This research proves that stigmergic signaling provides sufficient context for complex grid coordination, offering a robust, privacy-preserving alternative to expensive centralized communication infrastructure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用隐式合作：面向去中心化本地能源市场的多智能体强化学习方法</div>
<div class="mono" style="margin-top:8px">本文提出隐式合作框架，使去中心化智能体无需显式点对点通信即可在本地能源市场中近似实现最优协调。我们将问题建模为去中心化部分可观测马尔可夫决策过程，通过多智能体强化学习任务求解，其中智能体利用共识信号（系统层面的关键性能指标）推断全局状态并作出响应。基于IEEE 34节点拓扑的3×3因子设计，我们评估了三种训练范式（CTCE、CTDE、DTDE）和三种算法（PPO、APPO、SAC）。结果表明APPO-DTDE为最优配置，其协调分数达到理论集中式基准（CTCE）的91.7%。然而，效率与稳定性之间存在关键权衡：集中式基准以0.6的点对点交易比率实现分配效率最大化，而完全去中心化方法（DTDE）展现出更优的物理稳定性。具体而言，DTDE将电网平衡方差较混合架构降低31%，形成高度可预测的偏进口负荷曲线，简化了电网调控。拓扑分析进一步揭示了涌现的空间聚类现象——去中心化智能体自组织为稳定交易社区以最小化拥堵惩罚。虽然SAC在混合环境中表现优异，但其熵驱动的不稳定性导致在去中心化场景中失效。本研究证明共识信号能为复杂电网协调提供充分语境，为昂贵的集中式通信基础设施提供了兼具鲁棒性与隐私保护的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient coordination in local energy markets without relying on explicit communication, this paper introduces an implicit cooperation framework using multi-agent reinforcement learning, where agents infer global states via stigmergic signals like system-level performance indicators. The method involves solving a decentralized partially observable Markov decision problem through a factorial design comparing training paradigms and algorithms on an IEEE 34-node topology. Experimental results identify APPO-DTDE as the optimal configuration, achieving 91.7% coordination relative to a centralized benchmark, while revealing a trade-off: centralized approaches maximize allocative efficiency, but fully decentralized DTDE enhances grid stability by reducing balance variance by 31% and fostering emergent spatial clustering among agents.</div>
<div class="mono" style="margin-top:8px">本文旨在解决本地能源市场中无需显式通信的高效协调问题，提出了一种基于多智能体强化学习的隐式合作框架，其中智能体通过系统级性能指标等间接信号推断全局状态。方法上，将问题建模为分散式部分可观测马尔可夫决策过程，并在IEEE 34节点拓扑上通过因子设计比较训练范式和算法。实验结果表明，APPO-DTDE为最优配置，相对于集中式基准实现了91.7%的协调度，但揭示了效率与稳定性之间的权衡：集中式方法最大化分配效率，而完全分散的DTDE通过降低电网平衡方差31%并促进智能体自组织成稳定交易社区，显著提升了物理稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching</div>
<div class="meta-line">Authors: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza, Guanya Shi, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-17T18:59:11+00:00 · Latest: 2026-02-17T18:59:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15827v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知型人形机器人跑酷：通过运动匹配串联动态人体技能</div>
<div class="mono" style="margin-top:8px">尽管人形机器人运动控制领域近期已在多变地形稳定行走方面取得进展，但如何实现高度动态人体动作的敏捷性与适应性仍是开放挑战。复杂环境中的敏捷跑酷不仅需要底层鲁棒性，更要求类人动作表现力、长时程技能组合及感知驱动决策。本文提出感知型人形机器人跑酷框架，该模块化系统使人形机器人能基于视觉自主完成挑战性障碍场景的长时程跑酷。我们首先通过特征空间最近邻搜索实现运动匹配，将重定向的原子化人体技能组合为长时程运动轨迹，在保持动态人体动作优雅流畅性的同时，实现复杂技能链的灵活组合与平滑过渡。随后，针对组合动作训练运动跟踪强化学习专家策略，并融合DAgger与强化学习将其蒸馏为基于深度的多技能学生策略。感知与技能组合的协同机制实现了自主情境感知决策：机器人仅凭机载深度传感与离散二维速度指令，即可根据障碍物几何形态与高度自主选择并执行跨越、攀爬、撑越或滚落等动作。通过在Unitree G1人形机器人上的大量实物实验验证，本框架成功展示了攀越1.25米障碍（相当于机器人身高96%）等高动态跑酷技能，以及应对实时障碍扰动的长时程多障碍闭环自适应穿越能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling humanoid robots to perform agile, long-horizon parkour in complex environments, which requires not only robustness but also human-like motion expressiveness and perception-driven decision-making. The proposed Perceptive Humanoid Parkour (PHP) framework first uses motion matching to compose retargeted human skills into long-horizon kinematic trajectories, then trains and distills reinforcement learning policies into a single depth-based student policy. Experimental results on a Unitree G1 robot demonstrate autonomous execution of dynamic skills like climbing obstacles up to 1.25m tall and adapting to perturbations in multi-obstacle courses.</div>
<div class="mono" style="margin-top:8px">本文旨在解决人形机器人在复杂环境中执行敏捷、长时程跑酷的挑战，这需要不仅具备鲁棒性，还需具有类人运动表现力和感知驱动的决策能力。提出的感知人形跑酷框架首先利用运动匹配将重定向的人类技能组合成长时程运动轨迹，然后训练并将强化学习策略蒸馏为单一的基于深度的策略。在Unitree G1机器人上的实验结果表明，该系统能自主执行动态技能，如攀爬高达1.25米的障碍物，并在多障碍物场景中适应实时扰动。</div>
</details>
</div>
<div class="card">
<div class="title">Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning</div>
<div class="meta-line">Authors: Oswin So, Eric Yang Yu, Songyuan Zhang, Matthew Cleaveland, Mitchell Black, Chuchu Fan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-17T18:53:31+00:00 · Latest: 2026-02-17T18:53:31+00:00</div>
<div class="meta-line">Comments: ICLR 2026. The project page can be found at https://oswinso.xyz/fge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15817v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15817v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习求解可行性未知的参数鲁棒可达性问题</div>
<div class="mono" style="margin-top:8px">深度强化学习（RL）近期在高维控制任务中取得显著进展，但将其应用于可达性问题时存在根本性错配：可达性旨在最大化系统能永久保持安全的状态集合，而RL则针对用户指定的分布优化期望回报。这种错配可能导致策略在安全集内低概率状态上表现不佳。一种自然的替代方案是将问题构建为对指定初始状态、动态模型和安全集的初始条件集合进行鲁棒优化，但该问题是否存在解取决于指定集合的可行性——这是先验未知的。我们提出可行性引导探索（FGE）方法，该方法能同时识别存在安全策略的可行初始条件子集，并学习策略以解决该初始条件集合上的可达性问题。实验结果表明，在MuJoCo模拟器和基于像素观测的Kinetix模拟器中，针对具有挑战性的初始条件，FGE学习到的策略比现有最佳方法覆盖范围提升超过50%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a mismatch between deep reinforcement learning (RL), which optimizes expected returns over a specified distribution, and reachability problems, which aim to maximize the set of states from which a system remains safe indefinitely; this mismatch can lead to poor performance on low-probability safe states. To overcome this, the authors propose Feasibility-Guided Exploration (FGE), a method that concurrently identifies a feasible subset of initial conditions—where a safe policy exists—and learns a policy to solve the reachability problem over this set. Experimental results in MuJoCo and Kinetix simulators show that FGE achieves over 50% more coverage than existing methods for challenging initial conditions.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习（RL）与可达性问题之间的不匹配问题展开研究：RL优化指定分布上的期望回报，而可达性旨在最大化系统保持安全的状态集合，这种不匹配可能导致在低概率安全状态下性能不佳。为此，作者提出了可行性引导探索（FGE）方法，该方法同时识别出存在安全策略的可行初始条件子集，并学习在该子集上解决可达性问题的策略。在MuJoCo和Kinetix模拟器上的实验结果表明，对于具有挑战性的初始条件，FGE比现有最佳方法实现了超过50%的覆盖率提升。</div>
</details>
</div>
<div class="card">
<div class="title">Horizon Imagination: Efficient On-Policy Rollout in Diffusion World Models</div>
<div class="meta-line">Authors: Lior Cohen, Ofir Nabati, Kaixin Wang, Navdeep Kumar, Shie Mannor</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-08T16:07:04+00:00 · Latest: 2026-02-17T18:50:34+00:00</div>
<div class="meta-line">Comments: This paper will be published in the ICLR 2026 proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08032v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08032v2">PDF</a> · <a href="https://github.com/leor-c/horizon-imagination">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>地平线想象：扩散世界模型中的高效同策略推演</div>
<div class="mono" style="margin-top:8px">本文研究基于扩散的世界模型在强化学习中的应用，这类模型虽具备高生成保真度，但在控制任务中面临严重的效率挑战。现有方法要么依赖推理时的高复杂度模型，要么采用高度序列化的想象过程，两者均带来难以承受的计算开销。我们提出地平线想象（HI），一种面向离散随机策略的同策略想象过程，能够并行地对多个未来观测进行去噪。HI融合了稳定化机制与一种新颖的采样调度方案，该方案将去噪计算预算与去噪作用的有效时间范围解耦，同时支持子帧级预算。在Atari 100K和Craftium上的实验表明，我们的方法在仅使用一半去噪步数的子帧预算下仍能保持控制性能，并在多种调度方案下实现更优的生成质量。代码发布于https://github.com/leor-c/horizon-imagination。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational inefficiency of diffusion-based world models in reinforcement learning, which suffer from high inference costs due to either large models or sequential imagination processes. The authors propose Horizon Imagination (HI), an on-policy method that enables parallel denoising of multiple future observations for discrete stochastic policies, incorporating a stabilization mechanism and a novel sampling schedule to decouple denoising steps from the effective horizon and allow sub-frame budgets. Experimental results on Atari 100K and Craftium demonstrate that HI maintains control performance with only half the denoising steps and achieves superior generation quality across different schedules.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中基于扩散的世界模型计算效率低下的问题，这类模型因需要大型模型或顺序想象过程而面临高昂推理成本。作者提出了地平线想象（HI），这是一种用于离散随机策略的在线策略方法，能够并行去噪多个未来观测，结合了稳定机制和新颖的采样计划，以解耦去噪步骤与有效视野，并支持子帧预算。在Atari 100K和Craftium上的实验结果表明，HI仅用一半的去噪步骤即可保持控制性能，并在不同计划下实现了更优的生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">GLM-5: from Vibe Coding to Agentic Engineering</div>
<div class="meta-line">Authors: GLM-5 Team, :, Aohan Zeng, Xin Lv, Zhenyu Hou, Zhengxiao Du, Qinkai Zheng, Bin Chen, Da Yin, Chendi Ge, Chengxing Xie, Cunxiang Wang, Gengzheng Pan, Hao Zeng, Haoke Zhang, Haoran Wang, Huilong Chen, Jiajie Zhang, Jian Jiao, Jiaqi Guo, Jingsen Wang, Jingzhao Du, Jinzhu Wu, Kedong Wang, Lei Li, Lin Fan, Lucen Zhong, Mingdao Liu, Mingming Zhao, Pengfan Du, Qian Dong, Rui Lu, Shuang-Li, Shulin Cao, Song Liu, Ting Jiang, Xiaodong Chen, Xiaohan Zhang, Xuancheng Huang, Xuezhen Dong, Yabo Xu, Yao Wei, Yifan An, Yilin Niu, Yitong Zhu, Yuanhao Wen, Yukuo Cen, Yushi Bai, Zhongpei Qiao, Zihan Wang, Zikang Wang, Zilin Zhu, Ziqiang Liu, Zixuan Li, Bojie Wang, Bosi Wen, Can Huang, Changpeng Cai, Chao Yu, Chen Li, Chen Li, Chenghua Huang, Chengwei Hu, Chenhui Zhang, Chenzheng Zhu, Congfeng Yin, Daoyan Lin, Dayong Yang, Di Wang, Ding Ai, Erle Zhu, Fangzhou Yi, Feiyu Chen, Guohong Wen, Hailong Sun, Haisha Zhao, Haiyi Hu, Hanchen Zhang, Hanrui Liu, Hanyu Zhang, Hao Peng, Hao Tai, Haobo Zhang, He Liu, Hongwei Wang, Hongxi Yan, Hongyu Ge, Huan Liu, Huan Liu, Huanpeng Chu, Jia&#x27;ni Zhao, Jiachen Wang, Jiajing Zhao, Jiamin Ren, Jiapeng Wang, Jiaxin Zhang, Jiayi Gui, Jiayue Zhao, Jijie Li, Jing An, Jing Li, Jingwei Yuan, Jinhua Du, Jinxin Liu, Junkai Zhi, Junwen Duan, Kaiyue Zhou, Kangjian Wei, Ke Wang, Keyun Luo, Laiqiang Zhang, Leigang Sha, Liang Xu, Lindong Wu, Lintao Ding, Lu Chen, Minghao Li, Nianyi Lin, Pan Ta, Qiang Zou, Rongjun Song, Ruiqi Yang, Shangqing Tu, Shangtong Yang, Shaoxiang Wu, Shengyan Zhang, Shijie Li, Shuang Li, Shuyi Fan, Wei Qin, Wei Tian, Weining Zhang, Wenbo Yu, Wenjie Liang, Xiang Kuang, Xiangmeng Cheng, Xiangyang Li, Xiaoquan Yan, Xiaowei Hu, Xiaoying Ling, Xing Fan, Xingye Xia, Xinyuan Zhang, Xinze Zhang, Xirui Pan, Xunkai Zhang, Yandong Wu, Yanfu Li, Yidong Wang, Yifan Zhu, Yijun Tan, Yilin Zhou, Yiming Pan, Ying Zhang, Yinpei Su, Yipeng Geng, Yipeng Geng, Yong Yan, Yonglin Tan, Yuean Bi, Yuhan Shen, Yuhao Yang, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yurong Wu, Yutao Zhang, Yuxi Duan, Yuxuan Zhang, Zezhen Liu, Zhengtao Jiang, Zhenhe Yan, Zheyu Zhang, Zhixiang Wei, Zhuo Chen, Zhuoer Feng, Zijun Yao, Ziwei Chai, Ziyuan Wang, Zuzhou Zhang, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</div>
<div class="meta-line">First: 2026-02-17T17:50:56+00:00 · Latest: 2026-02-17T17:50:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15763v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15763v1">PDF</a> · <a href="https://github.com/zai-org/GLM-5">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLM-5：从氛围编码到智能体工程</div>
<div class="mono" style="margin-top:8px">我们推出GLM-5，这是一款旨在将范式从氛围编码转向智能体工程的下一代基础模型。基于前代模型的智能体、推理与编码（ARC）能力，GLM-5采用DSA架构显著降低训练与推理成本，同时保持长上下文保真度。为推进模型对齐与自主性，我们构建了新型异步强化学习基础设施，通过解耦生成与训练大幅提升后训练效率。此外，我们提出创新的异步智能体强化学习算法，进一步提升强化学习质量，使模型能更有效地从复杂长程交互中学习。通过这些创新，GLM-5在主流开放基准测试中取得最先进性能。最关键的是，GLM-5在实际编码任务中展现出前所未有的能力，在处理端到端软件工程挑战方面超越以往基线。代码、模型及更多信息详见https://github.com/zai-org/GLM-5。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GLM-5, a next-generation foundation model motivated by advancing from vibe coding to agentic engineering, aiming to enhance agentic, reasoning, and coding capabilities. The method employs DSA to reduce training and inference costs while preserving long-context fidelity, and implements an asynchronous reinforcement learning infrastructure with novel algorithms to improve post-training efficiency and learning from complex interactions. Experimental results show that GLM-5 achieves state-of-the-art performance on major open benchmarks and demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in end-to-end software engineering challenges.</div>
<div class="mono" style="margin-top:8px">本文介绍了GLM-5，这是一个新一代基础模型，其动机是从氛围编码转向代理工程，旨在提升代理、推理和编码能力。方法上采用DSA以降低训练和推理成本，同时保持长上下文保真度，并实施异步强化学习基础设施及新算法，以提高训练后效率并从复杂长期交互中更有效地学习。实验结果表明，GLM-5在主要开放基准测试中达到最先进性能，并在现实世界编码任务中展现出前所未有的能力，在处理端到端软件工程挑战方面超越了先前基线。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Gradients for Cumulative Prospect Theory in Reinforcement Learning</div>
<div class="meta-line">Authors: Olivier Lepel, Anas Barakat</div>
<div class="meta-line">First: 2024-10-03T15:45:39+00:00 · Latest: 2026-02-17T17:15:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.02605v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.02605v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We derive a policy gradient theorem for Cumulative Prospect Theory (CPT) objectives in finite-horizon Reinforcement Learning (RL), generalizing the standard policy gradient theorem and encompassing distortion-based risk objectives as special cases. Motivated by behavioral economics, CPT combines an asymmetric utility transformation around a reference point with probability distortion. Building on our theorem, we design a first-order policy gradient algorithm for CPT-RL using a Monte Carlo gradient estimator based on order statistics. We establish statistical guarantees for the estimator and prove asymptotic convergence of the resulting algorithm to first-order stationary points of the (generally non-convex) CPT objective. Simulations illustrate qualitative behaviors induced by CPT and compare our first-order approach to existing zeroth-order methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中累积前景理论的政策梯度方法</div>
<div class="mono" style="margin-top:8px">本文针对有限时域强化学习中的累积前景理论目标，推导了政策梯度定理，该定理推广了标准政策梯度定理，并将基于扭曲的风险目标作为特例涵盖。受行为经济学启发，累积前景理论结合了参考点附近的不对称效用变换与概率扭曲。基于此定理，我们设计了一种基于顺序统计量的蒙特卡洛梯度估计器的一阶政策梯度算法，用于累积前景理论强化学习。我们为该估计器建立了统计保证，并证明了算法对（通常非凸的）累积前景理论目标的一阶稳定点的渐近收敛性。仿真实验展示了累积前景理论诱导的定性行为，并将我们的一阶方法与现有的零阶方法进行了比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by behavioral economics, this paper introduces a policy gradient theorem for optimizing Cumulative Prospect Theory (CPT) objectives in finite-horizon reinforcement learning, which generalizes standard policy gradients to incorporate asymmetric utility and probability distortion around a reference point. The method involves designing a first-order policy gradient algorithm that uses a Monte Carlo gradient estimator based on order statistics, with established statistical guarantees. Experimental simulations demonstrate the qualitative behaviors induced by CPT and show that this first-order approach outperforms existing zeroth-order methods, with proven asymptotic convergence to stationary points of the non-convex CPT objective.</div>
<div class="mono" style="margin-top:8px">受行为经济学启发，本文针对有限时域强化学习中的累积前景理论目标，推导出了一个策略梯度定理，该定理推广了标准策略梯度，包含了围绕参考点的不对称效用和概率扭曲。方法上，基于该定理设计了一种使用基于顺序统计量的蒙特卡洛梯度估计器的一阶策略梯度算法，并建立了其统计保证。实验模拟展示了累积前景理论诱导的定性行为，并证明该一阶方法优于现有的零阶方法，同时理论证明了算法能渐近收敛到非凸累积前景理论目标的一阶稳定点。</div>
</details>
</div>
<div class="card">
<div class="title">FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring</div>
<div class="meta-line">Authors: Yousef Emami, Hao Zhou, Miguel Gutierrez Gaitan, Kai Li, Luis Almeida</div>
<div class="meta-line">First: 2025-07-14T10:24:43+00:00 · Latest: 2026-02-17T17:12:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10134v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.10134v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncrewed Aerial Vehicles (UAVs) play a vital role in public safety, especially in monitoring wildfires, where early detection reduces environmental impact. In UAV-Assisted Wildfire Monitoring (UAWM) systems, jointly optimizing the data collection schedule and UAV velocity is essential to minimize the average Age of Information (AoI) for sensory data. Deep Reinforcement Learning (DRL) has been used for this optimization, but its limitations-including low sampling efficiency, discrepancies between simulation and real-world conditions, and complex training make it unsuitable for time-critical applications such as wildfire monitoring. Recent advances in Large Language Models (LLMs) provide a promising alternative. With strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation using natural language prompts and example-based guidance without retraining. This paper proposes a novel online Flight Resource Allocation scheme based on LLM-Enabled In-Context Learning (FRSICL) to jointly optimize the data collection schedule and UAV velocity along the trajectory in real time, thereby asymptotically minimizing the average AoI across all ground sensors. Unlike DRL, FRSICL generates data collection schedules and velocities using natural language task descriptions and feedback from the environment, enabling dynamic decision-making without extensive retraining. Simulation results confirm the effectiveness of FRSICL compared to state-of-the-art baselines, namely Proximal Policy Optimization, Block Coordinate Descent, and Nearest Neighbor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FRSICL：基于大语言模型情境学习的无人机辅助森林火灾监测中实时飞行资源分配策略</div>
<div class="mono" style="margin-top:8px">无人机在公共安全领域发挥着关键作用，尤其在森林火灾监测中，早期发现能显著降低环境影响。在无人机辅助火灾监测系统中，联合优化数据采集调度与飞行速度对最小化传感数据的平均信息时效至关重要。传统深度强化学习方法存在采样效率低、仿真与实境差异大、训练复杂等局限，难以适用于火灾监测等时效敏感场景。大语言模型凭借强大的推理与泛化能力，可通过情境学习机制仅依靠自然语言提示与示例指导实现任务适配，无需重新训练。本文提出基于大语言模型情境学习的在线飞行资源分配方案，实时联合优化轨迹上的数据采集调度与飞行速度，渐进最小化所有地面传感器的平均信息时效。该方案通过自然语言任务描述与环境反馈生成决策，无需大量重训练即可实现动态优化。仿真实验表明，相较于近端策略优化、块坐标下降及最近邻等先进基线方法，本方案具有显著优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of minimizing data age in UAV-assisted wildfire monitoring by jointly optimizing data collection schedules and UAV velocities, a task where traditional deep reinforcement learning suffers from inefficiency and simulation-to-reality gaps. The authors propose FRSICL, a novel online flight resource allocation scheme that leverages large language models (LLMs) with in-context learning, enabling dynamic decision-making through natural language prompts and environmental feedback without extensive retraining. Experimental simulations demonstrate that FRSICL effectively reduces the average Age of Information and outperforms baseline methods like Proximal Policy Optimization, Block Coordinate Descent, and Nearest Neighbor.</div>
<div class="mono" style="margin-top:8px">本文针对无人机辅助森林火灾监测中数据时效性最小化的挑战，提出联合优化数据收集调度与无人机速度，传统深度强化学习方法在此存在效率低下和仿真与现实差距的问题。作者设计了FRSICL方案，一种基于大语言模型上下文学习的在线飞行资源分配方法，通过自然语言提示和环境反馈实现动态决策，无需大量重新训练。仿真实验表明，FRSICL能有效降低平均信息年龄，其性能优于近端策略优化、块坐标下降和最近邻等先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction</div>
<div class="meta-line">Authors: Qiang Zhang, Jiahao Ma, Peiran Liu, Shuai Shi, Zeran Su, Zifan Wang, Jingkai Sun, Wei Cui, Jialin Yu, Gang Han, Wen Zhao, Pihai Sun, Kangning Yin, Jiaxu Wang, Jiahang Cao, Lingfeng Zhang, Hao Cheng, Xiaoshuai Hao, Yiding Ji, Junwei Liang, Jian Tang, Renjing Xu, Yijie Guo</div>
<div class="meta-line">First: 2026-02-17T17:09:45+00:00 · Latest: 2026-02-17T17:09:45+00:00</div>
<div class="meta-line">Comments: 17 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15733v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled &quot;motion-terrain&quot; interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MeshMimic：通过三维场景重建实现几何感知的人形机器人运动学习</div>
<div class="mono" style="margin-top:8px">近年来，人形机器人运动控制领域取得重大突破，深度强化学习成为实现复杂类人行为的主要推动力。然而，人形机器人高维度和复杂的动力学特性使得手动运动设计不切实际，导致严重依赖昂贵的动作捕捉数据。这些数据集不仅获取成本高，且常缺乏周围物理环境必要的几何上下文。因此，现有运动合成框架常面临运动与场景脱节的问题，导致在感知地形任务中出现接触滑移或网格穿透等物理不一致现象。本研究提出MeshMimic创新框架，通过融合三维场景重建与具身智能，使人形机器人能直接从视频中学习耦合的“运动-地形”交互。借助前沿三维视觉模型，本框架精确分割并重建人体运动轨迹及地形物体的底层三维几何结构。我们提出基于运动学一致性的优化算法，从含噪声的视觉重建中提取高质量运动数据，并引入接触不变的重定向方法，将人-环境交互特征迁移至人形智能体。实验结果表明，MeshMimic在多样复杂地形中均能实现鲁棒且高度动态的性能。该方法证明，仅使用消费级单目传感器的低成本流程即可训练复杂物理交互，为人形机器人在非结构化环境中的自主演进提供了可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for MeshMimic stems from the high cost and geometric limitations of motion capture data for humanoid control, which often leads to physical inconsistencies like slippage or penetration in terrain-aware tasks. The method introduces a framework that bridges 3D scene reconstruction and embodied intelligence, leveraging 3D vision models to segment and reconstruct human trajectories and terrain geometry from video, followed by an optimization algorithm for motion extraction and a contact-invariant retargeting method to transfer interactions to a humanoid agent. Experimental results show that MeshMimic achieves robust, dynamic performance across diverse terrains, demonstrating that a low-cost pipeline using consumer monocular sensors can effectively train complex physical interactions for autonomous humanoid evolution in unstructured environments.</div>
<div class="mono" style="margin-top:8px">MeshMimic的动机源于人形机器人运动控制中动作捕捉数据成本高昂且缺乏几何环境信息，常导致地形感知任务中出现滑动或穿透等物理不一致问题。该方法提出了一个连接3D场景重建与具身智能的框架，利用先进的3D视觉模型从视频中分割并重建人体轨迹和地形几何，通过基于运动一致性的优化算法从噪声视觉重建中提取高质量运动数据，并采用接触不变的重定向方法将人与环境交互特征迁移至人形智能体。实验结果表明，MeshMimic在多样且具有挑战性的地形上实现了鲁棒、高度动态的性能，证明仅使用消费级单目传感器的低成本流程能有效训练复杂物理交互，为人形机器人在非结构化环境中的自主演进提供了可扩展路径。</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Concept Evolution for Compositional Reasoning in Large Language Models</div>
<div class="meta-line">Authors: Sarim Chaudhry</div>
<div class="meta-line">First: 2026-02-17T17:01:42+00:00 · Latest: 2026-02-17T17:01:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15725v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15725v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model&#x27;s latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型组合推理中的递归概念演化</div>
<div class="mono" style="margin-top:8px">大语言模型在复杂推理任务中表现优异，但在需要组合推理的基准测试（包括ARC-AGI-2、GPQA、MATH、BBH和HLE）中准确性显著下降。现有方法通过思维链提示、自洽性或强化学习扩展词元级搜索以改进推理，但保持模型的潜在表示空间固定。当所需抽象未编码于该空间时，性能会崩溃。我们提出递归概念演化框架，使预训练语言模型能在推理过程中修改其内部表示几何结构。该框架引入动态生成的低秩概念子空间：在检测到表示不足时生成，通过最小描述长度准则选择，在协同作用时合并，并通过约束优化进行固化以保持稳定性。这一过程使模型能构建新抽象而非重组现有抽象。我们将该框架与Mistral-7B集成，在组合推理基准测试中评估：ARC-AGI-2提升12-18个百分点，GPQA和BBH提升8-14个百分点，并在MATH和HLE上持续降低深度诱导误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of large language models in compositional reasoning tasks, where performance drops despite existing methods like chain-of-thought prompting, as these methods fail to adapt the model&#x27;s fixed latent representation space when required abstractions are missing. To overcome this, the authors propose Recursive Concept Evolution (RCE), a framework that dynamically generates low-rank concept subspaces during inference, detected via representational inadequacy, selected with minimum description length, merged for synergy, and consolidated through constrained optimization to maintain stability, enabling the construction of new abstractions. Experimental results show that integrating RCE with Mistral-7B leads to significant improvements, including 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent error reductions on MATH and HLE.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在组合推理任务中的局限性提出研究，现有方法如思维链提示无法调整模型固定的潜在表示空间，导致所需抽象缺失时性能下降。为此，作者提出递归概念演化框架，在推理过程中动态生成低秩概念子空间，通过检测表示不足、基于最小描述长度准则选择、合并协同概念，并利用约束优化进行整合以保持稳定性，从而构建新抽象。实验结果表明，将RCE与Mistral-7B集成后，在ARC-AGI-2上获得12-18分提升，在GPQA和BBH上提高8-14分，并在MATH和HLE上持续降低深度诱导错误。</div>
</details>
</div>
<div class="card">
<div class="title">On the Role of Iterative Computation in Reinforcement Learning</div>
<div class="meta-line">Authors: Raj Ghugare, Michał Bortkiewicz, Alicja Ziarko, Benjamin Eysenbach</div>
<div class="meta-line">First: 2026-02-05T18:45:57+00:00 · Latest: 2026-02-17T16:47:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05999v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05999v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论迭代计算在强化学习中的作用</div>
<div class="mono" style="margin-top:8px">强化学习策略可用的计算量如何影响其学习效果？使用固定参数量的策略是否仍能受益于额外计算？标准强化学习框架缺乏正式回答这些问题的理论语言。实践中，深度强化学习策略常被参数化为静态架构的神经网络，混淆了计算量与参数数量。本文形式化了计算受限策略，并证明使用更多计算量的策略能够解决计算量较少策略无法处理的问题，并泛化至更长时域的任务。基于算法学习和无模型规划的前期研究，我们提出一种能够使用可变计算量的最小化架构。实验与理论相互印证：在涵盖在线与离线强化学习的31项不同任务中，该架构（1）仅通过增加计算量即可获得更强性能，（2）与使用多达5倍参数的标准前馈网络或深度残差网络相比，在更长时域测试任务中展现出更强的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how the computational budget, distinct from parameter count, influences reinforcement learning (RL) performance and generalization. The authors formalize compute-bounded policies and propose a minimal architecture that can leverage variable amounts of iterative computation, drawing on concepts from algorithmic learning and model-free planning. Experimental results across 31 online and offline RL tasks demonstrate that this architecture achieves stronger performance with increased compute and better generalization to longer-horizon tasks compared to standard feedforward or deeper residual networks, even those with many more parameters.</div>
<div class="mono" style="margin-top:8px">本文研究了计算预算（区别于参数数量）如何影响强化学习的性能与泛化能力。作者形式化了计算受限的策略，并基于算法学习和无模型规划的概念，提出了一种能够利用可变迭代计算量的最小化架构。在31个在线与离线强化学习任务上的实验结果表明，该架构通过使用更多计算资源，不仅获得了更强的性能，而且在更长视野的测试任务上比标准前馈网络或参数多出5倍的深度残差网络具有更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV</div>
<div class="meta-line">Authors: Yousef Emami, Hao Zhou, Miguel Gutierrez Gaitan, Kai Li, Luis Almeida, Zhu Han</div>
<div class="meta-line">First: 2025-06-03T09:01:33+00:00 · Latest: 2026-02-17T16:45:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.02649v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.02649v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A public safety Uncrewed Aerial Vehicle (UAV) enhances situational awareness during emergency response. Its agility, mobility optimization, and ability to establish Line-of-Sight (LoS) communication make it increasingly important for managing emergencies such as disaster response, search and rescue, and wildfire monitoring. Although Deep Reinforcement Learning (DRL) has been used to optimize UAV navigation and control, its high training complexity, low sample efficiency, and the simulation-to-reality gap limit its practicality in public safety applications. Recent advances in Large Language Models (LLMs) present a promising alternative. With strong reasoning and generalization abilities, LLMs can adapt to new tasks through In-Context Learning (ICL), enabling task adaptation via natural language prompts and example-based guidance without retraining. Deploying LLMs at the network edge, rather than in the cloud, further reduces latency and preserves data privacy, making them suitable for real-time, mission-critical public safety UAVs. This paper proposes integrating LLM-assisted ICL with public safety UAVs to address key functions such as path planning and velocity control in emergency response. We present a case study on data collection scheduling, demonstrating that the LLM-assisted ICL framework can significantly reduce packet loss compared to conventional approaches while also mitigating potential jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and outline future research directions. The ICL framework enables adaptive, context-aware decision-making for public safety UAVs, offering a lightweight and efficient solution to enhance UAV autonomy and responsiveness in emergencies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从提示到防护：基于大语言模型的上下文学习赋能智能公共安全无人机</div>
<div class="mono" style="margin-top:8px">公共安全无人机在应急响应中提升态势感知能力，其敏捷性、移动优化及视距通信建立能力，使其在灾害响应、搜救和野火监测等应急管理中日益重要。尽管深度强化学习已用于优化无人机导航与控制，但其高训练复杂度、低样本效率及仿真与现实差距限制了其在公共安全应用中的实用性。大语言模型的最新进展提供了有前景的替代方案，凭借强大的推理与泛化能力，可通过上下文学习适应新任务，仅需自然语言提示和示例引导即可实现任务适配而无需重新训练。将大语言模型部署于网络边缘而非云端，进一步降低了延迟并保障数据隐私，使其适用于实时、任务关键的公共安全无人机。本文提出将大语言模型辅助的上下文学习与公共安全无人机集成，以解决应急响应中的路径规划和速度控制等关键功能。通过数据收集调度的案例研究，证明该框架相比传统方法能显著降低数据包丢失，同时缓解潜在越狱漏洞风险。最后探讨了大语言模型优化器并展望未来研究方向。该上下文学习框架为公共安全无人机提供自适应、情境感知的决策能力，以轻量高效的解决方案提升无人机在紧急情况下的自主性与响应能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to enhance the autonomy and responsiveness of public safety UAVs for emergency tasks like disaster response, where traditional Deep Reinforcement Learning methods face limitations due to training complexity and simulation-to-reality gaps. The proposed method integrates Large Language Models (LLMs) with In-Context Learning at the network edge, enabling adaptive decision-making for functions such as path planning through natural language prompts without retraining. In a case study on data collection scheduling, the LLM-assisted framework significantly reduces packet loss compared to conventional approaches while also addressing jailbreaking vulnerabilities, demonstrating its potential as a lightweight, efficient solution for real-time UAV operations.</div>
<div class="mono" style="margin-top:8px">本文旨在提升公共安全无人机在灾害响应等紧急任务中的自主性和响应能力，传统深度强化学习方法因训练复杂性和仿真与现实差距而受限。该方法将大型语言模型与网络边缘的上下文学习相结合，通过自然语言提示实现路径规划等功能的适应性决策，无需重新训练。在一个数据收集调度的案例研究中，基于大型语言模型的框架相比传统方法显著降低了数据包丢失，并缓解了越狱漏洞风险，证明了其作为轻量高效解决方案在实时无人机操作中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">cadrille: Multi-modal CAD Reconstruction with Reinforcement Learning</div>
<div class="meta-line">Authors: Maksim Kolodiazhnyi, Denis Tarasov, Dmitrii Zhemchuzhnikov, Alexander Nikulin, Ilya Zisman, Anna Vorontsova, Anton Konushin, Vladislav Kurenkov, Danila Rukhovich</div>
<div class="meta-line">Venue: ICLR 2026 Oral</div>
<div class="meta-line">First: 2025-05-28T22:32:31+00:00 · Latest: 2026-02-17T16:31:55+00:00</div>
<div class="meta-line">Comments: ICLR 2026 (Oral)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22914v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.22914v3">PDF</a> · <a href="https://github.com/col14m/cadrille">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one. Code is avaliable at https://github.com/col14m/cadrille .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>cadrille：基于强化学习的多模态CAD重建</div>
<div class="mono" style="margin-top:8px">计算机辅助设计（CAD）在工程与制造领域具有核心地位，能够创建精确且可编辑的三维模型。利用多种传感器或用户提供的数据作为CAD重建的输入，可降低设计应用的使用门槛。然而，现有方法通常仅关注单一输入模态（如点云、图像或文本），限制了其泛化能力与鲁棒性。借助视觉语言模型（VLM）的最新进展，我们提出一种多模态CAD重建模型，可同时处理全部三种输入模态。受大语言模型（LLM）训练范式启发，我们采用两阶段流程：首先在大规模程序生成数据上进行监督微调（SFT），随后通过程序化获取的在线反馈进行强化学习（RL）微调。此外，我们首次探索将LLM的RL微调应用于CAD任务，证明在线RL算法（如组相对偏好优化GRPO）优于离线方法。在DeepCAD基准测试中，我们的SFT模型在三种输入模态上均优于现有单模态方法。更重要的是，经过RL微调后，cadrille在三个具有挑战性的数据集（包括真实世界数据集）上取得了最先进的性能。代码发布于 https://github.com/col14m/cadrille。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to democratize CAD design through flexible input modalities and to overcome the limitations of single-modal reconstruction methods, this paper introduces cadrille, a multi-modal CAD reconstruction model that processes point clouds, images, and text simultaneously. The method employs a two-stage pipeline: first, supervised fine-tuning on procedurally generated data, followed by reinforcement learning fine-tuning using online feedback, specifically exploring Group Relative Preference Optimization for improved performance. Experimentally, the supervised model outperforms existing single-modal approaches on the DeepCAD benchmark across all three modalities, and after RL fine-tuning, cadrille achieves state-of-the-art results on three challenging datasets, including a real-world one.</div>
<div class="mono" style="margin-top:8px">为通过灵活输入方式普及CAD设计并克服单模态重建方法的局限性，本文提出了cadrille，一种能同时处理点云、图像和文本的多模态CAD重建模型。该方法采用两阶段流程：首先在程序生成的数据上进行监督微调，随后利用在线反馈进行强化学习微调，特别探索了组相对偏好优化以提升性能。实验表明，监督模型在DeepCAD基准测试中所有三种模态上均优于现有单模态方法，且经过强化学习微调后，cadrille在包括真实世界数据在内的三个挑战性数据集上取得了最先进的成果。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis</div>
<div class="meta-line">Authors: Sara Giordano, Kornikar Sen, Miguel A. Martin-Delgado</div>
<div class="meta-line">Venue: Quantum Mach. Intell. 8, 9 (2026)</div>
<div class="meta-line">First: 2025-07-22T14:39:20+00:00 · Latest: 2026-02-17T15:53:12+00:00</div>
<div class="meta-line">Comments: 35 pages, 7 figures, color figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.16641v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.16641v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A reinforcement learning (RL) framework is introduced for the efficient synthesis of quantum circuits that generate specified target quantum states from a fixed initial state, addressing a central challenge in both the Noisy Intermediate-Scale Quantum (NISQ) era and future fault-tolerant quantum computing. The approach utilizes tabular Q-learning, based on action sequences, within a discretized quantum state space, to effectively manage the exponential growth of the space dimension. The framework introduces a hybrid reward mechanism, combining a static, domain-informed reward that guides the agent toward the target state with customizable dynamic penalties that discourage inefficient circuit structures such as gate congestion and redundant state revisits. This is a circuit-aware reward, in contrast to the current trend of works on this topic, which are primarily fidelity-based. By leveraging sparse matrix representations and state-space discretization, the method enables practical navigation of high-dimensional environments while minimizing computational overhead. Benchmarking on graph-state preparation tasks for up to seven qubits, we demonstrate that the algorithm consistently discovers minimal-depth circuits with optimized gate counts. Moreover, extending the framework to a universal gate set still yields low depth circuits, highlighting the algorithm robustness and adaptability. The results confirm that this RL-driven approach, with our completely circuit-aware method, efficiently explores the complex quantum state space and synthesizes near-optimal quantum circuits, providing a resource-efficient foundation for quantum circuit optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合奖励驱动的强化学习在高效量子电路合成中的应用</div>
<div class="mono" style="margin-top:8px">本文提出一种强化学习框架，用于高效合成从固定初始态生成指定目标量子态的量子电路，以应对含噪声中等规模量子时代及未来容错量子计算中的核心挑战。该方法在离散化量子态空间中，采用基于动作序列的表格Q学习，有效应对空间维度的指数增长。框架引入混合奖励机制：结合静态的领域知识奖励（引导智能体趋向目标态）与可定制的动态惩罚（抑制门拥堵、状态重复访问等低效电路结构）。这是一种电路感知的奖励机制，区别于当前该领域主要基于保真度的研究趋势。通过利用稀疏矩阵表示和状态空间离散化，本方法能在最小化计算开销的同时，实现对高维环境的实际导航。在最多七量子比特的图态制备任务上的基准测试表明，该算法能持续发现具有优化门数量的最小深度电路。此外，将框架扩展至通用门集仍能获得低深度电路，凸显了算法的鲁棒性与适应性。结果证实，这种采用完全电路感知方法的强化学习驱动方案，能高效探索复杂量子态空间并合成接近最优的量子电路，为量子电路优化提供了资源高效的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient quantum circuit synthesis in both NISQ and fault-tolerant quantum computing, this paper introduces a reinforcement learning framework using tabular Q-learning within a discretized quantum state space to manage exponential dimensionality growth. The method employs a hybrid reward mechanism combining a static, domain-informed reward to guide toward target states with dynamic penalties to discourage inefficient circuit structures like gate congestion and redundant revisits, distinguishing it from fidelity-based approaches. Experimental results on graph-state preparation for up to seven qubits show the algorithm consistently discovers minimal-depth circuits with optimized gate counts, and extension to a universal gate set maintains low-depth performance, confirming its robustness and resource efficiency for near-optimal circuit synthesis.</div>
<div class="mono" style="margin-top:8px">本文针对嘈杂中型量子（NISQ）时代和未来容错量子计算中量子电路高效合成的核心挑战，提出了一种强化学习框架，采用基于动作序列的表格Q学习，在离散化量子态空间中管理维度指数增长。该方法引入混合奖励机制，结合静态领域知识奖励引导智能体朝向目标态，以及可定制动态惩罚以抑制门拥堵和状态重复访问等低效电路结构，区别于当前主流的保真度驱动方法。在多达七个量子比特的图态制备任务上的实验结果表明，该算法能一致地发现具有优化门数的最小深度电路，且扩展到通用门集仍保持低深度性能，验证了其鲁棒性和适应性，为量子电路优化提供了资源高效的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Optimization for Design Parameters of 3D Image Data Analysis</div>
<div class="meta-line">Authors: David Exler, Joaquin Eduardo Urrutia Gómez, Martin Krüger, Maike Schliephake, John Jbeily, Mario Vitacolonna, Rüdiger Rudolf, Markus Reischl</div>
<div class="meta-line">First: 2026-02-17T15:31:48+00:00 · Latest: 2026-02-17T15:31:48+00:00</div>
<div class="meta-line">Comments: 10 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15660v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15660v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三维图像数据分析设计参数的贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">基于深度学习的分割与分类技术对大规模生物医学影像分析至关重要，尤其对于人工分析不切实际的三维数据。尽管现有方法众多，但在实践中选择合适的模型并调整参数仍是主要瓶颈。为此，我们提出了三维数据分析优化流程，该方法通过两个贝叶斯优化阶段来促进分割与分类的设计与参数化。首先，该流程利用领域适应的合成基准数据集选择分割模型并优化后处理参数。为简明评估分割性能，我们引入了作为目标函数的分割质量度量指标。其次，该流程优化分类器的设计选择，包括编码器与分类头架构、先验知识融合及预训练策略。为减少人工标注工作量，此阶段包含辅助类别标注工作流：从分割结果中提取预测实例并顺序呈现给操作者，无需人工追踪。在四个案例研究中，三维数据分析优化流程高效地为各数据集确定了有效的模型与参数配置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the practical bottleneck of model selection and parameter tuning in deep learning-based 3D biomedical image analysis, this paper introduces the 3D data Analysis Optimization Pipeline. The method employs a two-stage Bayesian Optimization process: first, it selects a segmentation model and optimizes its postprocessing parameters using a domain-adapted benchmark and a novel segmentation quality metric as the objective; second, it optimizes classifier design choices, including architecture and pretraining, aided by an assisted annotation workflow to reduce manual labeling effort. Experimental results from four case studies demonstrate that the pipeline efficiently identifies effective model and parameter configurations for individual 3D datasets.</div>
<div class="mono" style="margin-top:8px">本文针对基于深度学习的3D生物医学图像分析中模型选择和参数调优的实际瓶颈，提出了3D数据分析优化流程。该方法采用两阶段贝叶斯优化：第一阶段使用领域适应的基准数据集和一种新的分割质量度量作为目标函数，选择分割模型并优化其后处理参数；第二阶段优化分类器的设计选择，如编码器和分类头架构，并辅以减少人工标注的辅助注释工作流。在四个案例研究中，该流程有效地为不同3D数据集找到了高效的模型和参数配置。</div>
</details>
</div>
<div class="card">
<div class="title">Latency-aware Human-in-the-Loop Reinforcement Learning for Semantic Communications</div>
<div class="meta-line">Authors: Peizheng Li, Xinyi Lin, Adnan Aijaz</div>
<div class="meta-line">First: 2026-02-17T15:07:41+00:00 · Latest: 2026-02-17T15:07:41+00:00</div>
<div class="meta-line">Comments: 6 pages, 8 figures. This paper has been accepted for publication in IEEE ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15640v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15640v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic communication promises task-aligned transmission but must reconcile semantic fidelity with stringent latency guarantees in immersive and safety-critical services. This paper introduces a time-constrained human-in-the-loop reinforcement learning (TC-HITL-RL) framework that embeds human feedback, semantic utility, and latency control within a semantic-aware Open radio access network (RAN) architecture. We formulate semantic adaptation driven by human feedback as a constrained Markov decision process (CMDP) whose state captures semantic quality, human preferences, queue slack, and channel dynamics, and solve it via a primal--dual proximal policy optimization algorithm with action shielding and latency-aware reward shaping. The resulting policy preserves PPO-level semantic rewards while tightening the variability of both air-interface and near-real-time RAN intelligent controller processing budgets. Simulations over point-to-multipoint links with heterogeneous deadlines show that TC-HITL-RL consistently meets per-user timing constraints, outperforms baseline schedulers in reward, and stabilizes resource consumption, providing a practical blueprint for latency-aware semantic adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向语义通信的时延感知人机协同强化学习</div>
<div class="mono" style="margin-top:8px">语义通信有望实现任务对齐传输，但必须在沉浸式与安全关键服务中兼顾语义保真度与严格时延保障。本文提出一种时延约束人机协同强化学习框架，将人类反馈、语义效用与时延控制嵌入语义感知的开放无线接入网架构。我们将人类反馈驱动的语义适配建模为约束马尔可夫决策过程，其状态涵盖语义质量、用户偏好、队列余量与信道动态，并通过结合动作屏蔽与时延感知奖励塑形的原始-对偶近端策略优化算法求解。所得策略在保持PPO级语义奖励的同时，显著降低了空口及近实时RAN智能控制器处理预算的波动性。基于异构截止时间的点对多点链路仿真表明，该框架能持续满足单用户时延约束，在奖励指标上超越基线调度器，并稳定资源消耗，为时延感知语义适配提供了实用方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to balance semantic fidelity with strict latency requirements in immersive and safety-critical semantic communication services, this paper proposes a time-constrained human-in-the-loop reinforcement learning (TC-HITL-RL) framework integrated into a semantic-aware Open RAN architecture. The method formulates semantic adaptation as a constrained Markov decision process (CMDP) that incorporates human feedback, semantic utility, and latency control, solving it via a primal-dual proximal policy optimization algorithm enhanced with action shielding and latency-aware reward shaping. Experimental simulations on point-to-multipoint links demonstrate that TC-HITL-RL consistently meets per-user timing deadlines, outperforms baseline schedulers in reward metrics, and stabilizes resource consumption, offering a practical approach for latency-aware semantic adaptation.</div>
<div class="mono" style="margin-top:8px">本文的动机是在沉浸式和关键安全语义通信服务中，需协调语义保真度与严格延迟保证之间的矛盾。为此，研究提出了一种时间约束的人机交互强化学习（TC-HITL-RL）框架，并将其嵌入语义感知的开放式无线接入网络（RAN）架构中。该方法将基于人类反馈的语义适配建模为一个约束马尔可夫决策过程（CMDP），其状态涵盖语义质量、人类偏好、队列余量和信道动态，并通过结合动作屏蔽和延迟感知奖励塑形的原始-对偶近端策略优化算法进行求解。在具有异构截止时间的点对多点链路上的仿真结果表明，TC-HITL-RL能持续满足每用户时序约束，在奖励指标上优于基线调度器，并稳定了资源消耗，为延迟感知的语义适配提供了实用方案。</div>
</details>
</div>
<div class="card">
<div class="title">Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL</div>
<div class="meta-line">Authors: Julian Lemmel, Felix Resch, Mónika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu</div>
<div class="meta-line">First: 2026-02-02T15:41:53+00:00 · Latest: 2026-02-17T14:36:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02236v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.02236v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents&#x27; performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于实时循环强化学习的自动驾驶预训练控制器在线微调</div>
<div class="mono" style="margin-top:8px">在现实应用中部署预训练策略存在显著挑战，从根本上限制了基于学习的控制系统的实际适用性。当自主系统遭遇系统动力学变化、传感器漂移或任务目标变更时，固定策略的性能会迅速下降。研究表明，采用实时循环强化学习（RTRRL）——一种具有生物合理性的在线自适应算法——能够有效微调预训练策略，提升自主智能体在驾驶任务中的表现。进一步研究发现，RTRRL可与近期受生物启发的循环网络模型（液阻-液容循环神经网络）产生协同效应。通过在模拟CarRacing环境及配备事件相机的RoboRacer实车循线任务中的实验，验证了该闭环方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to address the performance degradation of fixed pretrained policies in autonomous driving when encountering environmental changes, such as shifts in dynamics or sensors. The method employs Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible online adaptation algorithm, to fine-tune pretrained controllers, and integrates it with a Liquid-Resistance Liquid-Capacitance recurrent network model for enhanced synergy. Main experimental results demonstrate effectiveness in both a simulated CarRacing environment and a real-world line-following task using a RoboRacer car with an event camera, showing improved adaptation and performance.</div>
<div class="mono" style="margin-top:8px">本文旨在解决自动驾驶中预训练策略在遇到环境变化（如系统动态或传感器漂移）时性能下降的问题。方法采用实时循环强化学习（RTRRL），一种生物启发的在线适应算法，对预训练控制器进行微调，并结合液体电阻-液体电容循环网络模型以增强协同效应。主要实验结果在模拟CarRacing环境和真实世界线跟随任务（使用配备事件相机的RoboRacer汽车）中验证了该方法的有效性，展示了其提升适应性和性能的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents</div>
<div class="meta-line">Authors: Davide Paglieri, Bartłomiej Cupiał, Jonathan Cook, Ulyana Piterbarg, Jens Tuyls, Edward Grefenstette, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim Rocktäschel</div>
<div class="meta-line">First: 2025-09-03T18:00:13+00:00 · Latest: 2026-02-17T14:21:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03581v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.03581v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities and highlighting the potential for safer and more collaborative agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习何时规划：为LLM智能体高效分配测试时计算资源</div>
<div class="mono" style="margin-top:8px">通过强化学习训练大语言模型进行推理，能显著提升其问题解决能力。在智能体场景中，现有方法（如ReAct）要求LLM在每次行动前显式规划；但我们证明持续规划会带来高昂计算成本并损害长周期任务性能，而从不规划则会进一步限制性能。为此，我们提出了一个形式化LLM智能体动态规划的概念框架，使其能灵活决定何时分配测试时计算资源进行规划。我们设计了一个简单的两阶段训练流程：（1）在多样化合成数据上进行监督微调，为动态规划建立基础；（2）在长周期环境中通过强化学习优化该能力。Crafter环境实验表明，采用此方法训练的动态规划智能体具有更高样本效率，并能持续完成更复杂目标。此外，这些智能体可有效遵循人工编写的规划方案，超越其独立能力，展现了构建更安全、更具协作性智能体系统的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of always or never planning in LLM agents by introducing a dynamic planning framework that allows agents to decide when to allocate test-time compute for planning, aiming to balance performance and computational cost. The method employs a two-stage training pipeline: first, supervised fine-tuning on synthetic data to prime models for dynamic planning, and second, reinforcement learning to refine this capability in long-horizon environments. Experimental results in the Crafter environment show that dynamic planning agents are more sample-efficient and achieve more complex objectives, and they can be effectively steered by human-written plans to surpass independent capabilities, indicating potential for safer collaborative systems.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLM）智能体中始终规划或从不规划的低效问题，提出了一个动态规划框架，使智能体能够灵活决定何时分配测试时计算资源进行规划，以平衡性能与计算成本。方法采用两阶段训练流程：首先在合成数据上进行监督微调以初始化动态规划能力，然后通过强化学习在长视野环境中优化该能力。在Crafter环境中的实验结果表明，动态规划智能体具有更高的样本效率并能实现更复杂的目标，且可通过人类编写的计划有效引导，超越其独立能力，展现了构建更安全协作智能体系统的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL</div>
<div class="meta-line">Authors: Yihan Wang, Peiyu Liu, Runyu Chen, Wei Xu</div>
<div class="meta-line">First: 2026-02-17T13:24:56+00:00 · Latest: 2026-02-17T13:24:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15564v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15564v1">PDF</a> · <a href="https://github.com/Satissss/SquRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs&#x27; reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越静态流程：面向文本到SQL的动态工作流学习</div>
<div class="mono" style="margin-top:8px">文本到SQL技术近期取得显著进展，但在实际场景中仍难以有效应用。这一差距源于对单一静态工作流的依赖，从根本上限制了其在分布外和长尾场景的可扩展性。我们尝试让系统在推理时自适应构建工作流，而非要求用户通过大量实验选择合适方法。通过理论与实证分析，我们证明最优动态策略始终优于最佳静态工作流，其性能提升主要由候选工作流间的异质性驱动。基于此，我们提出强化学习框架SquRL，以增强大语言模型在自适应工作流构建中的推理能力。我们设计了基于规则的奖励函数，并引入两种高效训练机制：动态参与者掩码以促进广泛探索，以及伪奖励以提高训练效率。在广泛使用的文本到SQL基准测试中，动态工作流构建始终优于最佳静态工作流方法，在复杂查询和分布外查询上提升尤为显著。代码已开源：https://github.com/Satissss/SquRL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static workflows in real-world Text-to-SQL applications, which struggle with out-of-distribution and long-tail queries, this paper proposes a method for adaptive, dynamic workflow construction at inference time. The core approach, SquRL, is a reinforcement learning framework that enhances large language models&#x27; reasoning by using a rule-based reward function and training mechanisms like dynamic actor masking and pseudo rewards to optimize workflow selection. Experimental results on standard benchmarks show that this dynamic policy consistently surpasses the best static workflows, with particularly significant improvements on complex and out-of-distribution queries, demonstrating the advantage of leveraging heterogeneity across candidate methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决文本到SQL任务中静态工作流在现实场景中的局限性，特别是在处理分布外和长尾查询时难以扩展的问题。为此，该方法提出在推理时自适应构建动态工作流，其核心是SquRL——一个强化学习框架，通过基于规则的奖励函数以及动态参与者掩码和伪奖励等训练机制，来增强大语言模型在流程构建中的推理能力。在广泛使用的文本到SQL基准测试上的实验结果表明，这种动态工作流构建方法持续优于最佳的静态工作流，尤其在复杂和分布外查询上提升显著，验证了利用候选方法异构性带来的根本优势。</div>
</details>
</div>
<div class="card">
<div class="title">FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning</div>
<div class="meta-line">Authors: Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang, Rui Mao, Erik Cambria</div>
<div class="meta-line">First: 2026-02-02T05:30:42+00:00 · Latest: 2026-02-17T11:19:20+00:00</div>
<div class="meta-line">Comments: 41 pages, 7 figures, 6 tables. Project page: http://flowsteer.org/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01664v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.01664v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowSteer：基于端到端强化学习的交互式智能体工作流编排框架</div>
<div class="mono" style="margin-top:8px">近年来，各类强大的智能体工作流已被广泛应用于解决人类面临的多样化问题。然而，现有工作流编排仍面临关键挑战，包括高昂的人工成本、对特定算子/大语言模型（LLM）的依赖，以及稀疏的奖励信号。为应对这些挑战，我们提出FlowSteer——一种端到端强化学习框架，以轻量级策略模型作为智能体，配合可执行画布环境，通过多轮交互实现工作流编排的自动化。在此过程中，策略模型分析执行状态并选择编辑动作，画布则执行算子并返回反馈以进行迭代优化。此外，FlowSteer提供即插即用框架，支持多样化算子库与可替换的LLM后端。为有效训练该交互范式，我们提出画布工作流相对策略优化（CWRPO），通过引入带条件释放的多样性约束奖励来稳定学习过程并抑制捷径行为。在十二个数据集上的实验结果表明，FlowSteer在多项任务中显著优于基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for FlowSteer is to overcome the high manual effort, dependency on specific operators or large language models (LLMs), and sparse rewards in existing agentic workflow orchestration. The method introduces an end-to-end reinforcement learning framework where a lightweight policy model interacts with an executable canvas environment, selecting editing actions based on execution states while the canvas runs operators and provides feedback for iterative refinement; it also supports plug-and-play operator libraries and interchangeable LLM backends, trained with a novel Canvas Workflow Relative Policy Optimization (CWRPO) that uses diversity-constrained rewards to stabilize learning and prevent shortcuts. Experimental results on twelve datasets demonstrate that FlowSteer significantly outperforms baseline methods across various tasks.</div>
<div class="mono" style="margin-top:8px">FlowSteer的动机是解决现有智能体工作流编排中手动成本高、依赖特定算子或大语言模型（LLM）以及奖励信号稀疏等关键挑战。该方法提出了一个端到端强化学习框架，其中轻量级策略模型与可执行画布环境进行多轮交互，根据执行状态选择编辑动作，同时画布运行算子并返回反馈以迭代优化；该框架还支持即插即用的算子库和可互换的LLM后端，并通过新颖的画布工作流相对策略优化（CWRPO）进行训练，该优化引入多样性约束奖励以稳定学习并抑制捷径行为。在十二个数据集上的实验结果表明，FlowSteer在各种任务中显著优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">NPG-Muse: Scaling Long Chain-of-Thought Reasoning with NP-Hard Graph Problems</div>
<div class="meta-line">Authors: Yuyao Wang, Bowen Liu, Jianheng Tang, Nuo Chen, Yuhan Li, Qifan Zhang, Chenyi Zi, Chen Zhang, Jia Li</div>
<div class="meta-line">First: 2025-08-28T02:40:27+00:00 · Latest: 2026-02-17T11:02:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20373v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.20373v2">PDF</a> · <a href="https://github.com/littlewyy/NPG-Muse">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are the core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long-CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. The resulting NPG-Muse-series models exhibit substantially enhanced Long CoT reasoning capabilities, achieving consistent gains across mathematics, coding, logical, and graph reasoning benchmarks. NPG-Muse-7B even surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLM post-training. Our implementation is available at https://github.com/littlewyy/NPG-Muse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NPG-Muse：基于NP难图问题扩展长链思维推理能力</div>
<div class="mono" style="margin-top:8px">推理大语言模型（RLLMs）近期在复杂推理任务上取得显著进展，主要得益于其长链思维（Long CoT）能力。然而，开发此类长链思维行为严重依赖高质量数据集的训练后调整，这类数据集通常成本高昂且需人工标注（如数学与代码领域），导致可扩展的替代方案未被充分探索。本研究引入NP难（NPH）图问题作为新型合成训练语料，因其本质要求深度推理、广泛探索与反思性策略，这正是长链思维推理的核心特征。基于此洞见，我们开发了一个两阶段训练后框架：（1）在经拒绝采样的NPH图实例上进行长链思维监督微调（SFT），显著提升推理深度；（2）结合细粒度奖励设计的强化学习（RL），优化推理效率。由此得到的NPG-Muse系列模型展现出显著增强的长链思维推理能力，在数学、编程、逻辑及图推理基准测试中均取得稳定提升。NPG-Muse-7B甚至在NPH图问题的准确率与推理效率上超越QwQ-32B。这些成果确立了NPH图问题作为推动大语言模型训练后长链思维推理的有效可扩展资源。项目实现已开源：https://github.com/littlewyy/NPG-Muse。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high cost and limited scalability of human-curated datasets for developing long chain-of-thought reasoning in large language models. It proposes a method using NP-hard graph problems as a synthetic training corpus, which inherently demands deep reasoning, and introduces a two-stage post-training framework involving supervised fine-tuning on rejection-sampled instances followed by reinforcement learning with fine-grained rewards to enhance reasoning depth and efficiency. Experimental results show that the resulting NPG-Muse models achieve consistent gains across mathematics, coding, logical, and graph reasoning benchmarks, with the 7B variant surpassing a 32B model on NP-hard graph problems in both accuracy and efficiency, demonstrating the effectiveness of this scalable approach.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，为开发大语言模型的长思维链推理能力，依赖人工标注的高质量数据集成本高昂且可扩展性有限。该方法采用NP难图问题作为合成训练语料，因其本质需要深度推理，并提出了一个两阶段的后训练框架：包括基于拒绝采样实例的监督微调，以及采用细粒度奖励设计的强化学习，以提升推理深度和效率。实验结果表明，所得的NPG-Muse模型在数学、编码、逻辑和图推理基准上均取得一致提升，其中7B版本在NP难图问题上准确率和推理效率均超越了一个32B模型，证明了该可扩展方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">POP: Prior-fitted Optimizer Policies</div>
<div class="meta-line">Authors: Jan Kobiolka, Christian Frey, Gresa Shala, Arlind Kadra, Erind Bedalli, Josif Grabocka</div>
<div class="meta-line">First: 2026-02-17T10:27:07+00:00 · Latest: 2026-02-17T10:27:07+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15473v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POP：先验拟合优化器策略</div>
<div class="mono" style="margin-top:8px">优化任务旨在寻找目标函数的极值点。传统基于梯度的优化器对超参数选择极为敏感，在高度非凸场景中，其性能依赖精心调整的学习率、动量和梯度累积策略。为突破这些局限，我们提出POP（先验拟合优化器策略）——一种元学习优化器，能够根据优化轨迹提供的上下文信息预测逐坐标步长。该模型通过从涵盖凸与非凸目标的新型先验分布中采样数百万个合成优化问题进行训练。我们在包含47种不同复杂度优化函数的基准测试中评估POP，在相同计算预算下，其性能持续优于一阶梯度方法、非凸优化方法（如进化策略）、贝叶斯优化及近期元学习竞品。实验表明该方法具备强大的泛化能力，且无需针对特定任务进行调优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the sensitivity of classical gradient-based optimizers to hyperparameter tuning in non-convex optimization by introducing POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes using contextual information from the optimization trajectory. The method is trained on millions of synthetic problems sampled from a prior covering both convex and non-convex objectives. Experimental results on a benchmark of 47 functions show that POP consistently outperforms first-order gradient methods, evolutionary strategies, Bayesian optimization, and a recent meta-learned competitor under matched budget constraints, demonstrating strong generalization without task-specific tuning.</div>
<div class="mono" style="margin-top:8px">本文针对经典梯度优化器在非凸优化中对超参数调整敏感的问题，提出了POP（先验拟合优化器策略），这是一种元学习优化器，能利用优化轨迹中的上下文信息预测坐标步长。该方法通过在涵盖凸与非凸目标的先验分布上采样数百万合成问题进行训练。在包含47个函数的基准测试中，POP在相同预算约束下持续优于一阶梯度方法、进化策略、贝叶斯优化及近期元学习竞争对手，展现了无需任务特定调优的强大泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Reinforcement Learning via Recovery-based Shielding with Gaussian Process Dynamics Models</div>
<div class="meta-line">Authors: Alexander W. Goodall, Francesco Belardinelli</div>
<div class="meta-line">First: 2026-02-12T22:03:35+00:00 · Latest: 2026-02-17T09:48:52+00:00</div>
<div class="meta-line">Comments: Accepted at AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12444v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a powerful framework for optimal decision-making and control but often lacks provable guarantees for safety-critical applications. In this paper, we introduce a novel recovery-based shielding framework that enables safe RL with a provable safety lower bound for unknown and non-linear continuous dynamical systems. The proposed approach integrates a backup policy (shield) with the RL agent, leveraging Gaussian process (GP) based uncertainty quantification to predict potential violations of safety constraints, dynamically recovering to safe trajectories only when necessary. Experience gathered by the &#x27;shielded&#x27; agent is used to construct the GP models, with policy optimization via internal model-based sampling - enabling unrestricted exploration and sample efficient learning, without compromising safety. Empirically our approach demonstrates strong performance and strict safety-compliance on a suite of continuous control environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于高斯过程动力学模型的恢复式屏蔽安全强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是最优决策与控制的强大框架，但在安全关键应用中常缺乏可证明的安全保障。本文提出一种新颖的恢复式屏蔽框架，为未知非线性连续动态系统提供具备可证明安全下界的安全强化学习。该方法将备份策略（屏蔽器）与RL智能体相结合，利用基于高斯过程（GP）的不确定性量化预测潜在安全约束违反，仅在必要时动态恢复至安全轨迹。通过“屏蔽”智能体收集的经验构建GP模型，并采用基于内部模型的采样策略优化——在保障安全的前提下实现无限制探索与样本高效学习。实验表明，本方法在连续控制环境套件中展现出优异性能与严格的安全合规性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for provable safety guarantees in reinforcement learning for safety-critical applications, this paper introduces a recovery-based shielding framework that integrates a backup policy with an RL agent. The method leverages Gaussian process dynamics models to quantify uncertainty and predict safety constraint violations, enabling dynamic recovery to safe trajectories only when necessary, while using experience from the shielded agent to construct GP models and optimize policies via internal model-based sampling. Experimental results on continuous control environments demonstrate strong performance and strict safety compliance, achieving safe exploration and sample-efficient learning without compromising safety.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在安全关键应用中缺乏可证明安全保证的问题，提出了一种基于恢复的屏蔽框架，将备份策略与强化学习智能体结合。该方法利用高斯过程动力学模型量化不确定性并预测安全约束违反，仅在必要时动态恢复到安全轨迹，同时利用屏蔽智能体的经验构建高斯过程模型，并通过内部基于模型的采样优化策略。在连续控制环境中的实验结果表明，该方法具有强大的性能和严格的安全合规性，实现了无安全妥协的安全探索和样本高效学习。</div>
</details>
</div>
<div class="card">
<div class="title">MARS-Sep: Multimodal-Aligned Reinforced Sound Separation</div>
<div class="meta-line">Authors: Zihan Zhang, Xize Cheng, Zhennan Jiang, Dongjie Fu, Jingyuan Chen, Zhou Zhao, Tao Jin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-12T09:05:28+00:00 · Latest: 2026-02-17T09:47:18+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10509v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10509v2">PDF</a> · <a href="https://github.com/mars-sep/MARS-Sep">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://mars-sep.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Universal sound separation faces a fundamental misalignment: models optimized for low-level signal metrics often produce semantically contaminated outputs, failing to suppress perceptually salient interference from acoustically similar sources. We introduce a preference alignment perspective, analogous to aligning LLMs with human intent. To address this, we introduce MARS-Sep, a reinforcement learning framework that reformulates separation as decision making. Instead of simply regressing ground-truth masks, MARS-Sep learns a factorized Beta mask policy that is steered by a preference reward model and optimized by a stable, clipped trust-region surrogate. The reward, derived from a progressively-aligned audio-text-vision encoder, directly incentivizes semantic consistency with query prompts. Extensive experiments on multiple benchmarks demonstrate consistent gains in Text-, Audio-, and Image-Queried separation, with notable improvements in signal metrics and semantic quality. Our code is available at https://github.com/mars-sep/MARS-Sep. Sound separation samples are available at https://mars-sep.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARS-Sep：多模态对齐的强化声音分离方法</div>
<div class="mono" style="margin-top:8px">通用声音分离面临一个根本性错位问题：针对低层信号指标优化的模型常产生语义污染的输出，难以抑制声学相似源产生的感知显著干扰。我们引入一种偏好对齐视角，类似于将大语言模型与人类意图对齐。为此，我们提出MARS-Sep——一个将分离重构为决策过程的强化学习框架。该方法不直接回归真实掩码，而是学习由偏好奖励模型引导、通过稳定裁剪信任域代理优化的因子化Beta掩码策略。奖励源自逐步对齐的音频-文本-视觉编码器，直接激励输出与查询提示的语义一致性。在多个基准测试上的广泛实验表明，该方法在文本查询、音频查询和图像查询的分离任务中均取得稳定提升，信号指标与语义质量均有显著改善。代码发布于https://github.com/mars-sep/MARS-Sep，声音分离样本可访问https://mars-sep.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the misalignment between low-level signal metrics and semantic quality in universal sound separation, where models often fail to suppress perceptually salient interference, this paper introduces MARS-Sep, a reinforcement learning framework that reformulates separation as decision-making. The method employs a factorized Beta mask policy steered by a preference reward model derived from a progressively-aligned audio-text-vision encoder, optimized via a stable, clipped trust-region surrogate to directly incentivize semantic consistency with query prompts. Experimental results on multiple benchmarks show consistent improvements in Text-, Audio-, and Image-Queried separation, with gains in both signal metrics and semantic quality.</div>
<div class="mono" style="margin-top:8px">针对通用声音分离中低层次信号指标与语义质量不匹配的问题，即模型常无法抑制感知上显著的干扰，本文提出了MARS-Sep，这是一个将分离重新定义为决策过程的强化学习框架。该方法采用由逐步对齐的音频-文本-视觉编码器驱动的偏好奖励模型来引导因子化Beta掩码策略，并通过稳定的裁剪信任域代理进行优化，以直接激励与查询提示的语义一致性。在多个基准测试上的实验结果表明，该方法在文本、音频和图像查询分离中均取得了一致性提升，信号指标和语义质量均有显著改善。</div>
</details>
</div>
<div class="card">
<div class="title">Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward</div>
<div class="meta-line">Authors: Renjun Xu, Yang Yan</div>
<div class="meta-line">First: 2026-02-12T21:33:25+00:00 · Latest: 2026-02-17T09:08:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12430v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.12430v3">PDF</a> · <a href="https://github.com/scienceaix/agentskills">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The transition from monolithic language models to modular, skill-equipped agents marks a defining shift in how large language models (LLMs) are deployed in practice. Rather than encoding all procedural knowledge within model weights, agent skills -- composable packages of instructions, code, and resources that agents load on demand -- enable dynamic capability extension without retraining. It is formalized in a paradigm of progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP). This survey provides a comprehensive treatment of the agent skills landscape, as it has rapidly evolved during the last few months. We organize the field along four axes: (i) architectural foundations, examining the SKILL$.$md specification, progressive context loading, and the complementary roles of skills and MCP; (ii) skill acquisition, covering reinforcement learning with skill libraries, autonomous skill discovery (SEAgent), and compositional skill synthesis; (iii) deployment at scale, including the computer-use agent (CUA) stack, GUI grounding advances, and benchmark progress on OSWorld and SWE-bench; and (iv) security, where recent empirical analyses reveal that 26.1% of community-contributed skills contain vulnerabilities, motivating our proposed Skill Trust and Lifecycle Governance Framework -- a four-tier, gate-based permission model that maps skill provenance to graduated deployment capabilities. We identify seven open challenges -- from cross-platform skill portability to capability-based permission models -- and propose a research agenda for realizing trustworthy, self-improving skill ecosystems. Unlike prior surveys that broadly cover LLM agents or tool use, this work focuses specifically on the emerging skill abstraction layer and its implications for the next generation of agentic systems. Project repo: https://github.com/scienceaix/agentskills</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型的智能体技能：架构、获取、安全与未来路径</div>
<div class="mono" style="margin-top:8px">从单体语言模型向模块化、具备技能的智能体转变，标志着大语言模型在实际部署方式上的根本性变革。智能体技能——即智能体按需加载的指令、代码和资源的可组合包——无需重新训练即可实现动态能力扩展，取代了将所有程序性知识编码于模型权重中的传统方式。这一范式通过渐进式上下文加载、可移植技能定义以及与模型上下文协议的集成得以形式化。本综述全面梳理了智能体技能领域在过去数月快速演进的全貌，围绕四个维度展开：（一）架构基础，剖析SKILL$.$md规范、渐进式上下文加载机制，以及技能与MCP的互补作用；（二）技能获取，涵盖基于技能库的强化学习、自主技能发现（SEAgent）与组合式技能合成；（三）规模化部署，包括计算机使用智能体技术栈、图形用户界面基础能力进展，以及在OSWorld和SWE-bench基准测试上的进展；（四）安全领域，近期实证分析显示26.1%的社区贡献技能存在漏洞，据此我们提出技能信任与生命周期治理框架——一个基于四层级权限门控的模型，将技能来源与分级部署能力相映射。我们识别出从跨平台技能可移植性到基于能力的权限模型等七大开放挑战，并提出了实现可信、自我改进的技能生态系统的研究议程。与以往广泛涵盖LLM智能体或工具使用的综述不同，本文聚焦于新兴的技能抽象层及其对下一代智能体系统的影响。项目仓库：https://github.com/scienceaix/agentskills</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the shift from monolithic language models to modular, agentic systems, this survey formalizes the architecture, acquisition, deployment, and security of agent skills—composable packages that extend large language model capabilities without retraining. The method involves analyzing the field along four axes: architectural foundations like the SKILL$.$md specification and Model Context Protocol, skill acquisition through reinforcement learning and autonomous discovery, scalable deployment via stacks such as CUA, and security frameworks addressing vulnerabilities found in 26.1% of community-contributed skills. Key experimental results include benchmark progress on OSWorld and SWE-bench, alongside the proposal of a Skill Trust and Lifecycle Governance Framework to mitigate risks, highlighting open challenges like cross-platform portability and capability-based permissions for future research.</div>
<div class="mono" style="margin-top:8px">本文的动机源于从单体语言模型向模块化智能体系统的转变，旨在系统阐述智能体技能的架构、获取、部署与安全性，这些技能是可组合的包，能动态扩展大语言模型能力而无需重新训练。方法上，该综述从四个维度组织领域：包括基于SKILL$.$md规范和模型上下文协议的架构基础、通过强化学习和自主发现的技能获取、利用计算机使用智能体栈的可扩展部署，以及针对社区贡献技能中26.1%存在漏洞的安全框架。主要实验结果体现在OSWorld和SWE-bench基准上的进展，并提出了技能信任与生命周期治理框架以降低风险，同时指出了跨平台可移植性和基于能力的权限等开放挑战，为未来可信赖技能生态系统的研究指明方向。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action</div>
<div class="meta-line">Authors: Gong Gao, Weidong Zhao, Xianhui Liu, Ning Jia</div>
<div class="meta-line">Venue: Neural Networks,2026</div>
<div class="meta-line">First: 2026-01-27T15:43:02+00:00 · Latest: 2026-02-17T08:40:07+00:00</div>
<div class="meta-line">Comments: 13pages 11figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19720v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19720v2">PDF</a> · <a href="https://github.com/2706853499/IRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing value-based online reinforcement learning (RL) algorithms suffer from slow policy exploitation due to ineffective exploration and delayed policy updates. To address these challenges, we propose an algorithm called Instant Retrospect Action (IRA). Specifically, we propose Q-Representation Discrepancy Evolution (RDE) to facilitate Q-network representation learning, enabling discriminative representations for neighboring state-action pairs. In addition, we adopt an explicit method to policy constraints by enabling Greedy Action Guidance (GAG). This is achieved through backtracking historical actions, which effectively enhances the policy update process. Our proposed method relies on providing the learning algorithm with accurate $k$-nearest-neighbor action value estimates and learning to design a fast-adaptable policy through policy constraints. We further propose the Instant Policy Update (IPU) mechanism, which enhances policy exploitation by systematically increasing the frequency of policy updates. We further discover that the early-stage training conservatism of the IRA method can alleviate the overestimation bias problem in value-based RL. Experimental results show that IRA can significantly improve the learning efficiency and final performance of online RL algorithms on eight MuJoCo continuous control tasks.The code is available at https://github.com/2706853499/IRA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于即时回溯动作的在线强化学习策略利用改进方法</div>
<div class="mono" style="margin-top:8px">现有基于值的在线强化学习算法因探索效率低下和策略更新延迟导致策略利用缓慢。为解决这些问题，我们提出名为即时回溯动作的算法。具体而言，我们提出Q表示差异演化机制以促进Q网络表示学习，使相邻状态-动作对获得判别性表示。此外，我们采用贪婪动作引导的显式策略约束方法，通过回溯历史动作有效增强策略更新过程。该方法通过提供精确的k近邻动作值估计，并学习设计具有快速适应能力的约束策略。我们进一步提出即时策略更新机制，通过系统增加策略更新频率来增强策略利用。实验发现该方法的早期训练保守性能缓解基于值强化学习中的过高估计偏差问题。实验结果表明，在八项MuJoCo连续控制任务中，该方法能显著提升在线强化学习算法的学习效率和最终性能。代码发布于https://github.com/2706853499/IRA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the slow policy exploitation in value-based online reinforcement learning, which stems from ineffective exploration and delayed policy updates. The proposed Instant Retrospect Action (IRA) method introduces Q-Representation Discrepancy Evolution to learn discriminative representations for state-action pairs and employs Greedy Action Guidance via historical action backtracking to impose policy constraints, alongside an Instant Policy Update mechanism to increase policy update frequency. Experiments on eight MuJoCo continuous control tasks demonstrate that IRA significantly enhances learning efficiency and final performance, while its early-stage conservatism also mitigates overestimation bias.</div>
<div class="mono" style="margin-top:8px">该研究针对基于价值的在线强化学习中因探索效率低和政策更新延迟导致的政策利用缓慢问题，提出了即时回溯行动方法。该方法通过Q表示差异演化学习状态-动作对的判别性表示，并利用贪婪行动指导回溯历史动作以施加政策约束，同时采用即时政策更新机制提高政策更新频率。在八项MuJoCo连续控制任务上的实验结果表明，该方法能显著提升学习效率和最终性能，其早期训练保守性还有助于缓解价值估计过高偏差问题。</div>
</details>
</div>
<div class="card">
<div class="title">Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting</div>
<div class="meta-line">Authors: Md Muhtasim Munif Fahim, Soyda Humyra Yesmin, Saiful Islam, Md. Palash Bin Faruque, Md. A. Salam, Md. Mahfuz Uddin, Samiul Islam, Tofayel Ahmed, Md. Binyamin, Md. Rezaul Karim</div>
<div class="meta-line">First: 2026-01-30T19:03:21+00:00 · Latest: 2026-02-17T08:06:08+00:00</div>
<div class="meta-line">Comments: Accepted at the 2026 IEEE 2nd International Conference on Quantum Photonics, Artificial Intelligence &amp; Networking</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00240v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00240v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to &#x27;Green AI&#x27; principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Green-NAS：面向鲁棒高效边缘原生天气预报的全球尺度多目标神经架构搜索</div>
<div class="mono" style="margin-top:8px">本文提出Green-NAS，一种面向低资源环境的多目标神经架构搜索框架，并以天气预报为案例研究。该框架遵循&#x27;绿色AI&#x27;原则，明确最小化计算能耗与碳足迹，优先考虑可持续部署而非单纯计算规模。Green-NAS架构搜索方法同步优化模型精度与效率，通过多目标优化过程寻找参数量极少的高精度轻量化模型。性能最优的Green-NAS-A模型仅使用15.3万参数，在均方根误差上达到0.0988（即与人工调优基线误差仅差1.4%），参数量比GraphCast等全球应用的天气预报模型少239倍。此外，研究还表明：在目标城市历史气象数据有限时，采用迁移学习相比为每个城市单独训练新模型的朴素方法，可将预报精度提升约5.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for sustainable and efficient AI in low-resource edge environments, this paper introduces Green-NAS, a multi-objective neural architecture search framework that prioritizes minimizing computational energy and carbon footprint while maintaining accuracy for weather forecasting. The method employs a multi-objective optimization process to simultaneously search for models that are both accurate and highly parameter-efficient. Key experimental results show that the best model, Green-NAS-A, achieves an RMSE of 0.0988 (within 1.4% of a manual baseline) with only 153k parameters—239 times fewer than models like GraphCast—and that transfer learning can improve forecasting accuracy by approximately 5.2% in data-scarce scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在为资源受限的边缘环境开发可持续、高效的人工智能，提出了Green-NAS框架，这是一个遵循“绿色AI”原则的多目标神经架构搜索方法，专注于在天气预测案例中同时最小化计算能耗与碳足迹。该方法通过多目标优化过程，协同搜索高精度且参数极简的模型架构。主要实验结果表明，最优模型Green-NAS-A仅用15.3万个参数就达到了0.0988的均方根误差（与人工基准模型相差1.4%以内），参数量比GraphCast等全球天气预测模型少239倍；此外，迁移学习的应用在历史数据有限的城市中，可将预测准确率提升约5.2%。</div>
</details>
</div>
<div class="card">
<div class="title">SR-Scientist: Scientific Equation Discovery With Agentic AI</div>
<div class="meta-line">Authors: Shijie Xia, Yuhan Sun, Pengfei Liu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-13T17:35:23+00:00 · Latest: 2026-02-17T07:50:43+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11661v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11661v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Large Language Models (LLMs) have been applied to scientific equation discovery, leveraging their embedded scientific knowledge for hypothesis generation. However, current methods typically confine LLMs to the role of an equation proposer within search algorithms like genetic programming. In this paper, we present SR-Scientist, a framework that elevates the LLM from a simple equation proposer to an autonomous AI scientist that writes code to analyze data, implements the equation as code, submits it for evaluation, and optimizes the equation based on experimental feedback. Specifically, we wrap the code interpreter into a set of tools for data analysis and equation evaluation. The agent is instructed to optimize the equation by utilizing these tools over a long horizon with minimal human-defined pipelines. Empirical results show that SR-Scientist outperforms baseline methods by an absolute margin of 6% to 35% on datasets covering four science disciplines. Additionally, we demonstrate our method&#x27;s robustness to noise, the generalization of the discovered equations to out-of-domain data, and their symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning framework to enhance the agent&#x27;s capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SR-Scientist：基于智能体AI的科学方程发现</div>
<div class="mono" style="margin-top:8px">近期，大语言模型（LLMs）凭借其内嵌的科学知识被应用于科学方程发现，主要用于假设生成。然而，现有方法通常将LLMs局限于遗传编程等搜索算法中的方程提议者角色。本文提出SR-Scientist框架，将LLM从简单的方程提议者提升为自主AI科学家：能够编写代码分析数据、将方程实现为代码、提交评估，并根据实验反馈优化方程。具体而言，我们将代码解释器封装为一套数据分析与方程评估工具集，指导智能体在最小化人工流程干预下，长期利用这些工具优化方程。实验结果表明，在涵盖四个科学领域的数据集上，SR-Scientist以6%至35%的绝对优势超越基线方法。此外，我们验证了该方法对噪声的鲁棒性、发现方程在域外数据的泛化能力及其符号准确性。进一步地，我们开发了端到端强化学习框架以增强智能体能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of current LLMs being confined as mere equation proposers in scientific discovery, this paper introduces SR-Scientist, a framework that elevates an LLM to an autonomous AI scientist capable of writing code for data analysis, implementing equations, and iteratively optimizing them based on experimental feedback using a code interpreter as tools. The method employs an agentic approach with minimal predefined pipelines, allowing long-horizon optimization. Experimental results demonstrate that SR-Scientist outperforms baseline methods by 6% to 35% across four scientific disciplines, showing robustness to noise, generalization to out-of-domain data, and high symbolic accuracy, with an end-to-end reinforcement learning framework further enhancing agent capabilities.</div>
<div class="mono" style="margin-top:8px">针对当前大语言模型在科学方程发现中仅作为方程提议者的局限，本文提出了SR-Scientist框架，将大语言模型提升为自主的AI科学家，能够编写代码进行数据分析、实现方程，并利用代码解释器作为工具，基于实验反馈迭代优化方程。该方法采用智能体驱动的方式，最小化预定义流程，支持长周期优化。实验结果表明，在覆盖四个科学领域的数据集上，SR-Scientist比基线方法绝对提升6%至35%，表现出对噪声的鲁棒性、对域外数据的泛化能力以及高符号准确性，并通过端到端强化学习框架进一步增强了智能体能力。</div>
</details>
</div>
<div class="card">
<div class="title">Fairness over Equality: Correcting Social Incentives in Asymmetric Sequential Social Dilemmas</div>
<div class="meta-line">Authors: Alper Demir, Hüseyin Aydın, Kale-ab Abebe Tessera, David Abel, Stefano V. Albrecht</div>
<div class="meta-line">First: 2026-02-17T07:31:20+00:00 · Latest: 2026-02-17T07:31:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15407v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15407v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sequential Social Dilemmas (SSDs) provide a key framework for studying how cooperation emerges when individual incentives conflict with collective welfare. In Multi-Agent Reinforcement Learning, these problems are often addressed by incorporating intrinsic drives that encourage prosocial or fair behavior. However, most existing methods assume that agents face identical incentives in the dilemma and require continuous access to global information about other agents to assess fairness. In this work, we introduce asymmetric variants of well-known SSD environments and examine how natural differences between agents influence cooperation dynamics. Our findings reveal that existing fairness-based methods struggle to adapt under asymmetric conditions by enforcing raw equality that wrongfully incentivize defection. To address this, we propose three modifications: (i) redefining fairness by accounting for agents&#x27; reward ranges, (ii) introducing an agent-based weighting mechanism to better handle inherent asymmetries, and (iii) localizing social feedback to make the methods effective under partial observability without requiring global information sharing. Experimental results show that in asymmetric scenarios, our method fosters faster emergence of cooperative policies compared to existing approaches, without sacrificing scalability or practicality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>公平优于平等：非对称序贯社会困境中的社会激励修正</div>
<div class="mono" style="margin-top:8px">序贯社会困境为研究个体激励与集体利益冲突时合作如何形成提供了关键框架。在多智能体强化学习中，通常通过引入鼓励亲社会或公平行为的内在驱动力来解决这类问题。然而，现有方法大多假设智能体在困境中面临相同的激励，且需要持续获取其他智能体的全局信息以评估公平性。本研究在经典序贯社会困境环境中引入非对称变体，探究智能体间的自然差异如何影响合作动态。研究发现，现有基于公平的方法在非对称条件下难以适应，因其强制推行原始平等反而错误激励了背叛行为。为此，我们提出三项改进：（1）通过考虑智能体奖励范围重新定义公平性；（2）引入基于智能体的加权机制以更好处理固有不对称性；（3）将社会反馈局部化，使方法在部分可观测条件下无需全局信息共享仍能有效运作。实验结果表明，在非对称场景中，我们的方法相较于现有方案能更快催生合作策略，且不牺牲可扩展性或实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fostering cooperation in asymmetric sequential social dilemmas (SSDs), where existing fairness-based multi-agent reinforcement learning methods often fail because they enforce strict equality without considering natural differences between agents, such as varying reward ranges, and rely on global information. To correct this, the authors propose three key modifications: redefining fairness to account for individual reward ranges, introducing an agent-based weighting mechanism to handle inherent asymmetries, and localizing social feedback to operate under partial observability without global sharing. Experimental results in asymmetric SSD environments demonstrate that their approach enables faster emergence of cooperative policies compared to prior methods, while maintaining scalability and practicality.</div>
<div class="mono" style="margin-top:8px">本文针对非对称序贯社会困境中促进合作的挑战展开研究，指出现有基于公平的多智能体强化学习方法常因强制严格平等而失败，未考虑智能体间的自然差异（如奖励范围不同），且依赖全局信息。为此，作者提出三项关键改进：根据智能体奖励范围重新定义公平性，引入基于智能体的加权机制以处理固有不对称性，以及局部化社会反馈以在部分可观测条件下无需全局信息共享即可运作。在非对称序贯社会困境环境中的实验结果表明，该方法相比现有方法能更快地促生合作策略，同时保持可扩展性和实用性。</div>
</details>
</div>
<div class="card">
<div class="title">General Exploratory Bonus for Optimistic Exploration in RLHF</div>
<div class="meta-line">Authors: Wendi Li, Changdae Oh, Sharon Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-27T04:54:59+00:00 · Latest: 2026-02-17T07:26:18+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03269v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.03269v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $α$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $α$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习人类反馈中乐观探索的通用探索奖励</div>
<div class="mono" style="margin-top:8px">乐观探索对于提升强化学习人类反馈的样本效率至关重要，但现有激励探索的奖励方法往往未能实现乐观原则。理论分析表明，当前基于KL散度或α-散度正则化的方法会无意中将探索偏向参考模型的高概率区域，从而强化保守行为而非促进不确定区域的发现。为解决此缺陷，我们提出通用探索奖励（GEB）这一新颖理论框架，可证明满足乐观原则。GEB通过参考依赖的奖励调节抵消散度诱导的偏差，将现有启发式奖励统一为特例，并自然扩展至整个α-散度族。实证表明，GEB在多种散度设置和大语言模型基座上始终优于基线对齐任务。这些结果证明GEB为RLHF中的乐观探索提供了兼具原则性与实用性的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a critical flaw in optimistic exploration for reinforcement learning with human feedback, where existing exploratory bonus methods, constrained by KL or α-divergence regularization, inadvertently bias exploration toward high-probability regions of a reference model, thus promoting conservative rather than optimistic behavior. To rectify this, the authors propose the General Exploratory Bonus, a theoretical framework that provably adheres to the optimism principle by countering divergence-induced bias through reference-dependent reward regulation, unifying prior heuristic bonuses as special cases and extending across the α-divergence family. Experimental results demonstrate that GEB consistently outperforms baseline methods on alignment tasks across various divergence settings and large language model backbones, establishing it as a principled and practical solution for optimistic exploration in RLHF.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习中的乐观探索问题，指出现有基于KL或α散度正则化的探索奖励方法存在缺陷，会无意中将探索偏向参考模型的高概率区域，从而强化保守行为而非促进对不确定区域的发现。为解决这一问题，作者提出了通用探索奖励这一理论框架，它通过参考依赖的奖励调节来抵消散度引起的偏差，可证明地满足乐观原则，并将先前的启发式奖励方法统一为特例，同时自然扩展至整个α散度族。实验结果表明，在不同散度设置和大语言模型骨干上，GEB在对齐任务中始终优于基线方法，证明了其为RLHF中的乐观探索提供了一个既有理论依据又实用的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control</div>
<div class="meta-line">Authors: Ehsan Futuhi, Shayan Karimi, Chao Gao, Martin Müller</div>
<div class="meta-line">First: 2024-10-07T17:31:52+00:00 · Latest: 2026-02-17T07:23:17+00:00</div>
<div class="meta-line">Comments: We have expanded the related work section with more detailed discussions and enhanced our experiments by incorporating additional data and analysis</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.05225v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.05225v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider deep deterministic policy gradient (DDPG) in the context of reinforcement learning with sparse rewards. To enhance exploration, we introduce a search procedure, \emph{$ε{t}$-greedy}, which generates exploratory options for exploring less-visited states. We prove that search using $εt$-greedy has polynomial sample complexity under mild MDP assumptions. To more efficiently use the information provided by rewarded transitions, we develop a new dual experience replay buffer framework, \emph{GDRB}, and implement \emph{longest n-step returns}. The resulting algorithm, \emph{ETGL-DDPG}, integrates all three techniques: \bm{$εt$}-greedy, \textbf{G}DRB, and \textbf{L}ongest $n$-step, into DDPG. We evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms DDPG, as well as other state-of-the-art methods, across all tested sparse-reward continuous environments. Ablation studies further highlight how each strategy individually enhances the performance of DDPG in this setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ETGL-DDPG：一种面向稀疏奖励连续控制的深度确定性策略梯度算法</div>
<div class="mono" style="margin-top:8px">本文在稀疏奖励强化学习背景下研究深度确定性策略梯度（DDPG）算法。为增强探索能力，我们引入一种搜索方法——εt-贪婪策略，该策略通过生成探索性选项来访问较少到达的状态。我们证明在温和的马尔可夫决策过程假设下，采用εt-贪婪策略的搜索具有多项式样本复杂度。为更高效利用奖励转移提供的信息，我们开发了新型双重经验回放缓冲框架（GDRB），并实现了最长n步回报机制。最终算法ETGL-DDPG将三项技术——εt-贪婪策略、GDRB框架与最长n步回报——集成至DDPG架构。通过在标准基准测试中评估，ETGL-DDPG在所有测试的稀疏奖励连续控制环境中均优于DDPG及其他前沿方法。消融实验进一步揭示了各项策略对DDPG性能的独立增强作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of sparse rewards in continuous control tasks by enhancing the Deep Deterministic Policy Gradient (DDPG) algorithm. The motivation stems from the need to improve exploration and data efficiency in such environments. The method introduces three key techniques: an εt-greedy search procedure to explore less-visited states, a dual experience replay buffer (GDRB) to better utilize rewarded transitions, and longest n-step returns for credit assignment. Experimental results on standard benchmarks show that the integrated algorithm, ETGL-DDPG, outperforms DDPG and other state-of-the-art methods across all tested sparse-reward continuous environments, with ablation studies confirming the individual contributions of each strategy.</div>
<div class="mono" style="margin-top:8px">本文针对连续控制任务中稀疏奖励的挑战，改进了深度确定性策略梯度（DDPG）算法。其动机在于提升此类环境中的探索能力和数据利用效率。方法上引入了三项关键技术：εt-greedy搜索过程以探索较少访问的状态，双重经验回放缓冲区（GDRB）以更有效地利用奖励转换，以及最长n步回报用于信用分配。在标准基准测试中的实验结果表明，整合这些技术的ETGL-DDPG算法在所有测试的稀疏奖励连续环境中均优于DDPG及其他先进方法，消融研究进一步验证了每种策略对性能的单独提升作用。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Deep Reinforcement Learning against Adversarial Behavior Manipulation</div>
<div class="meta-line">Authors: Shojiro Yamabe, Kazuto Fukuchi, Jun Sakuma</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2024-06-06T08:49:51+00:00 · Latest: 2026-02-17T05:50:40+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.03862v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.03862v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study investigates behavior-targeted attacks on reinforcement learning and their countermeasures. Behavior-targeted attacks aim to manipulate the victim&#x27;s behavior as desired by the adversary through adversarial interventions in state observations. Existing behavior-targeted attacks have some limitations, such as requiring white-box access to the victim&#x27;s policy. To address this, we propose a novel attack method using imitation learning from adversarial demonstrations, which works under limited access to the victim&#x27;s policy and is environment-agnostic. In addition, our theoretical analysis proves that the policy&#x27;s sensitivity to state changes impacts defense performance, particularly in the early stages of the trajectory. Based on this insight, we propose time-discounted regularization, which enhances robustness against attacks while maintaining task performance. To the best of our knowledge, this is the first defense strategy specifically designed for behavior-targeted attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对对抗性行为操纵的鲁棒深度强化学习</div>
<div class="mono" style="margin-top:8px">本研究探讨了针对强化学习的行为导向攻击及其防御策略。行为导向攻击旨在通过对状态观测施加对抗性干预，使受害者的行为按攻击者意图被操纵。现有行为导向攻击存在一定局限，例如需白盒访问受害者策略。为此，我们提出一种基于对抗性演示模仿学习的新型攻击方法，该方法在有限访问受害者策略的条件下仍能生效，且与环境无关。此外，理论分析证明策略对状态变化的敏感性会影响防御性能，尤其在轨迹早期阶段。基于此发现，我们提出时间折扣正则化方法，在保持任务性能的同时增强对攻击的鲁棒性。据我们所知，这是首个专门针对行为导向攻击设计的防御策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of deep reinforcement learning to behavior-targeted adversarial attacks, which manipulate an agent&#x27;s actions by perturbing its state observations. To overcome limitations of prior attacks that require full policy access, the authors propose a novel black-box attack method using imitation learning from adversarial demonstrations, which is both policy-agnostic and environment-agnostic. Theoretically analyzing policy sensitivity to state perturbations, they find early trajectory stages are most critical, leading to the development of a time-discounted regularization defense that improves robustness while preserving task performance, representing the first dedicated defense against such targeted attacks.</div>
<div class="mono" style="margin-top:8px">本研究针对深度强化学习易受行为目标对抗攻击的问题展开研究，这类攻击通过扰动状态观测来操纵智能体的行为。为解决现有攻击方法需白盒访问策略等限制，作者提出了一种基于对抗演示模仿学习的黑盒攻击方法，该方法不依赖具体策略或环境。通过理论分析策略对状态扰动的敏感性，他们发现轨迹早期阶段最为关键，据此提出了时间折扣正则化防御方法，在保持任务性能的同时增强了对抗攻击的鲁棒性，这是首个专门针对此类行为目标攻击的防御策略。</div>
</details>
</div>
<div class="card">
<div class="title">CDRL: A Reinforcement Learning Framework Inspired by Cerebellar Circuits and Dendritic Computational Strategies</div>
<div class="meta-line">Authors: Sibo Zhang, Rui Jing, Liangfu Lv, Jian Zhang, Yunliang Zang</div>
<div class="meta-line">First: 2026-02-17T05:25:09+00:00 · Latest: 2026-02-17T05:25:09+00:00</div>
<div class="meta-line">Comments: 14pages, 8 figures, 6 tabels</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15367v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15367v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has achieved notable performance in high-dimensional sequential decision-making tasks, yet remains limited by low sample efficiency, sensitivity to noise, and weak generalization under partial observability. Most existing approaches address these issues primarily through optimization strategies, while the role of architectural priors in shaping representation learning and decision dynamics is less explored. Inspired by structural principles of the cerebellum, we propose a biologically grounded RL architecture that incorporate large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. Experiments on noisy, high-dimensional RL benchmarks show that both the cerebellar architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to conventional designs. Sensitivity analysis of architectural parameters suggests that cerebellum-inspired structures can offer optimized performance for RL with constrained model parameters. Overall, our work underscores the value of cerebellar structural priors as effective inductive biases for RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CDRL：受小脑回路与树突计算策略启发的强化学习框架</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在高维序列决策任务中已取得显著性能，但仍受限于样本效率低、对噪声敏感以及在部分可观测性下泛化能力弱等问题。现有方法主要通过优化策略应对这些挑战，而对架构先验在表征学习与决策动态形成中的作用探索较少。受小脑结构原理启发，我们提出一种基于生物机制的强化学习架构，融合大规模扩展、稀疏连接、稀疏激活及树突层级调制机制。在噪声高维强化学习基准测试中，与传统设计相比，小脑架构与树突调制均能持续提升样本效率、鲁棒性和泛化能力。架构参数的敏感性分析表明，受小脑启发的结构可在有限模型参数下为强化学习提供优化性能。总体而言，本研究凸显了小脑结构先验作为强化学习有效归纳偏置的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of conventional reinforcement learning (RL) in sample efficiency, noise robustness, and generalization under partial observability, this paper introduces a biologically inspired RL framework that incorporates architectural principles from the cerebellum, including large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. The method involves designing an RL architecture grounded in these cerebellar structural priors to enhance representation learning and decision dynamics. Experimental results on noisy, high-dimensional RL benchmarks demonstrate that the proposed cerebellar-inspired architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to standard designs, with sensitivity analysis indicating optimized performance under constrained model parameters.</div>
<div class="mono" style="margin-top:8px">针对传统强化学习在样本效率、噪声鲁棒性和部分可观测性下泛化能力方面的不足，本文受小脑结构原理启发，提出了一种融合大规模扩展、稀疏连接、稀疏激活和树突级调制的生物启发性强化学习框架。该方法基于小脑的结构先验设计强化学习架构，以改进表示学习和决策动态。在噪声高维强化学习基准测试上的实验结果表明，所提出的小脑启发架构和树突调制相比传统设计，能持续提升样本效率、鲁棒性和泛化能力，敏感性分析表明该结构在受限模型参数下可实现优化性能。</div>
</details>
</div>
<div class="card">
<div class="title">MyoInteract: A Framework for Fast Prototyping of Biomechanical HCI Tasks using Reinforcement Learning</div>
<div class="meta-line">Authors: Ankit Bhattarai, Hannah Selder, Florian Fischer, Arthur Fleig, Per Ola Kristensson</div>
<div class="meta-line">First: 2026-02-16T22:51:57+00:00 · Latest: 2026-02-16T22:51:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15245v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15245v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL)-based biomechanical simulations have the potential to revolutionise HCI research and interaction design, but currently lack usability and interpretability. Using the Human Action Cycle as a design lens, we identify key limitations of biomechanical RL frameworks and develop MyoInteract, a novel framework for fast prototyping of biomechanical HCI tasks. MyoInteract allows designers to setup tasks, user models, and training parameters from an easy-to-use GUI within minutes. It trains and evaluates muscle-actuated simulated users within minutes, reducing training times by up to 98%. A workshop study with 12 interaction designers revealed that MyoInteract allowed novices in biomechanical RL to successfully setup, train, and assess goal-directed user movements within a single session. By transforming biomechanical RL from a days-long expert task into an accessible hour-long workflow, this work significantly lowers barriers to entry and accelerates iteration cycles in HCI biomechanics research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MyoInteract：基于强化学习的生物力学人机交互任务快速原型设计框架</div>
<div class="mono" style="margin-top:8px">基于强化学习（RL）的生物力学仿真有望革新人机交互研究与交互设计，但目前存在可用性与可解释性不足的问题。本研究以人类行为循环为设计视角，剖析了现有生物力学RL框架的核心局限，进而开发了MyoInteract——一个面向生物力学人机交互任务的快速原型设计新框架。该框架允许设计者通过易用的图形界面在数分钟内完成任务配置、用户模型构建及训练参数设置，并能在数分钟内完成肌肉驱动仿真用户的训练与评估，将训练时间缩短最高达98%。一项包含12名交互设计师的研讨实验表明，即使不具备生物力学RL背景的新手也能在单次会话中成功配置、训练并评估目标导向的用户动作模型。本工作将生物力学RL从耗时数日的专家级任务转化为可在一小时内完成的标准化流程，显著降低了该领域的研究门槛，并加速了人机交互生物力学研究的迭代周期。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to make reinforcement learning (RL)-based biomechanical simulations more usable and interpretable for HCI research, this paper introduces MyoInteract, a framework for rapidly prototyping biomechanical interaction tasks. The method centers on an easy-to-use GUI that allows designers to quickly configure tasks, user models, and training parameters, enabling the training and evaluation of muscle-actuated simulated users in minutes with up to a 98% reduction in training time. Experimental results from a workshop with 12 interaction designers demonstrated that even novices could successfully set up, train, and assess goal-directed user movements within a single session, transforming a previously expert-driven, days-long process into an accessible hour-long workflow.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决基于强化学习的生物力学模拟在人机交互研究中可用性和可解释性不足的问题，为此提出了MyoInteract框架，用于快速原型化生物力学交互任务。该方法通过一个易于使用的图形界面，使设计者能在几分钟内配置任务、用户模型和训练参数，从而在数分钟内完成肌肉驱动模拟用户的训练与评估，将训练时间减少了高达98%。一项有12位交互设计师参与的研讨会实验结果表明，即使是新手也能在单次会话中成功设置、训练并评估目标导向的用户动作，将原本需要数日的专家级任务转变为可在一小时内完成的流程。</div>
</details>
</div>
<div class="card">
<div class="title">Individualized Federated Learning for Traffic Prediction with Error Driven Aggregation</div>
<div class="meta-line">Authors: Hang Chen, Collin Meese, Mark Nejad, Chien-Chung Shen</div>
<div class="meta-line">First: 2024-07-17T00:42:47+00:00 · Latest: 2026-02-16T21:11:24+00:00</div>
<div class="meta-line">Comments: 30 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.12226v2">Abs</a> · <a href="https://arxiv.org/pdf/2407.12226v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-latency traffic prediction is vital for smart city traffic management. Federated Learning has emerged as a promising technique for Traffic Prediction (FLTP), offering several advantages such as privacy preservation, reduced communication overhead, improved prediction accuracy, and enhanced adaptability to changing traffic conditions. However, majority of the current FLTP frameworks lack a real-time model updating scheme, which hinders their ability to continuously incorporate new incoming traffic data and adapt effectively to the changing dynamics of traffic trends. Another concern with the existing FLTP frameworks is their reliance on the conventional FL model aggregation method, which involves assigning an identical model (i.e., the global model) to all traffic monitoring devices to predict their individual local traffic trends, thereby neglecting the non-IID characteristics of traffic data collected in different locations. Building upon these findings and harnessing insights from reinforcement learning, we propose NeighborFL, an individualized real-time federated learning scheme that introduces a haversine distance-based and error-driven, personalized local models grouping heuristic from the perspective of each individual traffic node. This approach allows NeighborFL to create location-aware and tailored prediction models for each client while fostering collaborative learning. Simulations demonstrate the effectiveness of NeighborFL, offering improved real-time prediction accuracy over three baseline models, with one experimental setting showing a 16.9% reduction in MSE value compared to a naive FL setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于误差驱动聚合的个性化联邦学习交通预测方法</div>
<div class="mono" style="margin-top:8px">低延迟交通预测对智慧城市交通管理至关重要。联邦学习已成为交通预测领域一项前景广阔的技术，具备隐私保护、降低通信开销、提升预测精度及增强对动态交通条件适应能力等优势。然而，现有大多数联邦学习交通预测框架缺乏实时模型更新机制，难以持续整合新产生的交通数据，也无法有效适应交通趋势的动态变化。现有框架的另一局限在于依赖传统联邦学习模型聚合方法，即向所有交通监测设备分配相同全局模型以预测其本地交通趋势，忽视了不同地理位置采集的交通数据具有非独立同分布特性。基于上述发现并融合强化学习思想，本文提出NeighborFL——一种个性化的实时联邦学习方案。该方案从单个交通节点视角出发，引入基于半正矢距离与误差驱动的个性化本地模型分组启发式策略，使NeighborFL能为每个客户端构建具有位置感知能力的定制化预测模型，同时促进协同学习。仿真实验表明，NeighborFL在三种基线模型对比中展现出更优的实时预测精度，其中一组实验设置的均方误差值较基础联邦学习设置降低16.9%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for low-latency traffic prediction in smart cities and identifies limitations in existing Federated Learning for Traffic Prediction (FLTP) frameworks, specifically their lack of real-time model updates and reliance on a one-size-fits-all global model that fails to account for non-IID traffic data across locations. To address this, the authors propose NeighborFL, an individualized federated learning scheme that uses a haversine distance-based and error-driven heuristic to group neighboring traffic nodes, enabling the creation of personalized, location-aware prediction models while maintaining collaborative learning. Experimental simulations show that NeighborFL outperforms three baseline models in real-time prediction accuracy, with one scenario achieving a 16.9% reduction in mean squared error compared to a naive federated learning approach.</div>
<div class="mono" style="margin-top:8px">本文的动机源于智慧城市对低延迟交通预测的需求，并指出现有联邦学习交通预测框架的局限性，特别是缺乏实时模型更新以及依赖适用于所有设备的单一全局模型，忽略了不同位置交通数据的非独立同分布特性。为此，作者提出了NeighborFL，一种个性化的联邦学习方案，它采用基于半正矢距离和误差驱动的启发式方法对相邻交通节点进行分组，从而为每个客户端创建定制化、位置感知的预测模型，同时促进协作学习。实验模拟结果表明，NeighborFL在实时预测准确性上优于三种基线模型，其中一种实验设置相比朴素联邦学习实现了均方误差降低16.9%。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-Based Planning for Improving Science Return of Earth Observation Satellites</div>
<div class="meta-line">Authors: Abigail Breitfeld, Alberto Candela, Juan Delfa, Akseli Kangaslahti, Itai Zilberstein, Steve Chien, David Wettergreen</div>
<div class="meta-line">First: 2025-09-05T13:11:50+00:00 · Latest: 2026-02-16T20:37:05+00:00</div>
<div class="meta-line">Comments: International Symposium on Artificial Intelligence, Robotics and Automation in Space, November 2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.07997v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.07997v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Earth observing satellites are powerful tools for collecting scientific information about our planet, however they have limitations: they cannot easily deviate from their orbital trajectories, their sensors have a limited field of view, and pointing and operating these sensors can take a large amount of the spacecraft&#x27;s resources. It is important for these satellites to optimize the data they collect and include only the most important or informative measurements. Dynamic targeting is an emerging concept in which satellite resources and data from a lookahead instrument are used to intelligently reconfigure and point a primary instrument. Simulation studies have shown that dynamic targeting increases the amount of scientific information gathered versus conventional sampling strategies. In this work, we present two different learning-based approaches to dynamic targeting, using reinforcement and imitation learning, respectively. These learning methods build on a dynamic programming solution to plan a sequence of sampling locations. We evaluate our approaches against existing heuristic methods for dynamic targeting, showing the benefits of using learning for this application. Imitation learning performs on average 10.0\% better than the best heuristic method, while reinforcement learning performs on average 13.7\% better. We also show that both learning methods can be trained effectively with small amounts of data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的规划提升地球观测卫星科学回报</div>
<div class="mono" style="margin-top:8px">地球观测卫星是收集地球科学信息的强大工具，但其存在局限性：难以偏离轨道轨迹、传感器视场有限、传感器指向与操作需消耗大量航天器资源。优化数据采集并仅纳入最重要或信息量最大的测量至关重要。动态瞄准是一种新兴概念，利用卫星资源与前瞻仪器数据智能重构并指向主仪器。仿真研究表明，相比传统采样策略，动态瞄准能提升科学信息获取量。本研究提出两种基于学习的动态瞄准方法，分别采用强化学习与模仿学习。这些学习方法基于动态规划解来规划采样位置序列。通过与现有启发式动态瞄准方法对比评估，展示了学习技术在此应用中的优势：模仿学习平均优于最佳启发式方法10.0%，强化学习平均优于13.7%。同时证明两种学习方法均能通过少量数据有效训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to maximize the scientific return of Earth observation satellites, which are constrained by fixed orbits, limited sensor fields of view, and resource-intensive operations, this paper introduces learning-based planning for dynamic targeting. The method employs reinforcement learning and imitation learning to build upon a dynamic programming foundation, enabling intelligent sequential reconfiguration and pointing of a primary instrument using lookahead data. Experimental results demonstrate that both learning approaches outperform existing heuristic methods, with imitation learning improving performance by 10.0% on average and reinforcement learning by 13.7%, while also showing effective training with small datasets.</div>
<div class="mono" style="margin-top:8px">针对地球观测卫星因轨道固定、传感器视场有限及操作资源密集而难以最大化科学回报的问题，本文提出了基于学习的动态目标规划方法。该方法分别采用强化学习和模仿学习，在动态规划的基础上，利用前瞻仪器数据智能规划主仪器的序列采样位置。实验结果表明，两种学习方法均优于现有启发式方法，其中模仿学习平均性能提升10.0%，强化学习平均提升13.7%，且两者都能通过少量数据有效训练。</div>
</details>
</div>
<div class="card">
<div class="title">Generalized Parallel Scaling with Interdependent Generations</div>
<div class="meta-line">Authors: Harry Dong, David Brandfonbrener, Eryk Helenowski, Yun He, Mrinal Kumar, Han Fang, Yuejie Chi, Karthik Abinav Sankararaman</div>
<div class="meta-line">First: 2025-10-01T17:33:35+00:00 · Latest: 2026-02-16T19:52:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01143v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.01143v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parallel LLM inference scaling involves sampling a set of $N&gt;1$ responses for a single input prompt. However, these $N$ parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 39% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有代际依赖性的广义并行扩展</div>
<div class="mono" style="margin-top:8px">并行大语言模型推理扩展涉及为单个输入提示采样一组$N&gt;1$个响应。然而，这些$N$个并行响应往往彼此独立生成，这既分割了计算资源，又使单次生成中潜在的有用信息无法被其他生成过程利用。这与响应长度扩展形成对比——后者会在所有后续步骤中复用历史计算结果。为获得更高质量的响应及响应集合，我们提出Bridge方法，通过将批量大语言模型隐藏状态重新构想为整体张量而非独立切片，实现具有依赖关系的并行响应生成。仅需引入少量新参数（2.8%-5.1%），Bridge即可将基于可验证奖励的强化学习相对平均准确率提升幅度提高至39%，并增强正确响应的一致性。经单次训练后，Bridge可扩展至任意生成宽度，其性能始终优于独立生成方案，由此解锁了一种更通用的并行扩展模式——该模式能有效利用序列间信息，且兼容所有后生成聚合技术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation in parallel LLM inference where multiple responses for a single prompt are generated independently, failing to leverage useful information across responses. The authors propose Bridge, a method that treats batched hidden states as holistic tensors to create interdependent responses in parallel, adding only 2.8%-5.1% new parameters. Experimental results show that Bridge improves relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 39% and enhances the consistency of correct responses, scaling effectively to any generation width while outperforming independent generation approaches.</div>
<div class="mono" style="margin-top:8px">本文针对并行大语言模型推理中多个响应独立生成、无法利用跨响应有用信息的问题，提出了一种新方法。作者设计了Bridge，通过将批处理隐藏状态视为整体张量来并行生成相互依赖的响应，仅需增加2.8%-5.1%的新参数。实验结果表明，Bridge将基于可验证奖励的强化学习的相对平均准确率提升高达39%，并提高了正确响应的一致性，该方法可扩展到任意生成宽度，性能优于独立生成方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260219_0406.html">20260219_0406</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
