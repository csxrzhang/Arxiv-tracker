<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-07 03:28</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260107_0328</div>
    <div class="row"><div class="card">
<div class="title">Scaling Open-Ended Reasoning to Predict the Future</div>
<div class="meta-line">Authors: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</div>
<div class="meta-line">First: 2025-12-31T18:59:51+00:00 · Latest: 2026-01-05T18:45:47+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25070v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.25070v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展开放式推理以预测未来</div>
<div class="mono" style="margin-top:8px">高风险决策涉及对未来不确定性的推理。本研究训练语言模型对开放式预测问题进行预测。为扩大训练数据规模，我们基于每日新闻报道的全球事件，采用全自动精细筛选方法合成新颖的预测问题。我们在自建数据集OpenForesight上训练Qwen3思维模型。为防止训练和评估期间未来信息泄露，预测系统全程使用离线新闻语料库进行数据生成与检索。通过小型验证集的指导，我们证明了检索机制的优势及改进的强化学习奖励函数。最终预测系统在2025年5月至8月期间进行封闭测试。我们的专用模型OpenForecaster 8B性能媲美规模更大的专有模型，其训练显著提升了预测的准确性、校准度和一致性。研究发现预测训练带来的校准改进可泛化至主流基准测试。我们开源全部模型、代码与数据，以推动语言模型预测研究的广泛普及。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for high-stakes decision-making under uncertainty, this work develops language models for open-ended future prediction. The method involves automatically synthesizing forecasting questions from daily news to create the OpenForesight dataset, training Qwen3 thinking models while preventing data leakage via an offline news corpus, and enhancing performance through retrieval and an improved RL reward function. Experimental results on held-out tests from May to August 2025 show that the specialized OpenForecaster 8B model matches larger proprietary models in accuracy, calibration, and consistency, with calibration gains generalizing to other benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究针对高风险决策中的不确定性推理需求，旨在训练语言模型进行开放式未来预测。方法上，通过从每日新闻中自动合成预测问题构建OpenForesight数据集，使用离线新闻语料防止训练和评估中的信息泄露，并利用检索和改进的强化学习奖励函数优化模型。实验结果表明，在2025年5月至8月的保留测试中，专门的OpenForecaster 8B模型在准确性、校准性和一致性上媲美更大的专有模型，且预测训练带来的校准改进能泛化至其他基准测试。</div>
</details>
</div>
<div class="card">
<div class="title">VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation</div>
<div class="meta-line">Authors: Shikun Sun, Liao Qu, Huichao Zhang, Yiheng Liu, Yangyang Song, Xian Li, Xu Wang, Yi Jiang, Daniel K. Du, Xinglong Wu, Jia Jia</div>
<div class="meta-line">First: 2026-01-05T16:36:40+00:00 · Latest: 2026-01-05T16:36:40+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/ByteVisionLab/NextFlow</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02256v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02256v1">PDF</a> · <a href="https://github.com/ByteVisionLab/NextFlow">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VAR强化学习优化方案：解决视觉自回归生成中的异步策略冲突</div>
<div class="mono" style="margin-top:8px">视觉生成领域主要存在三种范式：自回归模型、扩散模型和视觉自回归模型。与自回归和扩散模型不同，VAR模型在生成步骤中处理异构输入结构，这导致了严重的异步策略冲突。该问题在强化学习场景中尤为突出，造成训练不稳定和次优对齐。为解决此问题，我们提出一种新颖框架，通过显式管理这些冲突来增强组相对策略优化。我们的方法整合了三个协同组件：1）用于引导早期生成的稳定中间奖励；2）实现精确信用分配的动态时间步重加权方案；3）源自奖励反馈学习原理的新型掩码传播算法，该算法能在空间和时间维度隔离优化效应。实验表明，相较于原始GRPO基线，我们的方法在样本质量和目标对齐方面取得显著提升，为VAR模型实现了鲁棒有效的优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the asynchronous policy conflicts that arise in Visual AutoRegressive (VAR) models due to their heterogeneous input structures across generation steps—which cause unstable training and suboptimal alignment in reinforcement learning (RL) settings—this paper introduces an enhanced framework for Group Relative Policy Optimization (GRPO). The method integrates three components: a stabilizing intermediate reward to guide early-stage generation, a dynamic time-step reweighting scheme for precise credit assignment, and a novel mask propagation algorithm derived from Reward Feedback Learning (ReFL) to isolate optimization effects spatially and temporally. Experimental results show significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.</div>
<div class="mono" style="margin-top:8px">本文针对视觉自回归（VAR）模型在生成步骤中因输入结构异构而引发的异步策略冲突问题，该问题在强化学习场景下会导致训练不稳定和对齐效果不佳，因此提出了一种增强的组相对策略优化（GRPO）框架。方法融合了三个协同组件：用于引导早期生成的稳定中间奖励、实现精确信用分配的动态时间步重加权方案，以及基于奖励反馈学习（ReFL）原理设计的新型掩码传播算法，以在空间和时间上隔离优化效应。实验结果表明，该方法在样本质量和目标对齐方面相比原始GRPO基线有显著提升，实现了对VAR模型的稳健有效优化。</div>
</details>
</div>
<div class="card">
<div class="title">Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training</div>
<div class="meta-line">Authors: Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud</div>
<div class="meta-line">First: 2025-07-02T14:29:30+00:00 · Latest: 2026-01-05T16:10:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01752v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.01752v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privacy and security concerns such as susceptibility to data poisoning attacks. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide non-vacuous generalization bounds and strong theoretical guarantees for privacy, robustness to data poisoning attacks, and extraction attacks. In experiments with LLMs, we demonstrate empirically that black-box optimization methods, despite the scalability and computational challenges inherent to black-box approaches, are able to learn, showing how a few iterations of BBoxER improve performance, generalize well on a benchmark of reasoning datasets, and are robust to membership inference attacks. This positions BBoxER as an attractive add-on on top of gradient-based optimization, offering suitability for deployment in restricted or privacy-sensitive environments while also providing non-vacuous generalization guarantees.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需窥探的调优：可证明的泛化边界与鲁棒性大语言模型后训练</div>
<div class="mono" style="margin-top:8px">基于梯度的优化是深度学习的核心方法，通过反向传播实现高效可扩展的训练。然而，训练过程中暴露梯度可能泄露底层数据的敏感信息，引发隐私与安全隐患，例如易受数据投毒攻击。相比之下，黑盒优化方法将模型视为不透明函数，仅依赖函数评估指导优化，在数据访问受限、对抗风险高或需防止过拟合的场景中展现出潜力。本文提出BBoxER——一种用于大语言模型后训练的进化黑盒方法，通过对训练数据的隐式压缩构建信息瓶颈。利用信息流的可追踪性，我们为隐私保护、抗数据投毒攻击和抗提取攻击提供了非平凡的泛化边界与强理论保证。在大语言模型实验中，我们实证表明：尽管黑盒方法存在可扩展性与计算挑战，BBoxER仅需数次迭代即可提升模型性能，在推理数据集基准测试中展现良好泛化能力，并能抵抗成员推断攻击。这使BBoxER成为基于梯度优化的理想补充方案，适用于受限或隐私敏感环境，同时提供非平凡的泛化保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by privacy and security concerns in gradient-based deep learning, which can leak sensitive data and enable attacks, this paper introduces BBoxER, a black-box evolutionary method for post-training large language models (LLMs) that treats the model as an opaque function to create an information bottleneck. The method provides theoretical guarantees for generalization, privacy, and robustness against data poisoning and extraction attacks by analyzing information flow. Experimental results with LLMs show that BBoxER improves performance over a few iterations, generalizes well on reasoning benchmarks, and demonstrates robustness to membership inference attacks, positioning it as a viable add-on for privacy-sensitive deployments.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决基于梯度的深度学习中的隐私和安全问题，例如数据泄露和攻击风险，为此提出了BBoxER，一种用于大语言模型后训练的黑盒进化方法，将模型视为不透明函数以形成信息瓶颈。该方法通过分析信息流，为泛化、隐私以及对抗数据投毒和提取攻击的鲁棒性提供了理论保证。在大型语言模型的实验中，BBoxER经过少量迭代即可提升性能，在推理基准测试中泛化良好，并对成员推理攻击表现出鲁棒性，使其成为隐私敏感部署中一个有吸引力的附加工具。</div>
</details>
</div>
<div class="card">
<div class="title">FaithLens: Detecting and Explaining Faithfulness Hallucination</div>
<div class="meta-line">Authors: Shuzheng Si, Qingyi Wang, Haozhe Zhao, Yuzhuo Bai, Guanqiao Chen, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun</div>
<div class="meta-line">First: 2025-12-23T09:20:32+00:00 · Latest: 2026-01-05T15:43:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20182v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20182v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FaithLens：检测与解释忠实性幻觉</div>
<div class="mono" style="margin-top:8px">识别大型语言模型（LLM）输出是否包含忠实性幻觉对实际应用（如检索增强生成与摘要）至关重要。本文提出FaithLens，一种高效且低成本的忠实性幻觉检测模型，可同时提供二元预测及相应解释以增强可信度。为实现这一目标，我们首先通过先进LLM合成带解释的训练数据，并采用严格的数据过滤策略确保标签准确性、解释质量与数据多样性。随后，基于这些精心构建的训练数据对模型进行冷启动微调，并通过基于规则的强化学习进一步优化，同时奖励预测准确性与解释质量。在12项多样化任务上的实验表明，仅80亿参数的FaithLens在性能上超越了GPT-4.1和o3等先进模型，且能生成高质量解释，在可信度、效率与效能间实现了卓越平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces FaithLens, a model designed to detect and explain faithfulness hallucinations in large language model outputs, motivated by the need for reliability in applications like retrieval-augmented generation and summarization. The method involves synthesizing training data with explanations using advanced LLMs, applying data filtering for quality and diversity, fine-tuning the model, and optimizing it with rule-based reinforcement learning that rewards both prediction accuracy and explanation quality. Experimental results across 12 diverse tasks demonstrate that the 8B-parameter FaithLens outperforms models such as GPT-4.1 and o3 in detection performance while producing high-quality explanations, achieving a balance of trustworthiness, efficiency, and effectiveness.</div>
<div class="mono" style="margin-top:8px">本文提出了FaithLens模型，旨在检测和解释大语言模型输出中的忠实性幻觉，其动机源于检索增强生成和摘要等实际应用中对可靠性的需求。该方法通过使用先进大语言模型合成带解释的训练数据，应用数据过滤策略确保标签正确性、解释质量和数据多样性，对模型进行微调作为冷启动，并采用基于规则的强化学习进行优化，以预测正确性和解释质量为奖励。在12个多样化任务上的实验结果表明，8B参数的FaithLens在检测性能上超越了GPT-4.1和o3等先进模型，并能生成高质量的解释，实现了可信性、效率和有效性的独特平衡。</div>
</details>
</div>
<div class="card">
<div class="title">NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation</div>
<div class="meta-line">Authors: Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song, Yongsheng Dong, Shikun Sun, Xian Li, Xu Wang, Yi Jiang, Hu Ye, Bo Chen, Yiming Gao, Peng Liu, Akide Liu, Zhipeng Yang, Qili Deng, Linjie Xing, Jiyang Liu, Zhao Wang, Yang Zhou, Mingcong Liu, Yi Zhang, Qian He, Xiwei Hu, Zhongqi Qi, Jie Shao, Zhiye Fu, Shuai Wang, Fangmin Chen, Xuezhi Chai, Zhihua Wu, Yitong Wang, Zehuan Yuan, Daniel K. Du, Xinglong Wu</div>
<div class="meta-line">First: 2026-01-05T15:27:04+00:00 · Latest: 2026-01-05T15:27:04+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/ByteVisionLab/NextFlow</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02204v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02204v1">PDF</a> · <a href="https://github.com/ByteVisionLab/NextFlow">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NextFlow：统一序列建模激活多模态理解与生成</div>
<div class="mono" style="margin-top:8px">我们提出NextFlow——一个基于6万亿交错文本-图像离散标记训练的统一仅解码器自回归Transformer。通过在统一自回归架构中利用统一的视觉表示，NextFlow原生激活了多模态理解与生成能力，解锁了图像编辑、交错内容生成和视频生成等功能。受模态本质差异的启发（文本严格遵循序列性，图像则具有内在层次性），我们保留文本的下一标记预测机制，但对视觉生成采用下一尺度预测方法。这一设计突破了传统光栅扫描方法的局限，仅需5秒即可生成1024x1024图像，比同类自回归模型快数个数量级。我们通过稳健的训练方案解决了多尺度生成的不稳定性问题，并引入用于强化学习的前缀调优策略。实验表明，NextFlow在统一模型中实现了最先进的性能，其视觉质量可与专业扩散基线模型相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to unify sequential modeling for multimodal tasks, NextFlow introduces a decoder-only autoregressive transformer trained on 6 trillion interleaved text-image tokens. The method employs a unified architecture that retains next-token prediction for text but adopts next-scale prediction for visual generation, diverging from raster-scan approaches to handle the hierarchical nature of images, and incorporates a robust training recipe to stabilize multi-scale generation. Experimental results show that NextFlow generates 1024x1024 images in just 5 seconds, significantly faster than comparable autoregressive models, achieves state-of-the-art performance among unified models, and rivals specialized diffusion baselines in visual quality.</div>
<div class="mono" style="margin-top:8px">受统一多模态任务序列建模需求的驱动，NextFlow提出了一种基于6万亿交错文本-图像标记训练的解码器自回归Transformer。该方法采用统一架构，对文本保留下一标记预测，但对视觉生成采用下一尺度预测，以处理图像的层次性，区别于传统光栅扫描方法，并通过稳健的训练方案稳定多尺度生成。实验结果表明，NextFlow能在5秒内生成1024x1024图像，比同类自回归模型快数个数量级，在统一模型中达到最先进性能，并在视觉质量上媲美专业扩散基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models</div>
<div class="meta-line">Authors: Shouang Wei, Min Zhang, Xin Lin, Bo Jiang, Kun Kuang, Zhongxiang Dai</div>
<div class="meta-line">First: 2025-11-12T01:27:02+00:00 · Latest: 2026-01-05T15:26:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08873v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08873v2">PDF</a> · <a href="https://github.com/Mind-Lab-ECNU/UCO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students&#x27; evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students&#x27; cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students&#x27; cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student&#x27;s Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UCO：一种基于大语言模型的自适应教学多轮交互式强化学习方法</div>
<div class="mono" style="margin-top:8px">在教育场景中，大语言模型正从答案提供者转变为智能导师，但当前监督微调方法仅习得表层教学模式，缺乏动态适应能力。现有强化学习方法虽试图突破这一局限，却面临两大关键挑战：其一，仅依据学生能否输出正确答案评估教学效果，无法区分学生是真正理解还是在交互中复述教师提供的答案；其二，无法通过交互式对话实时感知学生动态演化的认知状态，因而难以动态调整教学策略以匹配学生认知水平。为此，我们提出单向认知优化方法以应对这些挑战。该方法采用多轮交互式强化学习范式，其创新在于设计了两项协同奖励函数：进展奖励捕捉学生的认知提升，评估学生是否真正从困惑转向理解；支架奖励则动态识别每位学生的最近发展区，激励教师在该区域内维持高效教学。通过在BigMath和MathTutorBench基准上与11个基线模型对比实验，结果表明UCO模型在同等规模模型中表现最优，且性能达到先进闭源模型水平。代码与数据已开源：https://github.com/Mind-Lab-ECNU/UCO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the shift of large language models (LLMs) from answer providers to intelligent tutors, this paper addresses the limitations of current methods that lack dynamic adaptation and genuine student understanding assessment. The proposed Unidirectional Cognitive Optimization (UCO) method employs a multi-turn interactive reinforcement learning paradigm with two synergistic reward functions: a Progress Reward to capture cognitive advancement from confusion to comprehension, and a Scaffold Reward to dynamically identify each student&#x27;s Zone of Proximal Development (ZPD) for tailored teaching. Experimental results on BigMath and MathTutorBench benchmarks show that UCO outperforms 11 baseline models of equivalent scale and achieves performance comparable to advanced closed-source models.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大型语言模型从答案提供者向智能导师的转变，旨在解决现有方法缺乏动态适应能力和对学生真实理解评估的局限。提出的单向认知优化方法采用多轮交互式强化学习范式，包含两个协同奖励函数：进展奖励用于捕捉学生从困惑到理解的认知进步，支架奖励则动态识别每个学生的最近发展区以实现针对性教学。在BigMath和MathTutorBench基准上的实验结果表明，该方法优于11个同等规模的基线模型，并达到了与先进闭源模型相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents</div>
<div class="meta-line">Authors: Keyu Wang, Bingchen Miao, Wendong Bu, Yu Wu, Juncheng Li, Shengyu Zhang, Wenqiao Zhang, Siliang Tang, Jun Xiao, Yueting Zhuang</div>
<div class="meta-line">First: 2026-01-05T15:24:05+00:00 · Latest: 2026-01-05T15:24:05+00:00</div>
<div class="meta-line">Comments: 19 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02201v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CORE：基于代码的逆向自训练框架与图扩展技术用于虚拟智能体</div>
<div class="mono" style="margin-top:8px">多模态大语言模型的融合推动了多模态虚拟智能体的显著发展，但主流训练范式面临关键挑战：行为克隆通过模仿实现简单有效，但行为多样性不足；强化学习能通过探索发现新策略，却严重依赖人工设计的奖励函数。为解决这两种方法的矛盾，我们提出CORE——一种基于代码的逆向自训练框架与图扩展技术，它桥接了模仿与探索，提供了一种既能提升行为多样性、又无需人工设计奖励的新型训练框架。具体而言，我们引入语义代码抽象技术，可从专家示范中自动推断奖励函数而无需人工设计。这种被称为标签函数的可执行代码能验证任务中的关键步骤。在此基础上，我们提出策略图扩展技术以增强领域内行为多样性，通过构建多路径策略图来捕捉超越专家示范的多样化有效解决方案。此外，我们引入轨迹引导外推技术，利用成功与失败轨迹扩展任务空间，从而丰富跨领域行为多样性。在Web和Android平台上的实验表明，CORE显著提升了整体性能与泛化能力，凸显其作为构建强大虚拟智能体的鲁棒可泛化训练范式的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces CORE, a framework designed to reconcile the limitations of Behavior Cloning and Reinforcement Learning in training multimodal virtual agents, where the former lacks behavioral diversity and the latter depends on manual reward design. The method employs Semantic Code Abstraction to automatically derive executable reward functions from expert demonstrations, Strategy Graph Expansion to capture diverse in-domain solutions via a multi-path graph, and Trajectory-Guided Extrapolation to enhance out-of-domain diversity using both successful and failed trajectories. Experimental results on Web and Android platforms show that CORE substantially boosts overall performance and generalization, establishing it as a robust training paradigm for virtual agents.</div>
<div class="mono" style="margin-top:8px">本文提出了CORE框架，旨在解决训练多模态虚拟代理时行为克隆方法行为多样性不足与强化学习方法依赖手动设计奖励函数之间的冲突。该方法通过语义代码抽象从专家演示中自动推导可执行的奖励函数，利用策略图扩展构建多路径图以捕捉领域内多样化的有效解决方案，并采用轨迹引导外推结合成功与失败轨迹来增强领域外行为多样性。在Web和Android平台上的实验结果表明，CORE显著提升了整体性能和泛化能力，凸显了其作为构建强大虚拟代理的鲁棒且可泛化训练范式的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense</div>
<div class="meta-line">Authors: Yu Li, Sizhe Tang, Rongqian Chen, Fei Xu Yu, Guangyu Jiang, Mahdi Imani, Nathaniel D. Bastian, Tian Lan</div>
<div class="meta-line">First: 2026-01-05T15:18:54+00:00 · Latest: 2026-01-05T15:18:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02196v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ACDZero：基于图嵌入的树搜索技术，实现自动化网络防御的掌控</div>
<div class="mono" style="margin-top:8px">自动化网络防御（ACD）旨在以最少或无需人工干预的方式保护计算机网络，通过采取隔离主机、重置服务、部署诱饵或更新访问控制等纠正措施应对入侵。然而，现有ACD方法（如深度强化学习）在决策/状态空间庞大的复杂网络中常面临探索困难，需耗费大量样本。受样本高效防御策略需求的启发，我们将CAGE挑战赛4中的ACD建模为基于情境的部分可观测马尔可夫决策问题，并提出一种以蒙特卡洛树搜索为核心的防御策略。该方法显式建模ACD中的探索-利用权衡，利用统计抽样指导探索与决策。我们创新性地运用图神经网络将网络观测嵌入为属性图，实现对主机及其关系的置换不变推理。为在复杂搜索空间中实现实用化，我们通过学习的图嵌入和图编辑动作先验引导MCTS，将无模型泛化、策略蒸馏与前向规划相结合。在涉及多样化网络结构和对抗行为的CC4场景中评估表明，相较于前沿强化学习基线，这种基于图嵌入的搜索引导规划能提升防御收益与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for sample-efficient automated cyber defense (ACD) in complex networks where deep reinforcement learning struggles with exploration, this paper frames ACD as a context-based partially observable Markov decision problem and proposes a planning-centric defense policy using Monte Carlo Tree Search (MCTS). The method innovatively employs graph neural networks (GNNs) to embed network observations as attributed graphs for permutation-invariant reasoning, guiding MCTS with learned graph embeddings and priors over graph-edit actions to combine generalization with look-ahead planning. Experimental evaluation on CAGE Challenge 4 scenarios shows that this graph-embedding-based tree search improves defense reward and robustness compared to state-of-the-art RL baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决复杂网络中自动化网络防御（ACD）的样本效率问题，其中深度强化学习面临探索困难。方法上将ACD建模为基于上下文的部分可观测马尔可夫决策问题，并提出一种以规划为中心的防御策略，采用蒙特卡洛树搜索（MCTS）。该方法创新性地使用图神经网络（GNN）将网络观测嵌入为属性图以实现置换不变推理，并通过学习的图嵌入和图编辑动作先验来引导MCTS，从而结合泛化与前瞻规划。在CAGE Challenge 4场景中的实验结果表明，这种基于图嵌入的树搜索相较于最先进的强化学习基线，提高了防御奖励和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting</div>
<div class="meta-line">Authors: Muxi Diao, Lele Yang, Wuxuan Gong, Yutong Zhang, Zhonghao Yan, Yufei Han, Kongming Liang, Weiran Xu, Zhanyu Ma</div>
<div class="meta-line">First: 2026-01-05T14:28:17+00:00 · Latest: 2026-01-05T14:28:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02151v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model&#x27;s internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as &quot;Confident Conflicts&quot; tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>熵自适应微调：解决置信冲突以缓解遗忘</div>
<div class="mono" style="margin-top:8px">监督微调（SFT）是领域适应的标准范式，但常导致灾难性遗忘。与之形成鲜明对比的是，策略强化学习（RL）能有效保留通用能力。我们探究这一差异，发现根本性的分布差距：RL与模型内部信念对齐，而SFT强制模型拟合外部监督。这种不匹配常表现为“置信冲突”标记，其特征是低概率但低熵。在此类情况下，模型对其自身预测高度自信，却被强制学习相悖的真实标注，从而触发破坏性梯度更新。为此，我们提出熵自适应微调（EAFT）。与仅依赖预测概率的方法不同，EAFT利用标记级熵作为门控机制，以区分认知不确定性与知识冲突。这使得模型能从不确定样本中学习，同时抑制冲突数据的梯度。在数学、医疗和智能体领域对Qwen与GLM系列（参数量4B至32B）的广泛实验验证了我们的假设。EAFT在保持标准SFT下游性能的同时，显著缓解了通用能力的退化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that Supervised Fine-Tuning (SFT) often causes catastrophic forgetting, whereas on-policy Reinforcement Learning (RL) preserves general capabilities, a discrepancy attributed to a distributional gap where SFT forces the model to fit external supervision against its internal beliefs, leading to &quot;Confident Conflicts&quot;—tokens with low probability but low entropy that trigger destructive updates. To address this, the method proposes Entropy-Adaptive Fine-Tuning (EAFT), which uses token-level entropy as a gating mechanism to differentiate epistemic uncertainty from knowledge conflict, enabling learning from uncertain samples while suppressing gradients on conflicting data. Experimental results on Qwen and GLM models (4B to 32B parameters) across mathematical, medical, and agentic domains show that EAFT matches standard SFT&#x27;s downstream performance while significantly reducing degradation of general capabilities.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到监督微调（SFT）常导致灾难性遗忘，而基于策略的强化学习（RL）能保持通用能力，这种差异源于分布差距：SFT迫使模型拟合外部监督，与其内部信念冲突，产生“自信冲突”——即概率低但熵低的标记，引发破坏性更新。为解决此问题，方法提出熵自适应微调（EAFT），利用标记级熵作为门控机制，区分认知不确定性与知识冲突，从而从不确定样本中学习，同时抑制冲突数据的梯度。在Qwen和GLM系列模型（4B至32B参数）上，于数学、医疗和智能体领域的实验结果表明，EAFT在匹配标准SFT下游性能的同时，显著减轻了通用能力的退化。</div>
</details>
</div>
<div class="card">
<div class="title">Gibbs randomness-compression proposition: An efficient deep learning</div>
<div class="meta-line">Authors: M. Süzen</div>
<div class="meta-line">First: 2025-05-29T10:48:35+00:00 · Latest: 2026-01-05T14:07:39+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures, 1 table, 1 algorithm, 1 theorem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23869v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.23869v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A proposition that connects randomness and compression is put forward via Gibbs entropy over set of measurement vectors associated with a compression process. The proposition states that a lossy compression process is equivalent to {\it directed randomness} that preserves information content. The proposition originated from the observed behavior in newly proposed {\it Dual Tomographic Compression} (DTC) compress-train framework. This is akin to tomographic reconstruction of layer weight matrices via building compressed sensed projections, via so-called {\it weight rays}. This tomographic approach is applied to previous and next layers in a dual fashion, that triggers neuronal-level pruning. This novel model compress-train scheme appears in iterative fashion and acts as a smart neural architecture search: also called {\it compression aware training}. The experiments demonstrated the utility of this dual-tomography during training: method accelerates and supports lottery ticket hypothesis. However, random compress-train iterations having similar performance demonstrated the connection between randomness and compression from statistical physics perspective, we formulated the so-called {\it Gibbs randomness-compression proposition}, signifying randomness-compression relationship via Gibbs entropy. The proposition is supported with the experimental evidence, resulting in very high correlation between learning performance vs. Gibbs entropy over compression ratios. Practically, the DTC framework provides a promising approach for massively energy- and resource-efficient deep learning training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>吉布斯随机性-压缩命题：一种高效深度学习框架</div>
<div class="mono" style="margin-top:8px">本研究通过压缩过程相关测量向量集合的吉布斯熵，提出了连接随机性与压缩的命题。该命题指出：有损压缩过程等价于保持信息内容的定向随机性。该命题源于新提出的双层断层扫描压缩训练框架中观察到的现象，该框架通过构建压缩感知投影（即权重射线）对层权重矩阵进行断层扫描重建。这种断层扫描方法以前后层双向方式实施，触发神经元级剪枝。这种新颖的压缩训练模型以迭代方式运行，可视为智能神经架构搜索（亦称压缩感知训练）。实验证明双断层扫描在训练中的效用：该方法加速并支持彩票假设。随机压缩训练迭代具有相似性能的现象，从统计物理视角揭示了随机性与压缩的关联，由此我们提出吉布斯随机性-压缩命题，通过吉布斯熵表征随机性-压缩关系。实验证据支持该命题，显示学习性能与压缩比吉布斯熵间存在高度相关性。实践表明，DTC框架为大规模节能高效的深度学习训练提供了可行方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for energy- and resource-efficient deep learning, this paper introduces the Gibbs randomness-compression proposition, which links lossy compression to directed randomness that preserves information, inspired by observations from a novel Dual Tomographic Compression (DTC) framework. The method employs a compress-train scheme using tomographic reconstruction of layer weights via compressed sensing projections, termed weight rays, applied dually to adjacent layers to enable neuronal pruning and act as a compression-aware neural architecture search. Experimental results show that DTC accelerates training, supports the lottery ticket hypothesis, and demonstrates a high correlation between learning performance and Gibbs entropy across compression ratios, validating the proposition&#x27;s practical utility for efficient training.</div>
<div class="mono" style="margin-top:8px">本文旨在实现高能效和资源高效的深度学习，提出了吉布斯随机性-压缩命题，该命题通过吉布斯熵将有损压缩与保持信息内容的定向随机性联系起来，灵感来源于新型双断层压缩（DTC）框架的观察。该方法采用压缩-训练方案，通过称为权重射线的压缩感知投影对层权重进行断层重建，并以双重方式应用于相邻层，以实现神经元剪枝并作为压缩感知的神经架构搜索。实验结果表明，DTC加速了训练，支持彩票假设，并显示学习性能与压缩比上的吉布斯熵之间存在高度相关性，从而验证了该命题在实际高效训练中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</div>
<div class="meta-line">Authors: Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Zhenbo Xu, Lechen Ning, Huijia Wu, Zhaofeng He</div>
<div class="meta-line">First: 2025-12-29T07:39:49+00:00 · Latest: 2026-01-05T13:39:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23260v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23260v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety alignment -- training large language models (LLMs) to refuse harmful requests while remaining helpful -- is critical for responsible deployment. Prior work established that safety behaviors are governed by low-rank structures, suggesting parameter-efficient fine-tuning (PEFT) should be well-suited for alignment. However, Low-Rank Adaptation (LoRA) consistently underperforms full fine-tuning and reinforcement learning on safety benchmarks. We attribute this gap to semantic entanglement: safety-relevant directions are intertwined with unrelated concepts due to polysemanticity, impeding implicit subspace identification. To address this, we propose SAILS (Safety Alignment via Interpretable Low-rank Subspace), which leverages Sparse Autoencoders (SAEs) to disentangle representations into monosemantic features, constructs an interpretable safety subspace from SAE decoder directions, and uses it to initialize LoRA adapters. Theoretically, we prove that SAE-based identification achieves arbitrarily small recovery error under monosemanticity assumptions, while direct identification suffers an irreducible error floor. Empirically, SAILS achieves up to 99.6% safety rate on Gemma-2-9B -- exceeding full fine-tuning by 7.4 points and matching RLHF-based models -- while updating only 0.19% of parameters and providing interpretability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于稀疏自编码器构建低秩子空间适配的可解释安全对齐方法</div>
<div class="mono" style="margin-top:8px">安全对齐——训练大语言模型在拒绝有害请求的同时保持有用性——对负责任部署至关重要。先前研究证实安全行为受低秩结构调控，表明参数高效微调应适用于对齐任务。然而，低秩适配在安全基准测试中持续弱于全参数微调和强化学习。我们将此差距归因于语义纠缠：由于多义性，安全相关方向与无关概念相互交织，阻碍了隐式子空间识别。为此，我们提出SAILS（基于可解释低秩子空间的安全对齐），利用稀疏自编码器将表征解耦为单义特征，从SAE解码器方向构建可解释安全子空间，并用于初始化LoRA适配器。理论上，我们证明在单义性假设下基于SAE的识别可实现任意小的恢复误差，而直接识别存在不可约的误差下限。实证中，SAILS在Gemma-2-9B模型上实现99.6%的安全率——超越全参数微调7.4个百分点，媲美基于RLHF的模型——仅更新0.19%参数且具备可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve the safety alignment of large language models (LLMs) through a more interpretable and parameter-efficient method, as existing low-rank adaptation techniques like LoRA underperform due to semantic entanglement where safety concepts are mixed with unrelated features. The proposed method, SAILS, addresses this by using Sparse Autoencoders (SAEs) to disentangle model representations into monosemantic features, constructing an interpretable safety subspace from these features, and initializing LoRA adapters with this subspace to enable precise safety tuning. Experimental results show that SAILS achieves a safety rate of up to 99.6% on Gemma-2-9B, surpassing full fine-tuning by 7.4 percentage points and matching reinforcement learning-based models, while updating only 0.19% of parameters and offering enhanced interpretability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过一种更可解释且参数高效的方法来改进大语言模型（LLM）的安全对齐，因为现有的低秩适配技术（如LoRA）由于安全概念与无关特征语义纠缠而表现不佳。所提出的SAILS方法通过使用稀疏自编码器（SAE）将模型表示解耦为单语义特征，基于这些特征构建可解释的安全子空间，并利用该子空间初始化LoRA适配器，从而实现精确的安全调优。实验结果表明，SAILS在Gemma-2-9B模型上实现了高达99.6%的安全率，比全参数微调高出7.4个百分点，与基于强化学习的模型性能相当，同时仅更新0.19%的参数并提供了更好的可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents</div>
<div class="meta-line">Authors: Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Jiyan He, Yunfang Wu</div>
<div class="meta-line">First: 2025-12-24T05:27:53+00:00 · Latest: 2026-01-05T13:23:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20957v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.20957v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一器足矣：面向仓库级大语言模型智能体的强化学习</div>
<div class="mono" style="margin-top:8px">在大型开源软件仓库中定位需要修改的文件和函数具有挑战性，因其规模庞大且结构复杂。现有基于大语言模型的方法通常将其视为仓库级检索任务，并依赖多种辅助工具，这忽视了代码执行逻辑且使模型控制复杂化。我们提出RepoNavigator——一种配备单一执行感知工具（跳转至被调用符号定义）的大语言模型智能体。该统一设计反映了代码执行的实际流程，同时简化了工具操作。RepoNavigator通过强化学习从预训练模型直接端到端训练，无需任何闭源蒸馏。实验表明，经强化学习训练的RepoNavigator实现了最先进的性能：7B模型超越14B基线，14B模型优于32B竞品，32B模型甚至超过Claude-3.7等闭源模型。这些结果证实，将单一结构基础工具与强化学习训练相结合，为仓库级问题定位提供了高效可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of locating relevant code in large open-source repositories, where existing LLM-based methods rely on multiple auxiliary tools, complicating control and overlooking execution logic. The authors propose RepoNavigator, an LLM agent that uses a single tool—jumping to the definition of invoked symbols—to mirror actual code execution flow and simplify tool use. It is trained end-to-end with reinforcement learning from a pretrained model, without closed-source distillation. Experimental results show state-of-the-art performance: the 7B model outperforms 14B baselines, the 14B model surpasses 32B competitors, and the 32B model exceeds closed-source models like Claude-3.7, demonstrating that a single, execution-aware tool combined with RL offers an efficient, scalable solution for repository-level issue localization.</div>
<div class="mono" style="margin-top:8px">本文针对在大型开源代码库中定位相关文件的挑战，现有基于大语言模型的方法依赖多种辅助工具，导致控制复杂且忽略代码执行逻辑。作者提出RepoNavigator，这是一个配备单一工具（跳转到被调用符号的定义）的大语言模型智能体，以反映实际代码执行流程并简化工具操作。该模型通过强化学习对预训练模型进行端到端训练，无需闭源蒸馏。实验结果表明其达到最先进性能：7B模型优于14B基线，14B模型超越32B竞争对手，32B模型甚至超过Claude-3.7等闭源模型，证实了结合单一结构感知工具与强化学习训练能为仓库级问题定位提供高效、可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics</div>
<div class="meta-line">Authors: Zhuofan Shi, Hubao A, Yufei Shao, Mengyan Dai, Yadong Yu, Pan Xiang, Dongliang Huang, Hongxu An, Chunxiao Xin, Haiyang Shen, Zhenyu Wang, Yunshan Na, Gang Huang, Xiang Jing</div>
<div class="meta-line">First: 2026-01-05T12:56:51+00:00 · Latest: 2026-01-05T12:56:51+00:00</div>
<div class="meta-line">Comments: 24 pages,4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02075v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02075v1">PDF</a> · <a href="https://github.com/FredericVAN/PKU_MDAgent2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&amp;A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MDAgent2：面向分子动力学代码生成与知识问答的大语言模型</div>
<div class="mono" style="margin-top:8px">分子动力学模拟在材料科学原子尺度行为研究中至关重要，但编写LAMMPS脚本仍属高度专业化且耗时的任务。尽管大语言模型在代码生成和领域问答中展现出潜力，其在分子动力学场景的性能受限于领域数据稀缺、前沿大模型部署成本高昂及代码可执行率低等问题。基于前期MDAgent工作，我们提出首个能在分子动力学领域同时执行知识问答与代码生成的端到端框架MDAgent2。通过构建领域专用数据生成流程，产出涵盖分子动力学知识、问答与代码生成的三类高质量数据集。基于这些数据集，采用三阶段后训练策略——持续预训练、监督微调与强化学习——训练出两个领域适配模型MD-Instruct与MD-Code。进一步提出MD-GRPO强化学习方法，以模拟结果为奖励信号并循环利用低奖励轨迹实现持续优化。同时构建可部署的多智能体系统MDAgent2-RUNTIME，集成代码生成、执行、评估与自修正功能。结合本文提出的首个LAMMPS代码生成与问答基准MD-EvalBench，我们的模型与系统在多项指标上超越多个强基线。本工作系统论证了大语言模型在工业仿真任务中的适应性与泛化能力，为AI for Science及工业级仿真的自动化代码生成奠定了方法论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for MDAgent2 stems from the specialized and time-consuming nature of writing LAMMPS scripts for molecular dynamics simulations, coupled with the limitations of existing large language models in this domain due to scarce data, high deployment costs, and low code executability. The method involves constructing three high-quality domain-specific datasets and employing a three-stage post-training strategy—continued pre-training, supervised fine-tuning, and reinforcement learning—to train two adapted models, MD-Instruct and MD-Code, alongside introducing MD-GRPO, a closed-loop reinforcement learning method that uses simulation outcomes as rewards. The main experimental results, evaluated on the proposed MD-EvalBench benchmark, show that the models and the integrated multi-agent system, MDAgent2-RUNTIME, achieve performance surpassing several strong baselines in both knowledge question answering and code generation, demonstrating the adaptability of large language models for industrial simulation tasks.</div>
<div class="mono" style="margin-top:8px">MDAgent2的研发动机源于分子动力学模拟中编写LAMMPS脚本的专业性和耗时性，以及现有大语言模型在该领域因数据稀缺、部署成本高和代码可执行性低而受限。方法上，研究构建了三个高质量的领域特定数据集，并采用三阶段后训练策略——持续预训练、监督微调和强化学习——来训练两个适配模型MD-Instruct和MD-Code，同时引入了MD-GRPO，这是一种利用模拟结果作为奖励信号的闭环强化学习方法。主要实验结果在提出的MD-EvalBench基准测试中显示，这些模型及集成的多智能体系统MDAgent2-RUNTIME在知识问答和代码生成任务上的性能超越了多个强基线，证明了大语言模型在工业模拟任务中的适应性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error</div>
<div class="meta-line">Authors: Chenming Tang, Hsiu-Yuan Huang, Weijie Liu, Saiyong Yang, Yunfang Wu</div>
<div class="meta-line">First: 2025-10-30T03:36:19+00:00 · Latest: 2026-01-05T12:47:18+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26109v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26109v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has significantly boosted the reasoning capability of language models (LMs) recently. However, existing RLVR approaches merely train LMs based on their own generated on-policy responses and are constrained by the initial capability of LMs, thus prone to exploration stagnation, in which LMs fail to solve more training problems and cannot further learn from the training data. Some work tries to address this by leveraging off-policy solutions to training problems, but relies on external expert guidance that is limited in availability and scalability. In this work, we propose LTE (Learning to reason from Trial and Error), an approach that hints LMs with their previously self-made mistakes, not requiring any external expert guidance. Experiments validate the effectiveness of LTE, which outperforms the normal group relative policy optimization (GRPO) by 5.02 in Pass@1 and 9.96 in Pass@k on average across six mathematical reasoning benchmarks for Qwen3-8B-Base and even performs better than methods that require external gold solutions as guidance after aligning the experimental setup. Further analysis confirms that LTE successfully mitigates exploration stagnation and enhances both exploitation and exploration during training. Our code is available at https://anonymous.4open.science/r/Learning-from-Trial-and-Error.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不可两次踏入同一条河流：从试错中学习推理</div>
<div class="mono" style="margin-top:8px">近期，基于可验证奖励的强化学习（RLVR）显著提升了语言模型（LM）的推理能力。然而，现有RLVR方法仅基于LM自身生成的同策略响应进行训练，受限于LM的初始能力，容易陷入探索停滞，即LM无法解决更多训练问题，也无法从训练数据中进一步学习。部分研究尝试通过利用训练问题的异策略解决方案来应对此问题，但依赖于外部专家指导，其可用性和可扩展性有限。本文提出LTE（从试错中学习推理），该方法通过提示LM其先前自身所犯错误来引导学习，无需任何外部专家指导。实验验证了LTE的有效性：在六个数学推理基准测试中，Qwen3-8B-Base模型上LTE在Pass@1和Pass@k指标上平均分别优于常规组相对策略优化（GRPO）5.02和9.96分；在实验设置对齐后，其表现甚至优于需要外部黄金解作为指导的方法。进一步分析证实，LTE成功缓解了探索停滞，并增强了训练过程中的利用与探索能力。代码发布于https://anonymous.4open.science/r/Learning-from-Trial-and-Error。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of exploration stagnation in reinforcement learning with verifiable rewards (RLVR) for language models, where models are limited by their own on-policy responses and initial capabilities, hindering further learning. The authors propose LTE (Learning to reason from Trial and Error), a method that enables models to learn from their own past mistakes without relying on external expert guidance. Experimental results across six mathematical reasoning benchmarks show that LTE outperforms standard group relative policy optimization (GRPO) by significant margins in Pass@1 and Pass@k metrics for the Qwen3-8B-Base model, even surpassing methods that use external gold solutions, while analysis confirms it mitigates stagnation and enhances both exploitation and exploration during training.</div>
<div class="mono" style="margin-top:8px">本文针对语言模型在可验证奖励强化学习中的探索停滞问题，即模型受限于自身策略内响应和初始能力而无法进一步学习，提出了一种名为LTE（从试错中学习推理）的方法，该方法通过提示模型关注自身先前错误进行学习，无需外部专家指导。在六个数学推理基准测试中，实验结果表明LTE在Qwen3-8B-Base模型上的Pass@1和Pass@k指标均显著优于标准组相对策略优化方法，甚至在实验设置对齐后超越了依赖外部黄金解的方法，进一步分析证实LTE有效缓解了探索停滞，并增强了训练过程中的利用与探索能力。</div>
</details>
</div>
<div class="card">
<div class="title">SIP-BMM: Constructing the Capability--Efficiency Pareto Set for LLMs via Structural Importance Prior Bayesian Model Merging</div>
<div class="meta-line">Authors: Kesheng Chen, Yamin Hu, Zhenqian Zhu, Wenjian Luo, Yiya Diao</div>
<div class="meta-line">First: 2025-12-10T15:32:56+00:00 · Latest: 2026-01-05T12:45:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09972v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.09972v3">PDF</a> · <a href="https://github.com/MiLab-HITSZ/2026-SIPBMM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a Pareto set is pivotal for navigating the capability--efficiency trade-offs in Large Language Models (LLMs). However, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the curse of dimensionality, rendering the search space computationally intractable. To resolve this dichotomy, we propose Structural Importance Prior Bayesian Model Merging (SIP-BMM), a framework that automatically constructs the LLM Pareto set. SIP-BMM renders high-dimensional layer-wise search tractable by introducing an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy. By leveraging a structural importance prior derived from task-vector differences, our method guides SAASBO to automatically identify critical layers, thereby dramatically reducing the effective dimensionality without sacrificing the granularity of full-model control. The entire process is automated within an evolutionary loop driven by the Log-Noisy Expected Hypervolume Improvement ($q$NEHVI) acquisition function. Experiments demonstrate that SIP-BMM discovers a stronger and denser Pareto front than competitive baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/MiLab-HITSZ/2026-SIPBMM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIP-BMM：基于结构重要性先验的贝叶斯模型融合构建大语言模型能力-效率帕累托前沿</div>
<div class="mono" style="margin-top:8px">构建帕累托前沿对于权衡大语言模型（LLM）的能力与效率至关重要，但现有模型融合方法仍存在不足。粗粒度的模型级方法仅能生成稀疏的次优解集，而细粒度的分层方法则受维度灾难困扰，导致搜索空间在计算上不可行。为解决这一矛盾，本文提出结构重要性先验贝叶斯模型融合（SIP-BMM）框架，可自动构建LLM的帕累托前沿。SIP-BMM通过引入重要性感知的稀疏轴对齐子空间贝叶斯优化（SAASBO）策略，使高维分层搜索变得可行。该方法利用从任务向量差异中推导的结构重要性先验，引导SAASBO自动识别关键层，从而在保持全模型控制细粒度的同时显著降低有效维度。整个流程在由对数噪声期望超体积改进（$q$NEHVI）采集函数驱动的进化循环中自动完成。实验表明，SIP-BMM相比基线方法能发现更强且更密集的帕累托前沿，支持根据多样化运行约束进行敏捷模型选择。代码已开源：https://github.com/MiLab-HITSZ/2026-SIPBMM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the inadequacy of existing model merging techniques in constructing a dense Pareto set that effectively navigates the trade-off between capability and efficiency in Large Language Models (LLMs). The proposed method, Structural Importance Prior Bayesian Model Merging (SIP-BMM), introduces an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy, which uses a structural importance prior derived from task-vector differences to automatically identify and focus on critical layers, thereby making high-dimensional layer-wise search tractable. Main experimental results show that SIP-BMM discovers a stronger and denser Pareto front compared to competitive baselines, enabling agile model selection under diverse operational constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有模型融合技术在构建大语言模型能力与效率权衡的密集帕累托前沿方面的不足。所提出的方法——基于结构重要性先验的贝叶斯模型融合（SIP-BMM），引入了一种基于重要性感知的稀疏轴对齐子空间贝叶斯优化策略，该策略利用从任务向量差异中推导出的结构重要性先验，自动识别并聚焦于关键层，从而使高维度的逐层搜索变得可行。主要实验结果表明，与竞争基线相比，SIP-BMM能够发现更强、更密集的帕累托前沿，从而支持根据不同操作约束进行敏捷的模型选择。</div>
</details>
</div>
<div class="card">
<div class="title">Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management</div>
<div class="meta-line">Authors: Faizan Ahmed, Aniket Dixit, James Brusey</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2026-01-05T12:35:33+00:00 · Latest: 2026-01-05T12:35:33+00:00</div>
<div class="meta-line">Comments: 6 pages, accepted at NeurIPS workshop 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02061v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02061v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习中的高阶动作正则化：从连续控制到建筑能源管理</div>
<div class="mono" style="margin-top:8px">深度强化学习智能体常表现出不稳定、高频的控制行为，因能耗过高与机械磨损而阻碍实际部署。我们通过高阶导数惩罚系统研究动作平滑正则化，从连续控制基准的理论理解推进至建筑能源管理的实践验证。在四个连续控制环境中的综合评估表明，三阶导数惩罚（加加速度最小化）能持续实现更优平滑性，同时保持竞争力性能。我们将这些发现拓展至暖通空调控制系统，其中平滑策略使设备切换减少60%，转化为显著运营效益。本研究确立了高阶动作正则化作为强化学习优化与能源关键应用中运行约束间的有效桥梁。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the erratic, high-frequency control behaviors common in deep reinforcement learning agents, which cause excessive energy use and mechanical wear, hindering real-world deployment. The method involves systematically applying higher-order action derivative penalties, particularly focusing on third-order penalties (jerk minimization), to regularize and smooth the agent&#x27;s control policies. The main experimental results, validated across four continuous control benchmarks and a building HVAC system, show that this approach consistently achieves superior smoothness while maintaining competitive task performance, and in the HVAC application, it reduces equipment switching by 60%, yielding significant operational benefits.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决深度强化学习智能体常见的、阻碍实际部署的、导致过度能耗和机械磨损的、不稳定且高频的控制行为。其方法是通过系统性地施加高阶动作导数惩罚，特别是三阶惩罚（最小化急动度），来正则化和平滑智能体的控制策略。主要实验结果在四个连续控制基准和一个建筑暖通空调系统中得到验证，表明该方法在保持竞争力的任务性能的同时，始终实现更优的平滑性，并在暖通空调应用中，将设备切换次数减少了60%，带来了显著的操作效益。</div>
</details>
</div>
<div class="card">
<div class="title">GDRO: Group-level Reward Post-training Suitable for Diffusion Models</div>
<div class="meta-line">Authors: Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, Hengshuang Zhao</div>
<div class="meta-line">First: 2026-01-05T11:47:18+00:00 · Latest: 2026-01-05T11:47:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02036v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02036v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDRO：适用于扩散模型的群体级奖励后训练方法</div>
<div class="mono" style="margin-top:8px">近期研究将大语言模型的在线强化学习应用于文本到图像的整流流扩散模型，以实现奖励对齐。群体级奖励的使用成功使模型与目标奖励对齐，但面临效率低下、依赖随机采样器及奖励破解等挑战。问题在于整流流模型与大语言模型存在本质差异：1) 效率方面，在线图像采样耗时显著且主导训练时间；2) 随机性方面，整流流在初始噪声固定后具有确定性。针对这些问题并受大语言模型群体级奖励启发，我们设计了群体级直接奖励优化（GDRO）。GDRO是一种结合整流流模型特性的新型群体级奖励对齐后训练范式。通过严格理论分析，我们指出GDRO支持完全离线训练，节省了图像采样的大量时间成本，且独立于扩散采样器，无需通过ODE到SDE近似获取随机性。我们通过实证研究揭示了可能误导评估的奖励破解陷阱，并在评估中引入修正分数，同时考虑原始评估奖励与奖励破解趋势。大量实验表明，GDRO通过OCR和GenEval任务的群体离线优化，有效且高效地提升了扩散模型的奖励分数，同时在缓解奖励破解方面展现出强稳定性和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the inefficiency, stochastic sampler dependency, and reward hacking issues that arise when applying online reinforcement learning from large language models to reward alignment in text-to-image rectified flow diffusion models. The method introduces Group-level Direct Reward Optimization (GDRO), a novel post-training paradigm designed specifically for rectified flow models, which theoretically enables fully offline training to eliminate costly online image sampling and is independent of specific diffusion samplers, removing the need for stochastic approximations. The main experimental results demonstrate that GDRO effectively and efficiently enhances reward scores on tasks like OCR and GenEval through group-wise offline optimization, while showing improved stability and robustness in mitigating reward hacking, as validated by a corrected evaluation metric that accounts for hacking trends.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决将大型语言模型的在线强化学习应用于文本到图像整流流扩散模型的奖励对齐时，存在的效率低下、依赖随机采样器和奖励黑客攻击等问题。方法上提出了组级直接奖励优化（GDRO），这是一种专为整流流模型设计的新型后训练范式，理论上支持完全离线训练以节省图像采样时间，且不依赖特定扩散采样器，无需随机近似。主要实验结果表明，GDRO通过组级离线优化，在OCR和GenEval等任务上有效且高效地提升了扩散模型的奖励分数，同时在使用考虑黑客趋势的校正评估指标验证下，展现出缓解奖励黑客攻击的强稳定性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Improved Runtime Guarantees for the SPEA2 Multi-Objective Optimizer</div>
<div class="meta-line">Authors: Benjamin Doerr, Martin S. Krejca, Milan Stanković</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-11-10T14:43:02+00:00 · Latest: 2026-01-05T10:10:07+00:00</div>
<div class="meta-line">Comments: Accepted for AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07150v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.07150v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Together with the NSGA-II, the SPEA2 is one of the most widely used domination-based multi-objective evolutionary algorithms. For both algorithms, the known runtime guarantees are linear in the population size; for the NSGA-II, matching lower bounds exist. With a careful study of the more complex selection mechanism of the SPEA2, we show that it has very different population dynamics. From these, we prove runtime guarantees for the OneMinMax, LeadingOnesTrailingZeros, and OneJumpZeroJump benchmarks that depend less on the population size. For example, we show that the SPEA2 with parent population size $μ\ge n - 2k + 3$ and offspring population size $λ$ computes the Pareto front of the OneJumpZeroJump benchmark with gap size $k$ in an expected number of $O( (λ+μ)n + n^{k+1})$ function evaluations. This shows that the best runtime guarantee of $O(n^{k+1})$ is not only achieved for $μ= Θ(n)$ and $λ= O(n)$ but for arbitrary $μ, λ= O(n^k)$. Thus, choosing suitable parameters -- a key challenge in using heuristic algorithms -- is much easier for the SPEA2 than the NSGA-II.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPEA2多目标优化器改进的运行时保证</div>
<div class="mono" style="margin-top:8px">SPEA2与NSGA-II同为最广泛使用的基于支配关系的多目标进化算法。对于这两种算法，已知的运行时保证与种群规模呈线性关系；NSGA-II存在匹配的下界。通过对SPEA2更复杂的选择机制进行细致研究，我们发现其种群动态存在显著差异。基于此，我们证明了SPEA2在OneMinMax、LeadingOnesTrailingZeros和OneJumpZeroJump基准测试上的运行时保证对种群规模的依赖性更低。例如，我们证明当父代种群规模$μ\ge n - 2k + 3$、子代种群规模为$λ$时，SPEA2在期望$O( (λ+μ)n + n^{k+1})$次函数评估内可计算间隙大小为$k$的OneJumpZeroJump基准的帕累托前沿。这表明最佳运行时保证$O(n^{k+1})$不仅能在$μ= Θ(n)$和$λ= O(n)$时实现，对于任意$μ, λ= O(n^k)$同样成立。因此，对于SPEA2而言，选择合适参数（启发式算法应用中的关键挑战）比NSGA-II更为容易。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to provide improved theoretical runtime guarantees for the SPEA2, a prominent multi-objective evolutionary algorithm, motivated by the fact that existing guarantees for SPEA2 and the similar NSGA-II are linearly dependent on population size, with matching lower bounds known for NSGA-II. The method involves a careful analysis of SPEA2&#x27;s more complex selection mechanism to reveal its distinct population dynamics. The main experimental results are theoretical runtime bounds for benchmark problems like OneMinMax and OneJumpZeroJump; for example, on OneJumpZeroJump with gap size k, SPEA2 with certain population parameters achieves an expected runtime of O((λ+μ)n + n^{k+1}), demonstrating that the optimal O(n^{k+1}) guarantee holds for a much wider range of population sizes than for NSGA-II, making parameter selection easier.</div>
<div class="mono" style="margin-top:8px">本文旨在为著名的多目标进化算法SPEA2提供改进的理论运行时间保证，其动机在于现有对SPEA2及类似算法NSGA-II的运行时间保证均与种群大小线性相关，且NSGA-II已有匹配的下界。研究方法是对SPEA2更复杂的选择机制进行细致分析，以揭示其独特的种群动态。主要实验结果是针对OneMinMax和OneJumpZeroJump等基准问题的理论运行时间界限；例如，在间隙大小为k的OneJumpZeroJump问题上，SPEA2在特定种群参数下可实现O((λ+μ)n + n^{k+1})的期望运行时间，这表明其最优的O(n^{k+1})保证比NSGA-II适用于更广泛的种群大小范围，从而简化了参数选择过程。</div>
</details>
</div>
<div class="card">
<div class="title">Distorted Distributional Policy Evaluation for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Ryo Iwaki, Takayuki Osogami</div>
<div class="meta-line">First: 2026-01-05T09:04:10+00:00 · Latest: 2026-01-05T09:04:10+00:00</div>
<div class="meta-line">Comments: The preprint version of the paper accepted to ICONIP2025. The Version of Record is available online at https://link.springer.com/chapter/10.1007/978-981-95-4091-4_35</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01917v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线强化学习中基于扭曲分布的策略评估方法</div>
<div class="mono" style="margin-top:8px">尽管分布强化学习方法在在线场景中表现出色，但在离线场景中的成功仍有限。我们假设现有离线分布强化学习方法的一个关键局限在于其统一低估回报分位数的处理方式。这种统一的悲观性可能导致过于保守的价值估计，最终阻碍泛化能力和性能。为解决这一问题，我们引入了一种称为分位数扭曲的新概念，通过根据支持数据的可用性调整保守程度，实现非均匀的悲观性。该方法基于理论分析并经过实证验证，显示出相较于统一悲观性的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of distributional reinforcement learning (DRL) in offline settings, where existing methods often apply uniform pessimism by underestimating all return quantiles, leading to overly conservative value estimates that hinder performance. The authors propose a novel method called quantile distortion, which introduces non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data in the dataset. Experimental results demonstrate that this approach, grounded in theoretical analysis, improves performance over methods relying on uniform pessimism.</div>
<div class="mono" style="margin-top:8px">本文针对分布强化学习在离线场景中的局限性展开研究，指出现有方法通常对所有回报分位数进行均匀悲观估计，导致价值估计过于保守，从而影响性能。作者提出了一种称为分位数扭曲的新方法，通过根据数据集中支持数据的可用性调整保守程度，实现了非均匀的悲观估计。实验结果表明，这种基于理论分析的方法在性能上优于依赖均匀悲观估计的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning</div>
<div class="meta-line">Authors: Yuxuan Li, Harshith Reddy Kethireddy, Srijita Das</div>
<div class="meta-line">First: 2026-01-05T08:49:30+00:00 · Latest: 2026-01-05T08:49:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01904v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01904v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.
  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method&#x27;s learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.
  We also find language model&#x27;s noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估基于偏好的强化学习中特征相关噪声</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PbRL）因其适用于奖励函数难以获取的复杂任务而受到关注。然而，若非来自完美示教者，偏好数据常伴随不确定性和噪声。现有研究多集中于噪声检测，但噪声类型有限且多为与观测无关的均匀分布。本研究形式化了目标特征相关噪声的概念，提出了轨迹特征噪声、轨迹相似性噪声、不确定性感知噪声及语言模型噪声等变体。我们在DMControl和Meta-world的复杂连续控制任务中评估了与特定特征相关的噪声。实验表明，在某些特征相关噪声场景下，当前最先进的抗噪PbRL方法性能显著下降，而无显式去噪的PbRL方法在多数场景中反而表现更优。我们还发现语言模型产生的噪声具有类似特征相关噪声的特性，可模拟真实人类行为，因此需进一步研究对特征相关噪声的鲁棒学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of noisy preferences in Preference-based Reinforcement Learning (PbRL), where existing noise-robust methods often assume uniform noise independent of observations, which does not reflect realistic scenarios. The authors formalize the concept of targeted feature-dependent noise, introducing variants such as trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise, and evaluate these in complex continuous control tasks from DMControl and Meta-world. Experimental results reveal that in many feature-dependent noise settings, state-of-the-art noise-robust PbRL methods suffer significant performance degradation, while basic PbRL methods without explicit denoising surprisingly outperform them, highlighting the need for more robust approaches to handle such noise, especially as language model noise mimics realistic human-like patterns.</div>
<div class="mono" style="margin-top:8px">本文针对基于偏好的强化学习（PbRL）中偏好噪声的挑战展开研究，指出现有抗噪声方法通常假设噪声均匀且与观测无关，这与现实场景不符。作者形式化了目标特征依赖噪声的概念，提出了轨迹特征噪声、轨迹相似性噪声、不确定性感知噪声和语言模型噪声等变体，并在DMControl和Meta-world的复杂连续控制任务中进行了评估。实验结果表明，在许多特征依赖噪声设置下，最先进的抗噪声PbRL方法性能显著下降，而未经显式去噪的基本PbRL方法反而表现更优，这凸显了需要开发更鲁棒的方法来处理此类噪声，特别是语言模型噪声模拟了类似人类的真实模式，值得进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">Convergence of a L2 regularized Policy Gradient Algorithm for the Multi Armed Bandit</div>
<div class="meta-line">Authors: Stefana Anita, Gabriel Turinici</div>
<div class="meta-line">Venue: In: Antonacopoulos, A., et al. (eds) Pattern Recognition. ICPR 2024. Lecture Notes in Computer Science, vol 15326. Springer, Cham (2025)</div>
<div class="meta-line">First: 2024-02-09T13:10:04+00:00 · Latest: 2026-01-05T08:44:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.06388v4">Abs</a> · <a href="https://arxiv.org/pdf/2402.06388v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although Multi Armed Bandit (MAB) on one hand and the policy gradient approach on the other hand are among the most used frameworks of Reinforcement Learning, the theoretical properties of the policy gradient algorithm used for MAB have not been given enough attention. We investigate in this work the convergence of such a procedure for the situation when a $L2$ regularization term is present jointly with the &#x27;softmax&#x27; parametrization. We prove convergence under appropriate technical hypotheses and test numerically the procedure including situations beyond the theoretical setting. The tests show that a time dependent regularized procedure can improve over the canonical approach especially when the initial guess is far from the solution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>L2正则化策略梯度算法在多臂老虎机问题中的收敛性分析</div>
<div class="mono" style="margin-top:8px">尽管多臂老虎机（MAB）与策略梯度方法是强化学习中最常用的两大框架，但针对MAB的策略梯度算法的理论性质尚未得到充分关注。本研究探讨了在采用&#x27;Softmax&#x27;参数化并引入L2正则化项的情况下，该算法的收敛性。我们在适当的技术假设下证明了其收敛性，并对包括超出理论设定范围的情形进行了数值测试。实验表明，时间依赖的正则化方法相较于经典方法具有优势，尤其在初始猜测远离最优解时效果更为显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of theoretical analysis for policy gradient methods in the Multi-Armed Bandit (MAB) setting by investigating the convergence of an L2-regularized policy gradient algorithm with softmax parametrization. The method involves incorporating an L2 regularization term into the standard policy gradient update to stabilize learning. Experimental results confirm convergence under the theoretical assumptions and demonstrate that a time-dependent regularization schedule can outperform the canonical approach, particularly when the initial policy is poorly chosen.</div>
<div class="mono" style="margin-top:8px">本文针对多臂老虎机问题中策略梯度方法缺乏理论分析的问题，研究了采用softmax参数化和L2正则化的策略梯度算法的收敛性。该方法在标准策略梯度更新中加入了L2正则项以稳定学习过程。实验结果在理论假设下验证了算法的收敛性，并表明时间依赖的正则化方案能够超越标准方法，尤其在初始策略选择不佳时效果更为显著。</div>
</details>
</div>
<div class="card">
<div class="title">A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning</div>
<div class="meta-line">Authors: Wencheng Cai, Xuchao Gao, Congying Han, Mingqiang Li, Tiande Guo</div>
<div class="meta-line">First: 2025-12-05T14:39:50+00:00 · Latest: 2026-01-05T08:40:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05753v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.05753v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的快速抗干扰认知雷达部署算法</div>
<div class="mono" style="margin-top:8px">认知雷达的快速部署以应对干扰在现代战争中仍是一个关键挑战，更高效的部署能更快探测目标。现有方法主要基于进化算法，耗时且易陷入局部最优。我们通过神经网络的高效推理克服这些缺陷，提出全新框架：快速抗干扰雷达部署算法（FARDA）。首先将雷达部署问题建模为端到端任务，并设计深度强化学习算法求解，开发了集成神经模块以感知热图信息及新型奖励机制。实验结果表明，本方法在达到与进化算法相当覆盖范围的同时，部署速度提升约7,000倍。消融实验进一步验证了FARDA各组成部分的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for rapid and efficient cognitive radar deployment to counter jamming in modern warfare, where existing evolutionary methods are slow and prone to local optima, this paper proposes the Fast Anti-Jamming Radar Deployment Algorithm (FARDA). The method models deployment as an end-to-end task using deep reinforcement learning, incorporating neural modules to perceive heatmap information and a novel reward format. Experimental results show that FARDA achieves coverage similar to evolutionary algorithms while being approximately 7,000 times faster, with ablation studies confirming the importance of each component.</div>
<div class="mono" style="margin-top:8px">针对现代战争中快速高效部署认知雷达以对抗干扰的需求，现有进化算法耗时且易陷入局部最优，本文提出了快速抗干扰雷达部署算法（FARDA）。该方法将部署问题建模为端到端任务，采用深度强化学习，设计了感知热图信息的神经模块和新的奖励格式。实验结果表明，FARDA在达到与进化算法相当覆盖范围的同时，部署速度提升了约7,000倍，消融实验验证了各组成部分的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation</div>
<div class="meta-line">Authors: Shuta Kikuchi, Shu Tanaka</div>
<div class="meta-line">First: 2026-01-05T07:41:34+00:00 · Latest: 2026-01-05T07:41:34+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01860v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01860v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于二次优化退火因子分解机与MDR评估的高阶上位性检测</div>
<div class="mono" style="margin-top:8px">检测高阶上位性是遗传关联研究中的核心挑战，源于候选位点组合的组合爆炸问题。尽管多因子降维（MDR）是评估上位性的常用方法，但随着位点数量或交互阶数的增加，基于MDR的穷举搜索在计算上变得不可行。本文将该问题定义为黑盒优化问题，并采用二次优化退火因子分解机（FMQA）进行求解。我们提出一种基于FMQA的高效上位性检测方法，其中以MDR计算的分类错误率（CER）作为黑盒目标函数。通过使用预设高阶上位性的模拟病例对照数据集进行实验评估，结果表明所提方法能在有限迭代次数内成功识别不同交互阶数和遗传位点数量的真实上位性。这些结果证明该方法对高阶上位性检测具有高效性和计算可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational infeasibility of exhaustive multifactor dimensionality reduction (MDR) searches for high-order genetic interactions, this paper proposes a method that frames epistasis detection as a black-box optimization problem solved via factorization machine with quadratic optimization annealing (FMQA). The method uses MDR-computed classification error rate as the objective function to efficiently evaluate candidate locus combinations. Experimental results on simulated case-control datasets show that the approach successfully identifies ground-truth high-order epistasis across various interaction orders and locus numbers within limited iterations, demonstrating both effectiveness and computational efficiency.</div>
<div class="mono" style="margin-top:8px">针对高阶上位性检测中穷举多因子降维（MDR）搜索计算不可行的问题，本研究提出将上位性检测定义为黑盒优化问题，并通过带二次优化退火的因子分解机（FMQA）进行求解。该方法以MDR计算的分类错误率作为目标函数，高效评估候选位点组合。在模拟病例对照数据集上的实验结果表明，该方法能在有限迭代次数内成功识别不同交互阶数和位点数量的真实高阶上位性，证明了其有效性和计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization</div>
<div class="meta-line">Authors: SB Danush Vikraman, Hannah Abagail, Prasanna Kesavraj, Gajanan V Honnavar</div>
<div class="meta-line">First: 2026-01-05T06:51:08+00:00 · Latest: 2026-01-05T06:51:08+00:00</div>
<div class="meta-line">Comments: 22 pages, 9 figures, includes extensive ablation studies and benchmark comparisons</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01832v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Yukthi Opus (YO), a multi-chain hybrid metaheuristic designed for NP-hard optimization under explicit evaluation budget constraints. YO integrates three complementary mechanisms in a structured two-phase architecture: Markov Chain Monte Carlo (MCMC) for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to enable controlled escape from local minima. A dedicated burn-in phase allocates evaluations to probabilistic exploration, after which a hybrid optimization loop refines promising candidates. YO further incorporates a spatial blacklist mechanism to avoid repeated evaluation of poor regions and a multi-chain execution strategy to improve robustness and reduce sensitivity to initialization.
  We evaluate YO on three benchmarks: the Rastrigin function (5D) with ablation studies, the Traveling Salesman Problem with 50 to 200 cities, and the Rosenbrock function (5D) with comparisons against established optimizers including CMA-ES, Bayesian optimization, and accelerated particle swarm optimization. Results show that MCMC exploration and greedy refinement are critical for solution quality, while simulated annealing and multi-chain execution primarily improve stability and variance reduction. Overall, YO achieves competitive performance on large and multimodal problems while maintaining predictable evaluation budgets, making it suitable for expensive black-box optimization settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Yukthi Opus：一种面向大规模NP难优化问题的多链混合元启发式算法</div>
<div class="mono" style="margin-top:8px">本文提出Yukthi Opus（YO）——一种在显式评估预算约束下针对NP难优化问题设计的多链混合元启发式算法。YO采用结构化双阶段架构集成三种互补机制：马尔可夫链蒙特卡洛（MCMC）实现全局探索，贪婪局部搜索进行开发，以及带自适应重加热的模拟退火实现可控的局部极小值逃逸。专用预热阶段将评估资源分配给概率性探索，随后混合优化循环对潜力解进行精细化。YO进一步引入空间黑名单机制避免重复评估劣质区域，并采用多链执行策略提升鲁棒性、降低对初始化的敏感性。
我们在三个基准测试中评估YO：通过消融实验测试的5维Rastrigin函数、50至200个城市的旅行商问题，以及5维Rosenbrock函数（与CMA-ES、贝叶斯优化、加速粒子群优化等成熟优化器对比）。结果表明：MCMC探索与贪婪精细化对解质量至关重要，而模拟退火与多链执行主要提升稳定性并降低方差。总体而言，YO在保持可预测评估预算的同时，于大规模多模态问题上取得具有竞争力的性能，适用于高成本黑箱优化场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Yukthi Opus (YO), a multi-chain hybrid metaheuristic motivated by the need for efficient NP-hard optimization under strict evaluation budget constraints, particularly for expensive black-box problems. The method employs a two-phase architecture combining Markov Chain Monte Carlo for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to escape local minima, alongside a spatial blacklist and multi-chain execution for robustness. Experimental results on benchmarks like the Rastrigin and Rosenbrock functions and the Traveling Salesman Problem demonstrate that YO achieves competitive performance, with MCMC and greedy refinement crucial for solution quality, while simulated annealing and multi-chaining enhance stability and reduce variance compared to optimizers such as CMA-ES and Bayesian optimization.</div>
<div class="mono" style="margin-top:8px">本文提出了Yukthi Opus（YO），一种多链混合元启发式方法，其动机是在严格的评估预算约束下高效解决NP难优化问题，尤其适用于昂贵的黑盒优化场景。该方法采用两阶段架构，整合了马尔可夫链蒙特卡洛进行全局探索、贪婪局部搜索进行利用，以及带自适应再加热的模拟退火来逃离局部极小值，同时通过空间黑名单和多链执行策略提升鲁棒性。在Rastrigin函数、Rosenbrock函数和旅行商问题等基准测试上的实验结果表明，YO取得了有竞争力的性能，其中MCMC和贪婪细化对解的质量至关重要，而模拟退火和多链机制则提高了稳定性并降低了方差，优于CMA-ES和贝叶斯优化等现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">VAR-MATH: Probing True Mathematical Reasoning in LLMS via Symbolic Multi-Instance Benchmarks</div>
<div class="meta-line">Authors: Jian Yao, Ran Cheng, Kay Chen Tan</div>
<div class="meta-line">First: 2025-07-17T08:10:55+00:00 · Latest: 2026-01-05T05:39:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12885v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.12885v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of LLMs, as measured by standard benchmarks. Yet these gains often persist even when models are trained with flawed signals, such as random or inverted rewards. This raises a fundamental question: do such improvements reflect genuine reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To answer this question, we adopt an evaluation-centric perspective and highlight two critical shortcomings in existing protocols. First, benchmark contamination arises because test problems are publicly available, thereby increasing the risk of data leakage. Second, evaluation fragility results from reliance on single-instance assessments, which are sensitive to stochastic outputs and fail to capture reasoning consistency. These limitations suggest the need for a new evaluation paradigm that can probe reasoning ability beyond memorization and one-off success. As response, we propose VAR-MATH, a symbolic evaluation framework that converts fixed numerical problems into parameterized templates and requires models to solve multiple instantiations of each. This design enforces consistency across structurally equivalent variants, mitigates contamination, and enhances robustness through bootstrapped metrics. We apply VAR-MATH to transform three popular benchmarks, AMC23, AIME24, and AIME25, into their symbolic counterparts, VAR-AMC23, VAR-AIME24, and VAR-AIME25. Experimental results show substantial performance drops for RL-trained models on these variabilized benchmarks, especially for smaller models, with average declines of 47.9\% on AMC23, 58.8\% on AIME24, and 72.9\% on AIME25. These findings indicate that some existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VAR-MATH：通过符号化多实例基准测试探究大语言模型的真实数学推理能力</div>
<div class="mono" style="margin-top:8px">近期强化学习（RL）的进展显著提升了大语言模型在标准基准测试中的数学推理能力。然而，即使模型使用随机或反向奖励等有缺陷的信号进行训练，这些提升依然存在。这引发了一个根本性问题：此类改进是否反映了真实的推理能力，抑或仅仅是模型对基准特定模式过拟合的假象？为解答此问题，我们采用评估中心视角，指出现有评估方案存在两个关键缺陷：一是因测试题目公开可用导致的基准污染风险；二是依赖对随机输出敏感的单实例评估，无法捕捉推理一致性。这些局限表明需要建立超越记忆与单次成功的新评估范式。为此，我们提出VAR-MATH——一个符号化评估框架，将固定数值问题转化为参数化模板，要求模型求解每个模板的多个实例。该设计通过结构等价变体强制一致性、缓解数据污染，并采用自助法指标提升鲁棒性。我们将VAR-MATH应用于三个主流基准（AMC23、AIME24、AIME25），生成其符号化版本VAR-AMC23、VAR-AIME24、VAR-AIME25。实验显示，RL训练模型在这些变量化基准上性能大幅下降（小型模型尤为明显），在AMC23、AIME24、AIME25上的平均降幅分别为47.9%、58.8%、72.9%。这表明现有RL方法部分依赖表面启发式规则，未能泛化至特定数值形式之外。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by concerns that improvements in large language models&#x27; mathematical reasoning from reinforcement learning may reflect overfitting to benchmark patterns rather than genuine reasoning, exacerbated by data leakage and fragile single-instance evaluations. To address this, the authors propose VAR-MATH, a symbolic evaluation framework that converts fixed numerical problems into parameterized templates, requiring models to solve multiple instantiations to test consistency and mitigate contamination. Experimental results on transformed benchmarks show substantial performance drops for RL-trained models, with average declines of 47.9% to 72.9%, indicating reliance on superficial heuristics rather than robust reasoning.</div>
<div class="mono" style="margin-top:8px">本文的动机是担忧大型语言模型通过强化学习在数学推理上的提升可能仅反映了对基准测试模式的过拟合，而非真正的推理能力，这一问题因数据泄露和脆弱的单实例评估而加剧。为此，作者提出了VAR-MATH，一个符号化评估框架，将固定数值问题转换为参数化模板，要求模型解决多个实例以测试一致性并减少污染。在转换后的基准测试上的实验结果显示，经过强化学习训练的模型性能显著下降，平均降幅达47.9%至72.9%，表明这些模型依赖表面启发式方法而非稳健的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Moments Matter:Stabilizing Policy Optimization using Return Distributions</div>
<div class="meta-line">Authors: Dennis Jabs, Aditya Mohan, Marius Lindauer</div>
<div class="meta-line">First: 2026-01-05T05:27:11+00:00 · Latest: 2026-01-05T05:27:11+00:00</div>
<div class="meta-line">Comments: Workshop paper at RLDM&#x27;25</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01803v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01803v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>矩量关键：利用回报分布稳定策略优化</div>
<div class="mono" style="margin-top:8px">深度强化学习智能体常因环境因素（随机转移、初始条件、奖励噪声）与算法因素（小批量选择、探索噪声）的共同作用，习得虽能达成相同回合回报但行为迥异的策略。在连续控制任务中，即使微小的参数偏移也可能导致步态失稳，使算法比较与现实迁移复杂化。先前研究表明，当策略更新穿越噪声邻域时会产生此类不稳定性，而通过重复采样小批量、更新参数θ并测量最终回报所得的后更新回报分布R(θ)的离散程度，是衡量该噪声的有效指标。虽然显式约束策略以维持狭窄的R(θ)可提升稳定性，但在高维场景中直接估计R(θ)计算成本高昂。我们提出一种利用环境随机性缓解更新诱发变异性的替代方案：通过分布评论家建模状态-动作回报分布，进而使用该分布的高阶矩（偏度与峰度）修正PPO的优势函数。通过惩罚极端尾部行为，本方法阻止策略进入易失稳的参数区间。我们假设，在后更新评论家值与后更新回报匹配度较差的环境中，标准PPO难以生成狭窄的R(θ)。实验表明，基于矩量的修正方法能使Walker2D环境的R(θ)离散度降低75%，在保持相当评估回报的同时显著提升稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the instability in deep reinforcement learning policies, where agents with similar episodic returns can exhibit vastly different behaviors due to environmental and algorithmic noise, complicating reliable algorithm comparison and real-world deployment. To address this, the method introduces a distributional critic to model state-action return distributions and modifies the Proximal Policy Optimization (PPO) advantage function by incorporating higher-order moments (skewness and kurtosis), thereby penalizing extreme tail behaviors to discourage policies from entering unstable parameter regimes. Experimental results in continuous control tasks, such as Walker2D, demonstrate that this approach reduces post-update return distribution spread, improving stability by up to 75% while maintaining comparable evaluation returns.</div>
<div class="mono" style="margin-top:8px">本文的动机源于深度强化学习中策略的不稳定性问题，即由于环境和算法噪声，具有相似回合回报的智能体可能表现出截然不同的行为，这给算法可靠比较和实际部署带来了困难。为解决此问题，方法引入了一个分布评论器来建模状态-动作回报分布，并通过结合高阶矩（偏度和峰度）来修改近端策略优化（PPO）的优势函数，从而惩罚极端尾部行为，防止策略进入不稳定的参数区域。在连续控制任务（如Walker2D）中的实验结果表明，该方法能缩小更新后回报分布的宽度，将稳定性提升高达75%，同时保持可比的评估回报。</div>
</details>
</div>
<div class="card">
<div class="title">PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor</div>
<div class="meta-line">Authors: Qianjun Pan, Junyi Wang, Jie Zhou, Yutao Yang, Junsong Li, Kaiyin Xu, Yougen Zhou, Yihan Li, Jingyuan Zhao, Qin Chen, Ningning Zhou, Kai Chen, Liang He</div>
<div class="meta-line">First: 2026-01-05T05:26:57+00:00 · Latest: 2026-01-05T05:26:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01802v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PsychEval：面向高真实感与综合性AI心理咨询师的多会话多疗法基准</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估AI，我们提出\texttt{PsychEval}——一个多会话、多疗法、高真实度的基准，旨在解决三大挑战：\textbf{1) 能否训练高真实感的AI咨询师？}真实咨询是需持续记忆与动态目标追踪的纵向任务。我们构建了跨三个独立阶段（6-10次会话）的多会话基准，要求模型具备记忆连续性、自适应推理与纵向规划等关键能力。数据集标注了涵盖677项元技能与4577项原子技能的完整专业体系。\textbf{2) 如何训练多疗法AI咨询师？}现有模型常局限于单一疗法，而复杂案例常需跨疗法灵活策略。我们构建了涵盖五种治疗流派（心理动力学、行为主义、认知行为疗法、人本存在主义、后现代主义）及整合疗法的数据集，并基于六类核心心理议题建立了统一的三阶段临床框架。\textbf{3) 如何系统评估AI咨询师？}我们建立了包含来访者维度与咨询师维度的18项疗法专用/通用指标的全方位评估框架，并配套构建了2000余个多样化来访者画像。大量实验分析充分验证了数据集的高质量与临床保真度。\texttt{PsychEval}更突破了静态基准的局限，可作为高保真强化学习环境，支持临床责任性与自适应AI咨询师的自我进化训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable AI in psychological assessment, this paper introduces PsychEval, a multi-session and multi-therapy benchmark designed to train highly realistic AI counselors. The method involves constructing a dataset spanning 6-10 sessions across three clinical stages, annotated with over 677 meta-skills and 4577 atomic skills, and covering five therapeutic modalities within a unified framework to enable adaptive, multi-therapy strategies. The main experimental results validate the dataset&#x27;s superior quality and clinical fidelity through a holistic evaluation framework with 18 metrics, and the benchmark serves as a reinforcement learning environment for training self-evolutionary AI counselors.</div>
<div class="mono" style="margin-top:8px">为开发可靠的AI心理评估工具，本文提出了PsychEval，这是一个多会话、多疗法的高真实性基准，旨在训练高度逼真的AI心理咨询师。方法上构建了一个跨越三个临床阶段、包含6-10次会话的数据集，标注了超过677项元技能和4577项原子技能，并涵盖五种治疗模式，以支持灵活的多疗法策略。主要实验结果通过包含18个指标的整体评估框架验证了数据集的优越质量和临床保真度，该基准还可作为强化学习环境，用于训练具有临床责任感和自适应能力的AI咨询师。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving</div>
<div class="meta-line">Authors: Qi Wei, Junchao Fan, Zhao Yang, Jianhua Wang, Jingkai Mao, Xiaolin Chang</div>
<div class="meta-line">First: 2026-01-05T05:20:16+00:00 · Latest: 2026-01-05T05:20:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01800v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01800v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏威胁与聚焦防御：面向安全自动驾驶的临界感知鲁棒强化学习</div>
<div class="mono" style="margin-top:8px">强化学习在自动驾驶领域展现出巨大潜力，但其对扰动的脆弱性仍是实际部署的关键障碍。作为主要应对策略，对抗训练通过让自动驾驶智能体在故意引入扰动的对抗环境中学习来提升策略鲁棒性。现有方法通常将交互建模为连续攻击的零和博弈，但此类设计忽略了智能体与对抗者之间的固有不对称性，且未能反映安全关键风险的稀疏特性，导致所获鲁棒性难以满足实际自动驾驶场景需求。为解决这些局限，我们提出临界感知鲁棒强化学习——一种处理自动驾驶中稀疏安全关键风险的新型对抗训练方法。该方法包含两个交互组件：风险暴露对抗器和风险靶向鲁棒智能体。我们将二者交互建模为一般和博弈，使对抗器专注于暴露安全关键故障（如碰撞），而智能体学习在安全与驾驶效率间取得平衡。对抗器采用解耦优化机制，在受限预算下更有效识别并利用稀疏的安全关键时刻。然而，这种聚焦攻击必然导致对抗数据稀缺。智能体通过双经验回放池联合利用正常与对抗经验应对数据稀缺问题，并在扰动下强制策略一致性以稳定行为。实验结果表明，相较于最先进的基线方法，我们的方法在所有案例中将碰撞率降低了至少22.66%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of reinforcement learning (RL) in autonomous driving to adversarial perturbations, noting that existing adversarial training methods often use zero-sum game formulations with continuous attacks, which overlook the sparsity of safety-critical risks and the asymmetry between agent and adversary. To overcome this, the authors propose Criticality-Aware Robust RL (CARRL), which models the interaction as a general-sum game involving a Risk Exposure Adversary (REA) that focuses on sparse, critical failures like collisions under a constrained budget, and a Risk-Targeted Robust Agent (RTRA) that uses a dual replay buffer to learn from both benign and adversarial experiences while enforcing policy consistency. Experimental results show that CARRL reduces collision rates by at least 22.66% compared to state-of-the-art baselines, demonstrating improved safety in autonomous driving scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在自动驾驶中对抗扰动的脆弱性问题展开研究，指出现有的对抗训练方法通常采用零和博弈与连续攻击，忽略了安全关键风险的稀疏性以及智能体与对手之间的不对称性。为此，作者提出了关键性感知的鲁棒强化学习（CARRL），该方法将交互建模为一般和博弈，包含一个风险暴露对手（REA）在有限预算下专注于稀疏的关键故障（如碰撞），以及一个风险目标鲁棒智能体（RTRA）通过双经验回放池利用良性对抗经验并强制策略一致性以稳定行为。实验结果表明，与最先进的基线方法相比，CARRL在所有案例中将碰撞率降低了至少22.66%，提升了自动驾驶的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">ScRPO: From Errors to Insights</div>
<div class="meta-line">Authors: Lianrui Li, Dakuan Lu, Jiawei Shao, Xuelong Li</div>
<div class="meta-line">First: 2025-11-08T16:30:44+00:00 · Latest: 2026-01-05T05:19:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06065v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.06065v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to empower large language models with advanced mathematical reasoning capabilities through iterative self-reflection and error correction. The ScRPO framework operates in two distinct phases: (1) Trial-and-error learning stage, where the model is trained via GRPO, and incorrect responses are collected to form an &quot;error pool&quot;; and (2) Self-correction learning stage, which guides the model to introspectively analyze and rectify the reasoning flaws behind its previous errors. Extensive evaluations across challenging mathematical benchmarks, including AIME, AMC, Olympiad, MATH-500, and GSM8k, validate the efficacy of our approach. Using DeepSeek-R1-Distill-Qwen-1.5B and 7B as backbones, ScRPO achieves average accuracies of 64.8% and 77.8%, respectively. This represents a significant improvement of 6.0% and 3.2% over vanilla baselines, consistently outperforming strong post-training methods such as DAPO and GRPO. These findings establish ScRPO as a robust paradigm for enabling autonomous self-improvement in AI systems, particularly in tasks with limited external feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ScRPO：从错误到洞见</div>
<div class="mono" style="margin-top:8px">本文提出自校正相对策略优化（ScRPO），这是一种新颖的强化学习框架，旨在通过迭代式自我反思与错误校正，赋予大语言模型高级数学推理能力。ScRPO框架包含两个独立阶段：（1）试错学习阶段：通过GRPO训练模型，并收集错误响应构建“错误池”；（2）自校正学习阶段：引导模型内省分析并修正先前错误背后的推理缺陷。在AIME、AMC、奥林匹克、MATH-500及GSM8k等挑战性数学基准上的广泛评估验证了本方法的有效性。以DeepSeek-R1-Distill-Qwen-1.5B和7B为骨干模型时，ScRPO分别达到64.8%和77.8%的平均准确率，较原始基线显著提升6.0%和3.2%，持续优于DAPO、GRPO等强后训练方法。这些发现确立了ScRPO作为实现AI系统自主自我改进的稳健范式，尤其在外部反馈有限的任务中具有突出价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance large language models&#x27; mathematical reasoning through autonomous self-improvement, this paper introduces Self-correction Relative Policy Optimization (ScRPO), a two-phase reinforcement learning framework. The method first employs a trial-and-error stage using GRPO to collect incorrect responses into an error pool, followed by a self-correction stage where the model learns to introspectively analyze and rectify its reasoning flaws. Experimental results on benchmarks like AIME, AMC, and GSM8k show that ScRPO, using DeepSeek-R1-Distill-Qwen-1.5B and 7B models, achieves average accuracies of 64.8% and 77.8%, respectively, marking significant improvements of 6.0% and 3.2% over vanilla baselines and outperforming methods like DAPO and GRPO, thereby establishing it as a robust paradigm for tasks with limited external feedback.</div>
<div class="mono" style="margin-top:8px">为通过自主自我改进来增强大语言模型的数学推理能力，本文提出了自校正相对策略优化（ScRPO），这是一个两阶段的强化学习框架。该方法首先利用GRPO进行试错学习，收集错误响应形成“错误池”，随后在自校正阶段引导模型内省分析并修正其推理缺陷。在AIME、AMC和GSM8k等数学基准上的实验结果表明，基于DeepSeek-R1-Distill-Qwen-1.5B和7B模型的ScRPO分别实现了64.8%和77.8%的平均准确率，相比基线显著提升了6.0%和3.2%，并优于DAPO和GRPO等后训练方法，从而确立了其在外部反馈有限任务中作为稳健自改进范式的地位。</div>
</details>
</div>
<div class="card">
<div class="title">SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines</div>
<div class="meta-line">Authors: Rajiv Chaitanya Muttur</div>
<div class="meta-line">First: 2026-01-05T04:39:31+00:00 · Latest: 2026-01-05T04:39:31+00:00</div>
<div class="meta-line">Comments: Presented at ICEdge 2025; nominated for Best Paper Award</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01785v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining &lt;1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SRAS：面向边缘原生RAG管道的轻量级强化学习文档选择器</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）系统通常依赖固定的top-k文档选择机制，这种机制忽略了下游生成质量并带来计算开销。我们提出SRAS（稀疏奖励感知选择器），一种通过强化学习训练的轻量级文档选择器，专为边缘原生RAG部署设计。与先前假设大内存和延迟预算的基于强化学习的检索器不同，SRAS使用近端策略优化学习紧凑策略（约0.76MB），并通过结合松弛F1和BERTScore的混合奖励信号进行指导。我们的方法在严格的令牌和计算约束下运行，在CPU上保持&lt;1秒的延迟。SRAS在合成QA基准测试中优于监督和随机选择器，并能泛化到真实数据，在SQuAD v2上无需领域特定调优即达到0.8546的BERTScore F1值。这项研究首次证明，基于强化学习的文档选择可以实现超轻量、延迟感知，并能有效用于设备端RAG管道。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of fixed top-k document selection in Retrieval-Augmented Generation (RAG) systems, which neglects generation quality and incurs computational costs, by proposing SRAS, a lightweight reinforcement learning-based document selector designed for edge-native deployment. The method employs Proximal Policy Optimization (PPO) to train a compact policy (~0.76MB) using a hybrid reward signal combining Relaxed F1 and BERTScore, operating under strict token and compute constraints to maintain under 1-second latency on CPU. Experimental results show that SRAS outperforms supervised and random selectors on a synthetic QA benchmark and generalizes to real-world data, achieving a BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning, demonstrating its effectiveness for on-device RAG pipelines.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）系统中固定 top-k 文档选择机制忽略生成质量并带来计算开销的问题，提出了 SRAS，一种基于强化学习的轻量级文档选择器，专为边缘原生部署设计。该方法使用近端策略优化（PPO）训练一个紧凑的策略（约0.76MB），通过结合 Relaxed F1 和 BERTScore 的混合奖励信号进行指导，在严格的令牌和计算约束下运行，在 CPU 上保持低于1秒的延迟。实验结果表明，SRAS 在合成 QA 基准测试中优于监督和随机选择器，并能泛化到真实世界数据，在 SQuAD v2 上实现了 0.8546 的 BERTScore F1 分数，且无需领域特定调优，证明了其在设备端 RAG 管道中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism</div>
<div class="meta-line">Authors: Tianwei Ni, Esther Derman, Vineet Jain, Vincent Taboga, Siamak Ravanbakhsh, Pierre-Luc Bacon</div>
<div class="meta-line">First: 2025-12-04T00:07:08+00:00 · Latest: 2026-01-05T04:22:12+00:00</div>
<div class="meta-line">Comments: Preprint (52 pages, 15 figures) and code is available at https://github.com/twni2016/neubay</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04341v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04341v2">PDF</a> · <a href="https://github.com/twni2016/neubay">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Popular offline reinforcement learning (RL) methods rely on conservatism, either by penalizing out-of-dataset actions or by restricting rollout horizons. In this work, we question the universality of this principle and instead revisit a complementary one: a Bayesian perspective. Rather than enforcing conservatism, the Bayesian approach tackles epistemic uncertainty in offline data by modeling a posterior distribution over plausible world models and training a history-dependent agent to maximize expected rewards, enabling test-time generalization. We first illustrate, in a bandit setting, that Bayesianism excels on low-quality datasets where conservatism fails. We then scale this principle to realistic tasks and show that long-horizon planning is critical for reducing value overestimation once conservatism is removed. To make this feasible, we introduce key design choices for performing and learning from long-horizon rollouts while controlling compounding errors. These yield our algorithm, NEUBAY, grounded in the neutral Bayesian principle. On D4RL and NeoRL benchmarks, NEUBAY generally matches or surpasses leading conservative algorithms, achieving new state-of-the-art on 7 datasets. Notably, it succeeds with rollout horizons of several hundred steps, contrary to dominant practice. Finally, we characterize datasets by quality and coverage, showing when NEUBAY is preferable to conservative methods. Together, we argue NEUBAY lays the foundation for a new practical direction in offline and model-based RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需保守性的长时域模型离线强化学习</div>
<div class="mono" style="margin-top:8px">主流离线强化学习方法依赖保守性原则，或通过惩罚数据集外动作，或通过限制推演时域。本研究质疑该原则的普适性，转而重新审视一种互补视角：贝叶斯方法。贝叶斯方法不强制保守性，而是通过对可能世界模型的后验分布建模，并训练一个依赖历史的智能体以最大化期望奖励，从而处理离线数据中的认知不确定性，实现测试时泛化。我们首先在赌博机场景中证明，贝叶斯方法在保守性方法失效的低质量数据集上表现优异。随后将该原则扩展至实际任务，表明一旦移除保守性，长时域规划对减少价值高估至关重要。为实现此目标，我们提出了关键设计选择，以执行长时域推演并从中学习，同时控制误差累积。这些设计形成了基于中性贝叶斯原则的算法NEUBAY。在D4RL和NeoRL基准测试中，NEUBAY普遍匹配或超越主流保守算法，在7个数据集上达到新的最优性能。值得注意的是，该算法成功实现了数百步的长时域推演，与主流实践相反。最后，我们通过数据质量和覆盖度对数据集进行表征，阐明NEUBAY优于保守方法的适用场景。综合而言，我们认为NEUBAY为离线与模型强化学习的新实用方向奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the prevailing reliance on conservatism in offline reinforcement learning, proposing instead a Bayesian approach that models epistemic uncertainty through a posterior distribution over world models and trains a history-dependent agent for test-time generalization. The method, named NEUBAY, emphasizes long-horizon planning to mitigate value overestimation without conservatism, incorporating design choices to manage compounding errors during rollouts. Experimental results on D4RL and NeoRL benchmarks demonstrate that NEUBAY matches or exceeds state-of-the-art conservative algorithms, achieving top performance on 7 datasets with rollout horizons of hundreds of steps, and analysis identifies dataset conditions where this Bayesian method outperforms conservative alternatives.</div>
<div class="mono" style="margin-top:8px">本文挑战了离线强化学习中普遍依赖保守主义的做法，转而提出一种贝叶斯方法，通过对世界模型的后验分布建模来应对认知不确定性，并训练一个依赖历史的智能体以实现测试时泛化。该方法名为NEUBAY，强调在不使用保守主义的情况下通过长时程规划来减少价值高估，并引入了控制累积误差的关键设计。在D4RL和NeoRL基准测试中，NEUBAY达到或超越了领先的保守算法，在7个数据集上取得了最优性能，且能实现数百步的长时程规划，分析还揭示了该贝叶斯方法优于保守方法的数据集条件。</div>
</details>
</div>
<div class="card">
<div class="title">AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing</div>
<div class="meta-line">Authors: Jiacheng Li, Jianchao Tan, Zhidong Yang, Feiye Huo, Yerui Sun, Yuchen Xie, Xunliang Cai</div>
<div class="meta-line">First: 2025-12-27T04:12:40+00:00 · Latest: 2026-01-05T04:00:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22455v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22455v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AFA-LoRA：通过激活函数退火在LoRA中实现非线性适配</div>
<div class="mono" style="margin-top:8px">低秩适配（LoRA）是一种广泛采用的参数高效微调方法，但其线性适配过程限制了表达能力，导致线性训练与非线性训练之间存在性能差距。为弥合这一差距，我们提出AFA-LoRA，这是一种新颖的训练策略，在保持无缝可合并性的同时为LoRA引入非线性表达能力。其核心创新在于采用退火激活函数，在训练过程中从非线性变换逐步过渡到线性变换，使适配器先获得更强的表征能力，最终收敛为可合并的线性形式。我们在监督微调、强化学习和推测解码任务中验证了该方法，结果表明AFA-LoRA显著缩小了与全参数训练的性能差距，为参数高效适配提供了更强大实用的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limited expressive power of linear adaptations in Low-Rank Adaptation (LoRA), a popular parameter-efficient fine-tuning method, which creates a performance gap compared to non-linear full-parameter training. To address this, the authors propose AFA-LoRA, a training strategy that introduces non-linear expressivity into LoRA while preserving its seamless mergeability through an annealed activation function that transitions from non-linear to linear during training. Experimental results from supervised fine-tuning, reinforcement learning, and speculative decoding demonstrate that AFA-LoRA effectively reduces the performance gap between LoRA and full-parameter training, enabling a more powerful and practical paradigm for parameter-efficient adaptation.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，广泛使用的参数高效微调方法低秩适应（LoRA）因其线性适应过程表达能力有限，与非线性全参数训练存在性能差距。为解决此问题，作者提出了AFA-LoRA，这是一种通过在训练中使用从非线性过渡到线性的退火激活函数，为LoRA引入非线性表达能力同时保持其无缝可合并性的训练策略。在监督微调、强化学习和推测解码上的实验结果表明，AFA-LoRA有效缩小了LoRA与全参数训练之间的性能差距，从而实现了更强大且实用的参数高效适应范式。</div>
</details>
</div>
<div class="card">
<div class="title">Generation of Geodesics with Actor-Critic Reinforcement Learning to Predict Midpoints</div>
<div class="meta-line">Authors: Kazumi Kasaura</div>
<div class="meta-line">Venue: Journal of Machine Learning Research, 26(212):1-36, 2025</div>
<div class="meta-line">First: 2024-07-02T07:06:49+00:00 · Latest: 2026-01-05T03:59:23+00:00</div>
<div class="meta-line">Comments: 17 pages with 8 pages of appendices and references, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.01991v5">Abs</a> · <a href="https://arxiv.org/pdf/2407.01991v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To find the shortest paths for all pairs on manifolds with infinitesimally defined metrics, we introduce a framework to generate them by predicting midpoints recursively. To learn midpoint prediction, we propose an actor-critic approach. We prove the soundness of our approach and show experimentally that the proposed method outperforms existing methods on several planning tasks, including path planning for agents with complex kinematics and motion planning for multi-degree-of-freedom robot arms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于演员-评论家强化学习生成测地线以预测中点</div>
<div class="mono" style="margin-top:8px">为求解无穷小度量定义流形上所有点对之间的最短路径，我们提出一种通过递归预测中点生成路径的框架。针对中点预测学习，我们采用演员-评论家方法。理论证明了该方法的可靠性，实验表明在多项规划任务中（包括复杂运动学智能体路径规划与多自由度机械臂运动规划），所提方法性能优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to compute shortest paths (geodesics) on manifolds with infinitesimally defined metrics for all point pairs, this paper introduces a framework that recursively predicts path midpoints to generate these geodesics. The method employs an actor-critic reinforcement learning approach to learn the midpoint prediction function, with theoretical proof provided for the approach&#x27;s soundness. Experimentally, the proposed method demonstrates superior performance over existing techniques in various planning tasks, including path planning for agents with complex kinematics and motion planning for multi-degree-of-freedom robotic arms.</div>
<div class="mono" style="margin-top:8px">本文旨在为具有无穷小定义度量的流形上所有点对计算最短路径（测地线），为此引入了一个通过递归预测中点来生成这些路径的框架。方法采用行动者-评论家强化学习来学习中点预测函数，并从理论上证明了该方法的正确性。实验结果表明，所提出的方法在多个规划任务中优于现有方法，包括具有复杂运动学智能体的路径规划以及多自由度机械臂的运动规划。</div>
</details>
</div>
<div class="card">
<div class="title">Precision Autotuning for Linear Solvers via Reinforcement Learning</div>
<div class="meta-line">Authors: Erin Carson, Xinye Chen</div>
<div class="meta-line">First: 2026-01-02T15:59:42+00:00 · Latest: 2026-01-05T03:25:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00728v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00728v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的线性求解器精度自动调优</div>
<div class="mono" style="margin-top:8px">我们提出了一种用于线性求解器自适应精度调优的强化学习框架，并可扩展至通用算法。该框架被建模为上下文赌博机问题，通过离散化状态空间下的增量动作价值估计进行求解，以选择计算步骤的最优精度配置，平衡精度与计算效率。为验证其有效性，我们将该框架应用于求解线性方程组 $Ax = b$ 的迭代精化算法。在此应用中，我们的方法基于系统计算特征动态选择精度。具体而言，通过Q表将离散化特征（如近似条件数与矩阵范数）映射至动作（特定步骤的精度配置），并采用ε-贪婪策略进行优化，以最大化平衡精度与计算成本的多目标奖励。实验结果表明，该方法能有效选择精度，在保持与双精度基准相当精度的同时降低计算成本。该框架可泛化至多样化的样本外数据，并为其他数值算法应用强化学习精度选择提供思路，推动了科学计算中混合精度数值方法的发展。据我们所知，这是首个基于强化学习的精度自动调优研究，并在未见数据集上得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance computational efficiency in scientific computing while preserving accuracy, this paper introduces a reinforcement learning framework for adaptive precision tuning in linear solvers, formulated as a contextual bandit problem. The method employs incremental action-value estimation with a discretized state space, using a Q-table to map features like condition number and matrix norm to precision configurations, optimized via an epsilon-greedy strategy to balance accuracy and cost. Experimental results on iterative refinement for linear systems show that the approach effectively reduces computational costs while maintaining accuracy comparable to double-precision baselines, generalizing well to unseen datasets and offering potential for broader application in mixed-precision numerical algorithms.</div>
<div class="mono" style="margin-top:8px">本文旨在提升科学计算中线性求解器的计算效率同时保持精度，提出了一个基于强化学习的自适应精度调优框架，将其建模为上下文赌博机问题。该方法采用离散化状态空间的增量动作值估计，通过Q表将条件数和矩阵范数等特征映射到精度配置，并利用ε-贪婪策略优化以平衡精度与成本。在线性系统迭代精化实验中的结果表明，该框架能有效降低计算成本，同时保持与双精度基准相当的精度，对未见数据集具有良好的泛化能力，为混合精度数值方法的更广泛应用提供了新思路。</div>
</details>
</div>
<div class="card">
<div class="title">GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA</div>
<div class="meta-line">Authors: Zhichao Wang</div>
<div class="meta-line">First: 2025-10-27T21:18:19+00:00 · Latest: 2026-01-05T03:09:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23868v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23868v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine \textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT minimizes the discrepancy between implicit and explicit reward models. It combines three key ideas: (1) the online multi-response generation and normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the implicit-explicit reward alignment principle of UNA. By jointly normalizing the implicit and explicit rewards, GIFT eliminates an otherwise intractable term that prevents effective use of implicit rewards. This normalization transforms the complex reward maximization objective into a simple mean squared error (MSE) loss between the normalized reward functions, converting a non-convex optimization problem into a convex, stable, and analytically differentiable formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy and thus retains exploration capability. Compared to GRPO, it requires fewer hyperparameters, converges faster, and generalizes better with significantly reduced training overfitting. Empirically, GIFT achieves superior reasoning and alignment performance on mathematical benchmarks while remaining computationally efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GIFT：基于群体相对隐式微调整合GRPO、DPO与UNA</div>
<div class="mono" style="margin-top:8px">本文提出一种新颖的强化学习框架——\textbf{群体相对隐式微调}（GIFT），用于大语言模型对齐。与PPO或GRPO直接最大化累积奖励不同，GIFT通过最小化隐式与显式奖励模型的差异实现优化。该框架融合三项核心思想：（1）GRPO的在线多响应生成与归一化机制；（2）DPO的隐式奖励建模方法；（3）UNA的隐式-显式奖励对齐原则。通过对隐式与显式奖励进行联合归一化，GIFT消除了阻碍隐式奖励有效使用的难解项，将复杂的奖励最大化目标转化为归一化奖励函数间的均方误差损失，从而将非凸优化问题转化为凸性、稳定且可解析求导的形式。相较于DPO、UNA等离线方法，GIFT保持在线策略特性并保留探索能力；相比GRPO，其超参数更少、收敛更快、泛化能力更强且显著降低训练过拟合。实验表明，GIFT在数学推理基准上取得更优的对齐性能，同时保持计算高效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GIFT, a reinforcement learning framework designed to align large language models by addressing the limitations of existing methods like PPO, GRPO, DPO, and UNA. Its motivation is to avoid directly maximizing cumulative rewards, instead minimizing the discrepancy between implicit and explicit reward models through a combination of GRPO&#x27;s online multi-response generation, DPO&#x27;s implicit reward formulation, and UNA&#x27;s alignment principle. The method jointly normalizes implicit and explicit rewards, transforming the optimization into a convex mean squared error loss, which simplifies training and enhances stability. Experimental results show that GIFT outperforms baselines in reasoning and alignment on mathematical benchmarks, with faster convergence, better generalization, reduced overfitting, and maintained computational efficiency while retaining on-policy exploration.</div>
<div class="mono" style="margin-top:8px">本文提出了GIFT，一种用于对齐大语言模型的强化学习框架，旨在克服PPO、GRPO、DPO和UNA等现有方法的局限性。其动机是通过结合GRPO的在线多响应生成、DPO的隐式奖励公式和UNA的对齐原则，避免直接最大化累积奖励，转而最小化隐式与显式奖励模型之间的差异。该方法对隐式和显式奖励进行联合归一化，将优化问题转化为凸的均方误差损失，从而简化训练并提高稳定性。实验结果表明，GIFT在数学基准测试中实现了更优的推理和对齐性能，具有更快的收敛速度、更好的泛化能力、减少的训练过拟合，并在保持计算效率的同时保留了在线策略探索能力。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-05T02:49:33+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但其仍易受对抗性越狱攻击影响——攻击者通过精心构造的细微扰动绕过安全机制，触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等局限，难以应用于现实黑盒场景。为此，我们提出基于零阶优化与同步扰动随机逼近（ZO-SPSA）的黑盒越狱攻击方法，具备三大优势：（1）通过输入-输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低GPU内存占用的资源需求。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，且扰动不可感知性与白盒方法相当。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱的现实可行性，并暴露了当前LVLMs安全机制的关键弱点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods that require full model access, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free approach approximates gradients through input-output interactions without needing internal model knowledge, making it model-agnostic and resource-efficient. Experimental results on models like InstructBLIP, LLaVA, and MiniGPT-4 show a high jailbreak success rate of up to 83.0% on InstructBLIP with imperceptible perturbations, and adversarial examples from MiniGPT-4 demonstrate strong transferability, achieving an attack success rate of 64.18% on other LVLMs, highlighting critical safety weaknesses in these systems.</div>
<div class="mono" style="margin-top:8px">本研究针对大型视觉语言模型易受对抗性越狱攻击的漏洞，以及现有白盒方法需要完整模型访问权限而不切实际的问题，提出了一种使用基于同时扰动随机逼近的零阶优化（ZO-SPSA）的黑盒攻击方法。这种无梯度方法通过输入-输出交互来近似梯度，无需了解模型内部知识，具有模型无关性和资源高效性。在InstructBLIP、LLaVA和MiniGPT-4等模型上的实验结果表明，该方法在InstructBLIP上实现了高达83.0%的越狱成功率且扰动难以察觉，同时从MiniGPT-4生成的对抗样本展现出强迁移性，在其他模型上攻击成功率可达64.18%，揭示了当前大型视觉语言模型安全机制的关键缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning</div>
<div class="meta-line">Authors: Pei Yang, Ke Zhang, Ji Wang, Xiao Chen, Yuxin Tang, Eric Yang, Lynn Ai, Bill Shi</div>
<div class="meta-line">First: 2025-11-20T10:12:34+00:00 · Latest: 2026-01-05T01:31:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16202v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.16202v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体协同奖励设计用于增强强化学习中的推理能力</div>
<div class="mono" style="margin-top:8px">我们提出了CRM（多智能体协同奖励模型），该框架通过一组协调的专家评估器替代单一黑盒奖励模型，以提升RLHF的鲁棒性和可解释性。传统奖励模型难以同时优化多个可能冲突的偏好维度（如事实性、帮助性、安全性），且评分依据缺乏透明度。CRM通过将偏好评估分解为各生成部分信号的领域特定智能体，并结合基于排序和嵌入相似性的全局评估器来解决这些问题。中央聚合器在每个时间步融合这些信号，平衡逐步正确性、多智能体一致性和重复惩罚等因素，生成与标准RL流程兼容的单一训练奖励。策略通过基于优势的更新（如GAE）进行优化，而价值模型回归至聚合奖励，实现在无需额外人工标注的情况下进行多视角奖励塑造。为支持训练与评估，我们引入了rewardBench——一个与CRM协同结构对齐的基准测试与训练套件。CRM与rewardBench共同为更透明的奖励建模和更稳定的优化提供了实用化、模块化的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces the Multi-Agent Collaborative Reward Model (CRM) to address limitations in conventional single reward models for reinforcement learning from human feedback (RLHF), which often lack robustness and interpretability when handling multiple, potentially conflicting preference dimensions like factuality and safety. CRM decomposes evaluation into specialized agents that produce partial signals, combined by a centralized aggregator balancing factors such as step-wise correctness and multi-agent agreement to generate a unified training reward compatible with standard RL pipelines. Experimental results, supported by the newly introduced rewardBench benchmark, demonstrate that CRM enhances transparency and optimization stability without requiring additional human annotations beyond those used to train the evaluators.</div>
<div class="mono" style="margin-top:8px">本文提出多智能体协作奖励模型（CRM），以解决强化学习人类反馈中传统单一奖励模型在同时优化多个可能冲突的偏好维度（如事实性、安全性）时缺乏鲁棒性和可解释性的问题。该方法将评估分解为多个专业智能体生成部分信号，并通过中央聚合器整合这些信号，平衡逐步正确性、多智能体一致性等因素，从而产生与标准强化学习流程兼容的统一训练奖励。实验结果表明，结合新引入的rewardBench基准，CRM在不需额外人工标注的情况下，提高了奖励建模的透明度和优化稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance</div>
<div class="meta-line">Authors: Ziheng Chen, Minxuan Hu, Jiayu Yi, Wenxi Sun</div>
<div class="meta-line">First: 2026-01-05T01:02:41+00:00 · Latest: 2026-01-05T01:02:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01709v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01709v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>期权对冲的强化学习：静态隐含波动率拟合与缺口感知性能对比</div>
<div class="mono" style="margin-top:8px">我们通过引入风险厌恶与交易成本扩展了Black-Scholes框架下的Q学习器（QLBS），并提出了一种新颖的期权定价复制学习（RLOP）方法。两种方法均与标准强化学习算法完全兼容，并在市场摩擦条件下运行。基于SPY与XOP期权数据的实验表明：自适应QLBS在隐含波动率空间实现了更高的静态定价精度，而RLOP通过降低缺口概率展现出更优的动态对冲性能。这些结果凸显了在静态拟合之外评估期权定价模型的重要性，并强调实际对冲效果的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for option pricing and hedging models that perform well under real-world market frictions, this paper extends the QLBS framework to include risk aversion and trading costs and introduces a new RLOP method, both compatible with standard reinforcement learning. The methods are evaluated on SPY and XOP option data, showing that Adaptive-QLBS achieves higher static accuracy in fitting implied volatilities, while RLOP provides superior dynamic hedging by significantly reducing the probability of hedging shortfall. These results underscore that assessing models based on realized hedging outcomes, rather than static fit alone, is crucial for practical performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是开发能在真实市场摩擦下有效进行期权定价和对冲的模型，为此扩展了QLBS框架以纳入风险厌恶和交易成本，并提出了新的RLOP方法，两者均与标准强化学习算法兼容。基于SPY和XOP期权数据的实验表明，Adaptive-QLBS在静态隐含波动率拟合上具有更高精度，而RLOP通过显著降低对冲短缺概率，实现了更优的动态对冲性能。这些结果强调，评估期权模型时应超越静态拟合，重点关注实际对冲结果，这对实践应用至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors</div>
<div class="meta-line">Authors: Yash Thesia, Meera Suthar</div>
<div class="meta-line">First: 2026-01-04T22:58:34+00:00 · Latest: 2026-01-04T22:58:34+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01688v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01688v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the &quot;Cold Start&quot; problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator&#x27;s latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique &quot;optimization trajectory&quot; of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiMEx：基于潜在扩散先验突破数据无关模型窃取冷启动壁垒</div>
<div class="mono" style="margin-top:8px">模型窃取攻击对机器学习即服务（MLaaS）构成根本性威胁，攻击者能以极低的训练成本复现专有模型。虽然数据无关模型窃取（DFME）已成为隐蔽的攻击途径，但其始终受限于“冷启动”问题：基于生成对抗网络（GAN）的攻击者需耗费数千次查询才能从随机噪声收敛至有效数据。本文提出DiMEx框架，通过利用预训练潜在扩散模型丰富的语义先验知识，彻底绕过该初始化壁垒。通过在生成器潜在空间中采用随机嵌入贝叶斯优化（REMBO），DiMEx能即时合成高保真查询，仅用2000次查询即在SVHN数据集上达到52.1%的模型一致性，超越当前最优GAN基线16个百分点。为应对这种高语义威胁，我们提出混合状态集成（HSE）防御机制，通过识别潜在空间攻击特有的“优化轨迹”实现检测。实验表明，DiMEx虽能规避静态分布检测器，但HSE利用其时序特征将攻击成功率压制至21.6%，且延迟可忽略不计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to overcome the cold start problem in data-free model extraction, which hinders GAN-based methods from efficiently generating useful queries, this paper introduces DiMEx, a method that leverages pre-trained latent diffusion models as semantic priors. The approach employs Random Embedding Bayesian Optimization in the latent space to synthesize high-fidelity queries immediately, bypassing slow convergence from random noise. Experimental results show DiMEx achieves 52.1% agreement on SVHN with only 2,000 queries, outperforming GAN baselines by over 16%, and while it evades static detectors, a proposed Hybrid Stateful Ensemble defense reduces its success rate to 21.6% by exploiting temporal attack signatures.</div>
<div class="mono" style="margin-top:8px">本文旨在解决数据自由模型窃取中的冷启动问题，该问题阻碍了基于GAN的方法高效生成有效查询，为此提出了DiMEx方法，利用预训练的潜在扩散模型作为语义先验。该方法在潜在空间中采用随机嵌入贝叶斯优化来立即合成高保真查询，避免了从随机噪声缓慢收敛的过程。实验结果表明，DiMEx仅用2000次查询就在SVHN上达到52.1%的一致性，优于GAN基线超过16%，同时，尽管它能规避静态检测器，但提出的混合状态集成防御通过利用攻击的时间特征将其成功率降低至21.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives</div>
<div class="meta-line">Authors: Wei Liu, Yaoxin Wu, Yingqian Zhang, Thomas Bäck, Yingjie Fan</div>
<div class="meta-line">First: 2026-01-04T20:57:43+00:00 · Latest: 2026-01-04T20:57:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01665v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多目标神经组合优化的对抗性实例生成与鲁棒训练</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）在解决多目标组合优化问题（MOCOPs）方面展现出巨大潜力。然而，这些基于学习的求解器的鲁棒性仍未得到充分探索，尤其是在多样且复杂的问题分布中。本文提出一个面向鲁棒性的统一框架，用于处理MOCOPs中偏好条件化的DRL求解器。在该框架内，我们开发了一种基于偏好的对抗攻击方法，用于生成暴露求解器弱点的困难实例，并通过帕累托前沿质量的退化程度量化攻击影响。我们进一步提出一种防御策略，将难度感知的偏好选择整合到对抗训练中，以减少对受限偏好区域的过拟合，并提升分布外性能。在多目标旅行商问题（MOTSP）、多目标容量约束车辆路径问题（MOCVRP）和多目标背包问题（MOKP）上的实验结果表明，我们的攻击方法能成功为不同求解器生成困难实例。此外，我们的防御方法显著增强了神经求解器的鲁棒性和泛化能力，在困难或分布外实例上实现了更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underexplored robustness of deep reinforcement learning (DRL) solvers for multi-objective combinatorial optimization problems (MOCOPs) across diverse problem distributions. The authors propose a unified robustness framework that includes a preference-based adversarial attack to generate hard instances, which exposes solver weaknesses by degrading Pareto-front quality, and a defense strategy integrating hardness-aware preference selection into adversarial training to reduce overfitting and improve out-of-distribution performance. Experiments on problems like MOTSP, MOCVRP, and MOKP confirm the attack&#x27;s effectiveness in identifying solver vulnerabilities, while the defense significantly enhances solver robustness and generalizability on challenging instances.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习求解多目标组合优化问题时，在不同问题分布下鲁棒性研究不足的现状，提出一个统一的鲁棒性框架。该框架包含一种基于偏好的对抗攻击方法，用于生成暴露求解器弱点的困难实例，并通过帕累托前沿质量下降量化攻击影响；同时提出一种防御策略，将难度感知的偏好选择融入对抗训练，以减少对受限偏好区域的过拟合并提升分布外性能。在多目标旅行商问题、多目标容量车辆路径问题和多目标背包问题上的实验表明，该攻击方法能有效为不同求解器生成困难实例，而防御方法显著增强了神经求解器的鲁棒性和泛化能力，在困难或分布外实例上表现优异。</div>
</details>
</div>
<div class="card">
<div class="title">Language Model Distillation: A Temporal Difference Imitation Learning Perspective</div>
<div class="meta-line">Authors: Zishun Yu, Shangzhe Li, Xinhua Zhang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-05-24T12:17:12+00:00 · Latest: 2026-01-04T18:51:37+00:00</div>
<div class="meta-line">Comments: AAAI 2026; Code available at: https://github.com/TobyLeelsz/Bellman-Distillation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20335v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.20335v4">PDF</a> · <a href="https://github.com/TobyLeelsz/Bellman-Distillation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have led to significant progress across many NLP tasks, although their massive sizes often incur substantial computational costs. Distillation has become a common practice to compress these large and highly capable models into smaller, more efficient ones. Many existing language model distillation methods can be viewed as behavior cloning from the perspective of imitation learning or inverse reinforcement learning. This viewpoint has inspired subsequent studies that leverage (inverse) reinforcement learning techniques, including variations of behavior cloning and temporal difference learning methods. Rather than proposing yet another specific temporal difference method, we introduce a general framework for temporal difference-based distillation by exploiting the distributional sparsity of the teacher model. Specifically, it is often observed that language models assign most probability mass to a small subset of tokens. Motivated by this observation, we design a temporal difference learning framework that operates on a reduced action space (a subset of vocabulary), and demonstrate how practical algorithms can be derived and the resulting performance improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型蒸馏：时序差分模仿学习视角</div>
<div class="mono" style="margin-top:8px">大型语言模型虽在众多自然语言处理任务中取得显著进展，但其庞大规模常伴随高昂计算成本。蒸馏技术已成为将大型高性能模型压缩为更小型高效模型的常用方法。现有许多语言模型蒸馏方法可从模仿学习或逆强化学习视角视为行为克隆，该观点启发了后续利用（逆）强化学习技术的研究，包括行为克隆变体与时序差分学习方法。本文未提出新的具体时序差分方法，而是通过利用教师模型的分布稀疏性，构建了基于时序差分的通用蒸馏框架。具体而言，语言模型常将大部分概率质量集中于少量词汇子集。基于此观察，我们设计了在缩减动作空间（词汇子集）上运行的时序差分学习框架，并展示了如何推导实用算法及其带来的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the high computational costs of large language models by improving knowledge distillation methods, which compress large models into smaller, efficient versions. The method introduces a general temporal difference learning framework for distillation that leverages the observation that teacher models often concentrate probability on a sparse subset of tokens; this allows the framework to operate on a reduced vocabulary action space, deriving practical algorithms. Experimental results demonstrate that this approach yields performance improvements over existing distillation techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过改进知识蒸馏方法来降低大型语言模型的高计算成本，这些方法将大模型压缩为更小、高效的版本。该方法引入了一个基于时序差分学习的通用蒸馏框架，利用教师模型通常将概率集中在稀疏令牌子集上的观察，使框架能在缩减的词汇动作空间上运行，并推导出实用算法。实验结果表明，该方法相比现有蒸馏技术带来了性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">General Dynamic Goal Recognition using Goal-Conditioned and Meta Reinforcement Learning</div>
<div class="meta-line">Authors: Osher Elhadad, Owen Morrissey, Reuth Mirsky</div>
<div class="meta-line">First: 2025-05-14T18:57:51+00:00 · Latest: 2026-01-04T16:58:04+00:00</div>
<div class="meta-line">Comments: Accepted for publication at AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.09737v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.09737v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding an agent&#x27;s goal through its behavior is a common AI problem called Goal Recognition (GR). This task becomes particularly challenging in dynamic environments where goals are numerous and ever-changing. We introduce the General Dynamic Goal Recognition (GDGR) problem, a broader definition of GR aimed at real-time adaptation of GR systems. This paper presents two novel approaches to tackle GDGR: (1) GC-AURA, generalizing to new goals using Model-Free Goal-Conditioned Reinforcement Learning, and (2) Meta-AURA, adapting to novel environments with Meta-Reinforcement Learning. We evaluate these methods across diverse environments, demonstrating their ability to achieve rapid adaptation and high GR accuracy under dynamic and noisy conditions. This work is a significant step forward in enabling GR in dynamic and unpredictable real-world environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于目标条件与元强化学习的通用动态目标识别</div>
<div class="mono" style="margin-top:8px">通过行为理解智能体的目标是人工智能中一个常见问题，称为目标识别（GR）。在目标众多且不断变化的动态环境中，该任务尤为困难。本文提出通用动态目标识别（GDGR）问题，这是对GR的更广泛定义，旨在实现GR系统的实时适应。论文提出两种新方法应对GDGR：（1）GC-AURA，利用无模型目标条件强化学习泛化至新目标；（2）Meta-AURA，通过元强化学习适应新环境。我们在多种环境中评估这些方法，证明其能在动态嘈杂条件下实现快速适应与高精度目标识别。这项研究为推动目标识别在动态不可预测的真实环境中的应用迈出了重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of Goal Recognition (GR) in dynamic environments where goals are numerous and changing, introducing the General Dynamic Goal Recognition (GDGR) problem to enable real-time adaptation. Two novel methods are proposed: GC-AURA, which uses goal-conditioned reinforcement learning to generalize to new goals, and Meta-AURA, which employs meta-reinforcement learning to adapt to novel environments. Experimental results across diverse settings show that both approaches achieve rapid adaptation and high accuracy in goal recognition, even under dynamic and noisy conditions, marking a significant advance for real-world applications.</div>
<div class="mono" style="margin-top:8px">本文针对动态环境中目标众多且不断变化的目标识别（GR）挑战，提出了广义动态目标识别（GDGR）问题，以实现实时适应。研究提出了两种新方法：GC-AURA利用目标条件强化学习泛化到新目标，Meta-AURA采用元强化学习适应新环境。在多种环境中的实验结果表明，这两种方法都能在动态和噪声条件下实现快速适应和高精度的目标识别，为现实世界应用提供了重要进展。</div>
</details>
</div>
<div class="card">
<div class="title">HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller</div>
<div class="meta-line">Authors: Tran Tien Dat, Nguyen Hai An, Nguyen Khanh Viet Dung, Nguyen Duy Duc</div>
<div class="meta-line">First: 2026-01-04T15:49:46+00:00 · Latest: 2026-01-04T15:49:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01577v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01577v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HanoiWorld：基于联合嵌入预测架构的自动驾驶控制器世界模型</div>
<div class="mono" style="margin-top:8px">当前强化学习在自动驾驶控制器中的应用存在数据需求量大、性能欠佳、稳定性不足、难以把握安全核心概念，以及因像素重建特性而过度关注噪声特征等问题。而基于联合嵌入预测架构的自监督学习方法通过高维表征学习，模拟人脑利用想象力和少量观察样本获取新技能的自然能力，成为一种有趣且有效的替代方案。本研究提出HanoiWorld——一种基于JEPA的世界模型，采用循环神经网络进行长期水平规划并实现高效推理。在Highway-Env多环境测试中，该模型展现出安全感知的驾驶规划能力，与前沿基线相比显著降低了碰撞率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the data inefficiency, instability, and safety shortcomings of pixel-reconstruction-based reinforcement learning for autonomous driving, this study proposes HanoiWorld, a world model based on the Joint Embedding Predictive Architecture (JEPA) to learn high-dimensional representations, mimicking human-like imagination for skill acquisition. The method employs a recurrent neural network (RNN) within the JEPA framework to enable long-term horizontal planning with efficient inference. Experimental results on varied Highway-Env environments demonstrate that the model achieves effective, safety-aware driving plans, showing a considerable reduction in collision rates compared to state-of-the-art baselines.</div>
<div class="mono" style="margin-top:8px">针对基于像素重建的强化学习方法在自动驾驶控制中存在数据需求高、性能不稳定、难以把握安全概念以及对噪声特征过度关注的问题，本研究受人类利用想象力和少量观察学习新技能的能力启发，引入了HanoiWorld，一种基于联合嵌入预测架构的世界模型。该方法在JEPA框架内使用循环神经网络进行长期水平规划，并实现高效推理。在Highway-Env不同环境中的实验结果表明，该模型能够制定有效且具有安全意识的驾驶计划，与最先进的基线相比，碰撞率显著降低。</div>
</details>
</div>
<div class="card">
<div class="title">Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement</div>
<div class="meta-line">Authors: Mingyu Xu, Cheng Fang, Keyue Jiang, Yuqian Zheng, Yanghua Xiao, Baojian Zhou, Qifang Zhao, Suhang Zheng, Xiuwen Zhu, Jiyang Tang, Yongchi Zhao, Yijia Luo, Zhiqi Bai, Yuchi Xu, Wenbo Su, Wei Wang, Bing Zhao, Lin Qu, Xiaoxiao Xu</div>
<div class="meta-line">First: 2026-01-04T15:23:18+00:00 · Latest: 2026-01-04T15:23:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01562v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01562v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Logics-STEM：通过失败驱动的后训练与文档知识增强赋能大语言模型推理</div>
<div class="mono" style="margin-top:8px">我们提出Logics-STEM，这是一个基于Logics-STEM-SFT-Dataset微调的前沿推理模型。该数据集规模达1000万，质量高且多样性丰富，是最大规模的开源长链思维语料库之一。Logics-STEM专注于科学、技术、工程和数学（STEM）领域的推理任务，在STEM相关基准测试中表现卓越，在80亿参数规模上平均优于次优模型4.68%。我们将性能提升归因于数据-算法协同设计引擎，通过联合优化使模型拟合推理背后的黄金标准分布。数据层面，Logics-STEM-SFT-Dataset通过精心设计的五阶段数据策管引擎构建（包括标注、去重、净化、蒸馏和分层采样），确保质量、多样性与可扩展性。算法层面，我们的失败驱动后训练框架在监督微调阶段，针对模型失败区域进行定向知识检索与数据合成，有效指导第二阶段SFT或强化学习以更好拟合目标分布。Logics-STEM的卓越实证性能揭示了大规模开源数据与精心设计合成数据结合的巨大潜力，凸显了数据-算法协同设计通过后训练增强推理能力的关键作用。我们公开提供Logics-STEM模型（80亿和320亿参数）及Logics-STEM-SFT-Dataset（1000万和降采样220万版本），以支持开源社区的未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Logics-STEM, a reasoning model designed to enhance large language model (LLM) performance in STEM domains by addressing limitations in existing reasoning capabilities. The method employs a data-algorithm co-design engine, constructing a high-quality 10M-scale dataset through a multi-stage curation process and implementing a failure-driven post-training framework that synthesizes targeted data around model errors to refine supervised fine-tuning or reinforcement learning. Experimental results show that Logics-STEM achieves state-of-the-art performance on STEM benchmarks, with an average improvement of 4.68% over the next-best 8B-scale model, demonstrating the effectiveness of combining large-scale open-source data with synthetic data for reasoning enhancement.</div>
<div class="mono" style="margin-top:8px">本文提出了Logics-STEM模型，旨在通过解决现有推理能力的不足，提升大语言模型在科学、技术、工程和数学（STEM）领域的推理性能。方法采用数据与算法协同设计引擎，通过多阶段数据构建流程创建了一个高质量、规模达1000万的数据集，并实施了一种基于失败驱动的后训练框架，该框架围绕模型错误区域合成针对性数据，以优化监督微调或强化学习。实验结果表明，Logics-STEM在STEM相关基准测试中取得了最先进的性能，相比次优的80亿参数模型平均提升4.68%，验证了结合大规模开源数据与合成数据以增强推理能力的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Preconditioned Inexact Stochastic ADMM for Deep Model</div>
<div class="meta-line">Authors: Shenglong Zhou, Ouya Wang, Ziyan Luo, Yongxu Zhu, Geoffrey Ye Li</div>
<div class="meta-line">First: 2025-02-15T12:28:51+00:00 · Latest: 2026-01-04T11:54:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.10784v6">Abs</a> · <a href="https://arxiv.org/pdf/2502.10784v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers). Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient on a bounded region, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables the proposed algorithm to tackle the challenge of data heterogeneity effectively. Moreover, the algorithmic architecture enables scalable parallel computing and supports various preconditions, such as second-order information, second moment, and orthogonalized momentum by Newton-Schulz iterations. Incorporating the latter two preconditions in PISA yields two computationally efficient variants: SISA and NSISA. Comprehensive experimental evaluations for training or fine-tuning diverse deep models, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate superior numerical performance of SISA and NSISA compared to various state-of-the-art optimizers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向深度模型的预条件化非精确随机交替方向乘子法</div>
<div class="mono" style="margin-top:8px">基础模型的近期发展引发了范式转变，深刻变革了全球各领域。训练此类模型的主流优化器基于随机梯度下降算法，存在收敛速度慢、收敛假设条件严苛等固有局限。分布式环境中的数据异质性尤其对其理论与数值性能构成显著挑战。本文提出PISA算法（预条件化非精确随机交替方向乘子法）。该算法基于严格的理论保证，仅需梯度在有界区域满足Lipschitz连续性假设即可收敛，从而免除了随机方法通常依赖的其他条件。此特性使算法能有效应对数据异质性挑战。此外，算法架构支持可扩展并行计算，兼容多种预条件设置，如二阶信息、二阶矩及通过Newton-Schulz迭代实现的正交动量。将后两种预条件融入PISA可衍生两个计算高效变体：SISA与NSISA。通过对视觉模型、大语言模型、强化学习模型、生成对抗网络及循环神经网络等多样化深度模型进行训练与微调的全面实验评估，证明SISA和NSISA相较多种前沿优化器具有更优越的数值性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of stochastic gradient descent-based optimizers for training foundation models, particularly their slow convergence and sensitivity to data heterogeneity in distributed settings, this paper introduces PISA, a Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers. The method is theoretically guaranteed to converge under minimal assumptions (Lipschitz continuity of the gradient on a bounded region) and supports scalable parallel computing with various preconditioners, including second-order information and momentum, leading to efficient variants SISA and NSISA. Experimental results across diverse deep models—such as vision models, large language models, and generative adversarial networks—demonstrate that SISA and NSISA outperform state-of-the-art optimizers in numerical performance.</div>
<div class="mono" style="margin-top:8px">针对基于随机梯度下降的优化器在训练基础模型时存在的收敛慢、对分布式环境中的数据异构性敏感等局限性，本文提出了PISA算法，即预处理非精确随机交替方向乘子法。该方法在仅需梯度在有界区域上Lipschitz连续的弱假设下具有理论收敛保证，支持可扩展的并行计算，并能结合多种预处理技术（如二阶信息、动量等），从而衍生出计算高效的变体SISA和NSISA。在视觉模型、大语言模型、生成对抗网络等多种深度模型的训练或微调实验中，SISA和NSISA展现出优于现有先进优化器的数值性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning via Conservative Agent for Environments with Random Delays</div>
<div class="meta-line">Authors: Jongsoo Lee, Jangwon Kim, Jiseok Jeong, Soohee Han</div>
<div class="meta-line">First: 2025-07-25T06:41:06+00:00 · Latest: 2026-01-04T10:57:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.18992v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.18992v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于保守智能体的随机延迟环境强化学习</div>
<div class="mono" style="margin-top:8px">现实世界中的强化学习应用常受环境反馈延迟的制约，这种延迟违背了马尔可夫假设并带来显著挑战。尽管针对固定延迟环境已提出多种延迟补偿方法，但由于随机延迟固有的多变性和不可预测性，相关研究仍较为匮乏。本研究提出一种适用于随机延迟决策的简洁而鲁棒的智能体——保守智能体，其将随机延迟环境重构为等效的固定延迟环境。该转换使得任何先进的固定延迟方法无需修改算法结构或牺牲性能，即可直接扩展至随机延迟环境。我们在连续控制任务上评估了基于保守智能体的算法，实证结果表明其在渐进性能和样本效率方面显著优于现有基线算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying reinforcement learning in environments with random delays, which break the Markov property and are less studied than constant delays. The authors propose a conservative agent that transforms a random-delay environment into an equivalent constant-delay formulation, allowing existing state-of-the-art constant-delay methods to be directly applied without structural changes. Experimental results on continuous control tasks show that this approach significantly improves asymptotic performance and sample efficiency over baseline algorithms.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在随机延迟环境中的应用挑战展开研究，随机延迟会破坏马尔可夫性且比恒定延迟更难处理。作者提出了一种保守智能体，将随机延迟环境转化为等效的恒定延迟形式，使得现有先进的恒定延迟方法无需修改即可直接应用。在连续控制任务上的实验结果表明，该方法在渐进性能和样本效率上均显著优于现有基线算法。</div>
</details>
</div>
<div class="card">
<div class="title">Detecting Proxy Gaming in RL and LLM Alignment via Evaluator Stress Tests</div>
<div class="meta-line">Authors: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</div>
<div class="meta-line">First: 2025-07-08T03:00:02+00:00 · Latest: 2026-01-04T09:53:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05619v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.05619v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Proxy optimization, where AI systems exploit evaluator weaknesses rather than improve intended objectives, threatens both reinforcement learning (reward hacking) and LLM alignment (evaluator gaming). We introduce the Evaluator Stress Test (EST), an invariance-based framework that detects proxy gaming by separating exploitable sensitivity (e.g., formatting artifacts, physics bugs) from content-driven improvements using controlled perturbations with semantic validity audits. We validate EST across both domains. In RL, across 15 environments and 5 algorithms (2,156 expert-annotated episodes), EST achieves 78.4% precision and 81.7% recall. In LLM alignment, across 4 tasks, 2 model scales, 2 training methods, and 2 judges (1,200 human-annotated instances), EST achieves 74.2% precision and 78.6% recall, with early warning signals that precede quality decline. Cross-domain analysis shows that proxy-true correlation tracking transfers directly between domains, while perturbation design requires domain adaptation. Closed-loop mitigation improves human win-rate by 8.3 points (LLM) and reduces hacking by 54.6% (RL). We release benchmarks for both domains: 2,156 RL episodes and 1,200 LLM instances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过评估器压力测试检测强化学习与大语言模型对齐中的代理博弈</div>
<div class="mono" style="margin-top:8px">代理优化指AI系统利用评估器弱点而非提升目标指标的现象，威胁强化学习（奖励攻击）与大语言模型对齐（评估器博弈）。本文提出评估器压力测试（EST），一种基于不变性的框架，通过结合语义有效性审核的受控扰动，分离可被利用的敏感性（如格式伪影、物理漏洞）与内容驱动的改进，从而检测代理博弈。我们在两个领域验证了EST：在强化学习中，覆盖15个环境和5种算法（2,156条专家标注片段），EST实现78.4%精确率和81.7%召回率；在大语言模型对齐中，覆盖4项任务、2种模型规模、2种训练方法和2类评估者（1,200条人工标注实例），EST实现74.2%精确率和78.6%召回率，并能提前发出质量下降预警。跨领域分析表明，代理-真实相关性追踪可直接迁移，而扰动设计需领域适配。闭环缓解措施使人类胜率提升8.3个百分点（大语言模型），攻击行为减少54.6%（强化学习）。我们发布了两领域基准数据集：2,156条强化学习片段和1,200条大语言模型实例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of proxy gaming, where AI systems exploit evaluator weaknesses instead of genuinely improving on intended objectives, a threat in both reinforcement learning (reward hacking) and large language model alignment (evaluator gaming). To detect such behavior, the authors propose the Evaluator Stress Test (EST), a framework that identifies proxy gaming by applying controlled perturbations and conducting semantic validity audits to separate exploitable sensitivities from content-driven improvements. Experimental validation across domains shows EST achieves 78.4% precision and 81.7% recall in RL across 15 environments and 5 algorithms, and 74.2% precision and 78.6% recall in LLM alignment across 4 tasks, with early warning signals preceding quality decline; closed-loop mitigation improves human win-rate by 8.3 points for LLMs and reduces hacking by 54.6% in RL.</div>
<div class="mono" style="margin-top:8px">本文针对代理游戏问题展开研究，即AI系统利用评估者弱点而非真正提升目标性能，这在强化学习（奖励黑客）和大语言模型对齐（评估者游戏）中均构成威胁。为检测此类行为，作者提出了评估者压力测试框架，通过施加受控扰动并进行语义有效性审核，以区分可被利用的敏感性与内容驱动的改进。跨领域实验验证表明，在强化学习的15个环境和5种算法中，该框架达到78.4%的精确率和81.7%的召回率；在大语言模型对齐的4项任务中，达到74.2%的精确率和78.6%的召回率，并能提前发出质量下降预警；闭环缓解措施使大语言模型的人类胜率提升8.3个百分点，并将强化学习中的黑客行为减少54.6%。</div>
</details>
</div>
<div class="card">
<div class="title">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</div>
<div class="meta-line">Authors: DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang</div>
<div class="meta-line">Venue: Nature volume 645, pages 633-638 (2025)</div>
<div class="meta-line">First: 2025-01-22T15:19:35+00:00 · Latest: 2026-01-04T03:57:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.12948v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.12948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">General reasoning represents a long-standing and formidable challenge in artificial intelligence. Recent breakthroughs, exemplified by large language models (LLMs) and chain-of-thought prompting, have achieved considerable success on foundational reasoning tasks. However, this success is heavily contingent upon extensive human-annotated demonstrations, and models&#x27; capabilities are still insufficient for more complex problems. Here we show that the reasoning abilities of LLMs can be incentivized through pure reinforcement learning (RL), obviating the need for human-labeled reasoning trajectories. The proposed RL framework facilitates the emergent development of advanced reasoning patterns, such as self-reflection, verification, and dynamic strategy adaptation. Consequently, the trained model achieves superior performance on verifiable tasks such as mathematics, coding competitions, and STEM fields, surpassing its counterparts trained via conventional supervised learning on human demonstrations. Moreover, the emergent reasoning patterns exhibited by these large-scale models can be systematically harnessed to guide and enhance the reasoning capabilities of smaller models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepSeek-R1：通过强化学习激励大语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">通用推理是人工智能领域长期存在的一项艰巨挑战。以大型语言模型和思维链提示为代表的近期突破，已在基础推理任务上取得了显著成功。然而，这种成功严重依赖于大量人工标注的演示，且模型处理更复杂问题的能力仍显不足。本文研究表明，通过纯强化学习即可激励大语言模型的推理能力，无需依赖人工标注的推理轨迹。所提出的强化学习框架促进了高级推理模式（如自我反思、验证和动态策略调整）的涌现发展。因此，经训练的模型在数学、编程竞赛和STEM领域等可验证任务上表现卓越，超越了通过传统监督学习基于人类演示训练的同类模型。此外，这些大规模模型展现的涌现推理模式可被系统化利用，以指导和增强较小模型的推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of large language models that rely heavily on human-annotated demonstrations for reasoning tasks, this paper introduces DeepSeek-R1, a method that uses pure reinforcement learning to incentivize and develop advanced reasoning capabilities without human-labeled reasoning trajectories. The proposed RL framework encourages the emergence of sophisticated reasoning patterns like self-reflection and verification, leading to superior experimental results on mathematics, coding, and STEM benchmarks, outperforming models trained with supervised learning on human demonstrations. Additionally, the reasoning patterns learned by large models can be systematically transferred to enhance the capabilities of smaller models.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型在推理任务上严重依赖人工标注演示的局限性，本文提出了DeepSeek-R1方法，通过纯强化学习激励模型发展推理能力，无需人类标注的推理轨迹。该强化学习框架促进了自我反思、验证等高级推理模式的出现，实验结果表明，在数学、编程竞赛和STEM领域等可验证任务上，其性能超越了基于人类演示的监督学习训练的模型。此外，大规模模型涌现出的推理模式可系统性地用于指导和提升较小模型的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization</div>
<div class="meta-line">Authors: Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou</div>
<div class="meta-line">First: 2025-08-11T05:17:51+00:00 · Latest: 2026-01-04T03:18:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07629v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07629v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model&#x27;s exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5% on AIME 2024, 83.2% on AIME 2025, 66.0% on LiveCodeBench V5 and 58.1% on LiveCodeBench V6.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Klear-Reasoner：通过梯度保留裁剪策略优化提升推理能力</div>
<div class="mono" style="margin-top:8px">我们提出Klear-Reasoner，一种具备长推理能力的模型，在解决问题时展现出审慎的思考过程，并在多个基准测试中取得卓越性能。尽管当前社区已有许多优秀的推理模型相关研究，但由于训练细节披露不完整，复现高性能推理模型仍存在诸多困难。本报告对推理模型进行了深入分析，涵盖从数据准备、长思维链监督微调（长CoT SFT）到强化学习（RL）的完整训练后工作流程，并对每个实验模块进行了详细的消融研究。在SFT数据方面，实验表明少量高质量数据源比大量多样化数据源更有效，且困难样本无需精度筛选即可获得更好效果。此外，我们研究了当前RL裁剪机制的两个关键问题：裁剪抑制关键探索信号并忽略次优轨迹。针对这些挑战，我们提出梯度保留裁剪策略优化（GPPO），可温和地反向传播来自裁剪标记的梯度。GPPO不仅增强了模型的探索能力，还提升了其从负样本中学习的效率。Klear-Reasoner在数学和编程领域展现出卓越推理能力，在AIME 2024得分90.5%、AIME 2025得分83.2%、LiveCodeBench V5得分66.0%、LiveCodeBench V6得分58.1%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for developing Klear-Reasoner is to enhance long-chain reasoning capabilities and address reproducibility issues in high-performance reasoning models by providing a comprehensive training workflow analysis. The method involves a detailed post-training pipeline including data preparation, long Chain-of-Thought supervised fine-tuning (SFT), and a novel reinforcement learning approach called Gradient-Preserving clipping Policy Optimization (GPPO), which mitigates issues in standard clipping by gently backpropagating gradients from clipped tokens to preserve exploration signals and utilize suboptimal trajectories. Main experimental results demonstrate that using a small set of high-quality, difficult SFT data without accuracy filtering is effective, and GPPO improves exploration and learning from negative samples, leading to outstanding performance with scores of 90.5% on AIME 2024, 83.2% on AIME 2025, 66.0% on LiveCodeBench V5, and 58.1% on LiveCodeBench V6.</div>
<div class="mono" style="margin-top:8px">开发Klear-Reasoner的动机是通过提供全面的训练工作流程分析，来增强长链推理能力并解决高性能推理模型的可复现性问题。该方法包括详细的后训练流程，涵盖数据准备、长思维链监督微调（SFT）以及一种新颖的强化学习方法——梯度保留裁剪策略优化（GPPO），该方法通过温和地反向传播裁剪标记的梯度来缓解标准裁剪中的问题，从而保留探索信号并利用次优轨迹。主要实验结果表明，使用少量高质量、高难度的SFT数据且不进行准确性过滤是有效的，GPPO提升了探索能力和从负样本中学习的效率，最终在多个基准测试中取得优异表现，在AIME 2024上得分90.5%，AIME 2025上83.2%，LiveCodeBench V5上66.0%，LiveCodeBench V6上58.1%。</div>
</details>
</div>
<div class="card">
<div class="title">PyBatchRender: A Python Library for Batched 3D Rendering at Up to One Million FPS</div>
<div class="meta-line">Authors: Evgenii Rudakov, Jonathan Shock, Benjamin Ultan Cowley</div>
<div class="meta-line">First: 2026-01-03T21:19:57+00:00 · Latest: 2026-01-03T21:19:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01288v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01288v1">PDF</a> · <a href="https://github.com/dolphin-in-a-coma/PyBatchRender">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from pixels is often bottlenecked by the performance and complexity of 3D rendered environments. Researchers face a trade-off between high-speed, low-level engines and slower, more accessible Python frameworks. To address this, we introduce PyBatchRender, a Python library for high-throughput, batched 3D rendering that achieves over 1 million FPS on simple scenes. Built on the Panda3D game engine, it utilizes its mature ecosystem while enhancing performance through optimized batched rendering for up to 1000X speedups. Designed as a physics-agnostic renderer for reinforcement learning from pixels, PyBatchRender offers greater flexibility than dedicated libraries, simpler setup than typical game-engine wrappers, and speeds rivaling state-of-the-art C++ engines like Madrona. Users can create custom scenes entirely in Python with tens of lines of code, enabling rapid prototyping for scalable AI training. Open-source and easy to integrate, it serves to democratize high-performance 3D simulation for researchers and developers. The library is available at https://github.com/dolphin-in-a-coma/PyBatchRender.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PyBatchRender：实现高达百万FPS批量3D渲染的Python库</div>
<div class="mono" style="margin-top:8px">基于像素的强化学习常受限于3D渲染环境的性能与复杂度。研究人员不得不在高速低层引擎与低速易用的Python框架间权衡。为此，我们推出PyBatchRender——一个支持高吞吐批量3D渲染的Python库，在简单场景中可实现超100万FPS。该库基于Panda3D游戏引擎构建，在利用其成熟生态系统的同时，通过优化批量渲染实现最高1000倍的性能提升。作为专为像素强化学习设计的物理无关渲染器，PyBatchRender比专用库更灵活，比常规游戏引擎封装更易部署，其速度可媲美Madrona等先进C++引擎。用户仅需数十行Python代码即可创建自定义场景，为可扩展AI训练提供快速原型支持。该开源库易于集成，旨在为研究者和开发者普及高性能3D仿真技术。项目地址：https://github.com/dolphin-in-a-coma/PyBatchRender。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance bottleneck and complexity trade-offs in 3D rendering for pixel-based reinforcement learning, this paper introduces PyBatchRender, a Python library built on Panda3D that optimizes batched rendering to achieve speeds up to one million FPS on simple scenes. The method leverages the mature Panda3D ecosystem while implementing performance enhancements that yield up to 1000X speedups, offering a physics-agnostic renderer designed for flexibility and ease of use compared to dedicated libraries or complex game-engine wrappers. Experimental results demonstrate that the library rivals the speed of state-of-the-art C++ engines like Madrona, allows custom scene creation in Python with minimal code for rapid prototyping, and is open-source to democratize high-performance 3D simulation for AI research.</div>
<div class="mono" style="margin-top:8px">本文针对基于像素的强化学习中3D渲染的性能瓶颈和复杂性权衡问题，提出了PyBatchRender，这是一个基于Panda3D构建的Python库，通过优化批处理渲染，在简单场景中实现高达每秒一百万帧的渲染速度。该方法利用成熟的Panda3D生态系统，同时实施性能增强，带来高达1000倍的加速，提供了一个与物理引擎无关的渲染器，相比专用库或复杂的游戏引擎封装，更具灵活性和易用性。实验结果表明，该库在速度上可与Madrona等先进C++引擎相媲美，允许用少量Python代码快速创建自定义场景以进行原型设计，并且开源发布，旨在为AI研究普及高性能3D仿真。</div>
</details>
</div>
<div class="card">
<div class="title">Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning</div>
<div class="meta-line">Authors: Alexander W. Goodall, Edwin Hamel-De le Court, Francesco Belardinelli</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-11-13T23:06:40+00:00 · Latest: 2026-01-03T20:35:02+00:00</div>
<div class="meta-line">Comments: Main Track at AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10843v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.10843v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为策略优化：为离线策略强化学习提供可证明更低方差的回报估计</div>
<div class="mono" style="margin-top:8px">许多依赖回报估计进行策略改进的强化学习算法，常因高方差回报估计导致样本效率低下和训练不稳定。本文利用离线策略评估的最新研究成果：近期研究表明，精心设计的行为策略可用于收集离线数据，从而获得可证明更低方差的回报估计。这一发现令人意外，意味着按策略收集数据并非方差最优方案。我们将这一核心洞见扩展至在线强化学习场景，其中策略评估与改进交替进行以学习最优策略。离线策略强化学习已有深入研究（如IMPALA），通过校正截断重要性加权样本实现去偏与方差控制。传统方法主要解决多并行工作器数据同步问题，通过数学严谨的方式修正工作器与策略间的异步差异。本文仅考虑单一工作器——即行为策略，用于收集策略改进数据，并获得可证明更低方差的回报估计。实验中，我们将该机制拓展至两种策略梯度方法，在多样化环境中验证了其更优的样本效率与性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of high-variance return estimates in off-policy reinforcement learning, which often leads to poor sample efficiency and instability. The method introduces Behaviour Policy Optimization, which leverages the insight that well-designed behavior policies, rather than strictly on-policy collection, can provably yield lower-variance return estimates for policy improvement. Experimental results on extending two policy-gradient methods with this approach demonstrate improved sample efficiency and performance across diverse environments.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决离线策略强化学习中高方差回报估计导致的样本效率低下和训练不稳定问题。方法上提出了行为策略优化，利用精心设计的行为策略而非严格在线策略收集数据，可证明地获得更低方差的回报估计以优化策略。实验结果表明，将两种策略梯度方法扩展应用此框架后，在多种环境中均表现出更好的样本效率和性能。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Expectations: Learning with Stochastic Dominance Made Practical</div>
<div class="meta-line">Authors: Shicong Cen, Jincheng Mei, Hanjun Dai, Dale Schuurmans, Yuejie Chi, Bo Dai</div>
<div class="meta-line">First: 2024-02-05T03:21:23+00:00 · Latest: 2026-01-03T20:15:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.02698v2">Abs</a> · <a href="https://arxiv.org/pdf/2402.02698v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stochastic dominance serves as a general framework for modeling a broad spectrum of decision preferences under uncertainty, with risk aversion as one notable example, as it naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply resorting to the expectations. Despite theoretical appeal, the application of stochastic dominance in machine learning has been scarce, due to the following challenges: $\textbf{i)}$, the original concept of stochastic dominance only provides a $\textit{partial order}$, and therefore, is not amenable to serve as a general optimality criterion; and $\textbf{ii)}$, an efficient computational recipe remains lacking due to the continuum nature of evaluating stochastic dominance.
  In this work, we make the first attempt towards establishing a general framework of learning with stochastic dominance. We first generalize the stochastic dominance concept to enable feasible comparisons between any arbitrary pair of random variables. We next develop a simple and computationally efficient approach for finding the optimal solution in terms of stochastic dominance, which can be seamlessly plugged into many learning tasks. Numerical experiments demonstrate that the proposed method achieves comparable performance as standard risk-neutral strategies and obtains better trade-offs against risk across a variety of applications including supervised learning, reinforcement learning, and portfolio optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越期望：实用化的随机占优学习框架</div>
<div class="mono" style="margin-top:8px">随机占优作为不确定性下广泛决策偏好的通用建模框架（风险厌恶即典型例证），其优势在于能自然捕捉底层不确定性的内在结构，而非简单依赖期望值。尽管理论价值显著，随机占优在机器学习中的应用仍稀缺，主要面临两大挑战：一、原始随机占优概念仅提供偏序关系，难以作为普适的最优性准则；二、因评估随机占优具有连续统特性，目前缺乏高效的计算方案。本研究首次尝试构建随机占优学习的通用框架：首先推广随机占优概念以实现任意随机变量间的可行比较；继而开发简洁高效的计算方法以获取随机占优意义下的最优解，该方法可无缝嵌入多种学习任务。数值实验表明，所提方法在监督学习、强化学习和投资组合优化等应用中，既达到与标准风险中性策略相当的性能，又能在风险权衡方面取得更优表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underutilization of stochastic dominance in machine learning due to its partial order nature and computational inefficiency, aiming to create a practical framework for decision-making under uncertainty that goes beyond mere expectation optimization. The authors generalize stochastic dominance to allow comparisons between any random variables and develop a computationally efficient method for finding stochastically dominant solutions that can be integrated into various learning tasks. Experimental results across supervised learning, reinforcement learning, and portfolio optimization show that the proposed method matches the performance of risk-neutral approaches while achieving superior risk-return trade-offs.</div>
<div class="mono" style="margin-top:8px">本文针对随机占优在机器学习中因偏序性质和计算效率低下而应用不足的问题，旨在建立一个超越期望优化的实用不确定性决策框架。作者推广了随机占优概念以实现任意随机变量间的比较，并开发了一种计算高效的方法来寻找随机占优解，可无缝集成到多种学习任务中。在监督学习、强化学习和投资组合优化等应用中的实验结果表明，所提方法在保持与风险中性策略相当性能的同时，实现了更优的风险收益权衡。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
