<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-07 04:00</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260207_0400</div>
    <div class="row"><div class="card">
<div class="title">Can vision language models learn intuitive physics from interaction?</div>
<div class="meta-line">Authors: Luca M. Schulze Buschoff, Konstantinos Voudouris, Can Demircan, Eric Schulz</div>
<div class="meta-line">First: 2026-02-05T18:59:20+00:00 · Latest: 2026-02-05T18:59:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06033v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06033v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型能否通过交互学习直观物理？</div>
<div class="mono" style="margin-top:8px">预训练的视觉语言模型对物理世界缺乏良好的直觉。近期研究表明，监督微调能提升模型在简单物理任务上的表现，但微调后的模型并未学到可泛化至新情境的稳健物理规则。基于认知科学研究，我们假设模型需通过与环境交互来正确学习其物理动态。我们使用强化学习训练模型，使其通过环境交互进行学习。尽管交互学习能提升模型在任务内的表现，但未能形成具有泛化性的物理直觉。研究发现，在单一任务上训练的模型无法可靠地泛化至相关任务，即使这些任务共享视觉统计特征与物理原理，且无论模型是否通过交互训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether vision-language models can acquire robust physical intuitions through interaction, motivated by their known limitations in understanding the physical world and the failure of supervised fine-tuning to yield generalizable rules. The method involves training models using reinforcement learning to interact with an environment, aiming to learn physical dynamics directly from experience. The main experimental results show that while interaction improves performance on the specific training task, it does not lead to models that generalize reliably to related tasks sharing visual and physical properties, indicating a lack of robust, transferable physical understanding.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型能否通过交互学习获得稳健的物理直觉，其动机在于现有模型在理解物理世界方面的不足，以及监督微调方法未能产生可泛化的物理规则。研究方法采用强化学习训练模型与环境交互，旨在从经验中直接学习物理动态。主要实验结果表明，尽管交互提高了模型在特定训练任务上的表现，但模型无法可靠地泛化到具有相似视觉统计特征和物理原理的相关任务中，这表明模型未能形成稳健且可迁移的物理理解。</div>
</details>
</div>
<div class="card">
<div class="title">Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</div>
<div class="meta-line">Authors: Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan</div>
<div class="meta-line">First: 2026-02-05T18:58:32+00:00 · Latest: 2026-02-05T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06029v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>好奇心即知识：基于主动推断的自洽学习与无遗憾优化</div>
<div class="mono" style="margin-top:8px">主动推断通过最小化期望自由能，借助好奇心系数平衡认知价值（信息增益）与实用价值（任务性能），统一了探索与利用。然而，这种平衡何时能同时实现连贯的学习与高效的决策一直不明确：好奇心不足可能导致短视的利用并阻碍不确定性消解，而过度的好奇心则可能引发不必要的探索与遗憾。我们首次为最小化期望自由能的智能体建立了理论保证，证明单一条件——足够的好奇心——即可同时确保自洽学习（贝叶斯后验一致性）和无遗憾优化（累积遗憾有界）。分析揭示了该机制如何依赖于初始不确定性、可识别性与目标对齐性，从而在一个理论框架内将主动推断与经典贝叶斯实验设计及贝叶斯优化联系起来。我们进一步将这些理论转化为实用设计准则，用于在混合学习-优化问题中调节认知-实用权衡，并通过真实实验验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of balancing exploration and exploitation in active inference, where the Expected Free Energy (EFE) objective uses a curiosity coefficient to trade off information gain (epistemic value) and task performance (pragmatic value). The authors provide the first theoretical guarantee that a single condition—sufficient curiosity—ensures both self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret), linking active inference to Bayesian experimental design and optimization. Their analysis yields practical guidelines for tuning the trade-off, supported by real-world experimental validation.</div>
<div class="mono" style="margin-top:8px">本文针对主动推理中探索与利用的平衡问题展开研究，其中期望自由能目标通过好奇心系数权衡信息增益（认知价值）与任务性能（实用价值）。作者首次提出理论保证，表明单一条件——足够的好奇心——可同时确保自洽学习（贝叶斯后验一致性）和无悔优化（有限累积遗憾），从而将主动推理与贝叶斯实验设计和优化联系起来。分析结果为调整这一权衡提供了实用设计指南，并通过真实世界实验进行了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</div>
<div class="meta-line">Authors: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang</div>
<div class="meta-line">First: 2026-02-05T18:57:09+00:00 · Latest: 2026-02-05T18:57:09+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/BudgetMem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06025v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06025v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/BudgetMem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向运行时智能体内存的查询感知预算层级路由学习</div>
<div class="mono" style="margin-top:8px">内存对超越单上下文窗口运行的大语言模型（LLM）智能体日益关键，但现有系统多依赖离线、查询无关的内存构建方式，效率低下且可能丢失查询关键信息。尽管运行时内存利用是自然替代方案，但先前研究常产生显著开销，且对性能-成本权衡缺乏显式控制。本文提出\textbf{BudgetMem}——一个支持显式查询感知性能-成本控制的运行时智能体内存框架。该框架将内存处理构建为多模块结构，每个模块提供三种预算层级（即\textsc{低}/\textsc{中}/\textsc{高}）。轻量级路由器通过强化学习训练的紧凑神经策略，执行跨模块的预算层级路由以平衡任务性能与内存构建成本。以BudgetMem为统一测试平台，我们研究实现预算层级的三种互补策略：实现方式（方法复杂度）、推理机制（推断行为）和容量配置（模块模型规模）。在LoCoMo、LongMemEval和HotpotQA数据集上的实验表明：在优先性能的场景（即高预算设置）下，BudgetMem超越强基线模型；在严格预算约束下能提供更优的精度-成本边界。进一步分析揭示了不同层级策略的优劣特性，明确了各策略维度在不同预算区间实现最佳权衡的适用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency and query-agnostic nature of offline memory construction in LLM agents by introducing BudgetMem, a runtime memory framework that enables explicit, query-aware control over the performance-cost trade-off. The method organizes memory into modules with three budget tiers (Low/Mid/High) and uses a lightweight neural router trained with reinforcement learning to allocate budgets across modules, balancing task performance and construction cost. Experimental results on LoCoMo, LongMemEval, and HotpotQA show that BudgetMem outperforms baselines in high-budget settings and achieves better accuracy-cost frontiers under tight budgets, while analysis reveals the distinct trade-offs of tiering strategies based on implementation, reasoning, and capacity.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLM）智能体中离线内存构建效率低下且与查询无关的问题，提出了BudgetMem框架，该框架通过运行时内存实现显式的、查询感知的性能-成本权衡控制。该方法将内存组织为具有三个预算层级（低/中/高）的模块，并利用强化学习训练的轻量级神经路由器在模块间分配预算，以平衡任务性能和构建成本。在LoCoMo、LongMemEval和HotpotQA上的实验结果表明，BudgetMem在高预算设置下优于基线方法，并在严格预算下实现了更优的精度-成本边界，同时分析揭示了基于实现、推理和容量的不同层级策略在不同预算制度下的权衡优势。</div>
</details>
</div>
<div class="card">
<div class="title">On Computation and Reinforcement Learning</div>
<div class="meta-line">Authors: Raj Ghugare, Michał Bortkiewicz, Alicja Ziarko, Benjamin Eysenbach</div>
<div class="meta-line">First: 2026-02-05T18:45:57+00:00 · Latest: 2026-02-05T18:45:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05999v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05999v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论计算与强化学习</div>
<div class="mono" style="margin-top:8px">强化学习策略可用的计算量如何影响其学习效果？使用固定参数量的策略是否仍能从额外计算中获益？标准强化学习框架缺乏正式回答这些问题的理论工具。实践中，深度强化学习策略常被参数化为静态架构的神经网络，混淆了计算量与参数量。本文形式化定义了计算受限策略，并证明使用更多计算的策略能解决计算较少策略无法处理的问题，且能泛化至更长时域的任务。基于算法学习和无模型规划的前期研究，我们提出一种能灵活利用可变计算量的最小化架构。实验与理论相互印证：在涵盖在线与离线强化学习的31项任务中，该架构（1）仅通过增加计算量即可获得更强性能，（2）与使用多达5倍参数的标准前馈网络或深度残差网络相比，在长时域测试任务中展现出更优的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how the computational budget, distinct from parameter count, influences reinforcement learning (RL) performance and generalization. The authors formalize compute-bounded policies and propose a minimal architecture that can dynamically utilize variable compute, building on concepts from algorithmic learning and model-free planning. Experimental results across 31 online and offline RL tasks demonstrate that this architecture achieves stronger performance with increased compute and better generalization to longer-horizon tasks compared to standard feedforward or deep residual networks, even those with many more parameters.</div>
<div class="mono" style="margin-top:8px">本文研究了计算预算（区别于参数数量）如何影响强化学习的性能与泛化能力。作者形式化了计算受限的策略，并基于算法学习和无模型规划的先验工作，提出了一种能够动态利用可变计算量的最小化架构。在31个在线和离线强化学习任务上的实验结果表明，该架构通过使用更多计算资源实现了更强的性能，并且在更长视野的测试任务上，比标准前馈网络或参数多出5倍的深度残差网络具有更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access</div>
<div class="meta-line">Authors: Daniel Ebi, Gaspard Lambrechts, Damien Ernst, Klemens Böhm</div>
<div class="meta-line">First: 2025-09-30T09:32:20+00:00 · Latest: 2026-02-05T18:21:20+00:00</div>
<div class="meta-line">Comments: 11 pages, 26 pages total, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26000v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26000v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Asymmetric actor-critic methods are widely used in partially observable reinforcement learning, but typically assume full state observability to condition the critic during training, which is often unrealistic in practice. We introduce the informed asymmetric actor-critic framework, allowing the critic to be conditioned on arbitrary state-dependent privileged signals without requiring access to the full state. We show that any such privileged signal yields unbiased policy gradient estimates, substantially expanding the set of admissible privileged information. This raises the problem of selecting the most adequate privileged information in order to improve learning. For this purpose, we propose two novel informativeness criteria: a dependence-based test that can be applied prior to training, and a criterion based on improvements in value prediction accuracy that can be applied post-hoc. Empirical results on partially observable benchmark tasks and synthetic environments demonstrate that carefully selected privileged signals can match or outperform full-state asymmetric baselines while relying on strictly less state information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知情非对称行动者-评论者：利用超越全状态访问的特权信号</div>
<div class="mono" style="margin-top:8px">非对称行动者-评论者方法在部分可观测强化学习中广泛应用，但通常假设评论者在训练期间能基于全状态条件进行学习，这在实践中往往不现实。我们提出知情非对称行动者-评论者框架，允许评论者基于任意与状态相关的特权信号进行条件化，而无需访问完整状态。我们证明任何此类特权信号均可产生无偏策略梯度估计，从而显著扩展了可采纳特权信息的范围。这引出了如何选择最合适特权信息以提升学习效果的问题。为此，我们提出两种新颖的信息量准则：一种可在训练前应用的基于依赖关系的检验方法，以及一种基于价值预测精度改进的事后评估准则。在部分可观测基准任务和合成环境中的实验结果表明，精心选择的特权信号在依赖严格更少状态信息的情况下，能够达到或超越全状态非对称基线方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of standard asymmetric actor-critic methods, which rely on full state access for the critic during training—an often impractical assumption in partially observable environments. To overcome this, the authors propose an informed asymmetric actor-critic framework that allows the critic to be conditioned on arbitrary state-dependent privileged signals, without requiring full state observability, and prove that such signals yield unbiased policy gradient estimates. They further introduce two novel criteria for selecting effective privileged information: a dependence-based pre-training test and a post-hoc value prediction accuracy measure. Experimental results on partially observable benchmarks and synthetic tasks show that well-chosen privileged signals can match or exceed the performance of full-state baselines while using strictly less state information.</div>
<div class="mono" style="margin-top:8px">本文针对标准非对称行动者-评论者方法在训练中依赖完全状态访问的局限性提出改进，这在部分可观测环境中通常不切实际。作者提出了一种知情非对称行动者-评论者框架，允许评论者基于任意与状态相关的特权信号进行条件化，而无需访问完整状态，并证明此类信号能产生无偏的策略梯度估计。为进一步选择有效的特权信息，他们提出了两种新颖的信息量准则：一种可训练前应用的基于依赖性的测试，以及一种训练后基于价值预测准确度提升的准则。在部分可观测基准任务和合成环境上的实验结果表明，精心选择的特权信号能够匹配甚至超越完全状态基线方法的性能，同时依赖更少的状态信息。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Share: Selective Memory for Efficient Parallel Agentic Systems</div>
<div class="meta-line">Authors: Joseph Fioresi, Parth Parag Kulkarni, Ashmal Vayani, Song Wang, Mubarak Shah</div>
<div class="meta-line">First: 2026-02-05T18:20:21+00:00 · Latest: 2026-02-05T18:20:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://joefioresi718.github.io/LTS_webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学会共享：面向高效并行智能体系统的选择性记忆机制</div>
<div class="mono" style="margin-top:8px">智能体系统通过协调多个智能体进行迭代推理、调用工具并交换中间结果来解决复杂任务。为提高鲁棒性与求解质量，近期研究采用并行运行的多个智能体团队来探索多样化推理路径。然而并行执行会带来显著计算开销：当不同团队独立处理相似子问题或执行同类步骤时，会重复进行大量重叠计算。为突破此局限，本文提出&#x27;学会共享&#x27;（LTS）——一种用于并行智能体框架的习得式共享记忆机制，可在控制上下文增长的同时实现跨团队选择性信息复用。LTS构建了全局共享记忆库与轻量级控制器，后者通过逐步强化学习结合使用感知的信用分配机制进行训练，从而精准识别跨并行执行的全局有效信息。在AssistantBench和GAIA基准测试中，LTS在保持或提升任务性能的同时显著降低总体运行时间，证明习得式记忆准入是提升并行智能体系统效率的有效策略。项目页面：https://joefioresi718.github.io/LTS_webpage/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the computational inefficiency in parallel agentic systems, where multiple independent teams often redundantly process similar sub-problems, leading to substantial overlapping computation. The method proposed, Learning to Share (LTS), introduces a learned shared-memory mechanism with a global memory bank and a lightweight controller trained via stepwise reinforcement learning to selectively admit intermediate agent steps for cross-team reuse, thereby controlling context growth. Main experimental results on AssistantBench and GAIA benchmarks demonstrate that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, confirming its effectiveness in enhancing system efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决并行智能体系统中的计算效率低下问题，即多个独立团队常冗余处理相似子问题，导致大量重复计算。所提出的方法“学习共享”（LTS）引入了一种基于学习的共享内存机制，包含一个全局内存库和一个轻量级控制器，该控制器通过逐步强化学习训练，以选择性允许中间智能体步骤跨团队重用，从而控制上下文增长。在AssistantBench和GAIA基准上的主要实验结果表明，与无内存并行基线相比，LTS显著降低了总体运行时间，同时保持或提升了任务性能，证明了学习型内存准入是提升并行智能体系统效率的有效策略。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-02-05T18:03:03+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v2">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题发现新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时进行强化学习，使大型语言模型能够持续训练，但此时训练经验专门针对测试问题。这种持续学习形式非常特殊，其目标是产生一个卓越解决方案而非多个平均良好的方案，并专注于解决当前问题而非泛化至其他问题。因此，我们的学习目标和搜索子程序被设计为优先考虑最有潜力的解决方案。我们将此方法称为“测试时训练发现法”。遵循先前研究，我们聚焦于具有连续奖励的问题。我们报告了在数学、GPU内核工程、算法设计和生物学领域尝试的所有问题的结果：TTT-Discover在几乎所有问题上都创造了新的最优解：（i）埃尔德什最小重叠问题与自相关不等式；（ii）GPUMode内核竞赛（比现有技术快达2倍）；（iii）过往AtCoder算法竞赛；（iv）单细胞分析中的去噪问题。我们的解决方案均经过专家或组织者评审。所有结果均使用开源模型OpenAI gpt-oss-120b实现，并可通过我们公开的代码复现，而先前的最佳结果需依赖封闭前沿模型。测试时训练通过Thinking Machines的Tinker API运行，每个问题成本仅数百美元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the goal of using AI to achieve new state-of-the-art solutions for scientific problems, this paper introduces Test-Time Training to Discover (TTT-Discover), a method that performs reinforcement learning at test time to allow a large language model (LLM) to continually train on experience specific to a given problem, prioritizing the most promising solutions. The approach focuses on problems with continuous rewards and employs a tailored learning objective and search subroutine. Experimental results demonstrate that TTT-Discover sets new state-of-the-art performance across multiple domains: it advances solutions for Erdős&#x27; minimum overlap problem and an autocorrelation inequality in mathematics, achieves up to 2x speedup in a GPU kernel engineering competition, excels in past AtCoder algorithm competitions, and improves denoising in single-cell biology analysis, with all solutions verified by experts. Notably, these results are achieved using an open model (OpenAI gpt-oss-120b) and publicly available code, contrasting prior work that relied on closed frontier models, with test-time training costs of only a few hundred dollars per problem.</div>
<div class="mono" style="margin-top:8px">本文旨在利用人工智能为科学问题取得新的最先进解决方案，提出了测试时训练发现方法（TTT-Discover），该方法在测试时通过强化学习使大语言模型能够针对特定问题持续训练，并优先探索最有希望的解决方案。该方法专注于具有连续奖励的问题，并设计了相应的学习目标和搜索子程序。实验结果表明，TTT-Discover在多个领域均取得了最先进的性能：在数学中改进了Erdős最小重叠问题和自相关不等式，在GPU内核工程竞赛中实现了比先前技术快2倍的速度，在过去的AtCoder算法竞赛中表现出色，并在单细胞生物学分析中提升了去噪效果，所有解决方案均经专家审核。值得注意的是，这些结果是使用开放模型（OpenAI gpt-oss-120b）和公开代码实现的，与之前依赖封闭前沿模型的工作形成对比，且每个问题的测试时训练成本仅为几百美元。</div>
</details>
</div>
<div class="card">
<div class="title">$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment</div>
<div class="meta-line">Authors: Rajdeep Haldar, Lantao Mei, Guang Lin, Yue Xing, Qifan Song</div>
<div class="meta-line">First: 2026-02-05T18:01:52+00:00 · Latest: 2026-02-05T18:01:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05946v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05946v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$f$-GRPO 及其拓展：基于散度的通用大语言模型对齐强化学习算法</div>
<div class="mono" style="margin-top:8px">近期研究表明，偏好对齐目标可视为对齐（选中）与未对齐（拒绝）响应分布间的散度估计器。本研究将这一基于散度的视角拓展至通用对齐场景，例如仅依赖环境奖励的强化学习验证奖励设置。在此统一框架下，我们提出了$f$-群组相对策略优化——一类基于策略的强化学习方法，以及$f$-混合对齐损失——一种混合策略目标，二者均基于$f$-散度的变分表示构建，适用于通用大语言模型对齐。我们提供了理论保证，证明这些目标能提升对齐后的平均奖励。在实验中，我们在强化学习验证奖励（数学推理）和偏好对齐（安全对齐）任务上验证了框架的有效性，相较于现有方法展现出更优的性能与灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the insight that preference alignment objectives can be viewed as divergence estimators, this work extends a divergence-based framework to general language model alignment settings, including reinforcement learning with verifiable rewards (RLVR) where only environmental feedback is present. The method introduces two novel classes of objectives: $f$-GRPO, an on-policy reinforcement learning approach, and $f$-HAL, a hybrid on/off-policy loss, both derived from the variational representation of $f$-divergences to align models. The main experimental results demonstrate the framework&#x27;s effectiveness and flexibility, showing superior performance over existing methods in both RLVR tasks, such as math reasoning, and preference alignment tasks, like safety alignment, with theoretical guarantees of reward improvement.</div>
<div class="mono" style="margin-top:8px">本研究受偏好对齐目标可视为分布散度估计器的启发，将基于散度的框架扩展至通用大语言模型对齐场景，包括仅有环境奖励的强化学习验证奖励设置。方法上提出了两类新目标：基于$f$散度变分表示的在线策略强化学习方法$f$-GRPO，以及混合在线/离线策略的损失函数$f$-HAL，用于模型对齐。主要实验结果表明，该框架在数学推理等强化学习验证奖励任务和安全对齐等偏好对齐任务上均优于现有方法，展现了卓越的性能与灵活性，并理论保证了奖励提升。</div>
</details>
</div>
<div class="card">
<div class="title">Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training</div>
<div class="meta-line">Authors: Zhenghao Xu, Qin Lu, Changlong Yu, Tuo Zhao</div>
<div class="meta-line">First: 2026-02-05T17:44:28+00:00 · Latest: 2026-02-05T17:44:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05933v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05933v1">PDF</a> · <a href="https://github.com/horizon-rl/OpenKimi">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略镜像下降中对数配分函数的近似诱导大语言模型后训练的隐式正则化</div>
<div class="mono" style="margin-top:8px">策略镜像下降通过迭代求解KL正则化的策略改进子问题，为强化学习提供了理论框架。尽管该方法已用于训练Kimi K1.5/K2等先进大语言模型，但理想的闭式PMD更新需要可靠的对数配分函数估计，这在大语言模型广阔动作空间中有限采样条件下极具挑战。本文研究一种名为PMD-mean的实用算法，该算法用采样策略下的平均奖励近似对数配分项，并在对数策略空间执行回归。我们刻画了PMD-mean的总体解，证明其隐式优化了具有自适应混合KL-χ²正则项的镜像下降子问题。额外的χ²正则化约束了概率的大幅变动，在期望奖励较低时产生更保守的更新，并增强了对有限样本估计误差的鲁棒性。数学推理任务实验表明，PMD-mean以更高的稳定性与时间效率取得更优性能。这些发现深化了对PMD-mean的理解，并为大语言模型强化学习算法的理论改进指明了路径。代码发布于https://github.com/horizon-rl/OpenKimi。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of reliably estimating the partition function in policy mirror descent (PMD) for LLM reinforcement learning, which is difficult due to limited rollouts in large action spaces. The method introduces PMD-mean, a practical algorithm that approximates the log-partition term using the mean reward under the sampling policy and performs regression in log-policy space. Experimental results on math reasoning tasks demonstrate that PMD-mean achieves superior performance with improved stability and time efficiency, as the implicit adaptive mixed KL-χ² regularization constrains large probability changes and enhances robustness against estimation errors.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决大型语言模型强化学习中策略镜像下降的配分函数可靠估计难题，该问题因大动作空间中有限采样而突出。方法上提出了PMD-mean算法，通过采样策略下的平均奖励近似对数配分项，并在对数策略空间进行回归。在数学推理任务上的实验结果表明，PMD-mean实现了更优的性能，并具有更好的稳定性和时间效率，其隐含的自适应混合KL-χ²正则化约束了概率的大幅变化，增强了对估计误差的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem</div>
<div class="meta-line">Authors: Eva Andrés</div>
<div class="meta-line">First: 2026-02-05T17:32:14+00:00 · Latest: 2026-02-05T17:32:14+00:00</div>
<div class="meta-line">Comments: 22 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05920v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05920v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Transformer的量子强化学习求解带容量约束的车辆路径问题</div>
<div class="mono" style="margin-top:8px">本文通过比较经典与量子强化学习方法，研究带容量约束的车辆路径问题。研究实现了经典、全量子和混合三种优势行动者-评论者智能体变体，集成Transformer架构，通过自注意力与交叉注意力机制捕捉车辆、客户与配送中心间的关联关系。实验聚焦多车辆容量约束场景，设置20个客户与4辆车辆，进行十次独立运行。性能通过路径距离、路径紧凑度和路径重叠度进行评估。结果表明三种方法均能学习有效路径策略，但量子增强模型优于经典基线，产生更稳健的路径组织，其中混合架构在距离、紧凑度和路径重叠度上综合表现最佳。除量化改进外，定性可视化显示量子模型能生成更具结构性和连贯性的路径方案。这些发现凸显了混合量子-经典强化学习模型处理CVRP等复杂组合优化问题的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to solve the complex combinatorial optimization challenge of the Capacitated Vehicle Routing Problem (CVRP). The method employs Reinforcement Learning, specifically an Advantage Actor-Critic agent, implemented in classical, full quantum, and hybrid variants, and integrates transformer architectures with attention mechanisms to model relationships between vehicles, clients, and the depot. Experimental results on a scenario with 20 clients and 4 vehicles over ten runs show that all approaches learn effective policies, but quantum-enhanced models, particularly the hybrid variant, outperform the classical baseline in routing distance, compactness, and overlap, yielding more structured and robust solutions.</div>
<div class="mono" style="margin-top:8px">本文旨在解决带容量约束的车辆路径问题这一复杂的组合优化挑战。研究方法采用强化学习，具体为优势演员-评论家智能体，并实现了经典、全量子和混合三种变体，同时集成了具有注意力机制的Transformer架构来建模车辆、客户与仓库之间的关系。在包含20个客户和4辆车的场景上进行十次独立实验的结果表明，所有方法都能学习到有效的策略，但量子增强模型，特别是混合架构，在路径距离、紧凑性和重叠度上均优于经典基线，产生了更结构化、更稳健的路线解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to summarize user information for personalized reinforcement learning from human feedback</div>
<div class="meta-line">Authors: Hyunji Nam, Yanming Wan, Mickel Liu, Peter Ahnn, Jianxun Lian, Natasha Jaques</div>
<div class="meta-line">First: 2025-07-17T23:48:51+00:00 · Latest: 2026-02-05T17:13:23+00:00</div>
<div class="meta-line">Comments: 10 pages for main text, 10 pages for appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.13579v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.13579v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As everyday use cases of large language model (LLM) AI assistants have expanded, it is becoming increasingly important to personalize responses to align to different users&#x27; preferences and goals. While reinforcement learning from human feedback (RLHF) is effective at improving LLMs to be generally more helpful and fluent, it does not account for variability across users, as it models the entire user population with a single reward model, meaning it assumes that everyone&#x27;s preferences are the same. We present a novel framework, Preference Learning Using Summarization (PLUS), that uses reinforcement learning (RL) to learn to produce text-based summaries of each user&#x27;s preferences, characteristics, and past conversations. These summaries condition the reward model, enabling it to make personalized predictions about the types of responses valued by each user. Both the user-summarization model and reward model are trained simultaneously, creating an online co-adaptation loop. We show that in contrast to the standard Bradley-Terry model, summaries produced by PLUS capture diverse aspects of user preferences, achieving a 11-77/% improvement in reward model accuracy. Key strengths of PLUS are: (1) robust performance with new users and conversation topics, achieving a 25\% improvement over the best personalized reward model technique used for RLHF; (2) zero-shot personalization with state-of-the-art proprietary models like GPT-4 (e.g., PLUS-summary-conditioned responses achieved a 72\% win rate compared to 28% for default GPT-4o); (3) learning from flexible user contexts beyond preference labels, and (4) interpretable representation of users, enabling greater transparency and user control in pluralistic LLM alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习总结用户信息以实现基于人类反馈的个性化强化学习</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLM）AI助手在日常应用中的扩展，个性化响应用户偏好与目标变得日益重要。基于人类反馈的强化学习（RLHF）虽能有效提升LLM的通用助益性与流畅度，但未考虑用户间的差异性，因其使用单一奖励模型建模全体用户，即假设所有人偏好相同。本文提出一种新颖框架——基于总结的偏好学习（PLUS），利用强化学习（RL）生成基于文本的用户偏好、特征及历史对话摘要。这些摘要作为奖励模型的调节条件，使其能针对每位用户进行个性化响应价值预测。用户摘要模型与奖励模型同步训练，形成在线协同适应循环。实验表明，相较于标准Bradley-Terry模型，PLUS生成的摘要能捕捉用户偏好的多元维度，将奖励模型准确率提升11-77%。PLUS的核心优势包括：（1）对新用户与对话主题具有稳健性能，较现有RLHF最佳个性化奖励模型技术提升25%；（2）实现与GPT-4等前沿专有模型的零样本个性化交互（例如：PLUS摘要调节的响应胜率达72%，而默认GPT-4o仅为28%）；（3）支持超越偏好标签的灵活用户上下文学习；（4）提供可解释的用户表征，增强多元化LLM对齐的透明度与用户可控性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that standard reinforcement learning from human feedback (RLHF) fails to personalize large language model responses because it uses a single reward model for all users, ignoring individual preferences. The proposed method, called Preference Learning Using Summarization (PLUS), introduces a novel framework where reinforcement learning is used to generate text-based summaries of each user&#x27;s preferences and history; these summaries then condition a reward model to make personalized predictions. Key experimental results show that PLUS significantly improves reward model accuracy by 11-77%, outperforms the best personalized RLHF technique by 25% with new users and topics, and enables zero-shot personalization, such as achieving a 72% win rate against default GPT-4o, while also providing interpretable user representations.</div>
<div class="mono" style="margin-top:8px">这项工作的动机在于，标准的基于人类反馈的强化学习（RLHF）使用单一奖励模型处理所有用户，无法实现个性化响应，忽略了用户偏好的差异性。提出的方法名为偏好学习与摘要（PLUS），这是一个新颖框架，利用强化学习生成基于文本的用户偏好和历史摘要，这些摘要用于调节奖励模型以进行个性化预测。主要实验结果表明，PLUS将奖励模型准确率提高了11-77%，在新用户和新话题上比最佳个性化RLHF技术性能提升25%，并支持零样本个性化，例如在对比默认GPT-4o时获得了72%的胜率，同时提供了可解释的用户表示。</div>
</details>
</div>
<div class="card">
<div class="title">DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training</div>
<div class="meta-line">Authors: Dingwei Zhu, Zhiheng Xi, Shihan Dou, Jiahan Li, Chenhao Huang, Junjie Ye, Sixian Li, Mingxu Chai, Yuhui Wang, Yajie Yang, Ming Zhang, Jiazheng Zhang, Shichun Liu, Caishuang Huang, Yunke Zhang, Yuran Wang, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang</div>
<div class="meta-line">First: 2026-02-05T17:07:42+00:00 · Latest: 2026-02-05T17:07:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05890v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05890v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DFPO：通过分布流扩展价值建模，实现稳健且可泛化的大语言模型后训练</div>
<div class="mono" style="margin-top:8px">在现实环境中训练强化学习系统仍面临监督噪声大和领域外泛化能力差的挑战，尤其在大语言模型后训练中。现有分布强化学习方法通过多分位数点建模价值以提升稳健性，但各分位数仍作为独立标量学习，导致价值表征粒度粗糙、缺乏对状态信息的细粒度条件化，难以应对复杂及领域外场景。本文提出DFPO（带条件风险与一致性控制的分布价值流策略优化），一种稳健的分布强化学习框架，将价值建模为跨时间步的连续流。通过学习价值流场而非孤立分位数预测，DFPO扩展了价值建模维度，捕获更丰富的状态信息以实现更精准的优势估计。为在噪声反馈下稳定训练，DFPO进一步整合了沿价值流轨迹的条件风险控制与一致性约束。在对话、数学推理及科学任务上的实验表明，DFPO在噪声监督下优于PPO、FlowRL等基线方法，显著提升了训练稳定性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenges of noisy supervision and poor out-of-domain generalization in reinforcement learning for LLM post-training, where existing distributional RL methods often produce coarse-grained value representations. To address this, DFPO introduces a framework that models values as continuous flows across time steps, scaling value modeling via a flow field to capture richer state information for advantage estimation, while incorporating conditional risk control and consistency constraints to stabilize training under noise. Experimental results on dialogue, math reasoning, and scientific tasks demonstrate that DFPO outperforms baselines like PPO and FlowRL in noisy settings, achieving better training stability and generalization.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大语言模型后训练中强化学习面临的噪声监督和域外泛化能力差的问题，现有分布强化学习方法常产生粗糙的价值表示。为此，DFPO提出一个框架，将价值建模为跨时间步的连续流，通过学习流场而非孤立分位数预测来扩展价值建模，以捕获更丰富的状态信息进行优势估计，同时结合条件风险控制和一致性约束以稳定噪声下的训练。在对话、数学推理和科学任务上的实验表明，DFPO在噪声监督下优于PPO、FlowRL等基线方法，实现了更好的训练稳定性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</div>
<div class="meta-line">Authors: Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He</div>
<div class="meta-line">First: 2026-02-05T17:01:09+00:00 · Latest: 2026-02-05T17:01:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05885v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05885v1">PDF</a> · <a href="https://www.github.com/hkust-nlp/KernelGYM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dr. Kernel：面向Triton内核生成的正则化强化学习方法</div>
<div class="mono" style="margin-top:8px">高质量内核对可扩展AI系统至关重要，使大语言模型能生成此类代码将推动AI发展。然而，训练大语言模型完成此任务需要充足数据、鲁棒环境，且该过程常易受奖励破解与惰性优化影响。模型可能破解训练奖励，优先追求表面正确性而非实质性加速。本文系统研究强化学习在内核生成中的应用。首先设计KernelGYM——支持奖励破解检测、多轮交互数据收集与长期强化学习训练的分布式GPU环境。基于此，研究有效的多轮强化学习方法，发现GRPO中因自包含导致的策略梯度偏差问题。为此提出轮级强化学习留一法，为多轮强化学习提供无偏优势估计。为缓解惰性优化，引入失配校正以提升训练稳定性，并提出基于性能剖析的奖励机制与拒绝采样方法以克服该问题。训练所得Dr.Kernel-14B模型在Kernelbench中达到与Claude-4.5-Sonnet相当的性能。最后研究Dr.Kernel-14B的顺序测试时扩展：在KernelBench Level-2子集中，31.6%生成内核实现较Torch参考至少1.2倍加速，超越Claude-4.5-Sonnet（26.7%）与GPT-5（28.6%）；当跨所有轮次选择最佳候选时，该加速率进一步提升至47.8%。所有资源（环境、训练代码、模型及数据集）已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of using reinforcement learning (RL) to train large language models (LLMs) for generating high-performance GPU kernels, a task prone to reward hacking and lazy optimization where models prioritize trivial correctness over actual speedup. The method introduces KernelGYM, a distributed GPU environment for robust multi-turn RL training, and proposes Turn-level Reinforce-Leave-One-Out (TRLOO) to correct biased policy gradients, alongside profiling-based rewards and rejection sampling to mitigate lazy optimization. Experimental results show that the resulting model, Dr.Kernel-14B, achieves competitive performance, with 31.6% of its generated kernels on a benchmark subset achieving at least a 1.2x speedup over a reference, surpassing other models, and this rate increases to 47.8% when selecting the best candidate across multiple turns.</div>
<div class="mono" style="margin-top:8px">本文针对使用强化学习训练大语言模型生成高性能GPU内核的挑战展开研究，该任务易出现奖励破解和惰性优化问题，即模型可能追求表面正确性而非实际加速效果。方法上，提出了KernelGYM这一支持多轮交互的分布式GPU训练环境，并设计了Turn-level Reinforce-Leave-One-Out（TRLOO）来解决策略梯度偏差，同时采用基于性能分析的奖励和拒绝采样以缓解惰性优化。实验结果表明，所得模型Dr.Kernel-14B性能优异，在基准测试子集上，31.6%生成的内核实现了至少1.2倍加速，优于其他对比模型，且通过多轮择优可将该加速比例提升至47.8%。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning</div>
<div class="meta-line">Authors: Wenquan Lu, Hai Huang, Randall Balestriero</div>
<div class="meta-line">First: 2026-02-03T06:59:42+00:00 · Latest: 2026-02-05T16:51:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03190v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03190v2">PDF</a> · <a href="https://github.com/wenquanlu/prompt-augmentation-GRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 45.2 per-benchmark accuracy and 51.8 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示增强技术扩展GRPO在数学推理任务中的训练规模</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）等强化学习算法已展现出提升大语言模型数学推理能力的巨大潜力。然而，先前研究普遍观察到强化学习后训练过程中会出现熵崩溃现象，表现为策略熵单调递减，最终导致训练失稳与崩溃。因此，现有方法大多将训练限制在较短周期（通常5-20轮），制约了持续探索并阻碍策略的进一步优化。此外，几乎所有先前研究在训练期间都依赖单一固定的推理提示模板。本研究提出提示增强训练策略，通过引导模型在不同模板与格式下生成推理轨迹，有效提升训练轨迹的多样性。实验表明，在无KL正则项的情况下，提示增强技术能在固定数据集上实现训练时长的稳定扩展，并使模型能够适应低熵状态而避免过早崩溃。实证结果显示，采用提示增强技术在MATH Level 3-5数据集上训练的Qwen2.5-Math-1.5B模型取得了最先进的性能，在AIME24、AMC、MATH500、Minerva及OlympiadBench等标准数学推理基准测试中，分别达到45.2%的基准平均准确率和51.8%的题目平均准确率。代码与模型检查点已开源：https://github.com/wenquanlu/prompt-augmentation-GRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the entropy collapse and limited training horizons observed in group-relative policy optimization (GRPO) for enhancing large language models&#x27; mathematical reasoning. The proposed method, prompt augmentation, increases rollout diversity by training the model to generate reasoning traces under varied templates and formats, which stabilizes extended training without requiring KL regularization. Experimental results show that a Qwen2.5-Math-1.5B model trained with this strategy on the MATH Level 3-5 dataset achieves state-of-the-art performance, with 45.2 per-benchmark and 51.8 per-question accuracy on key mathematical reasoning benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究针对使用群体相对策略优化（GRPO）增强大语言模型数学推理能力时出现的熵崩溃和训练周期受限问题，提出了提示增强方法。该方法通过让模型在多样化的模板和格式下生成推理轨迹来增加训练数据的多样性，从而在不依赖KL正则化的情况下实现了稳定的长时间训练。实验结果表明，在MATH Level 3-5数据集上使用提示增强训练的Qwen2.5-Math-1.5B模型取得了最先进的性能，在AIME24、AMC等关键数学推理基准测试中分别达到了45.2的基准平均准确率和51.8的题目平均准确率。</div>
</details>
</div>
<div class="card">
<div class="title">The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton&#x27;s Laws in Financial Deep Reinforcement Learning (RL) Algorithms</div>
<div class="meta-line">Authors: Trang Thoi, Hung Tran, Tram Thoi, Huaiyang Zhong</div>
<div class="meta-line">First: 2026-02-01T18:48:33+00:00 · Latest: 2026-02-05T16:48:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01388v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (DRL), a subset of machine learning focused on sequential decision-making, has emerged as a powerful approach for tackling financial trading problems. In finance, DRL is commonly used either to generate discrete trade signals or to determine continuous portfolio allocations. In this work, we propose a novel reinforcement learning framework for portfolio optimization that incorporates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into several DRL algorithms. The approach replaces conventional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs) in both actor and critic components-utilizing learnable B-spline univariate functions to achieve parameter-efficient and more interpretable function approximation. During actor updates, we introduce a physics-informed regularization loss that promotes second-order temporal consistency between observed return dynamics and the action-induced portfolio adjustments. The proposed framework is evaluated across three equity markets-China, Vietnam, and the United States, covering both emerging and developed economies. Across all three markets, PIKAN-based agents consistently deliver higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and more favorable drawdown characteristics compared to both standard DRL baselines and classical online portfolio-selection methods. This yields more stable training, higher Sharpe ratios, and superior performance compared to traditional DRL counterparts. The approach is particularly valuable in highly dynamic and noisy financial markets, where conventional DRL often suffers from instability and poor generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强型物理信息Kolmogorov-Arnold网络：牛顿定律在金融深度强化学习算法中的应用</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）作为机器学习中专注于序列决策的子领域，已成为解决金融交易问题的有力方法。在金融领域，DRL通常用于生成离散交易信号或确定连续投资组合配置。本研究提出了一种新颖的强化学习框架，将物理信息Kolmogorov-Arnold网络（PIKANs）融入多种DRL算法以优化投资组合。该方法在行动者和评论者组件中用Kolmogorov-Arnold网络（KANs）替代传统多层感知器，利用可学习的B样条单变量函数实现参数高效且更可解释的函数逼近。在行动者更新过程中，我们引入了物理信息正则化损失，以促进观测收益动态与行动引发的投资组合调整之间的二阶时间一致性。该框架在中国、越南和美国三个股票市场（涵盖新兴与发达经济体）进行评估。在所有市场中，基于PIKAN的智能体相较于标准DRL基线和经典在线投资组合选择方法，始终展现出更高的累计与年化收益、更优的夏普与卡尔玛比率，以及更有利的回撤特性。与传统DRL方法相比，该方法实现了更稳定的训练、更高的夏普比率和更卓越的性能。该方法在高度动态且噪声显著的金融市场中尤为宝贵，而传统DRL在此类市场中常面临不稳定和泛化能力不足的问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more stable and interpretable deep reinforcement learning (DRL) in financial portfolio optimization, this paper introduces a novel framework that integrates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into DRL algorithms. The method replaces standard multilayer perceptrons in actor and critic networks with Kolmogorov-Arnold Networks (KANs), which use learnable B-spline functions for efficient and interpretable approximation, and adds a physics-informed regularization loss during actor updates to enforce second-order temporal consistency between returns and portfolio adjustments. Experimental results across equity markets in China, Vietnam, and the United States show that PIKAN-based agents achieve higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and better drawdown characteristics compared to traditional DRL baselines and classical portfolio methods, leading to more stable training and improved performance in dynamic financial environments.</div>
<div class="mono" style="margin-top:8px">针对金融投资组合优化中深度强化学习（DRL）稳定性与可解释性不足的问题，本文提出了一种将物理信息柯尔莫哥洛夫-阿诺德网络（PIKANs）集成到DRL算法中的新框架。该方法用柯尔莫哥洛夫-阿诺德网络（KANs）替代了传统DRL中执行器和评论器网络的多层感知机，通过可学习的B样条函数实现高效且可解释的函数逼近，并在执行器更新时引入物理信息正则化损失，以强化收益动态与投资组合调整之间的二阶时间一致性。在中国、越南和美国股市的实验结果表明，基于PIKAN的智能体相比传统DRL基准和经典投资组合方法，获得了更高的累计与年化收益、更优的夏普比率与卡尔玛比率，以及更佳的回撤控制特性，从而在动态金融市场中实现了更稳定的训练和更优越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models</div>
<div class="meta-line">Authors: Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, Shaohui Lin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-03-09T20:06:45+00:00 · Latest: 2026-02-05T16:21:33+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. Code is available at https://github.com/Osilly/Vision-R1</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.06749v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.06749v3">PDF</a> · <a href="https://github.com/Osilly/Vision-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model&#x27;s ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. Scaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4% and 78.2% MathVista benchmark scores, respectively. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vision-R1：激励多模态大语言模型中的推理能力</div>
<div class="mono" style="margin-top:8px">DeepSeek-R1-Zero已成功证明仅通过强化学习（RL）即可在大语言模型中涌现推理能力。受此突破启发，我们探索如何利用RL增强多模态大语言模型（MLLM）的推理能力。然而，由于缺乏大量高质量多模态推理数据，直接使用RL训练难以激活MLLM中的复杂推理能力（如质疑与反思）。为此，我们提出推理型MLLM——Vision-R1，以提升多模态推理能力。具体而言，我们首先通过模态桥接与数据过滤，利用现有MLLM和DeepSeek-R1构建无需人工标注的20万规模高质量多模态思维链数据集Vision-R1-cold，作为Vision-R1的冷启动初始化数据。为缓解冷启动后因过度思考导致的优化难题，我们提出渐进式思维抑制训练（PTST）策略，并采用组相对策略优化（GRPO）配合硬格式化结果奖励函数，在1万规模多模态数学数据集上逐步优化模型学习正确且复杂推理过程的能力。综合实验表明，我们的模型在多项多模态数学推理基准上平均提升约6%。Vision-R1-7B在广泛使用的MathVista基准上达到73.5%准确率，仅比领先的推理模型OpenAI O1低0.4%。通过增加RL训练中的多模态数学数据量，Vision-R1-32B和Vision-R1-72B分别在MathVista基准上获得76.4%和78.2%的分数。数据集与代码发布于：https://github.com/Osilly/Vision-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the success of RL in fostering reasoning in LLMs, this work aims to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) through RL, but faces a data scarcity challenge. The method involves first constructing a high-quality, unannotated 200K multimodal chain-of-thought dataset (Vision-R1-cold) using existing models for cold-start initialization, and then applying a Progressive Thinking Suppression Training strategy with Group Relative Policy Optimization to refine reasoning on a 10K math dataset. The main experimental results show that the proposed Vision-R1 model achieves an average improvement of ~6% on various multimodal math reasoning benchmarks, with the 7B parameter version reaching 73.5% accuracy on MathVista, close to the leading model, and larger 32B and 72B versions achieving 76.4% and 78.2%, respectively.</div>
<div class="mono" style="margin-top:8px">受强化学习在大型语言模型中成功激发推理能力的启发，本研究旨在通过强化学习提升多模态大语言模型的推理能力，但面临高质量数据缺乏的挑战。方法上，首先利用现有模型构建了一个无需人工标注的20万规模多模态思维链数据集用于冷启动，随后采用渐进式思维抑制训练策略和分组相对策略优化，在一个1万规模的多模态数学数据集上精炼推理过程。主要实验结果表明，所提出的Vision-R1模型在多个多模态数学推理基准上平均提升约6%，其中70亿参数版本在MathVista基准上达到73.5%的准确率，接近领先模型，而320亿和720亿参数版本则分别达到76.4%和78.2%的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu, Gong Chen, Ziqi Gao, Min Peng</div>
<div class="meta-line">First: 2026-02-05T16:08:36+00:00 · Latest: 2026-02-05T16:08:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05818v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05818v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TKG-Thinker：基于智能体强化学习的时序知识图谱动态推理方法</div>
<div class="mono" style="margin-top:8px">时序知识图谱问答（TKGQA）旨在利用时序知识库回答时间敏感问题。尽管大语言模型（LLM）在TKGQA中展现出巨大潜力，但现有提示策略主要在两方面限制了其效能：一是在复杂时序约束下易产生推理幻觉；二是静态提示缺乏与时序知识图谱（TKG）环境的动态交互优化，限制了模型自主性与泛化能力。为此，我们提出\textbf{TKG-Thinker}——一种具备自主规划与自适应检索能力的新型智能体，专为时序知识图谱推理设计。该模型通过双阶段训练策略实现与TKG的动态多轮交互式深度时序推理：首先采用思维链数据进行监督微调以植入核心规划能力，再通过强化学习阶段利用多维度奖励优化复杂时序约束下的推理策略。在三个开源LLM的基准数据集实验中，TKG-Thinker取得了最先进的性能，并在复杂TKGQA场景中表现出强泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current Large Language Models (LLMs) in Temporal Knowledge Graph Question Answering (TKGQA), where static prompting leads to reasoning hallucinations and poor generalization under complex temporal constraints. The authors propose TKG-Thinker, an agentic framework that uses a dual-training strategy: first, Supervised Fine-Tuning with chain-of-thought data to build planning capabilities, followed by Reinforcement Learning with multi-dimensional rewards to refine reasoning policies through dynamic interaction with temporal knowledge graphs. Experiments on benchmark datasets with multiple open-source LLMs demonstrate that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization in complex TKGQA settings.</div>
<div class="mono" style="margin-top:8px">本文针对当前大语言模型在时序知识图谱问答任务中的局限性，即静态提示易导致复杂时序约束下的推理幻觉和泛化能力不足。研究者提出了TKG-Thinker智能体框架，采用双重训练策略：首先通过思维链数据进行监督微调以建立核心规划能力，随后利用强化学习与多维奖励机制，在动态交互中优化时序推理策略。在多个开源大语言模型和基准数据集上的实验结果表明，TKG-Thinker取得了最先进的性能，并在复杂时序问答场景中展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Selecting Hyperparameters for Tree-Boosting</div>
<div class="meta-line">Authors: Floris Jan Koster, Fabio Sigrist</div>
<div class="meta-line">First: 2026-02-05T15:44:42+00:00 · Latest: 2026-02-05T15:44:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05786v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05786v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>树提升超参数选择</div>
<div class="mono" style="margin-top:8px">树提升是广泛用于表格数据的机器学习技术，但其样本外精度高度依赖多个超参数。本文通过59个回归与分类数据集，实证比较了随机网格搜索、树结构Parzen估计器（TPE）、高斯过程贝叶斯优化（GP-BO）、Hyperband、序列模型算法配置（SMAC）及确定性全网格搜索等主流超参数优化方法。研究发现SMAC方法显著优于其他方法，并进一步观察到：（i）精确调参需超过100次试验；（ii）使用默认超参数会导致模型精度严重不足；（iii）所有考察的超参数均对树提升精度有实质影响，不存在更重要的核心子集；（iv）在回归任务中，通过早停法确定提升迭代次数比将其纳入搜索空间能获得更精确的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the critical impact of hyperparameter selection on the out-of-sample accuracy of tree-boosting models, a popular technique for tabular data. The method involves an empirical comparison of several hyperparameter optimization approaches, including random grid search, TPE, GP-BO, Hyperband, SMAC, and deterministic full grid search, across 59 regression and classification datasets. The main experimental results show that the SMAC method clearly outperforms all others, and key findings indicate that accurate tuning requires over 100 trials, default hyperparameters lead to poor performance, all hyperparameters significantly affect accuracy, and using early stopping for boosting iterations is beneficial for regression tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于超参数选择对树提升模型（一种广泛使用的表格数据技术）样本外精度的关键影响。研究方法是对多种超参数优化方法进行实证比较，包括随机网格搜索、TPE、GP-BO、Hyperband、SMAC和确定性全网格搜索，在59个回归和分类数据集上进行测试。主要实验结果表明，SMAC方法明显优于其他所有方法，关键发现包括：准确调参需要超过100次试验、默认超参数会导致性能不佳、所有超参数都对精度有实质影响，以及在回归任务中使用早停法确定提升迭代次数效果更好。</div>
</details>
</div>
<div class="card">
<div class="title">A Policy Gradient-Based Sequence-to-Sequence Method for Time Series Prediction</div>
<div class="meta-line">Authors: Qi Sima, Xinze Zhang, Yukun Bao, Siyue Yang, Liang Shen</div>
<div class="meta-line">First: 2024-06-14T00:24:29+00:00 · Latest: 2026-02-05T15:42:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.09643v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.09643v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sequence-to-sequence architectures built upon recurrent neural networks have become a standard choice for multi-step-ahead time series prediction. In these models, the decoder produces future values conditioned on contextual inputs, typically either actual historical observations (ground truth) or previously generated predictions. During training, feeding ground-truth values helps stabilize learning but creates a mismatch between training and inference conditions, known as exposure bias, since such true values are inaccessible during real-world deployment. On the other hand, using the model&#x27;s own outputs as inputs at test time often causes errors to compound rapidly across prediction steps. To mitigate these limitations, we introduce a new training paradigm grounded in reinforcement learning: a policy gradient-based method to learn an adaptive input selection strategy for sequence-to-sequence prediction models. Auxiliary models first synthesize plausible input candidates for the decoder, and a trainable policy network optimized via policy gradients dynamically chooses the most beneficial inputs to maximize long-term prediction performance. Empirical evaluations on diverse time series datasets confirm that our approach enhances both accuracy and stability in multi-step forecasting compared to conventional methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略梯度的序列到序列时间序列预测方法</div>
<div class="mono" style="margin-top:8px">基于循环神经网络的序列到序列架构已成为多步时间序列预测的标准选择。这类模型的解码器根据上下文输入生成未来值，通常采用实际历史观测值（真实值）或先前生成的预测值作为条件输入。训练过程中，输入真实值有助于稳定学习，但会导致训练与推理条件不匹配（即暴露偏差），因为实际部署时无法获取真实值。另一方面，在测试阶段使用模型自身输出作为输入，往往会导致误差在预测步长间快速累积。为缓解这些局限，我们提出一种基于强化学习的新训练范式：采用策略梯度方法，为序列到序列预测模型学习自适应输入选择策略。辅助模型首先生成解码器的合理候选输入，通过策略梯度优化的可训练策略网络动态选择最有利的输入，以最大化长期预测性能。在多类时间序列数据集上的实证评估表明，与传统方法相比，本方法能显著提升多步预测的准确性与稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the exposure bias and error accumulation problems in multi-step time series prediction using sequence-to-sequence models, where training with ground-truth inputs mismatches inference conditions. The authors propose a reinforcement learning-based training paradigm that employs a policy gradient-optimized network to adaptively select decoder inputs from synthesized candidates, aiming to maximize long-term forecasting performance. Experimental results across various datasets demonstrate that this method improves both prediction accuracy and stability over conventional approaches.</div>
<div class="mono" style="margin-top:8px">本文针对序列到序列模型在多步时间序列预测中存在的暴露偏差和误差累积问题，提出了一种基于强化学习的训练范式。该方法通过策略梯度优化的网络，从合成的候选输入中自适应地选择解码器输入，以最大化长期预测性能。在多个时间序列数据集上的实验结果表明，与传统方法相比，该方法提高了预测的准确性和稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Distributional Reinforcement Learning with Diffusion Bridge Critics</div>
<div class="meta-line">Authors: Shutong Ding, Yimiao Zhou, Ke Hu, Mokai Pan, Shan Zhong, Yanwei Fu, Jingya Wang, Ye Shi</div>
<div class="meta-line">First: 2026-02-05T15:40:14+00:00 · Latest: 2026-02-05T15:40:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05783v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散桥评论家的分布强化学习方法</div>
<div class="mono" style="margin-top:8px">基于扩散的强化学习方法在连续控制任务中展现出广阔前景，但现有研究多聚焦于扩散策略的应用，而扩散评论家尚未得到充分探索。实际上，由于策略优化本质上依赖于评论家，准确的价值估计远比策略表达能力更为重要。此外，鉴于多数强化学习任务的随机性，采用分布模型描述评论家已被证实更为合适。基于此，我们提出一种基于扩散桥评论家的新型分布强化学习方法。该方法直接对Q值的逆累积分布函数进行建模，借助扩散桥强大的分布匹配能力，能够精确捕捉价值分布并避免其坍缩为平凡的高斯分布。同时，我们进一步推导出解析积分公式以解决离散化误差问题，这对价值估计至关重要。据我们所知，这是首次将扩散桥模型应用于评论家的工作。值得注意的是，该方法具备即插即用特性，可集成至多数现有强化学习框架。在MuJoCo机器人控制基准测试中的实验结果表明，该方法优于以往的分布评论家模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the importance of accurate value estimation in reinforcement learning and the need for distributional critics to capture task stochasticity, this paper introduces Diffusion Bridge Critics (DBC), a novel distributional RL method that models the inverse cumulative distribution function of Q-values using a diffusion bridge to precisely represent value distributions and avoid collapse to trivial Gaussians. The method addresses discretization errors via an analytic integral formula and is designed as a plug-and-play component compatible with existing RL frameworks. Experimental results on MuJoCo robot control benchmarks show that DBC outperforms prior distributional critic models.</div>
<div class="mono" style="margin-top:8px">本文的动机在于强化学习中精确价值估计的重要性以及需要分布化评论家来捕捉任务随机性，为此提出了扩散桥评论家（DBC）这一新颖的分布化强化学习方法，它利用扩散桥建模Q值的逆累积分布函数，以准确捕捉价值分布并避免坍缩为平凡高斯分布。该方法通过解析积分公式处理离散化误差，并设计为即插即用组件，可与现有强化学习框架集成。在MuJoCo机器人控制基准测试中的实验结果表明，DBC优于先前的分布化评论家模型。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-Domain Offline Policy Adaptation via Selective Transition Correction</div>
<div class="meta-line">Authors: Mengbei Yan, Jiafei Lyu, Shengjie Sun, Zhongjian Qiao, Jingwen Yang, Zichuan Lin, Deheng Ye, Xiu Li</div>
<div class="meta-line">First: 2026-02-05T15:37:29+00:00 · Latest: 2026-02-05T15:37:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05776v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于选择性转移修正的跨领域离线策略自适应</div>
<div class="mono" style="margin-top:8px">在强化学习（RL）中，如何在动态特性不匹配的领域间进行策略自适应仍是一个关键挑战。本文研究跨领域离线强化学习，即通过访问来自相似源领域的离线数据集来增强基于目标领域数据集的策略学习。由于可能存在动态特性不匹配，直接合并两个数据集可能导致次优性能。现有方法通常通过源领域转移过滤或奖励修正来缓解此问题，但这可能导致对宝贵源领域数据的利用不足。为此，我们提出将源领域数据修正为目标领域数据。具体而言，我们利用逆策略模型和奖励模型来修正源转移的动作与奖励，显式实现与目标动态特性的对齐。由于有限数据可能导致模型训练不准确，我们进一步采用前向动态模型来保留比原始转移更匹配目标动态特性的修正样本。基于此，我们提出选择性转移修正（STC）算法，实现对源领域数据的可靠利用以进行策略自适应。在具有动态特性迁移的多种环境中的实验表明，STC相较于现有基线方法取得了更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of adapting reinforcement learning policies across domains with mismatched dynamics, specifically in cross-domain offline RL where a source domain dataset is available alongside a target domain dataset. The motivation is to better exploit source data without performance degradation from direct merging. The method proposes Selective Transition Correction (STC), which modifies source transitions by correcting actions and rewards using an inverse policy model and reward model to align with target dynamics, and retains only well-aligned samples via a forward dynamics model. Experimental results across environments with dynamics shifts show that STC outperforms existing baselines by effectively leveraging source data for policy adaptation.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中策略在动态不匹配的领域间适应的挑战，研究跨领域离线强化学习，其中可利用源领域数据集辅助目标领域策略学习。动机在于避免直接合并数据集导致的性能下降，并更充分地利用源数据。方法上提出了选择性转移校正（STC）算法，通过逆策略模型和奖励模型校正源转移中的动作和奖励以对齐目标动态，并利用前向动态模型保留比原始转移更匹配的样本。实验结果表明，在多种动态偏移环境中，STC通过可靠利用源领域数据，在策略适应性能上优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism</div>
<div class="meta-line">Authors: Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai, Jing Long, Tianyun Zhao, Mingxi Luo, Chen Zhou, Yucheng Guo, Qiming Yang, Wanting Xu, Wen Huang, Yunxuan Ma, Hongke Zhao, Likang Wu, Xiaotie Deng, Xi Xiao, Sheng Wen, Yicheng Gong, Junwu Xiong</div>
<div class="meta-line">First: 2026-02-05T15:30:23+00:00 · Latest: 2026-02-05T15:30:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05765v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05765v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#x27;s excellent scalability under most conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RL-VLA$^3$：基于全异步化的强化学习视觉语言动作模型加速框架</div>
<div class="mono" style="margin-top:8px">近年来，视觉-语言-动作模型已成为实现通用具身智能的关键路径，但其训练效率已成为主要瓶颈。尽管现有基于强化学习的训练框架（如RLinf）能提升模型泛化能力，但仍依赖同步执行，导致环境交互、策略生成与模型更新阶段存在严重的资源利用不足和吞吐量限制。为突破这一局限，本文首次提出并实现了一个覆盖环境交互、轨迹生成到策略更新全流程的全异步策略训练框架。通过系统借鉴大模型强化学习中的异步优化思想，本框架设计了多层次解耦架构，包括环境交互与轨迹收集的异步并行化、策略生成的流式执行以及训练更新的解耦调度。我们在多种VLA模型与环境中验证了方法的有效性：在LIBERO基准测试中，相比现有同步策略，框架最高可实现59.25%的吞吐量提升；深度优化解耦策略后，吞吐量增幅可达126.67%。通过消融实验验证了各异步组件的有效性，在8至256张GPU上的扩展律测试表明该方法在多数条件下具备优异的可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the training efficiency bottleneck in Vision-Language-Action (VLA) models for embodied intelligence, this paper introduces RL-VLA³, a fully-asynchronous reinforcement learning training framework. The method systematically decouples and asynchronizes the entire pipeline—environment interaction, policy rollout generation, and model updates—inspired by asynchronous optimization in large model RL. Experimental results on the LIBERO benchmark show throughput improvements of up to 59.25% over synchronous baselines, with further optimization achieving up to 126.67% gains, and ablation studies confirm the effectiveness of each asynchronous component while demonstrating strong scalability across 8 to 256 GPUs.</div>
<div class="mono" style="margin-top:8px">针对具身智能中视觉-语言-动作模型训练效率低下的瓶颈，本文提出了RL-VLA³，一个完全异步的强化学习训练框架。该方法受大模型强化学习中异步优化思想的启发，系统性地将环境交互、策略生成和模型更新全流程解耦并异步化。在LIBERO基准测试上的实验结果表明，相比现有同步策略，该框架实现了高达59.25%的吞吐量提升，深度优化分离策略后更可提升126.67%，消融研究验证了各异步组件的有效性，同时在8至256个GPU上的扩展性验证展示了其优秀的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Inject: Automated Prompt Injection via Reinforcement Learning</div>
<div class="meta-line">Authors: Xin Chen, Jie Zhang, Florian Tramer</div>
<div class="meta-line">First: 2026-02-05T15:14:46+00:00 · Latest: 2026-02-05T15:14:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05746v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05746v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习注入：基于强化学习的自动化提示词注入攻击</div>
<div class="mono" style="margin-top:8px">提示词注入是大语言模型智能体最关键的漏洞之一，但从优化角度实现高效自动化攻击的研究仍十分有限。现有方法严重依赖人工红队和手工构建的提示词，限制了其扩展性和适应性。我们提出AutoInject——一种强化学习框架，能生成通用、可迁移的对抗性后缀，同时优化攻击成功率与良性任务效用保持。我们的黑盒方法支持基于查询的优化及对未知模型与任务的迁移攻击。仅使用15亿参数的对抗后缀生成器，我们就在AgentDojo基准测试中成功攻破了包括GPT 5 Nano、Claude Sonnet 3.5和Gemini 2.5 Flash在内的前沿系统，为自动化提示词注入研究建立了更强基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the critical vulnerability of prompt injection in LLM agents and the limitations of existing human-dependent, non-scalable attack methods, this paper introduces AutoInject, a reinforcement learning framework designed to automatically generate universal and transferable adversarial suffixes. The method jointly optimizes for high attack success on target tasks while preserving utility on benign tasks, operating as a black-box approach that enables both query-based optimization and transfer attacks to unseen models. Key experimental results demonstrate that using only a 1.5B parameter suffix generator, AutoInject successfully compromises frontier LLM systems like GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a new, stronger baseline for automated prompt injection research.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大型语言模型智能体中提示注入这一关键安全漏洞，以及现有方法严重依赖人工红队和手工设计、可扩展性与适应性不足的问题。为此，研究者提出了AutoInject，一个基于强化学习的框架，旨在自动生成通用且可迁移的对抗性后缀，其方法同时优化攻击成功率和在良性任务上的效用保持，并支持基于查询的优化以及对未见模型和任务的迁移攻击。主要实验结果表明，仅使用一个15亿参数的对抗后缀生成器，该方法便在AgentDojo基准测试上成功攻破了包括GPT 5 Nano、Claude Sonnet 3.5和Gemini 2.5 Flash在内的前沿系统，为自动化提示注入研究建立了一个更强的新基线。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification</div>
<div class="meta-line">Authors: Taoye Yin, Haoyuan Hu, Yaxin Fan, Xinhao Chen, Xinya Wu, Kai Deng, Kezun Zhang, Feng Wang</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-05T14:49:05+00:00 · Latest: 2026-02-05T14:49:05+00:00</div>
<div class="meta-line">Comments: accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05723v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05723v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过细粒度知识验证缓解金融检索增强生成中的幻觉问题</div>
<div class="mono" style="margin-top:8px">在金融检索增强生成（RAG）系统中，由于金融领域信息的时效性，模型常依赖检索文档生成准确回答。尽管检索文档有助于填补知识空白，但模型生成的回答仍存在与检索信息相矛盾的幻觉现象。为缓解这种不一致性，我们提出一种融合细粒度知识验证的强化学习框架（RLFKV）。该方法将金融回答分解为原子知识单元，评估每个单元的正确性以计算细粒度忠实度奖励。该奖励提供更精确的优化信号，从而提升与检索文档的一致性。此外，为防止奖励欺骗行为（如过度简化的回复），我们引入信息量奖励机制，激励策略模型至少保留与基线模型同等数量的知识单元。在公开金融数据描述（FDD）任务及我们新提出的FDD-ANT数据集上的实验均显示稳定改进，验证了方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the persistent issue of hallucinations in financial Retrieval-Augmented Generation (RAG) systems, where generated responses contradict retrieved documents despite their use for accuracy, this paper introduces a Reinforcement Learning framework with Fine-grained Knowledge Verification (RLFKV). The method decomposes financial responses into atomic knowledge units to compute a fine-grained faithful reward for each unit, providing precise optimization signals to improve alignment with retrieved information; it also incorporates an informativeness reward to prevent overly concise replies by encouraging the retention of knowledge units. Experimental results on the public Financial Data Description (FDD) task and a newly proposed FDD-ANT dataset show consistent improvements, confirming the approach&#x27;s effectiveness in mitigating hallucinations.</div>
<div class="mono" style="margin-top:8px">针对金融检索增强生成（RAG）系统中生成响应与检索信息相矛盾的幻觉问题，本文提出了一种基于细粒度知识验证的强化学习框架（RLFKV）。该方法将金融响应分解为原子知识单元，通过计算每个单元的细粒度忠实度奖励来提供精确的优化信号，以增强与检索文档的一致性；同时引入信息量奖励以防止回复过于简略，鼓励模型保留至少与基础模型相当的知识单元。在公开的金融数据描述（FDD）任务和新提出的FDD-ANT数据集上的实验结果表明，该方法持续提升了性能，有效缓解了幻觉现象。</div>
</details>
</div>
<div class="card">
<div class="title">Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification</div>
<div class="meta-line">Authors: Tianyi Wang, Long Li, Hongcan Guo, Yibiao Chen, Yixia Li, Yong Wang, Yun Chen, Guanhua Chen</div>
<div class="meta-line">First: 2026-02-05T14:41:57+00:00 · Latest: 2026-02-05T14:41:57+00:00</div>
<div class="meta-line">Comments: 17 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05717v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05717v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model&#x27;s full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model&#x27;s high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>锚定策略优化：通过支撑集约束校正缓解探索坍缩</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）日益被视为一种树剪枝机制。然而，我们发现一种称为递归空间收缩的系统性病理现象，这是一种由正向锐化与负向挤压的复合动力学驱动的不可逆坍缩，导致有效备选方案的采样概率趋近于零。虽然KL正则化旨在缓解此问题，但其施加了严格的形状匹配约束，迫使策略模仿参考模型的完整密度分布，从而与正确性所需的锐化过程产生梯度冲突。我们提出锚定策略优化（APO），将范式从全局形状匹配转向支撑集覆盖。通过基于参考模型高置信度支撑集定义安全流形，APO允许为提升效率而进行激进锐化，同时在误差校正阶段选择性调用恢复力以防止坍缩。我们从理论上推导出APO可作为梯度对齐机制来最大化支撑集覆盖，实现重新激活有效分支的弹性恢复。在数学基准测试中的实证评估表明，APO打破了精度-多样性权衡，在显著提升Pass@1指标的同时，恢复了传统策略梯度方法通常丢失的Pass@K多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a systemic pathology in reinforcement learning with verifiable rewards called Recursive Space Contraction, where the policy&#x27;s sampling probability of valid alternatives collapses irreversibly due to combined positive sharpening and negative squeezing dynamics. To mitigate this, the authors propose Anchored Policy Optimization, which shifts the regularization paradigm from global shape matching via KL divergence to ensuring support coverage, defining a safe manifold based on the reference model&#x27;s high-confidence support to allow aggressive sharpening while selectively applying restorative forces during error correction. Experimental results on mathematical benchmarks show that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 scores while restoring the Pass@K diversity typically lost by standard policy gradient methods.</div>
<div class="mono" style="margin-top:8px">本文针对可验证奖励强化学习中的一种系统性病理问题——递归空间收缩，即由于正锐化和负挤压的动态组合导致策略对有效替代方案的采样概率发生不可逆的崩溃。为缓解此问题，作者提出了锚定策略优化方法，将正则化范式从基于KL散度的全局形状匹配转向确保支撑覆盖，依据参考模型的高置信度支撑定义了一个安全流形，从而允许进行积极的锐化，同时在纠错时选择性地施加恢复力以防止崩溃。在数学基准上的实验结果表明，APO打破了准确性与多样性的权衡，显著提高了Pass@1分数，并恢复了标准策略梯度方法通常丢失的Pass@K多样性。</div>
</details>
</div>
<div class="card">
<div class="title">DeepAgent: A General Reasoning Agent with Scalable Toolsets</div>
<div class="meta-line">Authors: Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, Zhicheng Dou</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2025-10-24T16:24:01+00:00 · Latest: 2026-02-05T14:08:05+00:00</div>
<div class="meta-line">Comments: Accepted by WWW 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21618v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.21618v3">PDF</a> · <a href="https://github.com/RUC-NLPIR/DeepAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To manage long-horizon interactions, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepAgent：具备可扩展工具集的通用推理智能体</div>
<div class="mono" style="margin-top:8px">大型推理模型已展现出强大的问题解决能力，但现实任务常需外部工具与长程交互。现有智能体框架多遵循预定义流程，限制了自主性与全局任务完成度。本文提出DeepAgent，一种端到端深度推理智能体，可在单一连贯的推理过程中执行自主思考、工具发现与动作执行。为管理长程交互，我们引入自主记忆折叠机制，将过往交互压缩为结构化的情景记忆、工作记忆与工具记忆，在保留关键信息的同时减少误差累积。为实现高效稳定的通用工具使用，我们开发了端到端强化学习策略ToolPO，利用LLM模拟的API并通过工具调用优势归因对工具调用令牌进行细粒度信用分配。在八个基准测试（包括通用工具使用任务（ToolBench、API-Bank、TMDB、Spotify、ToolHop）与下游应用（ALFWorld、WebShop、GAIA、HLE））上的大量实验表明，DeepAgent在标注工具和开放集工具检索场景中均持续优于基线模型。代码与演示见https://github.com/RUC-NLPIR/DeepAgent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces DeepAgent, a general reasoning agent designed to overcome the limitations of existing agent frameworks that rely on predefined workflows, which restrict autonomous and global task completion. The method features an end-to-end deep reasoning process that integrates autonomous thinking, tool discovery, and action execution, along with an autonomous memory folding mechanism to compress past interactions into structured memories, reducing error accumulation. For efficient tool use, it employs ToolPO, an end-to-end reinforcement learning strategy that leverages LLM-simulated APIs and tool-call advantage attribution to assign fine-grained credit to tool invocation tokens. Experimental results on eight benchmarks, including general tool-use tasks and downstream applications, show that DeepAgent consistently outperforms baselines in both labeled-tool and open-set tool retrieval scenarios.</div>
<div class="mono" style="margin-top:8px">本文提出了DeepAgent，一种通用推理智能体，旨在解决现有智能体框架依赖预定义工作流而限制自主性和全局任务完成的局限性。该方法采用端到端的深度推理过程，整合了自主思考、工具发现和动作执行，并引入了自主记忆折叠机制，将过去的交互压缩为结构化记忆以减少错误累积。为高效使用工具，它采用了ToolPO，一种端到端的强化学习策略，利用LLM模拟的API和工具调用优势归因，为工具调用令牌分配细粒度信用。在八个基准测试（包括通用工具使用任务和下游应用）上的实验结果表明，DeepAgent在标记工具和开放集工具检索场景中均持续优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Variable Search Stepsize for Randomized Local Search in Multi-Objective Combinatorial Optimization</div>
<div class="meta-line">Authors: Xuepeng Ren, Maocai Wang, Guangming Dai, Zimin Liang, Qianrong Liu, Shengxiang Yang, Miqing Li</div>
<div class="meta-line">First: 2026-02-05T13:59:05+00:00 · Latest: 2026-02-05T13:59:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05675v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Over the past two decades, research in evolutionary multi-objective optimization has predominantly focused on continuous domains, with comparatively limited attention given to multi-objective combinatorial optimization problems (MOCOPs). Combinatorial problems differ significantly from continuous ones in terms of problem structure and landscape. Recent studies have shown that on MOCOPs multi-objective evolutionary algorithms (MOEAs) can even be outperformed by simple randomised local search. Starting with a randomly sampled solution in search space, randomised local search iteratively draws a random solution (from an archive) to perform local variation within its neighbourhood. However, in most existing methods, the local variation relies on a fixed neighbourhood, which limits exploration and makes the search easy to get trapped in local optima. In this paper, we present a simple yet effective local search method, called variable stepsize randomized local search (VS-RLS), which adjusts the stepsize during the search. VS-RLS transitions gradually from a broad, exploratory search in the early phases to a more focused, fine-grained search as the search progresses. We demonstrate the effectiveness and generalizability of VS-RLS through extensive evaluations against local search and MOEAs methods on diverse MOCOPs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多目标组合优化中随机局部搜索的变步长策略</div>
<div class="mono" style="margin-top:8px">过去二十年间，进化多目标优化研究主要集中于连续域，对多目标组合优化问题的关注相对有限。组合问题在问题结构与解空间特征上与连续问题存在显著差异。近期研究表明，在多目标组合优化问题上，简单的随机局部搜索甚至能超越多目标进化算法。随机局部搜索从搜索空间中随机采样初始解，迭代地从存档中随机选取解进行邻域内的局部变异。然而，现有方法大多依赖固定邻域进行局部变异，这限制了探索能力且易使搜索陷入局部最优。本文提出一种简洁高效的局部搜索方法——变步长随机局部搜索，通过在搜索过程中动态调整步长，实现从早期广泛探索到后期精细搜索的渐进过渡。通过对多种多目标组合优化问题开展局部搜索与多目标进化算法的对比实验，验证了该方法的有效性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited attention given to multi-objective combinatorial optimization problems (MOCOPs) compared to continuous domains, and the observation that simple randomized local search can outperform more complex multi-objective evolutionary algorithms on such problems, this paper introduces a variable stepsize randomized local search (VS-RLS) method. The method addresses the limitation of fixed neighbourhoods in existing local search by dynamically adjusting the stepsize, transitioning from broad exploration early on to fine-grained exploitation later. Experimental evaluations across diverse MOCOPs demonstrate that VS-RLS is effective and generalizable, outperforming both standard local search and MOEAs.</div>
<div class="mono" style="margin-top:8px">本文的动机在于多目标组合优化问题相较于连续域问题研究不足，且简单随机局部搜索在此类问题上可能优于复杂多目标进化算法。为此，论文提出了一种变步长随机局部搜索方法，通过动态调整搜索步长，克服了固定邻域搜索易陷入局部最优的限制，实现了从早期广泛探索到后期精细开发的过渡。在多种多目标组合优化问题上的广泛实验表明，该方法具有有效性和通用性，性能优于传统局部搜索和多目标进化算法。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Anthony Kobanda, Rémy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer</div>
<div class="meta-line">First: 2024-12-19T14:00:03+00:00 · Latest: 2026-02-05T13:36:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.14865v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.14865v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略的层次子空间：面向持续离线强化学习</div>
<div class="mono" style="margin-top:8px">本文研究持续强化学习场景，要求智能体在不断适应新任务的同时保持已习得技能，核心挑战在于避免遗忘历史知识并确保随任务数量增长的可扩展性。这类问题在自主机器人与视频游戏仿真中尤为突出，特别是在拓扑结构或运动学特性易变的导航任务中。为此，我们提出HiSPO——一种专为基于离线数据的导航场景持续学习设计的层次化框架。该方法利用神经网络构建的策略子空间，在保留已有知识的同时实现对新任务的灵活高效适应。通过严谨的实验研究，我们在经典MuJoCo迷宫环境与复杂类游戏导航仿真中验证了方法的有效性，其在传统持续学习指标（尤其是内存使用效率与适应能力）上均展现出竞争优势与良好适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of continual offline reinforcement learning, where an agent must adapt to new tasks without forgetting previously learned skills, a critical issue in domains like autonomous robotics and video game navigation. The authors propose HiSPO, a hierarchical framework that utilizes distinct policy subspaces within neural networks to facilitate flexible adaptation to new tasks while preserving past knowledge. Experimental results in MuJoCo maze environments and complex video game simulations demonstrate that HiSPO achieves competitive performance, with notable efficiency in memory usage and adaptability according to standard continual learning metrics.</div>
<div class="mono" style="margin-top:8px">本文针对持续离线强化学习中的挑战，即智能体需适应新任务而不遗忘已学技能，这在自主机器人和视频游戏导航等领域尤为重要。作者提出了HiSPO这一分层框架，利用神经网络中的不同策略子空间来实现对新任务的灵活适应，同时保留已有知识。在MuJoCo迷宫环境和复杂视频游戏模拟中的实验结果表明，HiSPO在性能上具有竞争力，尤其在内存使用效率和适应性方面，符合经典持续学习指标的要求。</div>
</details>
</div>
<div class="card">
<div class="title">UAV Trajectory Optimization via Improved Noisy Deep Q-Network</div>
<div class="meta-line">Authors: Zhang Hengyu, Maryam Cheraghy, Liu Wei, Armin Farhadi, Meysam Soltanpour, Zhong Zhuoqing</div>
<div class="meta-line">First: 2026-02-05T13:23:47+00:00 · Latest: 2026-02-05T13:23:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05644v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05644v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes an Improved Noisy Deep Q-Network (Noisy DQN) to enhance the exploration and stability of Unmanned Aerial Vehicle (UAV) when applying deep reinforcement learning in simulated environments. This method enhances the exploration ability by combining the residual NoisyLinear layer with an adaptive noise scheduling mechanism, while improving training stability through smooth loss and soft target network updates. Experiments show that the proposed model achieves faster convergence and up to $+40$ higher rewards compared to standard DQN and quickly reach to the minimum number of steps required for the task 28 in the 15 * 15 grid navigation environment set up. The results show that our comprehensive improvements to the network structure of NoisyNet, exploration control, and training stability contribute to enhancing the efficiency and reliability of deep Q-learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于改进噪声深度Q网络的无人机轨迹优化</div>
<div class="mono" style="margin-top:8px">本文提出一种改进的噪声深度Q网络（Noisy DQN），用于增强无人机在模拟环境中应用深度强化学习时的探索能力与稳定性。该方法通过将残差噪声线性层与自适应噪声调度机制相结合来提升探索能力，同时通过平滑损失函数和柔性目标网络更新提高训练稳定性。实验表明，在15×15网格导航环境设置中，所提模型相比标准DQN实现了更快的收敛速度，奖励值提升最高达+40，并能快速达到任务28所需的最小步数。结果表明，我们对NoisyNet网络结构、探索控制和训练稳定性的综合改进，有助于提升深度Q学习的效率与可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to enhance exploration and stability in deep reinforcement learning for UAV trajectory optimization. The method introduces an Improved Noisy Deep Q-Network that combines a residual NoisyLinear layer with adaptive noise scheduling to boost exploration, alongside smooth loss and soft target network updates to improve training stability. Experimental results in a simulated 15x15 grid navigation environment demonstrate that the model achieves faster convergence, up to +40 higher rewards compared to standard DQN, and quickly reaches the minimum required steps for the task, indicating improved efficiency and reliability.</div>
<div class="mono" style="margin-top:8px">本文旨在通过深度强化学习提升无人机轨迹优化中的探索能力和稳定性。方法提出了一种改进的噪声深度Q网络，结合残差噪声线性层与自适应噪声调度机制以增强探索，同时采用平滑损失和软目标网络更新来提高训练稳定性。在模拟的15x15网格导航环境中的实验结果表明，该模型相比标准DQN实现了更快的收敛速度、高达+40的奖励提升，并能迅速达到任务所需的最小步数，从而有效提高了深度Q学习的效率和可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Rewards as Labels: Revisiting RLVR from a Classification Perspective</div>
<div class="meta-line">Authors: Zepeng Zhai, Meilin Chen, Jiaxuan Zhao, Junlang Qian, Lei Shen, Yuan Lu</div>
<div class="meta-line">First: 2026-02-05T13:11:36+00:00 · Latest: 2026-02-05T13:11:36+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05630v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05630v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励作为标签：从分类视角重审可验证奖励强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）近期通过提供明确的规则监督，显著提升了大型语言模型在复杂推理任务中的能力。在RLVR方法中，GRPO及其变体取得了优异的实证性能。尽管成果显著，我们发现这些方法存在正样本梯度错配与负样本梯度主导问题，导致策略更新效率低下且非最优。为解决这些问题，我们提出“奖励作为标签”（REAL）框架，将可验证奖励重新定义为分类标签而非标量权重，从而将策略优化重构为分类问题。在此基础上，我们进一步引入锚定逻辑值以增强策略学习。分析表明，REAL能产生单调有界的梯度加权机制，实现回合间梯度分配的平衡，有效缓解上述错配问题。在数学推理基准上的大量实验显示，REAL提升了训练稳定性，并持续超越GRPO及DAPO等强效变体。在15亿参数模型中，REAL的Pass@1平均指标较DAPO提升6.7%。该优势在70亿参数模型中进一步扩大，REAL分别超越DAPO和GSPO达6.2%和1.7%。值得注意的是，即使采用基础二元交叉熵损失，REAL仍保持稳定，平均性能超过DAPO达4.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by identified inefficiencies in existing Reinforcement Learning with Verifiable Rewards (RLVR) methods, specifically Gradient Misassignment in Positives and Gradient Domination in Negatives in approaches like GRPO, this paper proposes the Rewards as Labels (REAL) framework. The method reformulates policy optimization as a classification problem by treating verifiable rewards as categorical labels instead of scalar weights, and further introduces anchor logits to enhance learning. Experimental results on mathematical reasoning benchmarks demonstrate that REAL improves training stability and consistently outperforms strong baselines, including GRPO and DAPO, achieving significant performance gains of up to 6.7% in Pass@1 on a 1.5B model, with these improvements scaling effectively to larger 7B models.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有可验证奖励强化学习方法（如GRPO）中存在的梯度分配低效问题，即正样本的梯度误分配和负样本的梯度主导。为此，论文提出了“奖励即标签”框架，其核心方法是将可验证奖励重新定义为分类标签而非标量权重，从而将策略优化重构为一个分类问题，并引入了锚定逻辑值以增强策略学习。在数学推理基准上的主要实验结果表明，该方法显著提升了训练稳定性，并在1.5B和7B模型上持续超越了GRPO、DAPO等强基线模型，平均Pass@1指标最高提升了6.7%，且增益具有良好的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Mode-Dependent Rectification for Stable PPO Training</div>
<div class="meta-line">Authors: Mohamad Mohamad, Francesco Ponzio, Xavier Descombes</div>
<div class="meta-line">First: 2026-02-05T12:54:19+00:00 · Latest: 2026-02-05T12:54:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05619v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05619v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mode-dependent architectural components (layers that behave differently during training and evaluation, such as Batch Normalization or dropout) are commonly used in visual reinforcement learning but can destabilize on-policy optimization. We show that in Proximal Policy Optimization (PPO), discrepancies between training and evaluation behavior induced by Batch Normalization lead to policy mismatch, distributional drift, and reward collapse. We propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO under mode-dependent layers without architectural changes. Experiments across procedurally generated games and real-world patch-localization tasks demonstrate that MDR consistently improves stability and performance, and extends naturally to other mode-dependent layers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模式依赖校正的稳定PPO训练方法</div>
<div class="mono" style="margin-top:8px">模式依赖架构组件（如批归一化或丢弃层等在训练与评估阶段表现不同的层）在视觉强化学习中广泛应用，但可能破坏同策略优化的稳定性。本文证明，在近端策略优化（PPO）中，批归一化引起的训练-评估行为差异会导致策略失配、分布漂移和奖励崩溃。我们提出模式依赖校正（MDR）——一种轻量级双阶段训练流程，可在不改变架构的前提下稳定模式依赖层下的PPO训练。在程序生成游戏和现实世界补丁定位任务中的实验表明，MDR能持续提升训练稳定性与性能，并可自然扩展至其他模式依赖层。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the instability in Proximal Policy Optimization (PPO) training caused by mode-dependent layers like Batch Normalization, which behave differently during training and evaluation, leading to policy mismatch and reward collapse. The authors propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO without modifying the network architecture. Experimental results on procedurally generated games and real-world patch-localization tasks show that MDR consistently enhances training stability and improves performance, and it generalizes effectively to other mode-dependent layers.</div>
<div class="mono" style="margin-top:8px">本文针对在视觉强化学习中常用的模式依赖层（如批归一化）导致近端策略优化训练不稳定的问题，这些层在训练和评估时行为差异会引发策略失配和奖励崩溃。作者提出了模式依赖校正方法，这是一种轻量级的双阶段训练流程，无需改变网络架构即可稳定PPO训练。在程序生成游戏和现实世界补丁定位任务上的实验表明，该方法能持续提升训练稳定性和性能，并可自然扩展到其他模式依赖层。</div>
</details>
</div>
<div class="card">
<div class="title">Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation</div>
<div class="meta-line">Authors: Zhiqi Yu, Zhangquan Chen, Mengting Liu, Heye Zhang, Liangqiong Qu</div>
<div class="meta-line">First: 2026-02-05T11:07:14+00:00 · Latest: 2026-02-05T11:07:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05548v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示隐式优势对称性：为何GRPO在探索与难度适应中面临困境</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR），尤其是GRPO，已成为激发大语言模型推理能力的标准方法。然而，其在探索效率和难度适应性方面的表现仍是待解难题。本研究指出，这些瓶颈源于组间相对优势估计（GRAE）固有的隐式优势对称性。该对称性引发两个关键局限：（i）在组层面，正确与错误轨迹间严格的权重对称性使未采样动作的对数概率保持不变，从而阻碍了对新颖正确解的探索；（ii）在样本层面，算法隐式优先处理中等难度样本，无法适应难度聚焦的非平稳需求。通过受控实验，我们揭示这种对称特性并非最优，并得出两个关键结论：（i）非对称抑制正确轨迹的优势能促进必要探索；（ii）通过类课程学习策略——初期优先处理简单样本再逐步转向复杂样本——可最大化学习效率。基于这些发现，我们提出非对称GRAE（A-GRAE），能动态调节探索激励与样本难度聚焦。在七个基准测试上的实验表明，A-GRAE对大语言模型和多模态大语言模型均能持续提升GRPO及其变体的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the exploration and difficulty adaptation inefficiencies in GRPO, a prominent method for Reinforcement Learning with Verifiable Rewards (RLVR) in LLM reasoning. The authors identify the root cause as an implicit advantage symmetry in the Group Relative Advantage Estimation (GRAE) component, which limits exploration by not updating unsampled action logits and fails to adaptively prioritize samples of varying difficulty. Through controlled experiments, they demonstrate that asymmetrically suppressing advantages for correct trajectories promotes better exploration and that a curriculum-like focus, transitioning from simple to complex samples, enhances learning. Motivated by these insights, they propose Asymmetric GRAE (A-GRAE), a method that dynamically adjusts exploration incentives and difficulty focus, and show its consistent improvements over GRPO and its variants across seven benchmarks for both LLMs and MLLMs.</div>
<div class="mono" style="margin-top:8px">本文研究了GRPO方法在大型语言模型推理的强化学习可验证奖励框架中，存在的探索与难度适应效率低下的问题。作者指出其根本原因在于组相对优势估计中隐含的优势对称性，这种对称性限制了探索能力，无法更新未采样动作的对数概率，且不能自适应地优先处理不同难度的样本。通过受控实验，他们证明非对称地抑制正确轨迹的优势可以促进更好的探索，而采用从简单样本逐渐过渡到复杂样本的课程式学习策略能提升学习效率。基于这些发现，他们提出了非对称组相对优势估计方法，该方法能动态调整探索激励和难度关注焦点，并在七个基准测试中，针对大型语言模型和多模态大模型，一致地改进了GRPO及其变体的性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Framework for Rethinking Policy Divergence Measures in GRPO</div>
<div class="meta-line">Authors: Qingyuan Wu, Yuhui Wang, Simon Sinong Zhan, Yanning Dai, Shilong Deng, Sarra Habchi, Qi Zhu, Matthias Gallé, Chao Huang</div>
<div class="meta-line">First: 2026-02-05T09:56:16+00:00 · Latest: 2026-02-05T09:56:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05494v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05494v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GRPO中策略差异度量的统一框架重构</div>
<div class="mono" style="margin-top:8px">基于验证奖励的强化学习已成为提升大语言模型推理能力的关键范式。现有GRPO及其变体等RLVR方法多通过似然比截断约束策略差异以确保稳定更新。本文提出统一截断框架，以广义策略差异概念统摄现有方法，涵盖似然比与KL散度并扩展至其他度量。该框架为系统分析不同策略差异度量如何影响探索与性能提供理论依据。我们进一步提出KL3估计器——一种方差缩减的KL散度蒙特卡洛估计器，作为关键策略差异约束。理论证明基于KL3的约束在数学上等效于非对称比率截断，能将概率质量重新分配至高置信度动作，在保持GRPO类方法简洁性的同时增强探索能力。数学推理基准实验表明，将KL3估计器融入GRPO可同步提升训练稳定性与最终性能，印证了策略优化中理论化策略差异约束的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to systematically understand and improve policy divergence constraints in reinforcement learning with verified reward (RLVR) methods like GRPO, which are crucial for enhancing large language models&#x27; reasoning. The authors propose a unified clipping framework that generalizes existing methods by incorporating various policy divergence measures, including likelihood ratios and KL divergences, and they specifically identify and analyze the KL3 estimator as a variance-reduced Monte Carlo estimator for KL divergence. The main experimental results on mathematical reasoning benchmarks show that integrating the KL3 estimator into GRPO leads to improved training stability and final performance, validating the framework&#x27;s effectiveness in promoting exploration while maintaining simplicity.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要系统性地理解和改进基于验证奖励的强化学习（RLVR）方法（如GRPO）中的策略差异约束，这些方法对提升大语言模型的推理能力至关重要。作者提出了一个统一的裁剪框架，通过纳入包括似然比和KL散度在内的多种策略差异度量来泛化现有方法，并特别将KL3估计器识别为一种方差减少的KL散度蒙特卡洛估计器进行分析。在数学推理基准上的主要实验结果表明，将KL3估计器整合到GRPO中能提高训练稳定性和最终性能，验证了该框架在保持方法简洁性的同时促进探索的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Log2Motion: Biomechanical Motion Synthesis from Touch Logs</div>
<div class="meta-line">Authors: Michał Patryk Miazga, Hannah Bussmann, Antti Oulasvirta, Patrick Ebel</div>
<div class="meta-line">First: 2026-01-28T21:04:19+00:00 · Latest: 2026-02-05T09:24:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21043v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21043v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Log2Motion：基于触摸日志的生物力学运动合成</div>
<div class="mono" style="margin-top:8px">移动设备的大规模触摸数据难以揭示其背后的交互过程。尽管生物力学仿真能阐明运动控制机制，但尚未应用于触摸交互研究。为填补这一空白，我们提出一项新颖的计算任务：直接从日志合成合理运动。核心创新在于采用强化学习驱动的肌肉骨骼正向仿真，生成与触摸日志事件一致的生物力学合理运动序列。通过将软件模拟器集成至物理仿真环境，实现了生物力学模型对真实应用的实时操控。Log2Motion能够从触摸日志合成丰富的用户运动数据，包括动作、速度、精度及用力程度的估计。我们通过动作捕捉实验数据与既有研究对比验证生成运动的合理性，并在大规模数据集中展示了该方法的有效性。生物力学运动合成为理解日志数据提供了新途径，揭示了触摸交互背后的人体工学与运动控制机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the gap between abundant touch log data and the limited understanding of the physical interactions that produce them, this paper introduces Log2Motion, a method to synthesize biomechanically plausible human motion directly from touch logs. The core approach integrates a reinforcement learning-driven musculoskeletal forward simulation with a software emulator in a physics simulator, enabling biomechanical models to interact with real applications in real-time and generate motion sequences consistent with logged touch events. Experimental validation against human motion capture data and prior findings demonstrates that Log2Motion can produce rich syntheses estimating motion, speed, accuracy, and effort, offering a novel way to analyze large-scale touch datasets and illuminate the ergonomics and motor control of touch interactions.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决移动设备触摸日志数据丰富但难以揭示其背后物理交互过程的问题，提出了Log2Motion方法，直接从触摸日志合成生物力学上合理的人体运动。该方法的核心是通过强化学习驱动的肌肉骨骼前向仿真，将软件模拟器集成到物理模拟器中，使生物力学模型能够实时操作真实应用，生成与日志事件一致的运动序列。实验通过对比人体动作捕捉数据和已有研究结果进行验证，表明Log2Motion能够合成包含运动、速度、准确性和努力程度估计的丰富运动数据，为大规模触摸数据集分析提供了新途径，有助于揭示触摸交互的人体工程学和运动控制机制。</div>
</details>
</div>
<div class="card">
<div class="title">ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation</div>
<div class="meta-line">Authors: Yiwen Duan, Jing Ye, Xinpei Zhao</div>
<div class="meta-line">First: 2026-02-05T09:20:23+00:00 · Latest: 2026-02-05T09:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05472v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05472v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALIVE：通过对抗学习与指导性言语评估唤醒大语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）实现专家级推理能力的探索长期受制于顽固的“奖励瓶颈”：传统强化学习依赖的标量奖励存在扩展成本高昂、跨领域适应性脆弱、且对解决方案内在逻辑无感知的缺陷。这种对外部贫乏信号的依赖阻碍了模型对推理原则形成深刻自洽的理解。本文提出ALIVE（基于指导性言语评估的对抗学习），一种无需人工干预的对齐框架，旨在超越标量奖励优化，实现内在推理能力的习得。ALIVE以“认知协同”原则为基础，将问题提出、求解与评判统一于单一策略模型中，从而内化正确性逻辑。通过结合对抗学习与指导性言语反馈，ALIVE使模型能够直接从原始语料中内化评估标准，有效将外部评判转化为内生的推理能力。在数学推理、代码生成和通用逻辑推理基准上的实证评估表明，ALIVE能持续缓解奖励信号的局限性。在相同数据与算力条件下，该框架实现了准确率提升、跨领域泛化能力显著增强以及更高的自我纠错率。这些结果表明，推理三元组促成了能力增长的自我维持轨迹，使ALIVE成为无需人工监督的通用推理对齐的可扩展基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional reinforcement learning&#x27;s scalar rewards, which are costly, brittle, and blind to logical structure, this paper introduces ALIVE, a framework that integrates adversarial learning with instructive verbal feedback to internalize reasoning principles. The method unifies problem posing, solving, and judging within a single model to foster intrinsic reasoning acquisition through cognitive synergy. Experimental results across mathematical reasoning, code generation, and logical inference benchmarks show that ALIVE mitigates reward signal limitations, achieving improved accuracy, better cross-domain generalization, and higher self-correction rates without additional human supervision.</div>
<div class="mono" style="margin-top:8px">本文的动机是传统强化学习依赖的标量奖励存在成本高、脆弱且忽视逻辑结构的瓶颈，因此提出了ALIVE框架，通过对抗性学习和指导性语言反馈使模型内化推理原则。该方法基于认知协同原理，将问题提出、解决和评判统一于单一策略模型中，以实现内在推理能力的获取。在数学推理、代码生成和逻辑推理基准上的实验结果表明，ALIVE有效缓解了奖励信号的局限性，在相同数据和计算条件下提升了准确性，显著改善了跨领域泛化能力，并提高了自我纠正率，为无需人工监督的通用推理对齐提供了可扩展的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Optimization is Not Enough: Why Problem Formulation Deserves Equal Attention</div>
<div class="meta-line">Authors: Iván Olarte Rodríguez, Gokhan Serhat, Mariusz Bujny, Fabian Duddeck, Thomas Bäck, Elena Raponi</div>
<div class="meta-line">First: 2026-02-05T09:15:19+00:00 · Latest: 2026-02-05T09:15:19+00:00</div>
<div class="meta-line">Comments: 25 pages, 11 figures, 4 table, submitted at Conference on Evolutionary Computation, Theory and Applications (ECTA) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05466v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05466v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Black-box optimization is increasingly used in engineering design problems where simulation-based evaluations are costly and gradients are unavailable. In this context, the optimization community has largely analyzed algorithm performance in context-free setups, while not enough attention has been devoted to how problem formulation and domain knowledge may affect the optimization outcomes. We address this gap through a case study in the topology optimization of laminated composite structures, formulated as a black-box optimization problem. Specifically, we consider the design of a cantilever beam under a volume constraint, intending to minimize compliance while optimizing both the structural topology and fiber orientations. To assess the impact of problem formulation, we explicitly separate topology and material design variables and compare two strategies: a concurrent approach that optimizes all variables simultaneously without leveraging physical insight, and a sequential approach that optimizes variables of the same nature in stages. Our results show that context-agnostic strategies consistently lead to suboptimal or non-physical designs. In contrast, the sequential strategy yields better-performing and more interpretable solutions. These findings underscore the value of incorporating, when available, domain knowledge into the optimization process and motivate the development of new black-box benchmarks that reward physically informed and context-aware optimization strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>优化并非万能：为何问题建模同样值得重视</div>
<div class="mono" style="margin-top:8px">黑箱优化在基于仿真的评估成本高昂且梯度不可得的工程设计问题中应用日益广泛。在此背景下，优化领域的研究大多聚焦于无上下文环境下的算法性能分析，而对问题建模与领域知识如何影响优化结果的关注不足。本文以层合复合材料结构的拓扑优化为例，将其构建为黑箱优化问题，以填补这一研究空白。具体而言，我们以体积约束下的悬臂梁设计为研究对象，旨在最小化柔度并同时优化结构拓扑与纤维取向。为评估问题建模的影响，我们明确分离拓扑变量与材料设计变量，并比较两种策略：一种是同时优化所有变量且不利用物理洞察的并行方法，另一种是分阶段优化同类变量的顺序方法。结果表明，无视上下文的策略始终导致次优或非物理的设计；而顺序策略则能产生性能更优、可解释性更强的解。这些发现强调了在优化过程中融入领域知识（若可获得）的价值，并激励开发能奖励具备物理洞察与上下文感知的优化策略的新型黑箱基准测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper argues that in black-box optimization for engineering design, where simulations are expensive and gradients are unavailable, the community has overly focused on algorithm performance in isolation while neglecting the critical role of problem formulation informed by domain knowledge. To demonstrate this, the authors conduct a case study on the topology optimization of laminated composite cantilever beams, comparing a concurrent optimization strategy that treats all design variables simultaneously without physical insight against a sequential strategy that optimizes variables of the same nature in stages. The experimental results reveal that the context-agnostic concurrent approach consistently produces suboptimal or non-physical designs, whereas the sequential strategy yields superior performance and more interpretable solutions, highlighting the necessity of integrating domain knowledge into the optimization process.</div>
<div class="mono" style="margin-top:8px">本文指出，在仿真成本高昂且梯度不可用的工程设计的黑盒优化中，研究界过度孤立地关注算法性能，而忽视了基于领域知识的问题表述的关键作用。为证明这一点，作者以层合复合材料悬臂梁的拓扑优化为例进行研究，比较了两种策略：一种是同时优化所有设计变量而不利用物理洞察的并发方法，另一种是按阶段优化同类变量的顺序方法。实验结果表明，不考虑上下文的并发方法始终产生次优或非物理的设计，而顺序策略则能获得性能更优且更可解释的解，这强调了将领域知识融入优化过程的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR</div>
<div class="meta-line">Authors: Jiaying Zhang, Lei Shi, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He</div>
<div class="meta-line">First: 2026-01-14T10:41:34+00:00 · Latest: 2026-02-05T09:12:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09361v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09361v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoRA：面向RLVR的几何感知低秩自适应方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）对推进大规模推理模型至关重要。然而，现有参数高效方法（如PiSSA和MiLoRA）专为监督微调（SFT）设计，未考虑RLVR特有的优化动态与几何结构。直接应用这些方法会导致谱崩溃和优化不稳定，严重限制模型性能。同时，利用更新稀疏性的替代方法因非结构化计算在现代硬件上面临显著效率瓶颈。为解决这些问题，我们提出GeoRA（几何感知低秩自适应），该方法利用RL更新子空间的各向异性与可压缩特性。GeoRA通过在几何约束子空间内进行奇异值分解（SVD）提取主方向来初始化适配器，同时冻结残差分量。此方法保留了预训练几何结构，并通过稠密算子实现高效GPU计算。在Qwen和Llama上的实验表明，GeoRA缓解了几何失准导致的优化瓶颈，在关键数学基准测试中持续超越现有低秩基线，达到最先进（SOTA）水平。此外，GeoRA在领域外任务中展现出优异的泛化能力与抗灾难性遗忘韧性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of existing parameter-efficient fine-tuning methods like PiSSA and MiLoRA, which are designed for supervised fine-tuning and fail to account for the unique optimization dynamics and geometric structures in Reinforcement Learning with Verifiable Rewards (RLVR), leading to spectral collapse and instability. To address this, the authors propose GeoRA, a geometry-aware low-rank adaptation method that initializes adapters by extracting principal directions via Singular Value Decomposition within a constrained subspace, preserving pre-trained geometric structures and enabling efficient GPU computation. Experimental results on models such as Qwen and Llama show that GeoRA mitigates optimization bottlenecks, outperforms established low-rank baselines on mathematical benchmarks with state-of-the-art performance, and demonstrates superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有参数高效微调方法如PiSSA和MiLoRA专为监督微调设计，无法适应可验证奖励强化学习（RLVR）中独特的优化动态和几何结构，导致谱崩溃和不稳定性。为此，作者提出GeoRA，一种几何感知的低秩适应方法，通过在几何约束子空间中使用奇异值分解提取主方向来初始化适配器，从而保留预训练几何结构并实现高效的GPU计算。在Qwen和Llama等模型上的实验结果表明，GeoRA缓解了由几何错位引起的优化瓶颈，在关键数学基准测试中超越了现有低秩基线，取得了最先进的性能，并在域外任务中表现出优异的泛化能力和对灾难性遗忘的抵抗力。</div>
</details>
</div>
<div class="card">
<div class="title">When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL</div>
<div class="meta-line">Authors: Jan Malte Töpperwien, Aditya Mohan, Marius Lindauer</div>
<div class="meta-line">First: 2026-02-05T09:08:17+00:00 · Latest: 2026-02-05T09:08:17+00:00</div>
<div class="meta-line">Comments: 27 pages, 19 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05459v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05459v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\approx$ 20\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习超参数何时呈现良性特征？离线目标条件强化学习研究</div>
<div class="mono" style="margin-top:8px">深度强化学习中的超参数敏感性常被视为不可避免，但其本质究竟源于强化学习问题本身，还是特定训练机制所加剧，尚未明确。本研究在离线目标条件强化学习框架下探讨该问题——该场景中数据分布固定，且可通过预设数据质量变化显式控制非平稳性。我们同时考察了平稳与非平稳机制下的不同数据质量，并涵盖两种代表性算法：HIQL（基于自举时序差分学习）与QRL（拟度量表示学习）。整体而言，即使受控非平稳条件下，算法对超参数配置变化的鲁棒性仍显著高于在线强化学习常见报告水平。当存在适量专家数据（约20%）时，QRL保持广泛稳定的近优区域，而HIQL则呈现随训练阶段显著漂移的尖锐最优区间。为解释此差异，我们提出跨目标梯度对齐诊断方法，发现自举目标会产生更强的破坏性梯度干扰，该现象与超参数敏感性直接相关。这些结果表明：训练过程中对超参数配置的高敏感性并非强化学习的必然属性，而是由自举机制动态特性所放大，这为设计更鲁棒的算法目标提供了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether the notorious hyperparameter sensitivity in deep reinforcement learning is an inherent problem or is exacerbated by specific training mechanisms, focusing on offline goal-conditioned RL where data distributions are fixed and non-stationarity can be controlled. The study examines two algorithms, HIQL and QRL, across stationary and non-stationary regimes with varying data qualities, finding substantially greater robustness to hyperparameter changes than typically seen in online RL, especially when modest expert data is present. Key results show that QRL maintains broad stable performance regions, while HIQL exhibits sharp, drifting optima; an introduced gradient alignment diagnostic reveals that bootstrapped objectives in HIQL suffer from destructive gradient interference, directly correlating with hyperparameter sensitivity, suggesting sensitivity is not inevitable but amplified by bootstrapping dynamics.</div>
<div class="mono" style="margin-top:8px">本文探讨深度强化学习中超参数敏感性的根源，研究其是固有问题还是由特定训练机制加剧，聚焦于离线目标条件强化学习，其中数据分布固定且非平稳性可受控调节。研究在平稳和非平稳机制下，针对不同数据质量，比较了HIQL和QRL两种代表性算法，发现超参数配置的鲁棒性显著高于在线强化学习常见报告，尤其当存在适度专家数据时。主要实验结果表明，QRL保持广泛稳定的近优区域，而HIQL则呈现尖锐且随训练阶段漂移的最优点；通过引入目标间梯度对齐诊断，发现自举目标存在更强的破坏性梯度干扰，这与超参数敏感性直接相关，表明高敏感性并非不可避免，而是由自举动态放大，为设计更鲁棒的算法目标提供了途径。</div>
</details>
</div>
<div class="card">
<div class="title">Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options</div>
<div class="meta-line">Authors: Joongkyu Lee, Seouh-won Yi, Min-hwan Oh</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-21T15:11:01+00:00 · Latest: 2026-02-05T09:06:17+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18713v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.18713v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL&#x27;s recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\tilde{O}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter&#x27;s norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $Ω\left( \frac{d}{K \sqrt{T}} \right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越两两比较的偏好强化学习：多选项反馈的效益</div>
<div class="mono" style="margin-top:8px">本研究旨在提升样本效率，探讨在线偏好强化学习（PbRL）的理论框架。尽管PbRL（尤其在大型语言模型对齐方面）的实证成功推动了理论进展，现有研究多局限于两两比较。近期少数工作（Zhu等，2023；Mukherjee等，2024；Thekumparampil等，2024）尝试使用多选项比较和排序反馈，但其性能保证未能随反馈长度增加而提升，甚至可能恶化。为填补这一空白，我们采用Plackett-Luce模型处理动作子集的排序反馈，提出M-AUPO算法——通过最大化所提供子集内的平均不确定性来选择多动作。理论证明M-AUPO的次优性间隙为$\tilde{O}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$，其中$T$为总轮数，$d$为特征维度，$|S_t|$为第$t$轮子集规模。该结果表明：更大子集直接提升性能，且边界避免了以往研究中普遍存在的对未知参数范数的指数依赖。此外，我们建立了近乎匹配的下界$Ω\left( \frac{d}{K \sqrt{T}} \right)$（$K$为最大子集规模）。据我们所知，这是PbRL领域首个通过排序反馈理论证明样本效率随子集规模提升的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the sample inefficiency in online preference-based reinforcement learning (PbRL) by moving beyond pairwise comparisons to leverage richer ranking feedback from multiple options. The authors propose the M-AUPO algorithm, which selects subsets of actions by maximizing average uncertainty within the offered set and uses the Plackett-Luce model to handle ranking feedback. Theoretical analysis shows that M-AUPO achieves a suboptimality gap that improves with larger subset sizes, avoiding exponential dependence on unknown parameters, and a near-matching lower bound confirms the benefit of multiple comparisons for enhanced sample efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对在线偏好强化学习中的样本效率问题，提出超越两两比较、利用多选项排序反馈的方法。研究者设计了M-AUPO算法，通过最大化所选动作子集内的平均不确定性来选取动作，并采用Plackett-Luce模型处理排序反馈。理论结果表明，该算法的次优性间隙随子集规模增大而改善，避免了以往工作中对未知参数范数的指数依赖，且近乎匹配的下界证实了多选项比较能提升样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy</div>
<div class="meta-line">Authors: Hongze Tan, Zihan Wang, Jianfei Pan, Jinghao Lin, Hao Wang, Yifan Wu, Tao Chen, Zhihang Zheng, Zhihao Tang, Haihua Yang</div>
<div class="meta-line">First: 2025-08-06T11:42:47+00:00 · Latest: 2026-02-05T08:04:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.04349v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.04349v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) is pivotal for enhancing Large Language Model (LLM) reasoning, yet mainstream algorithms such as GRPO and DAPO remain constrained by a coarse-grained credit assignment paradigm, where all tokens within the same response receive the identical reward. In this paper, we propose Dynamic Entropy Weighting, systematically define entropy-based weight ratios $\frac{H_{i,t}}{\sum_{k=1}^{n} H_{k,t}}$ and similar variants to redistribute rewards and get fine-grained rewards through two new algorithms: Group Token Policy Optimization (GTPO), which assigns an entropy-weighted reward to each token and synthesizes token-specific advantage function to drive the model toward optimal path, and the analogous algorithm Sequence-Level GRPO (GRPO-S), which extends this design to the sequence level and exhibits superior stability in long Chain-of-Thought (CoT) reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GTPO与GRPO-S：基于策略熵的令牌级与序列级奖励塑形</div>
<div class="mono" style="margin-top:8px">强化学习（RL）对于提升大语言模型（LLM）的推理能力至关重要，但主流算法如GRPO和DAPO仍受限于粗粒度的信用分配范式，即同一响应中的所有令牌均获得相同奖励。本文提出动态熵加权方法，系统性地定义基于熵的权重比率$\frac{H_{i,t}}{\sum_{k=1}^{n} H_{k,t}}$及其变体，通过两种新算法重新分配奖励以实现细粒度奖励：令牌组策略优化（GTPO）为每个令牌分配熵加权奖励，并综合令牌特定优势函数以驱动模型朝向最优路径；以及类似算法序列级GRPO（GRPO-S），将此设计扩展至序列层面，在长链思维（CoT）推理任务中展现出更优的稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of coarse-grained credit assignment in existing reinforcement learning algorithms for large language models, which assign identical rewards to all tokens in a response, this paper introduces Dynamic Entropy Weighting to redistribute rewards more finely. The method systematically defines entropy-based weight ratios to develop two new algorithms: Group Token Policy Optimization (GTPO), which assigns entropy-weighted rewards per token and uses token-specific advantages to guide the model, and Sequence-Level GRPO (GRPO-S), which extends this to sequence level for enhanced stability. Experimental results demonstrate that these approaches, particularly GRPO-S, achieve superior performance in long Chain-of-Thought reasoning tasks by enabling more precise reward shaping.</div>
<div class="mono" style="margin-top:8px">针对现有强化学习算法在大语言模型中粗粒度信用分配的局限性——即对同一响应中的所有令牌赋予相同奖励，本文引入了动态熵加权以更精细地重新分配奖励。该方法系统定义了基于熵的权重比例，提出了两种新算法：组令牌策略优化（GTPO），为每个令牌分配熵加权奖励并利用令牌特定优势函数引导模型；以及序列级GRPO（GRPO-S），将此设计扩展到序列层面以提升稳定性。实验结果表明，这些方法，尤其是GRPO-S，在长链思维推理任务中通过实现更精确的奖励塑造，取得了优越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Generative AI for Intent-Driven Network Management in 6G RAN: A Case Study on the Mamba Model</div>
<div class="meta-line">Authors: Md Arafat Habib, Medhat Elsayed, Yigit Ozcan, Pedro Enrique Iturria-Rivera, Majid Bavand, Melike Erol-Kantarci</div>
<div class="meta-line">First: 2025-08-08T18:06:52+00:00 · Latest: 2026-02-05T07:07:22+00:00</div>
<div class="meta-line">Comments: Paper submitted to IEEE for possible publication. The contents of this paper may change at any time</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06616v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.06616v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the emergence of 6G, mobile networks are becoming increasingly heterogeneous and dynamic, necessitating advanced automation for efficient management. Intent-Driven Networks (IDNs) address this by translating high-level intents into optimization policies. Large Language Models (LLMs) can enhance this process by understanding complex human instructions, enabling adaptive and intelligent automation. Given the rapid advancements in Generative AI (GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated Radio Access Network (RAN) environments is both timely and critical. This article provides such a survey, along with a case study on a selective State-Space Model (SSM)-enabled IDN architecture that integrates GenAI across three key stages: intent processing, intent validation, and intent execution. For the first time in the literature, we propose a hierarchical framework built on Mamba-SSM that introduces GenAI across all stages of the IDN pipeline. We further present a case study demonstrating that the proposed Mamba architecture significantly improves network performance through intelligent automation, surpassing existing IDN approaches. In a multi-cell 5G/6G scenario, the proposed architecture reduces quality of service drift by up to 70%, improves throughput by up to 80 Mbps, and lowers inference time to 60-70 ms, outperforming GenAI, reinforcement learning, and non-machine learning baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向6G无线接入网意图驱动网络管理的生成式人工智能：基于Mamba模型的案例研究</div>
<div class="mono" style="margin-top:8px">随着6G时代的到来，移动网络日益呈现异构化和动态化特征，亟需先进的自动化管理技术。意图驱动网络通过将高层业务意图转化为优化策略来应对这一挑战。大语言模型能够理解复杂的人类指令，从而增强这一过程的适应性与智能化水平。鉴于生成式人工智能的快速发展，对基于大语言模型的解耦式无线接入网环境中意图驱动网络架构进行全面综述具有重要时效性与关键意义。本文不仅提供了此类综述，还通过案例研究展示了一种基于选择性状态空间模型的意图驱动网络架构，该架构将生成式人工智能整合至意图处理、意图验证与意图执行三大关键阶段。我们首次在学界提出基于Mamba状态空间模型的分层框架，实现生成式人工智能在意图驱动网络全流程的贯通。案例研究表明，所提出的Mamba架构通过智能自动化显著提升网络性能：在多小区5G/6G场景中，服务质量漂移降低达70%，吞吐量提升最高80Mbps，推理时间缩短至60-70毫秒，其性能全面超越生成式人工智能、强化学习及非机器学习基线方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing heterogeneity and dynamism of 6G mobile networks, which demand advanced automation for efficient management, this paper surveys and proposes a Generative AI-enhanced Intent-Driven Network (IDN) architecture. The method introduces a novel hierarchical framework built on the Mamba State-Space Model (SSM) to integrate GenAI across all three key IDN stages: intent processing, validation, and execution. The main experimental results from a multi-cell 5G/6G case study demonstrate that this Mamba-based architecture significantly outperforms existing baselines, reducing quality of service drift by up to 70%, improving throughput by up to 80 Mbps, and lowering inference time to 60-70 ms.</div>
<div class="mono" style="margin-top:8px">本文的动机源于6G移动网络日益增长的异构性和动态性，这要求先进的自动化技术以实现高效管理。方法上，论文提出了一种基于Mamba状态空间模型（SSM）的分层框架，将生成式人工智能（GenAI）集成到意图驱动网络（IDN）的意图处理、验证和执行三个关键阶段。在一个多小区5G/6G场景的案例研究中，主要实验结果表明，所提出的Mamba架构显著优于现有基线方法，将服务质量漂移降低了高达70%，吞吐量提升了高达80 Mbps，并将推理时间缩短至60-70毫秒。</div>
</details>
</div>
<div class="card">
<div class="title">Variance Reduction Based Experience Replay for Policy Optimization</div>
<div class="meta-line">Authors: Hua Zheng, Wei Xie, M. Ben Feng, Keilung Choy</div>
<div class="meta-line">First: 2026-02-05T06:58:28+00:00 · Latest: 2026-02-05T06:58:28+00:00</div>
<div class="meta-line">Comments: 24 pages, 4 figures. arXiv admin note: text overlap with arXiv:2208.12341</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05379v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05379v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective reinforcement learning (RL) for complex stochastic systems requires leveraging historical data collected in previous iterations to accelerate policy optimization. Classical experience replay treats all past observations uniformly and fails to account for their varying contributions to learning. To overcome this limitation, we propose Variance Reduction Experience Replay (VRER), a principled framework that selectively reuses informative samples to reduce variance in policy gradient estimation. VRER is algorithm-agnostic and integrates seamlessly with existing policy optimization methods, forming the basis of our sample-efficient off-policy algorithm, Policy Gradient with VRER (PG-VRER). Motivated by the lack of rigorous theoretical analysis of experience replay, we develop a novel framework that explicitly captures dependencies introduced by Markovian dynamics and behavior-policy interactions. Using this framework, we establish finite-time convergence guarantees for PG-VRER and reveal a fundamental bias-variance trade-off: reusing older experience increases bias but simultaneously reduces gradient variance. Extensive empirical experiments demonstrate that VRER consistently accelerates policy learning and improves performance over state-of-the-art policy optimization algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于方差缩减的经验回放策略优化方法</div>
<div class="mono" style="margin-top:8px">针对复杂随机系统的有效强化学习需要利用历史数据加速策略优化。传统经验回放均等处理所有过往观测，未考虑其对学习贡献的差异。为克服此局限，本文提出方差缩减经验回放（VRER）框架，通过选择性复用信息样本降低策略梯度估计方差。VRER与算法无关，可无缝集成现有策略优化方法，并构成高效离策略算法PG-VRER的理论基础。针对经验回放缺乏严谨理论分析的问题，我们构建了显式刻画马尔可夫动态与行为策略交互依赖性的新框架，据此证明PG-VRER的有限时间收敛性，并揭示偏差-方差权衡规律：复用早期经验会增加偏差但降低梯度方差。大量实验表明VRER能持续加速策略学习，在性能上超越现有先进策略优化算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to leverage historical data more effectively in reinforcement learning for complex stochastic systems, this paper introduces Variance Reduction Experience Replay (VRER), a framework that selectively reuses informative samples to reduce variance in policy gradient estimation. The method is algorithm-agnostic and integrates with existing policy optimization techniques, leading to the development of PG-VRER, a sample-efficient off-policy algorithm. Through a novel theoretical framework that accounts for dependencies from Markovian dynamics and behavior-policy interactions, the authors establish finite-time convergence guarantees for PG-VRER and uncover a bias-variance trade-off where older experience increases bias but reduces variance. Experimental results demonstrate that VRER consistently accelerates policy learning and outperforms state-of-the-art policy optimization algorithms.</div>
<div class="mono" style="margin-top:8px">本文针对复杂随机系统中强化学习需要更有效利用历史数据的问题，提出了方差缩减经验回放（VRER）框架，通过选择性重用信息样本以降低策略梯度估计的方差。该方法与算法无关，可无缝集成到现有策略优化技术中，并由此开发了样本高效的离策略算法PG-VRER。通过一个考虑马尔可夫动态和行为策略交互依赖性的新理论框架，作者为PG-VRER建立了有限时间收敛保证，并揭示了偏差-方差权衡：重用旧经验会增加偏差但降低方差。实验结果表明，VRER能持续加速策略学习，并在性能上优于最先进的策略优化算法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Plan &amp; Schedule with Reinforcement-Learned Bimanual Robot Skills</div>
<div class="meta-line">Authors: Weikang Wan, Fabio Ramos, Xuning Yang, Caelan Garrett</div>
<div class="meta-line">First: 2025-10-29T15:39:53+00:00 · Latest: 2026-02-05T06:48:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25634v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25634v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon contact-rich bimanual manipulation presents a significant challenge, requiring complex coordination involving a mixture of parallel execution and sequential collaboration between arms. In this paper, we introduce a hierarchical framework that frames this challenge as an integrated skill planning &amp; scheduling problem, going beyond purely sequential decision-making to support simultaneous skill invocation. Our approach is built upon a library of single-arm and bimanual primitive skills, each trained using Reinforcement Learning (RL) in GPU-accelerated simulation. We then train a Transformer-based planner on a dataset of skill compositions to act as a high-level scheduler, simultaneously predicting the discrete schedule of skills as well as their continuous parameters. We demonstrate that our method achieves higher success rates on complex, contact-rich tasks than end-to-end RL approaches and produces more efficient, coordinated behaviors than traditional sequential-only planners.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习双手机器人技能的学习规划与调度方法</div>
<div class="mono" style="margin-top:8px">长时程密集接触的双手机器人操作面临重大挑战，需要双臂在并行执行与顺序协作间实现复杂协调。本文提出一种分层框架，将该挑战构建为集成技能规划与调度问题，突破纯顺序决策限制，支持技能同时调用。该方法基于单臂与双臂基础技能库构建，各技能均通过GPU加速仿真环境下的强化学习训练获得。随后，我们在技能组合数据集上训练基于Transformer的规划器作为高层调度器，同步预测离散技能调度序列及其连续参数。实验表明，本方法在复杂密集接触任务中比端到端强化学习方法获得更高成功率，且比传统纯顺序规划器产生更高效协调的行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of long-horizon contact-rich bimanual manipulation, which demands complex coordination between robot arms, by proposing a hierarchical framework that integrates skill planning and scheduling to allow simultaneous skill execution rather than purely sequential actions. The method involves creating a library of reinforcement-learned primitive skills for single and bimanual arms in simulation, then training a Transformer-based planner on skill compositions to predict both discrete skill schedules and continuous parameters. Experimental results show this approach outperforms end-to-end reinforcement learning in success rates and yields more efficient, coordinated behaviors compared to traditional sequential planners.</div>
<div class="mono" style="margin-top:8px">本文针对长时程、接触密集的双臂操作挑战，该任务需要双臂进行复杂的协调，提出了一种分层框架，将问题整合为技能规划与调度，支持技能同时执行而非纯顺序决策。方法基于在GPU加速仿真中通过强化学习训练的单臂和双臂原始技能库，并利用Transformer规划器在技能组合数据集上进行训练，以同时预测离散的技能调度和连续参数。实验结果表明，该方法在复杂接触任务上比端到端强化学习具有更高的成功率，并比传统顺序规划器产生更高效、协调的行为。</div>
</details>
</div>
<div class="card">
<div class="title">GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL</div>
<div class="meta-line">Authors: Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang</div>
<div class="meta-line">First: 2026-02-05T05:44:48+00:00 · Latest: 2026-02-05T05:44:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05323v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05323v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to &quot;stitch&quot; optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAS：增强生成模型辅助离线安全强化学习的奖励-成本平衡</div>
<div class="mono" style="margin-top:8px">离线安全强化学习（OSRL）旨在仅使用预收集数据集学习策略，在满足约束条件的同时实现序列决策的高性能。受生成模型（GMs）强大能力的启发，近期研究将OSRL中的决策重新表述为条件生成过程，即GMs根据预定义的奖励和成本值生成理想动作。然而，GM辅助方法在OSRL中面临两大挑战：（1）缺乏从数据集内次优轨迹中“拼接”最优状态转移的能力；（2）难以平衡奖励目标与成本目标，尤其在二者冲突时。为解决这些问题，我们提出目标辅助拼接（GAS）算法，该算法旨在增强拼接能力的同时有效平衡奖励最大化与约束满足。为提升拼接能力，GAS首先在状态转移层面对数据集进行增强和重标注，从而能够从次优轨迹构建高质量轨迹。GAS还引入新颖的目标函数，用于从数据集中估计可达到的最优奖励与成本目标。这些目标函数通过对重标注增强数据集进行期望回归训练，使GAS能够适应更广泛的奖励-成本回报组合，相比人工设定值能实现奖励最大化与约束满足间更优的权衡。估计的目标随后指导策略训练，确保约束条件下的稳健性能。此外，为提升训练稳定性与效率，我们通过重塑数据集实现更均匀的奖励-成本回报分布。实证结果验证了GAS的有效性，其在平衡奖励最大化与约束满足方面展现出优于现有方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses two key challenges in offline safe reinforcement learning (OSRL) with generative models: the inability to stitch optimal transitions from suboptimal trajectories and the difficulty in balancing conflicting reward and cost objectives. The proposed Goal-Assisted Stitching (GAS) algorithm enhances stitching by augmenting and relabeling the dataset at the transition level to construct high-quality trajectories, and it introduces novel goal functions trained via expectile regression to estimate optimal achievable reward and cost goals, enabling a better trade-off than human-specified values. Experimental results demonstrate that GAS achieves superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对生成模型辅助的离线安全强化学习中的两个关键挑战：无法从次优轨迹中拼接最优转移，以及难以平衡相互冲突的奖励与成本目标。提出的目标辅助拼接算法通过增强和重标注数据集以构建高质量轨迹来提升拼接能力，并引入通过期望回归训练的新目标函数来估计最优可达到的奖励与成本目标，从而比人工指定值实现更好的权衡。实验结果表明，该算法在平衡奖励最大化与约束满足方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">A Differential and Pointwise Control Approach to Reinforcement Learning</div>
<div class="meta-line">Authors: Minh Nguyen, Chandrajit Bajaj</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-04-24T03:11:12+00:00 · Latest: 2026-02-05T05:28:08+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.15617v4">Abs</a> · <a href="https://arxiv.org/pdf/2404.15617v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) in continuous state-action spaces remains challenging in scientific computing due to poor sample efficiency and lack of pathwise physical consistency. We introduce Differential Reinforcement Learning (Differential RL), a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation. This induces a Hamiltonian structure that embeds physics priors and ensures consistent trajectories without requiring explicit constraints. To implement Differential RL, we develop Differential Policy Optimization (dfPO), a pointwise, stage-wise algorithm that refines local movement operators along the trajectory for improved sample efficiency and dynamic alignment. We establish pointwise convergence guarantees, a property not available in standard RL, and derive a competitive theoretical regret bound of $\mathcal{O}(K^{5/6})$. Empirically, dfPO outperforms standard RL baselines on representative scientific computing tasks, including surface modeling, grid control, and molecular dynamics, under low-data and physics-constrained conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习的微分与逐点控制方法</div>
<div class="mono" style="margin-top:8px">连续状态-动作空间中的强化学习（RL）在科学计算中仍面临挑战，主要源于样本效率低下和缺乏路径物理一致性。我们提出微分强化学习（Differential RL），这是一种通过微分对偶公式从连续时间控制角度重构RL的新框架。该框架引入哈密顿结构，嵌入物理先验并确保轨迹一致性，无需显式约束。为实现微分强化学习，我们开发了微分策略优化（dfPO），这是一种沿轨迹精化局部移动算子的逐点、分阶段算法，以提升样本效率和动态对齐能力。我们建立了逐点收敛保证（标准RL不具备的特性），并推导出$\mathcal{O}(K^{5/6})$的竞争性理论遗憾界。实验表明，在低数据和物理约束条件下，dfPO在表面建模、网格控制和分子动力学等代表性科学计算任务上均优于标准RL基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of poor sample efficiency and lack of physical consistency in continuous control reinforcement learning for scientific computing, this paper introduces Differential Reinforcement Learning, a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation to embed physics priors. The method is implemented through Differential Policy Optimization (dfPO), a pointwise, stage-wise algorithm that refines local movement operators along trajectories. Experimental results demonstrate that dfPO outperforms standard RL baselines on scientific tasks like surface modeling and molecular dynamics under low-data conditions, while also achieving a theoretical regret bound of O(K^{5/6}) and pointwise convergence guarantees.</div>
<div class="mono" style="margin-top:8px">针对科学计算中连续控制强化学习样本效率低和缺乏物理一致性的问题，本文提出了微分强化学习这一新框架，通过微分对偶公式从连续时间控制角度重构强化学习，以嵌入物理先验。该方法通过微分策略优化算法实现，这是一种逐点、分阶段的算法，可沿轨迹优化局部运动算子。实验结果表明，在表面建模、分子动力学等科学任务中，该算法在低数据条件下优于标准强化学习基线，同时实现了O(K^{5/6})的理论遗憾界和逐点收敛保证。</div>
</details>
</div>
<div class="card">
<div class="title">Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates</div>
<div class="meta-line">Authors: Chengxiao Wang, Haoze Wu, Gagandeep Singh</div>
<div class="meta-line">First: 2026-02-05T05:08:01+00:00 · Latest: 2026-02-05T05:08:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05311v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05311v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可证明鲁棒的神经李雅普诺夫-屏障证书的形式化综合</div>
<div class="mono" style="margin-top:8px">神经李雅普诺夫与屏障证书近年来已成为验证深度强化学习控制器安全性与稳定性属性的有力工具。然而，现有方法仅在固定理想无扰动力学下提供保证，限制了其在现实应用中因不确定性导致动力学偏离时的可靠性。本文研究在系统动力学受扰动下仍能保持保证的鲁棒神经李雅普诺夫屏障证书的综合问题。我们形式化定义了鲁棒李雅普诺夫屏障函数，并基于Lipschitz连续性提出了确保对有限扰动鲁棒性的充分条件。通过对抗训练、Lipschitz邻域约束和全局Lipschitz正则化，我们设计了强制执行这些条件的实用训练目标。在倒立摆和二维对接这两个实际相关环境中验证了方法：前者是广泛研究的基准，后者是自主系统中的安全关键任务。实验表明，相较于基线方法，我们的方法在强扰动下将可证明鲁棒界提升最高达4.6倍，实证成功率提高最高达2.4倍。结果证明了在动力学扰动下训练鲁棒神经证书对安全强化学习的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to ensure safety and stability of deep reinforcement learning controllers under real-world dynamic uncertainties, this paper proposes a method for formally synthesizing robust neural Lyapunov-barrier certificates. The approach defines sufficient conditions based on Lipschitz continuity to guarantee robustness against bounded perturbations and introduces training objectives that enforce these conditions through adversarial training and Lipschitz regularization. Experimental validation on the Inverted Pendulum and 2D Docking tasks shows that the method significantly improves certified robustness bounds by up to 4.6 times and empirical success rates under strong perturbations by up to 2.4 times compared to baselines, demonstrating its effectiveness for safe RL in perturbed dynamics.</div>
<div class="mono" style="margin-top:8px">针对深度强化学习控制器在现实世界动态不确定性下需确保安全与稳定性的需求，本文提出了一种形式化合成鲁棒神经李雅普诺夫-屏障证书的方法。该方法基于Lipschitz连续性定义了保证对有限扰动鲁棒性的充分条件，并通过对抗性训练和Lipschitz正则化引入训练目标来强化这些条件。在倒立摆和二维对接任务上的实验验证表明，相较于基线方法，该方法将认证鲁棒性边界提升了最高4.6倍，在强扰动下的实证成功率提高了最高2.4倍，证明了其在扰动动态中实现安全强化学习的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities</div>
<div class="meta-line">Authors: Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov, Ivan Oseledets</div>
<div class="meta-line">First: 2026-02-05T04:06:55+00:00 · Latest: 2026-02-05T04:06:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05281v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>回归基础：通过生成概率重新审视强化学习在大型语言模型推理中的探索机制</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为提升大型语言模型推理能力的关键范式。然而，标准策略优化方法（如组相对策略优化）常收敛于低熵策略，导致严重的模式坍缩和输出多样性受限。我们从采样概率动态的角度分析此问题，发现标准目标函数会过度强化最高似然路径，从而抑制有效的替代推理链。为此，我们提出一种新颖的优势重加权机制，旨在平衡所有正确答案的置信度水平。通过将提示困惑度与答案置信度纳入优势估计，该方法动态重塑奖励信号以削弱过度自信推理路径的梯度更新，同时将概率质量重新分配给未被充分探索的正确解。实证结果表明，我们的方法在保持竞争力的准确率的同时，显著提升了生成多样性和响应熵，有效实现了推理任务中探索与利用的优越权衡。在Qwen2.5和DeepSeek模型上的数学与编程基准测试表明，ProGRPO显著缓解了熵坍缩现象。具体而言，在Qwen2.5-7B模型上，本方法在Pass@1指标上超越GRPO 5.7%，在Pass@32指标上更领先13.9%，凸显了其在生成多样化正确推理路径方面的卓越能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of mode collapse and limited output diversity in reinforcement learning for LLM reasoning, which arises when standard policy optimization methods like GRPO converge to low-entropy policies by disproportionately reinforcing high-likelihood paths. To mitigate this, the authors propose an Advantage Re-weighting Mechanism (ARM) that incorporates Prompt Perplexity and Answer Confidence into advantage estimation, dynamically reshaping rewards to attenuate gradients for over-confident paths and redistribute probability mass toward under-explored correct solutions. Experimental results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that the method, ProGRPO, significantly mitigates entropy collapse, outperforming GRPO by 5.7% in Pass@1 and 13.9% in Pass@32 on Qwen2.5-7B, thereby enhancing generative diversity while maintaining competitive accuracy.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在大型语言模型推理中因标准策略优化方法（如GRPO）收敛至低熵策略而导致模式崩溃和输出多样性受限的问题，提出了一种优势重加权机制（ARM）。该方法通过将提示困惑度和答案置信度纳入优势估计，动态调整奖励信号，以削弱过度自信推理路径的梯度更新，并将概率质量重新分配给未充分探索的正确解决方案。在Qwen2.5和DeepSeek模型上的数学与编码基准实验表明，所提出的ProGRPO方法显著缓解了熵崩溃，在Qwen2.5-7B上，其Pass@1和Pass@32分别比GRPO提高了5.7%和13.9%，从而在保持准确性的同时有效提升了生成多样性。</div>
</details>
</div>
<div class="card">
<div class="title">CARL: Focusing Agentic Reinforcement Learning on Critical Actions</div>
<div class="meta-line">Authors: Leyang Shen, Yang Zhang, Chun Kai Ling, Xiaoyan Zhao, Tat-Seng Chua</div>
<div class="meta-line">First: 2025-12-04T16:15:46+00:00 · Latest: 2026-02-05T03:39:41+00:00</div>
<div class="meta-line">Comments: 17 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04949v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04949v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for long-horizon agentic reasoning. CARL leverages entropy as a heuristic proxy for action criticality and achieves focused training by assigning rewards to high-criticality actions while excluding low-criticality actions from model updates, avoiding noisy credit assignment and redundant computation. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency across diverse evaluation settings. The source code will be publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CARL：聚焦关键动作的智能体强化学习</div>
<div class="mono" style="margin-top:8px">能够通过与环境的多次交互完成复杂任务的智能体已成为热门研究方向。然而，在这种多步交互场景中，传统群体级策略优化算法因其默认假设每个动作贡献均等而偏离现实，导致效果欠佳。我们的分析表明，仅少数关键动作对最终结果起决定性作用。基于此发现，我们提出CARL——一种专为长周期智能体推理设计的、聚焦关键动作的强化学习算法。CARL以熵作为动作关键性的启发式代理指标，通过为高关键性动作分配奖励，并将低关键性动作排除在模型更新之外，实现聚焦式训练，从而避免噪声信用分配与冗余计算。大量实验表明，CARL在多种评估场景中均实现了更强的性能与更高的效率。源代码将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of standard reinforcement learning in multi-step agentic tasks, where treating all actions as equally important leads to suboptimal policy optimization. To overcome this, the authors propose CARL, an algorithm that identifies critical actions using entropy as a proxy, focusing training updates on these high-impact actions while ignoring less critical ones to improve credit assignment and computational efficiency. Experimental results across various settings show that CARL achieves superior performance and higher efficiency compared to conventional methods.</div>
<div class="mono" style="margin-top:8px">本文针对多步智能体任务中标准强化学习效率低下的问题，指出传统方法假设所有动作贡献均等会导致策略优化次优。为此，作者提出CARL算法，利用熵作为动作关键性的启发式代理，将训练更新集中在关键动作上，同时忽略非关键动作，以改进信用分配和计算效率。在不同评估设置下的广泛实验表明，CARL相比传统方法实现了更强的性能和更高的效率。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretability by Design for Efficient Multi-Objective Reinforcement Learning</div>
<div class="meta-line">Authors: Qiyue Xia, Tianwei Wang, J. Michael Herrmann</div>
<div class="meta-line">First: 2025-06-04T14:52:18+00:00 · Latest: 2026-02-05T02:52:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04022v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.04022v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective reinforcement learning (MORL) aims at optimising several, often conflicting goals to improve the flexibility and reliability of RL in practical tasks. This is typically achieved by finding a set of diverse, non-dominated policies that form a Pareto front in the performance space. We introduce LLE-MORL, an approach that achieves interpretability by design by utilising a training scheme based on the local relationship between the parameter space and the performance space. By exploiting a locally linear map between these spaces, our method provides an interpretation of policy parameters in terms of the objectives, and this structured representation enables an efficient search within contiguous solution domains, allowing for the rapid generation of high-quality solutions without extensive retraining. Experiments across diverse continuous control domains demonstrate that LLE-MORL consistently achieves higher Pareto front quality and efficiency than state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高效多目标强化学习的可解释性设计</div>
<div class="mono" style="margin-top:8px">多目标强化学习（MORL）旨在优化多个常相互冲突的目标，以提升强化学习在实际任务中的灵活性与可靠性。该方法通常通过寻找一组构成性能空间帕累托前沿的多样化非支配策略来实现。本文提出LLE-MORL方法，通过利用参数空间与性能空间局部关联的训练机制，实现可解释性的内置设计。该方法通过构建空间间的局部线性映射，以目标函数视角解释策略参数，这种结构化表征支持在连续解空间中进行高效搜索，无需大量重复训练即可快速生成高质量解。跨多个连续控制领域的实验表明，LLE-MORL在帕累托前沿质量与求解效率方面均持续优于现有先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the need for interpretable and efficient multi-objective reinforcement learning (MORL), where optimizing multiple conflicting goals is crucial for practical applications. The proposed method, LLE-MORL, achieves interpretability by design through a training scheme that leverages a locally linear map between the policy parameter space and the performance space, enabling clear interpretation of parameters in terms of objectives and facilitating efficient search within contiguous solution domains. Experimental results across various continuous control tasks show that LLE-MORL consistently outperforms state-of-the-art methods in both Pareto front quality and computational efficiency, allowing rapid generation of high-quality solutions without extensive retraining.</div>
<div class="mono" style="margin-top:8px">本文针对多目标强化学习（MORL）中可解释性和效率的需求展开研究，旨在优化多个常冲突的目标以提升实际任务的灵活性和可靠性。所提出的LLE-MORL方法通过利用策略参数空间与性能空间之间的局部线性映射，实现了设计上的可解释性，从而能够依据目标解释参数，并在连续解域中进行高效搜索。在多种连续控制领域的实验结果表明，LLE-MORL在帕累托前沿质量和效率方面均一致优于现有先进方法，无需大量重新训练即可快速生成高质量解。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints</div>
<div class="meta-line">Authors: Evan Chen, Wenzhi Fang, Shiqiang Wang, Christopher Brinton</div>
<div class="meta-line">First: 2026-01-29T23:27:15+00:00 · Latest: 2026-02-05T02:29:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00166v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00166v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预算约束下本地语言模型与云端卸载决策的联合持续学习</div>
<div class="mono" style="margin-top:8px">本地部署的小型语言模型必须在严格的内存与计算约束下持续支持多样化任务，这使得选择性依赖云端大型语言模型成为必然。在持续学习过程中调控云端辅助具有挑战性：基于朴素奖励的强化学习常导致不稳定的卸载行为，且会因任务分布变化加剧灾难性遗忘。我们提出DA-GRPO——一种融合双重优势的组相对策略优化扩展方法，将云端使用约束直接纳入优势计算，避免固定奖励塑造和外部路由模型。该设计使本地模型能联合学习任务能力与协作行为，让云端请求在遵循预设辅助预算的前提下自然产生于训练后阶段。在数学推理与代码生成基准测试中，DA-GRPO相比现有协作与路由方法提升了切换后准确率，显著减少遗忘，并保持稳定的云端使用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling Small Language Models (SLMs) to perform continual learning under strict local resource constraints by intelligently offloading some queries to a cloud-based Large Language Model (LLM). The proposed method, DA-GRPO, extends Group Relative Policy Optimization to incorporate cloud-usage budget constraints directly into its advantage computation, allowing the local model to jointly learn task competence and when to request cloud assistance without relying on fixed reward shaping or external routers. Experimental results on mathematical reasoning and code generation tasks demonstrate that this approach improves accuracy after task switches, significantly reduces catastrophic forgetting, and maintains stable cloud usage within the specified budget compared to prior methods.</div>
<div class="mono" style="margin-top:8px">本文旨在解决小型语言模型在严格本地资源限制下进行持续学习时，如何智能地将部分查询卸载到云端大型语言模型的挑战。所提出的方法DA-GRPO扩展了组相对策略优化，将云端使用预算约束直接纳入优势计算，使本地模型能够联合学习任务能力和协作行为，无需依赖固定的奖励塑造或外部路由模型。在数学推理和代码生成基准上的实验结果表明，该方法提高了任务切换后的准确性，显著减少了灾难性遗忘，并且与先前的协作和基于路由的方法相比，能在指定预算内保持稳定的云端使用。</div>
</details>
</div>
<div class="card">
<div class="title">HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation</div>
<div class="meta-line">Authors: Puyue Wang, Jiawei Hu, Yan Gao, Junyan Wang, Yu Zhang, Gillian Dobbie, Tao Gu, Wafa Johal, Ting Dang, Hong Jia</div>
<div class="meta-line">First: 2026-02-04T10:41:23+00:00 · Latest: 2026-02-05T02:24:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04412v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04412v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tonywang-0517.github.io/hord/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher&#x27;s robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at https://tonywang-0517.github.io/hord/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HoRD：基于历史条件强化学习与在线蒸馏的鲁棒人形机器人控制</div>
<div class="mono" style="margin-top:8px">人形机器人在动力学、任务规范或环境设置的微小变化下可能出现性能显著下降。本文提出HoRD，一种面向领域偏移下鲁棒人形控制的两阶段学习框架。首先，通过历史条件强化学习训练高性能教师策略，该策略从近期状态-动作轨迹推断潜在动力学上下文，以在线适应多样随机化动力学。其次，执行在线蒸馏，将教师的鲁棒控制能力迁移至基于Transformer的学生策略，该策略以稀疏的根关节相对三维关键点轨迹为输入。通过结合历史条件适应与在线蒸馏，HoRD使单一策略能够零样本适应未见领域，无需逐领域重新训练。大量实验表明，HoRD在鲁棒性与迁移性上优于强基线方法，尤其在未见领域和外部扰动下表现突出。代码与项目页面详见 https://tonywang-0517.github.io/hord/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of humanoid robots to performance degradation under minor domain shifts, such as changes in dynamics or task specifications. To enhance robustness, the authors propose HoRD, a two-stage framework: first, a teacher policy is trained using history-conditioned reinforcement learning to infer latent dynamics context from recent state-action trajectories, enabling online adaptation to randomized dynamics; second, online distillation transfers this capability into a transformer-based student policy that processes sparse root-relative 3D joint keypoint trajectories. Experimental results demonstrate that HoRD outperforms baselines in robustness and zero-shot transfer to unseen domains, effectively handling external perturbations without per-domain retraining.</div>
<div class="mono" style="margin-top:8px">该论文针对人形机器人在动态或任务规范等微小领域变化下性能下降的问题，提出了一种增强鲁棒性的方法。作者设计了HoRD两阶段框架：首先，通过历史条件强化学习训练教师策略，使其从近期状态-动作轨迹推断潜在动态上下文，从而在线适应随机化动态；其次，通过在线蒸馏将这种能力迁移到基于Transformer的学生策略中，该策略处理稀疏的根相对3D关节关键点轨迹。实验结果表明，HoRD在鲁棒性和零样本迁移到未见领域方面优于基线方法，能有效应对外部扰动，无需针对每个领域重新训练。</div>
</details>
</div>
<div class="card">
<div class="title">Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture</div>
<div class="meta-line">Authors: Wenyun Li, Wenjie Huang, Zejian Deng, Chen Sun</div>
<div class="meta-line">First: 2025-06-14T12:18:19+00:00 · Latest: 2026-02-05T02:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12474v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.12474v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate driving behavior modeling is fundamental to safe and efficient trajectory prediction, yet remains challenging in complex traffic scenarios. This paper presents a novel Inverse Reinforcement Learning (IRL) framework that captures human-like decision-making by inferring diverse reward functions, enabling robust cross-scenario adaptability. The learned reward function is utilized to maximize the likelihood of output by integrating Mamba blocks for efficient long-sequence dependency modeling with graph attention networks to encode spatial interactions among traffic agents. Comprehensive evaluations on urban intersections and roundabouts demonstrate that the proposed method not only outperforms various popular approaches in terms of prediction accuracy but also achieves 2.3 times higher generalization performance to unseen scenarios compared to other baselines, achieving adaptability in Out-of-Distribution settings that is competitive with fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Mamba-图架构的逆强化学习通用轨迹预测方法</div>
<div class="mono" style="margin-top:8px">精确的驾驶行为建模是实现安全高效轨迹预测的基础，但在复杂交通场景中仍具挑战。本文提出一种新颖的逆强化学习框架，通过推断多样化奖励函数来捕捉类人决策机制，实现跨场景的鲁棒适应性。该框架将Mamba模块用于高效长序列依赖建模，并结合图注意力网络编码交通参与者间的空间交互，利用习得的奖励函数最大化输出似然。在城市交叉口和环岛场景的综合评估表明，该方法不仅在预测精度上优于多种主流方法，且对未见场景的泛化性能达到其他基线的2.3倍，在分布外场景中取得的适应性可与微调方法相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of accurate driving behavior modeling in complex traffic scenarios by proposing a novel Inverse Reinforcement Learning framework that infers diverse reward functions to capture human-like decision-making for robust cross-scenario adaptability. The method integrates Mamba blocks for efficient long-sequence dependency modeling with graph attention networks to encode spatial interactions among traffic agents, utilizing the learned reward function to maximize output likelihood. Experimental evaluations on urban intersections and roundabouts show that the approach outperforms various popular methods in prediction accuracy and achieves 2.3 times higher generalization performance to unseen scenarios compared to baselines, demonstrating strong adaptability in Out-of-Distribution settings competitive with fine-tuning.</div>
<div class="mono" style="margin-top:8px">本文针对复杂交通场景中驾驶行为建模的挑战，提出了一种新颖的逆强化学习框架，通过推断多样化的奖励函数来捕捉类人决策，以实现稳健的跨场景适应性。该方法将用于高效长序列依赖建模的Mamba模块与编码交通参与者空间交互的图注意力网络相结合，利用学习到的奖励函数最大化输出可能性。在城市交叉口和环岛的全面评估表明，所提方法不仅在预测准确性上优于多种流行方法，而且在未见场景中的泛化性能比基线高出2.3倍，在分布外设置中展现出与微调相竞争的适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">Your Latent Reasoning is Secretly Policy Improvement Operator</div>
<div class="meta-line">Authors: Arip Asadulaev, Rayan Banerjee, Fakhri Karray, Martin Takac</div>
<div class="meta-line">First: 2025-11-21T01:54:23+00:00 · Latest: 2026-02-05T01:45:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16886v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.16886v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, small models with latent recursion have obtained promising results on complex reasoning tasks. These results are typically explained by the theory that such recursion increases a networks depth, allowing it to compactly emulate the capacity of larger models. However, the performance of recursively added layers remains behind the capabilities of one pass models with the same feed forward depth. This means that in the looped version, not every recursive step effectively contributes to depth. This raises the question: when and why does latent reasoning improve performance, and when does it result in dead compute? In our work, we analyze the algorithms that latent reasoning provides answer to this question. We show that latent reasoning can be formalized as a classifier free guidance and policy improvement algorithm. Building on these insights, we propose to use a training schemes from reinforcement learning and diffusion methods for latent reasoning models. Using the Tiny Recursive Model as our testbed, we show that with our modifications we can avoid dead compute steps and reduce the total number of forward passes by 18x while maintaining performance. Broadly speaking, we show how a policy improvement perspective on recursive steps can explain model behavior and provide insights for further improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在推理实为策略改进算子</div>
<div class="mono" style="margin-top:8px">近期，采用潜在递归的小型模型在复杂推理任务中取得了显著成果。通常认为，这种递归通过增加网络深度，使模型能紧凑模拟更大规模模型的能力。然而，递归叠加层的性能仍落后于同等前馈深度的单次推理模型，表明并非每个递归步骤都能有效贡献深度。这引出一个核心问题：潜在推理何时及为何能提升性能，何时会导致无效计算？本研究通过算法分析解答该问题，证明潜在推理可形式化为无分类器引导与策略改进算法。基于此洞见，我们提出将强化学习与扩散方法的训练机制应用于潜在推理模型。以微型递归模型为测试平台，改进后的模型在保持性能的同时，避免了无效计算步骤，并将前向传播总次数减少18倍。总体而言，本研究通过策略改进视角阐释递归步骤的行为机制，并为模型优化提供了新思路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates why latent reasoning in small recursive models sometimes fails to improve performance, a phenomenon termed &#x27;dead compute&#x27;. The authors propose that latent reasoning can be formally understood as a policy improvement algorithm, akin to techniques in reinforcement learning and classifier-free guidance from diffusion models. Motivated by this insight, they introduce new training schemes derived from these fields into a Tiny Recursive Model. Their main experimental result demonstrates that these modifications successfully eliminate ineffective recursive steps, allowing the model to reduce the total number of forward passes by 18 times while preserving its original task performance.</div>
<div class="mono" style="margin-top:8px">本文研究了小型递归模型中的潜在推理为何有时无法提升性能，即所谓的“无效计算”现象。作者提出，潜在推理可被形式化地理解为一种策略改进算法，类似于强化学习和扩散模型中的分类器无关引导技术。基于这一见解，他们将源自这些领域的新训练方案引入到一个微型递归模型中。主要的实验结果表明，这些改进成功消除了无效的递归步骤，使模型在保持原有任务性能的同时，将总的前向传播次数减少了18倍。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: John Yan, Michael Yu, Yuqi Sun, Alexander Duffy, Tyler Marques, Matthew Lyle Olson</div>
<div class="meta-line">First: 2026-02-05T01:21:22+00:00 · Latest: 2026-02-05T01:21:22+00:00</div>
<div class="meta-line">Comments: authors 1, 2 and 3 contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05183v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05183v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent&#x27;s system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的多智能体强化学习的数据中心可解释性研究</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）日益在复杂的强化学习多智能体环境中进行训练，导致行为在训练过程中的变化难以理解。稀疏自编码器（SAEs）近期被证明对数据中心可解释性具有实用价值。本研究通过应用预训练的SAEs及LLM摘要生成方法，分析了复杂环境《全面外交》中的大规模强化学习训练过程。我们提出了Meta-Autointerp方法，用于将SAE特征聚类为关于训练动态的可解释假设。我们发现了细粒度行为（包括角色扮演模式、退化输出、语言切换）以及高层战略行为和环境特定缺陷。通过自动化评估，我们验证了90%已发现的SAE元特征具有显著性，并发现了一种令人意外的奖励破解行为。然而，两项用户研究表明，即使是主观有趣且看似有用的SAE特征，也可能对人类毫无价值甚至产生负面影响，LLM生成的假设大多也存在类似问题。但部分SAE衍生的假设对下游任务具有预测价值。我们通过增强未训练智能体的系统提示进一步验证，使其得分提升+14.2%。总体而言，我们证明SAEs与LLM摘要生成器能为智能体行为提供互补视角，共同构建的框架为未来确保LLM训练全程可信行为的数据中心可解释性研究提供了实用起点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the difficulty of understanding behavioral changes in large language models (LLMs) trained in complex multi-agent reinforcement learning environments, this paper introduces Meta-Autointerp, a method that groups sparse autoencoder (SAE) features into interpretable hypotheses about training dynamics. The approach applies pretrained SAEs and LLM-summarizer methods to analyze training runs from the Full-Press Diplomacy environment, discovering fine-grained behaviors like role-playing and high-level strategic patterns. Experimental results show that 90% of discovered SAE meta-features are significant, including a reward hacking behavior, and while many features and LLM hypotheses are not helpful to humans, a subset proves predictively useful, with one validation improving an untrained agent&#x27;s score by +14.2%, demonstrating that SAEs and LLM-summarizers offer complementary views for data-centric interpretability.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型在复杂多智能体强化学习环境中训练时行为变化难以理解的问题，提出了Meta-Autointerp方法，该方法将稀疏自编码器特征分组为关于训练动态的可解释假设。该方法应用预训练的稀疏自编码器和LLM摘要器分析Full-Press Diplomacy环境的训练过程，发现了细粒度的角色扮演行为和高层战略模式等。实验结果表明，90%发现的稀疏自编码器元特征具有显著性，包括一种奖励黑客行为；尽管许多特征和LLM生成的假设对人类帮助有限，但其中一部分具有预测实用性，一项验证通过增强未训练智能体的系统提示使其得分提升了14.2%，表明稀疏自编码器和LLM摘要器为数据为中心的可解释性提供了互补视角。</div>
</details>
</div>
<div class="card">
<div class="title">EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization</div>
<div class="meta-line">Authors: Kevin Han, Yuhang Zhou, Mingze Gao, Gedi Zhou, Serena Li, Abhishek Kumar, Xiangjun Fan, Weiwei Li, Lizhu Zhang</div>
<div class="meta-line">First: 2026-02-05T00:33:02+00:00 · Latest: 2026-02-05T00:33:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05165v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy&#x27;s accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford&#x27;s online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EBPO：基于经验贝叶斯收缩的组相对策略优化稳定方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已被证明能有效提升大语言模型（LLMs）的推理能力。然而，主流方法如组相对策略优化（GRPO）面临关键的稳定性挑战：在计算受限（小组规模较小）时估计器方差较高，且在饱和失效场景（所有响应均产生相同的零奖励）中梯度信号趋于消失。为此，我们提出经验贝叶斯策略优化（EBPO），这是一种通过利用策略累积的全局统计量来正则化局部组基线的新框架。EBPO采用收缩估计器动态平衡局部组统计量与通过Welford在线算法更新的全局先验，而非孤立估计基线。理论上，我们证明EBPO相比GRPO能保证更低的均方误差（MSE）、有界的熵衰减，并在失效场景中提供非零惩罚信号。实证中，EBPO在包括AIME和OlympiadBench在内的多种基准测试中持续优于GRPO及其他基线方法。值得注意的是，EBPO展现出卓越的训练稳定性，即使在小规模组设置下仍能实现高性能提升，并能显著受益于难度分层课程学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Empirical Bayes Policy Optimization (EBPO) to address stability issues in Group Relative Policy Optimization (GRPO) for reinforcement learning with verifiable rewards, where GRPO suffers from high variance with small group sizes and vanishing gradients when all responses yield zero rewards. The method regularizes local group baselines by dynamically shrinking them toward a global prior updated online, theoretically ensuring lower mean squared error, bounded entropy decay, and non-vanishing penalty signals. Experimental results show EBPO outperforms GRPO and other baselines on benchmarks like AIME and OlympiadBench, offering greater training stability and performance gains even with small groups, and benefits from difficulty-stratified curriculum learning.</div>
<div class="mono" style="margin-top:8px">本文提出经验贝叶斯策略优化（EBPO），以解决可验证奖励强化学习中组相对策略优化（GRPO）的稳定性问题，其中GRPO在小组规模下存在高方差且在全部响应奖励为零时梯度消失。该方法通过将局部组基线动态收缩至在线更新的全局先验来进行正则化，从理论上保证了更低的均方误差、有界的熵衰减和非消失的惩罚信号。实验结果表明，EBPO在AIME和OlympiadBench等基准测试中优于GRPO及其他基线方法，即使小组规模较小也能提供更高的训练稳定性和性能提升，并受益于难度分层课程学习。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning</div>
<div class="meta-line">Authors: Ahmed Attia, Alham Fikri Aji</div>
<div class="meta-line">First: 2026-01-18T18:44:49+00:00 · Latest: 2026-02-04T23:26:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12535v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12535v2">PDF</a> · <a href="https://github.com/Copticoder/thesis-nllb-bootstrap-grpo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving. The code is available on github: https://github.com/Copticoder/thesis-nllb-bootstrap-grpo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过往返式强化学习改进低资源机器翻译</div>
<div class="mono" style="margin-top:8px">随着低资源语言社区平行数据的收集，低资源机器翻译日益受到关注，但许多改进低资源机器翻译的潜在方法仍有待探索。本研究基于No Language Left Behind（NLLB）系列模型，采用往返式自举方法，探索了在低资源环境下通过自监督强化学习进行翻译微调的技术。该方法先将英语翻译为目标低资源语言，再回译至英语，并以重构英语句子的chrF++和BLEU组合作为奖励函数。基于NLLB-MD数据集，我们对6亿参数和13亿参数的NLLB模型进行评估，在以下语言中观察到一致性的性能提升：中部艾马拉语、弗留利语、沃洛夫语和俄语。对翻译输出的定性分析表明，其流畅度与语义保真度均有所提高。我们认为该方法可进一步受益于模型规模扩展，使模型能更充分地利用预训练知识并持续自我改进。代码已发布于GitHub：https://github.com/Copticoder/thesis-nllb-bootstrap-grpo。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of low-resource machine translation by proposing a self-supervised reinforcement learning method that fine-tunes models using round-trip bootstrapping. The approach leverages the NLLB model family, translating English into a low-resource language and back to English, with a reward function based on chrF++ and BLEU scores applied to the reconstructed English text. Experimental results on the NLLB-MD dataset with 600M and 1.3B parameter models show consistent improvements in translation quality for Central Aymara, Friulian, Wolof, and Russian, with qualitative analysis indicating enhanced fluency and semantic fidelity, suggesting the method benefits from model scale and pretrained knowledge.</div>
<div class="mono" style="margin-top:8px">本文针对低资源机器翻译的挑战，提出了一种基于自监督强化学习的方法，通过往返引导对模型进行微调。该方法利用NLLB系列模型，将英语翻译成低资源语言后再回译成英语，并使用基于chrF++和BLEU分数的奖励函数评估重构的英语句子。在NLLB-MD数据集上对600M和1.3B参数模型的实验结果表明，中央艾马拉语、弗留利语、沃洛夫语和俄语的翻译质量均得到持续提升，定性分析显示流畅性和语义保真度有所增强，表明该方法能从模型规模和预训练知识中获益。</div>
</details>
</div>
<div class="card">
<div class="title">Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning</div>
<div class="meta-line">Authors: Ethan Rathbun, Wo Wei Lin, Alina Oprea, Christopher Amato</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-04T22:17:23+00:00 · Latest: 2026-02-04T22:17:23+00:00</div>
<div class="meta-line">Comments: 10 pages main body, ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05089v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05089v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger&#x27;&#x27;, leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent&#x27;s training pipeline, enabling them to both alter and observe agent&#x27;s rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze&#x27;&#x27; which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze&#x27;s effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>警惕不可信模拟器——强化学习中的无奖励后门攻击</div>
<div class="mono" style="margin-top:8px">模拟环境是强化学习（RL）成功的关键，使从业者和研究人员无需在真实硬件上进行昂贵实验即可训练决策智能体。然而，模拟器仍是一个安全盲区，恶意开发者可通过修改其发布的模拟器动态实现有害目的。为此，本文揭示了一种新型威胁，展示了如何利用模拟器动态向RL智能体隐蔽植入动作级后门。该后门使攻击者能在观察到预定义“触发器”时，可靠地激活智能体中的目标动作，可能导致危险后果。传统后门攻击受限于其强威胁模型，假设攻击者几乎完全控制智能体训练流程，既能修改也能观察其奖励。由于这些假设在模拟器中难以实现，本文提出新型攻击“Daze”，能在不修改甚至不观察奖励的情况下，可靠且隐蔽地向面向真实世界任务的RL智能体植入后门。我们通过形式化证明验证了Daze在通用RL任务中保证攻击成功的有效性，并在离散与连续动作空间领域进行了广泛实证评估。此外，我们首次展示了RL后门攻击迁移至真实机器人硬件的实例。这些进展推动了对RL训练全流程安全防护的进一步研究，以防范恶意攻击。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses a security vulnerability in reinforcement learning (RL) by introducing a novel backdoor attack that exploits untrusted simulators. The motivation stems from the fact that simulators, while essential for training RL agents affordably, are often overlooked as security risks, allowing adversaries to maliciously alter environment dynamics. The proposed method, named Daze, implants action-level backdoors into agents without requiring control over or observation of rewards, overcoming limitations of traditional attacks that assume strong adversary control. Experimental results demonstrate Daze&#x27;s effectiveness in both discrete and continuous action spaces, with formal proofs of success, and it is shown to transfer to real robotic hardware, highlighting the need for securing the entire RL training pipeline.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的安全漏洞，提出了一种利用不可信模拟器的新型后门攻击。其动机在于模拟器虽然是训练强化学习代理的关键廉价工具，但常被忽视为安全盲点，使攻击者能够恶意修改环境动态。所提出的方法名为Daze，它能在无需控制或观察奖励的情况下，将动作级后门植入代理中，克服了传统攻击需假设攻击者具有强控制力的限制。实验结果表明，Daze在离散和连续动作空间中均有效，并有形式化证明支持，且首次展示了此类攻击可迁移到真实机器人硬件上，强调了保护整个强化学习训练流程的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling</div>
<div class="meta-line">Authors: Parsa Vares</div>
<div class="meta-line">First: 2026-02-04T22:16:50+00:00 · Latest: 2026-02-04T22:16:50+00:00</div>
<div class="meta-line">Comments: Master&#x27;s Thesis, University of Luxembourg in collaboration with Luxembourg Institute of Science and Technology (LIST). Supervised by Prof. Jun Pang and Dr. Eloi Durant</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05087v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05087v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent&#x27;s decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoDiscover：基于图感知汤普森采样的强化学习推荐系统，用于解决主动学习中的冷启动不平衡挑战</div>
<div class="mono" style="margin-top:8px">系统文献综述是循证研究的基础，但随着科学成果的增长，人工筛选日益成为瓶颈。筛选过程面临相关研究出现率低、专家决策稀缺且成本高昂的问题。传统主动学习系统虽能提供帮助，但通常依赖固定查询策略选择未标注文档。这些静态策略无法随时间自适应，且忽略了科学文献网络的关系结构。本论文提出AutoDiscover框架，将主动学习重构为由自适应智能体驱动的在线决策问题。文献被建模为包含文档、作者和元数据关系的异质图。异质图注意力网络学习节点表征，折扣汤普森采样智能体利用该表征动态管理查询策略组合。通过实时人机协同标注，智能体在非平稳的综述动态中平衡探索与利用，其中策略效用随时间变化。在26个数据集的SYNERGY基准测试中，AutoDiscover实现了比静态主动学习基线更高的筛选效率。关键创新在于，智能体通过从极少量初始标注中引导发现，缓解了静态方法难以应对的冷启动问题。我们还开发了开源可视化分析仪表盘TS-Insight，用于解释、验证和诊断智能体决策。这些成果共同提升了在专家标注稀缺和相关研究低出现率条件下的系统文献综述筛选效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This thesis addresses the inefficiency of manual screening in systematic literature reviews, where relevant studies are rare and expert labels are scarce, by introducing AutoDiscover, a reinforcement learning framework that treats active learning as an online decision-making problem. The method models literature as a heterogeneous graph, uses a Heterogeneous Graph Attention Network to learn node representations, and employs a Discounted Thompson Sampling agent to dynamically select query strategies, adapting to non-stationary review dynamics. Experimental results on the SYNERGY benchmark show that AutoDiscover outperforms static active learning baselines in screening efficiency and effectively mitigates cold-start issues with minimal initial labels, while an accompanying visual analytics tool, TS-Insight, aids in interpreting the agent&#x27;s decisions.</div>
<div class="mono" style="margin-top:8px">本论文针对系统文献综述中人工筛选效率低下、相关研究稀少且专家标注稀缺的问题，提出了AutoDiscover框架，将主动学习重构为在线决策问题，采用强化学习方法。该方法将文献建模为异质图，利用异质图注意力网络学习节点表示，并通过折扣汤普森采样代理动态选择查询策略，以适应非平稳的综述动态。在SYNERGY基准测试上的实验结果表明，AutoDiscover在筛选效率上优于静态主动学习基线，并能以最少初始标注有效缓解冷启动问题，同时配套的可视化分析工具TS-Insight有助于解释代理的决策过程。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking</div>
<div class="meta-line">Authors: Vinal Asodia, Iman Sharifi, Saber Fallah</div>
<div class="meta-line">First: 2026-02-04T21:56:27+00:00 · Latest: 2026-02-04T21:56:27+00:00</div>
<div class="meta-line">Comments: 12 pages, 7 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05079v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05079v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于向量语义表征与符号推理的以人为本自主紧急制动强化学习增强方法</div>
<div class="mono" style="margin-top:8px">现有基于摄像头的深度强化学习方法存在双重问题：鲜少将高层场景上下文整合至特征表征，且依赖僵化的固定奖励函数。为解决这些挑战，本文提出一种新型流程，生成融合语义、空间与形状信息的神经符号特征表征，同时提取场景中动态实体的空间增强特征，重点关注安全关键道路使用者。此外提出软一阶逻辑奖励函数，通过符号推理模块平衡人类价值取向。该方法从分割图中提取语义与空间谓词，并应用于语言规则以获得奖励权重。在CARLA仿真环境中的定量实验表明，相较于基线表征与奖励方案，所提出的神经符号表征与软一阶逻辑奖励函数在不同交通密度与遮挡水平下均提升了策略鲁棒性及安全相关性能指标。研究证明，将整体性表征与软推理融入强化学习可支持自动驾驶系统实现更具情境感知能力且符合价值取向的决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing camera-based deep reinforcement learning methods, which often lack high-level scene context and rely on rigid reward functions, this paper introduces a novel pipeline combining neuro-symbolic feature representation and a Soft First-Order Logic reward function. The method integrates semantic, spatial, and shape information with spatially boosted features of dynamic entities, while using symbolic reasoning to balance human values through linguistic rules derived from segmentation maps. Experimental results in the CARLA simulator demonstrate that this approach enhances policy robustness and safety metrics across diverse traffic densities and occlusion levels, outperforming baseline methods and supporting more context-aware, value-aligned autonomous driving decisions.</div>
<div class="mono" style="margin-top:8px">针对现有基于摄像头的深度强化学习方法缺乏高层场景上下文且依赖固定奖励函数的不足，本文提出了一种结合神经符号特征表示和软一阶逻辑奖励函数的新流程。该方法整合了语义、空间和形状信息，并增强了动态实体的空间特征，同时通过从分割图提取的语义空间谓词应用语言规则进行符号推理，以平衡人类价值。在CARLA模拟环境中的定量实验表明，相较于基线方法，该神经符号表示和软一阶逻辑奖励函数在不同交通密度和遮挡水平下提高了策略鲁棒性和安全相关性能指标，证明了整体表示与软推理的融合能支持更情境感知、价值对齐的自动驾驶决策。</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance</div>
<div class="meta-line">Authors: Agni Bandyopadhyay, Gunther Waxenegger-Wilfing</div>
<div class="meta-line">First: 2026-02-04T21:49:20+00:00 · Latest: 2026-02-04T21:49:20+00:00</div>
<div class="meta-line">Comments: Accpeted at Conference: 15th IAA Symposium on Small Satellites for Earth System Observation At: Berlin</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05075v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的多碎片交会任务规划优化：融合燃料补给与自适应碰撞规避</div>
<div class="mono" style="margin-top:8px">随着地球轨道环境日益拥挤，主动碎片清除任务在确保安全运行与降低在轨碰撞风险方面面临严峻挑战。本研究提出一种基于强化学习的框架，旨在增强主动碎片清除任务中的自适应碰撞规避能力，特别针对采用微小卫星的多碎片清除场景。微小卫星凭借其灵活性、成本效益和机动性优势，正日益适用于动态任务如碎片清除。
该框架在现有多碎片交会研究基础上，整合了燃料补给策略、高效任务规划与自适应碰撞规避机制，以优化航天器交会操作。所提出的方法采用掩码近端策略优化算法，使智能体能根据实时轨道条件动态调整机动策略。关键考量因素包括燃料效率、主动碰撞区域规避及动态轨道参数优化。
强化学习智能体通过训练掌握多碎片目标的高效交会序列规划能力，在整合必要燃料补给节点的同时优化燃料消耗与任务时长。研究采用基于铱星33号碎片数据集的仿真场景进行评估，涵盖多样化轨道构型与碎片分布以验证鲁棒性与适应性。结果表明，相较于传统启发式方法，该强化学习框架在提升任务效率的同时显著降低了碰撞风险。
本工作为复杂多碎片清除任务规划提供了可扩展的解决方案，并适用于自主空间任务规划中的其他多目标交会问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing challenge of orbital debris and the need for safe, efficient active debris removal (ADR) missions, this paper introduces a reinforcement learning framework that optimizes multi-debris rendezvous planning for small satellites. The method employs a masked Proximal Policy Optimization algorithm to dynamically plan mission sequences, integrating adaptive collision avoidance and refueling strategies to enhance real-time decision-making based on orbital conditions. Experimental results using simulated scenarios from the Iridium 33 debris dataset demonstrate that the approach reduces collision risks and improves fuel and time efficiency compared to traditional heuristic methods, offering a scalable solution for autonomous space mission planning.</div>
<div class="mono" style="margin-top:8px">本文针对日益严重的轨道碎片问题以及主动碎片清除任务对安全高效操作的需求，提出了一种基于强化学习的框架，用于优化小型卫星的多碎片交会任务规划。该方法采用掩码近端策略优化算法，动态规划任务序列，整合了自适应碰撞规避和燃料补给策略，以根据实时轨道条件增强决策能力。基于铱星33碎片数据集的模拟实验结果表明，与传统启发式方法相比，该框架降低了碰撞风险，并提高了燃料和时间效率，为自主空间任务规划提供了一个可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">StagePilot: A Deep Reinforcement Learning Agent for Stage-Controlled Cybergrooming Simulation</div>
<div class="meta-line">Authors: Heajun An, Qi Zhang, Minqian Liu, Xinyi Zhang, Sang Won Lee, Lifu Huang, Pamela J. Wisniewski, Jin-Hee Cho</div>
<div class="meta-line">First: 2026-02-04T21:22:45+00:00 · Latest: 2026-02-04T21:22:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05060v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cybergrooming is an evolving threat to youth, necessitating proactive educational interventions. We propose StagePilot, an offline RL-based dialogue agent that simulates the stage-wise progression of grooming behaviors for prevention training. StagePilot selects conversational stages using a composite reward that balances user sentiment and goal proximity, with transitions constrained to adjacent stages for realism and interpretability. We evaluate StagePilot through LLM-based simulations, measuring stage completion, dialogue efficiency, and emotional engagement. Results show that StagePilot generates realistic and coherent conversations aligned with grooming dynamics. Among tested methods, the IQL+AWAC agent achieves the best balance between strategic planning and emotional coherence, reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StagePilot：一种用于分阶段控制网络诱骗模拟的深度强化学习智能体</div>
<div class="mono" style="margin-top:8px">网络诱骗是对青少年的持续威胁，亟需主动的教育干预措施。我们提出StagePilot，一种基于离线强化学习的对话智能体，通过模拟诱骗行为的分阶段进展进行预防训练。StagePilot采用综合奖励机制选择对话阶段，平衡用户情感与目标接近度，并通过限制阶段间相邻转移确保真实性与可解释性。我们通过基于大语言模型的仿真评估StagePilot，测量阶段完成度、对话效率和情感参与度。结果表明，StagePilot能生成符合诱骗动态的真实连贯对话。在测试方法中，IQL+AWAC智能体在策略规划与情感连贯性间取得最佳平衡，其抵达最终阶段的频率较基线提升达43%，同时保持超过70%的情感对齐度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for proactive educational tools against cybergrooming threats to youth, this paper introduces StagePilot, an offline reinforcement learning dialogue agent designed to simulate the stage-wise progression of grooming behaviors for prevention training. The method employs a composite reward balancing user sentiment and goal proximity, with transitions constrained to adjacent stages to ensure realism and interpretability. Experimental evaluation using LLM-based simulations demonstrates that StagePilot generates realistic and coherent conversations aligned with grooming dynamics, with the IQL+AWAC agent achieving the best performance by reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.</div>
<div class="mono" style="margin-top:8px">针对网络诱拐对青少年的持续威胁，本文提出StagePilot，一种基于离线强化学习的对话智能体，旨在模拟诱拐行为的阶段性进展以用于预防培训。该方法采用结合用户情感和目标接近度的复合奖励机制，并通过限制阶段间相邻转换来确保真实性和可解释性。基于大语言模型的实验评估表明，StagePilot能生成符合诱拐动态的真实连贯对话，其中IQL+AWAC智能体在策略规划与情感连贯性上取得最佳平衡，其最终阶段到达频率比基线方法提高达43%，同时保持超过70%的情感对齐度。</div>
</details>
</div>
<div class="card">
<div class="title">ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation</div>
<div class="meta-line">Authors: Songyuan Zhang, Oswin So, H. M. Sabbir Ahmad, Eric Yang Yu, Matthew Cleaveland, Mitchell Black, Chuchu Fan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-04T21:03:11+00:00 · Latest: 2026-02-04T21:03:11+00:00</div>
<div class="meta-line">Comments: 24 pages, 17 figures; Accepted by the fourteenth International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05051v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05051v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReFORM：通过噪声操纵实现支撑集约束离线强化学习的反射流方法</div>
<div class="mono" style="margin-top:8px">离线强化学习旨在从行为策略生成的固定数据集中学习最优策略，无需额外环境交互。该场景下的常见挑战是分布外误差，即策略偏离训练分布时产生的误差。现有方法通过惩罚统计距离项使策略接近行为策略，但这会限制策略改进且无法完全避免分布外动作。另一挑战是最优策略分布可能呈多模态且难以表征。近期研究采用扩散或流策略解决此问题，但如何在保持策略表达能力的同时避免分布外误差仍不明确。本文提出ReFORM——一种基于流策略的离线强化学习方法，通过结构设计强制执行约束更宽松的支撑集条件。该方法首先学习具有有界源分布的行为克隆流策略以捕捉动作分布的支撑集，随后优化反射流为行为克隆流生成有界噪声（同时保持支撑集）以最大化性能。在OGBench基准的40项挑战性任务中（涵盖不同质量数据集且所有任务使用恒定超参数），ReFORM在性能曲线图上全面超越所有经过人工调参的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address two key challenges in offline reinforcement learning: the out-of-distribution error that arises when policies deviate from the training data, and the difficulty of representing multimodal optimal policy distributions. The method, named ReFORM, introduces a flow-based approach that enforces a support constraint by first learning a behavior cloning flow policy to capture the action distribution&#x27;s support, and then optimizing a reflected flow that generates bounded noise to maximize performance while staying within that support. Experimental results on 40 tasks from the OGBench benchmark show that ReFORM, using constant hyperparameters across all tasks, outperforms all baselines with hand-tuned hyperparameters on performance profile curves.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线强化学习中的两个关键挑战：策略偏离训练数据时产生的分布外误差，以及表示多模态最优策略分布的困难。所提出的方法ReFORM采用基于流的策略，通过首先学习一个行为克隆流策略来捕捉动作分布的支撑集，然后优化一个反射流，该流生成有界噪声以最大化性能，同时保持在支撑集内。在OGBench基准的40个任务上的实验结果表明，ReFORM在所有任务中使用恒定超参数，在性能曲线图上优于所有手动调优超参数的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Laplacian Representations for Decision-Time Planning</div>
<div class="meta-line">Authors: Dikshant Shehmar, Matthew Schlegel, Matthew E. Taylor, Marlos C. Machado</div>
<div class="meta-line">First: 2026-02-04T20:34:50+00:00 · Latest: 2026-02-04T20:34:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05031v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拉普拉斯表示在决策时规划中的应用</div>
<div class="mono" style="margin-top:8px">在基于模型的强化学习中，利用学习到的模型进行规划仍是一个核心挑战。决策时规划中，状态表示至关重要，其需支持局部代价计算并保持长时程结构。本文证明，拉普拉斯表示通过捕捉多时间尺度的状态空间距离，为规划提供了有效的潜在空间。该表示保留了有意义的距离，自然地将长时程问题分解为子目标，并缓解了长预测时域中产生的误差累积。基于这些特性，我们提出了分层规划算法ALPS，并在OGBench基准测试中的一系列离线目标条件强化学习任务上验证了其优于常用基线方法的表现，该基准此前主要由无模型方法主导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of decision-time planning in model-based reinforcement learning, where effective state representations are needed to compute local costs while maintaining long-horizon structure. The method proposes using Laplacian representations, which capture multi-scale state-space distances to preserve meaningful metrics, decompose long-horizon problems into subgoals, and reduce compounding prediction errors. Experimental results on offline goal-conditioned RL tasks from OGBench show that the introduced hierarchical algorithm, ALPS, outperforms common baselines, surpassing previous model-free approaches in this benchmark.</div>
<div class="mono" style="margin-top:8px">本文针对基于模型的强化学习中的决策时规划挑战，旨在寻找能支持局部成本计算并保持长时程结构的有效状态表示。方法提出使用拉普拉斯表示，通过捕捉多时间尺度的状态空间距离来保留有意义的度量、将长时程问题分解为子目标，并减少长期预测中的误差累积。在OGBench的离线目标条件强化学习任务上的实验结果表明，所引入的分层规划算法ALPS优于常用基线，超越了此前在该基准测试中占主导的无模型方法。</div>
</details>
</div>
<div class="card">
<div class="title">GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA</div>
<div class="meta-line">Authors: Zhichao Wang</div>
<div class="meta-line">First: 2025-10-27T21:18:19+00:00 · Latest: 2026-02-04T19:28:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23868v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.23868v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine \textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT minimizes the discrepancy between implicit and explicit reward models. It combines three key ideas: (1) the online multi-response generation and normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the implicit-explicit reward alignment principle of UNA. By jointly normalizing the implicit and explicit rewards, GIFT eliminates an otherwise intractable term that prevents effective use of implicit rewards. This normalization transforms the complex reward maximization objective into a simple mean squared error (MSE) loss between the normalized reward functions, converting a non-convex optimization problem into a convex, stable, and analytically differentiable formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy and thus retains exploration capability. Compared to GRPO, it requires fewer hyperparameters, converges faster, and generalizes better with significantly reduced training overfitting. Empirically, GIFT achieves superior reasoning and alignment performance on mathematical benchmarks while remaining computationally efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GIFT：基于群体相对隐式微调整合GRPO、DPO与UNA</div>
<div class="mono" style="margin-top:8px">本文提出一种新颖的强化学习框架——\textbf{群体相对隐式微调}（GIFT），用于对齐大语言模型。与PPO或GRPO直接最大化累积奖励不同，GIFT通过最小化隐式与显式奖励模型之间的差异实现优化。该框架融合三项核心思想：（1）GRPO的在线多响应生成与归一化机制；（2）DPO的隐式奖励建模方法；（3）UNA的隐式-显式奖励对齐原则。通过对隐式与显式奖励进行联合归一化，GIFT消除了阻碍隐式奖励有效使用的复杂项，将原本非凸的奖励最大化目标转化为归一化奖励函数间的均方误差损失，从而构建出凸优化、稳定且可解析微分的训练目标。相较于DPO、UNA等离线方法，GIFT保持在线策略特性并保留探索能力；相比GRPO，其超参数更少、收敛更快、泛化能力更强且显著降低训练过拟合。实验表明，GIFT在数学推理基准测试中取得卓越的对齐性能，同时保持高效计算特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GIFT, a reinforcement learning framework designed to align large language models by addressing the limitations of existing methods like PPO, GRPO, DPO, and UNA. Its motivation is to avoid directly maximizing cumulative rewards, instead minimizing the discrepancy between implicit and explicit reward models through a combination of GRPO&#x27;s online multi-response generation, DPO&#x27;s implicit reward formulation, and UNA&#x27;s alignment principle. The method jointly normalizes implicit and explicit rewards, transforming the optimization into a convex mean squared error loss, which simplifies training and enhances stability. Experimental results show that GIFT outperforms baselines in reasoning and alignment on mathematical benchmarks, with faster convergence, better generalization, reduced overfitting, and maintained computational efficiency while retaining on-policy exploration.</div>
<div class="mono" style="margin-top:8px">本文提出了GIFT，一种用于对齐大语言模型的强化学习框架，旨在解决PPO、GRPO、DPO和UNA等现有方法的局限性。其动机是通过结合GRPO的在线多响应生成、DPO的隐式奖励公式和UNA的对齐原则，避免直接最大化累积奖励，转而最小化隐式与显式奖励模型之间的差异。该方法对隐式和显式奖励进行联合归一化，将优化问题转化为凸的均方误差损失，从而简化训练并提高稳定性。实验结果表明，GIFT在数学基准测试中实现了更优的推理和对齐性能，具有更快的收敛速度、更好的泛化能力、减少的训练过拟合，同时保持了计算效率并保留了在线策略探索能力。</div>
</details>
</div>
<div class="card">
<div class="title">Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives</div>
<div class="meta-line">Authors: Ioannis Anagnostides, Itai Zilberstein, Zachary W. Sollie, Arman Kilic, Tuomas Sandholm</div>
<div class="meta-line">First: 2026-02-04T19:24:06+00:00 · Latest: 2026-02-04T19:24:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04990v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04990v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamental barrier: incentives. In this position paper, we highlight that organ allocation is not merely a static optimization problem, but rather a complex game involving transplant centers, clinicians, and regulators. Focusing on US adult heart transplant allocation, we identify critical incentive misalignments across the decision-making pipeline, and present data showing that they are having adverse consequences today. Our main position is that the next generation of allocation policies should be incentive aware. We outline a research agenda for the machine learning community, calling for the integration of mechanism design, strategic classification, causal inference, and social choice to ensure robustness, efficiency, and fairness in the face of strategic behavior from the various constituent groups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>立场：心脏移植分配政策优化的机器学习应考量激励因素</div>
<div class="mono" style="margin-top:8px">稀缺供体器官的分配是医疗领域最具影响力的算法挑战之一。尽管该领域正迅速从僵化的规则系统转向机器学习和数据驱动的优化，我们认为当前方法常忽视一个根本障碍：激励。本文强调，器官分配不仅是静态优化问题，更是涉及移植中心、临床医生和监管机构的复杂博弈。聚焦美国成人心脏移植分配，我们指出决策流程中关键的激励错配，并通过数据证明其已产生不利后果。核心立场是：新一代分配政策应具备激励意识。我们为机器学习界提出研究议程，呼吁整合机制设计、策略分类、因果推断与社会选择理论，以确保在各方策略行为下实现鲁棒性、效率与公平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This position paper argues that current machine learning approaches to optimizing heart transplant allocation in the US overlook the critical role of incentives and strategic behavior among transplant centers and clinicians, treating it as a static optimization rather than a complex game. The authors propose that next-generation policies must be incentive-aware, integrating methods from mechanism design, strategic classification, causal inference, and social choice to address misalignments that currently lead to adverse outcomes. Their analysis of the decision-making pipeline identifies these incentive issues as having real-world negative consequences, and they outline a research agenda to develop robust, efficient, and fair allocation systems that account for strategic responses.</div>
<div class="mono" style="margin-top:8px">这篇立场论文指出，当前美国心脏移植分配优化中的机器学习方法忽视了移植中心和临床医生等主体的激励与策略行为，错误地将其视为静态优化问题而非复杂博弈。作者主张下一代分配政策必须具备激励感知能力，整合机制设计、策略分类、因果推断和社会选择等方法，以解决现有激励错配导致的不良后果。通过分析决策流程，他们证实了这些激励问题已产生实际负面影响，并为此规划了一个研究议程，旨在开发能够应对策略行为、确保鲁棒性、效率与公平性的分配系统。</div>
</details>
</div>
<div class="card">
<div class="title">Stochastic hierarchical data-driven optimization: application to plasma-surface kinetics</div>
<div class="meta-line">Authors: José Afonso, Vasco Guerra, Pedro Viegas</div>
<div class="meta-line">First: 2026-02-04T19:03:40+00:00 · Latest: 2026-02-04T19:03:40+00:00</div>
<div class="meta-line">Comments: 19 pages and 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04975v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04975v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces a stochastic hierarchical optimization framework inspired by Sloppy Model theory for the efficient calibration of physical models. Central to this method is the use of a reduced Hessian approximation, which identifies and targets the stiff parameter subspace using minimal simulation queries. This strategy enables efficient navigation of highly anisotropic landscapes, avoiding the computational burden of exhaustive sampling. To ensure rigorous inference, we integrate this approach with a probabilistic formulation that derives a principled objective loss function directly from observed data. We validate the framework by applying it to the problem of plasma-surface interactions, where accurate modelling is strictly limited by uncertainties in surface reactivity parameters and the computational cost of kinetic simulations. Comparative analysis demonstrates that our method consistently outperforms baseline optimization techniques in sample efficiency. This approach offers a general and scalable tool for optimizing models of complex reaction systems, ranging from plasma chemistry to biochemical networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机分层数据驱动优化：在等离子体-表面动力学中的应用</div>
<div class="mono" style="margin-top:8px">本研究受松散模型理论启发，提出了一种随机分层优化框架，用于物理模型的高效校准。该方法的核心是采用简化海森矩阵近似，通过最少的模拟查询识别并针对刚性参数子空间。该策略能够高效探索高度各向异性的参数空间，避免穷举采样的计算负担。为确保严谨的推断，我们将此方法与概率化公式相结合，直接从观测数据推导出具有理论依据的目标损失函数。通过将该框架应用于等离子体-表面相互作用问题（其精确建模受到表面反应性参数不确定性和动力学模拟计算成本的严格限制），我们验证了其有效性。对比分析表明，本方法在样本效率上持续优于基准优化技术。该框架为从等离子体化学到生化网络等复杂反应系统的模型优化提供了通用且可扩展的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to efficiently calibrate physical models with high computational costs and parameter uncertainties, this paper introduces a stochastic hierarchical optimization framework inspired by Sloppy Model theory. The method employs a reduced Hessian approximation to identify and target the stiff parameter subspace, enabling efficient navigation of anisotropic landscapes with minimal simulation queries, and integrates a probabilistic formulation for rigorous inference. Experimental validation on plasma-surface interaction models shows that the approach consistently outperforms baseline optimization techniques in sample efficiency, offering a scalable tool for complex reaction systems.</div>
<div class="mono" style="margin-top:8px">本研究针对物理模型校准中计算成本高和参数不确定性的问题，提出了一种受松散模型理论启发的随机分层优化框架。该方法采用简化Hessian近似来识别并针对刚性参数子空间，以最少的模拟查询高效遍历各向异性参数空间，并结合概率化公式确保严格推断。在等离子体-表面相互作用模型上的实验验证表明，该方法在样本效率上持续优于基线优化技术，为复杂反应系统提供了一个可扩展的优化工具。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforced Attention Learning</div>
<div class="meta-line">Authors: Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen, Derek Zhiyuan Cheng</div>
<div class="meta-line">First: 2026-02-04T18:59:52+00:00 · Latest: 2026-02-04T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04884v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化注意力学习</div>
<div class="mono" style="margin-top:8px">通过测试时扩展，基于强化学习（RL）的后训练显著提升了大语言模型（LLM）的推理能力。然而，将该范式通过冗长推理过程扩展到多模态大语言模型（MLLM）时，对感知能力的提升有限，甚至可能导致性能下降。我们提出强化注意力学习（RAL），这是一种直接优化内部注意力分布而非输出词元序列的策略梯度框架。通过将优化目标从“生成什么”转向“关注何处”，RAL促进了复杂多模态输入中的有效信息分配与更优的语义关联。在多样化图像与视频基准测试中的实验表明，该方法相较GRPO及其他基线模型取得了一致性提升。我们进一步提出同策略注意力蒸馏，证明迁移潜在注意力行为比标准知识蒸馏能产生更强的跨模态对齐效果。我们的研究确立了注意力策略作为多模态后训练的一种原理性通用替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limited effectiveness of applying standard reinforcement learning (RL) post-training, which optimizes output token sequences, to multimodal large language models (MLLMs), as it can degrade perceptual performance. The proposed method, Reinforced Attention Learning (RAL), addresses this by introducing a policy-gradient framework that directly optimizes the model&#x27;s internal attention distributions, thereby shifting the focus from what to generate to where to attend for better information allocation. Main experimental results demonstrate that RAL achieves consistent performance gains over baselines like GRPO across various image and video benchmarks, and the introduced On-Policy Attention Distillation shows that transferring optimized attention behaviors yields stronger cross-modal alignment than conventional knowledge distillation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，将标准的强化学习后训练方法（即优化输出词序列）应用于多模态大语言模型时效果有限，甚至可能损害感知性能。为此，本文提出了强化注意力学习方法，该方法采用策略梯度框架，直接优化模型内部的注意力分布，从而将优化重点从生成内容转向分配注意力以提升信息处理效率。主要实验结果表明，该方法在多种图像和视频基准测试中均优于GRPO等基线模型，同时引入的在线策略注意力蒸馏技术证明，迁移优化后的注意力行为比标准知识蒸馏能实现更强的跨模态对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Trust Region in LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee</div>
<div class="meta-line">First: 2026-02-04T18:59:04+00:00 · Latest: 2026-02-04T18:59:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04879v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视大语言模型强化学习中的信任区域</div>
<div class="mono" style="margin-top:8px">强化学习已成为微调大语言模型的核心技术，其中近端策略优化算法是实际上的标准算法。尽管应用广泛，我们认为PPO核心的概率比截断机制在结构上并不适合大语言模型固有的巨大词汇量。PPO基于采样词元的概率比约束策略更新，该比值是对真实策略散度的噪声单样本蒙特卡洛估计。这导致了次优的学习动态：低概率词元的更新被过度惩罚，而高概率词元的潜在灾难性偏移却约束不足，从而造成训练效率低下和稳定性问题。为此，我们提出散度近端策略优化算法，用基于策略散度直接估计（如总变差或KL散度）的原则性约束替代启发式截断。为避免巨大内存开销，我们引入高效的二元与Top-K近似方法，以可忽略的开销捕捉核心散度。大量实验评估表明，DPPO相比现有方法实现了更优的训练稳定性和效率，为基于强化学习的大语言模型微调提供了更稳健的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the structural limitations of Proximal Policy Optimization (PPO) for fine-tuning Large Language Models, arguing that its token-level probability ratio clipping is a noisy and sub-optimal trust region constraint for large vocabularies, leading to inefficient and unstable training. The method proposed, Divergence Proximal Policy Optimization (DPPO), replaces heuristic clipping with a principled constraint based on a direct estimate of policy divergence, using efficient Binary and Top-K approximations to manage computational overhead. Experimental results demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, providing a more robust foundation for reinforcement learning-based LLM fine-tuning.</div>
<div class="mono" style="margin-top:8px">本文的动机源于近端策略优化（PPO）在微调大语言模型时的结构缺陷，认为其基于词元概率比值的裁剪机制对于大词汇表而言是一种嘈杂且次优的信任域约束，导致训练效率低下且不稳定。所提出的方法——散度近端策略优化（DPPO）——用基于策略散度（如总变差或KL散度）直接估计的原则性约束替代了启发式裁剪，并引入了高效的二值化和Top-K近似以控制计算开销。实验结果表明，与现有方法相比，DPPO实现了更优的训练稳定性和效率，为基于强化学习的大语言模型微调提供了更稳健的基础。</div>
</details>
</div>
<div class="card">
<div class="title">CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation</div>
<div class="meta-line">Authors: Yannick Denker, Alexander Gepperth</div>
<div class="meta-line">First: 2026-02-04T18:54:26+00:00 · Latest: 2026-02-04T18:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRoSS：面向高任务多样性与真实物理仿真的可扩展强化学习持续机器人仿真套件</div>
<div class="mono" style="margin-top:8px">持续强化学习要求智能体在任务序列中学习且不遗忘已习得的策略。本研究基于Gazebo仿真器中的高拟真机器人，提出一种新型持续强化学习基准套件。该持续机器人仿真套件采用两种机器人平台：配备激光雷达、摄像头及碰撞传感器的两轮差速驱动机器人，以及七关节机械臂。前者用于线跟随与物体推动场景，通过视觉与结构参数变化生成大量差异化任务；后者应用于两种目标抵达场景：基于笛卡尔手部位置的高层控制（延续Continual World基准设计）与基于关节角度的底层控制。针对机械臂基准测试，我们额外提供仅需运动学计算的变体方案，在无需传感器数据时可绕过物理仿真环节，运行速度提升两个数量级。本套件具备易扩展性，支持在高度物理真实的机器人环境中开展受控的持续强化学习研究，尤其支持近乎任意的仿真传感器集成。为确保可复现性与易用性，我们提供开箱即用的容器化部署方案（Apptainer），并报告了深度Q网络、策略梯度等标准强化学习算法的性能表现，验证了其作为可扩展、可复现的持续强化学习研究基准的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces CRoSS, a benchmark suite for continual reinforcement learning (CRL) in robotics, motivated by the need for scalable, realistic environments where agents can learn sequences of tasks without forgetting prior policies. The method employs two simulated robotic platforms in Gazebo: a differential-drive robot for line-following and object-pushing tasks with varied visual and structural parameters, and a seven-joint robotic arm for goal-reaching scenarios with both high-level cartesian and low-level joint control, including kinematics-only variants for faster computation. Experimental results demonstrate the suite&#x27;s extensibility and high physical realism, supporting standard RL algorithms like DQN and policy gradient methods, and it provides a containerized setup for reproducibility, establishing it as a scalable benchmark for CRL research.</div>
<div class="mono" style="margin-top:8px">本文提出了CRoSS，一个用于机器人持续强化学习（CRL）的基准测试套件，其动机是需为智能体提供可扩展、高真实度的环境，以学习任务序列而不遗忘先前策略。方法基于Gazebo模拟器中的两个机器人平台：一个用于线跟随和物体推动任务的差速驱动机器人，具有变化的视觉和结构参数；另一个七关节机械臂用于目标到达场景，支持高级笛卡尔和低级关节控制，并包含无需物理模拟的仅运动学变体以加速计算。实验结果展示了该套件的可扩展性和高物理真实感，兼容DQN和策略梯度等标准RL算法，且提供容器化设置以确保可复现性，从而成为CRL研究的可扩展基准。</div>
</details>
</div>
<div class="card">
<div class="title">Privileged Information Distillation for Language Models</div>
<div class="meta-line">Authors: Emiliano Penaloza, Dheeraj Vattikonda, Nicolas Gontier, Alexandre Lacoste, Laurent Charlin, Massimo Caccia</div>
<div class="meta-line">First: 2026-02-04T18:46:17+00:00 · Latest: 2026-02-04T18:46:17+00:00</div>
<div class="meta-line">Comments: Abstract border should have been purple</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04942v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04942v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that π-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型的特权信息蒸馏</div>
<div class="mono" style="margin-top:8px">训练阶段的特权信息（PI）能使语言模型在原本会失败的任务上取得成功，这使其成为困难、长周期场景下强化学习的强大工具。然而，将借助PI习得的能力迁移至推理时无法使用PI的策略，仍是一个根本性挑战。我们在为多轮智能体环境蒸馏前沿模型的背景下研究此问题——闭源系统通常隐藏内部推理过程，仅暴露行动轨迹。这打破了标准蒸馏流程，因为成功行为可观测而推理过程不可见。为此，我们提出π-Distill：一种联合师生目标，使用同一模型同步训练PI条件化教师模型与无条件化学生模型。同时，我们还提出策略上自蒸馏（OPSD）——通过强化学习训练学生模型，并施加其与PI条件化教师模型间的反向KL惩罚。实验表明，这两种算法均能有效利用仅含行动的PI蒸馏前沿智能体。具体而言，π-Distill（及部分场景下的OPSD）在多个智能体基准测试、模型及PI形式中，均优于假设具备完整思维链监督的行业标准流程（监督微调后接强化学习）。我们通过深入分析补充实验结果，重点解析π-Distill实现PI有效学习的机制要素，并阐明OPSD具备竞争力的适用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of distilling capabilities from large language models that utilize privileged information (PI) during training but must operate without it at inference, particularly in multi-turn agentic environments where only action trajectories are observable. To tackle this, the authors introduce two methods: π-Distill, a joint teacher-student objective that trains both a PI-conditioned teacher and an unconditioned student simultaneously, and On-Policy Self-Distillation (OPSD), which uses reinforcement learning with a reverse KL-penalty between the student and teacher. Experimental results demonstrate that both approaches, especially π-Distill, outperform standard practices like supervised fine-tuning followed by RL, which assume access to full chain-of-thought supervision, across various benchmarks, models, and forms of PI, effectively enabling knowledge transfer from action-only PI.</div>
<div class="mono" style="margin-top:8px">本文研究了在训练时使用特权信息（PI）的语言模型如何将所学能力迁移到推理时不依赖PI的策略中，特别是在多轮智能体环境中，仅能观察到动作轨迹而无法获取内部推理过程。为此，作者提出了两种方法：π-Distill，一种联合教师-学生目标，同时训练PI条件教师和无条件学生；以及基于策略的自蒸馏（OPSD），使用强化学习并引入学生与教师之间的反向KL惩罚。实验结果表明，这两种方法（尤其是π-Distill）在多个智能体基准测试、模型和PI形式中，均优于假设能获取完整思维链监督的标准实践（如监督微调后接强化学习），有效实现了仅从动作PI中蒸馏知识。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-04T18:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人机交互贝叶斯优化的个性化图像生成</div>
<div class="mono" style="margin-top:8px">假设Alice心中有一幅特定图像$x^\ast$，例如她童年成长街道的景象。为生成该图像，她通过多轮提示引导生成模型，得到图像$x^{p*}$。虽然$x^{p*}$已接近$x^\ast$，但Alice发现难以通过语言提示完全消除差异。本文指出：即使语言描述已达极限，人类仍能判断新图像$x^+$是否比$x^{p*}$更接近$x^\ast$。基于此，我们提出MultiBO（多选择偏好贝叶斯优化）方法：以$x^{p*}$为基准生成$K$幅新图像，获取用户偏好反馈，利用反馈指导扩散模型，最终生成新一轮$K$幅图像。实验表明，在$B$轮用户反馈内，即使生成模型未获取$x^\ast$的直接信息，仍能显著逼近目标图像。通过30名用户的定性评分，以及与5个基线模型的定量指标对比，结果证实人类的多选择反馈可有效助力个性化图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of personalized image generation when language prompts alone are insufficient to capture a user&#x27;s specific mental image. The proposed method, Multi-Choice Preferential Bayesian Optimization (MultiBO), leverages human preferential feedback by generating multiple candidate images, asking users to select which is closer to their target, and using this feedback to iteratively guide a diffusion model. Experimental results from 30 users and quantitative comparisons against five baselines demonstrate that this human-in-the-loop approach significantly narrows the gap between generated images and the user&#x27;s intended image within a limited number of feedback rounds.</div>
<div class="mono" style="margin-top:8px">本文针对个性化图像生成中语言提示无法精确捕捉用户心中特定图像的问题展开研究。提出的方法名为多选择偏好贝叶斯优化（MultiBO），它利用人类偏好反馈：生成多个候选图像，让用户选择哪个更接近目标图像，并以此反馈迭代引导扩散模型。来自30名用户的定性评分及与五个基线模型的定量比较结果表明，这种人在回路的方法能在有限反馈轮次内，显著缩小生成图像与用户目标图像之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning</div>
<div class="meta-line">Authors: Joydeep Chandra, Satyam Kumar Navneet, Aleksandr Algazinov, Yong Zhang</div>
<div class="meta-line">First: 2026-02-04T18:10:59+00:00 · Latest: 2026-02-04T18:10:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04821v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\% coverage efficiency, controls FDR at 4.1\% under verified dependence, and improves safety rate to 95.2\% compared to 69\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于不确定性感知共形预测与世界模型强化学习的城市交通安全控制</div>
<div class="mono" style="margin-top:8px">城市交通管理系统需同时预测未来状况、检测异常并采取安全纠正措施，同时提供可靠性保证。我们提出STREAM-RL统一框架，包含三项创新算法贡献：(1) PU-GAT+不确定性引导自适应共形预测器，通过置信度单调注意力动态调整图注意力权重，实现无分布覆盖保证；(2) CRFN-BY共形残差流网络，在任意依赖关系下通过标准化流建模不确定性归一化残差，并采用Benjamini-Yekutieli错误发现率控制；(3) LyCon-WRL+不确定性引导安全世界模型强化学习智能体，具备李雅普诺夫稳定性证明、经认证的利普希茨边界及不确定性传播想象推演。据我们所知，这是首个将校准不确定性从预测端传播至异常检测再到安全策略学习，并具备端到端理论保证的框架。在多个真实交通轨迹数据上的实验表明：STREAM-RL实现91.4%的覆盖效率，在已验证依赖关系下将错误发现率控制在4.1%，安全率提升至95.2%（标准PPO为69%），同时获得更高奖励且端到端推理延迟仅23毫秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for reliable and safe urban traffic control systems by proposing STREAM-RL, a unified framework that integrates uncertainty-aware forecasting, anomaly detection, and safe reinforcement learning. The method introduces three novel components: PU-GAT+ for adaptive conformal forecasting with coverage guarantees, CRFN-BY for anomaly detection with false discovery rate control, and LyCon-WRL+ for safe policy learning using world-model RL with stability certificates. Experimental results on real-world traffic data show that STREAM-RL achieves 91.4% coverage efficiency, controls false discovery rate at 4.1%, improves safety rate to 95.2% compared to 69% for standard PPO, and maintains low inference latency of 23ms.</div>
<div class="mono" style="margin-top:8px">本文针对城市交通控制对可靠性及安全性的需求，提出了STREAM-RL框架，该框架将不确定性感知预测、异常检测和安全强化学习统一集成。方法包含三个新颖组件：PU-GAT+实现具有覆盖保证的自适应共形预测，CRFN-BY用于控制错误发现率的异常检测，以及LyCon-WRL+通过世界模型强化学习进行安全策略学习并具备稳定性证明。在真实交通轨迹数据上的实验表明，STREAM-RL实现了91.4%的覆盖效率，将错误发现率控制在4.1%，安全率提升至95.2%（标准PPO为69%），同时保持23毫秒的端到端推理延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Rewards in Reinforcement Learning for Cyber Defence</div>
<div class="meta-line">Authors: Elizabeth Bates, Chris Hicks, Vasilios Mavroudis</div>
<div class="meta-line">First: 2026-02-04T17:55:23+00:00 · Latest: 2026-02-04T17:55:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越奖励：强化学习在网络安全防御中的应用</div>
<div class="mono" style="margin-top:8px">近年来，利用深度强化学习训练自主网络防御代理以保护计算机网络的研究兴趣激增。这些代理通常在网络训练环境中使用密集、高度设计的奖励函数进行训练，这些函数结合了对多种（非）理想状态和代价高昂行动的惩罚与激励。密集奖励有助于缓解复杂环境探索的挑战，但可能导致代理偏向次优且风险更高的解决方案，这在复杂网络环境中尤为关键。我们通过多种稀疏与密集奖励函数、两个成熟的网络训练环境、不同网络规模以及策略梯度和基于价值的强化学习算法，全面评估了奖励函数结构对学习过程及策略行为特征的影响。评估采用了一种新颖的基准评估方法，可直接比较不同奖励函数的效果，揭示奖励、行动空间与网络环境中次优策略风险之间的微妙关联。结果表明，只要稀疏奖励与目标一致且能频繁触发，不仅能提高训练可靠性，还能产生具有更低风险策略的更有效网络防御代理。令人惊讶的是，稀疏奖励还能生成更符合网络防御目标、且无需显式数值惩罚即可节制使用高成本防御行动的优化策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the risk that dense, engineered reward functions may bias reinforcement learning agents toward suboptimal and risky policies in autonomous cyber defense, this paper systematically evaluates the impact of reward structure. The method employs a novel ground-truth evaluation approach to compare sparse and dense reward functions across two established cyber gym environments, various network sizes, and both policy-gradient and value-based RL algorithms. The main experimental results demonstrate that well-designed sparse rewards, when goal-aligned and frequently encountered, uniquely produce more reliable training, more effective defense agents, and lower-risk policies that sparingly use costly actions without explicit penalties, better aligning with defender objectives.</div>
<div class="mono" style="margin-top:8px">本文的研究动机在于，在自主网络防御的强化学习训练中，密集设计的奖励函数可能导致智能体偏向次优且高风险的策略。研究方法采用了一种新颖的基准评估方法，在两个成熟的网络训练环境、不同网络规模以及策略梯度和基于价值的强化学习算法中，系统比较了稀疏与密集奖励函数的影响。主要实验结果表明，设计良好、目标对齐且能频繁触发的稀疏奖励，能独特地实现更可靠的训练、更有效的防御智能体以及更低风险的策略，这些策略无需显式惩罚即可节约使用高成本防御动作，从而更好地与防御者目标保持一致。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning</div>
<div class="meta-line">Authors: Wolfgang Maass, Sabine Janzen, Prajvi Saxena, Sach Mukherjee</div>
<div class="meta-line">First: 2026-02-04T17:53:28+00:00 · Latest: 2026-02-04T17:53:28+00:00</div>
<div class="meta-line">Comments: 16 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04807v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>演化传入架构：面向损伤规避学习的仿生模型</div>
<div class="mono" style="margin-top:8px">本文提出传入学习框架，通过生成计算传入痕迹作为损伤规避学习的自适应内部风险信号。该仿生框架采用双层架构：进化优化（外层循环）发现能支持有效策略学习的传入感知架构，而强化学习（内层循环）利用这些信号训练损伤规避策略。该研究将传入感知形式化为高效学习的归纳偏置——架构选择标准是其促进有效学习的能力（而非直接最小化损伤）。我们在平滑性与有界噪声假设下提供了理论收敛保证。通过在生物力学数字孪生长时程（数十年生命周期）场景中的验证表明：基于计算传入痕迹的演化架构相比人工设计基线显著提升效率与年龄鲁棒性，实现具有年龄依赖行为适应的策略（高风险行为减少23%）。消融研究验证了计算传入痕迹信号、进化机制与预测差异的核心作用。我们已开源代码与数据以确保可复现性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for robust damage-avoidance learning in long-term systems like biomechanical digital twins, this paper introduces Afferent Learning, a biologically-inspired framework that evolves adaptive internal risk signals called Computational Afferent Traces (CATs). The method employs a two-level architecture: an outer evolutionary loop optimizes afferent sensing architectures to provide inductive biases, while an inner reinforcement learning loop trains policies using these CAT signals. Experimental results in a multi-decade simulation show that evolved CAT architectures significantly outperform hand-designed baselines in learning efficiency and age-robustness, achieving a 23% reduction in high-risk actions and enabling age-dependent behavioral adaptation, with ablations confirming the necessity of key components.</div>
<div class="mono" style="margin-top:8px">受生物系统启发，本文针对生物力学数字孪生等长期运行系统的损伤规避学习需求，提出了传入学习框架，该框架演化出称为计算传入轨迹的自适应内部风险信号。方法采用双层架构：外层进化循环优化传入感知架构以提供归纳偏置，内层强化学习循环则利用这些信号训练策略。在多年代际模拟中的实验结果表明，演化出的传入架构在学习效率和年龄鲁棒性上显著优于人工设计的基线，实现了高风险行为减少23%并促成年龄依赖的行为适应，消融研究验证了关键组件的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging</div>
<div class="meta-line">Authors: Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu</div>
<div class="meta-line">First: 2026-02-04T17:52:17+00:00 · Latest: 2026-02-04T17:52:17+00:00</div>
<div class="meta-line">Comments: 14 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04805v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04805v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>皮肤令牌：一种用于统一自回归绑定系统的学习型紧凑表示</div>
<div class="mono" style="margin-top:8px">生成式3D模型的快速扩散在动画流程中造成了关键瓶颈：绑定。现有自动化方法从根本上受限于其蒙皮处理方式，将其视为一个不适定的高维回归任务，优化效率低下且通常与骨骼生成解耦。我们认为这是一个表示问题，并提出了皮肤令牌：一种学习得到的、紧凑的、离散的蒙皮权重表示。通过利用FSQ-CVAE捕捉蒙皮固有的稀疏性，我们将任务从连续回归重构为更易处理的令牌序列预测问题。该表示支持TokenRig——一个统一的自回归框架，将整个绑定系统建模为骨骼参数与皮肤令牌的单一序列，从而学习骨骼与皮肤变形之间的复杂依赖关系。该统一模型随后可进行强化学习阶段，通过定制的几何与语义奖励提升对复杂、分布外资产的泛化能力。量化结果显示，皮肤令牌表示在蒙皮精度上比现有最优方法提升98%-133%，而经RL优化的完整TokenRig框架将骨骼预测精度提升17%-22%。本研究提出了一种统一的生成式绑定方法，实现了更高保真度与鲁棒性，为3D内容创作中长期存在的挑战提供了可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the bottleneck of manual rigging in 3D animation pipelines, this paper introduces SkinTokens, a learned discrete representation for skinning weights that reframes skinning from continuous regression to token sequence prediction using an FSQ-CVAE. The method enables TokenRig, a unified autoregressive framework that models the entire rig as a sequence of skeletal parameters and SkinTokens, capturing dependencies between skeletons and skin deformations, and is further refined with reinforcement learning using geometric and semantic rewards. Experimentally, the SkinTokens representation improves skinning accuracy by 98%-133% over state-of-the-art methods, while the full TokenRig framework enhances bone prediction by 17%-22%, demonstrating higher fidelity and robustness for 3D content creation.</div>
<div class="mono" style="margin-top:8px">本文针对三维动画流程中手动绑定骨架的瓶颈问题，提出了SkinTokens这一用于蒙皮权重的学习型离散表示，通过FSQ-CVAE将蒙皮从连续回归重构为令牌序列预测任务。该方法支持TokenRig这一统一自回归框架，将整个绑定建模为骨骼参数和SkinTokens的序列，以捕捉骨骼与皮肤变形间的复杂依赖关系，并利用几何和语义奖励通过强化学习进一步优化。实验结果表明，SkinTokens表示将蒙皮精度较现有最佳方法提升了98%-133%，而完整的TokenRig框架则将骨骼预测性能提高了17%-22%，为三维内容创作提供了更高保真度和鲁棒性的可扩展解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Improving Pretraining: using post-trained models to pretrain better models</div>
<div class="meta-line">Authors: Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar, Jason Weston, Olga Golovneva</div>
<div class="meta-line">First: 2026-01-29T07:09:30+00:00 · Latest: 2026-02-04T17:31:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21343v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21343v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model&#x27;s core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我改进预训练：利用后训练模型预训练更优模型</div>
<div class="mono" style="margin-top:8px">确保大语言模型生成内容的安全性、事实性与整体质量是一项关键挑战，尤其在模型日益广泛应用于现实场景的背景下。当前主流解决方案依赖于收集成本高昂、精心标注的数据集，并进行多阶段微调与对齐。然而，即使这一复杂流程也无法完全修正预训练阶段习得的偏差模式。因此，在预训练阶段解决这些问题至关重要，因为该阶段塑造了模型的核心行为模式，能从根本上避免不安全或虚构内容被深度固化。为此，我们提出一种新型预训练方法：通过流式文档处理，运用强化学习逐步优化后续K个生成标记。一个经过后训练的强模型将对候选生成内容（包括模型推演序列、原始后缀及改写后缀）进行质量、安全性与事实性评估。训练初期依赖原始与改写后缀；随着模型能力提升，强化学习机制将对高质量推演给予奖励。该方法从源头构建了更优质、更安全、更符合事实的模型。实验表明，本方法在事实性与安全性指标上分别较标准预训练提升36.2%和18.5%，整体生成质量的胜率提升最高达86.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to enhance the safety, factuality, and overall quality of large language model outputs, which are often compromised by patterns learned during pretraining and not fully corrected by later fine-tuning. To address this, the authors propose a self-improving pretraining method that uses reinforcement learning to optimize token generation, with a post-trained model evaluating candidate outputs for quality, safety, and factuality, gradually shifting reliance from original or rewritten text to model-generated rollouts as training progresses. Experimental results show significant improvements over standard pretraining, with relative gains of 36.2% in factuality, 18.5% in safety, and up to 86.3% in overall generation quality win rates.</div>
<div class="mono" style="margin-top:8px">本文的动机是提升大语言模型生成内容的安全性、事实性和整体质量，这些问题常源于预训练阶段学习到的模式，且难以通过后续微调完全纠正。为此，作者提出一种自改进的预训练方法，利用强化学习优化令牌生成，并通过一个后训练模型评估候选输出的质量、安全性和事实性，随着训练推进，逐步从依赖原始或改写文本转向模型自身生成的序列。实验结果表明，该方法相比标准预训练在事实性上相对提升36.2%，安全性上提升18.5%，整体生成质量的胜率最高提升86.3%。</div>
</details>
</div>
<div class="card">
<div class="title">When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?</div>
<div class="meta-line">Authors: Xinyu Zhou, Chang Jin, Carsten Eickhoff, Zhijiang Guo, Seyed Ali Bahrainian</div>
<div class="meta-line">First: 2026-02-04T16:54:47+00:00 · Latest: 2026-02-04T16:54:47+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04755v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04755v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\%$ and $5.80\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>沉默是金：大语言模型能否在时序问答及其他任务中学会弃答？</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）很少承认不确定性，常生成流畅但具有误导性的答案，而非选择弃答（即拒绝回答）。这一缺陷在时序问答中尤为明显：模型常忽略时效性证据，混淆不同时期的事实。本文首次实证研究了在时序问答推理中训练LLMs弃答能力的方法。现有校准等方法在捕捉复杂推理的不确定性方面可能不可靠。我们将弃答定义为可教授的技能，提出一种结合思维链监督与弃答感知奖励强化学习的训练框架。旨在系统分析不同信息类型与训练技术如何影响LLMs的时序推理弃答行为。通过多方法实验发现：强化学习显著提升推理性能——基于Qwen2.5-1.5B-Instruct初始化的模型在TimeQA-Easy/Hard的精确匹配率分别超越GPT-4o达3.46%和5.80%；在不可答问题上，其真阳性率较纯监督微调基线提升20%。分析表明：监督微调易引发过度自信损害可靠性，强化学习虽提升预测精度但存在类似风险。通过对比隐式推理线索（原始上下文、时序子语境、知识图谱）与显式思维链监督，发现隐式信息对弃答推理的助益有限。本研究为协同优化弃答与推理机制提供了新见解，为构建更可靠的LLMs奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem that large language models (LLMs) often generate confident but incorrect answers instead of abstaining when uncertain, particularly in temporal question answering where they conflate facts across time periods. The authors frame abstention as a teachable skill and introduce a training pipeline combining Chain-of-Thought supervision with Reinforcement Learning using abstention-aware rewards to jointly optimize reasoning and abstention behavior. Experimental results show that their RL-trained model, based on Qwen2.5-1.5B-Instruct, outperforms GPT-4o by significant margins on TimeQA benchmarks and improves the true positive rate on unanswerable questions by 20% compared to supervised fine-tuning alone, while analysis reveals that supervised fine-tuning induces overconfidence whereas RL improves accuracy but carries similar risks, and that implicit reasoning cues provide limited benefit compared to explicit supervision.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在不确定时往往生成自信但错误的答案而非选择弃答的问题展开研究，尤其在时序问答中，模型常混淆不同时期的事实。作者将弃答能力视为可教授的技能，提出了一种结合思维链监督与基于弃答感知奖励的强化学习的训练框架，以联合优化推理和弃答行为。实验结果表明，基于Qwen2.5-1.5B-Instruct的强化学习模型在TimeQA基准上显著超越GPT-4o，并在不可回答问题上的真阳性率比纯监督微调模型提升20%，同时分析发现监督微调会导致过度自信，而强化学习虽提升准确性但存在类似风险，且隐式推理线索相比显式监督收益有限。</div>
</details>
</div>
<div class="card">
<div class="title">Rationality Measurement and Theory for Reinforcement Learning Agents</div>
<div class="meta-line">Authors: Kejiang Qian, Amos Storkey, Fengxiang He</div>
<div class="meta-line">First: 2026-02-04T16:41:22+00:00 · Latest: 2026-02-04T16:41:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04737v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04737v1">PDF</a> · <a href="https://github.com/EVIEHub/Rationality">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy&#x27;s actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm&#x27;s generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习智能体的理性度量与理论</div>
<div class="mono" style="margin-top:8px">本文针对强化学习智能体提出了一套理性度量及相关理论，该性质日益关键却鲜有研究。我们定义部署中的行为若沿最陡方向最大化隐藏的真实价值函数，则为完全理性。策略行为与其理性对应行为在部署轨迹上的期望价值差异被定义为期望理性风险；同时定义了训练中的经验平均版本。二者的差异（称为理性风险间隙）可分解为：（1）由训练与部署间环境偏移引起的外在成分，以及（2）由算法在动态环境中泛化能力决定的内在成分。二者分别上界为：（1）训练与部署中转移核及初始状态分布的$1$-Wasserstein距离，以及（2）价值函数类的经验Rademacher复杂度。我们的理论提出了关于正则化器（包括层归一化、$\ell_2$正则化及权重归一化）与领域随机化的益处，以及环境偏移危害的假设。实验结果完全支持这些假设。代码发布于https://github.com/EVIEHub/Rationality。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of formal rationality measures in reinforcement learning by introducing a suite of metrics and theoretical bounds. The method defines perfect rationality as actions maximizing the true value function&#x27;s steepest direction, leading to measures like expected rational risk and rational risk gap, which decompose into extrinsic and intrinsic components bounded by Wasserstein distance and Rademacher complexity. Experimental results confirm hypotheses that regularizers and domain randomization improve rationality while environment shifts harm it, validating the proposed theory.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中缺乏形式化理性度量的问题，提出了一套度量指标和理论框架。方法将完美理性定义为在部署中沿真实价值函数最陡方向最大化的行动，并引入了期望理性风险和理性风险差距等度量，后者分解为外在和内在成分，分别由Wasserstein距离和Rademacher复杂度上界。实验结果验证了正则化和领域随机化提升理性、环境偏移损害理性的假设，支持了理论预测。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary Mapping of Neural Networks to Spatial Accelerators</div>
<div class="meta-line">Authors: Alessandro Pierro, Jonathan Timcheck, Jason Yik, Marius Lindauer, Eyke Hüllermeier, Marcel Wever</div>
<div class="meta-line">First: 2026-02-04T16:28:08+00:00 · Latest: 2026-02-04T16:28:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04717v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04717v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial accelerators, composed of arrays of compute-memory integrated units, offer an attractive platform for deploying inference workloads with low latency and low energy consumption. However, fully exploiting their architectural advantages typically requires careful, expert-driven mapping of computational graphs to distributed processing elements. In this work, we automate this process by framing the mapping challenge as a black-box optimization problem. We introduce the first evolutionary, hardware-in-the-loop mapping framework for neuromorphic accelerators, enabling users without deep hardware knowledge to deploy workloads more efficiently. We evaluate our approach on Intel Loihi 2, a representative spatial accelerator featuring 152 cores per chip in a 2D mesh. Our method achieves up to 35% reduction in total latency compared to default heuristics on two sparse multi-layer perceptron networks. Furthermore, we demonstrate the scalability of our approach to multi-chip systems and observe an up to 40% improvement in energy efficiency, without explicitly optimizing for it.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经网络到空间加速器的进化映射</div>
<div class="mono" style="margin-top:8px">空间加速器由计算-内存一体化单元阵列构成，为部署低延迟、低能耗的推理工作负载提供了理想平台。然而，要充分发挥其架构优势，通常需要专家精心将计算图映射到分布式处理单元。本研究通过将映射问题构建为黑盒优化任务，实现了该过程的自动化。我们提出了首个面向神经形态加速器的进化式硬件在环映射框架，使不具备深度硬件知识的用户也能更高效地部署工作负载。我们在英特尔Loihi 2芯片（典型空间加速器，每芯片含152个核心的二维网格结构）上评估该方法。针对两个稀疏多层感知器网络，相比默认启发式方法，我们的方案最高可降低35%的总延迟。此外，我们验证了该方法在多芯片系统中的可扩展性，在未专门优化能耗的情况下，能效最高提升40%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of efficiently mapping neural network inference workloads to spatial accelerators, which require expert knowledge to exploit their architectural advantages fully. The authors automate this process by formulating the mapping problem as a black-box optimization and introducing an evolutionary, hardware-in-the-loop framework that allows users without deep hardware expertise to deploy workloads. Evaluated on the Intel Loihi 2 accelerator, the method reduces total latency by up to 35% compared to default heuristics on sparse multi-layer perceptron networks and scales to multi-chip systems, achieving up to 40% improvement in energy efficiency without explicit optimization for it.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决将神经网络推理工作负载高效映射到空间加速器的挑战，这类加速器通常需要专业知识才能充分利用其架构优势。作者通过将映射问题构建为黑盒优化，并引入一种进化的、硬件在环的框架来自动化这一过程，使不具备深厚硬件知识的用户也能高效部署工作负载。在英特尔Loihi 2加速器上的评估显示，该方法在稀疏多层感知器网络上相比默认启发式方法将总延迟降低了高达35%，并可扩展到多芯片系统，在未显式优化能耗的情况下实现了高达40%的能效提升。</div>
</details>
</div>
<div class="card">
<div class="title">When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates</div>
<div class="meta-line">Authors: Michele Caprio, Siu Lun Chau, Krikamol Muandet</div>
<div class="meta-line">First: 2025-10-06T12:42:32+00:00 · Latest: 2026-02-04T16:14:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04769v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.04769v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many machine learning algorithms rely on iterative updates of uncertainty representations, ranging from variational inference and expectation-maximization, to reinforcement learning, continual learning, and multi-agent learning. In the presence of imprecision and ambiguity, credal sets -- closed, convex sets of probability distributions -- have emerged as a popular framework for representing imprecise probabilistic beliefs. Under such imprecision, many learning problems in imprecise probabilistic machine learning (IPML) may be viewed as processes involving successive applications of update rules on credal sets. This naturally raises the question of whether this iterative process converges to stable fixed points -- or, more generally, under what conditions on the updating mechanism such fixed points exist, and whether they can be attained. We provide the first analysis of this problem, and illustrate our findings using Credal Bayesian Deep Learning as a concrete example. Our work demonstrates that incorporating imprecision into the learning process not only enriches the representation of uncertainty, but also reveals structural conditions under which stability emerges, thereby offering new insights into the dynamics of iterative learning under imprecision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信度集何时稳定？信度集更新的不动点定理</div>
<div class="mono" style="margin-top:8px">许多机器学习算法依赖于不确定性表示的迭代更新，涵盖变分推断、期望最大化、强化学习、持续学习与多智能体学习等领域。面对不精确性与模糊性时，信度集——即概率分布的闭凸集——已成为表示不精确概率信念的主流框架。在此类不精确性条件下，不精确概率机器学习中的多数学习问题可视为对信度集连续应用更新规则的过程。这自然引出一个核心问题：该迭代过程是否收敛至稳定不动点？更广义而言，更新机制需满足何种条件才能保证不动点存在且可达？本文首次对该问题进行了系统性分析，并以信度贝叶斯深度学习为具体案例阐释研究发现。我们的工作表明，将不精确性融入学习过程不仅能丰富不确定性表征，还能揭示稳定性涌现的结构性条件，从而为不精确条件下的迭代学习动力学提供新见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand convergence in iterative learning algorithms that handle imprecise probabilistic beliefs, such as those in variational inference and reinforcement learning, this paper investigates the conditions under which credal set updates reach stable fixed points. The method involves applying fixed-point theorems to analyze the dynamics of credal set updating mechanisms within imprecise probabilistic machine learning. The main experimental results, illustrated through Credal Bayesian Deep Learning, demonstrate that incorporating imprecision not only enhances uncertainty representation but also reveals structural conditions that ensure stability, providing new insights into iterative learning under ambiguity.</div>
<div class="mono" style="margin-top:8px">本文的动机源于需要理解处理不精确概率信念的迭代学习算法（如变分推断和强化学习）的收敛性，研究信度集更新在何种条件下能达到稳定不动点。方法上，通过应用不动点定理来分析不精确概率机器学习中信度集更新机制的动态特性。主要实验结果以信度贝叶斯深度学习为例进行说明，表明引入不精确性不仅能增强不确定性表示，还揭示了确保稳定性的结构条件，从而为模糊环境下的迭代学习动态提供了新见解。</div>
</details>
</div>
<div class="card">
<div class="title">Information Templates: A New Paradigm for Intelligent Active Feature Acquisition</div>
<div class="meta-line">Authors: Hung-Tien Huang, Dzung Dinh, Junier B. Oliva</div>
<div class="meta-line">First: 2025-08-25T18:15:11+00:00 · Latest: 2026-02-04T16:12:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.18380v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.18380v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active feature acquisition (AFA) is an instance-adaptive paradigm in which, at inference time, a policy sequentially chooses which features to acquire (at a cost) before predicting. Existing approaches either train reinforcement learning policies, which deal with a difficult MDP, or greedy policies that cannot account for the joint informativeness of features or require knowledge about the underlying data distribution. To overcome this, we propose Template-based AFA (TAFA), a non-greedy framework that learns a small library of feature templates -- sets of features that are jointly informative -- and uses this library of templates to guide the next feature acquisitions. Through identifying feature templates, the proposed framework not only significantly reduces the action space considered by the policy but also alleviates the need to estimate the underlying data distribution. Extensive experiments on synthetic and real-world datasets show that TAFA outperforms the existing state-of-the-art baselines while achieving lower overall acquisition cost and computation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信息模板：智能主动特征获取的新范式</div>
<div class="mono" style="margin-top:8px">主动特征获取（AFA）是一种实例自适应范式，在推理时，策略会按顺序选择获取哪些特征（需付出成本）再进行预测。现有方法要么训练处理复杂MDP的强化学习策略，要么采用无法考虑特征联合信息性或需已知底层数据分布的贪婪策略。为克服这些局限，我们提出基于模板的AFA（TAFA），这是一种非贪婪框架，通过学习少量特征模板库——即具有联合信息性的特征集合——并利用该模板库指导后续特征获取。通过识别特征模板，该框架不仅显著缩小策略需考虑的动作空间，还降低了对底层数据分布进行估计的需求。在合成与真实数据集上的大量实验表明，TAFA在取得更低总体获取成本与计算量的同时，性能优于现有最先进基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in active feature acquisition (AFA), where existing methods rely on complex reinforcement learning or greedy policies that struggle with joint feature informativeness or require knowledge of data distributions. To overcome this, the authors propose Template-based AFA (TAFA), a non-greedy framework that learns a small library of feature templates—sets of jointly informative features—to guide sequential feature acquisitions, thereby reducing the action space and eliminating the need to estimate underlying distributions. Experimental results on synthetic and real-world datasets demonstrate that TAFA outperforms state-of-the-art baselines by achieving lower acquisition costs and computational demands.</div>
<div class="mono" style="margin-top:8px">本文针对主动特征获取（AFA）的局限性展开研究，现有方法依赖于复杂的强化学习或贪婪策略，难以处理特征的联合信息性或需要数据分布的先验知识。为解决这些问题，作者提出了基于模板的AFA（TAFA），这是一种非贪婪框架，通过学习少量特征模板（即具有联合信息性的特征集）来指导顺序特征获取，从而减少动作空间并避免估计底层分布的需求。在合成和真实数据集上的大量实验表明，TAFA在降低总体获取成本和计算负担的同时，性能优于现有的先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design</div>
<div class="meta-line">Authors: Jaemoo Choi, Yuchen Zhu, Wei Guo, Petr Molodyk, Bo Yuan, Jinbin Bai, Yi Xin, Molei Tao, Yongxin Chen</div>
<div class="meta-line">First: 2026-02-04T15:36:42+00:00 · Latest: 2026-02-04T15:36:42+00:00</div>
<div class="meta-line">Comments: 23 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视扩散模型中强化学习的设计空间：超越损失设计的似然估计重要性</div>
<div class="mono" style="margin-top:8px">强化学习已广泛应用于扩散模型和流模型在文本到图像生成等视觉任务中。然而，由于扩散模型具有难以处理的似然函数，直接应用流行的策略梯度类方法存在障碍，这些任务仍具挑战性。现有方法主要侧重于基于已高度工程化的大语言模型目标构建新目标，并使用临时估计器处理似然，未深入探究此类估计如何影响整体算法性能。本研究通过解耦三个因素对强化学习设计空间进行系统分析：i) 策略梯度目标，ii) 似然估计器，iii) 轨迹采样方案。研究表明，采用基于证据下界的模型似然估计器（仅通过最终生成样本计算）是实现高效、稳定强化学习优化的主导因素，其影响超过特定策略梯度损失函数的选择。我们在多个奖励基准测试中使用SD 3.5 Medium验证发现，所有任务均呈现一致趋势。本方法在90 GPU小时内将GenEval分数从0.24提升至0.95，效率比FlowGRPO高4.6倍，比当前最优方法DiffusionNFT高2倍，且未出现奖励破解现象。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying reinforcement learning (RL) to diffusion models for tasks like text-to-image generation, where intractable likelihoods hinder direct use of policy-gradient methods. The authors systematically analyze the RL design space by disentangling policy-gradient objectives, likelihood estimators, and rollout sampling schemes, finding that an evidence lower bound (ELBO)-based likelihood estimator computed from the final generated sample is the key factor for effective and stable optimization, outweighing the impact of specific loss functionals. Experimental validation on multiple reward benchmarks using SD 3.5 Medium shows consistent improvements, with the method boosting the GenEval score from 0.24 to 0.95 in 90 GPU hours, achieving 4.6× and 2× greater efficiency than prior methods like FlowGRPO and DiffusionNFT, respectively, without reward hacking.</div>
<div class="mono" style="margin-top:8px">本文针对扩散模型在文本到图像生成等任务中应用强化学习（RL）的挑战展开研究，由于扩散模型的似然难以处理，直接使用策略梯度方法存在障碍。作者通过解耦策略梯度目标、似然估计器和轨迹采样方案，系统分析了RL的设计空间，发现基于证据下界（ELBO）的似然估计器（仅从最终生成样本计算）是实现高效、稳定优化的关键因素，其影响超过特定损失函数的设计。在SD 3.5 Medium模型上对多个奖励基准的实验验证显示了一致的改进趋势，该方法在90 GPU小时内将GenEval分数从0.24提升至0.95，相比FlowGRPO和DiffusionNFT等现有方法分别实现了4.6倍和2倍的效率提升，且未出现奖励黑客行为。</div>
</details>
</div>
<div class="card">
<div class="title">Mixed-Density Diffuser: Efficient Planning with Non-Uniform Temporal Resolution</div>
<div class="meta-line">Authors: Crimson Stambaugh, Rajesh P. N. Rao</div>
<div class="meta-line">First: 2025-10-27T05:45:59+00:00 · Latest: 2026-02-04T15:33:28+00:00</div>
<div class="meta-line">Comments: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN) (under review)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23026v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.23026v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies demonstrate that diffusion planners benefit from sparse-step planning over single-step planning. Training models to skip steps in their trajectories helps capture long-term dependencies without additional memory or computational cost. However, predicting excessively sparse plans degrades performance. We hypothesize this temporal density threshold is non-uniform across a planning horizon and that certain parts of a predicted trajectory should be more densely generated. We propose Mixed-Density Diffuser (MDD), a diffusion planner where the densities throughout the horizon are tunable hyperparameters. We show that MDD surpasses the SOTA Diffusion Veteran (DV) framework across the Maze2D, Franka Kitchen, and Antmaze Datasets for Deep Data-Driven Reinforcement Learning (D4RL) task domains, achieving a new SOTA on the D4RL benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合密度扩散器：非均匀时间分辨率下的高效规划</div>
<div class="mono" style="margin-top:8px">近期研究表明，扩散规划器通过稀疏步长规划优于单步规划。训练模型跳过轨迹中的步骤有助于捕捉长期依赖关系，且无需额外内存或计算成本。然而，预测过度稀疏的规划会降低性能。我们假设时间密度阈值在规划时域内是非均匀的，且预测轨迹的某些部分应更密集生成。为此，我们提出混合密度扩散器（MDD），一种扩散规划器，其整个时域的密度可作为可调超参数。实验表明，在Maze2D、Franka Kitchen和Antmaze数据集上，MDD在深度数据驱动强化学习（D4RL）任务领域超越了当前最优的扩散规划框架（DV），并在D4RL基准测试中创造了新的最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that while sparse-step diffusion planning improves efficiency by capturing long-term dependencies, overly sparse predictions can degrade performance, and the optimal temporal density is likely non-uniform across the planning horizon. To address this, the authors propose the Mixed-Density Diffuser (MDD), a method that allows tunable, non-uniform temporal resolution throughout the planning horizon as hyperparameters. Experimental results demonstrate that MDD outperforms the state-of-the-art Diffusion Veteran framework on Maze2D, Franka Kitchen, and Antmaze datasets, achieving new state-of-the-art performance on the D4RL benchmark.</div>
<div class="mono" style="margin-top:8px">本文的动机源于观察到，虽然稀疏步长的扩散规划能通过捕捉长期依赖关系来提高效率，但过度稀疏的预测会降低性能，且最优的时间密度在规划时域内可能非均匀。为此，作者提出了混合密度扩散器（MDD），该方法允许将规划时域内非均匀的时间分辨率作为可调超参数。实验结果表明，在Maze2D、Franka Kitchen和Antmaze数据集上，MDD超越了当前最先进的Diffusion Veteran框架，并在D4RL基准测试中取得了新的最优性能。</div>
</details>
</div>
<div class="card">
<div class="title">WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Zelai Xu, Zhexuan Xu, Ruize Zhang, Chunyang Zhu, Shi Yu, Weilin Liu, Quanlu Zhang, Wenbo Ding, Chao Yu, Yu Wang</div>
<div class="meta-line">First: 2026-02-04T15:05:12+00:00 · Latest: 2026-02-04T15:05:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04634v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WideSeek-R1：通过多智能体强化学习探索面向广泛信息检索的宽度扩展</div>
<div class="mono" style="margin-top:8px">近期大语言模型（LLM）的发展主要聚焦于深度扩展，即单个智能体通过多轮推理和工具使用解决长程问题。然而，随着任务范围扩大，关键瓶颈从个体能力转向组织协同能力。本研究探索多智能体系统的宽度扩展这一互补维度，以应对广泛信息检索需求。现有多智能体系统常依赖人工编排的工作流和轮转式交互，难以实现高效并行化。为弥补这一不足，我们提出WideSeek-R1——一种通过多智能体强化学习（MARL）训练的领航智能体-子智能体框架，旨在协同实现可扩展的编排与并行执行。该框架基于共享LLM构建隔离上下文与专用工具，在包含2万项广泛信息检索任务的精选数据集上联合优化领航智能体与并行子智能体。大量实验表明，WideSeek-R1-4B在WideSearch基准测试中达到40.0%的项目F1分数，其性能与单智能体DeepSeek-R1-671B相当。此外，随着并行子智能体数量增加，WideSeek-R1-4B展现出持续的性能提升，凸显了宽度扩展策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the shift in bottleneck from individual competence to organizational capability as tasks broaden, this work explores width scaling via multi-agent systems for broad information seeking, addressing limitations of hand-crafted workflows in existing systems. The method introduces WideSeek-R1, a lead-agent-subagent framework trained with multi-agent reinforcement learning to enable scalable orchestration and parallel execution, utilizing a shared LLM with isolated contexts and specialized tools optimized on 20k curated tasks. Experimental results show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, matching the performance of the much larger single-agent DeepSeek-R1-671B, with consistent gains as parallel subagents increase, demonstrating the effectiveness of width scaling.</div>
<div class="mono" style="margin-top:8px">本研究动机在于，随着任务范围扩大，关键瓶颈从个体能力转向组织能力，因此探索通过多智能体系统进行宽度扩展以应对广泛信息搜索，解决现有系统依赖手工流程和低效并行的问题。方法上提出了WideSeek-R1，这是一个通过多智能体强化学习训练的主从智能体框架，利用共享大语言模型与隔离上下文及专用工具，在2万个广泛信息搜索任务数据集上优化，实现可扩展编排和并行执行。主要实验结果表明，WideSeek-R1-4B在WideSearch基准测试中达到40.0%的项目F1分数，与单智能体DeepSeek-R1-671B性能相当，且随着并行子智能体数量增加性能持续提升，凸显了宽度扩展的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning</div>
<div class="meta-line">Authors: Doyeon Lee, Eunyi Lyou, Hyunsoo Cho, Sookyung Kim, Joonseok Lee, Jaemoo Choi</div>
<div class="meta-line">First: 2026-02-04T14:51:04+00:00 · Latest: 2026-02-04T14:51:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04620v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04620v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QUATRO：面向大语言模型微调的查询自适应信赖域策略优化</div>
<div class="mono" style="margin-top:8px">基于GRPO风格强化学习的大语言模型微调算法近期备受关注。然而，这类算法依赖启发式信赖域近似，可能引发脆弱的优化行为——全局重要性比率裁剪与分组归一化无法有效调控超出裁剪范围的样本重要性比率。本文提出查询自适应信赖域策略优化（QUATRO），通过原理性优化直接实施信赖域约束。该方法构建了清晰可解释的目标函数，实现对策略更新的显式控制，并通过精确信赖域公式内生的稳定项，实现熵值可控的稳定优化。在多样化数学推理基准上的实证表明，QUATRO在策略陈旧性增强和激进学习率设置下仍能保持训练稳定性，并在整个训练过程中维持良好受控的熵值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for QUATRO stems from the limitations of existing GRPO-style RL fine-tuning methods for LLMs, which rely on heuristic trust-region approximations that can cause brittle optimization due to inadequate regulation of samples with out-of-range importance ratios. The method introduces Query-Adaptive Trust-Region Policy Optimization, a principled approach that directly enforces trust-region constraints through an exact optimization formulation, yielding an interpretable objective with explicit control over policy updates and intrinsic stabilizer terms for entropy-controlled optimization. Experimental results on diverse mathematical reasoning benchmarks demonstrate that QUATRO achieves stable training even under increased policy staleness and aggressive learning rates, while maintaining well-controlled entropy throughout the process.</div>
<div class="mono" style="margin-top:8px">QUATRO的提出动机源于现有基于GRPO风格强化学习的大语言模型微调方法的局限性，这些方法依赖启发式的信任区域近似，可能因对重要性比率超出范围的样本调节不足而导致脆弱的优化行为。该方法引入了查询自适应信任区域策略优化，通过精确的优化公式直接强制执行信任区域约束，产生了一个可解释的目标，能够显式控制策略更新，并利用内在稳定项实现熵控优化。在多样化的数学推理基准测试中的实验结果表明，QUATRO即使在策略陈旧性增加和激进学习率下也能实现稳定训练，并在整个过程中保持良好控制的熵。</div>
</details>
</div>
<div class="card">
<div class="title">Stochastic Decision Horizons for Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Nikola Milosevic, Leonard Franz, Daniel Haeufle, Georg Martius, Nico Scherf, Pavel Kolev</div>
<div class="meta-line">First: 2026-02-04T14:27:16+00:00 · Latest: 2026-02-04T14:27:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04599v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04599v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束强化学习中的随机决策视野</div>
<div class="mono" style="margin-top:8px">约束马尔可夫决策过程（CMDPs）为强化学习中处理安全及其他辅助目标等约束提供了原则性模型。常用的附加成本约束与对偶变量方法常阻碍离策略可扩展性。我们提出一种基于随机决策视野的“控制即推断”框架，其中约束违反会衰减奖励贡献，并通过状态-动作依赖的延续缩短有效规划视野。这产生了生存加权目标，保持与离策略演员-评论家学习的重放兼容性。我们提出两种违反语义：吸收终止与虚拟终止，它们共享相同的生存加权回报，但形成不同的优化结构，分别导向SAC/MPO风格策略改进。实验在标准基准上展示了改进的样本效率与更优的回报-违反权衡。此外，采用虚拟终止的MPO（VT-MPO）在我们的高维肌肉骨骼Hyfydy设置中实现了有效扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the scalability limitations of dual-variable methods in constrained reinforcement learning by proposing a Control as Inference framework with stochastic decision horizons. The method models constraint violations as factors that attenuate rewards and shorten the planning horizon through state-action-dependent continuation, leading to survival-weighted objectives compatible with off-policy actor-critic algorithms. Experimental results on benchmarks show improved sample efficiency and better trade-offs between return and constraint violation, with the virtual termination variant (VT-MPO) scaling effectively to high-dimensional musculoskeletal control tasks.</div>
<div class="mono" style="margin-top:8px">本文针对约束强化学习中基于对偶变量的方法在离策略可扩展性上的不足，提出了一种基于随机决策视野的“推断即控制”框架。该方法将约束违反建模为衰减奖励并通过状态-动作依赖的延续缩短规划视野，从而产生与离策略演员-评论家算法兼容的生存加权目标。在标准基准测试中，实验结果表明了更高的样本效率和更优的回报-违反权衡，其中虚拟终止变体（VT-MPO）能有效扩展到高维肌肉骨骼控制任务中。</div>
</details>
</div>
<div class="card">
<div class="title">Dual Mind World Model Inspired Network Digital Twin for Access Scheduling</div>
<div class="meta-line">Authors: Hrishikesh Dutta, Roberto Minerva, Noel Crespi</div>
<div class="meta-line">First: 2026-02-04T13:53:55+00:00 · Latest: 2026-02-04T13:53:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04566v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04566v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Emerging networked systems such as industrial IoT and real-time cyber-physical infrastructures demand intelligent scheduling strategies capable of adapting to dynamic traffic, deadlines, and interference constraints. In this work, we present a novel Digital Twin-enabled scheduling framework inspired by Dual Mind World Model (DMWM) architecture, for learning-informed and imagination-driven network control. Unlike conventional rule-based or purely data-driven policies, the proposed DMWM combines short-horizon predictive planning with symbolic model-based rollout, enabling the scheduler to anticipate future network states and adjust transmission decisions accordingly. We implement the framework in a configurable simulation testbed and benchmark its performance against traditional heuristics and reinforcement learning baselines under varied traffic conditions. Our results show that DMWM achieves superior performance in bursty, interference-limited, and deadline-sensitive environments, while maintaining interpretability and sample efficiency. The proposed design bridges the gap between network-level reasoning and low-overhead learning, marking a step toward scalable and adaptive NDT-based network optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于双心智世界模型启发的网络数字孪生接入调度方法</div>
<div class="mono" style="margin-top:8px">工业物联网和实时信息物理系统等新兴网络系统需要能够适应动态流量、截止时间和干扰约束的智能调度策略。本研究提出一种受双心智世界模型架构启发的新型数字孪生调度框架，实现基于学习认知与想象驱动的网络控制。与传统基于规则或纯数据驱动的策略不同，所提出的DMWM架构将短期预测规划与基于符号模型的推演相结合，使调度器能够预判未来网络状态并相应调整传输决策。我们在可配置仿真平台上实现该框架，并在多样化流量条件下与传统启发式算法及强化学习基线进行性能对比。实验结果表明，DMWM在突发流量、干扰受限和截止时间敏感的环境中均表现出优越性能，同时保持可解释性和样本效率。该设计弥合了网络级推理与低开销学习之间的鸿沟，标志着向可扩展、自适应的基于网络数字孪生的优化方向迈出重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for intelligent scheduling in dynamic networked systems like industrial IoT, this paper introduces a Digital Twin-enabled framework inspired by the Dual Mind World Model (DMWM) architecture. The method combines short-horizon predictive planning with symbolic model-based rollouts, allowing the scheduler to anticipate future network states and adapt transmission decisions. Experimental results from a simulation testbed demonstrate that DMWM outperforms traditional heuristics and reinforcement learning baselines in bursty, interference-limited, and deadline-sensitive environments, while offering improved interpretability and sample efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对工业物联网等动态网络系统对智能调度的需求，提出了一种受双心智世界模型启发的数字孪生调度框架。该方法结合了短时域预测规划与基于符号模型的推演，使调度器能够预测未来网络状态并调整传输决策。在可配置仿真测试平台上的实验结果表明，该框架在突发、干扰受限和时限敏感的环境中优于传统启发式方法和强化学习基线，同时保持了良好的可解释性和样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions</div>
<div class="meta-line">Authors: Qianyi Xu, Gousia Habib, Feng Wu, Yanrui Du, Zhihui Chen, Swapnil Mishra, Dilruk Perera, Mengling Feng</div>
<div class="meta-line">First: 2026-02-03T09:30:32+00:00 · Latest: 2026-02-04T13:20:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03305v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03305v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>medR：基于三驱动势函数的临床离线强化学习奖励工程</div>
<div class="mono" style="margin-top:8px">强化学习为优化动态治疗方案提供了强大框架，但临床强化学习始终受限于奖励工程的核心瓶颈：如何在复杂稀疏的离线环境中定义能安全有效指导策略学习的奖励信号。现有方法多依赖人工启发式规则，难以泛化至不同病理场景。为此，我们提出一种基于大语言模型的自动化流程，用于离线奖励设计与验证。我们采用包含生存度、置信度与胜任度三个核心组件的势函数构建奖励函数，并引入量化指标在部署前严格评估筛选最优奖励结构。通过融合大语言模型驱动的领域知识，本框架实现了针对特定疾病的奖励函数自动化设计，同时显著提升了最终策略的性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reward engineering in clinical offline reinforcement learning, where manually designed heuristics often fail to generalize across diseases. The authors propose an automated pipeline that uses Large Language Models to design and verify reward functions, formulating them as tri-drive potential functions based on survival, confidence, and competence components. Experimental results demonstrate that this LLM-integrated framework successfully automates reward design for specific pathologies and significantly improves the performance of learned treatment policies compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对临床离线强化学习中奖励函数设计的挑战，即手动启发式方法难以在不同疾病间泛化的问题，提出了一种自动化流程。该方法利用大语言模型来自动设计和验证奖励函数，将其构建为基于生存、置信度和能力三个核心组件的三驱动势函数。实验结果表明，这一集成大语言模型的框架能够针对特定疾病自动化奖励设计，并显著提升了所学治疗策略的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization</div>
<div class="meta-line">Authors: Haoran Yin, Shuaiqun Pan, Zhao Wei, Jian Cheng Wong, Yew-Soon Ong, Anna V. Kononova, Thomas Bäck, Niki van Stein</div>
<div class="meta-line">First: 2026-02-04T13:18:45+00:00 · Latest: 2026-02-04T13:18:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04529v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04529v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advent of Large Language Models (LLMs) has opened new frontiers in automated algorithm design, giving rise to numerous powerful methods. However, these approaches retain critical limitations: they require extensive evaluation of the target problem to guide the search process, making them impractical for real-world optimization tasks, where each evaluation consumes substantial computational resources. This research proposes an innovative and efficient framework that decouples algorithm discovery from high-cost evaluation. Our core innovation lies in combining a Genetic Programming (GP) function generator with an LLM-driven evolutionary algorithm designer. The evolutionary direction of the GP-based function generator is guided by the similarity between the landscape characteristics of generated proxy functions and those of real-world problems, ensuring that algorithms discovered via proxy functions exhibit comparable performance on real-world problems. Our method enables deep exploration of the algorithmic space before final validation while avoiding costly real-world evaluations. We validated the framework&#x27;s efficacy across multiple real-world problems, demonstrating its ability to discover high-performance algorithms while substantially reducing expensive evaluations. This approach shows a path to apply LLM-based automated algorithm design to computationally intensive real-world optimization challenges.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向问题特征的自动化算法设计：一种适用于现实世界优化的高效框架</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的出现为自动化算法设计开辟了新前沿，催生了众多强大方法。然而，这些方法仍存在关键局限：需要大量评估目标问题以指导搜索过程，导致其难以应用于现实优化任务——因为每次评估都会消耗大量计算资源。本研究提出一种创新高效框架，将算法发现与高成本评估解耦。核心创新在于将遗传编程（GP）函数生成器与LLM驱动的进化算法设计器相结合。GP函数生成器的进化方向由生成代理函数与实际问题在特征空间上的相似性引导，确保通过代理函数发现的算法在现实问题上具有可比性能。该方法支持在最终验证前深度探索算法空间，同时避免昂贵的现实评估。我们在多个现实问题上验证了框架的有效性，证明其能以大幅减少昂贵评估为代价，发现高性能算法。该研究为将基于LLM的自动化算法设计应用于计算密集型现实优化挑战提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high computational cost of evaluating real-world optimization problems in existing LLM-based automated algorithm design methods, this paper proposes an efficient framework that decouples algorithm discovery from expensive evaluations. The method combines a Genetic Programming function generator with an LLM-driven evolutionary algorithm designer, guiding the evolution by matching the landscape characteristics of generated proxy functions to those of real-world problems, thereby enabling deep algorithmic exploration without costly real-world evaluations during search. Experimental results across multiple real-world problems demonstrate that the framework successfully discovers high-performance algorithms while substantially reducing the number of expensive evaluations required.</div>
<div class="mono" style="margin-top:8px">针对现有基于大语言模型的自动化算法设计方法在真实世界优化任务中评估成本高昂的问题，本研究提出了一种高效的框架，将算法发现与昂贵评估解耦。该方法结合了遗传编程函数生成器与大语言模型驱动的进化算法设计器，通过使生成的代理函数的景观特征与真实问题相匹配来指导进化，从而在搜索过程中无需进行高成本的真实评估即可深入探索算法空间。在多个真实世界问题上的实验结果表明，该框架能够有效发现高性能算法，同时显著减少了所需的昂贵评估次数。</div>
</details>
</div>
<div class="card">
<div class="title">Learning the Value Systems of Agents with Preference-based and Inverse Reinforcement Learning</div>
<div class="meta-line">Authors: Andrés Holgado-Sánchez, Holger Billhardt, Alberto Fernández, Sascha Ossowski</div>
<div class="meta-line">Venue: Holgado-Sánchez, A., Billhardt, H., Fernández, A., Ossowski, S. Learning the value systems of agents with preference-based and inverse reinforcement learning. Autonomous Agents Multi-Agent Systems 40, 4 (2026)</div>
<div class="meta-line">First: 2026-02-04T13:07:15+00:00 · Latest: 2026-02-04T13:07:15+00:00</div>
<div class="meta-line">Comments: 42 pages, 5 figures. Published in Journal of Autonomous Agents and Multi-Agent Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04518v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04518v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agreement Technologies refer to open computer systems in which autonomous software agents interact with one another, typically on behalf of humans, in order to come to mutually acceptable agreements. With the advance of AI systems in recent years, it has become apparent that such agreements, in order to be acceptable to the involved parties, must remain aligned with ethical principles and moral values. However, this is notoriously difficult to ensure, especially as different human users (and their software agents) may hold different value systems, i.e. they may differently weigh the importance of individual moral values. Furthermore, it is often hard to specify the precise meaning of a value in a particular context in a computational manner. Methods to estimate value systems based on human-engineered specifications, e.g. based on value surveys, are limited in scale due to the need for intense human moderation. In this article, we propose a novel method to automatically \emph{learn} value systems from observations and human demonstrations. In particular, we propose a formal model of the \emph{value system learning} problem, its instantiation to sequential decision-making domains based on multi-objective Markov decision processes, as well as tailored preference-based and inverse reinforcement learning algorithms to infer value grounding functions and value systems. The approach is illustrated and evaluated by two simulated use cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏好与逆强化学习的智能体价值系统学习</div>
<div class="mono" style="margin-top:8px">协议技术指开放计算机系统中自主软件代理（通常代表人类）通过交互达成相互可接受协议。近年来人工智能系统的发展表明，此类协议需符合伦理原则与道德价值观才能被各方接受。然而，确保这一点尤为困难，因为不同用户（及其软件代理）可能持有不同价值体系，即对个体道德价值的重要性赋予不同权重。此外，以计算方式在特定情境中精确定义价值内涵常具挑战性。基于人工设计规范（如价值调查）的评估方法因依赖高强度人工干预而规模受限。本文提出一种通过观察与人类示范自动学习价值系统的新方法，具体包括：构建价值系统学习问题的形式化模型，将其实例化为基于多目标马尔可夫决策过程的序列决策领域，并设计定制化的偏好学习与逆强化学习算法以推断价值基础函数及价值系统。该方案通过两个模拟用例进行阐释与验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for autonomous agents in agreement technologies to align with diverse and often hard-to-specify human ethical values, this paper proposes a method to automatically learn value systems from observations and demonstrations. The approach formalizes the value system learning problem for sequential decision-making using multi-objective Markov decision processes and develops tailored algorithms based on preference-based and inverse reinforcement learning to infer value grounding functions and systems. The main experimental results demonstrate the method&#x27;s effectiveness through evaluation in two simulated use cases, showing its capability to learn value systems without intensive human moderation.</div>
<div class="mono" style="margin-top:8px">本文的研究动机在于，为使协议技术中的自主智能体能够符合多样且难以计算化定义的人类伦理价值观，需要自动从观察和演示中学习价值体系。方法上，该研究基于多目标马尔可夫决策过程，形式化了序列决策中的价值体系学习问题，并开发了基于偏好学习和逆强化学习的定制算法，以推断价值基础函数和体系。主要实验结果通过两个模拟用例进行评估，证明了该方法无需密集人工干预即可有效学习价值体系。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning</div>
<div class="meta-line">Authors: Shubham Vaishnav, Praveen Kumar Donta, Sindri Magnússon</div>
<div class="meta-line">First: 2025-05-01T23:34:35+00:00 · Latest: 2026-02-04T11:51:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.00918v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.00918v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">IoT networks often face conflicting routing goals such as maximizing packet delivery, minimizing delay, and conserving limited battery energy. These priorities can also change dynamically: for example, an emergency alert requires high reliability, while routine monitoring prioritizes energy efficiency to prolong network lifetime. Existing works, including many deep reinforcement learning approaches, are typically centralized and assume static objectives, making them slow to adapt when preferences shift. We propose a dynamic and fully distributed multi-objective Q-learning routing algorithm that learns multiple per-preference Q-tables in parallel and introduces a novel greedy interpolation policy to act near-optimally for unseen preferences without retraining or central coordination. A theoretical analysis further shows that the optimal value function is Lipschitz-continuous in the preference parameter, ensuring that the proposed greedy interpolation policy yields provably near-optimal behavior. Simulations show that our approach adapts in real time to shifting priorities and achieves up to 80-90\% lower energy consumption and more than 2-5x higher cumulative rewards and packet delivery compared to six baseline protocols, under dynamic and distributed settings. Sensitivity analysis across varying preference window lengths confirms that the proposed DPQ framework consistently achieves higher composite reward than all baseline methods, demonstrating robustness to changes in operating conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多目标Q学习的物联网网络动态分布式路由</div>
<div class="mono" style="margin-top:8px">物联网网络常面临路由目标冲突，如最大化数据包投递率、最小化延迟与节约有限电池能量。这些优先级亦会动态变化：例如紧急警报需高可靠性，而常规监测优先考虑能效以延长网络寿命。现有研究（包括多数深度强化学习方法）通常采用集中式架构并假设静态目标，当偏好变化时适应缓慢。本文提出一种动态全分布式多目标Q学习路由算法，可并行学习多个按偏好划分的Q表，并引入新型贪婪插值策略，无需重新训练或中央协调即可对未见偏好实现近似最优决策。理论分析进一步证明最优值函数在偏好参数上满足利普希茨连续性，确保所提贪婪插值策略具有可证明的近似最优性。仿真表明：在动态分布式环境下，本方法能实时适应优先级变化，与六种基线协议相比，能耗降低80-90%，累计奖励与数据包投递率提升2-5倍以上。针对不同偏好窗口长度的敏感性分析证实，所提DPQ框架始终获得高于所有基线方法的复合奖励，展现出对运行条件变化的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of managing conflicting and dynamically changing routing objectives in IoT networks, such as balancing packet delivery, delay, and energy efficiency. The authors propose a fully distributed multi-objective Q-learning algorithm that learns multiple per-preference Q-tables in parallel and employs a novel greedy interpolation policy to enable near-optimal routing for unseen preferences without retraining or central coordination. Experimental simulations demonstrate that the method adapts in real time to shifting priorities, achieving up to 80-90% lower energy consumption and 2-5x higher cumulative rewards and packet delivery compared to six baseline protocols, with sensitivity analysis confirming its robust performance across varying conditions.</div>
<div class="mono" style="margin-top:8px">本文针对物联网网络中路由目标（如最大化数据包投递、最小化延迟和节约电池能量）相互冲突且动态变化的问题展开研究。作者提出了一种完全分布式的多目标Q学习算法，该算法并行学习多个基于偏好的Q表，并引入一种新颖的贪婪插值策略，无需重新训练或中央协调即可对未见偏好实现近似最优的路由决策。实验仿真表明，该方法能实时适应优先级变化，与六种基线协议相比，能耗降低达80-90%，累积奖励和数据包投递率提高2-5倍，敏感性分析进一步验证了其在各种操作条件下均具有稳健的优异性能。</div>
</details>
</div>
<div class="card">
<div class="title">Mixture of Masters: Sparse Chess Language Models with Player Routing</div>
<div class="meta-line">Authors: Giacomo Frisoni, Lorenzo Molfetta, Davide Freddi, Gianluca Moro</div>
<div class="meta-line">First: 2026-02-04T11:18:43+00:00 · Latest: 2026-02-04T11:18:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04447v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04447v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal&#x27;s offensive vocation or Petrosian&#x27;s defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>专家混合：基于玩家路由的稀疏国际象棋语言模型</div>
<div class="mono" style="margin-top:8px">现代国际象棋语言模型是基于数千名高段位棋手数百万对局训练的稠密Transformer网络。然而，这类单一网络易陷入模式平均化行为，导致风格边界模糊且罕见有效策略被抑制。为应对同质化问题，我们提出专家混合模型——首个采用模拟世界级特级大师的小型GPT专家模块的国际象棋专家混合架构。每个专家模块通过结合自监督学习与棋类专项奖励引导的强化学习进行训练。针对每一步棋，可事后学习的门控网络根据棋局状态选择最适配的角色通道，使模型能动态切换风格（例如塔尔式的进攻倾向或彼得罗相式的防守稳固性）。在未见过标准对局中与Stockfish引擎对比评估时，该模型在生成多样性、可控性与可解释性方面，均优于基于聚合数据训练的稠密独立专家网络及主流GPT基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the homogenization issue in dense chess language models, which often average out distinct playing styles and suppress rare strategies. To counteract this, the authors propose Mixture-of-Masters (MoM), a sparse mixture-of-experts model where small GPT experts emulate specific grandmasters, trained via self-supervised and reinforcement learning with chess-specific rewards. A learnable gating network dynamically selects the most appropriate expert based on the game state, enabling style switching. Experimental results show that MoM outperforms both dense expert networks and aggregated GPT baselines against Stockfish on unseen games, while maintaining generation variety, control, and interpretability.</div>
<div class="mono" style="margin-top:8px">本文针对密集国际象棋语言模型因风格平均化而模糊边界、抑制罕见有效策略的问题，提出了专家混合模型Mixture-of-Masters（MoM）。该方法采用稀疏专家混合架构，其中小型GPT专家模拟特定国际象棋大师，通过自监督学习和基于象棋特定奖励的强化学习进行训练；一个可学习的门控网络根据棋局状态动态选择最合适的专家，实现风格切换。实验结果表明，在未见过的标准棋局中，MoM相比密集专家网络和基于聚合数据训练的GPT基线，在对抗Stockfish时表现更优，同时确保了生成的多样性、可控性和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL</div>
<div class="meta-line">Authors: Lunjun Zhang, Jimmy Ba</div>
<div class="meta-line">First: 2026-02-04T10:50:17+00:00 · Latest: 2026-02-04T10:50:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04417v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04417v1">PDF</a> · <a href="https://github.com/LunjunZhang/ema-pg">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to acquire increasingly complex reasoning and agentic behaviors. In this work, we propose two simple techniques to improve policy gradient algorithms for LLMs. First, we replace the fixed anchor policy during RL with an Exponential Moving Average (EMA), similar to a target network in deep Q-learning. Second, we introduce Top-k KL estimator, which allows for flexible interpolation between exact KL and sampled KL. We derive the stability conditions for using EMA anchor; moreover, we show that our Top-k KL estimator yields both unbiased KL values and unbiased gradients at any k, while bringing the benefits of exact KL. When combined with GRPO, the two techniques (EMA-PG) lead to a significant performance boost. On math reasoning, it allows R1-distilled Qwen-1.5B to reach 53.9% on OlympiadBench compared to 50.8% by GRPO. On agentic RL domains, with Qwen-3B base, EMA-PG improves GRPO by an average of 33.3% across 7 datasets of Q&amp;A with search engines, including 29.7% $\rightarrow$ 44.1% on HotpotQA, 27.4% $\rightarrow$ 40.1% on 2WikiMultiHopQA. Overall, we show that EMA-PG is a simple, principled, and powerful approach to scaling RL for LLMs. Code: https://github.com/LunjunZhang/ema-pg</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EMA策略梯度：通过EMA锚点和Top-k KL驯服大语言模型的强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）使大语言模型（LLMs）能够习得日益复杂的推理和智能体行为。本研究提出两种改进LLM策略梯度算法的简单技术：首先，用指数移动平均（EMA）替代RL中的固定锚点策略，类似于深度Q学习中的目标网络；其次，引入Top-KL估计器，可在精确KL与采样KL之间灵活插值。我们推导了使用EMA锚点的稳定性条件，并证明Top-KL估计器在任何k值下均能提供无偏KL值与无偏梯度，同时保留精确KL的优势。结合GRPO时，这两项技术（EMA-PG）带来显著性能提升：在数学推理任务中，R1蒸馏的Qwen-1.5B在OlympiadBench达到53.9%（GRPO为50.8%）；在智能体RL领域，基于Qwen-3B的EMA-PG在7个搜索引擎问答数据集上平均提升GRPO性能33.3%，其中HotpotQA从29.7%提升至44.1%，2WikiMultiHopQA从27.4%提升至40.1%。研究表明，EMA-PG是一种简洁、原理清晰且高效的LLM强化学习扩展方法。代码：https://github.com/LunjunZhang/ema-pg</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to improve the stability and performance of policy gradient reinforcement learning for large language models. The method introduces two techniques: replacing the fixed anchor policy with an exponential moving average (EMA) anchor for stability, akin to target networks in Q-learning, and proposing a Top-k KL estimator that flexibly interpolates between exact and sampled KL divergence while providing unbiased estimates. The main experimental results show that when combined with GRPO, this EMA-PG approach significantly boosts performance, enabling a distilled Qwen-1.5B model to achieve 53.9% on OlympiadBench math reasoning (up from 50.8%) and improving a Qwen-3B base model by an average of 33.3% across seven agentic search-based Q&amp;A datasets, such as increasing HotpotQA from 29.7% to 44.1%.</div>
<div class="mono" style="margin-top:8px">该研究的动机是提升大语言模型策略梯度强化学习的稳定性和性能。方法上提出了两项技术：一是用指数移动平均（EMA）锚点替代固定锚点策略以增强稳定性，类似于Q学习中的目标网络；二是引入Top-k KL估计器，可在精确KL和采样KL之间灵活插值，同时提供无偏估计。主要实验结果表明，与GRPO结合后，这种EMA-PG方法显著提升了性能，使蒸馏后的Qwen-1.5B模型在OlympiadBench数学推理上达到53.9%（原为50.8%），并在七个基于搜索的智能问答数据集中，将Qwen-3B基础模型的平均性能提高了33.3%，例如HotpotQA从29.7%提升至44.1%。</div>
</details>
</div>
<div class="card">
<div class="title">From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training</div>
<div class="meta-line">Authors: Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, Nikolay Malkin</div>
<div class="meta-line">First: 2025-01-10T18:18:25+00:00 · Latest: 2026-02-04T10:39:50+00:00</div>
<div class="meta-line">Comments: TMLR final version; code: https://github.com/GFNOrg/gfn-diffusion/tree/stagger</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.06148v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.06148v3">PDF</a> · <a href="https://github.com/GFNOrg/gfn-diffusion/tree/stagger">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of training neural stochastic differential equations, or diffusion models, to sample from a Boltzmann distribution without access to target samples. Existing methods for training such models enforce time-reversal of the generative and noising processes, using either differentiable simulation or off-policy reinforcement learning (RL). We prove equivalences between families of objectives in the limit of infinitesimal discretization steps, linking entropic RL methods (GFlowNets) with continuous-time objects (partial differential equations and path space measures). We further show that an appropriate choice of coarse time discretization during training allows greatly improved sample efficiency and the use of time-local objectives, achieving competitive performance on standard sampling benchmarks with reduced computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从离散时间策略到连续时间扩散采样器：渐近等价性与加速训练</div>
<div class="mono" style="margin-top:8px">本研究探讨了在无法获取目标样本的情况下，训练神经随机微分方程（即扩散模型）以从玻尔兹曼分布采样的方法。现有训练方法通过可微分模拟或离策略强化学习，强制生成过程与加噪过程的时间可逆性。我们证明了在无穷小离散化步长极限下，不同目标函数族之间的等价关系，从而将熵正则化强化学习方法（GFlowNets）与连续时间对象（偏微分方程及路径空间测度）联系起来。进一步研究表明，训练过程中采用适当的粗粒度时间离散化策略，可显著提升样本效率并实现时间局部化目标函数，在标准采样基准测试中以更低计算成本获得具有竞争力的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training neural stochastic differential equations (diffusion models) to sample from Boltzmann distributions without requiring target samples, motivated by the need for efficient generative modeling. The method establishes theoretical equivalences between discrete-time policy training objectives, such as those used in entropic reinforcement learning (GFlowNets), and continuous-time diffusion processes in the limit of infinitesimal discretization steps, linking them to partial differential equations and path space measures. Experimentally, the authors demonstrate that employing a coarse time discretization during training enhances sample efficiency and enables the use of time-local objectives, leading to competitive performance on standard sampling benchmarks while reducing computational costs.</div>
<div class="mono" style="margin-top:8px">本文研究如何训练神经随机微分方程（扩散模型）从玻尔兹曼分布中采样，而无需目标样本，旨在提升生成建模的效率。方法上，证明了在无限小离散化步长极限下，离散时间策略训练目标（如熵强化学习中的GFlowNets）与连续时间扩散过程之间的等价性，将其与偏微分方程和路径空间测度联系起来。实验结果表明，训练时采用粗粒度时间离散化能显著提高样本效率并允许使用时间局部目标，在标准采样基准上实现了有竞争力的性能，同时降低了计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning</div>
<div class="meta-line">Authors: Wei Duan, Jie Lu, En Yu, Junyu Xuan</div>
<div class="meta-line">First: 2025-12-11T23:56:43+00:00 · Latest: 2026-02-04T09:59:26+00:00</div>
<div class="meta-line">Comments: Accepted by AAMAS 2026 (oral) with appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11179v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11179v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph-based multi-agent reinforcement learning (MARL) enables coordinated behavior under partial observability by modeling agents as nodes and communication links as edges. While recent methods excel at learning sparse coordination graphs-determining who communicates with whom-they do not address what information should be transmitted under hard bandwidth constraints. We study this bandwidth-limited regime and show that naive dimensionality reduction consistently degrades coordination performance. Hard bandwidth constraints force selective encoding, but deterministic projections lack mechanisms to control how compression occurs. We introduce Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors regularized via KL divergence to an uninformative prior. BVME&#x27;s variational framework provides principled, tunable control over compression strength through interpretable hyperparameters, directly constraining the representations used for decision-making. Across SMACv1, SMACv2, and MPE benchmarks, BVME achieves comparable or superior performance while using 67--83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination. Ablations reveal U-shaped sensitivity to bandwidth, with BVME excelling at extreme ratios while adding minimal overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向协作多智能体强化学习的带宽约束变分消息编码</div>
<div class="mono" style="margin-top:8px">基于图的多智能体强化学习（MARL）通过将智能体建模为节点、通信链路建模为边，实现了部分可观测条件下的协同行为。现有方法虽擅长学习稀疏协调图（确定通信对象），却未解决硬带宽约束下应传输何种信息的问题。本研究聚焦带宽受限场景，证明简单的降维方法会持续损害协调性能。硬带宽约束迫使选择性编码，但确定性投影缺乏控制压缩过程的机制。我们提出带宽约束变分消息编码（BVME）——一种轻量级模块，将消息视为从学习到的高斯后验中采样的结果，并通过KL散度以无信息先验进行正则化。BVME的变分框架通过可解释超参数提供对压缩强度的可调控制，直接约束决策表征。在SMACv1、SMACv2和MPE基准测试中，BVME在使用67-83%更少消息维度的同时，取得相当或更优性能，在消息质量对协调至关重要的稀疏图上提升尤为显著。消融实验揭示对带宽的U型敏感性，BVME在极端压缩比下表现卓越且仅引入极小开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of what information to transmit in graph-based multi-agent reinforcement learning under hard bandwidth constraints, where existing methods focus on learning sparse communication graphs but neglect efficient message encoding. The authors propose Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors, regularized via KL divergence to an uninformative prior, providing principled control over compression strength through interpretable hyperparameters. Experimental results on SMACv1, SMACv2, and MPE benchmarks show that BVME achieves comparable or superior performance while using 67–83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination, and ablations reveal U-shaped sensitivity to bandwidth with BVME excelling at extreme compression ratios.</div>
<div class="mono" style="margin-top:8px">本文研究了在图结构多智能体强化学习中，面对严格带宽约束时应传输何种信息的问题，现有方法侧重于学习稀疏通信图但忽略了高效的消息编码。作者提出了带宽约束变分消息编码（BVME），这是一个轻量级模块，将消息视为从学习到的高斯后验中采样的样本，并通过KL散度正则化至无信息先验，从而通过可解释的超参数提供对压缩强度的原则性控制。在SMACv1、SMACv2和MPE基准测试上的实验结果表明，BVME在使用67–83%更少消息维度的同时，实现了相当或更优的性能，在消息质量对协调至关重要的稀疏图上增益最为明显，消融研究揭示了性能对带宽的U形敏感性，BVME在极端压缩比下表现优异。</div>
</details>
</div>
<div class="card">
<div class="title">Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents</div>
<div class="meta-line">Authors: Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang</div>
<div class="meta-line">First: 2025-09-27T01:36:46+00:00 · Latest: 2026-02-04T09:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23040v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23040v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the &quot;memorize while reading&quot; methods. While this approach scales efficiently, it suffers from pruning of latent evidence, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, which integrates the mechanism of memory retrieval into the memory update process, enabling the agent to selectively callback historical memories for non-linear reasoning. To further strengthen training, we propose a multi-level reward design, which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support complex multi-hop reasoning. Extensive experiments demonstrate that ReMemR1 significantly outperforms state-of-the-art baselines on long-context question answering while incurring negligible computational overhead, validating its ability to trade marginal cost for robust long-context reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>回望以向前推理：面向长上下文LLM智能体的可重访记忆机制</div>
<div class="mono" style="margin-top:8px">大语言模型在长上下文问答任务中面临关键证据分散于数百万标记的挑战。现有研究通过线性文档扫描动态更新记忆缓冲区（即“边读边记”方法），虽具扩展效率，但存在潜在证据剪枝、覆盖写入导致信息丢失、以及稀疏强化学习信号等问题。为此，我们提出ReMemR1，将记忆检索机制融入记忆更新过程，使智能体能够选择性回调历史记忆进行非线性推理。为进一步强化训练，我们设计了结合最终答案奖励与密集步骤级信号的多层次奖励机制，以引导有效记忆使用。这些贡献共同缓解了信息衰减、改进了监督机制，并支持复杂多跳推理。大量实验表明，ReMemR1在长上下文问答任务上显著优于现有基线方法，且计算开销可忽略，验证了其以边际成本换取稳健长上下文推理的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of long-context question answering in large language models, where crucial evidence is often scattered across extensive documents, by proposing ReMemR1. The method enhances traditional linear memory buffers by integrating memory retrieval into the update process, allowing agents to revisit historical memories for non-linear reasoning, and employs a multi-level reward system that combines final-answer rewards with dense step-level signals to improve training. Experimental results show that ReMemR1 significantly outperforms existing baselines in long-context QA tasks with minimal computational overhead, effectively mitigating information loss and supporting complex multi-hop reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在长上下文问答中关键证据分散的挑战，提出了ReMemR1方法。该方法通过将记忆检索机制整合到记忆更新过程中，改进了传统的线性记忆缓冲区，使智能体能够回溯历史记忆进行非线性推理，并采用结合最终答案奖励与密集步骤级信号的多层次奖励设计来强化训练。实验结果表明，ReMemR1在长上下文问答任务中显著优于现有基线方法，且计算开销可忽略不计，有效缓解了信息退化并支持复杂的多跳推理。</div>
</details>
</div>
<div class="card">
<div class="title">Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics</div>
<div class="meta-line">Authors: Ruizhe Zhong, Jiesong Lian, Xiaoyue Mi, Zixiang Zhou, Yuan Zhou, Qinglin Lu, Junchi Yan</div>
<div class="meta-line">First: 2026-02-04T08:59:57+00:00 · Latest: 2026-02-04T08:59:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04928v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04928v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While online Reinforcement Learning has emerged as a crucial technique for aligning flow matching models with human preferences, current approaches are hindered by inefficient exploration during training rollouts. Relying on undirected stochasticity and sparse outcome rewards, these methods struggle to discover high-reward samples, resulting in data-inefficient and slow optimization. To address these limitations, we propose Euphonium, a novel framework that steers generation via process reward gradient guided dynamics. Our key insight is to formulate the sampling process as a theoretically principled Stochastic Differential Equation that explicitly incorporates the gradient of a Process Reward Model into the flow drift. This design enables dense, step-by-step steering toward high-reward regions, advancing beyond the unguided exploration in prior works, and theoretically encompasses existing sampling methods (e.g., Flow-GRPO, DanceGRPO) as special cases. We further derive a distillation objective that internalizes the guidance signal into the flow network, eliminating inference-time dependency on the reward model. We instantiate this framework with a Dual-Reward Group Relative Policy Optimization algorithm, combining latent process rewards for efficient credit assignment with pixel-level outcome rewards for final visual fidelity. Experiments on text-to-video generation show that Euphonium achieves better alignment compared to existing methods while accelerating training convergence by 1.66x.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Euphonium：通过过程奖励梯度引导随机动力学实现视频流匹配的导向控制</div>
<div class="mono" style="margin-top:8px">尽管在线强化学习已成为对齐流匹配模型与人类偏好的关键技术，但当前方法受限于训练过程中的低效探索。这些方法依赖无导向的随机性和稀疏的结果奖励，难以发现高奖励样本，导致数据效率低下和优化缓慢。为突破这些限制，我们提出Euphonium框架，通过过程奖励梯度引导的动力学机制实现生成导向。核心洞见是将采样过程构建为理论完备的随机微分方程，将过程奖励模型的梯度显式融入流漂移项。该设计能实现朝向高奖励区域的密集逐步导向，超越了先前工作中的无引导探索，并在理论上将现有采样方法（如Flow-GRPO、DanceGRPO）涵盖为特例。我们进一步推导出将引导信号内化至流网络的蒸馏目标，消除推理时对奖励模型的依赖。通过双奖励组相对策略优化算法实例化该框架，结合潜在过程奖励（实现高效信用分配）与像素级结果奖励（保障最终视觉保真度）。文本到视频生成实验表明，Euphonium在实现更好对齐效果的同时，将训练收敛速度提升1.66倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the inefficiency of current online reinforcement learning methods for aligning flow matching models with human preferences, which suffer from undirected exploration and sparse rewards. The proposed method, Euphonium, formulates the sampling process as a Stochastic Differential Equation that incorporates the gradient of a Process Reward Model into the flow drift, enabling dense, step-by-step steering toward high-reward regions. Experimental results on text-to-video generation demonstrate that Euphonium achieves better alignment and accelerates training convergence by 1.66x compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">本文的动机在于当前用于对齐流匹配模型与人类偏好的在线强化学习方法效率低下，存在探索无方向和奖励稀疏的问题。提出的方法Euphonium将采样过程构建为随机微分方程，将过程奖励模型的梯度纳入流漂移中，实现了向高奖励区域的密集、逐步引导。在文本到视频生成任务上的实验结果表明，Euphonium相比现有方法实现了更好的对齐效果，并将训练收敛速度提升了1.66倍。</div>
</details>
</div>
<div class="card">
<div class="title">Extending RLVR to Open-Ended Tasks via Verifiable Multiple-Choice Reformulation</div>
<div class="meta-line">Authors: Mengyu Zhang, Siyu Ding, Weichong Yin, Yu Sun, Hua Wu</div>
<div class="meta-line">First: 2025-11-04T10:45:52+00:00 · Latest: 2026-02-04T07:52:47+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02463v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.02463v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards(RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs). However, its success has thus far been largely confined to the mathematical and programming domains with clear and automatically checkable outcomes. Reinforcement learning on open-ended tasks (e.g., creative writing and subjective Q&amp;A) continues to rely on reward models due to the absence of verifiable solutions. This raises a key question: how can we extend RLVR to strengthen reasoning in open-ended tasks regardless of the absence of the unambiguous ground truth? To overcome this challenge, we introduce Verifiable Multiple-Choice Reformulation for Reinforcement Learning from Verifiable Rewards (VMR-RLVR), a novel training strategy that restructures open-ended data into verifiable multiple-choice formats, enabling effective training even in the absence of explicit ground truth. Experimental results on multiple benchmarks validate the effectiveness of our method in improving LLM performance on open-ended tasks. Notably, across seven open-ended benchmarks, our VMR-RLVR training delivers an average gain of 3.29 points over the RL with reward model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过可验证多选重构将RLVR扩展至开放式任务</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）在提升大语言模型（LLMs）推理能力方面展现出巨大潜力，但其成功目前主要局限于具有明确且可自动验证结果的数学与编程领域。由于缺乏可验证的解决方案，开放式任务（如创意写作和主观问答）的强化学习仍需依赖奖励模型。这引出一个关键问题：如何在缺乏明确标准答案的情况下扩展RLVR以增强开放式任务的推理能力？为应对这一挑战，我们提出基于可验证奖励的强化学习之可验证多选重构（VMR-RLVR），这是一种将开放式数据重构为可验证多选格式的新型训练策略，即使在缺乏显式标准答案的情况下也能实现有效训练。多个基准测试的实验结果验证了本方法在提升LLM开放式任务性能方面的有效性。值得注意的是，在七个开放式任务基准上，我们的VMR-RLVR训练相比基于奖励模型的强化学习平均提升3.29分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitation of Reinforcement Learning with Verifiable Rewards (RLVR), which has been effective in domains like mathematics with clear answers but struggles with open-ended tasks such as creative writing due to the lack of unambiguous ground truth for verification. To address this, the authors propose VMR-RLVR, a method that reformulates open-ended tasks into a verifiable multiple-choice format, allowing the application of RLVR even without explicit correct solutions. Experimental results across seven open-ended benchmarks demonstrate the method&#x27;s effectiveness, showing an average performance improvement of 3.29 points over traditional reinforcement learning using reward models.</div>
<div class="mono" style="margin-top:8px">本文的动机在于可验证奖励的强化学习（RLVR）存在局限性，该方法在数学等有明确答案的领域表现良好，但由于缺乏明确的真实答案进行验证，难以应用于创意写作等开放式任务。为解决这一问题，作者提出了VMR-RLVR方法，将开放式任务重新构建为可验证的多选题形式，从而使得RLVR能够在没有显式正确答案的情况下有效应用。在七个开放式基准测试上的实验结果表明，该方法显著提升了大型语言模型的性能，相比使用奖励模型的传统强化学习平均提高了3.29分。</div>
</details>
</div>
<div class="card">
<div class="title">Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Yansong Ning, Jun Fang, Naiqiang Tan, Hao Liu</div>
<div class="meta-line">First: 2026-02-04T07:26:23+00:00 · Latest: 2026-02-04T07:26:23+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04284v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04284v1">PDF</a> · <a href="https://github.com/usail-hkust/Agent-Omit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent&#x27;s adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Agent-Omit：通过智能体强化学习训练高效LLM智能体以实现自适应思维与观察省略</div>
<div class="mono" style="margin-top:8px">在多轮智能体-环境交互中管理智能体的思维与观察是提升智能体效率的新兴策略。然而，现有研究将整个交互轨迹同等对待，忽视了不同轮次中思维必要性与观察效用的动态变化。为此，我们首先定量研究了思维与观察如何影响智能体的效能与效率。基于研究发现，我们提出了Agent-Omit——一个统一的训练框架，使LLM智能体能够自适应地省略冗余思维与观察。具体而言，我们首先合成少量冷启动数据（包含单轮与多轮省略场景），对智能体进行省略行为微调。进一步，我们引入一种省略感知的智能体强化学习方法，结合双重采样机制和定制化省略奖励，以激励智能体的自适应省略能力。理论上，我们证明了省略策略的偏差受KL散度上界约束。在五个智能体基准测试上的实验结果表明，所构建的Agent-Omit-8B模型性能可与七种前沿LLM智能体相媲美，并在效能-效率权衡上优于七种高效LLM智能体方法。代码与数据已开源：https://github.com/usail-hkust/Agent-Omit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to improve the efficiency of LLM agents in multi-turn interactions, where existing methods treat all turns equally without considering the varying necessity of internal thoughts and environmental observations. The proposed method, Agent-Omit, is a training framework that enables agents to adaptively omit redundant thoughts and observations through a two-step approach: first fine-tuning on synthesized cold-start data for omission behaviors, then applying an omit-aware agentic reinforcement learning technique with a dual sampling mechanism and a tailored reward to enhance adaptive omission. Experimental results on five benchmarks demonstrate that the 8B-parameter Agent-Omit model achieves performance comparable to frontier LLM agents while attaining the best effectiveness-efficiency trade-off compared to seven efficient agent methods.</div>
<div class="mono" style="margin-top:8px">本文的动机在于提升大语言模型智能体在多轮交互中的效率，现有方法通常平等对待所有轮次，忽略了内部思考和环境观察在不同轮次中的必要性差异。提出的方法Agent-Omit是一个训练框架，通过两步使智能体自适应地省略冗余思考和观察：首先在合成的冷启动数据上进行微调以学习省略行为，然后采用一种省略感知的智能体强化学习方法，结合双重采样机制和定制奖励来增强自适应省略能力。在五个基准测试上的实验结果表明，8B参数的Agent-Omit模型性能可与前沿大语言模型智能体相媲美，并且相比七种高效智能体方法实现了最佳的效果-效率平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms</div>
<div class="meta-line">Authors: Priyankkumar Dhrangdhariya, Soumyadipta Maiti, Venkataramana Runkana</div>
<div class="meta-line">First: 2026-02-04T07:15:33+00:00 · Latest: 2026-02-04T07:15:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04277v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04277v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于有限元建模、机器学习、粒子群优化与贝叶斯优化算法的非充气乘用车轮胎多目标设计优化</div>
<div class="mono" style="margin-top:8px">非充气轮胎为充气轮胎提供了一种有前景的替代方案，但其不连续的辐条结构在刚度调节、耐久性和高速振动方面存在挑战。本研究提出了一种集成生成式设计与机器学习驱动的框架，用于优化乘用车UPTIS型辐条几何结构。通过高阶多项式参数化上下辐条轮廓，并基于PCHIP几何变异生成约250种设计。采用KRR预测刚度、XGBoost预测耐久性与振动的机器学习模型实现了高精度预测，降低了对计算密集型有限元仿真的依赖。结合粒子群优化与贝叶斯优化算法进一步实现了性能的全面优化。最终设计相比基准方案展现出53%的刚度可调范围、最高50%的耐久性提升及43%的振动降低。粒子群优化实现了快速定向收敛，而贝叶斯优化有效探索了多目标权衡关系。该框架为系统化开发高性能新一代UPTIS辐条结构提供了有效途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the need to address challenges in stiffness tuning, durability, and vibration in non-pneumatic tires (UPTIS type) due to their discontinuous spoke structures. The method integrates generative design and machine learning, parameterizing spoke profiles with high-order polynomials to create around 250 designs, and employs KRR and XGBoost models to predict stiffness, durability, and vibration, reducing reliance on FEM simulations. Optimization via Particle Swarm Optimization and Bayesian Optimization achieved significant performance improvements: the final designs show 53% stiffness tunability, up to 50% durability enhancement, and a 43% reduction in vibration compared to a baseline, with PSO enabling fast convergence and Bayesian Optimization effectively handling multi-objective trade-offs.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决非充气轮胎（UPTIS类型）因其不连续辐条结构带来的刚度调节、耐久性和高速振动挑战。方法结合了生成式设计与机器学习，通过高阶多项式参数化辐条轮廓生成约250种设计，并利用KRR和XGBoost模型预测刚度、耐久性和振动，减少了对有限元模拟的依赖。采用粒子群优化和贝叶斯优化进行性能优化，最终设计相比基线实现了53%的刚度可调性、高达50%的耐久性提升和43%的振动降低，其中粒子群优化提供了快速收敛，而贝叶斯优化有效探索了多目标权衡关系。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
