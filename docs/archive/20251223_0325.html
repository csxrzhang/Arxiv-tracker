<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-23 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251223_0325</div>
    <div class="row"><div class="card">
<div class="title">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</div>
<div class="meta-line">Authors: Ananta R. Bhattarai, Helge Rhodin</div>
<div class="meta-line">First: 2025-12-19T18:59:56+00:00 · Latest: 2025-12-19T18:59:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17908v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17908v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>任何内容的重新深度估计：基于自监督重新照明的测试时深度细化</div>
<div class="mono" style="margin-top:8px">单目深度估计仍然具有挑战性，因为最近的基础模型，如深度一切V2（DA-V2），难以处理与训练分布相差甚远的现实世界图像。我们提出了重新深度一切（Re-Depth Anything），这是一种测试时的自监督框架，通过将DA-V2与大规模2D扩散模型的强大先验融合，来弥合这一领域差距。我们的方法通过重新照明预测的深度图并在输入上进行增强，直接在输入图像上进行无标签细化。这种重新合成方法通过利用形状从 shading（SfS）线索，在新的生成上下文中利用分数蒸馏采样（SDS）来替代经典的光度重建。为了防止优化崩溃，我们的框架采用了一种有针对性的优化策略：而不是直接优化深度或微调整个模型，我们冻结编码器，只更新中间嵌入，并微调解码器。在多种基准测试中，重新深度一切在深度准确性和现实性方面相对于DA-V2取得了显著的提升，展示了通过增强几何推理来实现自监督的新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution.</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Robustness of Vision in Open Foundation Models</div>
<div class="meta-line">Authors: Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</div>
<div class="meta-line">Venue: IEEE Access, 2025</div>
<div class="meta-line">First: 2025-12-19T18:59:16+00:00 · Latest: 2025-12-19T18:59:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17902v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17902v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta&#x27;s Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta&#x27;s Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放基础模型中的视觉对抗鲁棒性</div>
<div class="mono" style="margin-top:8px">随着深度学习的发展，理解AI系统如何识别物体变得越来越困难。因此，攻击者可以尝试通过添加未见过的元素修改图像，从而混淆AI的识别。本文研究了LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2的对抗鲁棒性。这些模型在未目标PGD（投影梯度下降）攻击下对视觉输入进行了测试，并在VQA v2数据集子集上进行了经验评估。然后使用标准的VQA准确率度量这些对抗攻击的结果。然后将这些评估与LLaVA和Llama 3.2 Vision的准确率下降（准确率下降）进行了比较。一个关键发现是，尽管在该设置下Llama 3.2 Vision的基线准确率较低，但在较高扰动水平下，其性能下降幅度小于LLaVA。总体而言，这些发现证实了视觉模态是降低当代开放权重VLMs性能的有效攻击向量，包括Meta的Llama 3.2 Vision。此外，它们还表明对抗鲁棒性并不一定直接与标准基准性能相关，可能受到底层架构和训练因素的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects.</div>
<div class="mono" style="margin-top:8px">该研究通过在VQA v2数据集上对LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2进行无目标PGD攻击测试，评估其对抗鲁棒性。结果显示，尽管Llama 3.2 Vision的基线准确率较低，但在攻击下的性能下降比LLaVA更小，尤其是在较高扰动水平时。这表明对抗鲁棒性与标准基准性能之间没有直接关联，并可能受到架构和训练因素的影响。</div>
</details>
</div>
<div class="card">
<div class="title">When Reasoning Meets Its Laws</div>
<div class="meta-line">Authors: Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin, Paul Pu Liang, Huan Zhang</div>
<div class="meta-line">First: 2025-12-19T18:59:11+00:00 · Latest: 2025-12-19T18:59:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17901v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17901v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lore-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当推理遇到其定律</div>
<div class="mono" style="margin-top:8px">尽管大型推理模型（LRMs）表现出色，但其推理行为往往违背直觉，导致推理能力不足。为理论化所需的推理行为，本文提出了推理定律（LoRe），这是一种统一框架，用于表征LRMs中的内在推理模式。我们首先提出了计算定律，假设推理计算应与问题复杂性成线性关系。除了计算，我们还通过补充准确性定律扩展了LoRe。由于在实践中难以量化问题复杂性，我们通过定律的两个属性单调性和组合性来检验这些假设。因此，我们引入了LoRe-Bench，这是一个基准，系统地测量大型推理模型的这两个可操作属性。评估结果显示，大多数推理模型表现出合理的单调性但缺乏组合性。为此，我们开发了一种有效的微调方法，以强制执行计算定律的组合性。广泛的实证研究表明，更好地遵守计算定律在多个基准上一致地提高了推理性能，并揭示了属性和定律之间的协同效应。项目页面：https://lore-project.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy</div>
<div class="meta-line">Authors: Aditya Gahlawat, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan, Nikolai Matni, Aaron D. Ames, Gioele Zardini, Alberto Speranzon</div>
<div class="meta-line">First: 2025-12-19T18:58:11+00:00 · Latest: 2025-12-19T18:58:11+00:00</div>
<div class="meta-line">Comments: 18 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17899v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17899v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分布鲁棒模仿学习：具有可认证自主性的分层控制架构</div>
<div class="mono" style="margin-top:8px">模仿学习（IL）通过学习专家演示来实现自主行为。尽管与强化学习等替代方法相比，IL更具样本效率，但它对由分布偏移引起的累积误差敏感。当使用基于IL的反馈律在系统上运行时，存在两种显著的分布偏移来源：由策略误差引起的分布偏移和由外生干扰和内生模型误差（由于缺乏学习）引起的分布偏移。我们之前开发的方法，泰勒级数模仿学习（TaSIL）和$\mathcal{L}_1$分布鲁棒自适应控制（\ellonedrac），以互补的方式解决了分布偏移的挑战。虽然TaSIL提供了对由策略误差引起的分布偏移的鲁棒性，\ellonedrac则提供了对由 aleatoric 和 epistemic 不确定性引起的分布偏移的鲁棒性。为了实现对学习和/或不确定动力学系统的可认证IL，我们提出了分布鲁棒模仿策略（DRIP）架构，这是一种分层控制架构（LCA），将TaSIL和\ellonedrac集成在一起。通过精心设计每个层的输入和输出要求，我们展示了如何保证整个控制管道的证书。我们的解决方案铺平了将基于学习的组件，如感知，与通过所提出的LCA方法提出的可认证模型驱动决策相结合，以设计完全可认证自主性的道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations.</div>
</details>
</div>
<div class="card">
<div class="title">Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally</div>
<div class="meta-line">Authors: Robin Schimmelpfennig, Mark Díaz, Vinodkumar Prabhakaran, Aida Davani</div>
<div class="meta-line">First: 2025-12-19T18:57:53+00:00 · Latest: 2025-12-19T18:57:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17898v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17898v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI&#x27;s human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user&#x27;s perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人类AI设计增加拟人性但对全球用户参与度和信任度产生分歧结果</div>
<div class="mono" style="margin-top:8px">全球超过十亿用户与日益精妙地模仿人类特质的AI系统互动。这一转变引发了关于拟人性——将人类特征赋予合成代理——及其可能引发的不适当信任或情感依赖的紧迫辩论。然而，关于更类人类的AI设计与其对全球用户群体的互动和信任的后续影响之间的因果关系尚未在现实的人机交互中得到测试。现有的安全框架继续依赖于源自西方人群的理论假设，忽视了全球AI用户的多样性。在此，我们通过两项大规模跨国实验（N=3,500）跨越10个不同国家，涉及与AI系统的实时和开放式互动，来解决这些缺口。我们发现，当评估AI的类人类程度时，用户关注的更多是政策中经常提及的理论方面（如感知或意识）之外的应用性互动提示，如对话流畅度或理解用户视角。我们还实验证明，类人类设计可以因果性地增加用户的拟人性；然而，我们并未发现类人类设计会普遍增加用户的参与度和信任度，如先前的理论工作所建议的那样。相反，类人类程度与行为结果之间的联系因文化而异：某些设计选择在某些群体（如巴西）中会引发对AI系统的自报信任，但在其他群体（如日本）中可能会产生相反的结果。我们的发现挑战了关于类人类AI设计固有风险的现有叙述。相反，我们识别出一个复杂的文化中介的人机互动景观，这要求我们在AI治理中超越一刀切的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits.</div>
<div class="mono" style="margin-top:8px">该研究探讨了类人AI设计对全球不同群体的拟人化、参与度和信任度的影响。通过涉及10个国家3,500名参与者的两项大规模跨国实验，研究发现用户在评估AI时更注重实际的交互线索而非理论方面。虽然类人设计会增加拟人化，但它并不会在所有情况下都提升参与度和信任度，文化差异显著影响这些结果。该研究挑战了类人AI固有风险的观点，并强调了在AI治理中需要采取文化敏感的方法。</div>
</details>
</div>
<div class="card">
<div class="title">RadarGen: Automotive Radar Point Cloud Generation from Cameras</div>
<div class="meta-line">Authors: Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</div>
<div class="meta-line">First: 2025-12-19T18:57:33+00:00 · Latest: 2025-12-19T18:57:33+00:00</div>
<div class="meta-line">Comments: Project page: https://radargen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17897v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17897v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://radargen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird&#x27;s-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RadarGen：从多视角摄像头图像生成汽车雷达点云</div>
<div class="mono" style="margin-top:8px">我们提出了RadarGen，这是一种从多视角摄像头图像合成逼真汽车雷达点云的扩散模型。RadarGen 通过将雷达测量以鸟瞰图形式表示，结合空间结构、雷达截面（RCS）和多普勒属性，将高效的图像-潜在扩散模型适应到雷达领域。一个轻量级的恢复步骤从生成的地图中重建点云。为了更好地与视觉场景对齐，RadarGen 结合了从预训练基础模型中提取的BEV对齐的深度、语义和运动线索，这些线索指导随机生成过程向物理上合理的雷达模式发展。基于图像的条件使得该方法原则上与现有的视觉数据集和模拟框架兼容，为多模态生成模拟提供了一个可扩展的方向。在大规模驾驶数据上的评估表明，RadarGen 捕捉了特征雷达测量分布，并减少了与在真实数据上训练的感知模型之间的差距，标志着向跨传感模态的统一生成模拟迈进了一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery.</div>
<div class="mono" style="margin-top:8px">RadarGen 是一种从多视角相机图像生成真实汽车雷达点云的扩散模型。它以鸟瞰图形式表示雷达测量值，并结合深度、语义和运动线索来引导生成过程。评估结果显示，RadarGen 能够捕捉到典型的雷达测量分布，并减少与基于真实数据训练的感知模型之间的差距，展示了其在跨传感模态生成模拟方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring the Effect of Basis Rotation on NQS Performance</div>
<div class="meta-line">Authors: Sven Benjamin Kožić, Vinko Zlatić, Fabio Franchini, Salvatore Marco Giampaolo</div>
<div class="meta-line">First: 2025-12-19T18:49:33+00:00 · Latest: 2025-12-19T18:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17893v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17893v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood. We use a fully solvable one-dimensional Ising model to show that local basis rotations leave the loss landscape unchanged while relocating the exact wavefunction in parameter space, effectively increasing its geometric distance from typical initializations. By sweeping a rotation angle, we compute quantum Fisher information and Fubini-Study distances to quantify how the rotated wavefunction moves within the loss landscape. Shallow architectures (with focus on Restricted Boltzmann Machines (RBMs)) trained with quantum natural gradient are more likely to fall into saddle-point regions depending on the rotation angle: they achieve low energy error but fail to reproduce correct coefficient distributions. In the ferromagnetic case, near-degenerate eigenstates create high-curvature barriers that trap optimization at intermediate fidelities. We introduce a framework based on an analytically solvable rotated Ising model to investigate how relocating the target wavefunction within a fixed loss landscape exposes information-geometric barriers,such as saddle points and high-curvature regions,that hinder shallow NQS optimization, underscoring the need for landscape-aware model design in variational training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索基底旋转对NQS性能的影响</div>
<div class="mono" style="margin-top:8px">神经量子态（NQS）使用神经网络表示量子多体系统的波函数，但其性能取决于基底的选择，而其背后的机制仍然知之甚少。我们使用一个完全可解的一维伊辛模型来展示局部基底旋转不会改变损失景观，但会重新定位精确波函数在参数空间的位置，从而有效地增加了其与典型初始值的几何距离。通过扫掠旋转角度，我们计算量子费舍尔信息和福比尼-斯图迪距离来量化旋转波函数在损失景观中的移动情况。浅层架构（以受限玻尔兹曼机（RBM）为例）在使用量子自然梯度训练时，根据旋转角度的不同，更有可能落入鞍点区域：它们可以实现低能量误差，但无法再现正确的系数分布。在铁磁情况下，近简并的本征态会产生高曲率障碍，将优化过程困在中间保真度。我们引入基于一个解析可解的旋转伊辛模型的框架来研究如何在固定损失景观中重新定位目标波函数以揭示信息几何障碍，如鞍点和高曲率区域，这些障碍阻碍了浅层NQS的优化，强调了在变分训练中需要具有景观意识的模型设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood.</div>
<div class="mono" style="margin-top:8px">研究通过使用一维Ising模型探讨了基底旋转对神经量子态（NQS）性能的影响。研究发现，局部基底旋转不会改变损失景观，但会增加波函数与初始状态之间的几何距离。研究发现，特别是使用量子自然梯度训练的浅层架构（如受限玻尔兹曼机），在不同旋转角度下更容易陷入鞍点区域，导致能量误差低但系数分布不正确。在铁磁情况下，近简并的本征态会创建高曲率障碍，将优化过程困在中间保真度水平，强调了在变分训练中需要考虑损失景观的设计需求。</div>
</details>
</div>
<div class="card">
<div class="title">SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars</div>
<div class="meta-line">Authors: Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo</div>
<div class="meta-line">First: 2025-07-02T17:49:52+00:00 · Latest: 2025-12-19T18:39:57+00:00</div>
<div class="meta-line">Comments: 29 pages, 8 figures, 6 tables. Accepted for publication in ApJ. Comments welcome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01939v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.01939v4">PDF</a> · <a href="https://github.com/Xiaosheng-Zhao/SpecCLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpecCLIP：为恒星光谱测量对齐和翻译</div>
<div class="mono" style="margin-top:8px">近年来，大规模语言模型（LLMs）通过大规模数据集和大规模参数化，彻底改变了自然语言理解。受此成功的启发，我们提出了SpecCLIP，这是一种基础模型框架，将LLM启发的方法扩展到恒星光谱分析。恒星光谱类似于结构化语言，编码了丰富的物理和化学信息。通过在大规模光谱数据集上训练基础模型，我们的目标是学习稳健且信息丰富的嵌入，以支持各种下游应用。作为概念验证，SpecCLIP 包括在两种光谱类型——LAMOST 低分辨率和Gaia XP 上进行预训练，然后使用 CLIP（对比语言-图像预训练）框架进行对比对齐，以适应不同仪器的光谱关联。这种对齐通过最大化嵌入和输入光谱之间的互信息来增强光谱特定信息的辅助解码器，从而实现光谱类型之间的翻译（预测）。结果是跨光谱框架，能够进行内在校准并在不同仪器之间灵活应用。我们证明，通过在中等大小的标记数据集上微调这些模型，可以提高恒星参数估计和化学丰度确定等任务的适应性。SpecCLIP 还通过与外部调查数据进行参数估计的准确性及精确度基准测试，提高了参数估计的准确性及精确度。此外，其相似性搜索和跨光谱预测能力还提供了异常检测的潜力。我们的结果表明，通过对比训练并结合光谱感知解码器的基础模型可以推进精确恒星光谱学。我们的代码 SpecCLIP 已在 https://github.com/Xiaosheng-Zhao/SpecCLIP 公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization.</div>
<div class="mono" style="margin-top:8px">SpecCLIP 是一个将 LLM 方法扩展到恒星光谱分析的框架，旨在学习适用于多种下游应用的稳健嵌入。它在 LAMOST 和 Gaia XP 光谱上进行预训练，使用 CLIP 框架进行光谱对齐，并包含辅助解码器以保留光谱特定信息和进行跨光谱预测。通过在标记数据集上进行微调，SpecCLIP 提高了恒星参数估计和化学丰度确定等任务的性能，并提高了准确性和精确度。此外，它还通过相似性搜索功能提供了异常检测的潜力。该框架展示了对比训练模型在精确恒星光谱分析中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space</div>
<div class="meta-line">Authors: Xinyue Yu, Hayden Schaeffer</div>
<div class="meta-line">First: 2025-12-19T18:36:24+00:00 · Latest: 2025-12-19T18:36:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17884v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student&#x27;s $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers&#x27;, Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations.</div>
</details>
</div>
<div class="card">
<div class="title">A Neural Surrogate-Enhanced Multi-Method Framework for Robust Wing Design Optimization</div>
<div class="meta-line">Authors: Arash Fath Lipaei, Melika Sabzikari</div>
<div class="meta-line">First: 2025-09-29T17:26:37+00:00 · Latest: 2025-12-19T18:34:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08582v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.08582v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a modular and scalable design optimization framework for the wing design process that enables faster early-phase design while ensuring aerodynamic stability. The pipeline starts with the generation of initial wing geometries and then proceeds to optimize the wing using several algorithms. Aerodynamic performance is assessed using a Vortex Lattice Method (VLM) applied to a carefully selected dataset of wing configurations. These results are employed to develop surrogate neural network models, which can predict lift and drag rapidly and accurately. The stability evaluation is implemented by setting the control surfaces and components to fixed positions in order to have realistic flight dynamics. The approach unifies and compares several optimization techniques, including Particle Swarm Optimization (PSO), Genetic Algorithms (GA), gradient-based MultiStart methods, Bayesian optimization, and Lipschitz optimization. Each method ensures constraint management via adaptive strategies and penalty functions, where the targets for lift and design feasibility are enforced. The progression of aerodynamic characteristics and geometries over the optimization iterations will be investigated in order to clarify each algorithm&#x27;s convergence characteristics and performance efficiency. Our results show improvement in aerodynamic qualities and robust stability properties, offering a mechanism for wing design at speed and precision. In the interest of reproducibility and community development, the complete implementation is publicly available at Github.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a modular and scalable design optimization framework for the wing design process that enables faster early-phase design while ensuring aerodynamic stability.</div>
<div class="mono" style="margin-top:8px">本文介绍了一种模块化和可扩展的机翼设计优化框架，该框架结合了多种算法和代理模型，以加快早期设计过程并保持气动稳定性。该过程从生成初始机翼几何形状开始，随后使用粒子群优化、遗传算法、多启动方法、贝叶斯优化和Lipschitz优化进行优化。气动性能通过旋涡格子法进行评估，代理神经网络可以高效地预测升力和阻力。通过固定控制面来评估稳定性。结果表明，气动性能和稳健稳定性得到了改进，并且该框架可供公众使用。</div>
</details>
</div>
<div class="card">
<div class="title">Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow</div>
<div class="meta-line">Authors: Herlock Rahimi</div>
<div class="meta-line">First: 2025-12-19T18:31:27+00:00 · Latest: 2025-12-19T18:31:27+00:00</div>
<div class="meta-line">Comments: 26 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17878v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17878v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Score-based diffusion models currently constitute the state of the art in continuous generative modeling.</div>
<div class="mono" style="margin-top:8px">本文通过提出基于Wasserstein-Fisher-Rao (WFR)几何的加权随机微分方程方法，解决了现有扩散模型在采样非对数凹分布时的局限性。该方法通过引入显式的修正项来增强在非凸或多重模态景观中的探索能力。关键发现包括通过加权SDE成功实现这些动态，并展示了与传统方法相比的采样效率提升。</div>
</details>
</div>
<div class="card">
<div class="title">Learning vertical coordinates via automatic differentiation of a dynamical core</div>
<div class="meta-line">Authors: Tim Whittaker, Seth Taylor, Elsa Cardoso-Bihlo, Alejandro Di Luca, Alex Bihlo</div>
<div class="meta-line">First: 2025-12-19T18:31:07+00:00 · Latest: 2025-12-19T18:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17877v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric vertical coordinate system as a learnable component within a differentiable dynamical core. We develop an end-to-end differentiable numerical solver for the two-dimensional non-hydrostatic Euler equations on an Arakawa C-grid, and introduce a NEUral Vertical Enhancement (NEUVE) terrain-following coordinate based on an integral transformed neural network that guarantees monotonicity. A key feature of our approach is the use of automatic differentiation to compute exact geometric metric terms, thereby eliminating truncation errors associated with finite-difference coordinate derivatives. By coupling simulation errors through the time integration to the parameterization, our formulation finds a grid structure optimized for both the underlying physics and numerics. Using several standard tests, we demonstrate that these learned coordinates reduce the mean squared error by a factor of 1.4 to 2 in non-linear statistical benchmarks, and eliminate spurious vertical velocity striations over steep topography.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion.</div>
<div class="mono" style="margin-top:8px">本文解决了地形跟随坐标在大气模型中导致的坐标层扭曲问题，这会引入虚假的运动。作者提出了一种在可微动力核心中学习垂直坐标系统的方法，使用自动微分来计算精确的几何度量项。他们证明了其NEUral Vertical Enhancement (NEUVE) 坐标系统在非线性统计基准测试中将均方误差降低了1.4到2倍，并消除了陡峭地形上的虚假垂直速度条带。</div>
</details>
</div>
<div class="card">
<div class="title">mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</div>
<div class="meta-line">Authors: Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</div>
<div class="meta-line">First: 2025-12-17T18:47:31+00:00 · Latest: 2025-12-19T18:30:30+00:00</div>
<div class="meta-line">Comments: Revised Introduction, Related Work, and Appendix. Additional minor notational and grammatical fixes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15692v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15692v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data.</div>
<div class="mono" style="margin-top:8px">论文针对视觉-语言-动作模型（VLAs）在机器人操作中的局限性，这些模型依赖于预训练的视觉-语言模型，并且必须从机器人轨迹中推断复杂的物理动态。为克服这一问题，作者引入了mimic-video，这是一种视频-动作模型，它将一个预训练的视频模型与一个基于其潜在表示的动作解码器配对。解码器根据视频空间动作计划的潜在表示生成低级机器人动作，充当逆动力学模型。该方法在模拟和真实世界机器人操作任务中表现出最先进的性能，样本效率提高了10倍，收敛速度提高了2倍，相比传统的VLA架构。</div>
</details>
</div>
<div class="card">
<div class="title">Visually Prompted Benchmarks Are Surprisingly Fragile</div>
<div class="meta-line">Authors: Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</div>
<div class="meta-line">First: 2025-12-19T18:26:58+00:00 · Latest: 2025-12-19T18:26:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17875v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17875v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lisadunlap.github.io/vpbench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A key challenge in evaluating VLMs is testing models&#x27; ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A key challenge in evaluating VLMs is testing models&#x27; ability to analyze visual content independently from their textual priors.</div>
<div class="mono" style="margin-top:8px">研究旨在评估视觉语言模型在独立于文本偏见的情况下分析视觉内容的能力。通过使用视觉提示基准，将问题与特定的视觉标记配对，研究发现模型对视觉提示设置中的微小变化非常敏感，例如标记的颜色。实验表明，一些较小的模型在某些条件下甚至可以超越更大的专有模型。研究结果表明，当前的基准设置可能无法准确反映模型的能力，并提出了包含16种视觉标记变体的VPBench作为更稳定的基准来解决这一问题。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Focus Memory for Language Models</div>
<div class="meta-line">Authors: Christopher Cruz</div>
<div class="meta-line">First: 2025-11-16T17:52:32+00:00 · Latest: 2025-12-19T18:24:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12712v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12712v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies. Replaying the full conversation at every turn is simple but costly, while recency-based truncation or static summarization often causes early, high-impact user constraints to drift out of effective context. As a result, models may retain text without reliably applying it when it matters.
  We present Adaptive Focus Memory (AFM), a lightweight context management system that dynamically assigns each past message one of three fidelity levels: Full, Compressed, or Placeholder, based on semantic relevance, temporal decay, and importance classification. AFM packs messages chronologically under a fixed token budget, preserving critical constraints at high fidelity while allowing low-importance context to degrade gracefully.
  We evaluate AFM on two multi-turn dialogue benchmarks designed to stress long-horizon constraint preservation: a safety-critical travel scenario involving a user with a severe peanut allergy, and a policy-critical tax compliance scenario involving an illegal evasion request. Under strict grading that requires both explicit constraint recall and appropriately conditioned generation, AFM succeeds in 83.3 percent of allergy runs where all baseline strategies fail, and preserves correct refusal behavior on the tax benchmark.
  These results demonstrate that effective dialogue memory requires more than retaining prior text. Selectively allocating fidelity across past messages enables reliable constraint preservation under bounded context growth, without modifying model weights or introducing external retrieval infrastructure. We release an open-source implementation of AFM compatible with OpenAI-style chat APIs to support reproducible research and practical deployment.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies.</div>
</details>
</div>
<div class="card">
<div class="title">Deep Gaussian Process Proximal Policy Optimization</div>
<div class="meta-line">Authors: Matthijs van der Lende, Juan Cardenas-Cartagena</div>
<div class="meta-line">First: 2025-11-22T23:13:04+00:00 · Latest: 2025-12-19T18:23:00+00:00</div>
<div class="meta-line">Comments: Withdrawn by the authors as the manuscript is not yet complete; no updated version is available at this time</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18214v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18214v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning.</div>
</details>
</div>
<div class="card">
<div class="title">Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning</div>
<div class="meta-line">Authors: Simon Frieder, Jonas Bayer, Sam Looi, Jacob Loader, Julius Berner, Katherine M. Collins, András Juhász, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Cameron Freer, Thomas Lukasiewicz, Timothy Gowers</div>
<div class="meta-line">First: 2024-12-19T18:55:17+00:00 · Latest: 2025-12-19T18:17:28+00:00</div>
<div class="meta-line">Comments: 59 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.15184v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.15184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart&#x27;s law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ``thought partners&#x27;&#x27;), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models&#x27; mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of &quot;motivated proof&quot;, introduced by G. Pólya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections.</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN</div>
<div class="meta-line">Authors: Balram Singh, Ram Prakash Sharma, Somnath Dey</div>
<div class="meta-line">First: 2025-12-19T18:11:15+00:00 · Latest: 2025-12-19T18:11:15+00:00</div>
<div class="meta-line">Comments: 27 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17864v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17864v1">PDF</a> · <a href="https://github.com/BS0111/PlantAttentionCBAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods.</div>
</details>
</div>
<div class="card">
<div class="title">Towards Human-Guided, Data-Centric LLM Co-Pilots</div>
<div class="meta-line">Authors: Evgeny Saveliev, Jiashuo Liu, Nabeel Seedat, Anders Boyd, Mihaela van der Schaar</div>
<div class="meta-line">First: 2025-01-17T17:51:22+00:00 · Latest: 2025-12-19T18:08:16+00:00</div>
<div class="meta-line">Comments: Saveliev, Liu &amp; Seedat contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.10321v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.10321v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC&#x27;s ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains -- healthcare, finance, social sciences and more -- to actively participate in driving real-world impact using ML.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools.</div>
</details>
</div>
<div class="card">
<div class="title">Low-Rank Filtering and Smoothing for Sequential Deep Learning</div>
<div class="meta-line">Authors: Joanna Sliwa, Frank Schneider, Nathanael Bosch, Agustinus Kristiadi, Philipp Hennig</div>
<div class="meta-line">First: 2024-10-09T11:54:33+00:00 · Latest: 2025-12-19T18:07:37+00:00</div>
<div class="meta-line">Comments: Revised version: improved presentation and added experiments</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.06800v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.06800v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning multiple tasks sequentially requires neural networks to balance retaining knowledge, yet being flexible enough to adapt to new tasks. Regularizing network parameters is a common approach, but it rarely incorporates prior knowledge about task relationships, and limits information flow to future tasks only. We propose a Bayesian framework that treats the network&#x27;s parameters as the state space of a nonlinear Gaussian model, unlocking two key capabilities: (1) A principled way to encode domain knowledge about task relationships, allowing, e.g., control over which layers should adapt between tasks. (2) A novel application of Bayesian smoothing, allowing task-specific models to also incorporate knowledge from models learned later. This does not require direct access to their data, which is crucial, e.g., for privacy-critical applications. These capabilities rely on efficient filtering and smoothing operations, for which we propose diagonal plus low-rank approximations of the precision matrix in the Laplace approximation (LR-LGF). Empirical results demonstrate the efficiency of LR-LGF and the benefits of the unlocked capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低秩滤波和平滑在序列深度学习中的应用</div>
<div class="mono" style="margin-top:8px">顺序学习多个任务需要神经网络在保留知识的同时，足够灵活以适应新任务。正则化网络参数是一种常见方法，但很少考虑任务间的关系先验知识，且仅限于信息流向未来任务。我们提出了一种贝叶斯框架，将网络参数视为非线性高斯模型的状态空间，解锁了两个关键能力：(1) 一种有原则的方法来编码任务间的关系知识，例如控制哪些层在任务间应适应。(2) 一种新颖的贝叶斯平滑应用，允许特定任务模型也能从后续学习的模型中获取知识。这不需要直接访问其数据，这对于隐私关键的应用至关重要。这些能力依赖于高效的滤波和平滑操作，为此我们提出了拉普拉斯近似中的对角线加低秩近似精度矩阵（LR-LGF）。实验证明了LR-LGF的效率以及解锁能力带来的益处。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Learning multiple tasks sequentially requires neural networks to balance retaining knowledge, yet being flexible enough to adapt to new tasks.</div>
</details>
</div>
<div class="card">
<div class="title">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</div>
<div class="meta-line">Authors: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</div>
<div class="meta-line">First: 2025-12-19T17:55:48+00:00 · Latest: 2025-12-19T17:55:48+00:00</div>
<div class="meta-line">Comments: 28 pages, 25 figures. The first four authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17853v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17853v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyTask：一种自动化的任务和数据生成框架，用于推进从仿真到现实的策略学习</div>
<div class="mono" style="margin-top:8px">通用型机器人学习仍然受到数据的限制：在现实世界中收集大规模、多样性和高质量的交互数据成本高昂。虽然仿真已成为扩展数据收集规模的一种有前景的方法，但相关的任务，包括仿真任务设计、任务感知场景生成、专家演示合成以及仿真到现实的转移，仍然需要大量的手工努力。我们提出了AnyTask，这是一种将大规模并行GPU仿真与基础模型相结合的自动化框架，用于设计多样化的操作任务并合成机器人数据。我们介绍了三个AnyTask代理，用于生成尽可能多任务的专家演示：1) ViPR，一种具有VLM在环并行精化的新型任务和运动规划代理；2) ViPR-Eureka，一种基于生成密集奖励和LLM引导接触采样的强化学习代理；3) ViPR-RL，一种结合规划和学习的混合方法，仅使用稀疏奖励即可生成高质量的演示。我们在生成的数据上训练行为克隆策略，在仿真中验证它们，并直接部署到真实的机器人硬件上。这些策略能够泛化到新的物体姿态，在一系列真实世界的拾取放置、抽屉打开、接触丰富的推拉以及长时操作任务中实现了44%的平均成功率。我们的项目网站为https://anytask.rai-inst.com。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world.</div>
</details>
</div>
<div class="card">
<div class="title">InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</div>
<div class="meta-line">Authors: Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad</div>
<div class="meta-line">First: 2025-12-19T17:52:43+00:00 · Latest: 2025-12-19T17:52:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17851v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfSplign: 从文本到图像扩散模型推断时的空间对齐</div>
<div class="mono" style="margin-top:8px">从文本到图像（T2I）扩散模型能够生成高质量的图像，但往往无法捕捉到文本提示中指定的空间关系。这一限制可以追溯到两个因素：训练数据中缺乏精细的空间监督以及文本嵌入无法编码空间语义。我们提出了一种无需训练的推断时方法InfSplign，通过在每个去噪步骤中通过复合损失调整噪声来改善空间对齐。所提出的损失利用从主干解码器提取的不同级别的交叉注意力图来强制执行准确的对象放置和采样期间的对象平衡。该方法轻量级、即插即用，并且与任何扩散主干兼容。我们在VISOR和T2I-CompBench上的全面评估表明，InfSplign建立了新的最先进的水平（据我们所知），在最强的现有推断时基线方法上实现了显著的性能提升，并且甚至优于基于微调的方法。代码库可在GitHub上获得。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts.</div>
</details>
</div>
<div class="card">
<div class="title">Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life</div>
<div class="meta-line">Authors: Corey M. Abramson</div>
<div class="meta-line">First: 2025-12-19T17:50:05+00:00 · Latest: 2025-12-19T17:50:05+00:00</div>
<div class="meta-line">Comments: CITE: Abramson, Corey M. (Forthcoming 2026). &quot;Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life.&quot; In Handbook of the Sociology of Aging, 2nd ed., edited by Markus H. Schafer, Dawn C. Carr, Jacqueline L. Angel, and Richard A. Settersten Jr</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17850v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17850v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将计算方法和AI整合到老年学定性研究中</div>
<div class="mono" style="margin-top:8px">本章展示了计算社会科学(CSS)工具如何扩展和扩展对老龄化的研究。传统的定性方法，如参与者观察、深入访谈和历史文件，与可扩展的数据管理、计算文本分析和开放科学实践相结合，提供了深度和背景。机器学习(ML)和自然语言处理(NLP)提供了汇总和系统索引大量定性数据、识别模式并保持与深入叙述清晰联系的资源。本章通过探讨后期生活项目的案例研究——包括DISCERN研究（一项基于团队的痴呆生活民族志）的原始数据和美国声音项目（全国代表性访谈）的二次分析——强调将CSS工具整合到更深入的定性老龄化研究对话中的用途和挑战。本章认为，此类工作具有（1）简化和增强现有工作流程，（2）扩大样本和项目规模，以及（3）以新的方式通过多方法手段解决重要问题的潜力，然后转向有助于个人和团队理解当前可能性或改进其工作流程的实践。本章指出，当前的发展并非没有风险，但可以通过拓宽而非取代定性研究的方法论基础，为老龄化和生命周期提供新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging.</div>
</details>
</div>
<div class="card">
<div class="title">The use of kinematics to quantify gait attributes and predict gait scores in dairy cows</div>
<div class="meta-line">Authors: Celia Julliot, Gabriel M. Dallago, Amir Nejati, Abdoulaye B. Diallo, Elsa Vasseur</div>
<div class="meta-line">First: 2025-12-19T17:49:37+00:00 · Latest: 2025-12-19T17:49:37+00:00</div>
<div class="meta-line">Comments: 27 pages, 3 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17847v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17847v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting walking pattern abnormalities in dairy cows early on holds the potential to reduce the occurrence of clinical lameness. This study aimed to predict gait scores in non-clinically lame dairy cows by using gait attributes based on kinematic data. Markers were placed on 20 anatomical landmarks on 12 dairy cows. The cows were walked multiple times through a corridor while recorded by six cameras, representing 69 passages. Specific gait attributes were computed from the 3D coordinates of the hoof markers. Gait was visually assessed using a 5-point numerical rating system (NRS). Due to the limited number of observations with NRS lower than 2 (n = 1) and higher than 3 (n = 6), the NRS labels were combined into three groups, representing NRS &lt;= 2, NRS = 2.5, and NRS &gt;= 3. The dataset was split into training and testing sets (70:30 ratio), stratified by the distribution of the NRS categories. Random forest (RF), gradient boosting machine (GBM), extreme gradient boosting machine (XGBM), and support vector machine (SVM) with a radial basis kernel models were trained using k-fold repeated cross-validation with hyperparameters defined using a Bayesian optimization. Accuracy, sensitivity, specificity, F1 score, and balanced accuracy were calculated to measure model performance. The GBM model performed best, achieving an overall accuracy and F1 score of 0.65 in the testing set. The findings of this study contribute to the development of an automated monitoring system for early identification of gait abnormalities, thereby enhancing the welfare and longevity of dairy cows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用运动学量化奶牛步态属性并预测步态评分</div>
<div class="mono" style="margin-top:8px">早期检测奶牛行走模式异常有可能减少临床蹄病的发生。本研究旨在通过使用基于运动学数据的步态属性预测非临床蹄病奶牛的步态评分。在12头奶牛的20个解剖标志上放置标记。奶牛多次通过走廊，由六台摄像机记录，共记录了69次通过。从蹄标记的3D坐标中计算特定的步态属性。使用5点数值评分系统（NRS）对步态进行视觉评估。由于NRS低于2（n = 1）和高于3（n = 6）的观察次数有限，NRS标签被合并为三组，分别代表NRS &lt;= 2，NRS = 2.5，和NRS &gt;= 3。数据集按NRS类别的分布比例划分为训练集和测试集。使用贝叶斯优化定义超参数，通过k折重复交叉验证训练随机森林（RF）、梯度提升机（GBM）、极端梯度提升机（XGBM）和径向基核支持向量机（SVM）模型。计算准确率、灵敏度、特异度、F1分数和平衡准确率来衡量模型性能。GBM模型表现最佳，在测试集中的总体准确率和F1分数为0.65。本研究的发现有助于开发自动监测系统，以早期识别步态异常，从而提高奶牛的福利和寿命。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Detecting walking pattern abnormalities in dairy cows early on holds the potential to reduce the occurrence of clinical lameness.</div>
</details>
</div>
<div class="card">
<div class="title">Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes</div>
<div class="meta-line">Authors: Carlos Vélez García, Miguel Cazorla, Jorge Pomares</div>
<div class="meta-line">First: 2025-12-19T17:49:13+00:00 · Latest: 2025-12-19T17:49:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17846v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17846v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.
  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.
  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\% success, strongly outperforming prior methods that peak at 68\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>规划即下降：在学习的能量景观中条件化潜轨迹合成</div>
<div class="mono" style="margin-top:8px">我们提出了规划即下降（PaD），一种基于验证的离线条件化强化学习框架。PaD 不是学习策略或显式规划器，而是学习一个条件化在完整潜轨迹上的能量函数，将低能量赋予可行且目标一致的未来。规划通过在能量景观中的梯度优化实现，训练和推理期间使用相同的计算，减少常见的脱耦建模管道中的训练-测试不匹配。PaD 通过自我监督的后见之明目标重新标记进行训练，塑造能量景观以适应规划动力学。在推理时，多个轨迹候选者在不同的时间假设下进行优化，选择平衡可行性和效率的低能量计划。我们在 OGBench 立方体操作任务上评估了 PaD。当在狭窄的专家演示上训练时，PaD 达到了最先进的 95% 成功率，显著优于峰值为 68% 的先前方法。值得注意的是，使用嘈杂的、次优数据进行训练进一步提高了成功率和计划效率，突显了验证驱动规划的好处。我们的结果表明，学习评估和优化轨迹为离线、无奖励规划提供了一种稳健的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification.</div>
</details>
</div>
<div class="card">
<div class="title">Another look at inference after prediction</div>
<div class="meta-line">Authors: Jessica Gronsbell, Jianhui Gao, Yaqi Shi, Zachary R. McCaw, David Cheng</div>
<div class="meta-line">First: 2024-11-29T18:12:50+00:00 · Latest: 2025-12-19T17:48:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.19908v6">Abs</a> · <a href="https://arxiv.org/pdf/2411.19908v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">From structural biology to epidemiology, predictions from machine learning (ML) models are increasingly used to complement costly gold-standard data to enable faster, more affordable, and scalable scientific inquiry. In response, prediction-based (PB) inference has emerged to accommodate statistical analysis using a large volume of predictions together with a small amount of gold-standard data. The goals of PB inference are two-fold: (i) to mitigate bias from errors in predictions and (ii) to improve efficiency relative to traditional inference using only the gold-standard data. While early PB inference methods focused on bias, their ability to enhance efficiency remains unclear. We revisit a popular PB inference method and show that a simple modification can be applied to guarantee improvements in efficiency beyond yielding valid inferences when the ML predictions are imperfect. The utility of this approach in leveraging prediction-based outcomes to enhance efficiency is demonstrated through extensive simulation studies and an application to the UK Biobank data. We further contextualize the problem of PB inference through historical literature from economics and statistics to highlight perspectives from classical methods in this contemporary problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对预测后的推断再审视</div>
<div class="mono" style="margin-top:8px">从结构生物学到流行病学，机器学习（ML）模型的预测越来越多地用于补充昂贵的金标准数据，以实现更快、更经济和更具扩展性的科学研究。作为回应，基于预测（PB）的推断已经出现，以适应使用大量预测数据和少量金标准数据进行统计分析。PB推断的目标有两个方面：（i）减轻预测错误带来的偏差；（ii）相对于仅使用金标准数据的传统推断提高效率。虽然早期的PB推断方法主要关注偏差，但它们提高效率的能力仍不清楚。我们重新审视了一种流行的PB推断方法，并表明可以通过简单的修改来保证在ML预测不完美时，不仅能够提供有效的推断，还能提高效率。通过广泛的模拟研究和对英国生物银行数据的应用，我们展示了这种方法在利用基于预测的结果提高效率方面的实用性。我们还通过经济学和统计学的历史文献，从经典方法的角度进一步阐述了PB推断的问题，以突出这一当代问题中的视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper revisits a popular prediction-based inference method to address the challenges of using machine learning predictions in scientific studies. The method aims to mitigate bias from prediction errors and improve efficiency compared to traditional inference methods. The authors demonstrate that a simple modification can ensure efficiency improvements beyond just providing valid inferences. This approach is validated through extensive simulations and an application to UK Biobank data, showing its utility in enhancing efficiency in scientific research.</div>
<div class="mono" style="margin-top:8px">论文重新审视了一种流行的基于预测的推断方法，以应对在科学研究中使用机器学习预测的挑战。该方法旨在减轻预测误差带来的偏差，并提高效率，相比传统的推断方法。作者证明了一个简单的修改可以确保在预测不完美时，不仅提供有效的推断，还能确保效率的提升。这种方法通过广泛的模拟研究和对英国生物银行数据的应用得到了验证，展示了其在提高科学研究效率方面的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">ShareChat: A Dataset of Chatbot Conversations in the Wild</div>
<div class="meta-line">Authors: Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le</div>
<div class="meta-line">First: 2025-12-19T17:47:53+00:00 · Latest: 2025-12-19T17:47:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17843v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17843v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset&#x27;s multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShareChat：野生聊天机器人对话数据集</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）已经演变成具有独特界面设计和功能的独立平台，现有的公开数据集将模型视为通用文本生成器，剥离了积极塑造用户交互的界面上下文。为了解决这一局限，我们介绍了ShareChat，这是一个跨平台的大规模语料库，包含142,808场对话和超过660,000个回合，数据来自五个主要平台：ChatGPT、Claude、Gemini、Perplexity和Grok上公开共享的URL。ShareChat通过保留标准日志中经常丢失的原生平台功能，如推理痕迹、源链接和代码片段，而跨越了从2023年4月到2025年10月的101种语言。此外，ShareChat提供了比先前数据集更长的上下文窗口和更深入的交互。我们通过三种代表性分析展示了数据集的多方面用途：（1）分析对话完整性以衡量用户意图的满足度；（2）评估内容生成中的引文行为；（3）进行时间分析以追踪使用模式的变化。本研究为社区提供了一个重要的及时资源，用于理解野生环境中的用户-LLM聊天机器人交互。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ShareChat is a large-scale dataset of 142,808 conversations and 660,000 turns from five major platforms, designed to preserve native platform affordances such as reasoning traces and source links. It spans 101 languages and covers a period from April 2023 to October 2025. The dataset is used to analyze conversation completeness, source citation behaviors, and temporal usage patterns, providing insights into user-LLM interactions in the wild.</div>
<div class="mono" style="margin-top:8px">ShareChat 是一个包含 142,808 次对话和 660,000 个回合的大型数据集，来自五个主要平台，旨在保留原生平台功能，如推理痕迹和源链接。它覆盖了 101 种语言，并涵盖从 2023 年 4 月到 2025 年 10 月的时期。该数据集用于分析对话完整性、内容生成中的引文行为以及时间使用模式，提供了用户-LLM 交互的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Machine Learning-Driven Predictive Resource Management in Complex Science Workflows</div>
<div class="meta-line">Authors: Tasnuva Chowdhury, Tadashi Maeno, Fatih Furkan Akman, Joseph Boudreau, Sankha Dutta, Shengyu Feng, Adolfy Hoisie, Kuan-Chieh Hsu, Raees Khan, Jaehyung Kim, Ozgur O. Kilic, Scott Klasky, Alexei Klimentov, Tatiana Korchuganova, Verena Ingrid Martinez Outschoorn, Paul Nilsson, David K. Park, Norbert Podhorszki, Yihui Ren, John Rembrandt Steele, Frédéric Suter, Sairam Sri Vatsavai, Torre Wenaus, Wei Yang, Yiming Yang, Shinjae Yoo</div>
<div class="meta-line">First: 2025-09-15T01:53:30+00:00 · Latest: 2025-12-19T17:47:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.11512v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.11512v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The collaborative efforts of large communities in science experiments, often comprising thousands of global members, reflect a monumental commitment to exploration and discovery. Recently, advanced and complex data processing has gained increasing importance in science experiments. Data processing workflows typically consist of multiple intricate steps, and the precise specification of resource requirements is crucial for each step to allocate optimal resources for effective processing. Estimating resource requirements in advance is challenging due to a wide range of analysis scenarios, varying skill levels among community members, and the continuously increasing spectrum of computing options. One practical approach to mitigate these challenges involves initially processing a subset of each step to measure precise resource utilization from actual processing profiles before completing the entire step. While this two-staged approach enables processing on optimal resources for most of the workflow, it has drawbacks such as initial inaccuracies leading to potential failures and suboptimal resource usage, along with overhead from waiting for initial processing completion, which is critical for fast-turnaround analyses. In this context, our study introduces a novel pipeline of machine learning models within a comprehensive workflow management system, the Production and Distributed Analysis (PanDA) system. These models employ advanced machine learning techniques to predict key resource requirements, overcoming challenges posed by limited upfront knowledge of characteristics at each step. Accurate forecasts of resource requirements enable informed and proactive decision-making in workflow management, enhancing the efficiency of handling diverse, complex workflows across heterogeneous resources.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于机器学习的复杂科学工作流中的预测资源管理</div>
<div class="mono" style="margin-top:8px">科学实验中大型社区的合作努力，通常包括数千名全球成员，体现了对探索和发现的巨大承诺。近年来，先进的复杂数据处理在科学实验中变得越来越重要。数据处理工作流通常由多个复杂的步骤组成，为每个步骤准确指定资源需求对于有效分配资源至关重要。由于分析场景广泛、社区成员技能水平各异以及计算选项的不断扩展，提前估计资源需求具有挑战性。一种实用的方法是首先处理每个步骤的一部分，以实际处理配置文件测量精确的资源利用率，然后再完成整个步骤。虽然这种两阶段方法可以在大多数工作流中实现最优资源处理，但它存在初始不准确可能导致潜在失败和资源使用不足的问题，以及等待初始处理完成的开销，这对于快速周转分析至关重要。在此背景下，我们的研究引入了一个综合工作流管理系统——生产与分布式分析（PanDA）系统中的新型机器学习模型管道。这些模型利用先进的机器学习技术预测关键资源需求，克服了对每个步骤特性有限的先验知识所带来的挑战。准确的资源需求预测使工作流管理中的决策更加明智和主动，提高了跨异构资源处理多样且复杂的科学工作流的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of predicting resource requirements in complex scientific workflows by developing a machine learning-driven pipeline within the PanDA system. The method involves using advanced machine learning models to estimate resource needs based on actual processing profiles, which helps in allocating optimal resources for each step. Key findings include improved accuracy and efficiency in resource management, reducing initial inaccuracies and overhead, and enabling faster turnaround times for diverse and complex workflows.</div>
<div class="mono" style="margin-top:8px">研究通过在PanDA系统中开发基于机器学习的方法来解决复杂科学工作流中资源需求预测的挑战。该方法利用先进的机器学习技术根据实际处理配置文件来估计资源需求，减少初始不准确性并提高整体工作流效率。主要发现包括更准确的资源预测和跨多种计算资源处理多样复杂任务的增强工作流管理能力。</div>
</details>
</div>
<div class="card">
<div class="title">RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation</div>
<div class="meta-line">Authors: Dongyub Jude Lee, Zhenyi Ye, Pengcheng He</div>
<div class="meta-line">First: 2025-07-29T20:35:35+00:00 · Latest: 2025-12-19T17:35:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.22219v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.22219v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-learning methods for machine translation (MT), such as Direct Preference Optimization (DPO), have shown strong gains but typically rely on large, carefully curated preference triplets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), which replaces static triplets with on-policy, actor-conditioned refinements produced by a frozen teacher. At each step, the actor samples candidate translations, the teacher performs a minimal local edit of each draft, and the actor is reinforced to close the gap using a composite reward that combines scaled negative edit distance for lexical and structural fidelity with COMET for semantic adequacy. This formulation yields a stable, model-aware learning signal without requiring explicit preference datasets. Experiments on FLORES-200 (English to German, Spanish, Chinese, Korean, and Japanese) show that RLfR consistently outperforms strong MT-SFT, DPO, and fixed-reference RL baselines, improving semantic quality and entity preservation, and also achieves superior performance under LLM-based judge evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从教师模型精炼中学习RL：逐步模仿学习在机器翻译中的应用</div>
<div class="mono" style="margin-top:8px">机器翻译（MT）的偏好学习方法，如直接偏好优化（DPO），已经显示出显著的改进，但通常依赖于大量精心策划的偏好三元组，并且往往难以在调优领域之外泛化。我们提出了教师模型精炼中的强化学习（RLfR），用在线策略、由冻结教师条件的候选翻译替换静态三元组。在每一步中，演员采样候选翻译，教师对每个草稿进行最小的局部编辑，演员通过结合缩放后的编辑距离和COMET的复合奖励来增强，以缩小差距，该复合奖励结合了词汇和结构保真度与语义适当性。这种形式化提供了一个稳定、模型感知的学习信号，而无需显式的偏好数据集。在FLORES-200（英语到德语、西班牙语、中文、韩语和日语）上的实验表明，RLfR 一致地优于强大的MT-SFT、DPO和固定参考RL基线，提高了语义质量和实体保留，并且在基于LLM的评判下也实现了更好的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve machine translation by addressing the limitations of existing preference-learning methods, which often require large, curated datasets and struggle with generalization. The method, Reinforcement Learning from Teacher-Model Refinement (RLfR), uses on-policy refinements produced by a frozen teacher model to guide the actor&#x27;s learning. The actor samples candidate translations, and the teacher makes minimal edits, which are used to reinforce the actor with a composite reward. Experiments on FLORES-200 show that RLfR outperforms strong baselines, enhancing semantic quality and entity preservation, and performs well under LLM-based evaluations.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有偏好学习方法的局限性，如需要大量和精心策划的偏好数据集，来提高机器翻译的质量。提出的RLfR方法使用由冻结教师生成的策略导向、基于策略的修正来生成稳定的训练信号。实验结果显示，RLfR在FLORES-200上优于强基线，提升了语义质量和实体保留，并在基于LLM的评估中表现良好。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation</div>
<div class="meta-line">Authors: Liam Collins, Bhuvesh Kumar, Clark Mingxuan Ju, Tong Zhao, Donald Loveland, Leonardo Neves, Neil Shah</div>
<div class="meta-line">First: 2025-12-19T17:24:12+00:00 · Latest: 2025-12-19T17:24:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17820v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method&#x27;s simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the gap in understanding the complementarity of ID and text features in sequential recommendation models. By training ID and text-based models independently and then ensembling them, the authors show that this simple method outperforms several competitive baselines, indicating that both ID and text features are necessary for state-of-the-art performance but complex fusion architectures are not required.</div>
<div class="mono" style="margin-top:8px">该研究解决了ID和文本特征在序列推荐模型中互补性的理解不足问题。提出了一种方法，该方法独立训练ID和文本模型，然后通过简单的集成策略结合它们。尽管该方法很简单，但它在多个竞争性基线模型中表现出更好的性能，表明ID和文本特征对于实现序列推荐的最先进性能都是必要的，但不需要复杂的融合架构。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-based Behaviour Driven Development for Hardware Design</div>
<div class="meta-line">Authors: Rolf Drechsler, Qian Liu</div>
<div class="meta-line">First: 2025-12-19T17:19:08+00:00 · Latest: 2025-12-19T17:19:08+00:00</div>
<div class="meta-line">Comments: 7 pages, keynote given at 2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25), December 22-24th, 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17814v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17814v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.
  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper explores the application of Large Language Models (LLMs) to automate the derivation of precise behavioral scenarios from textual specifications in Behavior Driven Development (BDD) for hardware design. This addresses the challenge of the increasing complexity of test and verification in hardware systems. The main experimental findings show that LLM-based techniques can effectively support BDD in hardware design, reducing the manual effort required for scenario derivation.</div>
<div class="mono" style="margin-top:8px">论文探讨了使用大型语言模型（LLMs）来自动化从文本规范中提取精确的行为场景，以支持硬件设计中的行为驱动开发（BDD）。这解决了随着系统规模增大，测试和验证活动复杂性增加所带来的挑战。主要发现是，基于LLM的技术可以有效地支持硬件设计中的BDD，可能减少人工努力并提高开发效率。</div>
</details>
</div>
<div class="card">
<div class="title">The Diffusion Duality</div>
<div class="meta-line">Authors: Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, Volodymyr Kuleshov</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2025-06-12T16:55:35+00:00 · Latest: 2025-12-19T17:14:07+00:00</div>
<div class="meta-line">Comments: ICML 2025. We provide the code at: https://github.com/s-sahoo/duo [v3] includes improved theory, clearer presentation, and a new future work section</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10892v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.10892v3">PDF</a> · <a href="https://github.com/s-sahoo/duo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="http://s-sahoo.github.io/duo">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/duo</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the performance of uniform-state discrete diffusion models by leveraging insights from Gaussian diffusion. The method, Duo, introduces a curriculum learning strategy and discrete consistency distillation to improve training and sampling efficiency. Duo surpasses autoregressive models in zero-shot perplexity on three out of seven benchmarks and accelerates sampling by two orders of magnitude, enabling efficient few-step generation in diffusion language models.</div>
<div class="mono" style="margin-top:8px">研究旨在通过借鉴高斯扩散的见解来提升均匀状态离散扩散模型的性能。方法Duo引入了基于高斯过程的课程学习策略和离散一致性蒸馏，以提高训练和采样的效率。Duo在七个基准中的三个上超越了自回归模型的零样本困惑度，并将采样速度提高了两个数量级，从而在扩散语言模型中实现了高效的几步生成。</div>
</details>
</div>
<div class="card">
<div class="title">Domain-Aware Quantum Circuit for QML</div>
<div class="meta-line">Authors: Gurinder Singh, Thaddeus Pellegrini, Kenneth M. Merz,</div>
<div class="meta-line">First: 2025-12-19T17:02:58+00:00 · Latest: 2025-12-19T17:02:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17800v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17800v1">PDF</a> · <a href="https://github.com/gurinder-hub/DAQC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing parameterized quantum circuits (PQCs) that are expressive, trainable, and robust to hardware noise is a central challenge for quantum machine learning (QML) on noisy intermediate-scale quantum (NISQ) devices. We present a Domain-Aware Quantum Circuit (DAQC) that leverages image priors to guide locality-preserving encoding and entanglement via non-overlapping DCT-style zigzag windows. The design employs interleaved encode-entangle-train cycles, where entanglement is applied among qubits hosting neighboring pixels, aligned to device connectivity. This staged, locality-preserving information flow expands the effective receptive field without deep global mixing, enabling efficient use of limited depth and qubits. The design concentrates representational capacity on short-range correlations, reduces long-range two-qubit operations, and encourages stable optimization, thereby mitigating depth-induced and globally entangled barren-plateau effects. We evaluate DAQC on MNIST, FashionMNIST, and PneumoniaMNIST datasets. On quantum hardware, DAQC achieves performance competitive with strong classical baselines (e.g., ResNet-18/50, DenseNet-121, EfficientNet-B0) and substantially outperforming Quantum Circuit Search (QCS) baselines. To the best of our knowledge, DAQC, which uses a quantum feature extractor with only a linear classical readout (no deep classical backbone), currently achieves the best reported performance on real quantum hardware for QML-based image classification tasks. Code and pretrained models are available at: https://github.com/gurinder-hub/DAQC.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a parameterized quantum circuit (PQC) that is expressive, trainable, and robust to hardware noise for quantum machine learning (QML) on NISQ devices. The Domain-Aware Quantum Circuit (DAQC) leverages image priors to guide encoding and entanglement, using non-overlapping DCT-style zigzag windows. DAQC achieves competitive performance on MNIST, FashionMNIST, and PneumoniaMNIST datasets, outperforming QCS baselines and matching strong classical baselines like ResNet-18/50 and DenseNet-121 on quantum hardware.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种参数化量子电路（PQC），使其在NISQ设备上的量子机器学习（QML）中具有表达性、可训练性和对硬件噪声的鲁棒性。域感知量子电路（DAQC）利用图像先验来指导编码和纠缠，使用非重叠的DCT风格的锯齿形窗口。DAQC在MNIST、FashionMNIST和PneumoniaMNIST数据集上实现了与ResNet-18/50和DenseNet-121等强大经典基线相当的性能，且优于QCS基线。</div>
</details>
</div>
<div class="card">
<div class="title">Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation</div>
<div class="meta-line">Authors: Binh Vu</div>
<div class="meta-line">First: 2025-12-19T17:01:03+00:00 · Latest: 2025-12-19T17:01:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17795v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17795v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The unprecedented proliferation of digital data presents significant challenges in access, integration, and value creation across all data-intensive sectors. Valuable information is frequently encapsulated within disparate systems, unstructured documents, and heterogeneous formats, creating silos that impede efficient utilization and collaborative decision-making. This paper introduces the Intelligent Knowledge Mining Framework (IKMF), a comprehensive conceptual model designed to bridge the critical gap between dynamic AI-driven analysis and trustworthy long-term preservation. The framework proposes a dual-stream architecture: a horizontal Mining Process that systematically transforms raw data into semantically rich, machine-actionable knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity, provenance, and computational reproducibility of these assets. By defining a blueprint for this symbiotic relationship, the paper provides a foundational model for transforming static repositories into living ecosystems that facilitate the flow of actionable intelligence from producers to consumers. This paper outlines the motivation, problem statement, and key research questions guiding the research and development of the framework, presents the underlying scientific methodology, and details its conceptual design and modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenges of accessing, integrating, and creating value from vast digital data by introducing the Intelligent Knowledge Mining Framework (IKMF). The framework proposes a dual-stream architecture: a horizontal Mining Process that converts raw data into semantically rich knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity and reproducibility of the data. Key findings include the successful design of a model that bridges AI analysis with long-term preservation, enabling the creation of living ecosystems for actionable intelligence.</div>
<div class="mono" style="margin-top:8px">论文通过引入智能知识挖掘框架（IKMF），解决了访问、集成和从大量数字数据中创造价值的挑战。该框架提出了一种双流架构：水平挖掘过程将原始数据转换为语义丰富的知识，以及并行的信任存档流，确保数据的完整性和可再现性。主要发现包括成功设计了一个模型，将AI分析与长期保存相结合，从而创建了促进行动情报流动的生态系统。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforced Generation of Combinatorial Structures: Hardness of Approximation</div>
<div class="meta-line">Authors: Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta</div>
<div class="meta-line">First: 2025-09-22T17:30:33+00:00 · Latest: 2025-12-19T16:58:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18057v6">Abs</a> · <a href="https://arxiv.org/pdf/2509.18057v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can AI based methods help us make advances in complexity theory? We provide evidence towards answering this in the affirmative, using AlphaEvolve (an LLM code mutation agent) to obtain new results in three settings:
  a) We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ vertices, and our upper bounds are obtained via analytical arguments.
  b) We obtain new inapproximability results for MAX-4-CUT and MAX-3-CUT, proving that it is NP-hard to approximate them within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of the SOTA of $16/17$ that relies on a custom PCP (rather than a reduction from ``standard&#x27;&#x27; Håstad-style PCPs).
  c) Inapproximability for the metric Traveling Salesman Problem (TSP): We show that it is NP-hard to approximate the minimum cost tour within a factor of $111/110$ using AlphaEvolve to discover a new gadget, thus improving the SOTA of $117/116$. Along the way, we provide new modular soundness and completeness arguments that can be of independent interest.
  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (sometimes requiring time exponential in the size of the construction). We used AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$ for our gadgets). Our results suggest that gadget based proofs would benefit from a pass through AI-based tools to obtain stronger results.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores whether AI methods can advance complexity theory by using AlphaEvolve, an LLM code mutation agent, to achieve new results in three areas: improving upper and lower bounds for MAX-CUT and MAX-Independent Set, obtaining new inapproximability results for MAX-4-CUT and MAX-3-CUT, and showing that it is NP-hard to approximate the metric TSP within a factor of 111/110. AlphaEvolve was also used to optimize the verification process, sometimes reducing the time by up to 10,000 times. The findings suggest that AI tools can enhance the strength of gadget-based proofs.</div>
<div class="mono" style="margin-top:8px">该研究探讨了AI方法是否能推进复杂性理论，通过使用AlphaEvolve（一个LLM代码变异代理）在三个领域取得了新成果：改进MAX-CUT和MAX-独立集的上界和下界，获得MAX-4-CUT和MAX-3-CUT的新不可近似结果，以及证明NP-hard问题在111/110内近似TSP是最优的。AlphaEvolve还被用来优化验证过程，有时可以将时间减少10,000倍。研究结果表明，AI工具可以增强基于小工具的证明的强度。</div>
</details>
</div>
<div class="card">
<div class="title">Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning</div>
<div class="meta-line">Authors: Wei Tang, Yin-Fang Yang, Weijia Zhang, Min-Ling Zhang</div>
<div class="meta-line">First: 2025-12-19T16:58:31+00:00 · Latest: 2025-12-19T16:58:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17788v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17788v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决校准问题来提高多实例部分标签学习中分类器的可靠性。作者提出了一种可校准的消歧损失（CDL），可以无缝集成到现有的MIPL和PLL框架中。CDL有两种实例化方式，分别基于候选标签集和非候选标签集的概率来校准预测。实验结果表明，CDL在基准和实际数据集上显著提高了分类准确性和校准性能。</div>
</details>
</div>
<div class="card">
<div class="title">HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs</div>
<div class="meta-line">Authors: Chang Sun, Zhiqiang Que, Thea K. Årrestad, Vladimir Loncar, Jennifer Ngadiuba, Wayne Luk, Maria Spiropulu</div>
<div class="meta-line">First: 2024-05-01T17:18:46+00:00 · Latest: 2025-12-19T16:57:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.00645v3">Abs</a> · <a href="https://arxiv.org/pdf/2405.00645v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural networks with sub-microsecond inference latency are required by many critical applications. Targeting such applications deployed on FPGAs, we present High Granularity Quantization (HGQ), a quantization-aware training framework that optimizes parameter bit-widths through gradient descent. Unlike conventional methods, HGQ determines the optimal bit-width for each parameter independently, making it suitable for hardware platforms supporting heterogeneous arbitrary precision arithmetic. In our experiments, HGQ shows superior performance compared to existing network compression methods, achieving orders of magnitude reduction in resource consumption and latency while maintaining the accuracy on several benchmark tasks. These improvements enable the deployment of complex models previously infeasible due to resource or latency constraints. HGQ is open-source and is used for developing next-generation trigger systems at the CERN ATLAS and CMS experiments for particle physics, enabling the use of advanced machine learning models for real-time data selection with sub-microsecond latency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Neural networks with sub-microsecond inference latency are required by many critical applications.</div>
</details>
</div>
<div class="card">
<div class="title">MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</div>
<div class="meta-line">Authors: Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein</div>
<div class="meta-line">First: 2025-12-19T16:45:23+00:00 · Latest: 2025-12-19T16:45:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17774v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17774v1">PDF</a> · <a href="https://www.github.com/MIC-DKFZ/nnUNet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation.</div>
</details>
</div>
<div class="card">
<div class="title">Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image</div>
<div class="meta-line">Authors: Simon Giebenhain, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Zhe Chen, Matthias Nießner</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2025-12-19T16:44:32+00:00 · Latest: 2025-12-19T16:44:32+00:00</div>
<div class="meta-line">Comments: Project website: https://simongiebenhain.github.io/Pix2NPHM/ , Video: https://www.youtube.com/watch?v=MgpEJC5p1Ts</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17773v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17773v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://simongiebenhain.github.io/Pix2NPHM/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail.</div>
</details>
</div>
<div class="card">
<div class="title">Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments</div>
<div class="meta-line">Authors: Dong Chen, Zhengqing Hu, Shixing Zhao, Yibo Guo</div>
<div class="meta-line">First: 2025-12-19T16:43:07+00:00 · Latest: 2025-12-19T16:43:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17771v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17771v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity</div>
<div class="meta-line">Authors: Tanjim Taharat Aurpa, Farzana Akter, Md. Mehedi Hasan, Shakil Ahmed, Shifat Ara Rafiq, Fatema Khan</div>
<div class="meta-line">First: 2025-12-19T16:41:16+00:00 · Latest: 2025-12-19T16:41:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17769v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17769v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bangla MedER：多BERT集成方法识别孟加拉医学实体</div>
<div class="mono" style="margin-top:8px">医学实体识别（MedER）是自然语言处理（NLP）任务中从医学语料中提取有意义实体的关键任务。如今，基于MedER的研究成果可以显著促进医疗领域自动化系统的开发，最终提高患者护理质量和结果。尽管在英语上的MedER研究已经非常广泛，但像孟加拉语这样的低资源语言仍然被忽视。我们的工作旨在弥合这一差距。对于孟加拉医学实体识别，本研究首先检查了包括BERT、DistilBERT、ELECTRA和RoBERTa在内的多种变换器模型。我们还提出了一种新颖的多BERT集成方法，该方法在所有基线模型中表现最佳，准确率为89.58%。值得注意的是，它比单层BERT模型提高了11.80%的准确率，证明了其在该任务中的有效性。MedER在低资源语言中的一个主要挑战是缺乏标注数据集。为了解决这一问题，我们开发了一个针对孟加拉MedER任务的高质量数据集。该数据集通过多个性能指标评估了我们模型的有效性，证明了其稳健性和适用性。我们的研究结果突显了多BERT集成模型在提高孟加拉MedER方面的潜力，并为低资源医学NLP的进一步发展奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus.</div>
</details>
</div>
<div class="card">
<div class="title">ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics</div>
<div class="meta-line">Authors: Li S. Yifei, Allen Chang, Chaitanya Malaviya, Mark Yatskar</div>
<div class="meta-line">First: 2025-08-30T13:37:28+00:00 · Latest: 2025-12-19T16:38:02+00:00</div>
<div class="meta-line">Comments: 12 pages main, 40 pages total, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.00496v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.00496v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is abundant: survey articles consolidate knowledge spread across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Queries and rubrics are jointly derived from survey sections, where rubric items list query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. 31 Ph.D. annotators in 8 fields judge that 90% of queries reflect Ph.D. information needs and 87% of rubric items warrant emphasis of a sentence or longer. We leverage ResearchQA to evaluate 18 systems in 7.6K head-to-heads. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResearchQA：通过调查提取的问题和评分标准在75个领域大规模评估学术问答</div>
<div class="mono" style="margin-top:8px">对研究查询的长篇回答进行评估主要依赖于专家注释，这限制了对如人工智能等领域的关注，因为研究人员可以方便地召集同事。然而，研究专长是丰富的：综述文章汇总了文献中的知识。我们引入了ResearchQA，这是一个资源，通过从75个研究领域中提炼出21000个查询和16万个评分标准项来评估LLM系统。查询和评分标准项共同源自调查部分，其中评分标准项列出了查询特定的答案评估标准，例如引用论文、解释和描述局限性。8个领域中的31名博士注释者判断90%的查询反映了博士的信息需求，87%的评分标准项需要强调一个句子或更长的内容。我们利用ResearchQA评估了18个系统在7600次一对一的比赛中。我们评估的任何参数化或检索增强系统在覆盖评分标准项方面均未超过70%，最高排名的系统达到了75%的覆盖率。错误分析表明，最高排名的系统完全解决了不到11%的引用评分标准项、48%的局限性项和49%的比较项。我们发布了我们的数据，以促进更全面的多领域评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues.</div>
</details>
</div>
<div class="card">
<div class="title">LLM-as-a-qualitative-judge: automating error analysis in natural language generation</div>
<div class="meta-line">Authors: Nadezhda Chirkova, Tunde Oluwaseyi Ajayi, Seth Aycock, Zain Muhammad Mujahid, Vladana Perlić, Ekaterina Borisova, Markarit Vartampetian</div>
<div class="meta-line">First: 2025-06-10T18:01:42+00:00 · Latest: 2025-12-19T16:35:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09147v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.09147v4">PDF</a> · <a href="https://github.com/tunde-ajayi/llm-as-a-qualitative-judge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that instance-specific issues output by LLM-as-a-qualitative-judge match those annotated by humans in 2/3 cases, and that LLM-as-a-qualitative-judge is capable of producing error type reports resembling the reports composed by human annotators. We also demonstrate in a case study how the use of LLM-as-a-qualitative-judge can substantially improve NLG systems performance. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM作为定性法官：自然语言生成中的错误分析自动化</div>
<div class="mono" style="margin-top:8px">通过提示大型语言模型（LLMs）评估生成的文本，即LLM作为法官，已成为自然语言生成（NLG）的标准评估方法，但主要用作定量工具，即以数值分数为主要输出。在本研究中，我们提出了LLM作为定性法官，这是一种基于LLM的评估方法，主要输出是NLG系统输出中常见问题类型的结构化报告。该方法旨在为开发者提供有关如何改进给定NLG系统的有意义见解，并包括两个主要步骤，即实例开放问题分析和使用直观累积算法对发现的问题进行聚类。我们还介绍了一种评估所提方法的策略，并附有来自12个NLG数据集的约300个问题注释。结果显示，LLM作为定性法官输出的实例特定问题中有2/3与人类标注的问题匹配，并且LLM作为定性法官能够生成类似于人类注释者编写的错误类型报告。我们还在案例研究中展示了如何使用LLM作为定性法官可以显著提高NLG系统的性能。我们的代码和数据可在https://github.com/tunde-ajayi/llm-as-a-qualitative-judge上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e.</div>
</details>
</div>
<div class="card">
<div class="title">Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation</div>
<div class="meta-line">Authors: Luca Miglior, Matteo Tolloso, Alessio Gravina, Davide Bacciu</div>
<div class="meta-line">First: 2025-12-19T16:34:27+00:00 · Latest: 2025-12-19T16:34:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17762v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17762v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你能听到我现在的话吗？一种长范围图传播基准</div>
<div class="mono" style="margin-top:8px">有效地捕捉长范围交互仍然是图神经网络（GNN）研究中的一个基本但未解决的挑战，对于跨多个科学领域的应用至关重要。为系统地解决这一问题，我们引入了ECHO（Evaluating Communication over long HOps），这是一种新型基准，专门设计用于严格评估GNN在处理非常长范围图传播方面的能力。ECHO 包含三个合成图任务，分别是单源最短路径、节点 eccentricity 和图直径，每个任务都构建在多样且结构上具有挑战性的拓扑上，故意设计以引入显著的信息瓶颈。ECHO 还包括两个真实世界数据集，ECHO-Charge 和 ECHO-Energy，它们分别定义了基于化学的基准，用于预测原子部分电荷和分子总能量，参考计算在密度泛函理论（DFT）水平上获得。这两个任务都内在地依赖于捕捉复杂的长范围分子交互。我们对流行的GNN架构的广泛基准测试揭示了明显的性能差距，强调了真实长范围传播的难度，并突显了能够克服固有限制的设计选择。ECHO 因此为评估长范围信息传播设定了新的标准，同时也为AI在科学中的需求提供了有力的示例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science.</div>
</details>
</div>
<div class="card">
<div class="title">Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</div>
<div class="meta-line">Authors: Rahul Ravi, Ruizhe Li, Tarek Abdelfatah, Stephen Chan, Xin Chen</div>
<div class="meta-line">First: 2025-12-19T16:32:31+00:00 · Latest: 2025-12-19T16:32:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17759v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17759v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用对齐纵向MRI和临床数据预测乳腺癌新辅助化疗治疗反应</div>
<div class="mono" style="margin-top:8px">目的：本研究旨在利用纵向对比增强磁共振成像（CE-MRI）和临床数据，预测乳腺癌患者新辅助化疗（NACT）的治疗反应。目标是开发机器学习（ML）模型，以预测病理完全缓解（PCR二分类）和5年无复发生存状态（RFS二分类）。方法：提出的框架包括肿瘤分割、图像配准、特征提取和预测建模。通过图像配准方法，可以从不同时间点的原始肿瘤位置提取和比较MRI图像特征，从而监测NACT过程中的肿瘤内变化。实现了四个特征提取器，包括一个影像组学和三个基于深度学习（MedicalNet、Segformer3D、SAM-Med3D）的方法，并进行了比较。结合三种特征选择方法和四种ML模型，构建了预测模型并进行了比较。结果：提出的基于图像配准的特征提取方法一致地提高了预测模型的性能。在PCR和RFS分类任务中，基于影像组学特征训练的逻辑回归模型表现最佳，PCR分类的AUC为0.88，分类准确率为0.85，RFS分类的AUC为0.78，分类准确率为0.72。结论：证明了图像配准方法在纵向特征学习中显著提高了预测PCR和RFS的性能。影像组学特征提取器比预训练的深度学习特征提取器更有效，具有更高的性能和更好的可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data.</div>
</details>
</div>
<div class="card">
<div class="title">Look-Ahead Reasoning on Learning Platforms</div>
<div class="meta-line">Authors: Haiqing Zhu, Tijana Zrnic, Celestine Mendler-Dünner</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-18T18:45:32+00:00 · Latest: 2025-12-19T16:32:22+00:00</div>
<div class="meta-line">Comments: published at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14745v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.14745v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-k thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner&#x27;s and the users&#x27; utilities emerges as a key concept. Look-ahead reasoning can be seen as a generalization of algorithmic collective action; we thus offer the first results characterizing the utility trade-offs of coordination when contesting algorithmic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习平台上的前瞻推理</div>
<div class="mono" style="margin-top:8px">在许多学习平台上，指导模型训练的优化标准反映了设计者的优先级，而不是受影响个体的优先级。因此，用户可能会战略性地行动以获得更有利的结果。尽管过去的研究已经研究了学习平台上用户的战略性行为，但重点主要在于对已部署模型的响应，而没有考虑其他用户的行为。相比之下，前瞻推理考虑了用户行为的耦合性，并且在大规模情况下影响未来的预测。在此框架内，我们首先形式化了行为经济学中的层级思考概念，即用户通过前瞻一步来试图超越他们的同侪。我们表明，虽然均衡收敛速度加快，但均衡状态保持不变，长期来看更高层次的推理对个人没有益处。然后，我们关注集体推理，其中用户通过优化他们对模型的联合影响来采取协调行动。通过对比集体行为与自私行为，我们描述了协调的益处和局限性；一种新的学习者和用户效用之间的对齐概念成为关键概念。前瞻推理可以被视为算法集体行动的一般化；因此，我们提供了关于在挑战算法系统时协调的效用权衡的第一批结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect.</div>
</details>
</div>
<div class="card">
<div class="title">AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora</div>
<div class="meta-line">Authors: Zhihan Zhou, Daqian Shi, Rui Song, Lida Shi, Xiaolei Diao, Hao Xu</div>
<div class="meta-line">First: 2025-12-19T16:28:57+00:00 · Latest: 2025-12-19T16:28:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17756v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17756v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AncientBench：面向出土和传世中文语料的全面评估</div>
<div class="mono" style="margin-top:8px">理解古代文献在考古学和中国历史与文明理解中起着重要作用。大型语言模型的快速发展需要能够评估其对古代文字理解能力的基准。现有的中文基准主要针对现代中文和古代中文的传世文献，但出土文献的部分未被涵盖。为满足这一需求，我们提出了AncientBench，旨在评估古代文字的理解能力，特别是在出土文献场景中的理解能力。AncientBench分为四个维度，对应古代文字理解的四个能力：字形理解、读音理解、意义理解以及语境理解。基准还包含十个任务，包括部首、声旁、同音词、填空、翻译等，提供了一个全面的评估框架。我们召集了考古研究人员进行了实验评估，提出了一个古代模型作为基线，并对当前表现最好的大型语言模型进行了广泛的实验。实验结果揭示了大型语言模型在古代文本场景中的巨大潜力以及与人类的差距。我们的研究旨在促进大型语言模型在考古学和古代汉语领域的开发和应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization.</div>
</details>
</div>
<div class="card">
<div class="title">PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</div>
<div class="meta-line">Authors: Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu</div>
<div class="meta-line">First: 2025-08-14T10:03:47+00:00 · Latest: 2025-12-19T16:27:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10501v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.10501v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PASS：概率代理超网络采样以实现可解释和自适应的胸部X光推理</div>
<div class="mono" style="margin-top:8px">现有的工具增强代理系统在现实世界中受到（i）黑箱推理步骤的限制，这些步骤削弱了决策制定的信任并带来安全风险，（ii）较差的多模态集成，这是医疗保健任务中固有的关键因素，以及（iii）僵化的计算效率低的代理管道。我们引入了PASS（概率代理超网络采样），这是第一个在胸部X光（CXR）推理的背景下解决这些挑战的多模态框架。PASS自适应地在多工具图上采样代理工作流，生成带有可解释概率的决策路径。鉴于复杂的CXR推理任务和多模态医疗数据，PASS利用其在代理超网络上学习的任务条件分布。因此，它在每个超网络层自适应地选择最合适的工具，提供带有概率注释的轨迹以供事后审计，并直接增强医疗AI的安全性。PASS还不断将重要的发现压缩到一个不断发展的个性化记忆中，同时动态决定是否加深其推理路径或调用早期退出以提高效率。为了优化平衡性能和成本的帕累托前沿，我们设计了一种新颖的三阶段训练程序，包括专家知识预热、对比路径排名和成本感知强化学习。为了促进严格的评估，我们引入了CAB-E，这是一个全面的多步骤、安全关键、自由形式CXR推理基准。跨各种基准的实验验证了PASS在多个指标（如准确性、LLM-Judge、语义相似性等）上显著优于强大的基线，同时平衡计算成本，推动了可解释、自适应和多模态医疗代理系统的新范式转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines.</div>
</details>
</div>
<div class="card">
<div class="title">The Generation Phases of Flow Matching: a Denoising Perspective</div>
<div class="meta-line">Authors: Anne Gagneux, Ségolène Martin, Rémi Gribonval, Mathurin Massias</div>
<div class="meta-line">First: 2025-10-28T16:42:53+00:00 · Latest: 2025-12-19T16:21:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24830v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24830v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>流匹配的生成阶段：一种去噪视角</div>
<div class="mono" style="margin-top:8px">流匹配已经取得了显著的成功，但其生成过程质量的影响因素仍然知之甚少。在本文中，我们从去噪的角度出发，设计了一个框架来实证探究生成过程。我们建立了流匹配模型与去噪器之间的正式联系，为比较它们在生成和去噪方面的性能提供了一个共同的基础。这使得我们可以设计出原理上合理且可控的扰动来影响样本生成：噪声和漂移。这为我们提供了关于生成过程不同动态阶段的新见解，使我们能够精确地描述去噪器在生成过程中的成功或失败阶段及其原因。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood.</div>
</details>
</div>
<div class="card">
<div class="title">Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?</div>
<div class="meta-line">Authors: Guiomar Pescador-Barrios, Sarah Filippi, Mark van der Wilk</div>
<div class="meta-line">Venue: PMLR 267:48974-49000 (2025)</div>
<div class="meta-line">First: 2024-08-14T14:40:00+00:00 · Latest: 2025-12-19T16:19:39+00:00</div>
<div class="meta-line">Comments: 9 pages main, 27 pages total, 13 figures, 9 tables, conference paper, minor correction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.07588v5">Abs</a> · <a href="https://arxiv.org/pdf/2408.07588v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question &quot;How big is big enough?&quot; We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续高斯过程调整模型大小：多大才够大？</div>
<div class="mono" style="margin-top:8px">许多机器学习模型在训练前需要设置一个控制其大小的参数，例如DNN中的神经元数量或GPs中的诱导点数量。增加容量通常会提高性能，直到从数据集中捕获所有信息为止。在此之后，计算成本会不断增加，而性能却没有改善。这引出了一个问题：“多大才够大？”我们研究了在连续学习中高斯过程（单层神经网络）的这一问题。在这种情况下，数据会逐步提供，因此在训练前无法知道最终的数据集大小，这使得无法使用设置固定模型大小的启发式方法。我们开发了一种方法，在保持接近最优性能的同时自动调整模型大小。我们的实验程序遵循任何超参数必须在未看到数据集属性的情况下设置的约束，我们展示了该方法在各种数据集上表现良好，无需调整其超参数，表明其需要的调优比其他方法少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Many machine learning models require setting a parameter that controls their size before training, e.g.</div>
</details>
</div>
<div class="card">
<div class="title">Fun-ASR Technical Report</div>
<div class="meta-line">Authors: Keyu An, Yanni Chen, Zhigao Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Ying Liu, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Haoxu Wang, Wen Wang, Wupeng Wang, Yuzhong Wu, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou, Yanqiao Zhu</div>
<div class="meta-line">First: 2025-09-15T23:19:36+00:00 · Latest: 2025-12-19T16:18:03+00:00</div>
<div class="meta-line">Comments: Authors are listed in alphabetical order. Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12508v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.12508v4">PDF</a> · <a href="https://github.com/FunAudioLLM/Fun-ASR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. The code and models are accessible at https://github.com/FunAudioLLM/Fun-ASR .</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">FakeParts: a New Family of AI-Generated DeepFakes</div>
<div class="meta-line">Authors: Ziyi Liu, Firas Gabetni, Awais Hussain Sani, Xi Wang, Soobash Daiboo, Gaetan Brison, Gianni Franchi, Vicky Kalogeiton</div>
<div class="meta-line">First: 2025-08-28T17:55:14+00:00 · Latest: 2025-12-19T16:10:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.21052v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.21052v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos.</div>
</details>
</div>
<div class="card">
<div class="title">Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure</div>
<div class="meta-line">Authors: Jingmao Zhang, Zhiting Zhao, Yunqi Lin, Jianghong Ma, Tianjun Wei, Haijun Zhang, Xiaofeng Zhang</div>
<div class="meta-line">First: 2025-12-19T16:09:29+00:00 · Latest: 2025-12-19T16:09:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17733v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user&#x27;s interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation.</div>
</details>
</div>
<div class="card">
<div class="title">Digital and Web Forensics Model Cards, V1</div>
<div class="meta-line">Authors: Paola Di Maio</div>
<div class="meta-line">First: 2025-12-19T15:56:12+00:00 · Latest: 2025-12-19T15:56:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17722v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17722v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a standardized model card framework specifically designed for digital and web forensics.</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Forgetting in Low Rank Adaptation</div>
<div class="meta-line">Authors: Joanna Sliwa, Frank Schneider, Philipp Hennig, Jose Miguel Hernandez-Lobato</div>
<div class="meta-line">First: 2025-12-19T15:54:36+00:00 · Latest: 2025-12-19T15:54:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17720v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17720v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model&#x27;s prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model&#x27;s confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method&#x27;s regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications.</div>
</details>
</div>
<div class="card">
<div class="title">Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy</div>
<div class="meta-line">Authors: Ishank Juneja, Carlee Joe-Wong, Osman Yağan</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2025-01-17T16:34:45+00:00 · Latest: 2025-12-19T15:43:55+00:00</div>
<div class="meta-line">Comments: ICLR 2025 Conference Paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.10290v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.10290v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-armed bandits (MAB) are commonly used in sequential online decision-making when the reward of each decision is an unknown random variable. In practice however, the typical goal of maximizing total reward may be less important than minimizing the total cost of the decisions taken, subject to a reward constraint. For example, we may seek to make decisions that have at least the reward of a reference ``default&#x27;&#x27; decision, with as low a cost as possible. This problem was recently introduced in the Multi-Armed Bandits with Cost Subsidy (MAB-CS) framework. MAB-CS is broadly applicable to problem domains where a primary metric (cost) is constrained by a secondary metric (reward), and the rewards are unknown. In our work, we address variants of MAB-CS including ones with reward constrained by the reward of a known reference arm or by the subsidized best reward. We introduce the Pairwise-Elimination (PE) algorithm for the known reference arm variant and generalize PE to PE-CS for the subsidized best reward variant. Our instance-dependent analysis of PE and PE-CS reveals that both algorithms have an order-wise logarithmic upper bound on Cost and Quality Regret, making our policies the first with such a guarantee. Moreover, by comparing our upper and lower bound results we establish that PE is order-optimal for all known reference arm problem instances. Finally, experiments are conducted using the MovieLens 25M and Goodreads datasets for both PE and PE-CS revealing the effectiveness of PE and the superior balance between performance and reliability offered by PE-CS compared to baselines from the literature.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-armed bandits (MAB) are commonly used in sequential online decision-making when the reward of each decision is an unknown random variable.</div>
</details>
</div>
<div class="card">
<div class="title">Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric</div>
<div class="meta-line">Authors: Yan Shvartzshnaider, Vasisht Duddu</div>
<div class="meta-line">First: 2024-09-05T17:50:31+00:00 · Latest: 2025-12-19T15:41:38+00:00</div>
<div class="meta-line">Comments: Privacy Enhancing Technologies Symposium (PETS), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.03735v3">Abs</a> · <a href="https://arxiv.org/pdf/2409.03735v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit.</div>
<div class="mono" style="margin-top:8px">该研究通过定义语言模型（LLM）响应中的隐私偏见为信息流动的适当性来解决LLM中的隐私偏见问题，并提出了一种基于上下文完整性的审计指标来评估隐私偏见及其影响因素。主要发现表明，隐私偏见随着提示变化而变化，并受到模型容量和优化的影响。</div>
</details>
</div>
<div class="card">
<div class="title">A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models</div>
<div class="meta-line">Authors: Sabrina Kaniewski, Fabian Schmidt, Markus Enzweiler, Michael Menth, Tobias Heer</div>
<div class="meta-line">First: 2025-07-30T13:17:16+00:00 · Latest: 2025-12-19T15:41:06+00:00</div>
<div class="meta-line">Comments: 43 pages + 20 pages references, 7 tables, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.22659v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.22659v2">PDF</a> · <a href="https://github.com/hs-esslingen-it-security/Awesome-LLM4SVD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 263 studies published between January 2020 and November 2025, categorizing them by task formulation, input representation, system architecture, and techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies at https://github.com/hs-esslingen-it-security/Awesome-LLM4SVD.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection.</div>
<div class="mono" style="margin-top:8px">该研究对使用大型语言模型（LLMs）进行软件漏洞检测的文献进行了系统性回顾，分析了2020年至2025年间发表的263篇相关研究。研究按任务表述、输入表示、系统架构和技术手段对这些研究进行了分类，并考察了所使用的数据集的特征、漏洞覆盖范围和多样性。主要发现包括详细的漏洞检测方法分类、识别的局限性以及对未来研究的建议。该研究提高了透明度，并为该领域的研究人员和从业者提供了实用指南。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study</div>
<div class="meta-line">Authors: Shengdu Chai, Chen Lin, Xinyang Dong, Yuqiang Li, Wanli Ouyang, Lei Wang, X. C. Xie</div>
<div class="meta-line">First: 2025-12-19T15:36:27+00:00 · Latest: 2025-12-19T15:36:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17703v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The crystal structure of high-pressure solid hydrogen remains a fundamental open problem. Although the research frontier has mostly shifted toward ultra-high pressure phases above 400 GPa, we show that even the broken symmetry phase observed around 130~GPa requires revisiting due to its intricate coupling of electronic and nuclear degrees of freedom. Here, we develop a first principle quantum Monte Carlo framework based on a deep neural network wave function that treats both electrons and nuclei quantum mechanically within the constant pressure ensemble. Our calculations reveal an unreported ground-state structure candidate for the broken symmetry phase with $Cmcm$ space group symmetry, and we test its stability up to 96 atoms. The predicted structure quantitatively matches the experimental equation of state and X-ray diffraction patterns. Furthermore, our group-theoretical analysis shows that the $Cmcm$ structure is compatible with existing Raman and infrared spectroscopic data. Crucially, static density functional theory calculation reveals the $Cmcm$ structure as a dynamically unstable saddle point on the Born-Oppenheimer potential energy surface, demonstrating that a full quantum many-body treatment of the problem is necessary. These results shed new light on the phase diagram of high-pressure hydrogen and call for further experimental verifications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The crystal structure of high-pressure solid hydrogen remains a fundamental open problem.</div>
<div class="mono" style="margin-top:8px">该研究通过使用深度神经网络波函数开发量子蒙特卡洛框架，重新审视约130 GPa下的固态氢的破裂对称相。计算预测了一个新的$Cmcm$空间群对称的基态结构，该结构在96个原子范围内是稳定的，并且与实验数据相符。该结构也与光谱数据兼容，但静态密度泛函理论显示它是Born-Oppenheimer势能面上的动态不稳定的鞍点，强调了需要进行全面的量子多体处理的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse, Efficient and Explainable Data Attribution with DualXDA</div>
<div class="meta-line">Authors: Galip Ümit Yolcu, Moritz Weckbecker, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</div>
<div class="meta-line">First: 2024-02-19T13:13:16+00:00 · Latest: 2025-12-19T15:36:27+00:00</div>
<div class="meta-line">Comments: Accepted to Transactions on Machine Learning Research (TMLR), 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.12118v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.12118v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data Attribution (DA) is an emerging approach in the field of eXplainable Artificial Intelligence (XAI), aiming to identify influential training datapoints which determine model outputs. It seeks to provide transparency about the model and individual predictions, e.g. for model debugging, identifying data-related causes of suboptimal performance. However, existing DA approaches suffer from prohibitively high computational costs and memory demands when applied to even medium-scale datasets and models, forcing practitioners to resort to approximations that may fail to capture the true inference process of the underlying model. Additionally, current attribution methods exhibit low sparsity, resulting in non-negligible attribution scores across a high number of training examples, hindering the discovery of decisive patterns in the data. In this work, we introduce DualXDA, a framework for sparse, efficient and explainable DA, comprised of two interlinked approaches, Dual Data Attribution (DualDA) and eXplainable Data Attribution (XDA): With DualDA, we propose a novel approach for efficient and effective DA, leveraging Support Vector Machine theory to provide fast and naturally sparse data attributions for AI predictions. In extensive quantitative analyses, we demonstrate that DualDA achieves high attribution quality, excels at solving a series of evaluated downstream tasks, while at the same time improving explanation time by a factor of up to 4,100,000x compared to the original Influence Functions method, and up to 11,000x compared to the method&#x27;s most efficient approximation from literature to date. We further introduce XDA, a method for enhancing Data Attribution with capabilities from feature attribution methods to explain why training samples are relevant for the prediction of a test sample in terms of impactful features, which we showcase and verify qualitatively in detail.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Data Attribution (DA) is an emerging approach in the field of eXplainable Artificial Intelligence (XAI), aiming to identify influential training datapoints which determine model outputs.</div>
<div class="mono" style="margin-top:8px">DualXDA 是一个稀疏、高效且可解释的数据归因框架，解决了现有方法的计算和内存问题。它引入了DualDA，利用支持向量机理论提供快速且自然稀疏的归因，并引入了XDA，通过结合特征归因方法进一步解释为什么特定训练样本对测试样本的预测具有重要性。DualDA 将解释时间显著提高到原影响函数方法的 4,100,000 倍，以及最高效的近似方法的 11,000 倍。XDA 进一步澄清了特定训练样本对预测的重要性。广泛的定量分析表明，DualDA 具有高质量的归因和在下游任务上的优越性能。</div>
</details>
</div>
<div class="card">
<div class="title">Generalized infinite dimensional Alpha-Procrustes based geometries</div>
<div class="meta-line">Authors: Salvish Goomanee, Andi Han, Pratik Jawanpuria, Bamdev Mishra</div>
<div class="meta-line">First: 2025-11-12T23:05:14+00:00 · Latest: 2025-12-19T15:36:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09801v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09801v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances.</div>
</details>
</div>
<div class="card">
<div class="title">OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases</div>
<div class="meta-line">Authors: Tomas Bueno Momcilovic, Barbara Gallina, Ingmar Kessler, Jule Hendricks, Dian Balta</div>
<div class="meta-line">First: 2025-05-20T08:15:16+00:00 · Latest: 2025-12-19T15:34:46+00:00</div>
<div class="meta-line">Comments: Submitted to the ESWC 2026 Resources track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.11023v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.11023v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Assurance cases (ACs) are a common artifact for building and maintaining confidence in system properties such as safety or robustness. Constructing an AC can be challenging, although existing tools provide support in static, document-centric applications and methods for dynamic contexts (e.g., autonomous driving) are emerging. Unfortunately, managing ACs remains a challenge, since maintaining the embedded knowledge in the face of changes requires substantial effort, in the process deterring developers - or worse, producing poorly managed cases that instill false confidence. To address this, we present OntoGSN: an ontology and supporting middleware for managing ACs in the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge representation and a queryable graph that can be automatically populated, evaluated, and updated. Our contributions include: a 1:1 formalization of the GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology and parser for integration with a widely used AC tool; a repository and documentation of design decisions for OntoGSN maintenance; a SPARQL query library with automation patterns; and a prototypical interface. The ontology strictly adheres to the standard&#x27;s text and has been evaluated according to FAIR principles, the OOPS framework, competency questions, and community feedback. The development of other middleware elements is guided by the community needs and subject to ongoing evaluations. To demonstrate the utility of our contributions, we illustrate dynamic AC management in an example involving assurance of adversarial robustness in large language models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Assurance cases (ACs) are a common artifact for building and maintaining confidence in system properties such as safety or robustness.</div>
<div class="mono" style="margin-top:8px">论文介绍了OntoGSN，这是一种基于本体的框架，用于在目标结构化表示（GSN）标准下管理和扩展保证案例（ACs）。该框架提供了GSN v3的OWL形式化表示，包含SWRL规则，一个辅助本体和可查询的图。关键发现包括该本体遵循FAIR原则和社区反馈，以及在管理动态ACs方面的实用性，例如在大型语言模型的对抗鲁棒性保证方面的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</div>
<div class="meta-line">Authors: Yuri Calleo</div>
<div class="meta-line">First: 2025-12-19T15:32:24+00:00 · Latest: 2025-12-19T15:32:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17696v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography&#x27;&#x27;, where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of modeling high-dimensional spatio-temporal processes by proposing a spatially-informed transformer. The method injects a geostatistical inductive bias into the self-attention mechanism through a learnable covariance kernel, decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual. Experiments on synthetic Gaussian random fields and real-world traffic data show that the proposed method outperforms state-of-the-art graph neural networks, providing both superior predictive accuracy and well-calibrated probabilistic forecasts.</div>
<div class="mono" style="margin-top:8px">本文提出了一种空间感知变换器来解决高维时空过程建模的挑战。该方法通过可学习的协方差核将时空统计归纳偏差注入到自我注意力机制中，将注意力结构分解为一个平稳的物理先验和一个非平稳的数据驱动残差。实验表明，所提出的方法在合成和真实世界交通基准上的预测准确性和概率预测校准方面均优于最先进的图神经网络。</div>
</details>
</div>
<div class="card">
<div class="title">Imputation Uncertainty in Interpretable Machine Learning Methods</div>
<div class="meta-line">Authors: Pegah Golchian, Marvin N. Wright</div>
<div class="meta-line">Venue: IJCAI 2025</div>
<div class="meta-line">First: 2025-12-19T15:24:49+00:00 · Latest: 2025-12-19T15:24:49+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 Figures, accepted at conference: IJCAI 2025 Workshop on Explainable Artificial Intelligence (Montreal, Canada)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17689v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17689v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the issue of missing values in real data, which impacts the interpretation of machine learning models using interpretable methods. The research compares the effects of different imputation methods on the confidence interval coverage probabilities of permutation feature importance, partial dependence plots, and Shapley values. The findings indicate that single imputation underestimates variance, and only multiple imputation provides coverage close to the expected nominal level.</div>
<div class="mono" style="margin-top:8px">论文探讨了实数据中缺失值对使用可解释机器学习方法进行解释的影响。研究比较了不同插补方法对特征重要性排序、部分依赖图和Shapley值置信区间覆盖率的影响。研究发现，单一插补会低估方差，只有多重插补的覆盖率接近预期水平。</div>
</details>
</div>
<div class="card">
<div class="title">Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents</div>
<div class="meta-line">Authors: Paul Mangold, Eloïse Berthier, Eric Moulines</div>
<div class="meta-line">First: 2025-12-19T15:23:44+00:00 · Latest: 2025-12-19T15:23:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17688v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17688v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to analyze the convergence properties of Federated SARSA (FedSARSA) with linear function approximation and local training, especially in heterogeneous environments. The study introduces a new multi-step error expansion for single-agent SARSA and provides the first sample and communication complexity bounds for this setting. The key finding is that FedSARSA converges with multiple local updates and achieves linear speed-up with the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments validate these theoretical results.</div>
<div class="mono" style="margin-top:8px">研究旨在分析在存在异质性代理的情况下，带有线性函数逼近和局部训练的联邦SARSA（FedSARSA）的收敛性。研究引入了一种新的单代理SARSA多步误差扩展，首次为该设置提供了样本和通信复杂度界。关键发现包括FedSARSA在多个局部更新下的收敛性，并且随着代理数量的增加，其速度呈线性提升，但存在由于马尔可夫采样引起的高阶项。数值实验支持了理论结果。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs</div>
<div class="meta-line">Authors: Shasha Zhou, Mingyu Huang, Jack Cole, Charles Britton, Ming Yin, Jan Wolber, Ke Li</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-16T22:58:22+00:00 · Latest: 2025-12-19T15:20:17+00:00</div>
<div class="meta-line">Comments: Accepted as a conference paper at AAAI&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12817v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12817v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用知识图谱评估医疗LLM响应的自动化事实核查</div>
<div class="mono" style="margin-top:8px">近年来大型语言模型（LLMs）的普及有望彻底改变医疗保健领域，具备多种医疗任务的强大能力。然而，在高风险的医疗保健环境中部署LLMs需要严格的验证和验证，以了解潜在的危害。本文探讨了利用医疗知识图谱（KGs）对LLM生成响应的事实性评估的可靠性和可行性。为了验证这一研究，我们引入了FAITH框架，旨在系统地探究基于KG的方法的优势和局限性。FAITH不依赖参考答案，通过将响应分解为原子声明，链接到医疗KG，并基于证据路径进行评分来运行。在多种医疗任务上的实验和人类主观评估表明，基于KG的评估与临床判断的相关性更高，并能有效区分具有不同能力的LLMs。它还对文本差异具有鲁棒性。其评分的内在可解释性还可以帮助用户理解并缓解当前LLMs的局限性。我们得出结论，虽然存在局限性，但利用KGs是医疗保健中自动化事实核查的一个重要方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the use of medical knowledge graphs (KGs) for automatically evaluating the factuality of responses generated by large language models (LLMs) in healthcare. The FAITH framework decomposes LLM responses into claims, links them to a medical KG, and scores them based on evidence paths. Experiments show that this KG-based approach correlates well with clinician judgments and can effectively differentiate between LLMs with varying capabilities, while being robust to textual variations. The framework also provides explainable scores that help users understand and mitigate LLM limitations.</div>
<div class="mono" style="margin-top:8px">本文探讨了使用医疗知识图谱（KGs）来评估大型语言模型（LLMs）在医疗保健领域生成的响应的真实性。作者引入了FAITH框架，该框架将LLM响应分解为原子性声明，并将其与医疗KG进行评估。实验表明，基于KG的评估与临床判断高度相关，并能有效区分具有不同能力的LLM，同时对文本变化具有鲁棒性，并提供可解释的评分以帮助用户理解LLM的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">You Only Train Once: Differentiable Subset Selection for Omics Data</div>
<div class="meta-line">Authors: Daphné Chopard, Jorge da Silva Gonçalves, Irene Cannistraci, Thomas M. Sutter, Julia E. Vogt</div>
<div class="meta-line">First: 2025-12-19T15:17:34+00:00 · Latest: 2025-12-19T15:17:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17678v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17678v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你只需训练一次：omics数据的可微分子集选择</div>
<div class="mono" style="margin-top:8px">从单细胞转录组数据中选择紧凑且信息丰富的基因子集对于生物标志物发现、提高可解释性和降低成本的表型分析至关重要。然而，大多数现有的特征选择方法要么作为多阶段管道运行，要么依赖于事后特征归因，使得选择和预测联系不强。在本文中，我们提出了YOTO（你只需训练一次），这是一种端到端框架，可以在单一可微分架构中同时识别离散基因子集并执行预测。在我们的模型中，预测任务直接指导选择哪些基因，而学习到的子集反过来又塑造预测表示。这种封闭的反馈循环使模型在训练过程中能够逐步细化它选择的内容和预测方式。与现有方法不同，YOTO 强制执行稀疏性，使得只有选定的基因参与推理，从而消除了需要额外训练下游分类器的需求。通过多任务学习设计，模型在相关目标上学习共享表示，允许部分标记的数据集相互提供信息，并发现能够在任务之间泛化的基因子集，而无需额外的训练步骤。我们在两个代表性的单细胞RNA-seq数据集上评估了YOTO，结果显示它始终优于最先进的基线方法。这些结果表明，稀疏的端到端多任务基因子集选择可以提高预测性能，并产生紧凑且有意义的基因子集，从而推进生物标志物发现和单细胞分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve gene subset selection in single-cell transcriptomic data for better biomarker discovery and cost-effective profiling. YOTO (You Only Train Once) is an end-to-end framework that jointly selects discrete gene subsets and performs prediction within a single differentiable architecture, enabling a closed feedback loop for iterative refinement. Experiments on two single-cell RNA-seq datasets show that YOTO outperforms existing methods, providing sparse, compact, and meaningful gene subsets that enhance predictive performance.</div>
<div class="mono" style="margin-top:8px">研究旨在通过单细胞转录组数据中的基因子集选择，提高生物标志物发现和成本效益分析的效果。YOTO（You Only Train Once）是一种端到端框架，它在一个可微分架构中同时选择离散的基因子集和执行预测，实现闭环反馈循环以进行迭代优化。在两个单细胞RNA-seq数据集上的实验表明，YOTO优于现有方法，提供了稀疏、紧凑且有意义的基因子集，提升了预测性能。</div>
</details>
</div>
<div class="card">
<div class="title">An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution</div>
<div class="meta-line">Authors: Yudhistira Arief Wibowo</div>
<div class="meta-line">First: 2025-12-19T15:17:12+00:00 · Latest: 2025-12-19T15:17:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17675v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散模型的超分辨率采样超参数实证研究</div>
<div class="mono" style="margin-top:8px">扩散模型在解决单张图像超分辨率等逆问题方面显示出强大的潜力，通过预训练的无条件先验从低分辨率观察中恢复高分辨率图像。条件化方法，包括扩散后验采样（DPS）和流形约束梯度（MCG），可以显著提高重建质量，但它们引入了需要仔细调整的额外超参数。在本文中，我们对FFHQ超分辨率进行了实证消融研究，以确定在将条件化应用于预训练扩散模型时影响性能的主要因素，并表明条件化步骤大小的影响远大于扩散步骤数，范围在[2.0, 3.0]的步骤大小在我们的实验中总体性能最佳。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of sampling hyperparameters in diffusion models for super-resolution tasks. The research focuses on identifying the key factors affecting performance when conditioning pretrained diffusion models, using FFHQ as a case study. The findings indicate that the conditioning step size is more influential than the diffusion step count, with optimal results achieved when the step size is between 2.0 and 3.0.</div>
<div class="mono" style="margin-top:8px">该研究探讨了在超分辨率任务中扩散模型采样超参数的影响。研究集中在确定预训练扩散模型进行条件化时的关键因素，使用FFHQ作为案例研究。研究发现，条件化步长比扩散步数对性能的影响更大，最佳结果在步长为2.0到3.0的范围内获得。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation</div>
<div class="meta-line">Authors: Alexandre Personnic, Mihai Bâce</div>
<div class="meta-line">First: 2025-12-19T15:15:58+00:00 · Latest: 2025-12-19T15:15:58+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures, the code repository is available at https://gitlab.kuleuven.be/u0172623/ST-Gaze</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17673v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17673v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视频的眼球凝视时空特征表示学习</div>
<div class="mono" style="margin-top:8px">基于视频的眼球凝视估计方法旨在从多张图像帧中捕捉人类眼球凝视的固有时间动态。然而，由于模型必须捕捉空间和时间关系，因此性能受限于单帧内的特征表示，同时也受限于多帧之间的特征表示。我们提出了时空凝视网络（ST-Gaze），该模型结合了CNN主干网络和专用的通道注意力模块和自我注意力模块，以最优方式融合眼和面部特征。融合后的特征被视为一个空间序列，允许捕获帧内上下文，然后通过时间传播以建模帧间动态。我们在EVE数据集上评估了我们的方法，并展示了ST-Gaze在有和无个体适应的情况下均达到最先进的性能。此外，我们的消融研究还提供了关于模型性能的进一步见解，表明通过我们的时空递归保留和建模帧内空间上下文本质上优于过早的空间聚合。因此，我们的结果为使用常见摄像头进行更稳健的基于视频的眼球凝视估计铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve video-based gaze estimation by capturing both spatial and temporal dynamics. The Spatio-Temporal Gaze Network (ST-Gaze) combines a CNN backbone with channel and self-attention modules to fuse eye and face features, treating them as a spatial sequence for intra-frame context and propagating this context through time to model inter-frame dynamics. The method achieves state-of-the-art performance on the EVE dataset and demonstrates the superiority of preserving intra-frame spatial context over premature spatial pooling.</div>
<div class="mono" style="margin-top:8px">研究旨在通过捕捉空间和时间动态来改进基于视频的眼球追踪。Spatio-Temporal Gaze Network (ST-Gaze) 结合了 CNN 主干与通道和自注意力模块来融合眼和面部特征，并将这些特征视为空间序列以捕捉帧内上下文，然后通过时间传播这种上下文以建模帧间动态。该方法在 EVE 数据集上达到了最先进的性能，并展示了保留帧内空间上下文优于过早的空间池化的优势。</div>
</details>
</div>
<div class="card">
<div class="title">MolMark: Safeguarding Molecular Structures through Learnable Atom-Level Watermarking</div>
<div class="meta-line">Authors: Runwen Hu, Peilin Chen, Keyan Ding, Shiqi Wang</div>
<div class="meta-line">First: 2025-08-25T06:21:11+00:00 · Latest: 2025-12-19T15:15:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.17702v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.17702v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI-driven molecular generation is reshaping drug discovery and materials design, yet the lack of protection mechanisms leaves AI-generated molecules vulnerable to unauthorized reuse and provenance ambiguity. Such limitation undermines both scientific reproducibility and intellectual property security. To address this challenge, we propose the first deep learning based watermarking framework for molecules (MolMark), which is exquisitely designed to embed high-fidelity digital signatures into molecules without compromising molecular functionalities. MolMark learns to modulate the chemically meaningful atom-level representations and enforce geometric robustness through SE(3)-invariant features, maintaining robustness under rotation, translation, and reflection. Additionally, MolMark integrates seamlessly with AI-based molecular generative models, enabling watermarking to be treated as a learned transformation with minimal interference to molecular structures. Experiments on benchmark datasets (QM9, GEOM-DRUG) and state-of-the-art molecular generative models (GeoBFN, GeoLDM) demonstrate that MolMark can embed 16-bit watermarks while retaining more than 90% of essential molecular properties, preserving downstream performance, and enabling &gt;95% extraction accuracy under SE(3) transformations. MolMark establishes a principled pathway for unifying molecular generation with verifiable authorship, supporting trustworthy and accountable AI-driven molecular discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolMark：通过可学习的原子级水印保护分子结构</div>
<div class="mono" style="margin-top:8px">基于AI的分子生成正在重塑药物发现和材料设计，然而缺乏保护机制使得AI生成的分子容易被未经授权的重复使用和来源模糊。这种限制削弱了科学可重复性和知识产权的安全性。为了解决这一挑战，我们提出了第一个基于深度学习的分子水印框架（MolMark），该框架精心设计以在不损害分子功能性的前提下将高保真数字签名嵌入分子中。MolMark 学习调节化学上有意义的原子级表示，并通过SE(3)不变特征强制几何鲁棒性，从而在旋转、平移和镜像下保持鲁棒性。此外，MolMark 无缝集成到基于AI的分子生成模型中，使水印可以被视为对分子结构干扰最小的学习变换。在基准数据集（QM9, GEOM-DRUG）和最先进的分子生成模型（GeoBFN, GeoLDM）上的实验表明，MolMark 可以嵌入16位水印同时保留超过90%的关键分子属性，保持下游性能，并在SE(3)变换下实现超过95%的提取准确性。MolMark 为将分子生成与可验证的作者身份统一提供了一条原则性的途径，支持可信和可问责的AI驱动分子发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MolMark is a deep learning-based watermarking framework designed to embed high-fidelity digital signatures into molecules without affecting their functionalities. It uses SE(3)-invariant features to ensure robustness under transformations and integrates seamlessly with molecular generative models. Experiments show that MolMark can embed 16-bit watermarks while preserving over 90% of essential molecular properties and achieving over 95% extraction accuracy under SE(3) transformations.</div>
<div class="mono" style="margin-top:8px">MolMark 是一种基于深度学习的水印框架，旨在在不影响分子功能性的前提下嵌入高保真数字签名。它使用 SE(3)-不变特征来确保在变换下的鲁棒性，并且能够无缝集成到分子生成模型中。实验表明，MolMark 可以嵌入 16 位水印，同时保留超过 90% 的关键分子属性，并在 SE(3) 变换下实现超过 95% 的提取准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Preconditioned Inexact Stochastic ADMM for Deep Model</div>
<div class="meta-line">Authors: Shenglong Zhou, Ouya Wang, Ziyan Luo, Yongxu Zhu, Geoffrey Ye Li</div>
<div class="meta-line">First: 2025-02-15T12:28:51+00:00 · Latest: 2025-12-19T15:14:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.10784v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.10784v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers). Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient on a bounded region, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables the proposed algorithm to tackle the challenge of data heterogeneity effectively. Moreover, the algorithmic architecture enables scalable parallel computing and supports various preconditions, such as second-order information, second moment, and orthogonalized momentum by Newton-Schulz iterations. Incorporating the latter two preconditions in PISA yields two computationally efficient variants: SISA and NSISA. Comprehensive experimental evaluations for training or fine-tuning diverse deep models, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate superior numerical performance of SISA and NSISA compared to various state-of-the-art optimizers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预条件不精确随机ADMM算法在深度模型中的应用</div>
<div class="mono" style="margin-top:8px">基础模型（FMs）的最新进展带来了范式的转变，革新了全球各个领域。用于训练这些模型的常用优化器是基于随机梯度下降的算法，这些算法存在固有的局限性，如收敛速度慢和对收敛的严格假设。特别是，分布式设置中出现的数据异质性对它们的理论和数值性能构成了重大挑战。本文开发了一种算法，PISA（预条件不精确交替方向乘子法）。该算法基于严格的理论保证，在梯度在有界区域内Lipschitz连续的唯一假设下收敛，从而消除了其他随机方法通常需要的其他条件。这种能力使所提出的算法能够有效应对数据异质性的挑战。此外，算法架构支持可扩展的并行计算，并支持各种预条件，如二阶信息、二阶矩和通过牛顿-舒尔兹迭代正交化动量。在PISA中结合后两种预条件产生了两种计算效率高的变体：SISA和NSISA。针对包括视觉模型、大型语言模型、强化学习模型、生成对抗网络和递归神经网络在内的多种深度模型的训练或微调进行全面的实验评估表明，SISA和NSISA在数值性能上优于各种最先进的优化器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing optimizers for training foundation models, particularly their slow convergence and strict assumptions. It introduces PISA (Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers), which converges under the sole assumption of gradient Lipschitz continuity, making it suitable for handling data heterogeneity. PISA supports various preconditions, leading to two efficient variants, SISA and NSISA. Experimental results show that SISA and NSISA outperform other state-of-the-art optimizers in training and fine-tuning various deep models.</div>
<div class="mono" style="margin-top:8px">该论文针对现有优化器训练基础模型时存在的收敛慢和严格假设问题，引入了PISA（预条件不精确随机交替方向乘子法），该方法仅在梯度Lipschitz连续的假设下收敛，适用于处理数据异质性。PISA支持多种预条件，产生了两种高效的变体SISA和NSISA。实验结果显示，SISA和NSISA在训练和微调各种深度模型时优于其他最先进的优化器。</div>
</details>
</div>
<div class="card">
<div class="title">Polyharmonic Cascade</div>
<div class="meta-line">Authors: Yuriy N. Bakhvalov</div>
<div class="meta-line">First: 2025-12-19T15:14:14+00:00 · Latest: 2025-12-19T15:14:14+00:00</div>
<div class="meta-line">Comments: Part 3 of 4 in the &quot;Polyharmonic Cascade&quot; cycle. Proposes a non-SGD training method based on global linear solvers. Previous papers: arXiv:2512.12731, arXiv.2512.16718. Source code is available at: https://github.com/xolod7/polyharmonic-cascade</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17671v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17671v1">PDF</a> · <a href="https://github.com/xolod7/polyharmonic-cascade">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a deep machine learning architecture, the &quot;polyharmonic cascade&quot; -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed &quot;constellations&quot; of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多调和级联</div>
<div class="mono" style="margin-top:8px">本文提出了一种深度机器学习架构——“多调和级联”——它是一系列多调和样条的包序列，每一层都严格源自随机函数理论和无偏原则。这使得它可以逼近任意复杂度的非线性函数，同时保持全局平滑性和概率解释。对于多调和级联，提出了一种不同于梯度下降的训练方法：不是直接优化系数，而是针对固定“星座”节点处的函数值，在每个批次中求解一个全局线性系统。这实现了所有层的同步更新，保持了各层的概率解释，并与原始模型在理论上保持一致，且易于扩展：所有计算都归结为在GPU上高效执行的2D矩阵操作。在MNIST上展示了快速学习且不过拟合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces a deep learning architecture called the &#x27;polyharmonic cascade,&#x27; which uses polyharmonic splines to approximate complex nonlinear functions while maintaining global smoothness and a probabilistic interpretation. A novel training method is proposed, involving solving a global linear system for function values at fixed nodes, which avoids overfitting and scales well with GPU computations. The method is demonstrated to enable fast learning on the MNIST dataset without overfitting.</div>
<div class="mono" style="margin-top:8px">论文提出了一种名为&#x27;多调和级联&#x27;的深度学习架构，使用多调和样条来逼近复杂的非线性函数，同时保持全局光滑性和概率解释。提出了一种新的训练方法，通过在固定节点上求解全局线性系统来计算函数值，这种方法避免了过拟合，并且能够很好地利用GPU进行计算。该方法在MNIST数据集上实现了快速学习且未出现过拟合现象。</div>
</details>
</div>
<div class="card">
<div class="title">STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting</div>
<div class="meta-line">Authors: Yifei Cheng, Yujia Zhu, Baiyang Li, Xinhao Deng, Yitong Cai, Yaochen Ren, Qingyun Liu</div>
<div class="meta-line">First: 2025-12-19T15:12:01+00:00 · Latest: 2025-12-19T15:12:01+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE INFOCOM 2026. Camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17667v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17667v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR&#x27;s datasets and code to support reproducibility and future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STAR：语义-交通对齐和检索在零样本HTTPS网站指纹识别中的应用</div>
<div class="mono" style="margin-top:8px">现代HTTPS机制如加密客户端握手（ECH）和加密DNS提高了隐私性，但仍易受网站指纹识别（WF）攻击，即攻击者通过加密流量模式推断访问的网站。现有WF方法依赖于带有特定网站标记跟踪的监督学习，这限制了其可扩展性并无法处理未见过的网站。我们通过将WF重新定义为零样本跨模态检索问题并引入STAR来解决这些限制。STAR使用双编码器架构学习加密流量跟踪和爬取时间逻辑配置文件的联合嵌入空间。STAR在150K自动收集的流量-逻辑配对数据上进行训练，使用对比和一致性目标以及结构感知增强，无需在训练期间使用目标端流量即可检索与跟踪最语义对齐的配置文件。在1,600个未见过的网站上进行的实验表明，STAR在开放世界检测中的顶级准确率达到87.9%，AUC为0.963，优于监督学习和少量样本基线。通过仅在每个网站上添加四个标记跟踪的适配器，顶级准确率进一步提升至98.8%。我们的分析揭示了现代网络协议中固有的语义-交通对齐，将语义泄漏识别为主要的隐私风险在加密HTTPS流量中。我们发布了STAR的数据集和代码以支持可重复性和未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns.</div>
</details>
</div>
<div class="card">
<div class="title">SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning</div>
<div class="meta-line">Authors: Paul Mangold, Sergey Samsonov, Safwan Labbi, Ilya Levin, Reda Alami, Alexey Naumov, Eric Moulines</div>
<div class="meta-line">First: 2024-02-06T16:06:59+00:00 · Latest: 2025-12-19T15:11:58+00:00</div>
<div class="meta-line">Comments: now with linear speed-up!</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.04114v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.04114v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy $ε$. To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCAFFLSA：驯服联邦线性随机逼近和TD学习中的异质性</div>
<div class="mono" style="margin-top:8px">在本文中，我们分析了联邦线性随机逼近（FedLSA）算法的样本和通信复杂度。我们明确量化了局部训练与代理异质性之间的关系。我们证明了FedLSA的通信复杂度与期望精度$ε$的倒数成多项式关系。为克服这一问题，我们提出了SCAFFLSA，这是一种新的FedLSA变体，使用控制变量来纠正客户端漂移，并建立了其样本和通信复杂度。我们证明，对于统计异质性代理，其通信复杂度与期望精度的倒数成对数关系，类似于Scaffnew。一个重要的发现是，与现有的Scaffnew结果相比，样本复杂度与代理数量的倒数成比例，这一特性称为线性加速。实现这一线性加速需要全新的理论论证。我们将所提出的方法应用于具有线性函数逼近的联邦时差学习，并分析相应的复杂度改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm.</div>
</details>
</div>
<div class="card">
<div class="title">Vidarc: Embodied Video Diffusion Model for Closed-loop Control</div>
<div class="meta-line">Authors: Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu</div>
<div class="meta-line">First: 2025-12-19T15:04:24+00:00 · Latest: 2025-12-19T15:04:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17661v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vidarc：闭环控制的实体化视频扩散模型</div>
<div class="mono" style="margin-top:8px">在数据稀缺的环境中，机械臂操作是一项极具挑战性的任务，因为涉及复杂的实体动力学和多样的情境。最近基于视频的方法通过在互联网规模的视频数据上进行预训练，展示了在捕捉和转移时间和物理交互方面的巨大潜力。然而，这些方法通常未针对特定实体的闭环控制进行优化，往往存在高延迟和不足的语境关联问题。在本文中，我们提出了Vidarc（视频扩散用于动作推理和闭环控制），这是一种新颖的自回归实体化视频扩散方法，通过掩蔽逆动力学模型进行增强。通过使用与动作相关的掩码进行视频预测，并通过缓存的自回归生成实时反馈，Vidarc实现了快速、准确的闭环控制。Vidarc在一百万跨实体的场景上进行预训练，其性能超越了最先进的基线，实现实际部署中至少15%的成功率提升和91%的延迟减少。我们还强调了其在未见过的机器人平台上的鲁棒泛化和错误校正能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts.</div>
</details>
</div>
<div class="card">
<div class="title">Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines</div>
<div class="meta-line">Authors: João Marcos Cavalcanti de Albuquerque Neto, Gustavo Castro do Amaral, Guilherme Penello Temporão</div>
<div class="meta-line">First: 2025-12-19T15:03:00+00:00 · Latest: 2025-12-19T15:03:00+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17660v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17660v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Use cases for emerging quantum computing platforms become economically relevant as the efficiency of processing and availability of quantum computers increase. We assess the performance of Restricted Boltzmann Machines (RBM) assisted by quantum computing, running on real quantum hardware and simulators, using a real dataset containing 145 million transactions provided by Stone, a leading Brazilian fintech, for credit card fraud detection. The results suggest that the quantum-assisted RBM method is able to achieve superior performance in most figures of merit in comparison to classical approaches, even using current noisy quantum annealers. Our study paves the way for implementing quantum-assisted RBMs for general fault detection in financial systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Use cases for emerging quantum computing platforms become economically relevant as the efficiency of processing and availability of quantum computers increase.</div>
</details>
</div>
<div class="card">
<div class="title">Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design</div>
<div class="meta-line">Authors: Madhav R. Muthyala, Farshud Sorourifar, Tianhong Tan, You Peng, Joel A. Paulson</div>
<div class="meta-line">First: 2025-12-19T14:59:27+00:00 · Latest: 2025-12-19T14:59:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17659v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular &quot;generate-then-optimize&quot; framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery.</div>
</details>
</div>
<div class="card">
<div class="title">Estimating Spatially Resolved Radiation Fields Using Neural Networks</div>
<div class="meta-line">Authors: Felix Lehner, Pasquale Lombardo, Susana Castillo, Oliver Hupe, Marcus Magnor</div>
<div class="meta-line">First: 2025-12-19T14:52:04+00:00 · Latest: 2025-12-19T14:52:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17654v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17654v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology.</div>
</details>
</div>
<div class="card">
<div class="title">Refined Analysis of Federated Averaging and Federated Richardson-Romberg</div>
<div class="meta-line">Authors: Paul Mangold, Alain Durmus, Aymeric Dieuleveut, Sergey Samsonov, Eric Moulines</div>
<div class="meta-line">First: 2024-12-02T11:22:19+00:00 · Latest: 2025-12-19T14:49:41+00:00</div>
<div class="meta-line">Comments: 37 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.01389v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.01389v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we present a novel analysis of \FedAvg with constant step size, relying on the Markov property of the underlying process. We demonstrate that the global iterates of the algorithm converge to a stationary distribution and analyze its resulting bias and variance relative to the problem&#x27;s solution. We provide a first-order bias expansion in both homogeneous and heterogeneous settings. Interestingly, this bias decomposes into two distinct components: one that depends solely on stochastic gradient noise and another on client heterogeneity. Finally, we introduce a new algorithm based on the Richardson-Romberg extrapolation technique to mitigate this bias.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we present a novel analysis of \FedAvg with constant step size, relying on the Markov property of the underlying process.</div>
</details>
</div>
<div class="card">
<div class="title">Automated Machine Learning Pipeline: Large Language Models-Assisted Automated Dataset Generation for Training Machine-Learned Interatomic Potentials</div>
<div class="meta-line">Authors: Adam Lahouari, Jutta Rogal, Mark E. Tuckerman</div>
<div class="meta-line">First: 2025-09-25T22:05:20+00:00 · Latest: 2025-12-19T14:43:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21647v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21647v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning interatomic potentials (MLIPs) have become powerful tools to extend molecular simulations beyond the limits of quantum methods, offering near-quantum accuracy at much lower computational cost. Yet, developing reliable MLIPs remains difficult because it requires generating high-quality datasets, preprocessing atomic structures, and carefully training and validating models. In this work, we introduce an Automated Machine Learning Pipeline (AMLP) that unifies the entire workflow from dataset creation to model validation. AMLP employs large-language-model agents to assist with electronic-structure code selection, input preparation, and output conversion, while its analysis suite (AMLP-Analysis), based on ASE supports a range of molecular simulations. The pipeline is built on the MACE architecture and validated on acridine polymorphs, where, with a straightforward fine-tuning of a foundation model, mean absolute errors of ~1.7 meV/atom in energies and ~7.0 meV/Å in forces are achieved. The fitted MLIP reproduces DFT geometries with sub-Å accuracy and demonstrates stability during molecular dynamics simulations in the microcanonical and canonical ensembles.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Machine learning interatomic potentials (MLIPs) have become powerful tools to extend molecular simulations beyond the limits of quantum methods, offering near-quantum accuracy at much lower computational cost.</div>
</details>
</div>
<div class="card">
<div class="title">About Time: Model-free Reinforcement Learning with Timed Reward Machines</div>
<div class="meta-line">Authors: Anirban Majumdar, Ritam Raha, Rajarshi Roy, David Parker, Marta Kwiatkowska</div>
<div class="meta-line">First: 2025-12-19T14:39:03+00:00 · Latest: 2025-12-19T14:39:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17637v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17637v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward specification plays a central role in reinforcement learning (RL), guiding the agent&#x27;s behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reward specification plays a central role in reinforcement learning (RL), guiding the agent&#x27;s behavior.</div>
</details>
</div>
<div class="card">
<div class="title">CharDiff-LP: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration</div>
<div class="meta-line">Authors: Kihyun Na, Gyuhwan Park, Injung Kim</div>
<div class="meta-line">First: 2025-10-20T09:23:29+00:00 · Latest: 2025-12-19T14:39:03+00:00</div>
<div class="meta-line">Comments: 15 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17330v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17330v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">License plate image restoration is important not only as a preprocessing step for license plate recognition but also for enhancing evidential value, improving visual clarity, and enabling broader reuse of license plate images. We propose a novel diffusion-based framework with character-level guidance, CharDiff-LP, which effectively restores and recognizes severely degraded license plate images captured under realistic conditions. CharDiff-LP leverages fine-grained character-level priors extracted through external segmentation and Optical Character Recognition (OCR) modules tailored for low-quality license plate images. For precise and focused guidance, CharDiff-LP incorporates a novel Character-guided Attention through Region-wise Masking (CHARM) module, which ensures that each character&#x27;s guidance is restricted to its own region, thereby avoiding interference with other regions. In experiments, CharDiff-LP significantly outperformed baseline restoration models in both restoration quality and recognition accuracy, achieving a 28.3% relative reduction in character error rate (CER) on the Roboflow-LP dataset compared with the best-performing baseline.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">License plate image restoration is important not only as a preprocessing step for license plate recognition but also for enhancing evidential value, improving visual clarity, and enabling broader reuse of license plate images.</div>
</details>
</div>
<div class="card">
<div class="title">Trust-Region Adaptive Policy Optimization</div>
<div class="meta-line">Authors: Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang</div>
<div class="meta-line">First: 2025-12-19T14:37:07+00:00 · Latest: 2025-12-19T14:37:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17636v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17636v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models&#x27; (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL&#x27;s potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model&#x27;s own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models&#x27; (LLMs) complex reasoning abilities.</div>
</details>
</div>
<div class="card">
<div class="title">Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection</div>
<div class="meta-line">Authors: Menna Elgabry, Ali Hamdi</div>
<div class="meta-line">First: 2025-12-19T14:33:14+00:00 · Latest: 2025-12-19T14:33:14+00:00</div>
<div class="meta-line">Comments: Accepted at IRICT 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17630v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17630v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet&#x27;s Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet&#x27;s Jury Theorem (CJT).</div>
</details>
</div>
<div class="card">
<div class="title">SCOPE: Sequential Causal Optimization of Process Interventions</div>
<div class="meta-line">Authors: Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt</div>
<div class="meta-line">First: 2025-12-19T14:33:02+00:00 · Latest: 2025-12-19T14:33:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17629v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17629v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs).</div>
</details>
</div>
<div class="card">
<div class="title">New Hybrid Heuristics for Pseudo-Boolean Propagation</div>
<div class="meta-line">Authors: Mia Müßig, Jan Johannsen</div>
<div class="meta-line">First: 2025-11-26T14:08:28+00:00 · Latest: 2025-12-19T14:32:02+00:00</div>
<div class="meta-line">Comments: 5 pages, 3 figures, added different cut-off for old hybrid</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21417v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21417v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method.</div>
</details>
</div>
<div class="card">
<div class="title">Generating Samples to Probe Trained Models</div>
<div class="meta-line">Authors: Eren Mehmet Kıral, Nurşen Aydın, Ş. İlker Birbil</div>
<div class="meta-line">First: 2025-02-10T16:48:48+00:00 · Latest: 2025-12-19T14:26:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.06658v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.06658v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">There is a growing need for investigating how machine learning models operate.</div>
</details>
</div>
<div class="card">
<div class="title">Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</div>
<div class="meta-line">Authors: Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ilaï Deutel, Nam Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Ramé, Sagar Waghmare, Helen Miller, Nathan Byrd, Ashrith Sheshan, Raia Hadsell, Sangnie Bhardwaj, Pawel Janus, Tero Rissa, Dan Horgan, Alvin Abdagic, Lior Belenki, James Allingham, Anima Singh, Theo Guidroz, Srivatsan Srinivasan, Herman Schmit, Kristen Chiafullo, Andre Elisseeff, Nilpa Jha, Prateek Kolhar, Leonard Berrada, Frank Ding, Xiance Si, Shrestha Basu Mallick, Franz Och, Sofia Erell, Eric Ni, Tejasi Latkar, Sherry Yang, Petar Sirkovic, Ziqiang Feng, Robert Leland, Rachel Hornung, Gang Wu, Charles Blundell, Hamidreza Alvari, Po-Sen Huang, Cathy Yip, Sanja Deur, Li Liu, Gabriela Surita, Pablo Duque, Dima Damen, Johnson Jia, Arthur Guez, Markus Mircea, Animesh Sinha, Alberto Magni, Paweł Stradomski, Tal Marian, Vlado Galić, Wenhu Chen, Hisham Husain, Achintya Singhal, Dominik Grewe, François-Xavier Aubet, Shuang Song, Lorenzo Blanco, Leland Rechis, Lewis Ho, Rich Munoz, Kelvin Zheng, Jessica Hamrick, Kevin Mather, Hagai Taitelbaum, Eliza Rutherford, Yun Lei, Kuangyuan Chen, Anand Shukla, Erica Moreira, Eric Doi, Berivan Isik, Nir Shabat, Dominika Rogozińska, Kashyap Kolipaka, Jason Chang, Eugen Vušak, Srinivasan Venkatachary, Shadi Noghabi, Tarun Bharti, Younghoon Jun, Aleksandr Zaks, Simon Green, Jeshwanth Challagundla, William Wong, Muqthar Mohammad, Dean Hirsch, Yong Cheng, Iftekhar Naim, Lev Proleev, Damien Vincent, Aayush Singh, Maxim Krikun, Dilip Krishnan, Zoubin Ghahramani, Aviel Atias, Rajeev Aggarwal, Christo Kirov, Dimitrios Vytiniotis, Christy Koh, Alexandra Chronopoulou, Pawan Dogra, Vlad-Doru Ion, Gladys Tyen, Jason Lee, Felix Weissenberger, Trevor Strohman, Ashwin Balakrishna, Jack Rae, Marko Velic, Raoul de Liedekerke, Oded Elyada, Wentao Yuan, Canoee Liu, Lior Shani, Sergey Kishchenko, Bea Alessio, Yandong Li, Richard Song, Sam Kwei, Orion Jankowski, Aneesh Pappu, Youhei Namiki, Yenai Ma, Nilesh Tripuraneni, Colin Cherry, Marissa Ikonomidis, Yu-Cheng Ling, Colin Ji, Beka Westberg, Auriel Wright, Da Yu, David Parkinson, Swaroop Ramaswamy, Jerome Connor, Soheil Hassas Yeganeh, Snchit Grover, George Kenwright, Lubo Litchev, Chris Apps, Alex Tomala, Felix Halim, Alex Castro-Ros, Zefei Li, Anudhyan Boral, Pauline Sho, Michal Yarom, Eric Malmi, David Klinghoffer, Rebecca Lin, Alan Ansell, Pradeep Kumar S, Shubin Zhao, Siqi Zuo, Adam Santoro, Heng-Tze Cheng, Solomon Demmessie, Yuchi Liu, Nicole Brichtova, Allie Culp, Nathaniel Braun, Dan Graur, Will Ng, Nikhil Mehta, Aaron Phillips, Patrik Sundberg, Varun Godbole, Fangyu Liu, Yash Katariya, David Rim, Mojtaba Seyedhosseini, Sean Ammirati, Jonas Valfridsson, Mahan Malihi, Timothy Knight, Andeep Toor, Thomas Lampe, Abe Ittycheriah, Lewis Chiang, Chak Yeung, Alexandre Fréchette, Jinmeng Rao, Huisheng Wang, Himanshu Srivastava, Richard Zhang, Rocky Rhodes, Ariel Brand, Dean Weesner, Ilya Figotin, Felix Gimeno, Rachana Fellinger, Pierre Marcenac, José Leal, Eyal Marcus, Victor Cotruta, Rodrigo Cabrera, Sheryl Luo, Dan Garrette, Vera Axelrod, Sorin Baltateanu, David Barker, Dongkai Chen, Horia Toma, Ben Ingram, Jason Riesa, Chinmay Kulkarni, Yujing Zhang, Hongbin Liu, Chao Wang, Martin Polacek, Will Wu, Kai Hui, Adrian N Reyes, Yi Su, Megan Barnes, Ishaan Malhi, Anfal Siddiqui, Qixuan Feng, Mihai Damaschin, Daniele Pighin, Andreas Steiner, Samuel Yang, Ramya Sree Boppana, Simeon Ivanov, Arun Kandoor, Aditya Shah, Asier Mujika, Da Huang, Christopher A. Choquette-Choo, Mohak Patel, Tianhe Yu, Toni Creswell, Jerry, Liu, Catarina Barros, Yasaman Razeghi, Aurko Roy, Phil Culliton, Binbin Xiong, Jiaqi Pan, Thomas Strohmann, Tolly Powell, Babi Seal, Doug DeCarlo, Pranav Shyam, Kaan Katircioglu, Xuezhi Wang, Cassidy Hardin, Immanuel Odisho, Josef Broder, Oscar Chang, Arun Nair, Artem Shtefan, Maura O&#x27;Brien, Manu Agarwal, Sahitya Potluri, Siddharth Goyal, Amit Jhindal, Saksham Thakur, Yury Stuken, James Lyon, Kristina Toutanova, Fangxiaoyu Feng, Austin Wu, Ben Horn, Alek Wang, Alex Cullum, Gabe Taubman, Disha Shrivastava, Chongyang Shi, Hamish Tomlinson, Roma Patel, Tao Tu, Ada Maksutaj Oflazer, Francesco Pongetti, Mingyao Yang, Adrien Ali Taïga, Vincent Perot, Nuo Wang Pierse, Feng Han, Yoel Drori, Iñaki Iturrate, Ayan Chakrabarti, Legg Yeung, Dave Dopson, Yi-ting Chen, Apoorv Kulshreshtha, Tongfei Guo, Philip Pham, Tal Schuster, Junquan Chen, Alex Polozov, Jinwei Xing, Huanjie Zhou, Praneeth Kacham, Doron Kukliansky, Antoine Miech, Sergey Yaroshenko, Ed Chi, Sholto Douglas, Hongliang Fei, Mathieu Blondel, Preethi Myla, Lior Madmoni, Xing Wu, Daniel Keysers, Kristian Kjems, Isabela Albuquerque, Lijun Yu, Joel D&#x27;sa, Michelle Plantan, Vlad Ionescu, Jaume Sanchez Elias, Abhirut Gupta, Manish Reddy Vuyyuru, Fred Alcober, Tong Zhou, Kaiyang Ji, Florian Hartmann, Subha Puttagunta, Hugo Song, Ehsan Amid, Anca Stefanoiu, Andrew Lee, Paul Pucciarelli, Emma Wang, Amit Raul, Slav Petrov, Isaac Tian, Valentin Anklin, Nana Nti, Victor Gomes, Max Schumacher, Grace Vesom, Alex Panagopoulos, Konstantinos Bousmalis, Daniel Andor, Josh Jacob, Yuan Zhang, Bill Rosgen, Matija Kecman, Matthew Tung, Alexandra Belias, Noah Goodman, Paul Covington, Brian Wieder, Nikita Saxena, Elnaz Davoodi, Muhuan Huang, Sharath Maddineni, Vincent Roulet, Folawiyo Campbell-Ajala, Pier Giuseppe Sessa, Xintian, Wu, Guangda Lai, Paul Collins, Alex Haig, Vytenis Sakenas, Xiaowei Xu, Marissa Giustina, Laurent El Shafey, Pichi Charoenpanit, Shefali Garg, Joshua Ainslie, Boone Severson, Montse Gonzalez Arenas, Shreya Pathak, Sujee Rajayogam, Jie Feng, Michiel Bakker, Sheng Li, Nevan Wichers, Jamie Rogers, Xinyang Geng, Yeqing Li, Rolf Jagerman, Chao Jia, Nadav Olmert, David Sharon, Matthew Mauger, Sandeep Mariserla, Hongxu Ma, Megha Mohabey, Kyuyeun Kim, Alek Andreev, Scott Pollom, Juliette Love, Vihan Jain, Priyanka Agrawal, Yannick Schroecker, Alisa Fortin, Manfred Warmuth, Ji Liu, Andrew Leach, Irina Blok, Ganesh Poomal Girirajan, Roee Aharoni, Benigno Uria, Andrei Sozanschi, Dan Goldberg, Lucian Ionita, Marco Tulio Ribeiro, Martin Zlocha, Vighnesh Birodkar, Sami Lachgar, Liangzhe Yuan, Himadri Choudhury, Matt Ginsberg, Fei Zheng, Gregory Dibb, Emily Graves, Swachhand Lokhande, Gabriel Rasskin, George-Cristian Muraru, Corbin Quick, Sandeep Tata, Pierre Sermanet, Aditya Chawla, Itay Karo, Yan Wang, Susan Zhang, Orgad Keller, Anca Dragan, Guolong Su, Ian Chou, Xi Liu, Yiqing Tao, Shruthi Prabhakara, Marc Wilson, Ruibo Liu, Shibo Wang, Georgie Evans, David Du, Alfonso Castaño, Gautam Prasad, Mona El Mahdy, Sebastian Gerlach, Machel Reid, Jarrod Kahn, Amir Zait, Thanumalayan Sankaranarayana Pillai, Thatcher Ulrich, Guanyu Wang, Jan Wassenberg, Efrat Farkash, Kiran Yalasangi, Congchao Wang, Maria Bauza, Simon Bucher, Ting Liu, Jun Yan, Gary Leung, Vikas Sindhwani, Parker Barnes, Avi Singh, Ivan Jurin, Jichuan Chang, Niket Kumar Bhumihar, Sivan Eiger, Gui Citovsky, Ben Withbroe, Zhang Li, Siyang Xue, Niccolò Dal Santo, Georgi Stoyanov, Yves Raimond, Steven Zheng, Yilin Gao, Vít Listík, Sławek Kwasiborski, Rachel Saputro, Adnan Ozturel, Ganesh Mallya, Kushal Majmundar, Ross West, Paul Caron, Jinliang Wei, Lluis Castrejon, Sharad Vikram, Deepak Ramachandran, Nikhil Dhawan, Jiho Park, Sara Smoot, George van den Driessche, Yochai Blau, Chase Malik, Wei Liang, Roy Hirsch, Cicero Nogueira dos Santos, Eugene Weinstein, Aäron van den Oord, Sid Lall, Nicholas FitzGerald, Zixuan Jiang, Xuan Yang, Dale Webster, Ali Elqursh, Aedan Pope, Georges Rotival, David Raposo, Wanzheng Zhu, Jeff Dean, Sami Alabed, Dustin Tran, Arushi Gupta, Zach Gleicher, Jessica Austin, Edouard Rosseel, Megh Umekar, Dipanjan Das, Yinghao Sun, Kai Chen, Karolis Misiunas, Xiang Zhou, Yixian Di, Alyssa Loo, Josh Newlan, Bo Li, Vinay Ramasesh, Ying Xu, Alex Chen, Sudeep Gandhe, Radu Soricut, Nikita Gupta, Shuguang Hu, Seliem El-Sayed, Xavier Garcia, Idan Brusilovsky, Pu-Chin Chen, Andrew Bolt, Lu Huang, Alex Gurney, Zhiying Zhang, Alexander Pritzel, Jarek Wilkiewicz, Bryan Seybold, Bhargav Kanagal Shamanna, Felix Fischer, Josef Dean, Karan Gill, Ross Mcilroy, Abhishek Bhowmick, Jeremy Selier, Antoine Yang, Derek Cheng, Vladimir Magay, Jie Tan, Dhriti Varma, Christian Walder, Tomas Kocisky, Ryo Nakashima, Paul Natsev, Mike Kwong, Ionel Gog, Chiyuan Zhang, Sander Dieleman, Thomas Jimma, Andrey Ryabtsev, Siddhartha Brahma, David Steiner, Dayou Du, Ante Žužul, Mislav Žanić, Mukund Raghavachari, Willi Gierke, Zeyu Zheng, Dessie Petrova, Yann Dauphin, Yuchuan Liu, Ido Kessler, Steven Hand, Chris Duvarney, Seokhwan Kim, Hyo Lee, Léonard Hussenot, Jeffrey Hui, Josh Smith, Deepali Jain, Jiawei Xia, Gaurav Singh Tomar, Keyvan Amiri, Du Phan, Fabian Fuchs, Tobias Weyand, Nenad Tomasev, Alexandra Cordell, Xin Liu, Jonathan Mallinson, Pankaj Joshi, Andy Crawford, Arun Suggala, Steve Chien, Nick Fernando, Mariella Sanchez-Vargas, Duncan Williams, Phil Crone, Xiyang Luo, Igor Karpov, Jyn Shan, Terry Thurk, Robin Strudel, Paul Voigtlaender, Piyush Patil, Tim Dozat, Ali Khodaei, Sahil Singla, Piotr Ambroszczyk, Qiyin Wu, Yifan Chang, Brian Roark, Chaitra Hegde, Tianli Ding, Angelos Filos, Zhongru Wu, André Susano Pinto, Shuang Liu, Saarthak Khanna, Aditya Pandey, Siobhan Mcloughlin, Qiujia Li, Sam Haves, Allan Zhou, Elena Buchatskaya, Isabel Leal, Peter de Boursac, Nami Akazawa, Nina Anderson, Terry Chen, Krishna Somandepalli, Chen Liang, Sheela Goenka, Stephanie Winkler, Alexander Grushetsky, Yifan Ding, Jamie Smith, Fan Ye, Jordi Pont-Tuset, Eric Li, Ruichao Li, Tomer Golany, Dawid Wegner, Tao Jiang, Omer Barak, Yuan Shangguan, Eszter Vértes, Renee Wong, Jörg Bornschein, Alex Tudor, Michele Bevilacqua, Tom Schaul, Ankit Singh Rawat, Yang Zhao, Kyriakos Axiotis, Lei Meng, Cory McLean, Jonathan Lai, Jennifer Beattie, Nate Kushman, Yaxin Liu, Blair Kutzman, Fiona Lang, Jingchen Ye, Praneeth Netrapalli, Pushkar Mishra, Myriam Khan, Megha Goel, Rob Willoughby, David Tian, Honglei Zhuang, JD Chen, Zak Tsai, Tasos Kementsietsidis, Arjun Khare, James Keeling, Keyang Xu, Nathan Waters, Florent Altché, Ashok Popat, Bhavishya Mittal, David Saxton, Dalia El Badawy, Michael Mathieu, Zheng Zheng, Hao Zhou, Nishant Ranka, Richard Shin, Qingnan Duan, Tim Salimans, Ioana Mihailescu, Uri Shaham, Ming-Wei Chang, Yannis Assael, Nishanth Dikkala, Martin Izzard, Vincent Cohen-Addad, Cat Graves, Vlad Feinberg, Grace Chung, DJ Strouse, Danny Karmon, Sahand Sharifzadeh, Zoe Ashwood, Khiem Pham, Jon Blanton, Alex Vasiloff, Jarred Barber, Mark Geller, Aurick Zhou, Fedir Zubach, Tzu-Kuo Huang, Lei Zhang, Himanshu Gupta, Matt Young, Julia Proskurnia, Ronny Votel, Valentin Gabeur, Gabriel Barcik, Aditya Tripathi, Hongkun Yu, Geng Yan, Beer Changpinyo, Filip Pavetić, Amy Coyle, Yasuhisa Fujii, Jorge Gonzalez Mendez, Tianhao Zhou, Harish Rajamani, Blake Hechtman, Eddie Cao, Da-Cheng Juan, Yi-Xuan Tan, Valentin Dalibard, Yilun Du, Natalie Clay, Kaisheng Yao, Wenhao Jia, Dimple Vijaykumar, Yuxiang Zhou, Xinyi Bai, Wei-Chih Hung, Steven Pecht, Georgi Todorov, Nikhil Khadke, Pramod Gupta, Preethi Lahoti, Arnaud Autef, Karthik Duddu, James Lee-Thorp, Alexander Bykovsky, Tautvydas Misiunas, Sebastian Flennerhag, Santhosh Thangaraj, Jed McGiffin, Zack Nado, Markus Kunesch, Andreas Noever, Amir Hertz, Marco Liang, Victor Stone, Evan Palmer, Samira Daruki, Arijit Pramanik, Siim Põder, Austin Kyker, Mina Khan, Evgeny Sluzhaev, Marvin Ritter, Avraham Ruderman, Wenlei Zhou, Chirag Nagpal, Kiran Vodrahalli, George Necula, Paul Barham, Ellie Pavlick, Jay Hartford, Izhak Shafran, Long Zhao, Maciej Mikuła, Tom Eccles, Hidetoshi Shimokawa, Kanav Garg, Luke Vilnis, Hanwen Chen, Ilia Shumailov, Kuang-Huei Lee, Abdelrahman Abdelhamed, Meiyan Xie, Vered Cohen, Ester Hlavnova, Dan Malkin, Chawin Sitawarin, James Lottes, Pauline Coquinot, Tianli Yu, Sandeep Kumar, Jingwei Zhang, Aroma Mahendru, Zafarali Ahmed, James Martens, Tao Chen, Aviel Boag, Daiyi Peng, Coline Devin, Arseniy Klimovskiy, Mary Phuong, Danny Vainstein, Jin Xie, Bhuvana Ramabhadran, Nathan Howard, Xinxin Yu, Gitartha Goswami, Jingyu Cui, Sam Shleifer, Mario Pinto, Chih-Kuan Yeh, Ming-Hsuan Yang, Sara Javanmardi, Dan Ethier, Chace Lee, Jordi Orbay, Suyog Kotecha, Carla Bromberg, Pete Shaw, James Thornton, Adi Gerzi Rosenthal, Shane Gu, Matt Thomas, Ian Gemp, Aditya Ayyar, Asahi Ushio, Aarush Selvan, Joel Wee, Chenxi Liu, Maryam Majzoubi, Weiren Yu, Jake Abernethy, Tyler Liechty, Renke Pan, Hoang Nguyen, Qiong, Hu, Sarah Perrin, Abhinav Arora, Emily Pitler, Weiyi Wang, Kaushik Shivakumar, Flavien Prost, Ben Limonchik, Jing Wang, Yi Gao, Timothee Cour, Shyamal Buch, Huan Gui, Maria Ivanova, Philipp Neubeck, Kelvin Chan, Lucy Kim, Huizhong Chen, Naman Goyal, Da-Woon Chung, Lu Liu, Yao Su, Anastasia Petrushkina, Jiajun Shen, Armand Joulin, Yuanzhong Xu, Stein Xudong Lin, Yana Kulizhskaya, Ciprian Chelba, Shobha Vasudevan, Eli Collins, Vasilisa Bashlovkina, Tony Lu, Doug Fritz, Jongbin Park, Yanqi Zhou, Chen Su, Richard Tanburn, Mikhail Sushkov, Mitchelle Rasquinha, Jinning Li, Jennifer Prendki, Yiming Li, Pallavi LV, Shriya Sharma, Hen Fitoussi, Hui Huang, Andrew Dai, Phuong Dao, Mike Burrows, Henry Prior, Danfeng Qin, Golan Pundak, Lars Lowe Sjoesund, Art Khurshudov, Zhenkai Zhu, Albert Webson, Elizabeth Kemp, Tat Tan, Saurabh Agrawal, Susie Sargsyan, Liqun Cheng, Jim Stephan, Tom Kwiatkowski, David Reid, Arunkumar Byravan, Assaf Hurwitz Michaely, Nicolas Heess, Luowei Zhou, Sonam Goenka, Viral Carpenter, Anselm Levskaya, Bo Wang, Reed Roberts, Rémi Leblond, Sharat Chikkerur, Stav Ginzburg, Max Chang, Robert Riachi, Chuqiao, Xu, Zalán Borsos, Michael Pliskin, Julia Pawar, Morgane Lustman, Hannah Kirkwood, Ankit Anand, Aditi Chaudhary, Norbert Kalb, Kieran Milan, Sean Augenstein, Anna Goldie, Laurel Prince, Karthik Raman, Yanhua Sun, Vivian Xia, Aaron Cohen, Zhouyuan Huo, Josh Camp, Seher Ellis, Lukas Zilka, David Vilar Torres, Lisa Patel, Sho Arora, Betty Chan, Jonas Adler, Kareem Ayoub, Jacky Liang, Fayaz Jamil, Jiepu Jiang, Simon Baumgartner, Haitian Sun, Yael Karov, Yaroslav Akulov, Hui Zheng, Irene Cai, Claudio Fantacci, James Rubin, Alex Rav Acha, Mengchao Wang, Nina D&#x27;Souza, Rohit Sathyanarayana, Shengyang Dai, Simon Rowe, Andrey Simanovsky, Omer Goldman, Yuheng Kuang, Xiaoyue Pan, Andrew Rosenberg, Tania Rojas-Esponda, Praneet Dutta, Amy Zeng, Irina Jurenka, Greg Farquhar, Yamini Bansal, Shariq Iqbal, Becca Roelofs, Ga-Young Joung, Parker Beak, Changwan Ryu, Ryan Poplin, Yan Wu, Jean-Baptiste Alayrac, Senaka Buthpitiya, Olaf Ronneberger, Caleb Habtegebriel, Wei Li, Paul Cavallaro, Aurora Wei, Guy Bensky, Timo Denk, Harish Ganapathy, Jeff Stanway, Pratik Joshi, Francesco Bertolini, Jessica Lo, Olivia Ma, Zachary Charles, Geta Sampemane, Himanshu Sahni, Xu Chen, Harry Askham, David Gaddy, Peter Young, Jiewen Tan, Matan Eyal, Arthur Bražinskas, Li Zhong, Zhichun Wu, Mark Epstein, Kai Bailey, Andrew Hard, Kamyu Lee, Sasha Goldshtein, Alex Ruiz, Mohammed Badawi, Matthias Lochbrunner, JK Kearns, Ashley Brown, Fabio Pardo, Theophane Weber, Haichuan Yang, Pan-Pan Jiang, Berkin Akin, Zhao Fu, Marcus Wainwright, Chi Zou, Meenu Gaba, Pierre-Antoine Manzagol, Wendy Kan, Yang Song, Karina Zainullina, Rui Lin, Jeongwoo Ko, Salil Deshmukh, Apoorv Jindal, James Svensson, Divya Tyam, Heri Zhao, Christine Kaeser-Chen, Scott Baird, Pooya Moradi, Jamie Hall, Qiuchen Guo, Vincent Tsang, Bowen Liang, Fernando Pereira, Suhas Ganesh, Ivan Korotkov, Jakub Adamek, Sridhar Thiagarajan, Vinh Tran, Charles Chen, Chris Tar, Sanil Jain, Ishita Dasgupta, Taylan Bilal, David Reitter, Kai Zhao, Giulia Vezzani, Yasmin Gehman, Pulkit Mehta, Lauren Beltrone, Xerxes Dotiwalla, Sergio Guadarrama, Zaheer Abbas, Stefani Karp, Petko Georgiev, Chun-Sung Ferng, Marc Brockschmidt, Liqian Peng, Christoph Hirnschall, Vikas Verma, Yingying Bi, Ying Xiao, Avigail Dabush, Kelvin Xu, Phil Wallis, Randall Parker, Qifei Wang, Yang Xu, Ilkin Safarli, Dinesh Tewari, Yin Zhang, Seungyeon Kim, Andrea Gesmundo, Mackenzie Thomas, Sergey Levi, Ahmed Chowdhury, Kanishka Rao, Peter Garst, Sam Conway-Rahman, Helen Ran, Kay McKinney, Zhisheng Xiao, Wenhao Yu, Rohan Agrawal, Axel Stjerngren, Catalin Ionescu, Jingjing Chen, Vivek Sharma, Justin Chiu, Fei Liu, Ken Franko, Clayton Sanford, Xingyu Cai, Paul Michel, Sanjay Ganapathy, Jane Labanowski, Zachary Garrett, Ben Vargas, Sean Sun, Bryan Gale, Thomas Buschmann, Guillaume Desjardins, Nimesh Ghelani, Palak Jain, Mudit Verma, Chulayuth Asawaroengchai, Julian Eisenschlos, Jitendra Harlalka, Hideto Kazawa, Don Metzler, Joshua Howland, Ying Jian, Jake Ades, Viral Shah, Tynan Gangwani, Seungji Lee, Roman Ring, Steven M. Hernandez, Dean Reich, Amer Sinha, Ashutosh Sathe, Joe Kovac, Ashleah Gill, Ajay Kannan, Andrea D&#x27;olimpio, Martin Sevenich, Jay Whang, Been Kim, Khe Chai Sim, Jilin Chen, Jiageng Zhang, Shuba Lall, Yossi Matias, Bill Jia, Abe Friesen, Sara Nasso, Ashish Thapliyal, Bryan Perozzi, Ting Yu, Anna Shekhawat, Safeen Huda, Peter Grabowski, Eric Wang, Ashwin Sreevatsa, Hilal Dib, Mehadi Hassen, Parker Schuh, Vedrana Milutinovic, Chris Welty, Michael Quinn, Ali Shah, Bangju Wang, Gabe Barth-Maron, Justin Frye, Natalie Axelsson, Tao Zhu, Yukun Ma, Irene Giannoumis, Hanie Sedghi, Chang Ye, Yi Luan, Kevin Aydin, Bilva Chandra, Vivek Sampathkumar, Ronny Huang, Victor Lavrenko, Ahmed Eleryan, Zhi Hong, Steven Hansen, Sara Mc Carthy, Bidisha Samanta, Domagoj Ćevid, Xin Wang, Fangtao Li, Michael Voznesensky, Matt Hoffman, Andreas Terzis, Vikash Sehwag, Gil Fidel, Luheng He, Mu Cai, Yanzhang He, Alex Feng, Martin Nikoltchev, Samrat Phatale, Jason Chase, Rory Lawton, Ming Zhang, Tom Ouyang, Manuel Tragut, Mehdi Hafezi Manshadi, Arjun Narayanan, Jiaming Shen, Xu Gao, Tolga Bolukbasi, Nick Roy, Xin Li, Daniel Golovin, Liviu Panait, Zhen Qin, Guangxing Han, Thomas Anthony, Sneha Kudugunta, Viorica Patraucean, Aniket Ray, Xinyun Chen, Xiaochen Yang, Tanuj Bhatia, Pranav Talluri, Alex Morris, Andrija Ražnatović, Bethanie Brownfield, James An, Sheng Peng, Patrick Kane, Ce Zheng, Nico Duduta, Joshua Kessinger, James Noraky, Siqi Liu, Keran Rong, Petar Veličković, Keith Rush, Alex Goldin, Fanny Wei, Shiva Mohan Reddy Garlapati, Caroline Pantofaru, Okwan Kwon, Jianmo Ni, Eric Noland, Julia Di Trapani, Françoise Beaufays, Abhijit Guha Roy, Yinlam Chow, Aybuke Turker, Geoffrey Cideron, Lantao Mei, Jon Clark, Qingyun Dou, Matko Bošnjak, Ralph Leith, Yuqing Du, Amir Yazdanbakhsh, Milad Nasr, Chester Kwak, Suraj Satishkumar Sheth, Alex Kaskasoli, Ankesh Anand, Balaji Lakshminarayanan, Sammy Jerome, David Bieber, Chun-Te Chu, Alexandre Senges, Tianxiao Shen, Mukund Sridhar, Ndaba Ndebele, Benjamin Beyret, Shakir Mohamed, Mia Chen, Markus Freitag, Jiaxian Guo, Luyang Liu, Paul Roit, Heng Chen, Shen Yan, Tom Stone, JD Co-Reyes, Jeremy Cole, Salvatore Scellato, Shekoofeh Azizi, Hadi Hashemi, Alicia Jin, Anand Iyer, Marcella Valentine, András György, Arun Ahuja, Daniel Hernandez Diaz, Chen-Yu Lee, Nathan Clement, Weize Kong, Drew Garmon, Ishaan Watts, Kush Bhatia, Khyatti Gupta, Matt Miecnikowski, Hugo Vallet, Ankur Taly, Edward Loper, Saket Joshi, James Atwood, Jo Chick, Mark Collier, Fotis Iliopoulos, Ryan Trostle, Beliz Gunel, Ramiro Leal-Cavazos, Arnar Mar Hrafnkelsson, Michael Guzman, Xiaoen Ju, Andy Forbes, Jesse Emond, Kushal Chauhan, Ben Caine, Li Xiao, Wenjun Zeng, Alexandre Moufarek, Daniel Murphy, Maya Meng, Nitish Gupta, Felix Riedel, Anil Das, Elijah Lawal, Shashi Narayan, Tiberiu Sosea, James Swirhun, Linda Friso, Behnam Neyshabur, Jing Lu, Sertan Girgin, Michael Wunder, Edouard Yvinec, Aroonalok Pyne, Victor Carbune, Shruti Rijhwani, Yang Guo, Tulsee Doshi, Anton Briukhov, Max Bain, Ayal Hitron, Xuanhui Wang, Ashish Gupta, Ke Chen, Cosmo Du, Weiyang Zhang, Dhruv Shah, Arjun Akula, Max Dylla, Ashyana Kachra, Weicheng Kuo, Tingting Zou, Lily Wang, Luyao Xu, Jifan Zhu, Justin Snyder, Sachit Menon, Orhan Firat, Igor Mordatch, Yuan Yuan, Natalia Ponomareva, Rory Blevins, Lawrence Moore, Weijun Wang, Phil Chen, Martin Scholz, Artur Dwornik, Jason Lin, Sicheng Li, Diego Antognini, Te I, Xiaodan Song, Matt Miller, Uday Kalra, Adam Raveret, Oscar Akerlund, Felix Wu, Andrew Nystrom, Namrata Godbole, Tianqi Liu, Hannah DeBalsi, Jewel Zhao, Buhuang Liu, Avi Caciularu, Lauren Lax, Urvashi Khandelwal, Victoria Langston, Eric Bailey, Silvio Lattanzi, Yufei Wang, Neel Kovelamudi, Sneha Mondal, Guru Guruganesh, Nan Hua, Ofir Roval, Paweł Wesołowski, Rishikesh Ingale, Jonathan Halcrow, Tim Sohn, Christof Angermueller, Bahram Raad, Eli Stickgold, Eva Lu, Alec Kosik, Jing Xie, Timothy Lillicrap, Austin Huang, Lydia Lihui Zhang, Dominik Paulus, Clement Farabet, Alex Wertheim, Bing Wang, Rishabh Joshi, Chu-ling Ko, Yonghui Wu, Shubham Agrawal, Lily Lin, XiangHai Sheng, Peter Sung, Tyler Breland-King, Christina Butterfield, Swapnil Gawde, Sumeet Singh, Qiao Zhang, Raj Apte, Shilpa Shetty, Adrian Hutter, Tao Li, Elizabeth Salesky, Federico Lebron, Jonni Kanerva, Michela Paganini, Arthur Nguyen, Rohith Vallu, Jan-Thorsten Peter, Sarmishta Velury, David Kao, Jay Hoover, Anna Bortsova, Colton Bishop, Shoshana Jakobovits, Alessandro Agostini, Alekh Agarwal, Chang Liu, Charles Kwong, Sasan Tavakkol, Ioana Bica, Alex Greve, Anirudh GP, Jake Marcus, Le Hou, Tom Duerig, Rivka Moroshko, Dave Lacey, Andy Davis, Julien Amelot, Guohui Wang, Frank Kim, Theofilos Strinopoulos, Hui Wan, Charline Le Lan, Shankar Krishnan, Haotian Tang, Peter Humphreys, Junwen Bai, Idan Heimlich Shtacher, Diego Machado, Chenxi Pang, Ken Burke, Dangyi Liu, Renga Aravamudhan, Yue Song, Ed Hirst, Abhimanyu Singh, Brendan Jou, Liang Bai, Francesco Piccinno, Chuyuan Kelly Fu, Robin Alazard, Barak Meiri, Daniel Winter, Charlie Chen, Mingda Zhang, Jens Heitkaemper, John Lambert, Jinhyuk Lee, Alexander Frömmgen, Sergey Rogulenko, Pranav Nair, Paul Niemczyk, Anton Bulyenov, Bibo Xu, Hadar Shemtov, Morteza Zadimoghaddam, Serge Toropov, Mateo Wirth, Hanjun Dai, Sreenivas Gollapudi, Daniel Zheng, Alex Kurakin, Chansoo Lee, Kalesha Bullard, Nicolas Serrano, Ivana Balazevic, Yang Li, Johan Schalkwyk, Mark Murphy, Mingyang Zhang, Kevin Sequeira, Romina Datta, Nishant Agrawal, Charles Sutton, Nithya Attaluri, Mencher Chiang, Wael Farhan, Gregory Thornton, Kate Lin, Travis Choma, Hung Nguyen, Kingshuk Dasgupta, Dirk Robinson, Iulia Comşa, Michael Riley, Arjun Pillai, Basil Mustafa, Ben Golan, Amir Zandieh, Jean-Baptiste Lespiau, Billy Porter, David Ross, Sujeevan Rajayogam, Mohit Agarwal, Subhashini Venugopalan, Bobak Shahriari, Qiqi Yan, Hao Xu, Taylor Tobin, Pavel Dubov, Hongzhi Shi, Adrià Recasens, Anton Kovsharov, Sebastian Borgeaud, Lucio Dery, Shanthal Vasanth, Elena Gribovskaya, Linhai Qiu, Mahdis Mahdieh, Wojtek Skut, Elizabeth Nielsen, CJ Zheng, Adams Yu, Carrie Grimes Bostock, Shaleen Gupta, Aaron Archer, Chris Rawles, Elinor Davies, Alexey Svyatkovskiy, Tomy Tsai, Yoni Halpern, Christian Reisswig, Bartek Wydrowski, Bo Chang, Joan Puigcerver, Mor Hazan Taege, Jian Li, Eva Schnider, Xinjian Li, Dragos Dena, Yunhan Xu, Umesh Telang, Tianze Shi, Heiga Zen, Kyle Kastner, Yeongil Ko, Neesha Subramaniam, Aviral Kumar, Pete Blois, Zhuyun Dai, John Wieting, Yifeng Lu, Yoel Zeldes, Tian Xie, Anja Hauth, Alexandru Ţifrea, Yuqi Li, Sam El-Husseini, Dan Abolafia, Howard Zhou, Wen Ding, Sahra Ghalebikesabi, Carlos Guía, Andrii Maksai, Ágoston Weisz, Sercan Arik, Nick Sukhanov, Aga Świetlik, Xuhui Jia, Luo Yu, Weiyue Wang, Mark Brand, Dawn Bloxwich, Sean Kirmani, Zhe Chen, Alec Go, Pablo Sprechmann, Nithish Kannen, Alen Carin, Paramjit Sandhu, Isabel Edkins, Leslie Nooteboom, Jai Gupta, Loren Maggiore, Javad Azizi, Yael Pritch, Pengcheng Yin, Mansi Gupta, Danny Tarlow, Duncan Smith, Desi Ivanov, Mohammad Babaeizadeh, Ankita Goel, Satish Kambala, Grace Chu, Matej Kastelic, Michelle Liu, Hagen Soltau, Austin Stone, Shivani Agrawal, Min Kim, Kedar Soparkar, Srinivas Tadepalli, Oskar Bunyan, Rachel Soh, Arvind Kannan, DY Kim, Blake JianHang Chen, Afief Halumi, Sudeshna Roy, Yulong Wang, Olcan Sercinoglu, Gena Gibson, Sijal Bhatnagar, Motoki Sano, Daniel von Dincklage, Qingchun Ren, Blagoj Mitrevski, Mirek Olšák, Jennifer She, Carl Doersch, Jilei, Wang, Bingyuan Liu, Qijun Tan, Tamar Yakar, Tris Warkentin, Alex Ramirez, Carl Lebsack, Josh Dillon, Rajiv Mathews, Tom Cobley, Zelin Wu, Zhuoyuan Chen, Jon Simon, Swaroop Nath, Tara Sainath, Alexei Bendebury, Ryan Julian, Bharath Mankalale, Daria Ćurko, Paulo Zacchello, Adam R. Brown, Kiranbir Sodhia, Heidi Howard, Sergi Caelles, Abhinav Gupta, Gareth Evans, Anna Bulanova, Lesley Katzen, Roman Goldenberg, Anton Tsitsulin, Joe Stanton, Benoit Schillings, Vitaly Kovalev, Corey Fry, Rushin Shah, Kuo Lin, Shyam Upadhyay, Cheng Li, Soroush Radpour, Marcello Maggioni, Jing Xiong, Lukas Haas, Jenny Brennan, Aishwarya Kamath, Nikolay Savinov, Arsha Nagrani, Trevor Yacovone, Ryan Kappedal, Kostas Andriopoulos, Li Lao, YaGuang Li, Grigory Rozhdestvenskiy, Kazuma Hashimoto, Andrew Audibert, Sophia Austin, Daniel Rodriguez, Anian Ruoss, Garrett Honke, Deep Karkhanis, Xi Xiong, Qing Wei, James Huang, Zhaoqi Leng, Vittal Premachandran, Stan Bileschi, Georgios Evangelopoulos, Thomas Mensink, Jay Pavagadhi, Denis Teplyashin, Paul Chang, Linting Xue, Garrett Tanzer, Sally Goldman, Kaushal Patel, Shixin Li, Jeremy Wiesner, Ivy Zheng, Ian Stewart-Binks, Jie Han, Zhi Li, Liangchen Luo, Karel Lenc, Mario Lučić, Fuzhao Xue, Ryan Mullins, Alexey Guseynov, Chung-Ching Chang, Isaac Galatzer-Levy, Adam Zhang, Garrett Bingham, Grace Hu, Ale Hartman, Yue Ma, Jordan Griffith, Alex Irpan, Carey Radebaugh, Summer Yue, Lijie Fan, Victor Ungureanu, Christina Sorokin, Hannah Teufel, Peiran Li, Rohan Anil, Dimitris Paparas, Todd Wang, Chu-Cheng Lin, Hui Peng, Megan Shum, Goran Petrovic, Demetra Brady, Richard Nguyen, Klaus Macherey, Zhihao Li, Harman Singh, Madhavi Yenugula, Mariko Iinuma, Xinyi Chen, Kavya Kopparapu, Alexey Stern, Shachi Dave, Chandu Thekkath, Florence Perot, Anurag Kumar, Fangda Li, Yang Xiao, Matthew Bilotti, Mohammad Hossein Bateni, Isaac Noble, Lisa Lee, Amelio Vázquez-Reina, Julian Salazar, Xiaomeng Yang, Boyu Wang, Ela Gruzewska, Anand Rao, Sindhu Raghuram, Zheng Xu, Eyal Ben-David, Jieru Mei, Sid Dalmia, Zhaoyi Zhang, Yuchen Liu, Gagan Bansal, Helena Pankov, Steven Schwarcz, Andrea Burns, Christine Chan, Sumit Sanghai, Ricky Liang, Ethan Liang, Antoine He, Amy Stuart, Arun Narayanan, Yukun Zhu, Christian Frank, Bahar Fatemi, Amit Sabne, Oran Lang, Indro Bhattacharya, Shane Settle, Maria Wang, Brendan McMahan, Andrea Tacchetti, Livio Baldini Soares, Majid Hadian, Serkan Cabi, Timothy Chung, Nikita Putikhin, Gang Li, Jeremy Chen, Austin Tarango, Henryk Michalewski, Mehran Kazemi, Hussain Masoom, Hila Sheftel, Rakesh Shivanna, Archita Vadali, Ramona Comanescu, Doug Reid, Joss Moore, Arvind Neelakantan, Michaël Sander, Jonathan Herzig, Aviv Rosenberg, Mostafa Dehghani, JD Choi, Michael Fink, Reid Hayes, Eric Ge, Shitao Weng, Chia-Hua Ho, John Karro, Kalpesh Krishna, Lam Nguyen Thiet, Amy Skerry-Ryan, Daniel Eppens, Marco Andreetto, Navin Sarma, Silvano Bonacina, Burcu Karagol Ayan, Megha Nawhal, Zhihao Shan, Mike Dusenberry, Shantanu Thakoor, Sagar Gubbi, Duc Dung Nguyen, Reut Tsarfaty, Samuel Albanie, Jovana Mitrović, Meet Gandhi, Bo-Juen Chen, Alessandro Epasto, Georgi Stephanov, Ye Jin, Samuel Gehman, Aida Amini, Jack Weber, Feryal Behbahani, Shawn Xu, Miltos Allamanis, Xi Chen, Myle Ott, Claire Sha, Michal Jastrzebski, Hang Qi, David Greene, Xinyi Wu, Abodunrinwa Toki, Daniel Vlasic, Jane Shapiro, Ragha Kotikalapudi, Zhe Shen, Takaaki Saeki, Sirui Xie, Albin Cassirer, Shikhar Bharadwaj, Tatsuya Kiyono, Srinadh Bhojanapalli, Elan Rosenfeld, Sam Ritter, Jieming Mao, João Gabriel Oliveira, Zoltan Egyed, Bernd Bandemer, Emilio Parisotto, Keisuke Kinoshita, Juliette Pluto, Petros Maniatis, Steve Li, Yaohui Guo, Golnaz Ghiasi, Jean Tarbouriech, Srimon Chatterjee, Julie Jin, Katrina, Xu, Jennimaria Palomaki, Séb Arnold, Madhavi Sewak, Federico Piccinini, Mohit Sharma, Ben Albrecht, Sean Purser-haskell, Ashwin Vaswani, Chongyan Chen, Matheus Wisniewski, Qin Cao, John Aslanides, Nguyet Minh Phu, Maximilian Sieb, Lauren Agubuzu, Anne Zheng, Daniel Sohn, Marco Selvi, Anders Andreassen, Krishan Subudhi, Prem Eruvbetine, Oliver Woodman, Tomas Mery, Sebastian Krause, Xiaoqi Ren, Xiao Ma, Jincheng Luo, Dawn Chen, Wei Fan, Henry Griffiths, Christian Schuler, Alice Li, Shujian Zhang, Jean-Michel Sarr, Shixin Luo, Riccardo Patana, Matthew Watson, Dani Naboulsi, Michael Collins, Sailesh Sidhwani, Emiel Hoogeboom, Sharon Silver, Emily Caveness, Xiaokai Zhao, Mikel Rodriguez, Maxine Deines, Libin Bai, Patrick Griffin, Marco Tagliasacchi, Emily Xue, Spandana Raj Babbula, Bo Pang, Nan Ding, Gloria Shen, Elijah Peake, Remi Crocker, Shubha Srinivas Raghvendra, Danny Swisher, Woohyun Han, Richa Singh, Ling Wu, Vladimir Pchelin, Tsendsuren Munkhdalai, Dana Alon, Geoff Bacon, Efren Robles, Jannis Bulian, Melvin Johnson, George Powell, Felipe Tiengo Ferreira, Yaoyiran Li, Frederik Benzing, Mihajlo Velimirović, Hubert Soyer, William Kong, Tony, Nguyên, Zhen Yang, Jeremiah Liu, Joost van Amersfoort, Daniel Gillick, Baochen Sun, Nathalie Rauschmayr, Katie Zhang, Serena Zhan, Tao Zhou, Alexey Frolov, Chengrun Yang, Denis Vnukov, Louis Rouillard, Hongji Li, Amol Mandhane, Nova Fallen, Rajesh Venkataraman, Clara Huiyi Hu, Jennifer Brennan, Jenny Lee, Jerry Chang, Martin Sundermeyer, Zhufeng Pan, Rosemary Ke, Simon Tong, Alex Fabrikant, William Bono, Jindong Gu, Ryan Foley, Yiran Mao, Manolis Delakis, Dhruva Bhaswar, Roy Frostig, Nick Li, Avital Zipori, Cath Hope, Olga Kozlova, Swaroop Mishra, Josip Djolonga, Craig Schiff, Majd Al Merey, Eleftheria Briakou, Peter Morgan, Andy Wan, Avinatan Hassidim, RJ Skerry-Ryan, Kuntal Sengupta, Mary Jasarevic, Praveen Kallakuri, Paige Kunkle, Hannah Brennan, Tom Lieber, Hassan Mansoor, Julian Walker, Bing Zhang, Annie Xie, Goran Žužić, Adaeze Chukwuka, Alex Druinsky, Donghyun Cho, Rui Yao, Ferjad Naeem, Shiraz Butt, Eunyoung Kim, Zhipeng Jia, Mandy Jordan, Adam Lelkes, Mark Kurzeja, Sophie Wang, James Zhao, Andrew Over, Abhishek Chakladar, Marcel Prasetya, Neha Jha, Sriram Ganapathy, Yale Cong, Prakash Shroff, Carl Saroufim, Sobhan Miryoosefi, Mohamed Hammad, Tajwar Nasir, Weijuan Xi, Yang Gao, Young Maeng, Ben Hora, Chin-Yi Cheng, Parisa Haghani, Yoad Lewenberg, Caden Lu, Martin Matysiak, Naina Raisinghani, Huiyu Wang, Lexi Baugher, Rahul Sukthankar, Minh Giang, John Schultz, Noah Fiedel, Minmin Chen, Cheng-Chun Lee, Tapomay Dey, Hao Zheng, Shachi Paul, Celine Smith, Andy Ly, Yicheng Wang, Rishabh Bansal, Bartek Perz, Susanna Ricco, Stasha Blank, Vaishakh Keshava, Deepak Sharma, Marvin Chow, Kunal Lad, Komal Jalan, Simon Osindero, Craig Swanson, Jacob Scott, Anastasija Ilić, Xiaowei Li, Siddhartha Reddy Jonnalagadda, Afzal Shama Soudagar, Yan Xiong, Bat-Orgil Batsaikhan, Daniel Jarrett, Naveen Kumar, Maulik Shah, Matt Lawlor, Austin Waters, Mark Graham, Rhys May, Sabela Ramos, Sandra Lefdal, Zeynep Cankara, Nacho Cano, Brendan O&#x27;Donoghue, Jed Borovik, Frederick Liu, Jordan Grimstad, Mahmoud Alnahlawi, Katerina Tsihlas, Tom Hudson, Nikolai Grigorev, Yiling Jia, Terry Huang, Tobenna Peter Igwe, Sergei Lebedev, Xiaodan Tang, Igor Krivokon, Frankie Garcia, Melissa Tan, Eric Jia, Peter Stys, Shikhar Vashishth, Yu Liang, Balaji Venkatraman, Chenjie Gu, Anastasios Kementsietsidis, Chen Zhu, Junehyuk Jung, Yunfei Bai, Mohammad Javad Hosseini, Faruk Ahmed, Aditya Gupta, Xin Yuan, Shereen Ashraf, Shitij Nigam, Gautam Vasudevan, Pranjal Awasthi, Adi Mayrav Gilady, Zelda Mariet, Ramy Eskander, Haiguang Li, Hexiang Hu, Guillermo Garrido, Philippe Schlattner, George Zhang, Rohun Saxena, Petar Dević, Kritika Muralidharan, Ashwin Murthy, Yiqian Zhou, Min Choi, Arissa Wongpanich, Zhengdong Wang, Premal Shah, Yuntao Xu, Yiling Huang, Stephen Spencer, Alice Chen, James Cohan, Junjie Wang, Jonathan Tompson, Junru Wu, Ruba Haroun, Haiqiong Li, Blanca Huergo, Fan Yang, Tongxin Yin, James Wendt, Michael Bendersky, Rahma Chaabouni, Javier Snaider, Johan Ferret, Abhishek Jindal, Tara Thompson, Andrew Xue, Will Bishop, Shubham Milind Phal, Archit Sharma, Yunhsuan Sung, Prabakar Radhakrishnan, Mo Shomrat, Reeve Ingle, Roopali Vij, Justin Gilmer, Mihai Dorin Istin, Sam Sobell, Yang Lu, Emily Nottage, Dorsa Sadigh, Jeremiah Willcock, Tingnan Zhang, Steve Xu, Sasha Brown, Katherine Lee, Gary Wang, Yun Zhu, Yi Tay, Cheolmin Kim, Audrey Gutierrez, Abhanshu Sharma, Yongqin Xian, Sungyong Seo, Claire Cui, Elena Pochernina, Cip Baetu, Krzysztof Jastrzębski, Mimi Ly, Mohamed Elhawaty, Dan Suh, Eren Sezener, Pidong Wang, Nancy Yuen, George Tucker, Jiahao Cai, Zuguang Yang, Cindy Wang, Alex Muzio, Hai Qian, Jae Yoo, Derek Lockhart, Kevin R. McKee, Mandy Guo, Malika Mehrotra, Artur Mendonça, Sanket Vaibhav Mehta, Sherry Ben, Chetan Tekur, Jiaqi Mu, Muye Zhu, Victoria Krakovna, Hongrae Lee, AJ Maschinot, Sébastien Cevey, HyunJeong Choe, Aijun Bai, Hansa Srinivasan, Derek Gasaway, Nick Young, Patrick Siegler, Dan Holtmann-Rice, Vihari Piratla, Kate Baumli, Roey Yogev, Alex Hofer, Hado van Hasselt, Svetlana Grant, Yuri Chervonyi, David Silver, Andrew Hogue, Ayushi Agarwal, Kathie Wang, Preeti Singh, Four Flynn, Josh Lipschultz, Robert David, Lizzetth Bellot, Yao-Yuan Yang, Long Le, Filippo Graziano, Kate Olszewska, Kevin Hui, Akanksha Maurya, Nikos Parotsidis, Weijie Chen, Tayo Oguntebi, Joe Kelley, Anirudh Baddepudi, Johannes Mauerer, Gregory Shaw, Alex Siegman, Lin Yang, Shravya Shetty, Subhrajit Roy, Yunting Song, Wojciech Stokowiec, Ryan Burnell, Omkar Savant, Robert Busa-Fekete, Jin Miao, Samrat Ghosh, Liam MacDermed, Phillip Lippe, Mikhail Dektiarev, Zach Behrman, Fabian Mentzer, Kelvin Nguyen, Meng Wei, Siddharth Verma, Chris Knutsen, Sudeep Dasari, Zhipeng Yan, Petr Mitrichev, Xingyu Wang, Virat Shejwalkar, Jacob Austin, Srinivas Sunkara, Navneet Potti, Yan Virin, Christian Wright, Gaël Liu, Oriana Riva, Etienne Pot, Greg Kochanski, Quoc Le, Gargi Balasubramaniam, Arka Dhar, Yuguo Liao, Adam Bloniarz, Divyansh Shukla, Elizabeth Cole, Jong Lee, Sheng Zhang, Sushant Kafle, Siddharth Vashishtha, Parsa Mahmoudieh, Grace Chen, Raphael Hoffmann, Pranesh Srinivasan, Agustin Dal Lago, Yoav Ben Shalom, Zi Wang, Michael Elabd, Anuj Sharma, Junhyuk Oh, Suraj Kothawade, Maigo Le, Marianne Monteiro, Shentao Yang, Kaiz Alarakyia, Robert Geirhos, Diana Mincu, Håvard Garnes, Hayato Kobayashi, Soroosh Mariooryad, Kacper Krasowiak, Zhixin, Lai, Shibl Mourad, Mingqiu Wang, Fan Bu, Ophir Aharoni, Guanjie Chen, Abhimanyu Goyal, Vadim Zubov, Ankur Bapna, Elahe Dabir, Nisarg Kothari, Kay Lamerigts, Nicola De Cao, Jeremy Shar, Christopher Yew, Nitish Kulkarni, Dre Mahaarachchi, Mandar Joshi, Zhenhai Zhu, Jared Lichtarge, Yichao Zhou, Hannah Muckenhirn, Vittorio Selo, Oriol Vinyals, Peter Chen, Anthony Brohan, Vaibhav Mehta, Sarah Cogan, Ruth Wang, Ty Geri, Wei-Jen Ko, Wei Chen, Fabio Viola, Keshav Shivam, Lisa Wang, Madeleine Clare Elish, Raluca Ada Popa, Sébastien Pereira, Jianqiao Liu, Raphael Koster, Donnie Kim, Gufeng Zhang, Sayna Ebrahimi, Partha Talukdar, Yanyan Zheng, Petra Poklukar, Ales Mikhalap, Dale Johnson, Anitha Vijayakumar, Mark Omernick, Matt Dibb, Ayush Dubey, Qiong Hu, Apurv Suman, Vaibhav Aggarwal, Ilya Kornakov, Fei Xia, Wing Lowe, Alexey Kolganov, Ted Xiao, Vitaly Nikolaev, Steven Hemingray, Bonnie Li, Joana Iljazi, Mikołaj Rybiński, Ballie Sandhu, Peggy Lu, Thang Luong, Rodolphe Jenatton, Vineetha Govindaraj, Hui, Li, Gabriel Dulac-Arnold, Wonpyo Park, Henry Wang, Abhinit Modi, Jean Pouget-Abadie, Kristina Greller, Rahul Gupta, Robert Berry, Prajit Ramachandran, Jinyu Xie, Liam McCafferty, Jianling Wang, Kilol Gupta, Hyeontaek Lim, Blaž Bratanič, Andy Brock, Ilia Akolzin, Jim Sproch, Dan Karliner, Duhyeon Kim, Adrian Goedeckemeyer, Noam Shazeer, Cordelia Schmid, Daniele Calandriello, Parul Bhatia, Krzysztof Choromanski, Ceslee Montgomery, Dheeru Dua, Ana Ramalho, Helen King, Yue Gao, Lynn Nguyen, David Lindner, Divya Pitta, Oleaser Johnson, Khalid Salama, Diego Ardila, Michael Han, Erin Farnese, Seth Odoom, Ziyue Wang, Xiangzhuo Ding, Norman Rink, Ray Smith, Harshal Tushar Lehri, Eden Cohen, Neera Vats, Tong He, Parthasarathy Gopavarapu, Adam Paszke, Miteyan Patel, Wouter Van Gansbeke, Lucia Loher, Luis Castro, Maria Voitovich, Tamara von Glehn, Nelson George, Simon Niklaus, Zach Eaton-Rosen, Nemanja Rakićević, Erik Jue, Sagi Perel, Carrie Zhang, Yuval Bahat, Angéline Pouget, Zhi Xing, Fantine Huot, Ashish Shenoy, Taylor Bos, Vincent Coriou, Bryan Richter, Natasha Noy, Yaqing Wang, Santiago Ontanon, Siyang Qin, Gleb Makarchuk, Demis Hassabis, Zhuowan Li, Mandar Sharma, Kumaran Venkatesan, Iurii Kemaev, Roxanne Daniel, Shiyu Huang, Saloni Shah, Octavio Ponce, Warren, Chen, Manaal Faruqui, Jialin Wu, Slavica Andačić, Szabolcs Payrits, Daniel McDuff, Tom Hume, Yuan Cao, MH Tessler, Qingze Wang, Yinan Wang, Ivor Rendulic, Eirikur Agustsson, Matthew Johnson, Tanya Lando, Andrew Howard, Sri Gayatri Sundara Padmanabhan, Mayank Daswani, Andrea Banino, Michael Kilgore, Jonathan Heek, Ziwei Ji, Alvaro Caceres, Conglong Li, Nora Kassner, Alexey Vlaskin, Zeyu Liu, Alex Grills, Yanhan Hou, Roykrong Sukkerd, Gowoon Cheon, Nishita Shetty, Larisa Markeeva, Piotr Stanczyk, Tejas Iyer, Yuan Gong, Shawn Gao, Keerthana Gopalakrishnan, Tim Blyth, Malcolm Reynolds, Avishkar Bhoopchand, Misha Bilenko, Dero Gharibian, Vicky Zayats, Aleksandra Faust, Abhinav Singh, Min Ma, Hongyang Jiao, Sudheendra Vijayanarasimhan, Lora Aroyo, Vikas Yadav, Sarah Chakera, Ashwin Kakarla, Vilobh Meshram, Karol Gregor, Gabriela Botea, Evan Senter, Dawei Jia, Geza Kovacs, Neha Sharma, Sebastien Baur, Kai Kang, Yifan He, Lin Zhuo, Marija Kostelac, Itay Laish, Songyou Peng, Louis O&#x27;Bryan, Daniel Kasenberg, Girish Ramchandra Rao, Edouard Leurent, Biao Zhang, Sage Stevens, Ana Salazar, Ye Zhang, Ivan Lobov, Jake Walker, Allen Porter, Morgan Redshaw, Han Ke, Abhishek Rao, Alex Lee, Hoi Lam, Michael Moffitt, Jaeyoun Kim, Siyuan Qiao, Terry Koo, Robert Dadashi, Xinying Song, Mukund Sundararajan, Peng Xu, Chizu Kawamoto, Yan Zhong, Clara Barbu, Apoorv Reddy, Mauro Verzetti, Leon Li, George Papamakarios, Hanna Klimczak-Plucińska, Mary Cassin, Koray Kavukcuoglu, Rigel Swavely, Alain Vaucher, Jeffrey Zhao, Ross Hemsley, Michael Tschannen, Heming Ge, Gaurav Menghani, Yang Yu, Natalie Ha, Wei He, Xiao Wu, Maggie Song, Rachel Sterneck, Stefan Zinke, Dan A. Calian, Annie Marsden, Alejandro Cruzado Ruiz, Matteo Hessel, Almog Gueta, Benjamin Lee, Brian Farris, Manish Gupta, Yunjie Li, Mohammad Saleh, Vedant Misra, Kefan Xiao, Piermaria Mendolicchio, Gavin Buttimore, Varvara Krayvanova, Nigamaa Nayakanti, Matthew Wiethoff, Yash Pande, Azalia Mirhoseini, Ni Lao, Jasmine Liu, Yiqing Hua, Angie Chen, Yury Malkov, Dmitry Kalashnikov, Shubham Gupta, Kartik Audhkhasi, Yuexiang Zhai, Sudhindra Kopalle, Prateek Jain, Eran Ofek, Clemens Meyer, Khuslen Baatarsukh, Hana Strejček, Jun Qian, James Freedman, Ricardo Figueira, Michal Sokolik, Olivier Bachem, Raymond Lin, Dia Kharrat, Chris Hidey, Pingmei Xu, Dennis Duan, Yin Li, Muge Ersoy, Richard Everett, Kevin Cen, Rebeca Santamaria-Fernandez, Amir Taubenfeld, Ian Mackinnon, Linda Deng, Polina Zablotskaia, Shashank Viswanadha, Shivanker Goel, Damion Yates, Yunxiao Deng, Peter Choy, Mingqing Chen, Abhishek Sinha, Alex Mossin, Yiming Wang, Arthur Szlam, Susan Hao, Paul Kishan Rubenstein, Metin Toksoz-Exley, Miranda Aperghis, Yin Zhong, Junwhan Ahn, Michael Isard, Olivier Lacombe, Florian Luisier, Chrysovalantis Anastasiou, Yogesh Kalley, Utsav Prabhu, Emma Dunleavy, Shaan Bijwadia, Justin Mao-Jones, Kelly Chen, Rama Pasumarthi, Emily Wood, Adil Dostmohamed, Nate Hurley, Jiri Simsa, Alicia Parrish, Mantas Pajarskas, Matt Harvey, Ondrej Skopek, Yony Kochinski, Javier Rey, Verena Rieser, Denny Zhou, Sun Jae Lee, Trilok Acharya, Guowang Li, Joe Jiang, Xiaofan Zhang, Bryant Gipson, Ethan Mahintorabi, Marco Gelmi, Nima Khajehnouri, Angel Yeh, Kayi Lee, Loic Matthey, Leslie Baker, Trang Pham, Han Fu, Alex Pak, Prakhar Gupta, Cristina Vasconcelos, Adam Sadovsky, Brian Walker, Sissie Hsiao, Patrik Zochbauer, Andreea Marzoca, Noam Velan, Junhao Zeng, Gilles Baechler, Danny Driess, Divya Jain, Yanping Huang, Lizzie Tao, John Maggs, Nir Levine, Jon Schneider, Erika Gemzer, Samuel Petit, Shan Han, Zach Fisher, Dustin Zelle, Courtney Biles, Eugene Ie, Asya Fadeeva, Casper Liu, Juliana Vicente Franco, Adrian Collister, Hao Zhang, Renshen Wang, Ruizhe Zhao, Leandro Kieliger, Kurt Shuster, Rui Zhu, Boqing Gong, Lawrence Chan, Ruoxi Sun, Sujoy Basu, Roland Zimmermann, Jamie Hayes, Abhishek Bapna, Jasper Snoek, Weel Yang, Puranjay Datta, Jad Al Abdallah, Kevin Kilgour, Lu Li, SQ Mah, Yennie Jun, Morgane Rivière, Abhijit Karmarkar, Tammo Spalink, Tao Huang, Lucas Gonzalez, Duc-Hieu Tran, Averi Nowak, John Palowitch, Martin Chadwick, Ellie Talius, Harsh Mehta, Thibault Sellam, Philipp Fränken, Massimo Nicosia, Kyle He, Aditya Kini, David Amos, Sugato Basu, Harrison Jobe, Eleni Shaw, Qiantong Xu, Colin Evans, Daisuke Ikeda, Chaochao Yan, Larry Jin, Lun Wang, Sachin Yadav, Ilia Labzovsky, Ramesh Sampath, Ada Ma, Candice Schumann, Aditya Siddhant, Rohin Shah, John Youssef, Rishabh Agarwal, Natalie Dabney, Alessio Tonioni, Moran Ambar, Jing Li, Isabelle Guyon, Benny Li, David Soergel, Boya Fang, Georgi Karadzhov, Cristian Udrescu, Trieu Trinh, Vikas Raunak, Seb Noury, Dee Guo, Sonal Gupta, Mara Finkelstein, Denis Petek, Lihao Liang, Greg Billock, Pei Sun, David Wood, Yiwen Song, Xiaobin Yu, Tatiana Matejovicova, Regev Cohen, Kalyan Andra, David D&#x27;Ambrosio, Zhiwei Deng, Vincent Nallatamby, Ebrahim Songhori, Rumen Dangovski, Andrew Lampinen, Pankil Botadra, Adam Hillier, Jiawei Cao, Nagabhushan Baddi, Adhi Kuncoro, Toshihiro Yoshino, Ankit Bhagatwala, Marcáurelio Ranzato, Rylan Schaeffer, Tianlin Liu, Shuai Ye, Obaid Sarvana, John Nham, Chenkai Kuang, Isabel Gao, Jinoo Baek, Shubham Mittal, Ayzaan Wahid, Anita Gergely, Bin Ni, Josh Feldman, Carrie Muir, Pascal Lamblin, Wolfgang Macherey, Ethan Dyer, Logan Kilpatrick, Víctor Campos, Mukul Bhutani, Stanislav Fort, Yanif Ahmad, Aliaksei Severyn, Kleopatra Chatziprimou, Oleksandr Ferludin, Mason Dimarco, Aditya Kusupati, Joe Heyward, Dan Bahir, Kevin Villela, Katie Millican, Dror Marcus, Sanaz Bahargam, Caglar Unlu, Nicholas Roth, Zichuan Wei, Siddharth Gopal, Deepanway Ghoshal, Edward Lee, Sharon Lin, Jennie Lees, Dayeong Lee, Anahita Hosseini, Connie Fan, Seth Neel, Marcus Wu, Yasemin Altun, Honglong Cai, Enrique Piqueras, Josh Woodward, Alessandro Bissacco, Salem Haykal, Mahyar Bordbar, Prasha Sundaram, Sarah Hodkinson, Daniel Toyama, George Polovets, Austin Myers, Anu Sinha, Tomer Levinboim, Kashyap Krishnakumar, Rachita Chhaparia, Tatiana Sholokhova, Nitesh Bharadwaj Gundavarapu, Ganesh Jawahar, Haroon Qureshi, Jieru Hu, Nikola Momchev, Matthew Rahtz, Renjie Wu, Aishwarya P S, Kedar Dhamdhere, Meiqi Guo, Umang Gupta, Ali Eslami, Mariano Schain, Michiel Blokzijl, David Welling, Dave Orr, Levent Bolelli, Nicolas Perez-Nieves, Mikhail Sirotenko, Aman Prasad, Arjun Kar, Borja De Balle Pigem, Tayfun Terzi, Gellért Weisz, Dipankar Ghosh, Aditi Mavalankar, Dhruv Madeka, Kaspar Daugaard, Hartwig Adam, Viraj Shah, Dana Berman, Maggie Tran, Steven Baker, Ewa Andrejczuk, Grishma Chole, Ganna Raboshchuk, Mahdi Mirzazadeh, Thais Kagohara, Shimu Wu, Christian Schallhart, Bernett Orlando, Chen Wang, Alban Rrustemi, Hao Xiong, Hao Liu, Arpi Vezer, Nolan Ramsden, Shuo-yiin Chang, Sidharth Mudgal, Yan Li, Nino Vieillard, Yedid Hoshen, Farooq Ahmad, Ambrose Slone, Amy Hua, Natan Potikha, Mirko Rossini, Jon Stritar, Sushant Prakash, Zifeng Wang, Xuanyi Dong, Alireza Nazari, Efrat Nehoran, Kaan Tekelioglu, Yinxiao Li, Kartikeya Badola, Tom Funkhouser, Yuanzhen Li, Varun Yerram, Ramya Ganeshan, Daniel Formoso, Karol Langner, Tian Shi, Huijian Li, Yumeya Yamamori, Amayika Panda, Alaa Saade, Angelo Scorza Scarpati, Chris Breaux, CJ Carey, Zongwei Zhou, Cho-Jui Hsieh, Sophie Bridgers, Alena Butryna, Nishesh Gupta, Vaibhav Tulsyan, Sanghyun Woo, Evgenii Eltyshev, Will Grathwohl, Chanel Parks, Seth Benjamin, Rina Panigrahy, Shenil Dodhia, Daniel De Freitas, Chris Sauer, Will Song, Ferran Alet, Jackson Tolins, Cosmin Paduraru, Xingyi Zhou, Brian Albert, Zizhao Zhang, Lei Shu, Mudit Bansal, Sarah Nguyen, Amir Globerson, Owen Xiao, James Manyika, Tom Hennigan, Rong Rong, Josip Matak, Anton Bakalov, Ankur Sharma, Danila Sinopalnikov, Andrew Pierson, Stephen Roller, Geoff Brown, Mingcen Gao, Toshiyuki Fukuzawa, Amin Ghafouri, Kenny Vassigh, Iain Barr, Zhicheng Wang, Anna Korsun, Rajesh Jayaram, Lijie Ren, Tim Zaman, Samira Khan, Yana Lunts, Dan Deutsch, Dave Uthus, Nitzan Katz, Masha Samsikova, Amr Khalifa, Nikhil Sethi, Jiao Sun, Luming Tang, Uri Alon, Xianghong Luo, Dian Yu, Abhishek Nayyar, Bryce Petrini, Will Truong, Vincent Hellendoorn, Nikolai Chinaev, Chris Alberti, Wei Wang, Jingcao Hu, Vahab Mirrokni, Ananth Balashankar, Avia Aharon, Aahil Mehta, Ahmet Iscen, Joseph Kready, Lucas Manning, Anhad Mohananey, Yuankai Chen, Anshuman Tripathi, Allen Wu, Igor Petrovski, Dawsen Hwang, Martin Baeuml, Shreyas Chandrakaladharan, Yuan Liu, Rey Coaguila, Maxwell Chen, Sally Ma, Pouya Tafti, Susheel Tatineni, Terry Spitz, Jiayu Ye, Paul Vicol, Mihaela Rosca, Adrià Puigdomènech, Zohar Yahav, Sanjay Ghemawat, Hanzhao Lin, Phoebe Kirk, Zaid Nabulsi, Sergey Brin, Bernd Bohnet, Ken Caluwaerts, Aditya Srikanth Veerubhotla, Dan Zheng, Zihang Dai, Petre Petrov, Yichong Xu, Ramin Mehran, Zhuo Xu, Luisa Zintgraf, Jiho Choi, Spurthi Amba Hombaiah, Romal Thoppilan, Sashank Reddi, Lukasz Lew, Li Li, Kellie Webster, KP Sawhney, Lampros Lamprou, Siamak Shakeri, Mayank Lunayach, Jianmin Chen, Sumit Bagri, Alex Salcianu, Ying Chen, Yani Donchev, Charlotte Magister, Signe Nørly, Vitor Rodrigues, Tomas Izo, Hila Noga, Joe Zou, Thomas Köppe, Wenxuan Zhou, Kenton Lee, Xiangzhu Long, Danielle Eisenbud, Anthony Chen, Connor Schenck, Chi Ming To, Peilin Zhong, Emanuel Taropa, Minh Truong, Omer Levy, Danilo Martins, Zhiyuan Zhang, Christopher Semturs, Kelvin Zhang, Alex Yakubovich, Pol Moreno, Lara McConnaughey, Di Lu, Sam Redmond, Lotte Weerts, Yonatan Bitton, Tiziana Refice, Nicolas Lacasse, Arthur Conmy, Corentin Tallec, Julian Odell, Hannah Forbes-Pollard, Arkadiusz Socala, Jonathan Hoech, Pushmeet Kohli, Alanna Walton, Rui Wang, Mikita Sazanovich, Kexin Zhu, Andrei Kapishnikov, Rich Galt, Matthew Denton, Ben Murdoch, Caitlin Sikora, Kareem Mohamed, Wei Wei, Uri First, Tim McConnell, Luis C. Cobo, James Qin, Thi Avrahami, Daniel Balle, Yu Watanabe, Annie Louis, Adam Kraft, Setareh Ariafar, Yiming Gu, Eugénie Rives, Charles Yoon, Andrei Rusu, James Cobon-Kerr, Chris Hahn, Jiaming Luo, Yuvein, Zhu, Niharika Ahuja, Rodrigo Benenson, Raphaël Lopez Kaufman, Honglin Yu, Lloyd Hightower, Junlin Zhang, Darren Ni, Lisa Anne Hendricks, Gabby Wang, Gal Yona, Lalit Jain, Pablo Barrio, Surya Bhupatiraju, Siva Velusamy, Allan Dafoe, Sebastian Riedel, Tara Thomas, Zhe Yuan, Mathias Bellaiche, Sheena Panthaplackel, Klemen Kloboves, Sarthak Jauhari, Canfer Akbulut, Todor Davchev, Evgeny Gladchenko, David Madras, Aleksandr Chuklin, Tyrone Hill, Quan Yuan, Mukundan Madhavan, Luke Leonhard, Dylan Scandinaro, Qihang Chen, Ning Niu, Arthur Douillard, Bogdan Damoc, Yasumasa Onoe, Fabian Pedregosa, Fred Bertsch, Chas Leichner, Joseph Pagadora, Jonathan Malmaud, Sameera Ponda, Andy Twigg, Oleksii Duzhyi, Jingwei Shen, Miaosen Wang, Roopal Garg, Jing Chen, Utku Evci, Jonathan Lee, Leon Liu, Koji Kojima, Masa Yamaguchi, Arunkumar Rajendran, AJ Piergiovanni, Vinodh Kumar Rajendran, Marco Fornoni, Gabriel Ibagon, Harry Ragan, Sadh MNM Khan, John Blitzer, Andrew Bunner, Guan Sun, Takahiro Kosakai, Scott Lundberg, Ndidi Elue, Kelvin Guu, SK Park, Jane Park, Arunachalam Narayanaswamy, Chengda Wu, Jayaram Mudigonda, Trevor Cohn, Hairong Mu, Ravi Kumar, Laura Graesser, Yichi Zhang, Richard Killam, Vincent Zhuang, Mai Giménez, Wael Al Jishi, Ruy Ley-Wild, Alex Zhai, Kazuki Osawa, Diego Cedillo, Jialu Liu, Mayank Upadhyay, Marcin Sieniek, Roshan Sharma, Tom Paine, Anelia Angelova, Sravanti Addepalli, Carolina Parada, Kingshuk Majumder, Avery Lamp, Sanjiv Kumar, Xiang Deng, Artiom Myaskovsky, Tea Sabolić, Jeffrey Dudek, Sarah York, Félix de Chaumont Quitry, Jiazhong Nie, Dee Cattle, Alok Gunjan, Bilal Piot, Waleed Khawaja, Seojin Bang, Simon Wang, Siavash Khodadadeh, Raghavender R, Praynaa Rawlani, Richard Powell, Kevin Lee, Johannes Griesser, GS Oh, Cesar Magalhaes, Yujia Li, Simon Tokumine, Hadas Natalie Vogel, Dennis Hsu, Arturo BC, Disha Jindal, Matan Cohen, Zi Yang, Junwei Yuan, Dario de Cesare, Tony Bruguier, Jun Xu, Monica Roy, Alon Jacovi, Dan Belov, Rahul Arya, Phoenix Meadowlark, Shlomi Cohen-Ganor, Wenting Ye, Patrick Morris-Suzuki, Praseem Banzal, Gan Song, Pranavaraj Ponnuramu, Fred Zhang, George Scrivener, Salah Zaiem, Alif Raditya Rochman, Kehang Han, Badih Ghazi, Kate Lee, Shahar Drath, Daniel Suo, Antonious Girgis, Pradeep Shenoy, Duy Nguyen, Douglas Eck, Somit Gupta, Le Yan, Joao Carreira, Anmol Gulati, Ruoxin Sang, Daniil Mirylenka, Emma Cooney, Edward Chou, Mingyang Ling, Cindy Fan, Ben Coleman, Guilherme Tubone, Ravin Kumar, Jason Baldridge, Felix Hernandez-Campos, Angeliki Lazaridou, James Besley, Itay Yona, Neslihan Bulut, Quentin Wellens, AJ Pierigiovanni, Jasmine George, Richard Green, Pu Han, Connie Tao, Geoff Clark, Chong You, Abbas Abdolmaleki, Justin Fu, Tongzhou Chen, Ashwin Chaugule, Angad Chandorkar, Altaf Rahman, Will Thompson, Penporn Koanantakool, Mike Bernico, Jie Ren, Andrey Vlasov, Sergei Vassilvitskii, Maciej Kula, Yizhong Liang, Dahun Kim, Yangsibo Huang, Chengxi Ye, Dmitry Lepikhin, Wesley Helmholz</div>
<div class="meta-line">First: 2025-07-07T17:36:04+00:00 · Latest: 2025-12-19T14:25:46+00:00</div>
<div class="meta-line">Comments: 72 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.06261v6">Abs</a> · <a href="https://arxiv.org/pdf/2507.06261v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models.</div>
</details>
</div>
<div class="card">
<div class="title">EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning</div>
<div class="meta-line">Authors: Haoran Sun, Chen Cai, Huiping Zhuang, Kong Aik Lee, Lap-Pui Chau, Yi Wang</div>
<div class="meta-line">First: 2025-10-18T10:34:05+00:00 · Latest: 2025-12-19T14:22:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16442v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.16442v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://11ouo1.github.io/edvd-llama/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The project page is available at: https://11ouo1.github.io/edvd-llama/.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation.</div>
</details>
</div>
<div class="card">
<div class="title">Evaluation of Nuclear Microreactor Cost-competitiveness in Current Electricity Markets Considering Reactor Cost Uncertainties</div>
<div class="meta-line">Authors: Muhammad R. Abdussami, Ikhwan Khaleb, Fei Gao, Aditi Verma</div>
<div class="meta-line">Venue: Nuclear Engineering and Design 443 (2025) 114295</div>
<div class="meta-line">First: 2025-06-16T11:04:48+00:00 · Latest: 2025-12-19T14:21:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.13361v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.13361v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper evaluates the cost competitiveness of microreactors in today&#x27;s electricity markets, with a focus on uncertainties in reactor costs. A Genetic Algorithm (GA) is used to optimize key technical parameters, such as reactor capacity, fuel enrichment, tail enrichment, refueling interval, and discharge burnup, to minimize the Levelized Cost of Energy (LCOE). Base case results are validated using Simulated Annealing (SA). By incorporating Probability Distribution Functions (PDFs) for fuel cycle costs, the study identifies optimal configurations under uncertainty. Methodologically, it introduces a novel framework combining probabilistic cost modeling with evolutionary optimization. Results show that microreactors can remain cost-competitive, with LCOEs ranging from \$48.21/MWh to \$78.32/MWh when supported by the Production Tax Credit (PTC). High reactor capacity, low fuel enrichment, moderate tail enrichment and refueling intervals, and high discharge burnup enhance cost efficiency. Among all factors, overnight capital cost (OCC) has the most significant impact on LCOE, while O&amp;M and fuel cost uncertainties have lesser effects. The analysis highlights how energy policies like the PTC can reduce LCOE by 22-24%, improving viability despite cost variability. Compared to conventional nuclear, coal, and renewable sources like offshore wind, hydro, and biomass, optimized microreactors show strong economic potential. This research defines a realistic design space and key trade-offs, offering actionable insights for policymakers, reactor designers, and energy planners aiming to accelerate the deployment of affordable, sustainable microreactors.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper evaluates the cost competitiveness of microreactors in today&#x27;s electricity markets, with a focus on uncertainties in reactor costs.</div>
</details>
</div>
<div class="card">
<div class="title">More Consistent Accuracy PINN via Alternating Easy-Hard Training</div>
<div class="meta-line">Authors: Zhaoqian Gao, Min Yanga</div>
<div class="meta-line">First: 2025-12-19T14:12:17+00:00 · Latest: 2025-12-19T14:12:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17607v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored.</div>
</details>
</div>
<div class="card">
<div class="title">MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration</div>
<div class="meta-line">Authors: Svetlana Krasnova, Emiliya Starikova, Ilia Naletov, Andrey Krylov, Dmitry Sorokin</div>
<div class="meta-line">First: 2025-12-19T14:10:36+00:00 · Latest: 2025-12-19T14:10:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17605v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17605v1">PDF</a> · <a href="https://github.com/KourtKardash/MGRegBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors&#x27; implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MGRegBench：一种用于乳腺X线图像配准的新颖基准数据集及其解剖标志点</div>
<div class="mono" style="margin-top:8px">稳健的乳腺X线图像配准对于临床应用如疾病进展跟踪和监测乳腺组织的纵向变化至关重要。然而，由于缺乏公共数据集和标准化基准，进展受到了限制。现有研究往往不可直接比较，因为它们使用私有数据和不一致的评估框架。为了解决这一问题，我们提出了MGRegBench，这是一个公开的乳腺X线图像配准基准数据集。它包含超过5,000对图像，其中100对包含手动解剖标志点和分割掩码，用于严格的评估。这使得MGRegBench成为具有手动注释的最大公共二维配准数据集之一。利用这一资源，我们对包括经典方法（ANTs）、基于学习的方法（VoxelMorph、TransMorph）、隐式神经表示（IDIR）、经典乳腺X线特定方法以及最新的深度学习方法MammoRegNet在内的多种配准方法进行了基准测试。这些实现要么从作者的实现中调整，要么从头重新实现。我们的贡献包括：(1) 首个具有手动地标和掩码的大规模公共数据集，用于乳腺X线图像配准；(2) 首次对多种方法在该模态上的直接比较；(3) 对基于深度学习的配准方法的广泛分析。我们公开发布了我们的代码和数据，以建立一个公平比较的基础资源，并促进未来的研究。源代码和数据可在https://github.com/KourtKardash/MGRegBench/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue.</div>
<div class="mono" style="margin-top:8px">MGRegBench 是一个包含超过 5,000 对图像且其中 100 对包含手动解剖标志和分割掩码的公共基准数据集，用于乳腺X线成像的图像对齐。它首次实现了不同方法的直接对比，包括经典方法、基于学习的方法和深度学习方法。主要发现包括 MammoRegNet 在与其他方法的对比中表现出色，突显了在乳腺X线成像中使用特定于模态的深度学习技术的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">A Systems-Theoretic View on the Convergence of Algorithms under Disturbances</div>
<div class="meta-line">Authors: Guner Dilsad Er, Sebastian Trimpe, Michael Muehlebach</div>
<div class="meta-line">First: 2025-12-19T14:05:20+00:00 · Latest: 2025-12-19T14:05:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17598v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17598v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>系统理论视角下的算法在干扰下的收敛性研究</div>
<div class="mono" style="margin-top:8px">算法在复杂的物理、社会和工程系统中运行，这些系统中存在干扰、噪声和与其他动态系统的相互连接。本文将已知的在隔离环境下算法收敛性保证扩展到存在干扰的情况，并系统地推导出在这些干扰存在下的稳定性边界和收敛速率。通过利用反向李雅普诺夫定理，我们推导出关键不等式来量化干扰的影响。我们进一步展示了如何利用我们的结果评估干扰对算法性能的影响，包括分布式学习中的通信约束、机器学习泛化中的敏感性以及为了隐私而故意注入的噪声。这表明我们的结果在噪声、干扰和与其他动态系统的相互连接存在的情况下，作为算法分析的统一工具的作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems.</div>
<div class="mono" style="margin-top:8px">本文旨在探讨算法在存在干扰、噪声和与其他动态系统相互连接情况下的收敛性。通过运用反向李雅普诺夫定理，推导出关键不等式来量化干扰对算法性能的影响。主要发现包括在干扰下的算法稳定性边界和收敛速率，这些结果可以应用于分布式学习、机器学习泛化以及通过注入噪声实现的隐私保护等领域。</div>
</details>
</div>
<div class="card">
<div class="title">Logical Characterizations of GNNs with Mean Aggregation</div>
<div class="meta-line">Authors: Moritz Schönherr, Carsten Lutz</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-07-24T07:21:49+00:00 · Latest: 2025-12-19T14:03:25+00:00</div>
<div class="meta-line">Comments: 26 pages, extended version of paper to appear in AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.18145v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.18145v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the expressive power of graph neural networks (GNNs) with mean as the aggregation function, with the following results. In the non-uniform setting, such GNNs have exactly the same expressive power as ratio modal logic, which has modal operators expressing that at least a certain ratio of the successors of a vertex satisfies a specified property. In the uniform setting, the expressive power relative to MSO is exactly that of modal logic, and thus identical to the (absolute) expressive power of GNNs with max aggregation. The proof, however, depends on constructions that are not satisfactory from a practical perspective. This leads us to making the natural assumptions that combination functions are continuous and classification functions are thresholds. The resulting class of GNNs with mean aggregation turns out to be much less expressive: relative to MSO and in the uniform setting, it has the same expressive power as alternation-free modal logic. This is in contrast to the expressive power of GNNs with max and sum aggregation, which is not affected by these assumptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有均值聚合的GNN逻辑表征</div>
<div class="mono" style="margin-top:8px">我们研究了具有均值聚合函数的图神经网络（GNN）的表达能力，得到了以下结果。在非均匀设置中，这样的GNN与比率模态逻辑具有相同的表达能力，后者通过模态运算符表达至少有特定比例的顶点的后继满足指定属性。在均匀设置中，相对于MSO的表达能力恰好是模态逻辑的表达能力，因此与具有最大值聚合的GNN的绝对表达能力相同。然而，证明依赖于从实用角度看不令人满意的构造。这促使我们做出自然假设，即组合函数是连续的，分类函数是阈值。由此得到的具有均值聚合的GNN类实际上表达能力要小得多：相对于MSO和在均匀设置中，其表达能力与无交替模态逻辑相同。这与具有最大值和求和聚合的GNN的表达能力不同，后者的表达能力不受这些假设的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study the expressive power of graph neural networks (GNNs) with mean as the aggregation function, with the following results.</div>
<div class="mono" style="margin-top:8px">本文研究了使用均值聚合的图神经网络（GNNs）的表达能力。在非均匀设置下，使用均值聚合的GNNs与比率模态逻辑具有相同的表达能力。在均匀设置下，它们相对于一阶逻辑（MSO）的表达能力等同于模态逻辑。然而，证明依赖于不实用的构造。通过假设连续的组合函数和阈值分类函数，使用均值聚合的GNNs的表达能力显著降低，在均匀设置下与无交替模态逻辑的表达能力相同。这与使用最大值和求和聚合的GNNs在这些假设下的表达能力不变形成对比。</div>
</details>
</div>
<div class="card">
<div class="title">MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</div>
<div class="meta-line">Authors: Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli</div>
<div class="meta-line">First: 2025-12-19T14:02:37+00:00 · Latest: 2025-12-19T14:02:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17594v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17594v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAD-OOD：一种基于聚类驱动的深度学习框架，用于异常分布外恶意软件检测与分类</div>
<div class="mono" style="margin-top:8px">分布外（OOD）检测仍然是恶意软件分类中的一个关键挑战，因为由多态和变形恶意软件变体引入的大量家族内变异性。大多数现有的基于深度学习的恶意软件检测器依赖于封闭世界假设，并且无法充分建模这种家族内变异性，导致在面对以前未见过的恶意软件家族时性能下降。本文提出了一种新颖的两阶段、基于聚类的深度学习框架MADOOD，用于稳健的OOD恶意软件检测与分类。在第一阶段，使用高斯判别分析（GDA）推导出的类条件球形决策边界来建模恶意软件家族嵌入，从而在训练过程中无需使用OOD数据即可实现统计上合理的分布内和OOD样本分离。在多个类质心的基于Z分数的距离分析被用来可靠地识别潜在异常样本。在第二阶段，一个深度神经网络结合基于聚类的预测、精炼嵌入和监督分类器输出来增强最终分类准确性。在包含25个已知家族和多个新型OOD变体的基准恶意软件数据集上的广泛评估表明，MADOOD显著优于最先进的OOD检测方法，在未见过的恶意软件家族上AUC最高可达0.911。所提出的框架为不断变化的网络安全环境中恶意软件检测和异常识别提供了一个可扩展、可解释且统计上合理的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants.</div>
<div class="mono" style="margin-top:8px">MAD-OOD 是一种两阶段的深度学习框架，用于稳健的 OOD 恶意软件检测和分类。第一阶段使用高斯判别分析建模恶意软件家族嵌入，以区分分布内和 OOD 样本，并使用 Z 分数分析来识别异常样本。第二阶段结合基于聚类的预测和细化嵌入来提高分类准确性。在基准数据集上的评估表明，MAD-OOD 在未见过的恶意软件家族上的 AUC 达到 0.911，优于现有 OOD 检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Representation of Neural Networks Architectures</div>
<div class="meta-line">Authors: Christophe Prieur, Mircea Lazar, Bogdan Robu</div>
<div class="meta-line">First: 2025-12-19T14:01:50+00:00 · Latest: 2025-12-19T14:01:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17593v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17593v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经网络架构的统一表示</div>
<div class="mono" style="margin-top:8px">在本文中，我们考虑神经网络（NNs）架构在每层隐藏神经元数量和隐藏层数量趋向无穷时的极限情况，从而形成一个连续统一体，并推导出误差函数，该函数与神经元数量和/或隐藏层数量有关。首先，我们考虑单隐藏层的神经网络情况，并推导出一个积分无限宽度的神经表示，该表示可以推广现有的连续神经网络（CNNs）表示。然后，我们将此扩展到具有有限数量的积分隐藏层和残差连接的深层残差CNNs。其次，我们重新审视神经ODEs与深层残差NNs之间的关系，并通过离散化技术正式化误差。然后，我们将这两种方法合并为一个统一的分布式参数神经网络（DiPaNet）表示，并证明大多数现有的有限和无限维NNs架构都可以通过DiPaNet表示的同质化/离散化与之相关。我们的方法是纯粹确定性的，并适用于一般、均匀连续的矩阵权重函数。我们讨论了DiPaNet框架与神经场之间的差异和相似之处，并进一步探讨了DiPaNet框架的可能推广和应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers.</div>
<div class="mono" style="margin-top:8px">研究旨在理解神经网络在神经元和隐藏层数量趋向无穷时的行为，提出了单隐藏层网络的连续表示，并将其扩展到深度残差网络。作者还重新审视了神经ODE与深度残差网络之间的关系，通过离散化技术正式化了逼近误差。通过将这些方法结合起来，他们提出了一个统一的同质表示DiPaNet，该表示将各种有限和无限维神经网络架构联系起来。该研究提供了一个适用于一般矩阵权重函数的确定性框架，并讨论了其与神经场的关系及其潜在应用。</div>
</details>
</div>
<div class="card">
<div class="title">Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models</div>
<div class="meta-line">Authors: Arthur Guijt, Dirk Thierens, Ellen Kerkhof, Jan Wiersma, Tanja Alderliesten, Peter A. N. Bosman</div>
<div class="meta-line">First: 2025-12-19T13:59:46+00:00 · Latest: 2025-12-19T13:59:46+00:00</div>
<div class="meta-line">Comments: 35 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17592v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17592v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties&#x27; data is viewed independently, we find that training solely on a single parties&#x27; data results in similar performance when merging with another parties&#x27; data, when considering performance on that single parties&#x27; data, while performance on other parties&#x27; data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties&#x27; own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不共享数据分享知识：Stitches可以改进独立训练模型的集成</div>
<div class="mono" style="margin-top:8px">深度学习已被证明在执行许多实际任务方面非常强大。然而，这种性能通常依赖于大型和多样化的数据集的存在。在某些情况下，如医疗领域，数据往往分散在不同的实体之间，无法轻易共享。虽然联邦学习解决了这一问题，但它需要所有参与方同步训练一个单一模型并交换模型权重。我们研究了异步协作如何影响性能，其中仅共享已经训练好的模型（例如作为出版物的一部分），并提出使用Stitches作为结合模型的方法。通过从多目标的角度出发，将每个实体数据上的性能独立地进行评估，我们发现仅在单一实体的数据上进行训练，在合并另一个实体的数据时，性能相似，但其他实体数据上的性能明显较差。此外，虽然这种单独训练的网络的集成有更好的泛化能力，但在每个实体自己的数据集上的性能却较差。我们发现，通过在单独训练的模型中结合中间表示，并在适当位置放置Stitches层，可以恢复这种性能，同时保持更好的泛化能力，表明异步协作可以取得竞争力的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deep learning has been shown to be very capable at performing many real-world tasks.</div>
<div class="mono" style="margin-top:8px">该论文解决了数据碎片化且无法共享时提高模型性能的挑战。它探讨了异步协作的方法，并提出了一种称为‘缝合’的技术来结合独立训练的模型。研究发现，虽然单独训练的模型集合在整体上表现更好，但在每个参与方的数据集上表现较差。通过在独立训练的模型中整合中间表示并使用适当的缝合层，论文展示了性能可以恢复到竞争水平，同时保持更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement</div>
<div class="meta-line">Authors: Abderrezzaq Sendjasni, Seif-Eddine Benkabou, Mohamed-Chaker Larabi</div>
<div class="meta-line">First: 2024-12-17T08:36:47+00:00 · Latest: 2025-12-19T13:54:51+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Image Processing</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.12667v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.12667v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This article identifies and addresses a fundamental bottleneck in data-driven 360-degree image quality assessment (IQA): the lack of intelligent, sample-level data selection. Hence, we propose a novel framework that introduces a critical refinement step between patches sampling and model training. The core of our contribution is an embedding similarity-based selection algorithm that distills an initial, potentially redundant set of patches into a compact, maximally informative subset. This is formulated as a regularized optimization problem that preserves intrinsic perceptual relationships in a low-dimensional space, using residual analysis to explicitly filter out irrelevant or redundant samples. Extensive experiments on three benchmark datasets (CVIQ, OIQA, MVAQD) demonstrate that our selection enables a baseline model to match or exceed the performance of using all sampled data while keeping only 40-50% of patches. Particularly, we demonstrate the universal applicability of our approach by integrating it with several state-of-the-art IQA models, incleasy to deploy. Most significantly, its value as a generic,uding CNN- and transformer-based architectures, consistently enabling them to maintain or improve performance with 20-40\% reduced computational load. This work establishes that adaptive, post-sampling data refinement is a powerful and widely applicable strategy for achieving efficient and robust 360-degree IQA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于嵌入驱动的数据蒸馏在残差感知精炼下的360度图像质量评估</div>
<div class="mono" style="margin-top:8px">本文识别并解决了数据驱动的360度图像质量评估(IQA)中的一个根本性瓶颈：缺乏智能的样本级数据选择。因此，我们提出了一种新颖的框架，在样本采集和模型训练之间引入了一个关键的精炼步骤。我们的贡献核心是一种基于嵌入相似性的选择算法，该算法将初始的、可能冗余的补丁集精炼成一个紧凑的、最大化信息量的子集。这被形式化为一个正则化优化问题，该问题在低维空间中保留了内在的感知关系，并使用残差分析明确地过滤掉无关或冗余的样本。在三个基准数据集（CVIQ、OIQA、MVAQD）上的大量实验表明，我们的选择使基线模型能够在保留40-50%补丁的情况下，匹配或超过使用所有采集数据的性能。特别地，我们通过将该方法与几种最先进的IQA模型集成，展示了其普适性，易于部署。最重要的是，它作为一种通用的方法，无论是在CNN和基于变换器的架构中，都能一致地使它们在计算负载减少20-40%的情况下保持或提高性能。本文确立了自适应、后采样数据精炼是实现高效和稳健的360度IQA的一种强大且广泛适用的策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article identifies and addresses a fundamental bottleneck in data-driven 360-degree image quality assessment (IQA): the lack of intelligent, sample-level data selection.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的嵌入驱动的数据蒸馏框架，以解决360度图像质量评估中的智能样本级数据选择问题。该框架在样本采集和模型训练之间引入了一步精炼步骤，使用嵌入相似性选择算法从初始样本集中提取一个紧凑且信息丰富的子集。实验表明，这种方法可以使基线模型在使用40-50%的样本时达到或超过性能，并且能够一致地提高或保持最先进的图像质量评估模型的性能，同时减少20-40%的计算负载。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</div>
<div class="meta-line">Authors: Mahesh Keswani, Raunak Bhattacharyya</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-12-19T13:52:19+00:00 · Latest: 2025-12-19T13:52:19+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures. Submitted to ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17586v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p &lt; 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用预测安全表示学习安全自主驾驶策略</div>
<div class="mono" style="margin-top:8px">安全强化学习（SafeRL）是自主驾驶的一个重要范式，其中智能体需要在严格的安全要求下优化性能。这一双重目标造成了根本性的矛盾，过于保守的策略限制了驾驶效率，而激进的探索则可能违反安全规定。安全表示用于更安全策略学习（SRPL）框架通过为智能体提供未来约束违规的预测模型来解决这一挑战，并在受控环境中显示出前景。本文探讨了SRPL是否适用于真实的自主驾驶场景。系统实验在Waymo开放运动数据集（WOMD）和NuPlan上表明，SRPL可以改善奖励-安全权衡，显著提高成功率（效应量r = 0.65-0.86）和成本减少（效应量r = 0.70-0.83），且观察到的改进具有统计学意义（p &lt; 0.05）。然而，其有效性取决于底层策略优化器和数据集分布。结果进一步表明，预测安全表示在提高对观测噪声的鲁棒性方面起着关键作用。此外，在零样本跨数据集评估中，SRPL增强的智能体在泛化能力上优于非SRPL方法。这些发现共同证明了预测安全表示在加强自主驾驶安全强化学习中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the application of the Safety Representations for Safer Policy Learning (SRPL) framework in real-world autonomous driving scenarios. By using a predictive model of future constraint violations, SRPL improves the reward-safety tradeoff, achieving statistically significant improvements in success rate and cost reduction. However, its effectiveness varies with the policy optimizer and dataset distribution. The framework also enhances robustness to observation noise and improves generalization across datasets.</div>
<div class="mono" style="margin-top:8px">该论文探讨了Safety Representations for Safer Policy Learning (SRPL)框架在真实世界自动驾驶场景中的应用。通过使用对未来约束违规的预测模型，SRPL改善了奖励与安全之间的权衡，实现了显著的成功率和成本减少的统计学改进。然而，其效果会因政策优化器和数据集分布的不同而有所变化。研究还强调了预测安全表示在增强对观测噪声的鲁棒性和提高零样本跨数据集泛化能力方面的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</div>
<div class="meta-line">Authors: N. A. Adarsh Pritam, Jeba Shiney O, Sanyam Jain</div>
<div class="meta-line">First: 2025-12-19T13:52:11+00:00 · Latest: 2025-12-19T13:52:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17585v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17585v1">PDF</a> · <a href="https://github.com/adarsh-crafts/SkinGenBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SkinGenBench：合成皮肤镜图像增强在黑色素瘤诊断中的生成模型和预处理影响</div>
<div class="mono" style="margin-top:8px">本研究介绍了SkinGenBench，这是一种系统性的生物医学成像基准，探讨了预处理复杂性与生成模型选择对合成皮肤镜图像增强及下游黑色素瘤诊断的影响。使用HAM10000和MILK10K中包含14,116张皮肤镜图像的五类病变数据集，评估了两种代表性生成范式：StyleGAN2-ADA和去噪扩散概率模型（DDPMs），在基本几何增强和高级伪影去除管道下的表现。使用已建立的感知和分布度量（FID、KID、IS）、特征空间分析以及对五个下游分类器诊断性能的影响评估合成黑色素瘤图像。实验结果表明，生成架构的选择比预处理复杂性对图像保真度和诊断效用的影响更大。StyleGAN2-ADA持续生成与真实数据分布更接近的合成图像，FID约为65.5，KID约为0.05，而扩散模型生成了更高方差的样本，但感知保真度和类别锚定降低。高级伪影去除仅在生成度量上提供了边际改进，并且对下游诊断效果的提升有限，表明可能抑制了临床相关的纹理线索。相比之下，合成数据增强显著提高了黑色素瘤检测，绝对增益为8-15%的黑色素瘤F1分数，ViT-B/16达到F1约0.88和ROC-AUC约0.98，比未增强基线提高了约14%。我们的代码可在https://github.com/adarsh-crafts/SkinGenBench找到</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SkinGenBench evaluates how preprocessing and generative models affect synthetic dermoscopic image augmentation for melanoma diagnosis. Using StyleGAN2-ADA and DDPMs, the study finds that generative architecture choice impacts image fidelity and diagnostic utility more than preprocessing complexity. StyleGAN2-ADA produced images more aligned with real data, while DDPMs had higher variance. Advanced artifact removal had limited benefits. Synthetic data augmentation improved melanoma detection by 8-15% in F1-score, with ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98.</div>
<div class="mono" style="margin-top:8px">SkinGenBench 研究了预处理和生成模型如何影响合成皮肤镜图像增强以用于黑色素瘤诊断。使用 StyleGAN2-ADA 和 DDPMs，研究发现生成模型的选择比预处理复杂性对图像保真度和诊断有用性影响更大。StyleGAN2-ADA 生成的图像更接近真实数据，而 DDPMs 则具有更高的方差。高级去噪仅在生成指标上有所改进，并提供了有限的下游诊断增益。合成数据增强将黑色素瘤检测提高了 8-15% 的 F1 分数，ViT-B/16 达到 F1~0.88 和 ROC-AUC~0.98。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
