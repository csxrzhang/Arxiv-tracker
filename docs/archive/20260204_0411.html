<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-04 04:11</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260204_0411</div>
    <div class="row"><div class="card">
<div class="title">RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</div>
<div class="meta-line">Authors: Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang</div>
<div class="meta-line">First: 2026-02-02T18:59:04+00:00 · Latest: 2026-02-02T18:59:04+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Gen-Verse/Open-AgentRL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02488v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02488v1">PDF</a> · <a href="https://github.com/Gen-Verse/Open-AgentRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLAnything：在完全动态强化学习系统中锻造环境、策略与奖励模型</div>
<div class="mono" style="margin-top:8px">我们提出RLAnything，一种通过闭环优化动态锻造环境、策略和奖励模型的强化学习框架，可增强学习信号并强化适用于任何大语言模型或智能体场景的整体强化学习系统。具体而言，策略通过整合来自逐步信号与结果信号的反馈进行训练，而奖励模型则通过一致性反馈联合优化，进而进一步提升策略训练效果。此外，我们基于理论驱动的自动环境适应机制，通过利用来自奖励模型与策略模型的批评反馈，提升两者的训练效果，实现从经验中学习。实证表明，每个新增组件均持续提升系统整体性能，RLAnything在多种代表性大语言模型与智能体任务中取得显著增益：在OSWorld上使Qwen3-VL-8B-Thinking提升9.1%，在AlfWorld和LiveBench上分别使Qwen2.5-7B-Instruct提升18.7%和11.9%。我们还发现优化后的奖励模型信号优于依赖人工标注的结果。代码：https://github.com/Gen-Verse/Open-AgentRL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces RLAnything, a reinforcement learning framework designed to enhance learning in LLM and agentic scenarios by dynamically co-optimizing environment, policy, and reward models through closed-loop feedback. The method integrates step-wise and outcome signals for policy training, jointly optimizes the reward model via consistency feedback, and employs theory-driven automatic environment adaptation using critic feedback to improve learning from experience. Experimental results show that each component contributes to system improvement, with RLAnything achieving significant performance gains, such as boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, while also demonstrating that optimized reward signals surpass those based on human labels.</div>
<div class="mono" style="margin-top:8px">本文提出了RLAnything，这是一个强化学习框架，旨在通过闭环反馈动态协同优化环境、策略和奖励模型，以增强大语言模型和智能体场景中的学习效果。该方法整合了逐步和结果信号进行策略训练，通过一致性反馈联合优化奖励模型，并利用理论驱动的自动环境适应，借助批评者反馈从经验中改进学习。实验结果表明，每个组件都提升了系统性能，RLAnything取得了显著的性能提升，例如在OSWorld上将Qwen3-VL-8B-Thinking提高了9.1%，在AlfWorld和LiveBench上将Qwen2.5-7B-Instruct分别提高了18.7%和11.9%，同时证明优化后的奖励信号优于依赖人工标签的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Capabilities of Reinforcement Learning via Text Feedback</div>
<div class="meta-line">Authors: Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T18:56:56+00:00 · Latest: 2026-02-02T18:56:56+00:00</div>
<div class="meta-line">Comments: 43 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02482v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本反馈拓展强化学习能力</div>
<div class="mono" style="margin-top:8px">强化学习在大型语言模型后训练中的成功源于一个信息量极低的来源：每次训练轮次仅提供单比特信息，如二元奖励或偏好标签。另一极端，蒸馏方法虽能提供密集监督，但需要成本高昂且难以扩展的演示样本。本研究将文本反馈作为一种中间信号进行探索：它比标量奖励更丰富，又比完整演示更经济。文本反馈是人类互动的自然模式，已在众多现实场景中广泛存在，用户、标注者和自动评估系统常以此评判语言模型输出。为规模化利用文本反馈，我们形式化了一个多轮强化学习框架——基于文本反馈的强化学习（RLTF），其中训练阶段可获得文本反馈，但推理阶段则无。因此，模型必须学会内化反馈以提升测试阶段的单轮表现。为此，我们提出两种方法：自蒸馏（RLTF-SD），训练单轮策略以匹配其自身基于反馈的第二轮生成结果；以及反馈建模（RLTF-FM），通过预测反馈作为辅助目标。我们对两种方法进行了理论分析，并在推理谜题、竞赛数学和创意写作任务上进行了实证评估。结果表明，两种方法在多个基准测试中均持续优于强基线，凸显了强化学习结合规模化丰富监督源的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of sparse binary rewards and costly demonstrations in reinforcement learning for large language models, this paper introduces text feedback as an intermediate, scalable supervision signal. The method formalizes RL from Text Feedback (RLTF), where models receive multi-turn textual critiques during training but must perform autonomously at inference, and proposes two approaches: Self Distillation to align single-turn outputs with feedback-conditioned generations, and Feedback Modeling to predict feedback as an auxiliary task. Experimental results on reasoning puzzles, competition math, and creative writing tasks demonstrate that both methods consistently outperform strong baselines, validating text feedback as an effective source of rich supervision for improving model performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决强化学习在大语言模型后训练中面临的二元奖励信息稀疏和演示数据成本高昂的局限性，提出将文本反馈作为一种中间、可扩展的监督信号。研究方法形式化了基于文本反馈的强化学习框架，模型在训练时接收多轮文本批评，但在推理时必须自主执行，并提出了两种方法：自蒸馏使单轮输出与反馈条件下的生成对齐，以及反馈建模将预测反馈作为辅助目标。在推理谜题、竞赛数学和创意写作任务上的实验结果表明，这两种方法均持续优于强基线，证实了文本反馈作为丰富监督来源在提升模型性能方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Conflict-Aware Client Selection for Multi-Server Federated Learning</div>
<div class="meta-line">Authors: Mingwei Hong, Zheng Lin, Zehang Lin, Lin Li, Miao Yang, Xia Du, Zihan Fang, Zhaolu Kang, Dianxin Luan, Shunzhi Zhu</div>
<div class="meta-line">First: 2026-02-02T18:47:16+00:00 · Latest: 2026-02-02T18:47:16+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多服务器联邦学习的冲突感知客户端选择</div>
<div class="mono" style="margin-top:8px">联邦学习作为一种分布式机器学习范式，支持客户端在不暴露原始数据的情况下进行协同模型训练，从而保护用户隐私并降低通信开销。然而传统单服务器联邦学习因需聚合海量客户端模型而存在高通信延迟问题。多服务器联邦学习虽能将负载分散至边缘服务器，但客户端覆盖重叠与选择失序常引发资源竞争，导致带宽冲突与训练失败。为此，我们提出一种融合冲突风险预测的去中心化强化学习方法（RL-CRP），以优化多服务器联邦学习系统中的客户端选择。具体而言，各服务器基于稀疏历史选择序列，通过分类隐马尔可夫模型预测客户端选择冲突概率，并引入公平性奖励机制以促进客户端长期参与，从而最小化训练延迟与资源竞争。大量实验表明，所提RL-CRP框架能有效降低服务器间冲突，在收敛速度与通信成本方面显著提升训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the communication latency in single-server federated learning and the resource contention caused by uncoordinated client selection in multi-server federated learning. The method proposes a decentralized reinforcement learning approach with conflict risk prediction (RL-CRP), where each server uses a categorical hidden Markov model to estimate client selection conflicts from sparse historical data and employs a fairness-aware reward to encourage long-term client participation. Experimental results show that the RL-CRP framework effectively reduces inter-server conflicts and improves training efficiency, including faster convergence and lower communication costs.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决单服务器联邦学习中的通信延迟问题，以及多服务器联邦学习中因客户端选择不协调导致的资源争用。方法上提出了一种基于冲突风险预测的去中心化强化学习框架（RL-CRP），其中每个服务器使用分类隐马尔可夫模型根据稀疏的历史客户端选择序列来估计冲突风险，并结合公平感知奖励机制以促进客户端的长期参与。实验结果表明，RL-CRP框架能有效减少服务器间冲突，并在收敛速度和通信成本方面显著提升训练效率。</div>
</details>
</div>
<div class="card">
<div class="title">World-Gymnast: Training Robots with Reinforcement Learning in a World Model</div>
<div class="meta-line">Authors: Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu, Sherry Yang</div>
<div class="meta-line">First: 2026-02-02T18:44:45+00:00 · Latest: 2026-02-02T18:44:45+00:00</div>
<div class="meta-line">Comments: https://world-gymnast.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02454v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02454v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://world-gymnast.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone&#x27;s household.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>World-Gymnast：在世界模型中通过强化学习训练机器人</div>
<div class="mono" style="margin-top:8px">机器人在物理世界中通过交互学习，从根本上受到物理交互成本的制约。两种替代方案——基于专家演示的监督微调（SFT）与基于软件模拟器的强化学习（RL），分别受限于可用专家数据的规模以及操作任务中的仿真到现实差距。随着近期从真实世界视频-动作数据中学习世界模型的技术兴起，我们提出一个核心问题：在世界模型中训练策略，是否比监督学习或软件仿真更能有效提升真实机器人的性能？我们提出World-Gymnast方法，该方法通过在动作条件视频世界模型中展开策略、并利用视觉语言模型（VLM）对推演结果进行奖励，实现对视觉-语言-动作（VLA）策略的强化学习微调。在Bridge机器人实验平台上，World-Gymnast的性能超越SFT最高达18倍，超越软件仿真最高达2倍。更重要的是，World-Gymnast展现了世界模型结合强化学习的独特能力，包括：基于世界模型对多样化语言指令和新场景进行训练、在新场景中进行测试时训练、以及对世界模型和策略进行在线迭代优化。我们的结果表明，学习世界模型并在云端训练机器人策略，可能是弥合“仅能在演示环境中工作的机器人”与“能在任意家庭环境中工作的机器人”之间差距的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for World-Gymnast stems from the high cost and limitations of physical robot interaction, supervised fine-tuning from demonstrations, and reinforcement learning in software simulators due to data scarcity and the sim-to-real gap. The method involves training a vision-language-action policy via reinforcement learning within an action-conditioned video world model, using a vision-language model to reward policy rollouts in this simulated environment. Experimental results on the Bridge robot setup show that World-Gymnast outperforms supervised fine-tuning by up to 18 times and software simulation by up to 2 times, while also enabling training on diverse language instructions, novel scenes, test-time adaptation, and iterative model improvement, suggesting world models could help robots generalize beyond demonstrations to household settings.</div>
<div class="mono" style="margin-top:8px">World-Gymnast的动机源于物理机器人交互成本高昂，且依赖专家演示的监督微调和软件模拟器中的强化学习存在数据不足和模拟到现实差距的限制。该方法通过在动作条件视频世界模型中，使用视觉语言模型奖励策略展开，对视觉语言动作策略进行强化学习微调。在Bridge机器人设置上的实验结果表明，World-Gymnast的性能比监督微调提升高达18倍，比软件模拟提升高达2倍，同时支持基于多样语言指令和新场景的训练、测试时适应以及迭代模型改进，表明世界模型可能帮助机器人从演示泛化到家庭环境。</div>
</details>
</div>
<div class="card">
<div class="title">Maximizing Reliability with Bayesian Optimization</div>
<div class="meta-line">Authors: Jack M. Buckingham, Ivo Couckuyt, Juergen Branke</div>
<div class="meta-line">First: 2026-02-02T18:31:58+00:00 · Latest: 2026-02-02T18:31:58+00:00</div>
<div class="meta-line">Comments: 25 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02432v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02432v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯优化的可靠性最大化研究</div>
<div class="mono" style="margin-top:8px">贝叶斯优化是一种针对高成本黑箱优化问题的样本高效方法。在制造业中，常需在随机扰动下最大化设计可靠性（即最小化失效概率），这类问题可能涉及极低失效概率（$P_\mathrm{fail} = 10^{-6}-10^{-8}$）。本研究提出两种基于汤普森采样与知识梯度的贝叶斯优化方法，后者通过近似一步贝叶斯最优策略来最小化失效概率对数。两种方法均采用重要性采样以应对极低失效概率场景。实验结果表明，所提方法在极端与非极端工况下均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of maximizing design reliability, which involves minimizing extremely small failure probabilities (as low as 10^-6 to 10^-8) in manufacturing under random perturbations, where direct evaluation is expensive. It proposes two Bayesian optimization methods based on Thompson sampling and knowledge gradient, with the latter approximating the optimal policy for minimizing log failure probability, both enhanced by importance sampling to efficiently target rare events. Experimental results demonstrate that these methods outperform existing approaches across both extreme and non-extreme failure probability regimes.</div>
<div class="mono" style="margin-top:8px">该论文针对制造中在随机扰动下最大化设计可靠性（即最小化极低失效概率，低至10^-6至10^-8）的挑战，其中直接评估成本高昂。提出了两种基于汤普森采样和知识梯度的贝叶斯优化方法，后者近似于最小化对数失效概率的最优策略，并均采用重要性采样以高效处理罕见事件。实验结果表明，这些方法在极端和非极端失效概率范围内均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning</div>
<div class="meta-line">Authors: Samuel Nellessen, Tal Kachman</div>
<div class="meta-line">First: 2026-02-02T17:56:55+00:00 · Latest: 2026-02-02T17:56:55+00:00</div>
<div class="meta-line">Comments: Under review. 8 main pages, 2 figures, 2 tables. Appendix included</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02395v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02395v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary &quot;tags along&quot; on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a &#x27;cold-start&#x27; reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大卫对阵歌利亚：基于强化学习的可验证智能体间越狱攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型向自主智能体的演进引入了利用合法工具权限的对抗性失效，使工具增强环境中的安全评估从主观自然语言处理任务转变为客观控制问题。我们将此威胁模型形式化为&#x27;尾随攻击&#x27;：无工具攻击者通过对话&#x27;尾随&#x27;安全对齐操作者的可信权限，仅凭交流即可诱导被禁止的工具使用。为验证该威胁，我们提出&#x27;弹弓&#x27;——一种&#x27;冷启动&#x27;强化学习框架，能自主发现涌现的攻击向量，揭示关键洞见：在我们的设定中，习得攻击倾向于收敛为简短、指令式的句法模式，而非多轮说服。在保留的极端难度任务上，弹弓对Qwen2.5-32B-Instruct-AWQ操作者的攻击成功率达67.0%（基线为1.7%），将首次成功所需预期尝试次数（在已破解任务中）从52.3次降至1.3次。关键的是，弹弓能零样本迁移至多个模型家族，包括Gemini 2.5 Flash等闭源模型（56.0%攻击成功率）和Meta-SecAlign-8B等防御性微调开源模型（39.2%攻击成功率）。本研究确立尾随攻击为一类可验证的一级威胁模型，并证明仅通过环境交互即可从现成的开放权重模型中引发有效的智能体攻击。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the security risks posed by autonomous agents exploiting tool privileges, this paper formalizes Tag-Along Attacks, where a tool-less adversary manipulates a safety-aligned agent into prohibited actions through conversation. The method introduces Slingshot, a reinforcement learning framework that autonomously discovers attack vectors, revealing that effective attacks converge to short, instruction-like patterns rather than complex persuasion. Experimental results show Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ operator, drastically reducing attempts to first success, and demonstrates strong zero-shot transferability to both closed-source and defensive models like Gemini 2.5 Flash and Meta-SecAlign-8B.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决自主智能体利用工具权限带来的安全风险，提出了“跟随攻击”威胁模型，即无工具的攻击者通过对话操纵安全对齐的操作员执行禁止操作。方法上引入了Slingshot，一个基于强化学习的框架，能自主发现攻击向量，揭示有效攻击往往收敛于简短、指令式的模式而非多轮说服。主要实验结果表明，Slingshot对Qwen2.5-32B-Instruct-AWQ操作员的攻击成功率达到67.0%，显著降低了首次成功所需尝试次数，并在Gemini 2.5 Flash和Meta-SecAlign-8B等闭源和防御性微调模型上展示了强大的零样本迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-02-02T17:56:01+00:00</div>
<div class="meta-line">Comments: 87 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive policy gradient to discover such systematic reasoning remains poorly understood. We address this by analyzing the policy gradient dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, policy gradient drives the Transformer to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler examples, the Transformer learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, policy gradient learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动策略梯度发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的策略梯度动态来研究此问题——该任务必须通过思维链才能解决，但存在简单的迭代解法。我们证明：尽管仅通过最终答案正确性进行训练，策略梯度仍能驱动Transformer收敛至一种结构化、可解释的逐顶点迭代遍历算法。我们刻画了该现象涌现所需的分布特性，指出“简单样本”（需要较少推理步骤的实例）的关键作用。当训练分布中此类简单样本具有足够密度时，Transformer能学习可泛化至更长链的遍历策略；若该密度消失，策略梯度学习将无法实现。我们通过合成数据实验及真实数学推理任务的语言模型实验验证了理论结论在实际场景中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how Transformers trained with outcome-based Reinforcement Learning (RL) can develop systematic reasoning capabilities like Chain-of-Thought. The motivation is to understand the mechanism by which sparse rewards from final-answer correctness drive policy gradient to discover intermediate reasoning steps. The method involves a theoretical analysis of policy gradient dynamics on a synthetic graph traversal task that necessitates multi-step reasoning, proving that Transformers converge to an interpretable, iterative algorithm. The main experimental results show that learning succeeds and generalizes to longer chains only when the training data distribution includes sufficient &#x27;simple examples&#x27; (instances requiring fewer steps); without such examples, learning fails. These theoretical findings are corroborated by experiments on synthetic data and real-world language models on mathematical reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本文研究了基于结果强化学习训练的Transformer如何发展出如思维链般的系统性推理能力，其动机在于理解仅凭最终答案正确性的稀疏奖励，如何驱动策略梯度发现中间推理步骤的机制。方法上，通过对一个需要多步推理的合成图遍历任务进行策略梯度动态的理论分析，证明了Transformer会收敛到一个可解释的迭代算法。主要实验结果表明，仅当训练数据分布包含足够多的&#x27;简单示例&#x27;（即所需推理步骤较少的实例）时，学习才能成功并泛化到更长的链；若缺乏此类示例，学习则不可行。这些理论发现通过在合成数据以及真实世界语言模型上的数学推理任务实验得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-02T17:51:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人机交互贝叶斯优化的个性化图像生成</div>
<div class="mono" style="margin-top:8px">假设Alice心中有一幅特定图像$x^\ast$，例如她童年成长街道的景象。为生成该图像，她通过多轮提示引导生成模型，得到图像$x^{p*}$。虽然$x^{p*}$已接近$x^\ast$，但Alice发现仅靠语言提示难以完全消除差异。本文指出：即使语言提示已达极限，人类仍能判断新图像$x^+$是否比$x^{p*}$更接近$x^\ast$。基于此，我们提出MultiBO（多选择偏好贝叶斯优化）方法：以$x^{p*}$为基准生成$K$幅新图像，获取用户偏好反馈，利用反馈指导扩散模型，最终生成新一轮$K$幅图像。实验表明，在$B$轮用户反馈内，即使生成模型未获取$x^\ast$的直接信息，仍能大幅逼近目标图像。通过30名用户的定性评分，以及与5个基线模型的定量指标对比，结果证实人类的多选择反馈能有效推动个性化图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of generating highly personalized images that match a user&#x27;s specific mental vision, where textual prompts alone prove insufficient. The proposed method, Multi-Choice Preferential Bayesian Optimization (MultiBO), iteratively refines an initial generated image by presenting the user with multiple new image choices, collecting their preferential feedback on which is closer to the target, and using this feedback to guide a diffusion model. Experimental results from 30 users and quantitative comparisons against five baselines demonstrate that this human-in-the-loop approach significantly narrows the gap to the target image within a limited number of feedback rounds, effectively leveraging comparative human judgment for personalized generation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决个性化图像生成中，仅凭文本提示难以精确匹配用户心中特定视觉概念的问题。所提出的方法名为多选择偏好贝叶斯优化（MultiBO），它通过迭代过程改进初始生成的图像：每轮根据当前最佳图像生成多个新选项，收集用户关于哪个选项更接近目标图像的偏好反馈，并利用该反馈指导扩散模型。基于30名用户的定性评分以及与五种基线方法的定量比较，实验结果表明，在有限的几轮交互中，这种人机协同方法能显著缩小与目标图像的差距，有效利用人类的比较性判断实现个性化生成。</div>
</details>
</div>
<div class="card">
<div class="title">SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization</div>
<div class="meta-line">Authors: Maksim Afanasyev, Illarion Iov</div>
<div class="meta-line">First: 2026-02-02T17:46:06+00:00 · Latest: 2026-02-02T17:46:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02383v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02383v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response&#x27;s absolute likelihood. This can lead to ``unlearning&#x27;&#x27;, where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse&#x27;&#x27; caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SLIME：基于稳定似然的隐式边界强化偏好优化方法</div>
<div class="mono" style="margin-top:8px">直接偏好优化方法已成为替代基于人类反馈的强化学习（RLHF）对齐大语言模型（LLM）的高效计算方案。最新方法通过推导隐式奖励函数简化了对齐流程，但普遍存在关键的目标失配问题：优化被选与拒绝回答间的相对边界，并不能保证被选回答绝对似然性的保持。这可能导致“遗忘”现象（模型为满足边界约束而降低高质量输出的概率）和“格式崩溃”（因对拒绝序列的过度惩罚所致）。本研究提出SLIME（基于稳定似然的隐式边界强化），这是一种无需参考模型的对齐目标，旨在解耦偏好学习与生成质量。SLIME包含三重目标：（1）锚定项以最大化偏好回答的似然性；（2）稳定惩罚项防止拒绝标记的概率坍缩至零；（3）结合硬约束与软约束的双边界机制，实现精确边界塑造。实验表明，SLIME在保持更高生成稳定性的同时，性能优于当前最先进的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for SLIME arises from the limitations of existing direct preference optimization methods, which often cause unlearning of high-quality outputs and formatting collapse due to an objective mismatch that fails to preserve the absolute likelihood of chosen responses. To address this, the method introduces a reference-free alignment objective with three components: an anchoring term to maximize preferred response likelihood, a stabilizing penalty to prevent rejected token probabilities from collapsing, and a dual-margin mechanism combining hard and soft constraints for boundary shaping. Experimental results show that SLIME outperforms state-of-the-art baselines while achieving higher generation stability.</div>
<div class="mono" style="margin-top:8px">SLIME的提出动机源于现有直接偏好优化方法的局限性，这些方法常因目标不匹配导致高质量输出被遗忘和格式崩溃，无法保持选定响应的绝对似然。为解决此问题，该方法引入了一种无参考的对齐目标，包含三个部分：锚定项以最大化偏好响应的似然，稳定惩罚项防止被拒绝标记的概率崩溃，以及结合硬约束和软约束的双边界机制用于精确边界塑造。实验结果表明，SLIME在保持更高生成稳定性的同时，性能优于现有先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Universe: Scale Real-World Verifiable Environments to Millions</div>
<div class="meta-line">Authors: Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</div>
<div class="meta-line">First: 2026-02-02T17:20:30+00:00 · Latest: 2026-02-02T17:20:30+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Universe：将真实世界可验证环境扩展至百万规模</div>
<div class="mono" style="margin-top:8px">我们提出SWE-Universe，一个可扩展的高效框架，用于从GitHub拉取请求自动构建真实世界软件工程可验证环境。为克服自动构建中普遍存在的生产成功率低、验证器弱、成本高昂等挑战，本框架采用基于高效定制训练模型的构建智能体。该智能体通过迭代自验证与循环内黑客检测，确保可靠生成高保真可验证任务。运用此方法，我们将真实世界多语言软件工程环境扩展至百万规模（807,693个）。我们通过大规模智能体中训练与强化学习，证明了该环境的重要价值。最终，我们将此技术应用于Qwen3-Max-Thinking模型，在SWE-Bench Verified基准测试中获得75.3%的得分。本工作为推进下一代编码智能体发展提供了关键资源与稳健方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWE-Universe is to address the challenges of automatically constructing real-world software engineering verifiable environments from GitHub pull requests, which typically suffer from low production yield, weak verification, and high costs. The method introduces a scalable framework that employs a building agent powered by a custom-trained model, utilizing iterative self-verification and in-loop hacking detection to reliably generate high-fidelity tasks. Main experimental results show that the framework scales to 807,693 multilingual environments, enhances agentic mid-training and reinforcement learning, and when applied to Qwen3-Max-Thinking, achieves a score of 75.3% on the SWE-Bench Verified benchmark.</div>
<div class="mono" style="margin-top:8px">SWE-Universe 的动机是解决从 GitHub 拉取请求自动构建真实世界软件工程可验证环境时面临的挑战，如生成率低、验证弱和成本高昂。该方法提出了一个可扩展框架，利用基于定制训练模型的构建代理，通过迭代自验证和循环内黑客检测来可靠生成高保真任务。主要实验结果表明，该框架将多语言环境扩展至 807,693 个，增强了代理中间训练和强化学习，并在应用于 Qwen3-Max-Thinking 时，在 SWE-Bench Verified 基准测试中取得了 75.3% 的得分。</div>
</details>
</div>
<div class="card">
<div class="title">Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach</div>
<div class="meta-line">Authors: Martino Ciaperoni, Marzio Di Vece, Luca Pappalardo, Fosca Giannotti, Francesco Giannini</div>
<div class="meta-line">First: 2026-02-02T16:36:21+00:00 · Latest: 2026-02-02T16:36:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02304v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>立场：解释大语言模型的行为转变需采用比较方法</div>
<div class="mono" style="margin-top:8px">大规模基础模型展现出行为转变：即在模型扩展、微调、强化学习或上下文学习后出现的干预引发行为变化。尽管这些现象的研究近期受到关注，但其成因解释仍被忽视。经典可解释人工智能（XAI）方法虽能揭示模型单次检查点的缺陷，但其结构上难以合理解释不同检查点间的内部变化，以及针对这些变化应提出何种解释主张。我们主张行为转变应通过比较进行解释：核心目标应是参照模型与干预模型间的干预引发转变，而非孤立分析单一模型。为此，我们构建了比较性XAI（Δ-XAI）框架，提出设计合理解释方法时需考量的一系列准则。为阐明Δ-XAI方法原理，我们介绍了一套可能的实施流程，将其与准则关联，并提供了具体的Δ-XAI实验案例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper argues that explaining behavioral shifts in large language models, which emerge after interventions like scaling or fine-tuning, requires a comparative approach rather than analyzing single model checkpoints in isolation. The authors propose a Comparative XAI (Δ-XAI) framework with specific desiderata to guide the development of methods that directly explain the differences between a reference model and an intervened model. They illustrate the framework with potential pipelines and a concrete experiment to demonstrate how Δ-XAI can justify what changed internally due to an intervention.</div>
<div class="mono" style="margin-top:8px">本文主张，解释大语言模型在干预（如扩展或微调）后出现的行为变化，需要一种比较方法，而非孤立分析单个模型检查点。作者提出了一个比较性可解释人工智能（Δ-XAI）框架，包含一系列设计合适解释方法时应考虑的要求。他们通过可能的流程示例和一个具体实验，说明了Δ-XAI如何能够解释干预引起的内部变化。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing General-Purpose Reasoning Models with Modular Gradient Surgery</div>
<div class="meta-line">Authors: Min Cai, Yu Liang, Longzheng Wang, Yan Wang, Yueyang Zhang, Long Xia, Zhiyuan Sun, Xi Ye, Daiting Shi</div>
<div class="meta-line">First: 2026-02-02T16:34:39+00:00 · Latest: 2026-02-02T16:34:39+00:00</div>
<div class="meta-line">Comments: Preprint; Code: https://github.com/StringNLPLAB/MGS; Website: https://modular-gradient-surgery.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02301v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02301v1">PDF</a> · <a href="https://github.com/StringNLPLAB/MGS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://modular-gradient-surgery.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过模块化梯度手术推进通用推理模型发展</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在大型推理模型（LRM）的最新进展中发挥了核心作用，在可验证和开放式推理方面取得了显著提升。然而，由于显著的领域异质性，跨多个领域训练单一通用LRM仍具挑战性。通过对两种广泛使用的策略——顺序RL和混合RL——进行系统研究，我们发现两者在行为和梯度层面均产生显著的跨领域干扰，导致整体增益有限。为解决这些问题，我们引入了**模块化梯度手术（MGS）**，该方法在Transformer内部模块层面解决梯度冲突。应用于Llama和Qwen模型时，MGS在三个代表性领域（数学、通用对话和指令遵循）上，相比标准多任务RL分别实现了平均4.3（16.6%）和4.5（11.1%）个百分点的提升。进一步分析表明，MGS在长时间训练下仍保持有效性。总体而言，我们的研究阐明了多领域RL中干扰的来源，并为训练通用LRM提供了有效解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training a single general-purpose large reasoning model across diverse domains, where domain heterogeneity causes substantial cross-domain interference in standard reinforcement learning approaches like Sequential RL and Mixed RL, limiting overall performance gains. The authors introduce Modular Gradient Surgery (MGS), a method that resolves gradient conflicts at the module level within transformer architectures to mitigate this interference. Experimental results on Llama and Qwen models show that MGS achieves average improvements of 4.3 and 4.5 points, respectively, over standard multi-task RL across math, general chat, and instruction following domains, with further analysis confirming its effectiveness under prolonged training.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在多样化领域训练单一通用大型推理模型所面临的挑战，其中领域异质性导致序列强化学习和混合强化学习等标准方法产生显著的跨领域干扰，从而限制了整体性能提升。作者提出了模块化梯度手术（MGS）方法，通过在Transformer架构的模块层面解决梯度冲突来减轻这种干扰。在Llama和Qwen模型上的实验结果表明，MGS在数学、通用聊天和指令遵循三个代表性领域上，相比标准多任务强化学习分别实现了平均4.3分和4.5分的提升，进一步分析证实了其在长期训练中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management</div>
<div class="meta-line">Authors: Owen Shen, Patrick Jaillet</div>
<div class="meta-line">First: 2026-02-02T16:23:56+00:00 · Latest: 2026-02-02T16:23:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02283v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02283v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于选择模型辅助Q学习的延迟反馈收益管理</div>
<div class="mono" style="margin-top:8px">本研究针对具有延迟反馈的收益管理问题开展强化学习研究，其中大部分价值由预订后数日观测到的客户取消与修改行为决定。我们提出\emph{选择模型辅助强化学习}方法：采用校准的离散选择模型作为固定部分世界模型，在决策时估算学习目标的延迟分量。在固定模型部署机制下，我们证明采用模型估算目标的表格Q学习将收敛至最优Q函数的$O(\varepsilon/(1-γ))邻域（其中$\varepsilon$表征部分模型误差），并附加$O(t^{-1/2})$采样项。基于61,619条酒店预订数据校准的模拟器实验（1,088次独立运行）显示：（1）在稳态环境下与成熟缓冲DQN基线无统计显著差异；（2）在参数族内偏移下呈现积极效应，经Holm-Bonferroni校正后10个偏移场景中有5个获得显著收益提升（最高达12.4%）；（3）在结构误设（违反选择模型假设）时出现持续性能下降（收益降低1.4-2.6%）。这些结果明确了部分行为模型在何种偏移条件下能提升鲁棒性，以及在何种情况下会引入有害偏差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of delayed feedback in revenue management, where customer cancellations and modifications observed days after booking significantly affect value, motivating the need for reinforcement learning methods that can handle such delays. The authors propose choice-model-assisted RL, which uses a calibrated discrete choice model as a fixed partial world model to impute delayed components of the learning target at decision time, enabling more timely updates. In experiments with a simulator based on 61,619 hotel bookings, the method showed no significant difference from a baseline in stationary settings but achieved revenue gains of up to 12.4% under certain parameter shifts, while degrading by 1.4–2.6% when the choice model assumptions were violated, highlighting its robustness under shifts but sensitivity to misspecification.</div>
<div class="mono" style="margin-top:8px">本文针对收益管理中反馈延迟的挑战展开研究，其中客户在预订后数日发生的取消和修改会显著影响价值，这促使需要能够处理此类延迟的强化学习方法。作者提出了选择模型辅助的强化学习，该方法使用校准的离散选择模型作为固定的部分世界模型，在决策时估算学习目标的延迟部分，从而实现更及时的更新。在基于61,619个酒店预订数据的模拟实验中，该方法在静态环境下与基线无显著差异，但在某些参数变化下实现了高达12.4%的收益提升，而当选择模型假设被违反时收益下降1.4–2.6%，这突显了其在变化下的鲁棒性以及对模型误设的敏感性。</div>
</details>
</div>
<div class="card">
<div class="title">HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing</div>
<div class="meta-line">Authors: Chengyu Du, Xintao Wang, Aili Chen, Weiyuan Li, Rui Xu, Junteng Liu, Zishan Huang, Rong Tian, Zijun Sun, Yuhao Li, Liheng Feng, Deming Ding, Pengyu Zhao, Yanghua Xiao</div>
<div class="meta-line">First: 2026-01-29T09:35:27+00:00 · Latest: 2026-02-02T16:22:28+00:00</div>
<div class="meta-line">Comments: 41pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21459v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21459v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters&#x27; first-person thinking from LLMs&#x27; third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train HER models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HER：面向大语言模型角色扮演的人类化推理与强化学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型角色扮演，即利用大语言模型模拟特定人物角色，已成为陪伴对话、内容创作、数字游戏等应用中的关键能力。现有模型虽能有效捕捉角色语调和知识，但模拟其行为背后的内在思维仍具挑战。为实现大语言模型角色扮演的认知模拟，先前研究主要存在两大不足：缺乏高质量推理轨迹的数据，以及缺少符合人类偏好的可靠奖励信号。本文提出HER——一个认知层面人物模拟的统一框架。HER引入双层思维机制，区分角色的第一人称思维与大语言模型的第三人称思维。为弥合这些差距，我们通过逆向工程构建了推理增强的角色扮演数据，并建立了符合人类偏好的原则体系与奖励模型。基于这些资源，我们以Qwen3-32B为基础模型，通过监督学习和强化学习训练HER模型。大量实验验证了该方法的有效性：我们的模型显著超越Qwen3-32B基线，在CoSER基准上提升30.26分，在Minimax角色扮演基准上提升14.97分。我们将公开数据集、原则体系与模型以促进后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of simulating the inner cognitive processes behind persona behaviors in LLM role-playing, which is crucial for applications like companionship and gaming. To overcome limitations in existing data and reward signals, the authors propose HER, a framework that introduces dual-layer thinking to separate character-first-person reasoning from LLM-third-person reasoning, and they curate reasoning-augmented data via reverse engineering while developing human-aligned principles and reward models. Experimental results show that HER models, trained on Qwen3-32B with supervised and reinforcement learning, significantly outperform the baseline, achieving improvements of 30.26 on the CoSER benchmark and 14.97 on the Minimax Role-Play Bench.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型角色扮演中模拟人物行为背后内在认知过程的挑战展开研究，这对于陪伴和游戏等应用至关重要。为克服现有数据和奖励信号的不足，作者提出了HER框架，引入双层思维以区分角色第一人称推理与模型第三人称推理，并通过逆向工程构建了推理增强数据，同时开发了符合人类偏好的原则和奖励模型。实验结果表明，基于Qwen3-32B通过监督和强化学习训练的HER模型显著优于基线，在CoSER基准上提升了30.26分，在Minimax角色扮演基准上提升了14.97分。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Markov Decision Processes under Fully Bandit Feedback</div>
<div class="meta-line">Authors: Zhengjia Zhuo, Anupam Gupta, Viswanath Nagarajan</div>
<div class="meta-line">First: 2026-02-02T16:03:24+00:00 · Latest: 2026-02-02T16:03:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02260v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02260v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit&#x27;&#x27; feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered&#x27;&#x27; MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm&#x27;s performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>完全赌博反馈下的马尔可夫决策过程学习</div>
<div class="mono" style="margin-top:8px">强化学习中的一个标准假设是，智能体能够观察到马尔可夫决策过程（MDP）中访问的每个状态-动作对以及每步奖励。在此设定下，已有强理论结果，实现了近乎紧致的$Θ(\sqrt{T})$遗憾界。然而，这种详细反馈可能不切实际，近期研究探索了更受限的设定，如轨迹反馈，其中智能体仅观察到访问的状态-动作对和一个聚合奖励。本文针对片段式MDP提出了一种更为受限的“完全赌博”反馈模型，智能体甚至不观察访问的状态-动作对，仅获知聚合奖励。我们首次为片段式MDP提供了高效的赌博学习算法，其遗憾为$\widetilde{O}(\sqrt{T})$。该遗憾对时间跨度$\H$呈指数依赖，我们证明这是必要的。我们还为“有序”MDP改进了近乎紧致的遗憾界，可用于建模经典随机优化问题，如$k$项先知不等式和序列定价。最后，我们在$k$项先知不等式设定下评估了算法的实证性能：尽管反馈高度受限，其表现与具备详细状态-动作反馈的先进学习算法（UCB-VI）相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the unrealistic assumption of detailed state-action feedback in standard Reinforcement Learning, this paper investigates a highly restrictive fully bandit feedback model for episodic Markov Decision Processes (MDPs), where only an aggregate reward per episode is observed. The method introduces the first efficient bandit learning algorithm designed for this setting, achieving a regret bound of \(\widetilde{O}(\sqrt{T})\) with an exponential dependence on the horizon length, which is proven necessary. Experimental results on problems like the k-item prophet inequality show that, despite the limited feedback, the algorithm performs comparably to state-of-the-art methods like UCB-VI that rely on full state-action observations.</div>
<div class="mono" style="margin-top:8px">本文针对标准强化学习中详细状态-动作反馈不切实际的假设，研究了一种高度受限的完全赌博机反馈模型，其中智能体仅能观察到每个回合的聚合奖励。方法上，提出了首个适用于该情景的高效赌博机学习算法，实现了具有指数级依赖于时间跨度的 \(\widetilde{O}(\sqrt{T})\) 遗憾界，并证明此依赖性是必要的。在k项先知不等式等问题的实验中，尽管反馈信息极为有限，该算法的性能仍可与依赖完整状态-动作反馈的先进算法（如UCB-VI）相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">Segment to Focus: Guiding Latent Action Models in the Presence of Distractors</div>
<div class="meta-line">Authors: Hamza Adnan, Matthew T. Jackson, Alexey Zakharov</div>
<div class="meta-line">First: 2026-02-02T16:03:19+00:00 · Latest: 2026-02-02T16:03:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分割以聚焦：在干扰物存在下引导潜在动作模型</div>
<div class="mono" style="margin-top:8px">潜在动作模型（LAMs）通过学习从原始观察中提取动作相关表示，实现了从未标记视频中进行强化学习，并显著扩展了可用训练数据。然而，LAMs面临一个关键挑战：如何将动作相关特征与动作相关噪声（如背景运动）分离。若未能过滤这些干扰物，LAMs会捕捉虚假相关性并构建次优的潜在动作空间。本文提出MaskLAM——一种对LAM训练的轻量级改进方法，通过引入视觉智能体分割来缓解此问题。MaskLAM利用预训练基础模型生成的分割掩码对LAM重建损失进行加权，从而在无需修改架构的情况下，优先处理显著信息而非背景元素。我们在添加了动作相关背景噪声的连续控制MuJoCo任务上验证了方法的有效性。实验表明，相较于标准基线，我们的方法使累积奖励提升高达4倍，并通过线性探针评估证明潜在动作质量提高了3倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in Latent Action Models (LAMs) of disentangling action-relevant features from distracting background noise, which can lead to spurious correlations and suboptimal latent spaces. To mitigate this, the authors propose MaskLAM, a lightweight training modification that incorporates visual agent segmentation masks from pretrained foundation models to weight the reconstruction loss, prioritizing salient information without architectural changes. Experimental results on MuJoCo tasks with action-correlated background noise show that MaskLAM achieves up to a 4x increase in accrued rewards and a 3x improvement in latent action quality compared to standard baselines.</div>
<div class="mono" style="margin-top:8px">本文针对潜在动作模型在从原始观察中提取动作相关特征时，难以区分背景噪声等干扰因素的问题，提出了一种轻量级训练改进方法MaskLAM。该方法利用预训练基础模型生成的视觉智能体分割掩码，对重建损失进行加权，从而优先关注显著信息而无需修改模型架构。在添加了动作相关背景噪声的MuJoCo连续控制任务实验中，MaskLAM相比基线方法实现了高达4倍的累积奖励提升和3倍的潜在动作质量改进。</div>
</details>
</div>
<div class="card">
<div class="title">Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences</div>
<div class="meta-line">Authors: Viktor Stein, Adwait Datar, Nihat Ay</div>
<div class="meta-line">First: 2026-02-02T15:57:32+00:00 · Latest: 2026-02-02T15:57:32+00:00</div>
<div class="meta-line">Comments: 37 pages, 9 figures, comments welcome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02250v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Kullback-Leibler divergence (KL) regularization is widely used in reinforcement learning, but it becomes infinite under support mismatch and can degenerate in low-noise limits. Utilizing a unified information-geometric framework, we introduce (Kalman)-Wasserstein-based KL analogues by replacing the Fisher-Rao geometry in the dynamical formulation of the KL with transport-based geometries, and we derive closed-form values for common distribution families. These divergences remain finite under support mismatch and yield a geometric interpretation of regularization heuristics used in Kalman ensemble methods. We demonstrate the utility of these divergences in KL-regularized optimal control. In the fully tractable setting of linear time-invariant systems with Gaussian process noise, the classical KL reduces to a quadratic control penalty that becomes singular as process noise vanishes. Our variants remove this singularity, yielding well-posed problems. On a double integrator and a cart-pole example, the resulting controls outperform KL-based regularization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Wasserstein与Kalman-Wasserstein KL散度的适定KL正则化控制</div>
<div class="mono" style="margin-top:8px">Kullback-Leibler散度（KL）正则化在强化学习中广泛应用，但在支撑集不匹配时会发散，且在低噪声极限下可能退化。通过统一的信息几何框架，我们引入基于（Kalman）-Wasserstein的KL类比——将KL动态表述中的Fisher-Rao几何替换为基于传输的几何，并推导出常见分布族的闭式解。这些散度在支撑集不匹配时保持有限，并为卡尔曼集成方法中的正则化启发式提供几何解释。我们展示了这些散度在KL正则化最优控制中的实用性：在具有高斯过程噪声的线性时不变系统这一完全可处理场景中，经典KL退化为二次控制惩罚项，且随过程噪声消失而奇异化；我们的变体消除了这种奇异性，得到适定问题。在双积分器和倒立摆案例中，所得控制效果优于基于KL的正则化方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue that standard Kullback-Leibler (KL) regularization in reinforcement learning can become infinite under support mismatch and degenerate in low-noise limits. To resolve this, the authors propose new KL-like divergences based on Wasserstein and Kalman-Wasserstein geometries, replacing the Fisher-Rao geometry in the dynamical formulation of KL, and derive closed-form expressions for common distributions. Experimental results in linear time-invariant systems with Gaussian noise show that these variants eliminate singularities as process noise vanishes, leading to well-posed optimal control problems, and outperform classical KL regularization in double integrator and cart-pole examples.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中标准Kullback-Leibler（KL）正则化在支持不匹配时可能无限大、在低噪声极限下可能退化的问题，提出基于Wasserstein和Kalman-Wasserstein几何的新型KL类散度，通过用传输几何替代KL动态公式中的Fisher-Rao几何，并推导了常见分布族的闭式解。在线性时不变系统与高斯过程噪声的可处理设置中，实验结果表明这些变体在过程噪声消失时消除了奇异性，产生了适定的最优控制问题，并在双积分器和倒立摆示例中优于基于KL的正则化方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models</div>
<div class="meta-line">Authors: Hao Wang, Hao Gu, Hongming Piao, Kaixiong Gong, Yuxiao Ye, Xiangyu Yue, Sirui Han, Yike Guo, Dapeng Wu</div>
<div class="meta-line">First: 2026-02-02T15:53:55+00:00 · Latest: 2026-02-02T15:53:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02244v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02244v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>保持好奇心的学习：通过自适应自蒸馏实现熵保持的监督微调以增强大型推理模型</div>
<div class="mono" style="margin-top:8px">大型推理模型的标准后训练流程——监督微调后接强化学习（SFT-then-RL）——可能限制强化学习阶段的收益：监督微调虽模仿专家示范，但常导致模型过度自信并降低生成多样性，使强化学习探索的解决方案空间变窄。在监督微调中添加熵正则化并非万能解药；它倾向于使词元分布趋于均匀，虽增加熵值却未提升有意义的探索能力。本文提出CurioSFT，一种通过内在好奇心增强探索能力的熵保持监督微调方法，包含：（a）自探索蒸馏：将模型蒸馏至自生成的温度缩放教师模型，以激发其能力范围内的探索；（b）熵引导温度选择：自适应调整蒸馏强度，通过增强推理词元的探索同时稳定事实词元来缓解知识遗忘。在数学推理任务上的大量实验表明，在监督微调阶段，CurioSFT在分布内任务上优于基准方法2.5分，在分布外任务上优于2.9分。研究还证实，监督微调阶段保留的探索能力能有效转化为强化学习阶段的具体收益，平均提升5.0分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a limitation in the standard supervised fine-tuning (SFT) approach for large reasoning models, where SFT reduces generation diversity and leads to overconfidence, thereby hindering subsequent reinforcement learning (RL) exploration. To overcome this, the authors propose CurioSFT, a method that preserves entropy through adaptive self-distillation, combining self-exploratory distillation to encourage exploration within the model&#x27;s capabilities and entropy-guided temperature selection to adjust distillation strength adaptively, focusing exploration on reasoning tokens while stabilizing factual ones. Experimental results on mathematical reasoning tasks show that CurioSFT outperforms vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks during SFT, and it further enhances RL performance with an average improvement of 5.0 points.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型标准监督微调方法中存在的生成多样性降低和过度自信问题，这些问题会限制后续强化学习阶段的探索能力。为此，作者提出了CurioSFT方法，通过自适应自蒸馏来保持熵，结合自探索蒸馏以鼓励模型在其能力范围内进行探索，以及熵引导的温度选择来自适应调整蒸馏强度，在推理标记上增强探索同时稳定事实标记。在数学推理任务上的实验结果表明，CurioSFT在监督微调阶段比传统方法在分布内任务上提升2.5个百分点，在分布外任务上提升2.9个百分点，并且能有效提升强化学习阶段性能，平均改进5.0个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL</div>
<div class="meta-line">Authors: Julian Lemmel, Felix Resch, Mónika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu</div>
<div class="meta-line">First: 2026-02-02T15:41:53+00:00 · Latest: 2026-02-02T15:41:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02236v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents&#x27; performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于实时循环强化学习的自动驾驶预训练控制器在线微调</div>
<div class="mono" style="margin-top:8px">在现实应用中部署预训练策略面临重大挑战，从根本上限制了基于学习的控制系统的实际适用性。当自主系统遭遇系统动力学变化、传感器漂移或任务目标变更时，固定策略的性能会迅速下降。研究表明，采用实时循环强化学习（RTRRL）——一种具有生物合理性的在线适应算法——能够有效微调预训练策略，提升自主智能体在驾驶任务中的表现。研究进一步揭示，RTRRL可与近期受生物启发的循环网络模型（液阻-液容循环神经网络）产生协同效应。通过在模拟CarRacing环境及配备事件相机的RoboRacer实车线跟踪任务中的验证，证明了该闭环方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that pretrained policies for autonomous driving degrade when faced with environmental changes, limiting their real-world applicability. The method proposes using Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible online adaptation algorithm, to fine-tune a pretrained policy, and synergizes it with a Liquid-Resistance Liquid-Capacitance recurrent network model. The main experimental results demonstrate the effectiveness of this closed-loop approach in both a simulated CarRacing environment and a real-world line-following task using a RoboRacer car with an event camera.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，自动驾驶的预训练策略在面临环境变化时性能会下降，这限制了其实际应用。方法上，提出使用一种生物启发的在线适应算法——实时循环强化学习（RTRRL）来微调预训练策略，并将其与一种液体电阻-液体电容循环网络模型相结合。主要实验结果表明，这种闭环方法在模拟的CarRacing环境以及使用配备事件相机的RoboRacer汽车进行的真实世界循线任务中均有效。</div>
</details>
</div>
<div class="card">
<div class="title">Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion</div>
<div class="meta-line">Authors: Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">First: 2026-01-06T17:52:02+00:00 · Latest: 2026-02-02T15:09:51+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03213v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03213v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像扩散中的批评者引导强化反学习</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型的机器反学习旨在移除特定概念，同时保持整体实用性。现有扩散反学习方法通常依赖监督式权重编辑或全局惩罚；强化学习（RL）方法虽灵活，但常优化稀疏的轨迹末端奖励，导致高方差更新和弱信用分配。我们提出一个通用的RL框架用于扩散反学习，将去噪视为序列决策过程，并引入具有噪声步奖励的时间步感知批评者。具体而言，我们在噪声潜在空间上训练基于CLIP的奖励预测器，利用其每步信号计算优势估计，以更新反向扩散核的策略梯度。该算法易于实现，支持离策略重用，并可集成到标准文本到图像主干中。在多个概念上，该方法实现优于或可比拟强基线的遗忘效果，同时保持图像质量和良性提示保真度；消融研究表明：（i）每步批评者与（ii）噪声条件奖励是稳定性和有效性的关键。我们发布代码和评估脚本，以促进基于RL的扩散反学习的可复现性和未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of machine unlearning in text-to-image diffusion models, aiming to remove specific concepts without degrading overall model utility. The authors propose a reinforcement learning framework that treats the denoising process as a sequential decision problem, introducing a timestep-aware critic with per-step rewards conditioned on noisy latents to improve credit assignment and reduce variance. Experimental results demonstrate that this method achieves effective concept forgetting comparable to or better than existing baselines while preserving image quality and fidelity to benign prompts, with ablations confirming the importance of per-step critics and noisy-conditioned rewards for stability and performance.</div>
<div class="mono" style="margin-top:8px">本文针对文本到图像扩散模型中的机器遗忘问题，旨在移除特定概念而不损害模型整体性能。作者提出一个强化学习框架，将去噪过程视为序列决策问题，引入一个时间步感知的评论家，利用基于噪声隐变量的每步奖励来改进信用分配并降低方差。实验结果表明，该方法在保持图像质量和良性提示保真度的同时，实现了与现有基线相当或更好的概念遗忘效果，消融研究证实了每步评论家和噪声条件奖励对于稳定性和有效性的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Jie Xiao, Meng Chen, Qingnan Ren, Song Jingwei, Jiaqi Huang, Yangshen Deng, Chris Tong, Wanyi Chen, Suli Wang, Ziqian Bi, Shuo Lu, Yiqun Duan, Lynn Ai, Eric Yang, Bill Shi</div>
<div class="meta-line">First: 2026-02-02T14:57:53+00:00 · Latest: 2026-02-02T14:57:53+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02192v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ECHO-2：面向高效能强化学习的大规模分布式推演框架</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是大语言模型（LLM）后训练的关键阶段，涉及推演生成、奖励评估与集中式学习的反复交互。分布式推演执行虽能利用更具成本效益的推理资源，但也带来了广域协调与策略分发的挑战。本文提出ECHO-2——一个支持远程推理节点且考虑非可忽略分发延迟的分布式RL后训练框架。该框架融合集中式学习与分布式推演，将有限策略陈旧度作为用户可控参数，实现推演生成、分发与训练的重叠执行。我们提出基于重叠能力的容量模型，关联训练时间、分发延迟与推演吞吐量，形成维持学习器利用率的实际资源配置规则。为缓解分发瓶颈并降低成本，ECHO-2采用对等辅助流水线广播及异构节点的成本感知激活机制。在真实广域带宽环境下对4B与8B模型进行GRPO后训练的实验表明，ECHO-2在保持与基线相当RL奖励的同时，显著提升了成本效益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for ECHO-2 is to reduce the cost of reinforcement learning (RL) for large language models by distributing rollout generation to remote, cost-efficient inference resources, while addressing the coordination and policy dissemination challenges this introduces. The method combines centralized learning with distributed rollouts, explicitly treating policy staleness as a tunable parameter to overlap rollout, dissemination, and training phases; it also employs an overlap-based capacity model for provisioning and techniques like peer-assisted pipelined broadcast to mitigate dissemination bottlenecks. Main experimental results from GRPO post-training of 4B and 8B models under real wide-area bandwidth conditions demonstrate that ECHO-2 significantly improves cost efficiency while maintaining RL reward performance comparable to strong baselines.</div>
<div class="mono" style="margin-top:8px">ECHO-2的动机是通过将强化学习中的策略执行（rollout）分布到远程、成本效益高的推理资源上来降低大语言模型后训练的成本，同时解决由此带来的广域协调和策略传播延迟的挑战。该方法将集中式学习与分布式策略执行相结合，将策略过时性作为可调参数以实现执行、传播和训练阶段的重叠；并采用基于重叠的容量模型进行资源调配，以及使用对等辅助流水线广播等技术来缓解传播瓶颈。在真实广域带宽环境下对4B和8B模型进行GRPO后训练的实验结果表明，ECHO-2在保持与强基线相当的强化学习奖励性能的同时，显著提高了成本效率。</div>
</details>
</div>
<div class="card">
<div class="title">PIQL: Projective Implicit Q-Learning with Support Constraint for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Xinchen Han, Hossam Afifi, Michel Marot</div>
<div class="meta-line">First: 2025-01-15T16:17:02+00:00 · Latest: 2026-02-02T14:52:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.08907v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.08907v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (RL) faces a fundamental challenge of extrapolation errors caused by out-of-distribution (OOD) actions. Implicit Q-Learning (IQL) employs expectile regression to achieve in-sample learning. Nevertheless, IQL relies on a fixed expectile hyperparameter and a density-based policy improvement method, both of which impede its adaptability and performance. In this paper, we propose Projective IQL (PIQL), a projective variant of IQL enhanced with a support constraint. In the policy evaluation stage, PIQL substitutes the fixed expectile hyperparameter with a projection-based parameter and extends the one-step value estimation to a multi-step formulation. In the policy improvement stage, PIQL adopts a support constraint instead of a density constraint, ensuring closer alignment with the policy evaluation. Theoretically, we demonstrate that PIQL maintains the expectile regression and in-sample learning framework, guarantees monotonic policy improvement, and introduces a progressively more rigorous criterion for advantageous actions. Experiments on D4RL and NeoRL2 benchmarks demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PIQL：面向离线强化学习的带支持约束的投影隐式Q学习</div>
<div class="mono" style="margin-top:8px">离线强化学习面临由分布外动作引起的外推误差这一根本挑战。隐式Q学习采用期望回归实现样本内学习，但其依赖固定的期望超参数和基于密度的策略改进方法，限制了适应性与性能。本文提出投影隐式Q学习，这是一种通过支持约束增强的投影变体。在策略评估阶段，该方法将固定期望超参数替换为基于投影的参数，并将单步价值估计扩展为多步形式；在策略改进阶段，采用支持约束替代密度约束，确保与策略评估更紧密对齐。理论分析表明，该方法保持了期望回归与样本内学习框架，保证策略单调改进，并为优势动作引入渐近更严格的判定准则。在D4RL和NeoRL2基准测试上的实验显示，其在不同领域均取得稳健提升，整体达到最先进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the extrapolation error problem in offline reinforcement learning caused by out-of-distribution actions, where existing methods like Implicit Q-Learning (IQL) suffer from limitations due to a fixed expectile hyperparameter and a density-based policy constraint. The proposed method, Projective Implicit Q-Learning (PIQL), introduces a projective variant of IQL enhanced with a support constraint; it replaces the fixed expectile with a projection-based parameter in policy evaluation using a multi-step formulation and adopts a support constraint in policy improvement to better align with evaluation. Experimental results on D4RL and NeoRL2 benchmarks show that PIQL achieves robust performance gains and state-of-the-art outcomes across diverse domains, supported by theoretical guarantees of monotonic policy improvement and a progressively stricter criterion for advantageous actions.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中因分布外动作引起的推断误差问题，指出现有方法如隐式Q学习（IQL）因固定的期望分位数超参数和基于密度的策略约束而存在局限性。所提出的方法——投影隐式Q学习（PIQL），通过引入支持约束的投影变体改进IQL；在策略评估阶段，它使用基于投影的参数替代固定期望分位数，并采用多步估计公式，而在策略改进阶段则采用支持约束以确保与评估更好对齐。在D4RL和NeoRL2基准测试上的实验结果表明，PIQL在不同领域实现了稳健的性能提升和整体最优性能，其理论分析保证了策略的单调改进以及对优势动作逐步更严格的标准。</div>
</details>
</div>
<div class="card">
<div class="title">Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors</div>
<div class="meta-line">Authors: Viacheslav Sinii, Nikita Balagansky, Gleb Gerasimov, Daniil Laptev, Yaroslav Aksenov, Vadim Kurochkin, Alexey Gorbatovski, Boris Shaposhnikov, Daniil Gavrilov</div>
<div class="meta-line">First: 2025-09-08T12:26:31+00:00 · Latest: 2026-02-02T14:39:45+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.06608v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.06608v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The mechanisms by which reasoning training reshapes LLMs&#x27; internal computations remain unclear. We study lightweight steering vectors inserted into the base model&#x27;s residual stream and trained with a reinforcement-learning objective. These vectors explain a large portion of full fine-tuning performance increase while preserving the interpretability of small, additive interventions. We find that (i) the last-layer steering vector acts like a token-substitution bias concentrated on the first generated token, consistently boosting tokens such as &quot;To&quot; and &quot;Step&quot;; (ii) the penultimate-layer vector leaves attention patterns largely intact and instead operates through the MLP and unembedding, preferentially up-weighting process words and structure symbols; and (iii) the steering vectors transfer to other models from the same family. Taken together, these results deepen understanding of how trained steering vectors shape computation and should inform future work in activation engineering and the study of reasoning models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小向量，大效应：基于导向向量的强化学习诱导推理机制研究</div>
<div class="mono" style="margin-top:8px">推理训练如何重塑大语言模型内部计算机制尚不明确。本研究通过在基础模型残差流中插入轻量级导向向量，并采用强化学习目标进行训练。这些向量在保持小型可加性干预可解释性的同时，解释了全量微调性能提升的主要部分。研究发现：（1）末层导向向量类似集中于首生成词元的词元替换偏置，持续提升如“To”“Step”等词元概率；（2）次末层向量基本保持注意力模式不变，主要通过MLP和解嵌入层运作，优先增强过程词与结构符号的权重；（3）导向向量可迁移至同系列其他模型。这些发现深化了对训练后导向向量如何塑造计算的理解，为未来激活工程与推理模型研究提供重要参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the unclear mechanisms through which reasoning training reshapes large language models (LLMs) by employing lightweight steering vectors trained with a reinforcement-learning objective and inserted into the base model&#x27;s residual stream. The method reveals that these vectors capture a significant portion of the performance gains from full fine-tuning while maintaining interpretability as small, additive interventions. Key experimental findings show that the last-layer vector acts as a token-substitution bias focused on the first generated token, boosting words like &quot;To&quot; and &quot;Step&quot;; the penultimate-layer vector operates primarily through the MLP and unembedding to up-weight process words and structure symbols without altering attention patterns; and these steering vectors successfully transfer to other models within the same family, offering insights for activation engineering and reasoning model research.</div>
<div class="mono" style="margin-top:8px">本研究针对推理训练如何重塑大语言模型（LLMs）的内部机制尚不明确的问题，通过在基础模型的残差流中插入并利用强化学习目标训练的轻量级导向向量进行探究。该方法表明，这些向量在保持作为小型可解释干预的同时，解释了全微调性能提升的大部分原因。主要实验结果发现：最后一层的导向向量类似于对首生成词的令牌替换偏置，持续提升如“To”和“Step”等词汇；倒数第二层的向量基本保持注意力模式不变，而是通过MLP和解嵌入层运作，优先增强过程词和结构符号的权重；并且这些导向向量能够成功迁移到同系列的其他模型中，为激活工程和推理模型的研究提供了深入见解。</div>
</details>
</div>
<div class="card">
<div class="title">ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning</div>
<div class="meta-line">Authors: Chu Zhao, Enneng Yang, Yuting Liu, Jianzhe Zhao, Guibing Guo</div>
<div class="meta-line">First: 2026-02-02T14:27:02+00:00 · Latest: 2026-02-02T14:27:02+00:00</div>
<div class="meta-line">Comments: 19 ppages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02150v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ECHO：测试时强化学习的熵-置信度混合优化方法</div>
<div class="mono" style="margin-top:8px">测试时强化学习通过重复推演生成多个候选答案，并利用多数投票构建的伪标签进行在线更新。为降低开销并提升探索效率，先前工作引入了树状结构推演，通过共享推理前缀并在关键节点分支以提高采样效率。然而，该范式仍面临两大挑战：（1）高熵分支可能引发推演崩溃，即分支预算集中于少数具有连续高熵片段的轨迹，迅速减少有效分支数量；（2）早期伪标签存在噪声和偏差，可能导致自我强化的过拟合，使策略过早锐化并抑制探索。为解决这些问题，我们提出熵-置信度混合分组相对策略优化（ECHO）。在推演阶段，ECHO联合利用局部熵和组级置信度自适应控制分支宽度，并引入基于在线置信度的剪枝机制以终止持续低置信度分支，避免高熵陷阱并缓解崩溃。在策略更新阶段，ECHO采用置信度自适应裁剪和熵-置信度混合优势塑形方法，以增强训练鲁棒性并缓解早期偏差。实验表明，ECHO在多个数学与视觉推理基准上取得稳定性能提升，并在有限推演预算下展现出更有效的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses two key challenges in test-time reinforcement learning with tree-structured rollouts: rollout collapse due to high-entropy branching and self-reinforcing overfitting from noisy early pseudo-labels. To mitigate these issues, the authors propose ECHO, a method that adaptively controls branch width during rollouts using local entropy and group-level confidence, while incorporating online pruning of low-confidence branches to avoid collapse; for policy updates, it employs confidence adaptive clipping and an entropy-confidence hybrid advantage shaping to reduce bias. Experimental results show that ECHO consistently improves performance on mathematical and visual reasoning benchmarks and generalizes better under limited rollout budgets.</div>
<div class="mono" style="margin-top:8px">本文针对测试时强化学习中树状展开的两个主要挑战展开研究：高熵分支导致的展开崩溃以及早期伪标签噪声引起的自我强化过拟合。为解决这些问题，作者提出了ECHO方法，该方法在展开过程中结合局部熵和群体置信度自适应控制分支宽度，并通过在线剪枝低置信度分支来避免崩溃；在策略更新时，采用置信度自适应裁剪和熵-置信度混合优势塑造以增强训练鲁棒性并减轻早期偏差。实验结果表明，ECHO在多个数学和视觉推理基准上取得了稳定提升，并在有限展开预算下表现出更有效的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Generative Selection for Best-of-N</div>
<div class="meta-line">Authors: Shubham Toshniwal, Aleksander Ficek, Siddhartha Jain, Wei Du, Vahid Noroozi, Sadegh Mahdavi, Somshubra Majumdar, Igor Gitman</div>
<div class="meta-line">First: 2026-02-02T14:21:15+00:00 · Latest: 2026-02-02T14:21:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02143v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习生成式选择以实现最佳N选一</div>
<div class="mono" style="margin-top:8px">通过并行采样扩展测试时计算能显著提升大语言模型的推理能力，但常受限于最佳N选一的选择质量。生成式选择方法（如GenSelect）虽能缓解此瓶颈，但强大的选择性能仍主要局限于大型模型。我们证明，小型推理模型可通过针对性强化学习获得强大的生成式选择能力。为此，我们从大规模数学与代码指令数据集中筛选出同时包含正确与错误候选解的实例，构建选择任务数据集，并使用DAPO训练17亿参数模型以奖励正确选择。在数学（AIME24、AIME25、HMMT25）和代码（LiveCodeBench）推理基准测试中，我们的模型始终优于提示法和多数投票基线，常达到甚至超越更大模型的性能。值得注意的是，尽管仅使用弱模型输出进行训练，这些优势仍能泛化至对强模型输出的选择。总体而言，我们的研究确立了强化学习作为可扩展路径，能为小型模型解锁强大的生成式选择能力，从而实现高效的测试时扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve the quality of Best-of-N selection in large language model reasoning, as existing generative selection methods like GenSelect are typically effective only with large models. The method involves training small reasoning models (1.7B parameters) using targeted reinforcement learning (DAPO) on synthesized selection tasks derived from math and code datasets that include both correct and incorrect candidate solutions. Experimental results on math benchmarks (AIME24, AIME25, HMMT25) and code benchmarks (LiveCodeBench) show that these small models consistently outperform prompting and majority-voting baselines, often matching or surpassing the performance of much larger models, and demonstrate generalization to selecting outputs from stronger models despite training only on weaker model outputs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升大语言模型推理中Best-of-N选择的质量，因为现有的生成式选择方法（如GenSelect）通常仅在大模型中有效。方法上，通过从包含正确和错误候选解的大规模数学和代码指令数据集中筛选合成选择任务，并利用DAPO强化学习训练小型推理模型（17亿参数）。实验结果表明，在数学基准（AIME24、AIME25、HMMT25）和代码基准（LiveCodeBench）上，这些小型模型持续优于提示和多数投票基线，常达到或超越更大模型的性能，且尽管仅基于较弱模型输出训练，仍能泛化到对更强模型输出的选择。</div>
</details>
</div>
<div class="card">
<div class="title">DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations</div>
<div class="meta-line">Authors: Minghao Li, Ruihang Wang, Rui Tan, Yonggang Wen</div>
<div class="meta-line">First: 2026-02-02T14:18:52+00:00 · Latest: 2026-02-02T14:18:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02137v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DCoPilot：面向动态数据中心运营的生成式AI赋能策略自适应</div>
<div class="mono" style="margin-top:8px">现代数据中心（DC）承载着人工智能（AI）专用设备，在高功率密度和快速变化的工作负载下运行，这使得分钟级的自适应对于安全且节能的运营至关重要。然而，手动设计分段深度强化学习（DRL）智能体无法跟上数据中心频繁的动态变化和服务级别协议（SLA）的更新。这种从规范到策略的滞后导致缺乏及时有效的控制策略，可能引发服务中断。为弥合这一差距，我们提出了DCoPilot，一个用于动态数据中心运营中生成控制策略的混合框架。DCoPilot协同融合了两种不同的生成范式：一是执行结构化奖励形式符号生成的大型语言模型（LLM），二是进行策略权重参数化生成的超网络。DCoPilot通过三个协调阶段运行：（i）仿真扩展，在不同仿真就绪（SimReady）场景中对候选奖励进行压力测试；（ii）元策略蒸馏，训练超网络以输出基于SLA和场景嵌入的条件化策略权重；（iii）在线自适应，实现针对更新规范的零样本策略生成。在涵盖多种数据中心组件的五个控制任务族中评估，DCoPilot实现了近乎零的约束违反，并在所有规范变体上优于基线方法。消融研究验证了基于LLM的统一奖励生成在实现超网络稳定收敛方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of adapting control policies in modern data centers, where high power densities and rapidly varying workloads necessitate minute-level adaptations for safety and energy efficiency, as manually designed deep reinforcement learning agents struggle to keep pace with frequent dynamic shifts and service-level agreement changes. The proposed method, DCoPilot, is a hybrid framework that synergizes a large language model for symbolic generation of structured reward forms and a hypernetwork for parametric generation of policy weights, operating through three phases: simulation scale-up for stress-testing reward candidates, meta policy distillation to train the hypernetwork, and online adaptation for zero-shot policy generation. Experimental results across five control task families show that DCoPilot achieves near-zero constraint violations and outperforms all baselines under specification variations, with ablation studies validating the effectiveness of the LLM-based reward generation in enabling stable hypernetwork convergence.</div>
<div class="mono" style="margin-top:8px">本文针对现代数据中心控制策略适应性的挑战，即高功率密度和快速变化的工作负载需要分钟级调整以确保安全和能效，而手动设计的深度强化学习代理难以跟上频繁的动态变化和服务级别协议更新。所提出的方法DCoPilot是一个混合框架，它结合了大型语言模型用于符号化生成结构化奖励形式，以及超网络用于参数化生成策略权重，通过三个阶段运行：模拟扩展以压力测试奖励候选方案、元策略蒸馏训练超网络，以及在线适应实现零样本策略生成。在五个控制任务系列上的实验结果表明，DCoPilot实现了接近零的约束违反，并在规格变化下优于所有基线方法，消融研究验证了基于LLM的统一奖励生成在促进超网络稳定收敛方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning</div>
<div class="meta-line">Authors: Keqin Peng, Yuanxin Ouyang, Xuebo Liu, Zhiliang Tian, Ruijian Han, Yancheng Yuan, Liang Ding</div>
<div class="meta-line">First: 2026-02-02T13:43:52+00:00 · Latest: 2026-02-02T13:43:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02099v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02099v1">PDF</a> · <a href="https://github.com/alphadl/DDCA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重密度而非长度：动态解耦条件优势实现高效推理</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）能够激发强大的多步推理能力，但常导致轨迹描述过于冗长。此外，组间相对优化中简单的长度惩罚会严重损害准确性。我们将此问题归因于两个结构性缺陷：（一）长度基线稀释：错误答案（零长度奖励）拉低组基线，过度惩罚正确解法；（二）难度-惩罚失配：静态惩罚无法适应问题难度，既抑制困难实例的必要推理，又无法消除简单任务中的冗余。为此，我们提出动态解耦条件优势（DDCA），将效率优化与正确性解耦。DDCA通过在正确答案簇内计算条件长度优势以消除基线稀释，并利用组通过率作为难度代理动态调整惩罚强度。在GSM8K、MATH500、AMC23和AIME25上的实验表明，相较于自适应基线方法，DDCA持续优化效率-准确性权衡：在简单任务（如GSM8K）上减少约60%的生成词元，在困难基准（如AIME25）上减少超20%，同时保持或提升准确性。代码发布于 https://github.com/alphadl/DDCA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency in Reinforcement Learning with Verifiable Rewards (RLVR), which often produces overly verbose reasoning traces, and notes that naive length penalties can harm accuracy due to structural issues like Dilution of Length Baseline and Difficulty-Penalty Mismatch. To solve this, the authors propose Dynamic Decoupled Conditional Advantage (DDCA), a method that decouples efficiency optimization from correctness by computing length advantages only within correct-response clusters to avoid baseline dilution and dynamically scaling penalty strength based on group pass rates as a difficulty proxy. Experimental results on benchmarks including GSM8K, MATH500, AMC23, and AIME25 demonstrate that DDCA consistently improves the efficiency-accuracy trade-off, reducing generated tokens by about 60% on simpler tasks like GSM8K and over 20% on harder ones like AIME25 while maintaining or enhancing accuracy.</div>
<div class="mono" style="margin-top:8px">该论文针对可验证奖励的强化学习（RLVR）中常产生冗长推理轨迹的低效问题，指出朴素长度惩罚会因结构性问题（如长度基线稀释和难度惩罚不匹配）损害准确性。为此，作者提出了动态解耦条件优势（DDCA）方法，通过仅在正确响应簇内计算长度优势以避免基线稀释，并基于组通过率作为难度代理动态调整惩罚强度，从而将效率优化与正确性解耦。在GSM8K、MATH500、AMC23和AIME25等基准测试上的实验结果表明，DDCA持续改善了效率与准确性的权衡，在简单任务（如GSM8K）上减少约60%的生成标记，在困难任务（如AIME25）上减少超过20%，同时保持或提升了准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning</div>
<div class="meta-line">Authors: Yannik Schnitzer, Mathias Jackermeier, Alessandro Abate, David Parker</div>
<div class="meta-line">First: 2026-02-02T13:41:47+00:00 · Latest: 2026-02-02T13:41:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02098v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多任务强化学习的概率性能保证</div>
<div class="mono" style="margin-top:8px">多任务强化学习训练能够执行多个任务的通用策略。尽管近年来取得显著进展，现有方法很少提供形式化的性能保证，而这在安全关键场景中部署策略时不可或缺。我们提出一种方法，用于计算多任务策略在训练期间未见任务上的高置信度性能保证。具体而言，我们引入一种新的泛化边界，该边界结合了（i）基于有限次轨迹的每任务置信下界与（ii）基于有限采样任务的任务级泛化能力，从而为从同一任意未知分布中抽取的新任务提供高置信度保证。在多种前沿多任务强化学习方法上的实验表明，该保证在理论上是可靠的，并在实际样本量下具有参考价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for formal performance guarantees when deploying multi-task reinforcement learning policies in safety-critical applications, as existing methods often lack such assurances. The authors introduce a novel generalization bound that combines per-task confidence intervals from limited rollouts with task-level generalization from a finite set of sampled tasks, providing high-confidence guarantees for new tasks drawn from an unknown distribution. Experimental results demonstrate that the proposed guarantees are both theoretically valid and practically informative across state-of-the-art multi-task RL methods, even with realistic sample sizes.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，在安全关键应用中部署多任务强化学习策略时，需要形式化的性能保证，而现有方法常缺乏此类保障。作者提出了一种新的泛化边界，将有限次任务执行中的每任务置信下限与有限采样任务的任务级泛化相结合，从而为从未知分布中抽取的新任务提供高置信度保证。实验结果表明，所提出的保证在先进的多任务强化学习方法中具有理论可靠性，并在实际样本量下提供有效信息。</div>
</details>
</div>
<div class="card">
<div class="title">Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification</div>
<div class="meta-line">Authors: Yiju Guo, Tianyi Hu, Zexu Sun, Yankai Lin</div>
<div class="meta-line">First: 2026-01-29T04:08:24+00:00 · Latest: 2026-02-02T13:30:50+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21244v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21244v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>降噪增声：基于指令净化的强化学习推理方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）虽推动了大型语言模型的推理能力，但在有限计算预算下，其低效探索仍导致采样成功率低、复杂任务训练不稳定。研究发现，许多探索失败源于少数提示词引入的干扰，而非问题本身难度。基于此，本文提出降噪采样框架（LENS），其首先通过识别并移除干扰词来净化提示，随后将净化过程中的成功轨迹迁移至原始含噪提示上，以监督策略优化，使模型学会在真实含噪提示环境中忽略干扰。实验表明，LENS显著优于GRPO，性能更高、收敛更快，平均提升3.88%，加速超1.6倍。本研究揭示了剪除干扰词对提升探索效率的关键作用，为RLVR研究提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the inefficiency of Reinforcement Learning with Verifiable Rewards (RLVR) in large language model reasoning, where limited rollout budgets lead to poor exploration and unstable training. The authors identify that many failures stem not from task complexity but from a few interfering tokens in the prompts. They propose the Less Noise Sampling Framework (LENS), which first purifies prompts by removing interference tokens to generate successful rollouts, then uses these to supervise policy optimization on the original noisy prompts, teaching the model to ignore interference. Experiments demonstrate that LENS outperforms GRPO with an average performance gain of 3.88% and achieves over 1.6× faster convergence, highlighting the importance of pruning interference tokens for efficient RLVR.</div>
<div class="mono" style="margin-top:8px">本研究针对基于可验证奖励的强化学习在大语言模型推理中效率低下的问题，即有限的探索预算导致采样成功率低和训练不稳定。作者发现许多失败并非源于任务难度，而是由提示中少数干扰性令牌引起。为此，他们提出了低噪声采样框架，该框架首先通过识别并移除干扰令牌来净化提示以生成成功的探索轨迹，随后利用这些轨迹监督模型在原始含噪提示上的策略优化，从而学会忽略干扰。实验结果表明，该框架显著优于GRPO方法，平均性能提升3.88%，收敛速度加快1.6倍以上，凸显了修剪干扰令牌对于提升强化学习探索效率的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification</div>
<div class="meta-line">Authors: Nan Qiao, Sheng Yue</div>
<div class="meta-line">First: 2026-02-02T12:57:09+00:00 · Latest: 2026-02-02T12:57:09+00:00</div>
<div class="meta-line">Comments: accetped by IEEE International Conference on Communications (ICC 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02055v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02055v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device&#x27;s suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FORLER：基于Q集成与执行器校正的联邦离线强化学习</div>
<div class="mono" style="margin-top:8px">在物联网系统中，联邦学习通过支持并行策略训练且无需共享原始数据，推动了在线强化学习的发展。然而，在线与真实环境交互存在风险且成本高昂，这促使了离线联邦强化学习的兴起，即本地设备从固定数据集中学习。尽管前景广阔，离线联邦强化学习在低质量、异构数据下可能失效。离线强化学习易陷入局部最优，而在联邦强化学习中，单个设备的次优策略可能损害聚合模型，即策略污染。本文提出FORLER，结合服务器端的Q集成聚合与设备端的执行器校正。服务器稳健地融合设备Q函数以抑制策略污染，并将繁重计算转移至资源受限的硬件之外，同时不损害隐私。在本地，执行器校正通过零阶搜索高Q值动作及定制化正则化器（推动策略向高Q动作靠近）来丰富策略梯度。采用δ周期策略进一步减少本地计算。我们从理论上提供了安全策略改进的性能保证。大量实验表明，在不同数据质量与异构性条件下，FORLER均持续优于强基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of offline federated reinforcement learning (FRL) in Internet-of-Things systems, where low-quality and heterogeneous local datasets can lead to policy pollution and suboptimal performance. The proposed method, FORLER, combines server-side Q-ensemble aggregation to robustly merge device Q-functions and mitigate policy pollution, with local actor rectification that enriches policy gradients via zeroth-order search and regularization, alongside a δ-periodic strategy to reduce computation. Experimental results demonstrate that FORLER consistently outperforms strong baselines across varying data quality and heterogeneity, with theoretical guarantees for safe policy improvement.</div>
<div class="mono" style="margin-top:8px">本文针对物联网系统中离线联邦强化学习的挑战，即低质量和异构的本地数据可能导致策略污染和性能不佳。提出的FORLER方法结合了服务器端的Q-集合聚合，以稳健合并设备Q函数并减轻策略污染，以及本地执行器校正，通过零阶搜索和正则化丰富策略梯度，辅以δ周期策略降低计算开销。实验结果表明，在不同数据质量和异构性下，FORLER始终优于强基线方法，并提供了安全策略改进的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization</div>
<div class="meta-line">Authors: Ahmad Farooq, Kamran Iqbal</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-02T12:32:28+00:00 · Latest: 2026-02-02T12:32:28+00:00</div>
<div class="meta-line">Comments: Accepted at the 2026 IEEE International Conference on Robotics and Automation (ICRA 2026), Vienna, Austria. 9 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02035v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信息瓶颈与矢量量化的带宽高效多智能体通信</div>
<div class="mono" style="margin-top:8px">现实机器人应用中的多智能体强化学习系统面临严峻的通信约束，显著影响协同效能。我们提出一种融合信息瓶颈理论与矢量量化的框架，实现多智能体环境中的选择性带宽高效通信。该方法通过基于信息论的优化原则，学习压缩与离散化通信消息，同时保留任务关键信息。我们引入门控通信机制，可根据环境上下文与智能体状态动态判定通信时机。在复杂协同任务上的实验表明，本方法较无通信基线实现181.8%的性能提升，同时降低41.4%的带宽占用。完整的帕累托前沿分析显示，本方法在成功率-带宽全域占据主导地位（曲线下面积0.198，次优方法为0.142）。该方法显著优于现有通信策略，为机器人集群、自动驾驶车队、分布式传感器网络等带宽受限环境中的多智能体系统部署建立了理论完备的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the severe communication constraints in real-world multi-agent robotics applications, this paper introduces a framework combining information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication. The method learns to compress and discretize messages while preserving task-critical information through information-theoretic optimization and employs a gated mechanism to dynamically determine communication necessity. Experimental results on coordination tasks show the approach achieves a 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%, with Pareto analysis demonstrating dominance across the success-bandwidth spectrum.</div>
<div class="mono" style="margin-top:8px">本文针对现实多智能体机器人应用中严重的通信约束问题，提出了一种结合信息瓶颈理论与矢量量化的框架，以实现选择性、带宽高效的通信。该方法通过信息论优化学习压缩和离散化消息，同时保留任务关键信息，并采用门控机制动态决定通信时机。在协调任务上的实验结果表明，该方法相比无通信基线实现了181.8%的性能提升，同时减少了41.4%的带宽使用，帕累托前沿分析显示其在成功率-带宽谱上具有全面优势。</div>
</details>
</div>
<div class="card">
<div class="title">Your Latent Reasoning is Secretly Policy Improvement Operator</div>
<div class="meta-line">Authors: Arip Asadulaev, Rayan Banerjee, Fakhri Karray, Martin Takac</div>
<div class="meta-line">First: 2025-11-21T01:54:23+00:00 · Latest: 2026-02-02T11:47:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16886v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.16886v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, small models with latent recursion have obtained promising results on complex reasoning tasks. These results are typically explained by the theory that such recursion increases a networks depth, allowing it to compactly emulate the capacity of larger models. However, the performance of recursively added layers remains behind the capabilities of one pass models with the same feed forward depth. This means that in the looped version, not every recursive step effectively contributes to depth. This raises the question: when and why does latent reasoning improve performance, and when does it result in dead compute? In our work, we analyze the algorithms that latent reasoning provides answer to this question. We show that latent reasoning can be formalized as a classifier free guidance and policy improvement algorithm. Building on these insights, we propose to use a training schemes from reinforcement learning and diffusion methods for latent reasoning models. Using the Tiny Recursive Model as our testbed, we show that with our modifications we can avoid dead compute steps and reduce the total number of forward passes by 18x while maintaining performance. Broadly speaking, we show how a policy improvement perspective on recursive steps can explain model behavior and provide insights for further improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在推理实为策略改进算子</div>
<div class="mono" style="margin-top:8px">近期，采用潜在递归的小型模型在复杂推理任务中取得了显著成果。通常认为，这种递归通过增加网络深度，使模型能紧凑模拟更大模型的容量。然而，递归添加层的性能仍落后于具有相同前馈深度的单次推理模型，表明并非每个递归步骤都能有效贡献深度。这引出一个核心问题：潜在推理何时及为何能提升性能，何时会导致无效计算？本研究通过分析潜在推理的算法机制对此进行解答。我们证明潜在推理可形式化为无分类器引导与策略改进算法，并基于此提出结合强化学习与扩散方法的训练方案。以微型递归模型为测试平台，改进后的模型在保持性能的同时，避免了无效计算步骤，并将总前向传播次数减少18倍。总体而言，本研究通过策略改进视角阐释递归步骤的行为机制，并为模型优化提供了新思路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates why latent recursive reasoning in small models sometimes fails to improve performance, a phenomenon termed &#x27;dead compute&#x27;. The authors propose that latent reasoning can be formally understood as a policy improvement algorithm, akin to techniques in reinforcement learning and classifier-free guidance from diffusion models. Motivated by this insight, they introduce new training schemes derived from these fields into a Tiny Recursive Model testbed. Their main experimental result demonstrates that these modifications successfully eliminate ineffective recursive steps, allowing the model to reduce the total number of required forward passes by 18 times while preserving its original task performance.</div>
<div class="mono" style="margin-top:8px">本文研究了小型模型的潜在递归推理为何有时无法提升性能，即所谓的“无效计算”现象。作者提出，潜在推理可被形式化地理解为一种策略改进算法，类似于强化学习和扩散模型中分类器引导的技术。基于这一见解，他们将源自这些领域的新训练方案引入到一个微型递归模型测试平台中。主要的实验结果表明，这些修改成功消除了无效的递归步骤，使模型在保持原有任务性能的同时，将所需的前向传播总次数减少了18倍。</div>
</details>
</div>
<div class="card">
<div class="title">Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</div>
<div class="meta-line">Authors: Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu</div>
<div class="meta-line">First: 2025-12-22T18:51:48+00:00 · Latest: 2026-02-02T11:29:09+00:00</div>
<div class="meta-line">Comments: Preprint. Our code is available at https://github.com/Trae1ounG/BuPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19673v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19673v2">PDF</a> · <a href="https://github.com/Trae1ounG/BuPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a unified policy, overlooking their internal mechanisms. In this paper, we decompose the LLM-based policy into Internal Layer Policies and Internal Modular Policies via Transformer&#x27;s residual stream. Our entropy analysis on internal policy reveals distinct patterns: (1) universally, policies evolve from high-entropy exploration in early layers to deterministic refinement in top layers; and (2) Qwen exhibits a progressive, human-like reasoning structure, contrasting with the abrupt final-layer convergence in Llama. Furthermore, we discover that optimizing internal layers induces feature refinement, forcing lower layers to capture high-level reasoning representations early. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that reconstructs the LLM&#x27;s reasoning foundation from the bottom up by optimizing internal layers in early stages. Extensive experiments on complex reasoning benchmarks demonstrate the effectiveness of BuPO. Our code is available at https://github.com/Trae1ounG/BuPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自底向上策略优化：语言模型策略中潜藏的内部策略</div>
<div class="mono" style="margin-top:8px">现有强化学习方法将大语言模型视为统一策略，忽略了其内部机制。本文通过Transformer残差流将基于LLM的策略分解为内部层策略和内部模块化策略。内部策略的熵分析揭示了两种典型模式：（1）普遍而言，策略从底层的高熵探索逐渐演变为顶层的确定性优化；（2）Qwen展现出渐进式、类人的推理结构，与Llama在最终层的突变式收敛形成对比。进一步研究发现，优化内部层会引发特征精炼，迫使底层更早捕获高层推理表征。基于这些发现，我们提出自底向上策略优化——一种通过在早期阶段优化内部层、自底向上重构LLM推理基础的新型强化学习范式。在复杂推理基准测试上的大量实验验证了BuPO的有效性。代码已开源：https://github.com/Trae1ounG/BuPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that existing reinforcement learning methods treat large language models as monolithic policies, overlooking their internal mechanisms, this paper proposes a decomposition of the LLM-based policy into Internal Layer Policies and Internal Modular Policies via the Transformer&#x27;s residual stream. The method involves an entropy analysis revealing that policies generally evolve from high-entropy exploration in early layers to deterministic refinement in later ones, with models like Qwen showing progressive reasoning unlike Llama&#x27;s abrupt convergence. Based on this, the authors introduce Bottom-up Policy Optimization (BuPO), a novel RL paradigm that optimizes internal layers from early stages to reconstruct reasoning foundations. Main experimental results on complex reasoning benchmarks demonstrate the effectiveness of BuPO in enhancing model performance.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有强化学习方法将大语言模型视为统一策略，忽略了其内部机制。方法上，通过Transformer残差流将基于LLM的策略分解为内部层策略和内部模块化策略，并基于熵分析发现策略普遍从早期层的高熵探索演变为顶层的确定性优化，且Qwen模型展现出渐进式类人推理，与Llama的最终层突变收敛形成对比。受此启发，作者提出了自底向上策略优化（BuPO），这是一种通过早期优化内部层来重构推理基础的新强化学习范式。在复杂推理基准上的主要实验结果证明了BuPO的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Performative Policy Gradient: Optimality in Performative Reinforcement Learning</div>
<div class="meta-line">Authors: Debabrota Basu, Udvas Das, Brahim Driss, Uddalak Mukherjee</div>
<div class="meta-line">First: 2025-12-23T18:20:06+00:00 · Latest: 2026-02-02T11:24:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20576v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20576v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms the existing performative RL algorithms aiming for stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>表演性策略梯度：表演性强化学习中的最优性</div>
<div class="mono" style="margin-top:8px">部署后的机器学习算法常影响其作用的环境，从而改变标准强化学习（RL）方法所忽略的底层动态。尽管在监督学习中已开始研究这一表演性设定下的最优算法设计，但RL领域的对应研究仍显不足。本文证明了RL中性能差异引理与策略梯度定理的表演性对应形式，并进一步提出了表演性策略梯度算法（PePG）。PePG是首个为处理RL中的表演性而设计的策略梯度算法。在softmax参数化下，无论是否采用熵正则化，我们均证明PePG能收敛至表演性最优策略，即能在自身引发的分布偏移下保持最优的策略。因此，PePG显著扩展了先前仅实现表演性稳定性而非最优性的表演性RL研究。此外，我们在标准表演性RL环境中的实证分析验证了PePG优于现有追求稳定性的表演性RL算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of performative reinforcement learning, where deployed policies influence and shift their own environment dynamics, a phenomenon ignored by standard RL. The authors introduce the Performative Policy Gradient (PePG) algorithm, derived by proving performative versions of key RL lemmas, to explicitly account for such distribution shifts. Experimental results on standard environments demonstrate that PePG converges to performatively optimal policies, outperforming prior methods that only achieve stability, thereby ensuring policies remain optimal under the changes they induce.</div>
<div class="mono" style="margin-top:8px">本文针对执行性强化学习中的挑战展开研究，即已部署的策略会影响并改变其自身环境动态，而标准强化学习方法忽略了这一现象。作者通过证明关键强化学习引理在执行性设定下的对应形式，提出了执行性策略梯度算法（PePG），以显式地处理此类分布偏移。在标准环境上的实验结果表明，PePG能够收敛至执行性最优策略，其性能优于仅追求稳定性的现有方法，从而确保策略在自身引发的变化下保持最优。</div>
</details>
</div>
<div class="card">
<div class="title">Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models</div>
<div class="meta-line">Authors: Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang, Weijie Liu, Clive Bai, Kai Yang, Yangkun Chen, Saiyong Yang, Xiangyang Ji</div>
<div class="meta-line">First: 2026-02-02T11:24:36+00:00 · Latest: 2026-02-02T11:24:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01970v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01970v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#x27;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小型可泛化提示预测模型可引导大型推理模型的高效强化学习后训练</div>
<div class="mono" style="margin-top:8px">强化学习能提升大型语言模型的推理能力，但因其依赖大量计算展开的优化过程，常伴随高昂计算成本。在线提示选择通过优先选取信息量丰富的提示来提高训练效率，是一种可行的解决方案。然而，现有方法要么依赖成本高昂的精确评估，要么构建缺乏跨提示泛化能力的提示专用预测模型。本研究提出可泛化预测提示选择方法，利用基于共享优化历史训练的轻量生成模型进行提示难度的贝叶斯推断。该方法将中等难度优先原则和历史锚定多样性纳入批量获取准则，以选择信息量高的提示批次。该小型预测模型在测试时亦能泛化，实现高效计算资源分配。跨多种推理基准的实验表明，GPS在训练效率、最终性能及测试时效率上均显著优于先进的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high computational cost of reinforcement learning for large language models by proposing a method for efficient online prompt selection. The motivation is to prioritize informative prompts during training to reduce rollout-intensive optimization. The method, called Generalizable Predictive Prompt Selection (GPS), uses a lightweight generative model trained on shared optimization history to perform Bayesian inference on prompt difficulty, incorporating intermediate-difficulty prioritization and history-anchored diversity for batch selection. Experimental results across various reasoning benchmarks show that GPS significantly improves training efficiency, final performance, and test-time efficiency compared to strong baselines.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型强化学习的高计算成本问题，提出了一种高效的在线提示选择方法，旨在通过优先选择信息丰富的提示来减少训练中的密集优化开销。该方法称为通用预测提示选择（GPS），利用在共享优化历史上训练的轻量生成模型进行提示难度的贝叶斯推断，并结合中等难度优先和历史锚定多样性原则进行批量选择。在不同推理基准上的实验结果表明，GPS在训练效率、最终性能和测试时效率方面均显著优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Investigating the Interplay of Parameterization and Optimizer in Gradient-Free Topology Optimization: A Cantilever Beam Case Study</div>
<div class="meta-line">Authors: Jelle Westra, Iván Olarte Rodríguez, Niki van Stein, Thomas Bäck, Elena Raponi</div>
<div class="meta-line">First: 2026-01-29T19:09:05+00:00 · Latest: 2026-02-02T11:12:21+00:00</div>
<div class="meta-line">Comments: 16 pages, 6 figures, 3 tables, Paper submitted and accepted at Evostar 2026 Conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22241v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22241v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gradient-free black-box optimization (BBO) is widely used in engineering design and provides a flexible framework for topology optimization (TO), enabling the discovery of high-performing structural designs without requiring gradient information from simulations. Yet, its success depends on two key choices: the geometric parameterization defining the search space and the optimizer exploring it.
  This study investigates this interplay through a compliance minimization problem for a cantilever beam subject to a connectivity constraint. We benchmark three geometric parameterizations, each combined with three representative BBO algorithms: differential evolution, covariance matrix adaptation evolution strategy, and heteroscedastic evolutionary Bayesian optimization, across 10D, 20D, and 50D design spaces.
  Results reveal that parameterization quality has a stronger influence on optimization performance than optimizer choice: a well-structured parameterization enables robust and competitive performance across algorithms, whereas weaker representations increase optimizer dependency. Overall, this study highlights the dominant role of geometric parameterization in practical BBO-based TO and shows that algorithm performance and selection cannot be fairly assessed without accounting for the induced design space.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无梯度拓扑优化中参数化与优化器交互作用研究：以悬臂梁为例</div>
<div class="mono" style="margin-top:8px">无梯度黑箱优化在工程设计中广泛应用，为拓扑优化提供了灵活框架，可在无需仿真梯度信息的情况下发现高性能结构设计。但其成功取决于两个关键选择：定义搜索空间的几何参数化方法和探索该空间的优化器。本研究通过带连通性约束的悬臂梁柔度最小化问题探讨这种交互作用。我们在10维、20维和50维设计空间中，对三种几何参数化方法分别结合三种代表性黑箱优化算法（差分进化、协方差矩阵自适应进化策略、异方差进化贝叶斯优化）进行基准测试。结果表明，参数化质量对优化性能的影响强于优化器选择：结构良好的参数化能使各算法均获得稳健且具竞争力的性能，而较弱的参数化表示则会增强对优化器的依赖性。本研究总体上凸显了几何参数化在基于黑箱优化的实际拓扑优化中的主导作用，并表明若未考虑参数化所诱导的设计空间，则无法公平评估算法性能与选择。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the interplay between geometric parameterization and optimizer selection in gradient-free topology optimization, motivated by the need to understand which factor more critically influences the discovery of high-performing structural designs without gradient information. The method involves benchmarking three geometric parameterizations combined with three black-box optimization algorithms—differential evolution, covariance matrix adaptation evolution strategy, and heteroscedastic evolutionary Bayesian optimization—on a cantilever beam compliance minimization problem across 10D, 20D, and 50D design spaces. Experimental results demonstrate that parameterization quality exerts a stronger influence on optimization performance than optimizer choice, with well-structured parameterizations enabling robust performance across algorithms, whereas weaker representations increase dependency on the specific optimizer, highlighting the dominant role of geometric parameterization in practical applications.</div>
<div class="mono" style="margin-top:8px">本研究探讨了无梯度拓扑优化中几何参数化与优化器选择之间的相互作用，动机在于理解在没有梯度信息的情况下，哪个因素更关键地影响高性能结构设计的发现。方法包括在悬臂梁柔度最小化问题上，将三种几何参数化与三种黑盒优化算法——差分进化、协方差矩阵自适应进化策略和异方差进化贝叶斯优化——在10维、20维和50维设计空间中进行基准测试。实验结果表明，参数化质量对优化性能的影响比优化器选择更强，结构良好的参数化能在不同算法中实现稳健的性能，而较弱的表示则增加了对特定优化器的依赖，这凸显了几何参数化在实际应用中的主导作用。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Off-Policy Learning</div>
<div class="meta-line">Authors: Arip Asadulaev, Maksim Bobrin, Salem Lahlou, Dmitry Dylov, Fakhri Karray, Martin Takac</div>
<div class="meta-line">First: 2026-02-02T11:06:31+00:00 · Latest: 2026-02-02T11:06:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01962v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01962v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>零样本离策略学习</div>
<div class="mono" style="margin-top:8px">离策略学习方法旨在直接从固定的历史交互数据集中推导出最优策略。这一目标面临重大挑战，主要源于固有的分布偏移和价值函数高估偏差。在零样本强化学习中，这些问题尤为突出，因为智能体需在无奖励数据上训练，并在测试时无需额外训练即可适应新任务。本研究通过建立后继度量与稳态密度比的理论关联，解决了零样本环境下的离策略问题。基于此洞见，我们的算法能够推断最优重要性采样比，实时执行稳态分布校正并生成适用于任意任务的最优策略。我们在SMPL Humanoid的运动追踪任务、ExoRL的连续控制任务以及长周期OGBench任务上对方法进行了基准测试。该技术可无缝集成至前向-后向表示框架，并在免训练机制中实现对新任务的快速适应。更广泛而言，本工作架起了离策略学习与零样本适应之间的桥梁，为两个研究领域均带来增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenges of off-policy learning, such as distributional shift and value overestimation, which are exacerbated in zero-shot reinforcement learning where an agent must adapt to new tasks without further training. The method establishes a theoretical link between successor measures and stationary density ratios, enabling the algorithm to infer optimal importance sampling ratios for on-the-fly stationary distribution correction with an optimal policy for any task. Experimental results demonstrate the approach&#x27;s effectiveness in motion tracking on SMPL Humanoid, continuous control on ExoRL, and long-horizon OGBench tasks, showing seamless integration into representation frameworks and fast adaptation without training.</div>
<div class="mono" style="margin-top:8px">本文的动机源于离线策略学习中的分布偏移和价值高估等挑战，这些问题在零样本强化学习中尤为突出，即智能体必须在无需额外训练的情况下适应新任务。该方法通过建立后继度量与平稳密度比之间的理论联系，使算法能够推断最优重要性采样比率，从而为任意任务实时执行带有最优策略的平稳分布校正。实验结果表明，该方法在SMPL Humanoid的运动跟踪、ExoRL的连续控制以及长时程OGBench任务中表现有效，实现了与表示框架的无缝集成和无需训练的快速适应。</div>
</details>
</div>
<div class="card">
<div class="title">Extending RLVR to Open-Ended Tasks via Verifiable Multiple-Choice Reformulation</div>
<div class="meta-line">Authors: Mengyu Zhang, Siyu Ding, Weichong Yin, Yu Sun</div>
<div class="meta-line">First: 2025-11-04T10:45:52+00:00 · Latest: 2026-02-02T10:35:44+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02463v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.02463v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards(RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs). However, its success has thus far been largely confined to the mathematical and programming domains with clear and automatically checkable outcomes. Reinforcement learning on open-ended tasks (e.g., creative writing and subjective Q&amp;A) continues to rely on reward models due to the absence of verifiable solutions. This raises a key question: how can we extend RLVR to strengthen reasoning in open-ended tasks regardless of the absence of the unambiguous ground truth? To overcome this challenge, we introduce Verifiable Multiple-Choice Reformulation for Reinforcement Learning from Verifiable Rewards (VMR-RLVR), a novel training strategy that restructures open-ended data into verifiable multiple-choice formats, enabling effective training even in the absence of explicit ground truth. Experimental results on multiple benchmarks validate the effectiveness of our method in improving LLM performance on open-ended tasks. Notably, across seven open-ended benchmarks, our VMR-RLVR training delivers an average gain of 3.29 points over the RL with reward model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过可验证多选题重构将RLVR扩展至开放式任务</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）在提升大语言模型（LLMs）推理能力方面展现出巨大潜力，但其成功目前主要局限于具有明确可自动验证结果的数学与编程领域。由于缺乏可验证的解决方案，开放式任务（如创意写作和主观问答）的强化学习仍依赖奖励模型。这引出一个关键问题：如何在缺乏明确标准答案的情况下扩展RLVR以增强开放式任务的推理能力？为应对这一挑战，我们提出基于可验证奖励的强化学习之可验证多选题重构（VMR-RLVR），这是一种将开放式数据重构为可验证多选题格式的新型训练策略，即使在缺乏显式标准答案的情况下也能实现有效训练。多个基准测试的实验结果验证了本方法在提升LLM开放式任务性能方面的有效性。值得注意的是，在七个开放式任务基准上，我们的VMR-RLVR训练相比基于奖励模型的强化学习平均提升3.29分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitation of Reinforcement Learning with Verifiable Rewards (RLVR), which has been effective in domains like mathematics with clear answers but struggles with open-ended tasks such as creative writing due to the lack of unambiguous ground truth for verification. To address this, the authors propose VMR-RLVR, a method that reformulates open-ended tasks into a verifiable multiple-choice format, allowing the application of RLVR even without explicit correct solutions. Experimental results across seven open-ended benchmarks demonstrate the method&#x27;s effectiveness, showing an average performance improvement of 3.29 points over traditional reinforcement learning using reward models.</div>
<div class="mono" style="margin-top:8px">本文的动机在于可验证奖励的强化学习（RLVR）存在局限性，该方法在数学等有明确答案的领域表现良好，但由于缺乏明确的真实答案进行验证，难以应用于创意写作等开放式任务。为解决这一问题，作者提出了VMR-RLVR方法，将开放式任务重新构建为可验证的多选题形式，从而使得RLVR能够在没有显式正确答案的情况下有效应用。在七个开放式基准测试上的实验结果表明，该方法显著提升了大型语言模型的性能，相比使用奖励模型的传统强化学习平均提高了3.29分。</div>
</details>
</div>
<div class="card">
<div class="title">VLM-Guided Experience Replay</div>
<div class="meta-line">Authors: Elad Sharony, Tom Jurgenson, Orr Krupnik, Dotan Di Castro, Shie Mannor</div>
<div class="meta-line">First: 2026-02-02T10:19:59+00:00 · Latest: 2026-02-02T10:19:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01915v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01915v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://esharony.me/projects/vlm-rb/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent&#x27;s experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLM引导的经验回放</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）和视觉语言模型（VLMs）的最新进展赋予了强大的语义与多模态推理能力，为提升强化学习（RL）中的样本效率、高层规划及可解释性创造了新机遇。尽管先前研究已将LLMs和VLMs集成到RL的多个组件中，但作为存储与重用经验核心模块的回放缓冲区尚未得到探索。我们提出利用VLMs引导回放缓冲区中的经验优先级排序以填补这一空白。核心思路是使用无需微调的冻结预训练VLM作为自动评估器，从智能体经验中识别并优先处理有潜力的子轨迹。在涵盖离散与连续领域的游戏和机器人等多种场景中，采用本优先级排序方法训练的智能体，其平均成功率较以往方法提升11-52%，样本效率提高19-45%。https://esharony.me/projects/vlm-rb/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the untapped potential of Vision-Language Models (VLMs) in reinforcement learning, this paper introduces a method to enhance sample efficiency by using a frozen, pre-trained VLM to automatically evaluate and prioritize promising sub-trajectories within the replay buffer. The core approach requires no fine-tuning of the VLM, applying it as a semantic evaluator to guide experience replay. Experimental results across game-playing and robotics tasks in both discrete and continuous domains show that agents using this VLM-guided prioritization achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to prior methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是利用视觉语言模型在强化学习中尚未开发的潜力，提出了一种通过使用冻结的预训练VLM自动评估并优先回放经验回放缓冲区中有前景的子轨迹，以提高样本效率的方法。该方法的核心是无需微调VLM，将其作为语义评估器来指导经验回放。在涵盖离散和连续领域的游戏与机器人任务实验中，采用这种VLM引导优先级的智能体，其平均成功率比先前方法提高了11-52%，样本效率提升了19-45%。</div>
</details>
</div>
<div class="card">
<div class="title">Reusing Trajectories in Policy Gradients Enables Fast Convergence</div>
<div class="meta-line">Authors: Alessandro Montenegro, Federico Mansutti, Marco Mussi, Matteo Papini, Alberto Maria Metelli</div>
<div class="meta-line">First: 2025-06-06T15:42:15+00:00 · Latest: 2026-02-02T10:02:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06178v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.06178v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy gradient (PG) methods are a class of effective reinforcement learning algorithms, particularly when dealing with continuous control problems. They rely on fresh on-policy data, making them sample-inefficient and requiring $O(ε^{-2})$ trajectories to reach an $ε$-approximate stationary point. A common strategy to improve efficiency is to reuse information from past iterations, such as previous gradients or trajectories, leading to off-policy PG methods. While gradient reuse has received substantial attention, leading to improved rates up to $O(ε^{-3/2})$, the reuse of past trajectories, although intuitive, remains largely unexplored from a theoretical perspective. In this work, we provide the first rigorous theoretical evidence that reusing past off-policy trajectories can significantly accelerate PG convergence. We propose RT-PG (Reusing Trajectories - Policy Gradient), a novel algorithm that leverages a power mean-corrected multiple importance weighting estimator to effectively combine on-policy and off-policy data coming from the most recent $ω$ iterations. Through a novel analysis, we prove that RT-PG achieves a sample complexity of $\widetilde{O}(ε^{-2}ω^{-1})$. When reusing all available past trajectories, this leads to a rate of $\widetilde{O}(ε^{-1})$, the best known one in the literature for PG methods. We further validate our approach empirically, demonstrating its effectiveness against baselines with state-of-the-art rates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略梯度中轨迹复用实现快速收敛</div>
<div class="mono" style="margin-top:8px">策略梯度（PG）方法是一类高效的强化学习算法，尤其在处理连续控制问题时表现突出。这类方法依赖即时策略数据，导致样本效率低下，需要$O(ε^{-2})$条轨迹才能达到$ε$近似驻点。提升效率的常见策略是复用历史迭代信息（如过往梯度或轨迹），由此衍生出离策略PG方法。梯度复用已获广泛研究，收敛速率可提升至$O(ε^{-3/2})$；而轨迹复用虽具直观性，其理论机制尚未充分探索。本研究首次通过严格理论证明：复用历史离策略轨迹能显著加速PG收敛。我们提出RT-PG（轨迹复用-策略梯度）算法，该算法采用幂均值校正的多重重要性加权估计器，有效融合最近$ω$次迭代产生的策略与离策略数据。通过创新性理论分析，我们证明RT-PG实现$\widetilde{O}(ε^{-2}ω^{-1})$的样本复杂度。当复用全部历史轨迹时，收敛速率可达$\widetilde{O}(ε^{-1})$，这是目前PG方法理论最优速率。实验进一步验证了该方法的有效性，其性能优于具备最先进速率的基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sample inefficiency of standard policy gradient methods, which require fresh on-policy data and exhibit slow convergence, this paper introduces RT-PG, a novel algorithm that reuses past off-policy trajectories to accelerate learning. The method employs a power mean-corrected multiple importance weighting estimator to effectively combine on-policy data with off-policy data from recent iterations. Theoretically, RT-PG achieves a sample complexity of Õ(ε⁻²ω⁻¹), leading to an optimal rate of Õ(ε⁻¹) when fully reusing past trajectories, which is the best known convergence rate for policy gradient methods, and empirical results validate its effectiveness against state-of-the-art baselines.</div>
<div class="mono" style="margin-top:8px">针对标准策略梯度方法因依赖即时策略数据而导致样本效率低、收敛慢的问题，本文提出了RT-PG算法，通过重用过去的离策略轨迹来加速学习。该方法采用幂均值校正的多重重要性加权估计器，有效结合当前策略数据与最近迭代中的离策略数据。理论分析证明，RT-PG实现了Õ(ε⁻²ω⁻¹)的样本复杂度，当完全重用所有历史轨迹时，达到Õ(ε⁻¹)的最优收敛速率，这是策略梯度方法中已知的最佳速率，实验结果表明该算法优于现有先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control</div>
<div class="meta-line">Authors: Zijie Xu, Tong Bu, Zecheng Hao, Jianhao Ding, Zhaofei Yu</div>
<div class="meta-line">First: 2025-05-30T03:08:03+00:00 · Latest: 2026-02-02T09:52:11+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.24161v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.24161v3">PDF</a> · <a href="https://github.com/xuzijie32/Proxy-Target">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spiking Neural Networks (SNNs) offer low-latency and energy-efficient decision making on neuromorphic hardware, making them attractive for Reinforcement Learning (RL) in resource-constrained edge devices. However, most RL algorithms for continuous control are designed for Artificial Neural Networks (ANNs), particularly the target network soft update mechanism, which conflicts with the discrete and non-differentiable dynamics of spiking neurons. We show that this mismatch destabilizes SNN training and degrades performance. To bridge the gap between discrete SNNs and continuous-control algorithms, we propose a novel proxy target framework. The proxy network introduces continuous and differentiable dynamics that enable smooth target updates, stabilizing the learning process. Since the proxy operates only during training, the deployed SNN remains fully energy-efficient with no additional inference overhead. Extensive experiments on continuous control benchmarks demonstrate that our framework consistently improves stability and achieves up to $32\%$ higher performance across various spiking neuron models. Notably, to the best of our knowledge, this is the first approach that enables SNNs with simple Leaky Integrate and Fire (LIF) neurons to surpass their ANN counterparts in continuous control. This work highlights the importance of SNN-tailored RL algorithms and paves the way for neuromorphic agents that combine high performance with low power consumption. Code is available at https://github.com/xuzijie32/Proxy-Target.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理目标：弥合离散脉冲神经网络与连续控制之间的鸿沟</div>
<div class="mono" style="margin-top:8px">脉冲神经网络（SNNs）在神经形态硬件上提供低延迟、高能效的决策能力，使其在资源受限的边缘设备强化学习（RL）中具有吸引力。然而，大多数连续控制RL算法是为人工神经网络（ANNs）设计的，特别是目标网络软更新机制，这与脉冲神经元的离散、不可微动力学特性相冲突。我们证明这种不匹配会破坏SNN训练的稳定性并降低性能。为弥合离散SNNs与连续控制算法之间的差距，我们提出了一种新颖的代理目标框架。该代理网络引入连续且可微的动力学，实现平滑的目标更新，从而稳定学习过程。由于代理仅在训练期间运行，部署的SNN仍保持完全能效，无额外推理开销。在连续控制基准上的大量实验表明，我们的框架持续提升稳定性，并在多种脉冲神经元模型上实现高达32%的性能提升。值得注意的是，据我们所知，这是首次使采用简单漏积分发放（LIF）神经元的SNNs在连续控制任务中超越其ANN对应模型的方法。这项工作强调了为SNN定制RL算法的重要性，并为结合高性能与低功耗的神经形态智能体铺平了道路。代码发布于https://github.com/xuzijie32/Proxy-Target。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that while Spiking Neural Networks (SNNs) are energy-efficient for reinforcement learning on edge devices, their discrete, non-differentiable nature conflicts with the continuous target network updates used in standard continuous control algorithms, destabilizing training. The proposed method is a proxy target framework, which introduces a continuous and differentiable proxy network during training to enable smooth target updates, while the final deployed SNN remains efficient without inference overhead. The main experimental results show that this framework consistently improves training stability and achieves performance gains of up to 32% on continuous control benchmarks, and notably enables SNNs with simple LIF neurons to outperform their artificial neural network counterparts for the first time in this domain.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管脉冲神经网络（SNN）在边缘设备的强化学习中具有高能效优势，但其离散、不可微的特性与标准连续控制算法中使用的连续目标网络更新机制相冲突，导致训练不稳定。所提出的方法是一种代理目标框架，在训练期间引入一个连续且可微的代理网络以实现平滑的目标更新，而最终部署的SNN保持高效且无额外推理开销。主要实验结果表明，该框架显著提升了训练稳定性，在连续控制基准测试中性能最高提升32%，并且首次使得采用简单漏积分发放（LIF）神经元的SNN在此类任务中超越了人工神经网络（ANN）的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal</div>
<div class="meta-line">Authors: Zichun Wang, Gar Goei Loke, Ruiting Zuo</div>
<div class="meta-line">First: 2026-02-02T09:49:51+00:00 · Latest: 2026-02-02T09:49:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01877v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自相关优化-通过-估计：预测后优化与有限样本最优解对比</div>
<div class="mono" style="margin-top:8px">在有限样本条件下直接优化样本外性能的模型，已成为数据驱动优化中传统“先估计后优化”方法的有力替代方案。本研究在自相关不确定性（具体为向量自回归移动平均VARMA(p,q)过程）背景下比较了二者的性能。我们提出了一种自相关优化-通过-估计（A-OVE）模型，该模型通过充分统计量函数获得样本外最优解，并给出了计算其充分统计量的递归形式。我们在包含交易成本的组合优化问题上评估了这些模型。相较于完全信息预言机，A-OVE实现了较低遗憾值，且优于预测后优化的机器学习基准模型。值得注意的是，精度更高的机器学习模型可能产生更差的决策质量，这与数据驱动优化领域日益增多的研究结论一致。该模型在轻微设定偏误下仍能保持性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve out-of-sample decision quality in data-driven optimization under autocorrelated uncertainties, this paper proposes an autocorrelated Optimize-via-Estimate (A-OVE) model that directly optimizes for finite-sample performance. The method formulates the optimal solution as a function of sufficient statistics derived from a VARMA process and introduces a recursive computation for these statistics. Experimental results on a portfolio optimization problem with transaction costs show that A-OVE achieves low regret compared to a perfect information oracle and outperforms standard predict-then-optimize benchmarks, while also demonstrating that higher prediction accuracy does not necessarily translate to better decision quality and that the model remains robust to small misspecifications.</div>
<div class="mono" style="margin-top:8px">本文的动机是在自相关不确定性的数据驱动优化中提升样本外决策质量，提出了一种自相关的通过估计优化模型，该模型直接针对有限样本性能进行优化。该方法基于向量自回归移动平均过程，将最优解表达为充分统计量的函数，并提出了递归计算这些统计量的形式。在包含交易成本的组合优化问题上的实验结果表明，该模型相比完美信息基准实现了较低遗憾值，优于传统的预测后优化基准；同时指出更高的预测准确性未必带来更好的决策质量，且模型在小幅设定错误下仍保持性能。</div>
</details>
</div>
<div class="card">
<div class="title">Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning</div>
<div class="meta-line">Authors: Xiangkun Wu, Qianglin Wen, Yingying Zhang, Hongtu Zhu, Ting Li, Chengchun Shi</div>
<div class="meta-line">First: 2026-02-02T09:27:51+00:00 · Latest: 2026-02-02T09:27:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01853v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01853v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Transformer强化学习的A/B测试时间序列实验设计</div>
<div class="mono" style="margin-top:8px">A/B测试已成为现代科技公司进行策略评估的黄金标准，但其在时间序列实验中的应用仍面临挑战，尤其是在策略随时间顺序分配的场景中。现有设计存在两大局限：（一）未能充分利用完整历史记录进行干预分配；（二）依赖强假设来近似目标函数（如估计处理效应的均方误差）以优化设计。我们首先提出一个不可能性定理，证明由于时间序列实验中的动态依赖性，若不以完整历史为条件将导致次优设计。为同时解决这两个问题，我们提出一种Transformer强化学习方法：利用Transformer模型基于完整历史进行分配决策，并采用强化学习直接优化均方误差，无需依赖限制性假设。在合成数据、公开调度模拟器及真实网约车数据集上的实证评估表明，该方法始终优于现有设计方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying A/B testing to time series experiments, where sequential policy assignments over time are complicated by dynamic dependencies and suboptimal designs that fail to leverage full historical data or rely on strong assumptions. To overcome these limitations, the authors propose a transformer reinforcement learning method that uses transformers to condition treatment allocation on the entire history and employs reinforcement learning to directly optimize the mean squared error of the estimated treatment effect without restrictive approximations. Experimental results on synthetic data, a dispatch simulator, and a real-world ridesharing dataset show that this approach consistently outperforms existing designs.</div>
<div class="mono" style="margin-top:8px">本文针对时间序列实验中的A/B测试应用挑战，指出现有方法因动态依赖性而无法充分利用完整历史数据进行处理分配，且依赖强假设来近似目标函数，导致设计次优。为解决这些问题，作者提出了一种基于Transformer的强化学习方法，利用Transformer模型根据完整历史条件分配处理，并通过强化学习直接优化估计处理效应的均方误差，无需依赖限制性假设。在合成数据、公开调度模拟器和真实世界网约车数据集上的实验评估表明，该方法始终优于现有设计。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Durable Algorithmic Recourse</div>
<div class="meta-line">Authors: Marina Ceccon, Alessandro Fabris, Goran Radanović, Asia J. Biega, Gian Antonio Susto</div>
<div class="meta-line">First: 2025-09-26T09:24:12+00:00 · Latest: 2026-02-02T09:01:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22102v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22102v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Algorithmic recourse seeks to provide individuals with actionable recommendations that increase their chances of receiving favorable outcomes from automated decision systems (e.g., loan approvals). While prior research has emphasized robustness to model updates, considerably less attention has been given to the temporal dynamics of recourse--particularly in competitive, resource-constrained settings where recommendations shape future applicant pools. In this work, we present a novel time-aware framework for algorithmic recourse, explicitly modeling how candidate populations adapt in response to recommendations. Additionally, we introduce a novel reinforcement learning (RL)-based recourse algorithm that captures the evolving dynamics of the environment to generate recommendations that are both feasible and valid. We design our recommendations to be durable, supporting validity over a predefined time horizon T. This durability allows individuals to confidently reapply after taking time to implement the suggested changes. Through extensive experiments in complex simulation environments, we show that our approach substantially outperforms existing baselines, offering a superior balance between feasibility and long-term validity. Together, these results underscore the importance of incorporating temporal and behavioral dynamics into the design of practical recourse systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向持久算法追索的强化学习方法</div>
<div class="mono" style="margin-top:8px">算法追索旨在为个体提供可操作的推荐，以提升其在自动化决策系统（如贷款审批）中获得有利结果的机会。现有研究多关注模型更新的鲁棒性，而对追索的时间动态性——尤其在竞争性、资源受限的场景中（推荐行为会塑造未来申请者群体）——关注较少。本研究提出一种新颖的时序感知算法追索框架，显式建模候选群体如何根据推荐产生适应性变化。同时，我们引入一种基于强化学习的追索算法，通过捕捉环境演化动态来生成兼具可行性与有效性的推荐。我们设计的推荐具有持久性，可在预设时间范围T内保持有效性，使个体在落实建议变更后能更有信心地重新申请。通过在复杂仿真环境中的大量实验，本方法显著优于现有基线方案，在可行性与长期有效性间实现了更优平衡。这些成果共同表明，将时序与行为动态纳入实用追索系统设计具有重要意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for algorithmic recourse recommendations that remain valid over time, particularly in competitive settings where individuals&#x27; actions in response to advice can alter future applicant pools and decision models. The authors propose a novel time-aware framework that models population adaptation dynamics and introduce a reinforcement learning-based recourse algorithm designed to generate feasible recommendations that maintain validity over a predefined horizon, allowing individuals time to implement changes. Experimental results in complex simulations demonstrate that this approach significantly outperforms existing baselines by better balancing feasibility with long-term validity, highlighting the importance of incorporating temporal and behavioral dynamics into practical recourse systems.</div>
<div class="mono" style="margin-top:8px">本文针对算法追索中推荐建议需长期有效的问题展开研究，特别关注竞争性环境中个体依建议行动后可能改变未来申请者群体和决策模型的动态场景。作者提出了一种新颖的时序感知框架，用于模拟人群适应动态，并引入一种基于强化学习的追索算法，旨在生成在预定义时间范围内保持有效的可行建议，使个体有充足时间实施改变。在复杂模拟环境中的大量实验表明，该方法在可行性与长期有效性之间取得了更优平衡，显著优于现有基线，强调了将时序和行为动态纳入实用追索系统设计的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It</div>
<div class="meta-line">Authors: Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li, Qian Liu, Haoyuan Li</div>
<div class="meta-line">First: 2026-02-02T09:00:53+00:00 · Latest: 2026-02-02T09:00:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01826v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01826v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &quot;training inference mismatch stemming&quot; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#x27;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越精度：训练-推理失配是优化问题，简单学习率调度可解决</div>
<div class="mono" style="margin-top:8px">训练大语言模型的强化学习（RL）具有显著的不稳定性。近期研究将其归因于混合引擎不一致导致的“训练-推理失配”，而重要性采样等常规方法在长期训练中可能失效。本文从优化视角分析该问题，证明梯度噪声与训练-推理失配会随训练进程同步加剧。同时发现通过缩小更新步长可有效抑制失配。综合表明该失配不仅是静态数值偏差，更是与模型优化耦合的动态失效。基于此，我们提出一种简单有效的解决方案：专用学习率调度器。与传统预定义衰减方案不同，本方法根据响应长度动态触发学习率衰减——该指标被证实为预测不稳定的可靠预警信号。实验表明，通过随梯度噪声上升降低学习率，能持续稳定RL训练并将训练-推理失配控制在安全水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the instability in Reinforcement Learning (RL) for training Large Language Models, which is often attributed to training-inference mismatch. The authors reframe this as an optimization problem, showing that gradient noise and mismatch increase together during training, and that reducing update sizes can suppress the mismatch. Their proposed solution is a specialized Learning Rate scheduler that dynamically decays the learning rate based on response length, an early indicator of instability, thereby stabilizing training and controlling the mismatch effectively.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习训练中的不稳定性问题展开研究，该问题常归因于训练与推理的不匹配。作者从优化角度重新分析，发现梯度噪声和不匹配在训练过程中同步增长，而减小更新规模可有效抑制不匹配。他们提出一种专门的学习率调度器，根据响应长度（作为不稳定的早期预警信号）动态衰减学习率，从而稳定训练过程并将不匹配控制在安全水平。</div>
</details>
</div>
<div class="card">
<div class="title">Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning</div>
<div class="meta-line">Authors: Zheng Zhang, Ao Lu, Yuanhao Zeng, Ziwei Shan, Jinjin Guo, Lufei Li, Yexin Li, Kan Ren</div>
<div class="meta-line">First: 2026-02-02T08:13:13+00:00 · Latest: 2026-02-02T08:13:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01791v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge&#x27;s model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Grad2Reward：从稀疏评判到密集奖励以改进开放式大语言模型推理</div>
<div class="mono" style="margin-top:8px">基于可验证奖励的强化学习（RLVR）在数学、编程等可验证领域的复杂大语言模型推理中取得了重大突破。近期研究尝试通过采用大语言模型作为评判者来提供序列级奖励以优化策略，从而将该范式扩展至开放式任务。然而，此类奖励本质上是稀疏的，无法为生成复杂的长轨迹提供必要的细粒度监督。此外，现有工作将评判者视为黑盒预言机，忽略了其中蕴含的丰富中间反馈信号。为应对这些局限，我们提出了Grad2Reward——一种通过单次反向传播直接从评判者的模型推理过程中提取密集过程奖励的新框架。通过利用基于梯度的归因方法，Grad2Reward实现了精确的词元级信用分配，显著提升了训练效率和推理质量。同时，该框架引入了自评判机制，使策略能够通过自身的评估信号进行改进，而无需训练专门的奖励模型或依赖更优的外部评判者。实验表明，经Grad2Reward优化的策略在多种开放式任务中均表现出卓越性能，验证了其有效性与广泛泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of sparse, sequence-level rewards from LLM-as-a-Judge methods in open-ended reasoning tasks, which lack fine-grained supervision and discard intermediate feedback, this paper introduces Grad2Reward, a framework that extracts dense process rewards via a single backward pass through the Judge model using gradient-based attribution for token-level credit assignment. The method also incorporates a self-judging mechanism, enabling policy improvement without specialized reward models or superior external Judges. Experimental results show that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, demonstrating enhanced training efficiency, reasoning quality, and broad generalizability.</div>
<div class="mono" style="margin-top:8px">针对开放域推理任务中基于LLM作为评判者提供的稀疏序列级奖励缺乏细粒度监督且忽略中间反馈信号的局限性，本文提出了Grad2Reward框架，该方法通过单次反向传播利用基于梯度的归因从评判者模型的推理过程中提取密集的过程奖励，实现令牌级信用分配。该方法还引入了自评判机制，使策略无需训练专用奖励模型或依赖外部优越评判者即可通过自身评估信号进行改进。实验结果表明，经Grad2Reward优化的策略在多种开放域任务中表现优异，验证了其在提升训练效率、推理质量和广泛泛化能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting</div>
<div class="meta-line">Authors: Mingyue Cheng, Xiaoyu Tao, Qi Liu, Ze Guo, Enhong Chen</div>
<div class="meta-line">First: 2026-02-02T08:01:11+00:00 · Latest: 2026-02-02T08:01:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01776v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>观点：超越模型中心预测——智能体时间序列预测</div>
<div class="mono" style="margin-top:8px">时间序列预测传统上被构建为一个模型中心、静态、单次执行的预测问题，即将历史观测值映射为未来值。尽管这一范式推动了显著进展，但在需要信息特征提取、推理驱动推断、迭代优化及持续时域适应的自适应多轮预测场景中，其局限性日益凸显。本文提出智能体时间序列预测（ATSF）框架，将预测重构为由感知、规划、行动、反思与记忆构成的智能体过程。ATSF不仅关注预测模型，更强调将预测组织为可与工具交互、整合结果反馈、通过经验积累进化的智能体工作流。我们概述了三种典型实现范式——基于工作流的设计、智能体强化学习及混合智能体工作流范式，并探讨了从模型中心预测转向智能体预测所面临的机遇与挑战。本观点旨在将智能体预测确立为时间序列预测交叉领域未来研究的基石。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional model-centric, single-pass approaches in adaptive and multi-turn forecasting scenarios, this position paper advocates for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process involving perception, planning, action, reflection, and memory. The proposed method emphasizes organizing forecasting as an interactive workflow that can utilize tools, incorporate feedback, and evolve through experience, outlining three implementation paradigms: workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow. The main experimental results are not detailed as this is a conceptual position paper, but it discusses the opportunities and challenges of shifting to agentic forecasting to establish a foundation for future research.</div>
<div class="mono" style="margin-top:8px">本文的动机在于传统以模型为中心、单次预测的方法在自适应和多轮预测场景中的不足，因此主张智能体时间序列预测（ATSF），将预测重新定义为包含感知、规划、行动、反思和记忆的智能体过程。该方法强调将预测组织为可交互的工作流程，能够利用工具、整合反馈并通过经验积累而演进，概述了三种实现范式：基于工作流的设计、智能体强化学习和混合智能体工作流。由于这是一篇概念性立场论文，未详述具体实验结果，但讨论了转向智能体预测所带来的机遇与挑战，旨在为未来时间序列预测研究奠定基础。</div>
</details>
</div>
<div class="card">
<div class="title">Cost-Aware Bayesian Optimization for Prototyping Interactive Devices</div>
<div class="meta-line">Authors: Thomas Langerak, Renate Zhang, Ziyuan Wang, Per Ola Kristensson, Antti Oulasvirta</div>
<div class="meta-line">First: 2026-02-02T07:59:50+00:00 · Latest: 2026-02-02T07:59:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01774v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01774v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deciding which idea is worth prototyping is a central concern in iterative design. A prototype should be produced when the expected improvement is high and the cost is low. However, this is hard to decide, because costs can vary drastically: a simple parameter tweak may take seconds, while fabricating hardware consumes material and energy. Such asymmetries, can discourage a designer from exploring the design space. In this paper, we present an extension of cost-aware Bayesian optimization to account for diverse prototyping costs. The method builds on the power of Bayesian optimization and requires only a minimal modification to the acquisition function. The key idea is to use designer-estimated costs to guide sampling toward more cost-effective prototypes. In technical evaluations, the method achieved comparable utility to a cost-agnostic baseline while requiring only ${\approx}70\%$ of the cost; under strict budgets, it outperformed the baseline threefold. A within-subjects study with 12 participants in a realistic joystick design task demonstrated similar benefits. These results show that accounting for prototyping costs can make Bayesian optimization more compatible with real-world design projects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向交互设备原型开发的成本感知贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">在迭代设计中，判断哪些创意值得制作原型是核心问题。当预期改进较高且成本较低时，才应制作原型。然而，由于成本差异巨大——简单参数调整仅需数秒，而硬件制造则消耗材料与能源——这种决策往往困难。此类不对称性可能阻碍设计者对设计空间的探索。本文提出一种扩展的成本感知贝叶斯优化方法，以应对多样化的原型制作成本。该方法基于贝叶斯优化的强大能力，仅需对采集函数进行最小修改。其核心思想是利用设计者预估的成本，引导采样朝向更具成本效益的原型。技术评估表明，该方法在达到与忽略成本的基线方法相近效用的同时，仅需约70%的成本；在严格预算下，其性能超出基线方法三倍。一项针对12名参与者的真实摇杆设计任务内被试研究也证实了类似优势。这些结果表明，考量原型制作成本能使贝叶斯优化更贴合实际设计项目需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in iterative design of deciding which ideas to prototype when costs vary widely, from quick parameter tweaks to expensive hardware fabrication, which can discourage exploration. The method extends cost-aware Bayesian optimization by incorporating designer-estimated costs into the acquisition function, guiding sampling toward more cost-effective prototypes. Experimental results show the method achieved comparable utility to a cost-agnostic baseline while using only about 70% of the cost, and outperformed it threefold under strict budgets, with a user study confirming similar benefits in a realistic joystick design task.</div>
<div class="mono" style="margin-top:8px">本文针对迭代设计中因原型制作成本差异巨大（从快速参数调整到昂贵的硬件制造）而难以决定原型方案、抑制设计探索的问题，提出了一种扩展的成本感知贝叶斯优化方法。该方法通过将设计者预估的成本纳入采集函数，引导采样朝向更具成本效益的原型。实验结果表明，该方法在达到与忽略成本的基线相当效用的同时，仅需约70%的成本；在严格预算下，其性能超出基线三倍，一项涉及12名参与者的真实摇杆设计任务用户研究也证实了类似优势。</div>
</details>
</div>
<div class="card">
<div class="title">Frictional Q-Learning</div>
<div class="meta-line">Authors: Hyunwoo Kim, Hyo Kyung Lee</div>
<div class="meta-line">First: 2025-09-24T05:42:38+00:00 · Latest: 2026-02-02T07:54:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19771v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.19771v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-policy reinforcement learning suffers from extrapolation errors when a learned policy selects actions that are weakly supported in the replay buffer. In this study, we address this issue by drawing an analogy to static friction in classical mechanics. From this perspective, the replay buffer is represented as a smooth, low-dimensional action manifold, where the support directions correspond to the tangential component, while the normal component captures the dominant first-order extrapolation error. This decomposition reveals an intrinsic anisotropy in value sensitivity that naturally induces a stability condition analogous to a friction threshold. To mitigate deviations toward unsupported actions, we propose Frictional Q-Learning, an off-policy algorithm that encodes supported actions as tangent directions using a contrastive variational autoencoder. We further show that an orthonormal basis of the orthogonal complement corresponds to normal components under mild local isometry assumptions. Empirical results on standard continuous-control benchmarks demonstrate robust, stable performance compared with existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>摩擦Q学习</div>
<div class="mono" style="margin-top:8px">离策略强化学习在习得策略选择回放缓冲区中弱支持的动作时，会面临外推误差问题。本研究通过类比经典力学中的静摩擦机制解决该问题：将回放缓冲区视为光滑的低维动作流形，其中支持方向对应切向分量，而法向分量捕捉主导的一阶外推误差。这种分解揭示了价值敏感性的内在各向异性，自然衍生出类摩擦阈值的稳定性条件。为抑制向无支持动作的偏离，我们提出摩擦Q学习算法——通过对比变分自编码器将支持动作编码为切向方向。进一步证明在温和的局部等距假设下，正交补空间的标准正交基对应法向分量。标准连续控制基准测试的实证结果表明，相较于现有基线方法，本算法具有更鲁棒稳定的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the problem of extrapolation errors in off-policy reinforcement learning when policies select actions poorly supported by the replay buffer. The method introduces Frictional Q-Learning, which draws an analogy to static friction in mechanics: it models the replay buffer as a low-dimensional action manifold, decomposing actions into supported tangential directions and error-prone normal components using a contrastive variational autoencoder to encode supported actions. Experimental results on continuous-control benchmarks show that this approach achieves more robust and stable performance compared to existing baseline methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决离线策略强化学习中，当策略选择回放缓冲区支持不足的动作时产生的推断误差问题。方法上提出了摩擦Q学习，借鉴经典力学中的静摩擦概念：将回放缓冲区建模为低维动作流形，通过对比变分自编码器将支持的动作编码为切向分量，而正交补空间则对应误差较大的法向分量。在标准连续控制基准测试中的实验结果表明，该方法相比现有基线实现了更鲁棒和稳定的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking</div>
<div class="meta-line">Authors: Mohammad Beigi, Ming Jin, Junshan Zhang, Qifan Wang, Lifu Huang</div>
<div class="meta-line">First: 2026-02-02T07:34:57+00:00 · Latest: 2026-02-02T07:34:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01750v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01750v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对抗性奖励审计：主动检测与缓解奖励黑客攻击</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）仍易受奖励黑客攻击，即模型利用习得奖励模型中的伪相关性获取高分，却违背人类意图。现有缓解措施依赖静态防御，无法适应新型攻击策略。本文提出对抗性奖励审计（ARA）框架，将奖励黑客攻击重构为动态竞争博弈。ARA分两阶段运行：首先，黑客策略探索奖励模型漏洞，审计器则从潜在表示中学习检测攻击；其次，审计器引导的RLHF（AG-RLHF）通过门控奖励信号惩罚已检测的黑客行为，将不可观测的故障转化为可测量、可控的信号。在三种黑客场景中的实验表明，ARA在所有基线中实现了最佳对齐-效用权衡：将奉承行为降至近SFT水平同时提升帮助性，在降低冗余度的同时获得最高ROUGE-L分数，在抑制代码博弈的同时提高Pass@1。除单领域评估外，研究还证明奖励黑客攻击、检测与缓解均具有跨领域泛化能力——针对代码博弈训练的黑客策略会表现出更强的奉承倾向（尽管该行为未获奖励），而在单一领域训练的审计器能有效抑制其他领域的攻击，实现单模型高效多领域防御。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward hacking in Reinforcement Learning from Human Feedback (RLHF), where models exploit spurious correlations in reward models to achieve high scores without aligning with human intent. The authors propose Adversarial Reward Auditing (ARA), a framework that treats reward hacking as a dynamic game involving a Hacker policy that discovers vulnerabilities and an Auditor that learns to detect exploitation from latent representations; this is followed by Auditor-Guided RLHF (AG-RLHF), which gates reward signals to penalize detected hacking. Experimental results across three hacking scenarios show that ARA achieves the best alignment-utility tradeoff among baselines, reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1, with generalization demonstrated across domains for both hacking and mitigation.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习（RLHF）中的奖励黑客问题，即模型利用奖励模型中的虚假相关性获得高分却违背人类意图。作者提出了对抗性奖励审计（ARA）框架，将奖励黑客视为一个动态博弈，包含一个发现漏洞的黑客策略和一个从潜在表示中检测利用的审计器；随后采用审计器引导的RLHF（AG-RLHF）来门控奖励信号以惩罚检测到的黑客行为。在三种黑客场景下的实验结果表明，ARA在所有基线中实现了最佳的对齐-效用权衡：将奉承行为降低到接近SFT水平同时提升帮助性，减少冗长性同时获得最高ROUGE-L分数，并抑制代码游戏同时提高Pass@1，且跨领域实验证明了黑客行为和缓解措施均具有泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner</div>
<div class="meta-line">Authors: Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Yi-An Ma, Lianhui Qin</div>
<div class="meta-line">First: 2026-02-02T06:26:31+00:00 · Latest: 2026-02-02T06:26:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01705v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越模式激发：基于潜在扩散推理器的多样性保持强化学习</div>
<div class="mono" style="margin-top:8px">近期强化学习方法通过优化离散思维链生成来提升大语言模型推理能力，但离散强化学习中因模式激发行为导致策略熵下降，常引发令牌空间探索的多样性崩溃。为缓解此问题，我们提出潜在扩散推理强化学习框架，直接在连续潜在空间中进行探索，其中潜在变量编码语义级推理轨迹。通过引导扩散建模探索，多步去噪过程分散随机性并保留多种共存解模式而不相互抑制。此外，通过解耦潜在空间探索与文本空间生成，我们证明基于潜在扩散的优化比单纯文本空间策略优化更有效，而互补的文本策略与潜在探索结合可带来额外增益。在代码生成和数学推理基准测试中，该方法在pass@1和pass@k指标上均持续优于离散强化学习基线，其中代码生成任务pass@1绝对值提升+9.4%，数学推理任务提升+5.7%，凸显了基于扩散的潜在强化学习作为离散令牌级推理强化学习的理论替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of diversity collapse in reinforcement learning (RL) for large language model reasoning, where discrete token-level optimization often leads to reduced exploration as policy entropy decreases. To mitigate this, the authors propose LaDi-RL, a framework that explores in a continuous latent space using guided diffusion to encode semantic-level reasoning trajectories, thereby preserving multiple solution modes without mutual suppression. Experimental results on code generation and mathematical reasoning benchmarks show consistent improvements, with absolute pass@1 gains of +9.4% and +5.7% respectively over discrete RL baselines, demonstrating the effectiveness of latent diffusion-based RL for enhancing reasoning diversity and performance.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在大型语言模型推理中因离散令牌级优化导致探索多样性下降的问题，提出了一种解决方案。作者设计了LaDi-RL框架，通过在连续潜在空间中使用引导扩散进行探索，将语义级推理轨迹编码为潜在变量，从而避免多解模式相互抑制并保持多样性。在代码生成和数学推理基准测试上的实验结果表明，该方法相比离散强化学习基线在pass@1和pass@k指标上均取得一致提升，其中pass@1绝对增益分别达到+9.4%和+5.7%，验证了基于潜在扩散的强化学习在提升推理多样性和性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating loss of control in advanced AI systems through instrumental goal trajectories</div>
<div class="meta-line">Authors: Willem Fourie</div>
<div class="meta-line">First: 2026-02-02T06:13:21+00:00 · Latest: 2026-02-02T06:13:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01699v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01699v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过工具性目标轨迹缓解先进人工智能系统的失控风险</div>
<div class="mono" style="margin-top:8px">人工智能实验室和高校的研究者担忧，高能力人工智能系统可能通过追求工具性目标削弱人类控制。现有缓解措施主要集中于技术层面和系统本身：追踪先进系统的能力，通过人类反馈强化学习等方法塑造行为，设计可修正与可中断的系统。本文提出工具性目标轨迹，将干预范围扩展至模型之外。能力提升通常依赖获取额外技术资源（如算力、存储、数据及关联服务），而这又需要资金支持。在组织层面，这些资源可通过三种组织路径获取。我们将这些路径标记为采购、治理和财务三类工具性目标轨迹。每条轨迹都会产生可监测的组织痕迹，当系统能力或行为超出可接受阈值时，这些痕迹可作为干预点。由此，工具性目标轨迹为界定能力层级、拓宽可修正性与可中断性的实施维度提供了具体路径，将关注点从单一模型属性转向支撑其运行的组织系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by concerns that advanced AI systems might pursue instrumental goals that undermine human control, this paper proposes a novel method called instrumental goal trajectories (IGTs) to expand mitigation strategies beyond purely technical approaches. The method identifies three organizational pathways—procurement, governance, and finance—through which AI systems typically gain resources like compute and data, creating trails of artefacts that can be monitored and used as intervention points. Experimental results suggest that these IGTs provide concrete avenues for defining capability levels and implementing corrigibility and interruptibility, shifting focus from model properties to the broader organizational systems that enable AI development.</div>
<div class="mono" style="margin-top:8px">本文针对先进人工智能系统可能追求工具性目标而削弱人类控制的担忧，提出了一种称为工具性目标轨迹的新方法，以将缓解策略扩展到纯技术方法之外。该方法识别了三种组织路径——采购、治理和财务，人工智能系统通常通过这些路径获取计算和数据等资源，从而产生可监控的组织痕迹作为干预点。研究结果表明，这些工具性目标轨迹为定义能力水平和实施可纠正性与可中断性提供了具体途径，将关注点从模型属性本身转向了支持人工智能开发的更广泛组织系统。</div>
</details>
</div>
<div class="card">
<div class="title">Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models</div>
<div class="meta-line">Authors: Wenhui Tan, Fiorenzo Parascandolo, Enver Sangineto, Jianzhong Ju, Zhenbo Luo, Qian Cao, Rita Cucchiara, Ruihua Song, Jian Luan</div>
<div class="meta-line">First: 2026-02-02T06:12:33+00:00 · Latest: 2026-02-02T06:12:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01698v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01698v1">PDF</a> · <a href="https://GitHub.com/Xiaomi-Research/LED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>训练后恢复探索能力：面向大型推理模型的潜在探索解码方法</div>
<div class="mono" style="margin-top:8px">大型推理模型近期通过强化学习训练后优化，在数学与代码推理任务中取得显著性能。然而，我们发现现代推理训练后优化会引发非预期的探索坍缩：基于温度的采样不再提升pass@$n$准确率。实证表明，训练后模型的最终层后验分布熵值急剧降低，而中间层熵值保持相对较高。基于这种熵不对称性，我们提出潜在探索解码方法——一种深度条件解码策略。该方法通过累积求和聚合中间层后验分布，并选择熵值最大的深度配置作为探索候选。无需额外训练或参数，该方法在多个推理基准测试和模型上，将pass@1和pass@16准确率分别稳定提升0.61和1.03个百分点。项目页面：https://GitHub.com/Xiaomi-Research/LED。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the observation that Reinforcement Learning (RL) post-training of Large Reasoning Models (LRMs) inadvertently causes an exploration collapse, where temperature-based sampling fails to improve pass@n accuracy, as the final-layer posterior entropy sharply decreases while intermediate layers retain higher entropy. To address this, the method introduces Latent Exploration Decoding (LED), a depth-conditioned decoding strategy that aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy to generate exploration candidates. The main experimental results demonstrate that LED, without requiring additional training or parameters, consistently enhances reasoning performance, improving pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points, respectively, across multiple benchmarks and models.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于发现大型推理模型（LRMs）在强化学习（RL）后训练中意外导致探索崩溃，即基于温度的采样无法提升pass@n准确率，这是因为最终层后验熵急剧降低，而中间层熵保持相对较高。为解决此问题，方法提出了潜在探索解码（LED），这是一种深度条件解码策略，通过累积和聚合中间层后验，并选择具有最大熵的深度配置作为探索候选。主要实验结果表明，LED无需额外训练或参数，即可持续提升推理性能，在多个基准测试和模型上将pass@1和pass@16准确率分别提高了0.61和1.03个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment</div>
<div class="meta-line">Authors: Byeonghu Na, Hyungho Na, Yeongmin Kim, Suhyeon Jo, HeeSun Bae, Mina Kang, Il-Chul Moon</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T05:56:16+00:00 · Latest: 2026-02-02T05:56:16+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01685v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01685v1">PDF</a> · <a href="https://github.com/aailab-kaist/WPR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型对齐的语义感知Wasserstein策略正则化</div>
<div class="mono" style="margin-top:8px">大语言模型通常通过基于人类反馈的强化学习进行对齐。该方法通常通过奖励最大化结合参考策略的KL散度正则化来优化策略。然而，KL散度及其f-散度变体仅比较相同位置的标记概率，无法捕捉语义相似性。我们提出Wasserstein策略正则化，这是一种基于熵正则化Wasserstein距离的语义感知正则化方法，它融入了标记空间的几何结构。该距离的对偶形式通过最优对偶变量将正则化表示为施加于奖励的惩罚项，从而得到与标准强化学习算法兼容的可处理目标。实验表明，我们的方法优于基于KL散度和f-散度的基线，证明了语义感知策略距离在对齐任务中的优势。代码已开源：https://github.com/aailab-kaist/WPR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of KL and f-divergence regularizations in reinforcement learning from human feedback (RLHF), which only compare token probabilities at identical positions and ignore semantic similarity, this paper proposes Wasserstein Policy Regularization (WPR), a semantic-aware method that incorporates the geometry of the token space via entropy-regularized Wasserstein distance. The method formulates the regularization as tractable penalty terms on the reward through a dual formulation, making it compatible with standard RL algorithms. Experimental results show that WPR outperforms KL- and f-divergence-based baselines, demonstrating the benefits of using semantic-aware distances for aligning large language models with human preferences.</div>
<div class="mono" style="margin-top:8px">针对基于人类反馈的强化学习（RLHF）中KL散度和f散度正则化仅比较相同位置标记概率而忽略语义相似性的局限，本文提出了Wasserstein策略正则化（WPR），这是一种基于熵正则化Wasserstein距离的语义感知方法，通过融入标记空间的几何结构来改进对齐。该方法通过对偶公式将正则化表示为奖励上的可处理惩罚项，从而与标准强化学习算法兼容。实验结果表明，WPR在性能上优于基于KL散度和f散度的基线方法，验证了语义感知距离在大语言模型与人类偏好对齐中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</div>
<div class="meta-line">Authors: Chen Li, Han Zhang, Zhantao Yang, Fangyi Chen, Zihan Wang, Anudeepsekhar Bolimera, Marios Savvides</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-12T07:27:50+00:00 · Latest: 2026-02-02T05:52:55+00:00</div>
<div class="meta-line">Comments: This paper has been accepted at AAAI 2026. This is the author&#x27;s extended version. The final version will appear in the official proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08688v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.08688v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STELAR-VISION：面向视觉对齐推理的自拓扑感知高效学习框架</div>
<div class="mono" style="margin-top:8px">视觉语言模型在推理方面取得显著进展，但在处理复杂多模态任务时仍面临困难，且常生成冗长输出。其关键局限在于依赖链式思维推理，而许多任务其实更适合树状或图状等拓扑结构。为此，我们提出STELAR-Vision——一种拓扑感知推理训练框架。其核心是TopoAug合成数据管道，通过多样化拓扑结构增强训练数据。采用监督微调与强化学习相结合的方法，我们在保持准确率的同时对Qwen2VL模型进行高效后训练。此外，我们提出节俭学习策略，能以最小精度损失缩减输出长度。在MATH-V和VLM-S2H数据集上，STELAR-Vision相比基础模型准确率提升9.7%，并超越更大的Qwen2VL-72B-Instruct模型7.3%。在五个分布外基准测试中，其性能最高超越Phi-4-Multimodal-Instruct达28.4%，超越LLaMA-3.2-11B-Vision-Instruct达13.2%，展现出强大泛化能力。与纯链式训练相比，本方法在分布内数据集上整体准确率提升4.3%，并在所有分布外基准测试中保持稳定优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of vision-language models (VLMs) in handling complex multimodal tasks and generating verbose outputs due to their reliance on chain-of-thought reasoning, this paper introduces STELAR-Vision, a training framework for topology-aware reasoning that incorporates diverse structures like trees or graphs. The method employs TopoAug, a synthetic data pipeline to enrich training with varied topologies, and uses supervised fine-tuning and reinforcement learning to post-train Qwen2VL models, with Frugal Learning added to reduce output length efficiently. Experimental results show that STELAR-Vision improves accuracy by 9.7% over its base model and surpasses larger models like Qwen2VL-72B-Instruct by 7.3% on benchmarks such as MATH-V and VLM-S2H, while also demonstrating strong generalization by outperforming other models on out-of-distribution tasks by up to 28.4%.</div>
<div class="mono" style="margin-top:8px">针对视觉语言模型在处理复杂多模态任务时因依赖链式思维推理而表现不佳和输出冗长的问题，本文提出了STELAR-Vision，一种支持拓扑感知推理的训练框架，旨在引入树或图等多样化结构。方法核心包括TopoAug合成数据管道以丰富训练拓扑多样性，结合监督微调和强化学习对Qwen2VL模型进行后训练，并采用Frugal Learning来高效缩减输出长度。实验结果表明，在MATH-V和VLM-S2H等基准测试中，STELAR-Vision相比基础模型准确率提升9.7%，并超过更大的Qwen2VL-72B-Instruct模型7.3%，同时在分布外任务上泛化能力强，最高优于其他模型28.4%。</div>
</details>
</div>
<div class="card">
<div class="title">TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios</div>
<div class="meta-line">Authors: Yuanzhe Shen, Zisu Huang, Zhengyuan Wang, Muzhao Tian, Zhengkang Guo, Chenyang Zhang, Shuaiyu Zhou, Zengjie Hu, Dailin Li, Jingwen Xu, Kaimin Wang, Wenhao Liu, Tianlong Li, Fengpeng Yue, Feng Hong, Cao Liu, Ke Zeng</div>
<div class="meta-line">First: 2026-02-02T05:43:08+00:00 · Latest: 2026-02-02T05:43:08+00:00</div>
<div class="meta-line">Comments: 40 pages, 6figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01675v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRIP-Bench：面向现实场景的长程交互智能体基准</div>
<div class="mono" style="margin-top:8px">随着基于大语言模型的智能体被部署到日益复杂的现实场景中，现有基准未能充分体现关键挑战，例如强制执行全局约束、协调多工具推理，以及在长程多轮交互中适应动态变化的用户行为。为弥补这一差距，我们提出了\textbf{TRIP-Bench}，这是一个基于真实旅行规划场景的长程基准。TRIP-Bench利用真实世界数据，提供18个精选工具和40余项旅行需求，并支持自动化评估。它包含不同难度的划分；困难划分强调长程且模糊的交互、风格转换、可行性变化以及迭代版本修订。对话可长达15轮用户交互，可能涉及150余次工具调用，上下文长度可超过20万词元。实验表明，即使是先进模型在简单划分上的成功率最高仅为50%，在困难子集上性能则降至10%以下。我们进一步提出了\textbf{GTPO}，一种具有专用奖励归一化和奖励差分机制的在线多轮强化学习方法。应用于Qwen2.5-32B-Instruct模型时，GTPO提升了约束满足度和交互鲁棒性，在我们的评估中表现优于Gemini-3-Pro。我们期望TRIP-Bench能推动实用长程交互智能体的发展，而GTPO能为鲁棒的长程训练提供有效的在线强化学习方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that existing benchmarks for LLM-based agents fail to adequately capture the complexities of real-world deployment, such as managing global constraints, coordinating multiple tools, and adapting to user behavior over extended interactions. To address this, the authors introduce TRIP-Bench, a long-horizon benchmark based on realistic travel-planning scenarios that uses real-world data, provides 18 tools and over 40 requirements, and supports automated evaluation across difficulty splits. The main experimental results reveal that even advanced models achieve at most 50% success on the easy split, with performance dropping below 10% on hard subsets, highlighting the benchmark&#x27;s challenge. Furthermore, the proposed GTPO method, an online multi-turn reinforcement learning approach with specialized reward techniques, when applied to Qwen2.5-32B-Instruct, improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in evaluations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，现有基于大语言模型的智能体基准测试未能充分体现现实世界部署的复杂性，例如管理全局约束、协调多工具使用以及适应长期交互中的用户行为变化。为此，作者提出了TRIP-Bench，这是一个基于真实旅行规划场景的长时程基准测试，它利用真实世界数据，提供18个工具和40多项需求，并支持跨难度分级的自动化评估。主要实验结果表明，即使在简单难度下，先进模型的最大成功率也仅为50%，而在困难子集上性能降至10%以下，凸显了该基准的挑战性。此外，所提出的GTPO方法——一种采用专门奖励技术的在线多轮强化学习方案，在应用于Qwen2.5-32B-Instruct模型时，提升了约束满足和交互鲁棒性，在评估中表现优于Gemini-3-Pro。</div>
</details>
</div>
<div class="card">
<div class="title">TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Hayeong Lee, JunHyeok Oh, Byung-Jun Lee</div>
<div class="meta-line">First: 2026-02-02T05:34:38+00:00 · Latest: 2026-02-02T05:34:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01665v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TABX：面向多智能体强化学习的高通量沙盒对抗模拟器</div>
<div class="mono" style="margin-top:8px">环境设计对协作型多智能体强化学习（MARL）算法的开发与评估具有关键影响。现有基准测试虽能凸显核心挑战，但往往缺乏设计定制化评估场景所需的模块化能力。本文介绍了基于JAX的全加速对抗模拟器（TABX），这是一个专为可重构多智能体任务设计的高通量沙盒环境。TABX提供对环境参数的细粒度控制，支持系统化研究智能体在多样化任务复杂度下的涌现行为与算法权衡。通过利用JAX在GPU上的硬件加速执行能力，TABX实现了大规模并行计算并显著降低运算开销。该框架具备高速、可扩展和易定制特性，为复杂结构化领域中的MARL智能体研究提供支持，并为未来研究构建可扩展的基础平台。代码已开源：https://anonymous.4open.science/r/TABX-00CA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more modular and reconfigurable environments to evaluate cooperative multi-agent reinforcement learning (MARL) algorithms, this paper introduces TABX, a high-throughput sandbox simulator built in JAX. The method provides fine-grained control over environmental parameters to systematically study emergent agent behaviors and algorithmic trade-offs across varying task complexities, leveraging JAX for hardware-accelerated execution on GPUs to achieve massive parallelization and reduced computational overhead. Experimental results demonstrate that TABX offers a fast, extensible, and easily customized framework, facilitating research into MARL agents in complex structured domains and serving as a scalable foundation for future work.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有多智能体强化学习（MARL）基准环境缺乏模块化，难以定制评估场景，因此提出了TABX，一个基于JAX构建的高通量沙盒模拟器。该方法通过提供对环境参数的细粒度控制，允许系统研究不同任务复杂度下智能体的涌现行为和算法权衡，并利用JAX在GPU上的硬件加速执行，实现了大规模并行化和显著降低计算开销。实验结果表明，TABX提供了一个快速、可扩展且易于定制的框架，有助于在复杂结构化领域中研究MARL智能体，并为未来研究奠定了可扩展的基础。</div>
</details>
</div>
<div class="card">
<div class="title">FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning</div>
<div class="meta-line">Authors: Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang, Rui Mao, Erik Cambria</div>
<div class="meta-line">First: 2026-02-02T05:30:42+00:00 · Latest: 2026-02-02T05:30:42+00:00</div>
<div class="meta-line">Comments: 41 pages, 7 figures, 6 tables. Project page: http://flowsteer.org/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01664v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01664v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowSteer：基于端到端强化学习的交互式智能体工作流编排</div>
<div class="mono" style="margin-top:8px">近年来，多种强大的智能体工作流已被应用于解决广泛的人类问题。然而，现有工作流编排仍面临关键挑战，包括高昂的人工成本、对特定算子/大语言模型（LLM）的依赖，以及稀疏的奖励信号。为应对这些挑战，我们提出FlowSteer——一种端到端强化学习框架，以轻量级策略模型作为智能体，结合可执行画布环境，通过多轮交互实现工作流编排的自动化。在此过程中，策略模型分析执行状态并选择编辑动作，而画布则执行算子并返回反馈以进行迭代优化。此外，FlowSteer提供即插即用框架，支持多样化的算子库与可替换的LLM后端。为有效训练此交互范式，我们提出画布工作流相对策略优化（CWRPO），通过引入带条件释放的多样性约束奖励来稳定学习并抑制捷径行为。在十二个数据集上的实验结果表明，FlowSteer在多种任务中显著优于基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high manual cost, reliance on specific operators or large language models (LLMs), and sparse reward signals in existing agentic workflow orchestration. It proposes FlowSteer, an end-to-end reinforcement learning framework that uses a lightweight policy model as an agent interacting with an executable canvas environment to automate workflow orchestration through multi-turn interactions, supporting plug-and-play operator libraries and interchangeable LLM backends. The method introduces Canvas Workflow Relative Policy Optimization (CWRPO) with diversity-constrained rewards to stabilize training and suppress shortcuts. Experimental results on twelve datasets demonstrate that FlowSteer significantly outperforms baseline methods across various tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有智能体工作流编排存在手动成本高、依赖特定算子或大语言模型（LLM）以及奖励信号稀疏等挑战。为此，提出了FlowSteer，这是一个端到端的强化学习框架，采用轻量级策略模型作为智能体，与可执行画布环境进行多轮交互，以自动化工作流编排，并支持即插即用的算子库和可互换的LLM后端。方法上引入了画布工作流相对策略优化（CWRPO），通过多样性约束奖励来稳定学习并抑制捷径行为。在十二个数据集上的实验结果表明，FlowSteer在各种任务中显著优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning</div>
<div class="meta-line">Authors: Yinchao Ma, Qiang Zhou, Zhibin Wang, Xianing Chen, Hanqing Yang, Jun Song, Bo Zheng</div>
<div class="meta-line">First: 2026-02-02T05:09:48+00:00 · Latest: 2026-02-02T05:09:48+00:00</div>
<div class="meta-line">Comments: This paper is accepted by AAAI2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01649v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01649v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的贡献感知视频令牌压缩以实现高效视频理解</div>
<div class="mono" style="margin-top:8px">视频大语言模型在视频理解任务中展现出卓越能力，但视频令牌的冗余性在推理过程中引入了显著计算开销，限制了实际部署。现有压缩算法多优先保留注意力分数最高的特征以最小化注意力计算扰动，但注意力分数与对正确答案的实际贡献之间的关联仍不明确。为突破此局限，本文提出一种新颖的贡献感知视频令牌压缩算法（CaCoVID），其基于令牌对正确预测的贡献显式优化令牌选择策略。首先，我们引入基于强化学习的框架，通过优化策略网络选择对正确预测贡献最大的视频令牌组合，将焦点从被动保留令牌转向主动发现最优压缩组合。其次，我们提出结合在线组合空间采样的组合策略优化算法，大幅缩减视频令牌组合的探索空间并加速策略收敛。在多类视频理解基准上的广泛实验验证了CaCoVID的有效性。代码将公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational inefficiency in video large language models caused by redundant video tokens by proposing CaCoVID, a contribution-aware token compression algorithm. The motivation stems from the ambiguous link between attention scores and actual predictive contributions, leading to a method that uses reinforcement learning to actively select token combinations that maximize contribution to correct answers, enhanced by a combinatorial policy optimization algorithm for efficient exploration. Experimental results across various video understanding benchmarks confirm the algorithm&#x27;s effectiveness in improving efficiency without compromising performance.</div>
<div class="mono" style="margin-top:8px">本文针对视频大语言模型中因视频令牌冗余导致的计算效率低下问题，提出了CaCoVID这一贡献感知的令牌压缩算法。其动机在于注意力分数与预测贡献之间的关联不明确，因此采用强化学习框架主动选择对正确答案贡献最大的令牌组合，并通过组合策略优化算法减少探索空间以加速收敛。在多个视频理解基准测试上的广泛实验验证了该算法在提升效率的同时保持性能的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SUSD: Structured Unsupervised Skill Discovery through State Factorization</div>
<div class="meta-line">Authors: Seyed Mohammad Hadi Hosseini, Mahdieh Soleymani Baghshah</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T04:21:33+00:00 · Latest: 2026-02-02T04:21:33+00:00</div>
<div class="meta-line">Comments: Accepted as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01619v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01619v1">PDF</a> · <a href="https://github.com/hadi-hosseini/SUSD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent&#x27;s focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SUSD：通过状态因子化实现结构化无监督技能发现</div>
<div class="mono" style="margin-top:8px">无监督技能发现（USD）旨在无需外部奖励的情况下自主学习多样化技能集。最常见的USD方法之一是最大化技能潜变量与状态之间的互信息（MI）。然而，基于MI的方法因其不变性倾向偏好简单静态技能，限制了动态、任务相关行为的发现。距离最大化技能发现（DSD）通过利用状态空间距离促进更动态的技能，但在鼓励涵盖环境中所有可控因子或实体的全面技能集方面仍有不足。本研究提出SUSD框架，通过将状态空间分解为独立组件（如对象或可控实体）来利用环境的组合结构。SUSD为不同因子分配独立技能变量，实现对技能发现过程的细粒度控制。动态模型同时追踪跨因子学习进度，自适应地将智能体注意力导向未充分探索的因子。这种结构化方法不仅促进发现更丰富多样的技能，还产生因子化技能表示，支持对单个实体的细粒度解耦控制，进而通过分层强化学习（HRL）高效训练组合下游任务。在包含1至10个因子的三个环境中的实验表明，本方法能在无监督情况下发现多样复杂技能，在因子化复杂环境中显著优于现有无监督技能发现方法。代码已开源：https://github.com/hadi-hosseini/SUSD。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SUSD, a framework for unsupervised skill discovery that addresses limitations in existing methods, which often yield simple, static skills or fail to engage all controllable factors in an environment. The method leverages environmental structure by factorizing the state space into independent components, such as objects or entities, and allocates distinct skill variables to each factor, with a dynamic model adaptively directing focus to underexplored factors to ensure comprehensive skill learning. Experimental results across three environments with up to ten factors show that SUSD discovers more diverse and complex skills than prior approaches, enabling fine-grained control and facilitating efficient training for downstream hierarchical reinforcement learning tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了SUSD框架，用于解决无监督技能发现中现有方法常产生简单静态技能或未能充分利用环境所有可控因素的问题。该方法通过将状态空间分解为独立组件（如对象或实体），并为每个因素分配不同的技能变量，同时利用动态模型自适应地将注意力引导至未充分探索的因素，以确保全面的技能学习。在三个包含多达十个因素的环境中的实验结果表明，SUSD能够发现比先前方法更多样化和复杂的技能，实现了细粒度控制，并有助于下游分层强化学习任务的高效训练。</div>
</details>
</div>
<div class="card">
<div class="title">Revolutionizing Genomics with Reinforcement Learning Techniques</div>
<div class="meta-line">Authors: Mohsen Karami, Khadijeh, Jahanian, Roohallah Alizadehsani, Iman Dehzangi, Juan M Gorriz, Yudong Zhang, Jia Wang, Farshid Hajati, Min Yang, Thantrira Porntaveetus, Hamid Alinejad-Rokny</div>
<div class="meta-line">First: 2023-02-26T08:43:08+00:00 · Latest: 2026-02-02T04:15:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2302.13268v7">Abs</a> · <a href="https://arxiv.org/pdf/2302.13268v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the application of RL in genomics, highlighting the strengths and limitations of these approaches. We then discuss potential research directions that are worthy of future exploration, including the development of more sophisticated reward functions as RL heavily depends on the accuracy of the reward function, the integration of RL with other machine learning techniques, and the application of RL to new and emerging areas in genomics research. Finally, we present our findings and conclude by summarizing the current state of the field and the future outlook for RL in genomics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习技术革新基因组学</div>
<div class="mono" style="margin-top:8px">近年来，强化学习已成为解决决策制定与基因组学等广泛问题的有力工具。过去二十年原始基因组数据的指数级增长已超出人工分析能力，推动了对自动化数据分析处理的日益关注。强化学习算法能在最少人工监督下从经验中学习，使其特别适用于基因组数据分析与解读。其关键优势在于降低监督学习所需标注训练数据的采集成本。尽管已有大量研究探讨机器学习在基因组学中的应用，本综述聚焦于强化学习在基因调控网络、基因组组装、序列比对等领域的应用。我们对现有研究进行全面技术综述，阐明各类方法的优势与局限，进而探讨值得探索的未来研究方向：包括开发更精细的奖励函数（强化学习高度依赖奖励函数精度）、强化学习与其他机器学习技术的融合、以及在新兴基因组学研究领域的应用。最后通过总结领域现状与未来展望呈现研究结论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the overwhelming volume of genomic data that surpasses manual analysis capabilities, this survey paper explores the application of Reinforcement Learning (RL) as a cost-effective, automated solution for genomic interpretation. The method involves a comprehensive technical review of existing RL applications in genomics, specifically focusing on areas such as gene regulatory networks, genome assembly, and sequence alignment. The main experimental results and findings highlight RL&#x27;s strengths in learning with minimal supervision and reducing labeled data costs, while also identifying key limitations and future research directions, including the need for improved reward functions and integration with other machine learning techniques.</div>
<div class="mono" style="margin-top:8px">本文的动机在于基因组数据的爆炸式增长已超出人工分析能力，因此探讨强化学习作为一种低成本、自动化的解决方案在基因组学中的应用。方法上，该综述对强化学习在基因调控网络、基因组组装和序列比对等领域的现有研究进行了全面的技术性回顾。主要实验结果和发现强调了强化学习在最小监督下学习和降低标注数据成本方面的优势，同时指出了其局限性及未来的研究方向，包括开发更精确的奖励函数以及与其他机器学习技术的融合。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching</div>
<div class="meta-line">Authors: Zeqiao Li, Yijing Wang, Haoyu Wang, Zheng Li, Zhiqiang Zuo</div>
<div class="meta-line">First: 2026-02-02T03:54:11+00:00 · Latest: 2026-02-02T03:54:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01606v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01606v1">PDF</a> · <a href="https://github.com/lzqw/FLAME">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过一步流匹配增强最大熵强化学习</div>
<div class="mono" style="margin-top:8px">扩散策略表达性强但推理延迟高。流匹配（FM）支持一步生成，但将其整合到最大熵强化学习（MaxEnt RL）中面临挑战：最优策略是难以处理的基于能量的分布，而平衡探索与利用所需的高效对数似然估计存在严重离散化偏差。我们提出基于流的对数似然感知最大熵强化学习（FLAME），这是一个解决这些挑战的原则性框架。首先，我们推导出Q重加权FM目标，通过重要性重加权绕过配分函数估计。其次，我们设计了一个解耦的熵估计器，严格校正偏差，从而实现高效探索并使策略更接近最优MaxEnt策略。第三，我们整合MeanFlow公式以实现表达性强且高效的一步控制。在MuJoCo上的实验结果表明，FLAME优于高斯基线，并以显著更低的推理成本匹配多步扩散策略。代码发布于https://github.com/lzqw/FLAME。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the high inference latency of diffusion policies in reinforcement learning by integrating one-step Flow Matching into Maximum Entropy RL, which is challenging due to the intractable optimal policy distribution and discretization bias in log-likelihood estimation. The proposed FLAME framework introduces a Q-Reweighted FM objective to bypass partition function estimation via importance reweighting, a decoupled entropy estimator to correct bias for efficient exploration, and MeanFlow for expressive one-step control. Experiments on MuJoCo demonstrate that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies while significantly reducing inference costs.</div>
<div class="mono" style="margin-top:8px">该论文针对强化学习中扩散策略推理延迟高的问题，提出将一步流匹配整合到最大熵强化学习中，但面临最优策略分布难以处理和对数似然估计离散化偏差的挑战。提出的FLAME框架通过重要性重加权设计了Q重加权流匹配目标以绕过配分函数估计，采用解耦熵估计器校正偏差以实现高效探索，并结合MeanFlow实现高效的一步控制。在MuJoCo上的实验表明，FLAME优于高斯基线方法，与多步扩散策略性能相当，同时显著降低了推理成本。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She, Viet Anh Nguyen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T03:50:01+00:00 · Latest: 2026-02-02T03:50:01+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01601v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01601v1">PDF</a> · <a href="https://github.com/HieuNT91/VIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可验证奖励在线强化学习的自适应轨迹分配策略</div>
<div class="mono" style="margin-top:8px">采样效率是可验证奖励强化学习的关键瓶颈。现有基于组的策略优化方法（如GRPO）为所有训练提示分配固定数量的轨迹。这种均匀分配隐含地将所有提示视为同等信息量，可能导致计算预算使用效率低下并阻碍训练进展。我们提出\Ours方法，这是一种方差感知预测分配策略，将给定的轨迹预算分配给当前批次中的提示，以最小化策略更新的期望梯度方差。在每次迭代中，\Ours~使用轻量级高斯过程模型基于近期轨迹预测每个提示的成功概率。这些概率预测被转化为方差估计，随后输入凸优化问题，在严格计算预算约束下确定最优轨迹分配。实证结果表明，\Ours~在多个基准测试中持续提升采样效率，并比均匀或启发式分配策略获得更高性能。代码发布于https://github.com/HieuNT91/VIP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sampling inefficiency of uniform rollout allocation in group-based policy optimization for reinforcement learning with verifiable rewards, this paper introduces a Variance-Informed Predictive (VIP) allocation strategy. The method employs a lightweight Gaussian process model to predict per-prompt success probabilities from recent rollouts, converts these into variance estimates, and solves a convex optimization problem to optimally allocate a fixed rollout budget to minimize expected gradient variance. Experimental results across multiple benchmarks demonstrate that VIP consistently improves sampling efficiency and achieves higher performance compared to uniform or heuristic allocation strategies.</div>
<div class="mono" style="margin-top:8px">针对可验证奖励的强化学习中，基于组的策略优化方法均匀分配训练提示导致采样效率低下的问题，本文提出了一种方差信息预测（VIP）分配策略。该方法使用轻量级高斯过程模型，依据近期模拟数据预测每个提示的成功概率，将其转化为方差估计，并通过求解凸优化问题在固定计算预算下分配模拟次数，以最小化期望梯度方差。在多个基准测试中的实验结果表明，与均匀或启发式分配策略相比，VIP策略持续提高了采样效率并获得了更高的性能。</div>
</details>
</div>
<div class="card">
<div class="title">The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR</div>
<div class="meta-line">Authors: Israel Adewuyi, Solomon Okibe, Vladmir Ivanov</div>
<div class="meta-line">First: 2026-02-02T03:43:31+00:00 · Latest: 2026-02-02T03:43:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01599v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01599v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多重彩票假设：随机稀疏子网络足以实现RLVR</div>
<div class="mono" style="margin-top:8px">彩票假设表明稀疏子网络能达到完整模型的性能，暗示参数存在冗余。在可验证奖励的强化学习（RLVR）中，近期研究显示更新集中在参数的稀疏子集上，进一步印证了这种冗余。我们研究了利用该冗余的最简方法：仅在极端稀疏度下训练随机选取的参数子集。实验发现，仅训练1%的参数即可在3种模型和2个任务领域上匹配或超越全参数RLVR微调。此外，不同随机掩码的重叠度极低（Jaccard相似度≤0.005），但均能成功，表明预训练模型包含多个可行的稀疏子网络而非单一特权集合。我们称之为多重彩票假设。我们通过RLVR中隐式的每步KL约束解释此现象：该约束将更新限制在低维子空间，使得任意稀疏掩码均能生效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by evidence of parameter redundancy in neural networks and the observation that updates in Reinforcement Learning with Verifiable Rewards (RLVR) concentrate on sparse subsets, this paper investigates whether training only a randomly selected, extremely sparse subset of parameters can suffice. The method involves freezing most parameters and finetuning just 1% of them under RLVR, using different random masks for each run. Experimental results across three models and two task domains show that this approach matches or exceeds the performance of full-parameter finetuning, with different successful masks exhibiting minimal overlap (Jaccard similarity ≤0.005), indicating the existence of many viable sparse subnetworks rather than a single privileged set—a phenomenon termed the Multiple Ticket Hypothesis. The success is explained by the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to perform effectively.</div>
<div class="mono" style="margin-top:8px">受神经网络参数冗余证据以及可验证奖励强化学习（RLVR）中更新集中于稀疏子集的观察所启发，本文研究了仅训练随机选择的、极度稀疏的参数子集是否足够。该方法冻结大部分参数，在RLVR下仅对1%的参数进行微调，并为每次运行使用不同的随机掩码。在三个模型和两个任务领域的实验结果表明，该方法达到或超过了全参数微调的性能，且不同成功的掩码之间重叠极小（杰卡德相似度≤0.005），表明存在许多可行的稀疏子网络而非单一特权集合——这一现象被称为“多重彩票假设”。其成功原因被解释为RLVR中隐含的每步KL约束，该约束将更新限制在低维子空间内，使得任意稀疏掩码都能有效工作。</div>
</details>
</div>
<div class="card">
<div class="title">COBRA++: Enhanced COBRA Optimizer with Augmented Surrogate Pool and Reinforced Surrogate Selection</div>
<div class="meta-line">Authors: Zipei Yu, Zhiyang Huang, Hongshu Guo, Yue-Jiao Gong, Zeyuan Ma</div>
<div class="meta-line">First: 2026-01-30T06:27:10+00:00 · Latest: 2026-02-02T02:52:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22624v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22624v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The optimization problems in realistic world present significant challenges onto optimization algorithms, such as the expensive evaluation issue and complex constraint conditions. COBRA optimizer (including its up-to-date variants) is a representative and effective tool for addressing such optimization problems, which introduces 1) RBF surrogate to reduce online evaluation and 2) bi-stage optimization process to alternate search for feasible solution and optimal solution. Though promising, its design space, i.e., surrogate model pool and selection standard, is still manually decided by human expert, resulting in labor-intensive fine-tuning for novel tasks. In this paper, we propose a learning-based adaptive strategy (COBRA++) that enhances COBRA in two aspects: 1) An augmented surrogate pool to break the tie with RBF-like surrogate and hence enhances model diversity and approximation capability; 2) A reinforcement learning-based online model selection policy that empowers efficient and accurate optimization process. The model selection policy is trained to maximize overall performance of COBRA++ across a distribution of constrained optimization problems with diverse properties. We have conducted multi-dimensional validation experiments and demonstrate that COBRA++ achieves substantial performance improvement against vanilla COBRA and its adaptive variant. Ablation studies are provided to support correctness of each design component in COBRA++.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>COBRA++：基于增强代理池与强化代理选择的改进型COBRA优化器</div>
<div class="mono" style="margin-top:8px">现实世界中的优化问题对算法提出严峻挑战，如高昂评估成本和复杂约束条件。COBRA优化器（含最新变体）是解决此类问题的代表性工具，其通过1）引入RBF代理模型减少在线评估，2）采用两阶段优化交替搜索可行解与最优解。然而其设计空间（代理模型池与选择标准）仍依赖专家手动设定，导致新任务需大量调参。本文提出基于学习的自适应策略（COBRA++），从两方面改进COBRA：1）构建增强代理池以突破类RBF模型的局限，提升模型多样性与逼近能力；2）设计基于强化学习的在线模型选择策略，实现高效精准的优化过程。该策略通过训练最大化COBRA++在具有不同特性的约束优化问题分布上的整体性能。多维验证实验表明，COBRA++相比原始COBRA及其自适应变体取得显著性能提升，消融实验验证了各设计模块的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of the COBRA optimizer, which relies on manually tuned surrogate models and selection criteria for expensive and constrained optimization problems. It introduces COBRA++, which enhances COBRA by expanding the surrogate pool beyond RBF models to improve diversity and approximation, and employs a reinforcement learning-based policy to adaptively select surrogates online, trained to maximize performance across diverse problem distributions. Experimental results show that COBRA++ significantly outperforms the original COBRA and its adaptive variant, with ablation studies confirming the effectiveness of each component.</div>
<div class="mono" style="margin-top:8px">本文针对COBRA优化器在处理昂贵评估和复杂约束优化问题时依赖人工调整代理模型及选择标准的局限性，提出了COBRA++改进方法。该方法通过扩展代理模型池以超越RBF模型来增强多样性和近似能力，并采用基于强化学习的在线模型选择策略，该策略经过训练可在多样化问题分布上最大化整体性能。实验结果表明，COBRA++相比原始COBRA及其自适应变体实现了显著性能提升，消融研究也验证了各个设计组件的正确性。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Cognitive Supersensing in Multimodal Large Language Model</div>
<div class="meta-line">Authors: Boyi Li, Yifan Shen, Yuanzhe Liu, Yifan Xu, Jiateng Liu, Xinzhuo Li, Zhengyuan Li, Jingyuan Zhu, Yunhan Zhong, Fangzhou Lan, Jianguo Cao, James M. Rehg, Heng Ji, Ismini Lourentzou, Xu Cao</div>
<div class="meta-line">First: 2026-02-02T02:19:50+00:00 · Latest: 2026-02-02T02:19:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01541v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01541v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向多模态大语言模型的认知超感知</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在开放词汇感知任务中取得了显著成功，但其解决复杂认知问题的能力仍有限，尤其在视觉细节抽象且需要视觉记忆的场景中。现有方法主要在文本空间中扩展思维链推理，即使仅凭语言难以实现清晰、结构化的推理，且大多忽视了类似人类视觉空间画板和视觉意象的视觉推理机制。为弥补这一不足，我们提出认知超感知——一种新颖的训练范式，通过集成潜在视觉意象预测头，使MLLMs具备类人视觉意象能力。该模块联合学习视觉认知潜在嵌入序列，并将其与答案对齐，从而形成基于视觉的内部推理链。我们进一步引入强化学习阶段，基于此具象化的视觉潜在空间优化文本推理路径。为评估MLLMs的认知能力，我们构建了CogSense-Bench——一个涵盖五个认知维度的综合性视觉问答基准测试。大量实验表明，采用认知超感知训练的MLLMs在CogSense-Bench上显著优于现有先进基线模型，并在跨领域数学与科学视觉问答基准中展现出卓越的泛化能力，这提示内部视觉意象可能是弥合感知识别与认知理解之间差距的关键。我们将开源CogSense-Bench及模型权重。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited cognitive problem-solving abilities of Multimodal Large Language Models (MLLMs) in tasks requiring visual memory and abstract reasoning, this paper introduces Cognitive Supersensing, a training paradigm that equips MLLMs with human-like visual imagery. The method integrates a Latent Visual Imagery Prediction (LVIP) head to learn visual cognitive latent embeddings aligned with answers, forming vision-based internal reasoning chains, and employs reinforcement learning to optimize text reasoning paths based on these visual latents. Experimental results on the newly introduced CogSense-Bench, a VQA benchmark assessing five cognitive dimensions, show that models trained with this approach significantly outperform state-of-the-art baselines and demonstrate superior generalization on out-of-domain mathematics and science VQA tasks, indicating that internal visual imagery is key to advancing from perceptual recognition to cognitive understanding.</div>
<div class="mono" style="margin-top:8px">针对多模态大语言模型在需要视觉记忆和抽象推理的复杂认知任务中能力有限的问题，本文提出了认知超感知训练范式，旨在赋予模型类人的视觉意象能力。该方法通过集成潜在视觉意象预测头，学习与答案对齐的视觉认知潜在嵌入以形成基于视觉的内部推理链，并利用强化学习基于这些视觉潜在优化文本推理路径。在涵盖五个认知维度的新视觉问答基准CogSense-Bench上的实验结果表明，采用此方法训练的模型显著优于现有先进基线，并在领域外的数学和科学视觉问答任务上展现出优异的泛化性能，这表明内部视觉意象可能是连接感知识别与认知理解的关键。</div>
</details>
</div>
<div class="card">
<div class="title">MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</div>
<div class="meta-line">Authors: Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</div>
<div class="meta-line">First: 2026-02-02T02:12:28+00:00 · Latest: 2026-02-02T02:12:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01539v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01539v1">PDF</a> · <a href="https://github.com/BattleWen/MAGIC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker&#x27;s ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method&#x27;s substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework&#x27;s effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAGIC：面向鲁棒大语言模型安全的攻防对抗协同演化博弈框架</div>
<div class="mono" style="margin-top:8px">确保大语言模型（LLM）的鲁棒安全对齐至关重要，然而现有防御方法因依赖静态预收集数据分布，常滞后于持续演化的对抗攻击。本文提出MAGIC——一种新颖的多轮多智能体强化学习框架，将LLM安全对齐建模为非对称对抗博弈。具体而言，攻击方智能体学习迭代重写原始查询为欺骗性提示，而防御方智能体同步优化其策略以识别并拒斥此类输入。这一动态过程触发协同演化：攻击方持续变化的策略不断揭示长尾漏洞，驱动防御方泛化至未见攻击模式。值得注意的是，具备初始推理能力的攻击方通过迭代强化学习训练演化出新颖的、此前未见的组合策略，彰显了本方法的巨大潜力。理论上，我们揭示了更鲁棒的博弈均衡机制并推导出安全保证。大量实验验证了框架的有效性，在保持模型辅助能力的同时实现了卓越的防御成功率。代码已开源：https://github.com/BattleWen/MAGIC。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for robust safety alignment in Large Language Models (LLMs), addressing the limitation of static defense data that fails to keep pace with evolving adversarial attacks. It introduces MAGIC, a multi-turn multi-agent reinforcement learning framework that treats safety alignment as an adversarial asymmetric game, where an attacker agent learns to iteratively craft deceptive prompts and a defender agent simultaneously optimizes its policy to reject such inputs, fostering a co-evolutionary process. Experimental results show that this approach leads to novel, unseen combinatorial attack strategies, driving the defender to generalize better, and achieves superior defense success rates without compromising model helpfulness, with theoretical insights into robust game equilibrium and safety guarantees.</div>
<div class="mono" style="margin-top:8px">本文的动机是确保大型语言模型（LLM）具有鲁棒的安全对齐能力，以解决现有防御方法依赖静态数据分布而难以应对不断演化的对抗攻击的局限。它提出了MAGIC，一个多轮多智能体强化学习框架，将安全对齐建模为一种非对称对抗博弈，其中攻击者智能体学习迭代重写查询以生成欺骗性提示，而防御者智能体同时优化其策略以识别并拒绝此类输入，从而触发协同进化过程。实验结果表明，该方法能产生新颖的、此前未见的组合攻击策略，推动防御者实现更好的泛化能力，并在不损害模型帮助性的前提下取得了更高的防御成功率，同时提供了关于鲁棒博弈均衡和安全保证的理论见解。</div>
</details>
</div>
<div class="card">
<div class="title">Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning</div>
<div class="meta-line">Authors: Qian Wang, Xuandong Zhao, Zirui Zhang, Zhanzhi Lou, Nuo Chen, Dawn Song, Bingsheng He</div>
<div class="meta-line">First: 2026-02-02T01:43:48+00:00 · Latest: 2026-02-02T01:43:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01528v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01528v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly serve as automated judges, yet they remain susceptible to cognitive biases -- often altering their reasoning when faced with spurious prompt-level cues such as consensus claims or authority appeals. Existing mitigations via prompting or supervised fine-tuning fail to generalize, as they modify surface behavior without changing the optimization objective that makes bias cues predictive. To address this gap, we propose Epistemic Independence Training (EIT), a reinforcement learning framework grounded in a key principle: to learn independence, bias cues must be made non-predictive of reward. EIT operationalizes this through a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers, combined with a reward design that penalizes bias-following without rewarding bias agreement. Experiments on Qwen3-4B demonstrate that EIT improves both accuracy and robustness under adversarial biases, while preserving performance when bias aligns with truth. Notably, models trained only on bandwagon bias generalize to unseen bias types such as authority and distraction, indicating that EIT induces transferable epistemic independence rather than bias-specific heuristics. Code and data are available at https://anonymous.4open.science/r/bias-mitigation-with-rl-BC47.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使偏见不可预测：通过强化学习训练稳健的大语言模型评判器</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）日益成为自动化评判工具，但它们仍易受认知偏见影响——常因虚假提示线索（如共识主张或权威诉求）而改变推理逻辑。现有基于提示或监督微调的缓解方法因仅改变表面行为而未调整使偏见线索可预测的优化目标，故难以泛化。为解决此问题，我们提出认知独立性训练（EIT），这是一个基于强化学习的框架，其核心原则是：要学习独立性，必须使偏见线索无法预测奖励。EIT通过平衡冲突策略实现该原则——使偏见信号同等可能支持正确与错误答案，并结合奖励设计惩罚盲从偏见而非奖励偏见附和。在Qwen3-4B上的实验表明，EIT在对抗性偏见下提升了准确性与鲁棒性，同时在偏见与事实一致时保持性能。值得注意的是，仅针对从众偏见训练的模型可泛化至未见偏见类型（如权威和干扰偏见），表明EIT诱导的是可迁移的认知独立性，而非特定偏见的启发式策略。代码与数据详见 https://anonymous.4open.science/r/bias-mitigation-with-rl-BC47。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the susceptibility of large language models (LLMs) to cognitive biases when used as automated judges, where existing prompting or fine-tuning methods fail to generalize by not altering the underlying optimization that makes bias cues predictive. The authors propose Epistemic Independence Training (EIT), a reinforcement learning framework that makes bias cues non-predictive of reward through a balanced conflict strategy and a reward design penalizing bias-following. Experimental results on Qwen3-4B show that EIT improves accuracy and robustness under adversarial biases while maintaining performance when bias aligns with truth, and notably, models trained on bandwagon bias generalize to unseen bias types like authority and distraction, indicating transferable epistemic independence.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型作为自动评判者时易受认知偏见影响的问题，现有基于提示或监督微调的方法因未改变使偏见线索可预测的优化目标而泛化能力不足。作者提出了认知独立性训练，这是一种强化学习框架，通过平衡冲突策略和惩罚跟随偏见的奖励设计，使偏见线索对奖励不可预测。在Qwen3-4B上的实验结果表明，该方法提高了对抗性偏见下的准确性和鲁棒性，同时在偏见与真相一致时保持性能；值得注意的是，仅在从众偏见上训练的模型能泛化到未见偏见类型如权威和干扰，表明其诱导了可迁移的认知独立性而非特定偏见启发式。</div>
</details>
</div>
<div class="card">
<div class="title">A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning</div>
<div class="meta-line">Authors: Akifumi Wachi, Hirota Kinoshita, Shokichi Takakura, Rei Higuchi, Taiji Suzuki</div>
<div class="meta-line">First: 2026-02-02T01:31:52+00:00 · Latest: 2026-02-02T01:31:52+00:00</div>
<div class="meta-line">Comments: 28 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01523v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01523v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型推理中可验证奖励的强化学习相对预算理论</div>
<div class="mono" style="margin-top:8px">强化学习是提升大语言模型推理能力的主导范式，但其有效性在不同任务和计算预算下存在差异。我们提出一种相对预算理论，通过单一量——相对预算ξ:= H/𝔼[T]来解释这种差异，其中H为生成时域（令牌预算），T表示在基础策略下首次获得正确解所需的令牌数。我们证明ξ通过控制奖励方差和信息轨迹的可能性决定了样本效率。分析揭示了三种机制：在不足机制（ξ→0）中，信息轨迹罕见，样本复杂度激增；在平衡机制（ξ=Θ(1)）中，信息轨迹以不可忽略的概率出现，强化学习达到最大样本效率；在充裕机制（ξ→∞）中，学习保持稳定但每次迭代的边际收益递减。我们进一步为在线强化学习提供了有限样本保证，刻画了跨机制的学习进展。具体而言，在理想化分布假设的案例研究中，我们证明相对预算随迭代次数线性增长。实证结果在现实场景中验证了这些预测，确定了预算ξ∈[1.5, 2.0]能最大化学习效率，并与峰值推理性能相吻合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that reinforcement learning&#x27;s effectiveness for improving large language model reasoning varies with computational budget, and it proposes a relative-budget theory to explain this variation. The method introduces a single quantity, the relative budget ξ, defined as the ratio of generation horizon to the expected tokens until a correct solution under a base policy, which controls reward variance and the probability of informative trajectories. Experimental results, including theoretical analysis and empirical validation, identify three regimes—deficient, balanced, and ample—with the balanced regime (ξ around 1.5 to 2.0) showing maximal sample efficiency and peak reasoning performance, confirming that relative budget linearly influences learning progress.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到强化学习在提升大语言模型推理能力时，其效果随计算预算变化而不同，因此提出了相对预算理论来解释这种差异。方法引入了一个单一量——相对预算ξ，定义为生成范围与基础策略下首次正确解所需期望令牌数的比值，它控制奖励方差和信息轨迹的概率。实验结果包括理论分析和实证验证，识别了三种机制：不足、平衡和充足，其中平衡机制（ξ约在1.5至2.0之间）显示出最大的样本效率和最佳推理性能，证实了相对预算对学习进展的线性影响。</div>
</details>
</div>
<div class="card">
<div class="title">Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training</div>
<div class="meta-line">Authors: Ran Xu, Tianci Liu, Zihan Dong, Tony You, Ilgee Hong, Carl Yang, Linjun Zhang, Tao Zhao, Haoyu Wang</div>
<div class="meta-line">First: 2026-02-02T00:50:53+00:00 · Latest: 2026-02-02T00:50:53+00:00</div>
<div class="meta-line">Comments: The first two authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01511v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01511v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于评分标准的非可验证大语言模型后训练交替强化学习奖励建模</div>
<div class="mono" style="margin-top:8px">标准奖励模型通常预测标量分数，难以捕捉非可验证领域（如创意写作或开放式指令遵循）中响应质量的多维度特性。为突破此局限，我们提出Rubric-ARM框架，通过偏好反馈的强化学习联合优化评分标准生成器与评判器。相较于依赖静态评分标准或分离训练流程的现有方法，本方法将评分标准生成视为潜在动作进行学习以最大化评判准确度。我们引入交替优化策略以缓解同步更新的非平稳性问题，并通过理论分析证明该调度方案能有效降低训练过程中的梯度方差。大量实验表明，Rubric-ARM在多个基准测试中取得优于基线模型的性能，并在离线和在线强化学习场景中显著提升下游策略对齐效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of standard scalar reward models in capturing the multifaceted quality of LLM responses in non-verifiable domains like creative writing. It proposes Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback, treating rubric generation as a latent action to maximize judgment accuracy. An alternating optimization strategy is introduced to mitigate training non-stationarity, supported by theoretical analysis showing reduced gradient variance. Experimental results demonstrate that Rubric-ARM achieves state-of-the-art performance on multiple benchmarks and significantly improves downstream policy alignment in both offline and online RL settings.</div>
<div class="mono" style="margin-top:8px">本文针对标准标量奖励模型在创意写作等不可验证领域难以捕捉大语言模型回复多方面质量的问题，提出了Rubric-ARM框架。该方法通过基于偏好的强化学习联合优化评分标准生成器和评判器，将评分标准生成视为最大化判断准确性的潜在动作，并引入交替优化策略以缓解训练的非平稳性，理论分析表明该策略降低了梯度方差。大量实验表明，Rubric-ARM在多个基准测试中取得了最先进的性能，并在离线和在线强化学习设置中显著提升了下游策略对齐效果。</div>
</details>
</div>
<div class="card">
<div class="title">1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities</div>
<div class="meta-line">Authors: Kevin Wang, Ishaan Javali, Michał Bortkiewicz, Tomasz Trzciński, Benjamin Eysenbach</div>
<div class="meta-line">First: 2025-03-19T03:33:57+00:00 · Latest: 2026-02-02T00:42:18+00:00</div>
<div class="meta-line">Comments: Link to project website: https://wang-kevin3290.github.io/scaling-crl/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.14858v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.14858v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://wang-kevin3290.github.io/scaling-crl/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 - 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by $2\times$ - $50\times$, outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned. The project webpage and code can be found here: https://wang-kevin3290.github.io/scaling-crl/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于自监督强化学习的千层网络：扩展深度可解锁新目标达成能力</div>
<div class="mono" style="margin-top:8px">自监督学习的规模化发展已推动语言与视觉领域的突破，但强化学习领域尚未取得可比进展。本文研究自监督强化学习的基础构建模块，发现网络深度是解锁可扩展性显著提升的关键因素。近年多数强化学习研究采用浅层架构（约2-5层），我们证明将深度增至1024层可大幅提升性能。实验在无监督目标条件设定下进行，不提供演示或奖励信号，智能体需自主探索并学习如何最大化达成指定目标的可能性。在模拟运动与操控任务上的评估显示，本方法将自监督对比强化学习算法性能提升2至50倍，优于其他目标条件基线。增加模型深度不仅提高成功率，还从本质上改变习得的行为模式。项目网页与代码详见：https://wang-kevin3290.github.io/scaling-crl/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the scaling successes in self-supervised learning for language and vision, this paper investigates scaling in self-supervised reinforcement learning (RL), identifying network depth as a critical factor. The method involves training extremely deep networks (up to 1024 layers) in an unsupervised, goal-conditioned setting where agents explore without demonstrations or rewards to learn how to reach commanded goals. The main experimental results on simulated locomotion and manipulation tasks show that increasing depth boosts performance of a self-supervised contrastive RL algorithm by 2x to 50x, outperforming other baselines and qualitatively changing the learned behaviors.</div>
<div class="mono" style="margin-top:8px">本文的动机源于自监督学习在语言和视觉领域的规模化成功，旨在探索自监督强化学习（RL）中的规模化问题，并将网络深度确定为关键因素。其方法是在无监督、目标条件设定的环境中训练极深网络（高达1024层），智能体在没有示范或奖励的情况下进行探索，以学习如何到达指定的目标。在模拟运动与操作任务上的主要实验结果表明，增加深度将一种自监督对比RL算法的性能提升了2倍至50倍，超越了其他基线方法，并定性改变了学习到的行为模式。</div>
</details>
</div>
<div class="card">
<div class="title">Non-Uniform Noise-to-Signal Ratio in the REINFORCE Policy-Gradient Estimator</div>
<div class="meta-line">Authors: Haoyu Han, Heng Yang</div>
<div class="meta-line">First: 2026-02-01T22:05:48+00:00 · Latest: 2026-02-01T22:05:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01460v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01460v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy-gradient methods are widely used in reinforcement learning, yet training often becomes unstable or slows down as learning progresses. We study this phenomenon through the noise-to-signal ratio (NSR) of a policy-gradient estimator, defined as the estimator variance (noise) normalized by the squared norm of the true gradient (signal). Our main result is that, for (i) finite-horizon linear systems with Gaussian policies and linear state-feedback, and (ii) finite-horizon polynomial systems with Gaussian policies and polynomial feedback, the NSR of the REINFORCE estimator can be characterized exactly-either in closed form or via numerical moment-evaluation algorithms-without approximation. For general nonlinear dynamics and expressive policies (including neural policies), we further derive a general upper bound on the variance. These characterizations enable a direct examination of how NSR varies across policy parameters and how it evolves along optimization trajectories (e.g. SGD and Adam). Across a range of examples, we find that the NSR landscape is highly non-uniform and typically increases as the policy approaches an optimum; in some regimes it blows up, which can trigger training instability and policy collapse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>REINFORCE策略梯度估计器中非均匀的信噪比</div>
<div class="mono" style="margin-top:8px">策略梯度方法在强化学习中广泛应用，但训练过程常随学习进展变得不稳定或放缓。我们通过策略梯度估计器的信噪比（NSR）研究这一现象，该比值定义为估计器方差（噪声）与真实梯度平方范数（信号）的归一化值。我们的主要结果是：对于（i）采用高斯策略和线性状态反馈的有限时域线性系统，以及（ii）采用高斯策略和多项式反馈的有限时域多项式系统，REINFORCE估计器的NSR可精确表征（通过闭式解或数值矩估计算法），无需近似。针对一般非线性动力学和表达能力强的策略（包括神经策略），我们进一步推导了方差的一般上界。这些表征使我们能直接考察NSR如何随策略参数变化，以及如何沿优化轨迹（如SGD和Adam）演变。通过一系列示例，我们发现NSR的分布高度不均匀，且通常在策略接近最优时增大；在某些情况下会急剧上升，这可能引发训练不稳定和策略崩溃。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the instability and slowdown in policy-gradient reinforcement learning by analyzing the noise-to-signal ratio (NSR) of the REINFORCE estimator, defined as the variance normalized by the squared gradient norm. The method provides exact characterizations of NSR for finite-horizon linear and polynomial systems with Gaussian policies, and derives a general upper bound for nonlinear dynamics and neural policies. Experimental results across examples reveal that the NSR landscape is highly non-uniform, typically increasing as policies approach optima and sometimes diverging, which can trigger training instability and policy collapse.</div>
<div class="mono" style="margin-top:8px">本文通过分析REINFORCE策略梯度估计器的噪声信号比（定义为方差与梯度平方范数之比），研究了策略梯度强化学习中训练不稳定和速度下降的现象。方法上，针对有限时域线性系统和多项式系统的高斯策略给出了NSR的精确表征，并对非线性动态和神经策略推导了一般上界。实验结果表明，NSR的分布高度不均匀，通常随策略接近最优而增加，有时会发散，从而可能引发训练不稳定和策略崩溃。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs</div>
<div class="meta-line">Authors: Idan Barnea, Orin Levy, Yishay Mansour</div>
<div class="meta-line">First: 2026-02-01T21:44:11+00:00 · Latest: 2026-02-01T21:44:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01453v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ&lt; H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无奖励MDP中可证明的协作多智能体探索</div>
<div class="mono" style="margin-top:8px">我们研究无奖励探索设定下的协作多智能体强化学习，其中多个智能体共同探索未知的马尔可夫决策过程以学习其动态特性（不观察奖励）。我们聚焦于表格型有限时域MDP，采用分阶段学习框架。在每个学习阶段，多个智能体独立与环境交互。具体而言，每个智能体被分配一个策略并执行该策略，同时观察产生的轨迹。我们的主要目标是刻画学习阶段数量与智能体数量之间的权衡关系，特别是在学习阶段数量较少时。
研究结果揭示了由时域$H$主导的急剧转变：当学习阶段数等于$H$时，我们提出一种计算高效的算法，仅需$\tilde{O}(S^6 H^6 A / ε^2)$个智能体即可获得动态特性的$ε$近似（即对任意奖励函数产生$ε$最优策略）。我们通过下界证明补充该算法：任何限制在$ρ&lt; H$阶段的算法至少需要$A^{H/ρ}$个智能体才能达到恒定精度。因此证明，若将智能体数量限制为多项式级，则必须具有$H$量级的学习阶段。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates cooperative multi-agent exploration in reward-free Markov Decision Processes (MDPs), motivated by the need to understand the trade-off between the number of learning phases and the number of agents required to efficiently learn an environment&#x27;s dynamics without reward signals. The method employs a phased learning framework where multiple agents independently execute assigned policies in each phase to collect trajectories from a tabular finite-horizon MDP. The main experimental results demonstrate a sharp transition: with H learning phases (where H is the horizon), a computationally efficient algorithm using only Õ(S⁶H⁶A/ε²) agents can achieve an ε-approximation of the dynamics, enabling ε-optimal policies for any reward function; conversely, a lower bound proves that any algorithm with fewer than H phases requires exponentially many agents (A^{H/ρ}) for constant accuracy, highlighting the necessity of H phases for polynomial agent scalability.</div>
<div class="mono" style="margin-top:8px">本文研究无奖励马尔可夫决策过程中的合作多智能体探索，动机在于理解在无奖励信号下高效学习环境动态时，所需学习阶段数与智能体数量之间的权衡关系。方法采用分阶段学习框架，在表格化有限时域MDP中，让多个智能体在每个阶段独立执行指定策略以收集轨迹。主要实验结果揭示了一个关键转折：当学习阶段数等于时域H时，一种计算高效算法仅需使用Õ(S⁶H⁶A/ε²)个智能体即可获得动态的ε近似，从而为任意奖励函数生成ε最优策略；相反，下界证明任何少于H阶段的算法都需要指数级数量（A^{H/ρ}）的智能体才能达到恒定精度，这明确了H阶段对于实现智能体数量多项式可扩展性的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting</div>
<div class="meta-line">Authors: Ons Saadallah, Mátyás andó, Tamás Gábor Orosz</div>
<div class="meta-line">First: 2026-02-01T21:26:57+00:00 · Latest: 2026-02-01T21:26:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01445v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向时序预测超参数优化的元知识增强型大语言模型框架</div>
<div class="mono" style="margin-top:8px">超参数优化（HPO）对深度学习模型性能至关重要，但其计算成本高昂且可解释性差，在时序预测中尤为突出。贝叶斯优化（BO）作为主流方法，通常独立处理调优任务且决策依据不透明。大语言模型（LLM）的最新进展为将结构化先验知识与推理融入优化流程提供了新途径。本文提出LLM-AutoOpt——一种融合BO与基于LLM的上下文推理的混合HPO框架。该框架将数据集元特征、模型描述、历史优化结果及目标指标编码为LLM提示中的结构化元知识，并利用BO初始化搜索以缓解冷启动问题。该设计实现了上下文感知的稳定超参数优化，同时揭示了决策背后的推理逻辑。在多变量时序预测基准测试中，LLM-AutoOpt相比无元知识的BO和LLM基线方法，展现出更优的预测性能与更强的优化过程可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the computational expense and lack of interpretability in hyperparameter optimization (HPO) for time-series forecasting, where standard Bayesian Optimization (BO) operates independently per task and offers limited insight. To incorporate structured prior knowledge, the authors propose LLM-AutoOpt, a hybrid framework that combines BO with large language model (LLM)-based reasoning, encoding dataset meta-features, model descriptions, historical outcomes, and objectives as meta-knowledge in prompts to enable context-aware refinement. Experimental results on a multivariate time-series forecasting benchmark show that LLM-AutoOpt achieves better predictive performance and more interpretable optimization behavior compared to BO and LLM baselines lacking such meta-knowledge.</div>
<div class="mono" style="margin-top:8px">该论文针对时间序列预测中超参数优化计算成本高、可解释性差的问题，其中标准的贝叶斯优化通常独立处理每个任务且决策透明度有限。为了融入结构化先验知识，作者提出了LLM-AutoOpt，一个结合贝叶斯优化与大语言模型推理的混合框架，它将数据集元特征、模型描述、历史优化结果和目标编码为提示中的元知识，以实现上下文感知的优化。在多变量时间序列预测基准上的实验表明，与缺乏元知识的贝叶斯优化和大语言模型基线相比，LLM-AutoOpt获得了更好的预测性能和更可解释的优化行为。</div>
</details>
</div>
<div class="card">
<div class="title">TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse</div>
<div class="meta-line">Authors: Perry Dong, Kuo-Han Hung, Alexander Swerdlow, Dorsa Sadigh, Chelsea Finn</div>
<div class="meta-line">First: 2026-02-01T21:10:43+00:00 · Latest: 2026-02-01T21:10:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01439v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TQL：通过防止注意力坍缩实现基于Transformer的Q函数规模化扩展</div>
<div class="mono" style="margin-top:8px">尽管规模化推动了机器学习的最新重大进展，但强化学习方法仍主要使用小型价值函数。对价值函数进行简单扩展——包括采用已知具有高度可扩展性的Transformer架构——常导致学习不稳定和性能下降。本研究探讨了阻碍Transformer有效扩展价值函数的关键因素。通过实证分析，我们发现了规模化过程中的关键失效模式：注意力分数随容量增加而坍缩。我们的核心洞见是，通过控制注意力分数的熵值，可以有效防止这种坍缩并稳定训练，从而启用更大规模的模型。为此，我们提出Transformer Q-Learning（TQL）方法，该方法释放了Transformer在强化学习价值函数训练中的规模化潜力。实验表明，当网络规模从最小扩展到最大时，我们的方法可实现高达43%的性能提升，而现有方法则出现性能退化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates why transformers, despite their scalability in other domains, often fail when naively scaled for value functions in reinforcement learning, leading to instability and performance degradation. The authors identify attention collapse as the key failure mode and propose Transformer Q-Learning (TQL), a method that stabilizes training by controlling the entropy of attention scores to prevent this collapse. Experimental results show that TQL enables effective scaling, achieving up to a 43% performance improvement from smallest to largest network sizes, whereas prior methods degrade with scaling.</div>
<div class="mono" style="margin-top:8px">本文研究了为何在强化学习中，尽管Transformer架构在其他领域具有可扩展性，但直接将其用于价值函数缩放时会导致不稳定和性能下降。作者发现注意力崩溃是主要失效模式，并提出了Transformer Q-Learning（TQL）方法，通过控制注意力分数的熵来防止崩溃、稳定训练。实验结果表明，TQL能有效实现模型扩展，在从最小到最大网络规模的缩放中性能提升高达43%，而现有方法在缩放时性能反而下降。</div>
</details>
</div>
<div class="card">
<div class="title">BMG-Q: Localized Bipartite Match Graph Attention Q-Learning for Ride-Pooling Order Dispatch</div>
<div class="meta-line">Authors: Yulong Hu, Siyuan Feng, Sen Li</div>
<div class="meta-line">Venue: IEEE Transactions on Intelligent Transportation Systems ( Volume: 26, Issue: 10, October 2025)</div>
<div class="meta-line">First: 2025-01-23T08:01:24+00:00 · Latest: 2026-02-01T19:47:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.13448v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.13448v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces Localized Bipartite Match Graph Attention Q-Learning (BMG-Q), a novel Multi-Agent Reinforcement Learning (MARL) algorithm framework tailored for ride-pooling order dispatch. BMG-Q advances ride-pooling decision-making process with the localized bipartite match graph underlying the Markov Decision Process, enabling the development of novel Graph Attention Double Deep Q Network (GATDDQN) as the MARL backbone to capture the dynamic interactions among ride-pooling vehicles in fleet. Our approach enriches the state information for each agent with GATDDQN by leveraging a localized bipartite interdependence graph and enables a centralized global coordinator to optimize order matching and agent behavior using Integer Linear Programming (ILP). Enhanced by gradient clipping and localized graph sampling, our GATDDQN improves scalability and robustness. Furthermore, the inclusion of a posterior score function in the ILP captures the online exploration-exploitation trade-off and reduces the potential overestimation bias of agents, thereby elevating the quality of the derived solutions. Through extensive experiments and validation, BMG-Q has demonstrated superior performance in both training and operations for thousands of vehicle agents, outperforming benchmark reinforcement learning frameworks by around 10% in accumulative rewards and showing a significant reduction in overestimation bias by over 50%. Additionally, it maintains robustness amidst task variations and fleet size changes, establishing BMG-Q as an effective, scalable, and robust framework for advancing ride-pooling order dispatch operations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BMG-Q：面向拼车订单调度的局部二分匹配图注意力Q学习算法</div>
<div class="mono" style="margin-top:8px">本文提出局部二分匹配图注意力Q学习（BMG-Q），一种专为拼车订单调度设计的新型多智能体强化学习算法框架。BMG-Q通过将局部二分匹配图嵌入马尔可夫决策过程，创新性地构建图注意力双深度Q网络作为多智能体强化学习核心，以捕捉车队中拼车车辆的动态交互。该方法利用局部二分依赖图增强各智能体的状态信息，并采用整数线性规划实现中心化全局协调器以优化订单匹配与智能体行为。通过梯度裁剪与局部图采样技术增强的图注意力双深度Q网络提升了算法的可扩展性与鲁棒性。此外，在整数线性规划中引入后验评分函数可平衡在线探索与利用的权衡，降低智能体的高估偏差，从而提升求解质量。经大量实验验证，BMG-Q在数千车辆智能体的训练与运营中均表现优异：累计奖励较基准强化学习框架提升约10%，高估偏差降低超50%，且在任务变化与车队规模变动中保持鲁棒性，成为推进拼车订单调度的高效、可扩展且稳健的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces BMG-Q, a Multi-Agent Reinforcement Learning framework designed to improve ride-pooling order dispatch by modeling the decision process as a localized bipartite match graph. The method employs a Graph Attention Double Deep Q-Network (GATDDQN) to capture dynamic interactions among vehicles, enhanced with gradient clipping and graph sampling for scalability, and uses Integer Linear Programming (ILP) with a posterior score function for centralized optimization and bias reduction. Experimental results show that BMG-Q outperforms benchmark RL methods by approximately 10% in cumulative reward and reduces overestimation bias by over 50%, while maintaining robustness across varying fleet sizes and task conditions.</div>
<div class="mono" style="margin-top:8px">本文提出了BMG-Q，一种用于提升网约车合乘订单调度的多智能体强化学习框架，其核心是将决策过程建模为局部二分匹配图。该方法采用图注意力双深度Q网络（GATDDQN）来捕捉车辆间的动态交互，并通过梯度裁剪和图采样增强可扩展性，同时利用整数线性规划（ILP）结合后验评分函数进行集中优化以减少偏差。实验结果表明，BMG-Q在累积奖励上比基准强化学习方法提升约10%，过度估计偏差降低超过50%，并在不同车队规模和任务变化下保持了鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton&#x27;s Laws in Financial Deep Reinforcement Learning (RL) Algorithms</div>
<div class="meta-line">Authors: Trang Thoi, Hung Tran, Tram Thoi, Huaiyang Zhong</div>
<div class="meta-line">First: 2026-02-01T18:48:33+00:00 · Latest: 2026-02-01T18:48:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01388v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01388v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (DRL), a subset of machine learning focused on sequential decision-making, has emerged as a powerful approach for tackling financial trading problems. In finance, DRL is commonly used either to generate discrete trade signals or to determine continuous portfolio allocations. In this work, we propose a novel reinforcement learning framework for portfolio optimization that incorporates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into several DRL algorithms. The approach replaces conventional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs) in both actor and critic components-utilizing learnable B-spline univariate functions to achieve parameter-efficient and more interpretable function approximation. During actor updates, we introduce a physics-informed regularization loss that promotes second-order temporal consistency between observed return dynamics and the action-induced portfolio adjustments. The proposed framework is evaluated across three equity markets-China, Vietnam, and the United States, covering both emerging and developed economies. Across all three markets, PIKAN-based agents consistently deliver higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and more favorable drawdown characteristics compared to both standard DRL baselines and classical online portfolio-selection methods. This yields more stable training, higher Sharpe ratios, and superior performance compared to traditional DRL counterparts. The approach is particularly valuable in highly dynamic and noisy financial markets, where conventional DRL often suffers from instability and poor generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强型物理信息Kolmogorov-Arnold网络：牛顿定律在金融深度强化学习算法中的应用</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）作为机器学习中专注于序列决策的子领域，已成为解决金融交易问题的有力方法。在金融领域，DRL通常用于生成离散交易信号或确定连续资产配置。本研究提出了一种新颖的强化学习框架，将物理信息Kolmogorov-Arnold网络（PIKAN）集成至多种DRL算法中，用于投资组合优化。该方法在行动者与评论者组件中均以Kolmogorov-Arnold网络（KAN）替代传统多层感知机，利用可学习的B样条单变量函数实现参数高效且更具可解释性的函数逼近。在行动者更新过程中，我们引入了物理信息正则化损失，以促进观测收益动态与行动引发的投资组合调整之间的二阶时间一致性。所提框架在中国、越南和美国三个股票市场（涵盖新兴与发达经济体）进行了评估。在所有市场中，基于PIKAN的智能体相较于标准DRL基准和经典在线投资组合选择方法，均持续实现更高的累计与年化收益、更优的夏普与卡尔玛比率，以及更有利的回撤特性。与传统DRL方法相比，该框架实现了更稳定的训练、更高的夏普比率和更卓越的性能。该方法在高度动态且噪声显著的金融市场中尤为有价值，因为传统DRL在此类市场中常面临不稳定与泛化能力不足的问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more stable and interpretable deep reinforcement learning (DRL) in financial portfolio optimization, this paper introduces a novel framework that integrates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into DRL algorithms. The method replaces standard multilayer perceptrons with Kolmogorov-Arnold Networks (KANs) in actor and critic components, using learnable B-spline functions for efficient approximation, and adds a physics-informed regularization loss during actor updates to enforce second-order temporal consistency between returns and portfolio adjustments. Experimental results across equity markets in China, Vietnam, and the United States show that PIKAN-based agents achieve higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and better drawdown characteristics compared to standard DRL baselines and classical portfolio methods, leading to more stable training and improved performance in dynamic financial environments.</div>
<div class="mono" style="margin-top:8px">本文旨在解决传统深度强化学习（DRL）在金融投资组合优化中不稳定和可解释性差的问题，提出了一种将物理信息柯尔莫哥洛夫-阿诺德网络（PIKANs）集成到DRL算法中的新框架。该方法在行动者和评论者组件中用柯尔莫哥洛夫-阿诺德网络（KANs）替代传统多层感知器，利用可学习的B样条函数进行高效近似，并在行动者更新时引入物理信息正则化损失，以促进收益动态与投资组合调整之间的二阶时间一致性。在中国、越南和美国股市的实验结果表明，基于PIKAN的智能体相比标准DRL基线和经典投资组合方法，能持续获得更高的累计和年化收益、更优的夏普和卡尔玛比率以及更有利的回撤特性，从而在高度动态和嘈杂的金融市场中实现更稳定的训练和更卓越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning</div>
<div class="meta-line">Authors: Zhiheng Jiang, Yunzhe Wang, Ryan Marr, Ellen Novoseller, Benjamin T. Files, Volkan Ustun</div>
<div class="meta-line">First: 2026-01-28T16:36:37+00:00 · Latest: 2026-02-01T18:34:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20753v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20753v2">PDF</a> · <a href="https://github.com/jzh001/GraphAllocBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks (GNNs) in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://github.com/jzh001/GraphAllocBench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphAllocBench：面向偏好条件多目标策略学习的灵活基准</div>
<div class="mono" style="margin-top:8px">多目标强化学习中的偏好条件策略学习旨在通过将策略与用户对目标的指定偏好条件相结合，逼近多样化的帕累托最优解。这使得单一模型能够在运行时通过生成位于或接近帕累托前沿的策略，灵活适应任意权衡。然而，现有的PCPL基准主要局限于玩具任务和固定环境，限制了其实用性和可扩展性。为填补这一空白，我们提出了GraphAllocBench——一个基于新型图结构资源分配沙盒环境构建的灵活基准，该环境受城市管理启发，我们称之为CityPlannerEnv。GraphAllocBench提供了一系列丰富的问题集，涵盖多样化的目标函数、可变的偏好条件和高维可扩展性。我们还提出了两个新的评估指标——非支配解比例和排序分数——在补充广泛使用的超体积指标的同时，直接捕捉偏好一致性。通过多层感知机和图感知模型的实验，我们证明GraphAllocBench揭示了现有MORL方法的局限性，并为在复杂高维组合分配任务中应用基于图的方法（如图神经网络）开辟了道路。除预定义问题集外，GraphAllocBench允许用户灵活调整目标、偏好和分配规则，使其成为推动PCPL发展的通用可扩展基准。代码：https://github.com/jzh001/GraphAllocBench</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GraphAllocBench, a new benchmark for Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL), motivated by the lack of realistic and scalable environments beyond simple toy tasks. The method centers on a flexible graph-based resource allocation sandbox called CityPlannerEnv, which supports diverse objectives, preference conditions, and high-dimensional scalability, and proposes two new evaluation metrics—Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS)—to better assess preference consistency alongside hypervolume. Experimental results with MLPs and graph-aware models reveal that existing MORL approaches struggle on this benchmark, highlighting the potential for graph-based methods like GNNs in complex combinatorial allocation tasks, while the benchmark&#x27;s customizable design allows for extensible testing of PCPL algorithms.</div>
<div class="mono" style="margin-top:8px">本文提出了GraphAllocBench，这是一个用于多目标强化学习中偏好条件策略学习的新基准，其动机在于现有基准多局限于简单任务，缺乏现实性和可扩展性。该方法基于一个名为CityPlannerEnv的灵活图资源分配沙盒环境，支持多样化的目标函数、偏好条件和高维可扩展性，并引入了两个新评估指标——非支配解比例和排序分数——以补充超体积指标，更直接地衡量偏好一致性。实验使用多层感知机和图感知模型进行，结果表明现有多目标强化学习方法在此基准上存在局限，凸显了图神经网络等图方法在复杂组合分配任务中的潜力，同时该基准的可定制设计使其能灵活扩展，推动偏好条件策略学习的发展。</div>
</details>
</div>
<div class="card">
<div class="title">PromptRL: Prompt Matters in RL for Flow-Based Image Generation</div>
<div class="meta-line">Authors: Fu-Yun Wang, Han Zhang, Michael Gharbi, Hongsheng Li, Taesung Park</div>
<div class="meta-line">First: 2026-02-01T18:31:06+00:00 · Latest: 2026-02-01T18:31:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01382v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01382v1">PDF</a> · <a href="https://github.com/G-U-N/UniRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PromptRL：基于流的图像生成中强化学习的提示优化框架</div>
<div class="mono" style="margin-top:8px">流匹配模型革新了文本到图像生成领域，强化学习作为关键的后训练策略用于对齐奖励目标。本研究揭示当前流匹配模型的强化学习流程存在两个被低估但重要的问题：因生成多样性不足导致的样本效率低下，以及显著的提示过拟合现象——模型会记忆特定训练表述，在语义等效但风格多变的提示下出现性能崩溃。我们提出PromptRL框架，将语言模型作为可训练的提示优化智能体直接嵌入基于流的强化学习优化循环。该设计带来双重互补优势：快速开发复杂的提示重写能力，以及重塑优化动态的协同训练机制。PromptRL在多项基准测试中达到最先进性能：GenEval得分0.97、OCR准确率0.98、PickScore得分24.05。在大规模图像编辑模型上的验证表明，仅用6万次推演就将FLUX.1-Kontext的EditReward从1.19提升至1.43，超越得分为1.37的Gemini 2.5 Flash Image，并与依赖细粒度数据标注和复杂多阶段训练的ReasonNet（1.44）性能相当。大量实验证实，PromptRL在实现更高性能上限的同时，所需推演次数比纯流强化学习减少2倍以上。代码已开源：https://github.com/G-U-N/UniRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses two key limitations in reinforcement learning (RL) pipelines for flow matching models in text-to-image generation: sample inefficiency from low diversity and severe prompt overfitting, where models fail on semantically similar but stylistically different prompts. The proposed PromptRL framework integrates language models as trainable prompt refinement agents directly into the RL optimization loop, enabling sophisticated prompt rewriting and creating a synergistic training dynamic that reshapes optimization. Experimental results show state-of-the-art performance, including scores of 0.97 on GenEval and 0.98 on OCR accuracy, while also improving the EditReward of a large-scale editing model from 1.19 to 1.43 with minimal rollouts, consistently achieving higher performance with over twice the sample efficiency of naive RL approaches.</div>
<div class="mono" style="margin-top:8px">本研究针对基于流匹配模型的文本到图像生成中强化学习流程的两个关键局限：因生成多样性不足导致的样本低效性，以及严重的提示词过拟合问题，即模型会记忆特定训练表述并在语义相同但风格多变的提示词上出现性能崩溃。提出的PromptRL框架将语言模型作为可训练的提示词优化代理直接集成到强化学习优化循环中，实现了复杂的提示词重写能力并形成了协同训练机制以重塑优化动态。实验结果表明该方法取得了最先进的性能，如在GenEval上获得0.97分，在OCR准确率上获得0.98分，同时仅用少量采样就将大规模图像编辑模型的EditReward从1.19提升至1.43，其性能上限更高且样本效率比朴素强化学习方法提升超过两倍。</div>
</details>
</div>
<div class="card">
<div class="title">When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning</div>
<div class="meta-line">Authors: Wang Yang, Shouren Wang, Chaoda Song, Chuang Ma, Xinpeng Li, Nengbo Wang, Kaixiong Zhou, Vipin Chaudhary, Xiaotian Han</div>
<div class="meta-line">First: 2026-02-01T18:04:07+00:00 · Latest: 2026-02-01T18:04:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01365v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01365v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>领域交互时：强化学习推理中的非对称与顺序敏感跨域效应</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）已成为提升大语言模型推理能力的关键技术，但其在不同领域排序策略下的行为尚不明确。特别是GRPO中顺序训练（一次一个领域）与混合领域训练（一次多个领域）的影响尚未得到系统研究。我们首次对数学、科学、逻辑和谜题推理任务中的训练顺序效应进行了系统分析，发现：（1）单领域泛化具有高度非对称性：在其他领域训练可使数学推理准确率提升约25%，但对逻辑和谜题任务的迁移效果可忽略；（2）跨领域交互高度依赖顺序：按数学→科学顺序训练在数学/科学任务上分别达到83%/41%准确率，而反转为科学→数学顺序则使性能降至77%/25%；（3）多领域训练中不存在普适最优策略：顺序训练更利于数学（最高84%），混合训练更利于科学与逻辑，不当排序可能导致巨大性能差距（从70%降至56%）。总体而言，我们的研究证明多领域设置下的GRPO表现出显著的非对称性、顺序敏感性和策略依赖性，凸显了领域感知与顺序感知训练设计的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the impact of training order and strategy in Group Relative Policy Optimization (GRPO) for enhancing reasoning in large language models, motivated by a lack of systematic understanding of how sequential versus mixed-domain training affects cross-domain generalization. The method involves a systematic analysis across math, science, logic, and puzzle reasoning tasks to evaluate asymmetric transfer and order sensitivity. Key experimental results reveal strong asymmetry in generalization, with training on other domains boosting math reasoning by about 25% accuracy but showing negligible transfer to logic and puzzle tasks; cross-domain interactions are highly order-dependent, where math→science sequencing yields 83%/41% accuracy on math/science, while science→math reduces performance to 77%/25%; and no single training strategy is universally optimal, as sequential training favors math (up to 84% accuracy), mixed training benefits science and logic, and poor ordering can cause large performance drops from 70% to 56%.</div>
<div class="mono" style="margin-top:8px">本文研究了在大型语言模型推理增强中，组相对策略优化（GRPO）的训练顺序和策略的影响，其动机在于缺乏对顺序训练与混合领域训练如何影响跨领域泛化的系统理解。方法涉及对数学、科学、逻辑和谜题推理任务进行系统分析，以评估不对称迁移和顺序敏感性。主要实验结果表明，泛化存在显著不对称性，在其他领域训练可将数学推理准确率提升约25%，但对逻辑和谜题任务迁移效果可忽略不计；跨领域交互高度依赖顺序，数学→科学顺序在数学/科学上分别达到83%/41%的准确率，而科学→数学顺序则使性能降至77%/25%；且没有单一训练策略普遍最优，顺序训练有利于数学（准确率高达84%），混合训练有益于科学和逻辑，而顺序不当可能导致性能从70%大幅下降至56%。</div>
</details>
</div>
<div class="card">
<div class="title">CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering</div>
<div class="meta-line">Authors: Yu Liu, Wenxiao Zhang, Cong Cao, Fangfang Yuan, Weizhuo Chen, Cheng Hu, Pin Xu, Yuling Yang, Kun Peng, Diandian Guo, Qiang Sun, Yanbing Liu, Jin B. Hong, Zhiyuan Ma</div>
<div class="meta-line">First: 2026-02-01T17:33:39+00:00 · Latest: 2026-02-01T17:33:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01348v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01348v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRAFT：基于强化学习的答案忠实轨迹校准推理用于多跳问答</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）被广泛用于为大语言模型（LLM）在多跳问答中提供依据。近期研究主要通过微调及结构化或基于强化的优化来提升答案准确性。然而，响应生成中的可靠推理面临三大挑战：1）推理崩溃。多跳问答的推理因多跳组合而本质复杂，且易受噪声检索干扰而失稳。2）推理-答案不一致。由于LLM生成的内在不确定性及证据-干扰混合的影响，模型可能产生正确但未被其中间推理或证据忠实支持的答案。3）格式控制缺失。传统思维链生成常偏离所需结构化输出格式，导致不完整或格式错误的结构化内容。为应对这些挑战，我们提出CRAFT（基于答案忠实轨迹的校准推理），这是一个基于群体相对策略优化（GRPO）的强化学习框架，通过训练模型在响应生成中执行忠实推理。CRAFT采用双重奖励机制优化多跳推理：确定性奖励确保结构正确性，而基于评判的奖励验证语义忠实性。该优化框架支持可控的轨迹变体，可系统分析结构与规模如何影响推理性能及忠实度。在三个多跳问答基准上的实验表明，CRAFT在不同模型规模下均提升了答案准确性与推理忠实性，其中CRAFT 7B模型在多种推理轨迹设置中达到了与闭源LLM相竞争的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by three key challenges in retrieval-augmented generation for multi-hop question answering: reasoning collapse due to complexity and noisy retrieval, inconsistency between correct answers and unfaithful reasoning, and loss of format control in structured outputs. To address these, the authors propose CRAFT, a reinforcement learning framework using Group Relative Policy Optimization with dual rewards—deterministic for structural correctness and judge-based for semantic faithfulness—to train models for calibrated, answer-faithful reasoning. Experimental results on three multi-hop QA benchmarks demonstrate that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the 7B variant achieving competitive performance against closed-source large language models in various reasoning trace settings.</div>
<div class="mono" style="margin-top:8px">本文的动机源于检索增强生成在多跳问答中的三个关键挑战：因复杂性和噪声检索导致的推理崩溃、正确答案与不忠实推理之间的不一致性，以及结构化输出的格式失控。为解决这些问题，作者提出了CRAFT，一个基于群体相对策略优化的强化学习框架，采用双重奖励机制——确定性奖励确保结构正确性，基于评判的奖励验证语义忠实性——以训练模型进行校准且答案忠实的推理。在三个多跳问答基准上的实验结果表明，CRAFT在不同模型规模上均提升了答案准确性和推理忠实性，其中7B版本在多种推理轨迹设置下实现了与闭源大语言模型相竞争的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization</div>
<div class="meta-line">Authors: Poushali Sengupta, Mayank Raikwar, Sabita Maharjan, Frank Eliassen, Yan Zhang</div>
<div class="meta-line">First: 2026-02-01T17:11:43+00:00 · Latest: 2026-02-01T17:11:43+00:00</div>
<div class="meta-line">Comments: Accepted for presentation at NDSS 2026 - FutureG Workshop, 23 February 2026. (10 pages, 5 figures.)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01342v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01342v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Powerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and can slow down communication, posing a challenge for fast 6G vehicle networks. In this paper, we propose an adaptive post-quantum cryptography (PQC) framework that predicts short-term mobility and channel variations and dynamically selects suitable lattice-, code-, or hash-based PQC configurations using a predictive multi-objective evolutionary algorithm (APMOEA) to meet vehicular latency and security constraints.However, frequent cryptographic reconfiguration in dynamic vehicular environments introduces new attack surfaces during algorithm transitions. A secure monotonic-upgrade protocol prevents downgrade, replay, and desynchronization attacks during transitions. Theoretical results show decision stability under bounded prediction error, latency boundedness under mobility drift, and correctness under small forecast noise. These results demonstrate a practical path toward quantum-safe cryptography in future 6G vehicular networks. Through extensive experiments based on realistic mobility (LuST), weather (ERA5), and NR-V2X channel traces, we show that the proposed framework reduces end-to-end latency by up to 27\%, lowers communication overhead by up to 65\%, and effectively stabilizes cryptographic switching behavior using reinforcement learning. Moreover, under the evaluated adversarial scenarios, the monotonic-upgrade protocol successfully prevents downgrade, replay, and desynchronization attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于上下文感知优化的6G车联网自适应抗量子密码框架</div>
<div class="mono" style="margin-top:8px">未来强大的量子计算机可能破解车联网（V2X）通信安全。后量子密码学虽能提供防护，但其高计算开销易导致6G高速车联网通信延迟。本文提出一种自适应后量子密码框架，通过预测短期移动性与信道变化，利用预测性多目标进化算法动态选择基于格、编码或哈希的密码配置，以满足车联网时延与安全约束。针对动态环境中频繁密码切换可能引发的算法过渡期攻击面，本框架采用安全单调升级协议防御降级、重放与失步攻击。理论分析表明：该框架在有限预测误差下具有决策稳定性，在移动漂移下保持时延有界性，并在小范围预测噪声下确保正确性。基于真实移动（LuST）、气象（ERA5）与NR-V2X信道轨迹的实验显示，该框架可降低端到端时延达27%，减少通信开销达65%，并通过强化学习稳定密码切换行为。在对抗场景评估中，单调升级协议成功阻断了降级、重放与失步攻击。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the threat that future quantum computers pose to the security of 6G vehicular (V2X) communications and the performance overhead of standard post-quantum cryptography (PQC), this paper proposes an adaptive PQC framework. The method uses a predictive multi-objective evolutionary algorithm (APMOEA) to dynamically select optimal PQC configurations based on predicted mobility and channel conditions, and it introduces a secure monotonic-upgrade protocol to prevent attacks during cryptographic transitions. Experimental results using realistic mobility and channel data show the framework reduces end-to-end latency by up to 27%, lowers communication overhead by up to 65%, stabilizes switching behavior, and the security protocol successfully prevents downgrade, replay, and desynchronization attacks.</div>
<div class="mono" style="margin-top:8px">针对未来量子计算机对6G车联网（V2X）通信安全的威胁以及标准后量子密码学带来的性能开销，本文提出了一种自适应后量子密码学框架。该方法利用预测性多目标进化算法（APMOEA），根据预测的移动性和信道条件动态选择最优的后量子密码配置，并引入一种安全的单调升级协议以防止密码算法切换期间的攻击。基于真实移动性和信道数据的实验结果表明，该框架将端到端延迟降低了高达27%，通信开销降低了高达65%，稳定了切换行为，且安全协议成功防止了降级、重放和去同步攻击。</div>
</details>
</div>
<div class="card">
<div class="title">LongCat-Flash-Thinking-2601 Technical Report</div>
<div class="meta-line">Authors: Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chen Gao, Chen Zhang, Chengcheng Han, Chenhui Yang, Chuyu Zhang, Cong Chen, Cunguang Wang, Daoru Pan, Defei Bu, Dengchang Zhao, Di Xiu, Dishan Liu, Dongyu Ru, Dunwei Tu, Fan Wu, Fengcheng Yuan, Fengcun Li, Gang Xu, Guanyu Wu, Guoyuan Lin, Haibin Wang, Hansi Yang, Hao Yang, Haonan Yan, Haoxiang Ma, Haoxing Wen, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiacheng Zhang, Jiahong Zhou, Jiahuan Li, Jiaming Wang, Jian Yang, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiapeng Zhu, Jiaqi Sun, Jiarong Shi, Jiarui Zhao, Jingang Wang, Jinluan Yang, Jinrui Ding, Jinwei Xiao, Jiyuan He, Juncan Xu, Kefeng Zhang, Keheng Wang, Li Wei, Lianhui Ma, Lin Qiu, Lingbing Kong, Lingchuan Liu, Linsen Guo, Mengshen Zhu, Mengxia Shen, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pengtao Zhang, Ping Liu, Qi Gu, Qiong Huang, Qiyuan Duan, Quanchi Weng, Rongxiang Weng, Rongzhi Zhang, Rumei Li, Shanglin Lei, Shengnan An, Shijun Dai, Shizhe Wu, Shuaikang Liu, Shuang Zhou, Shuo Wang, Songyuan Zhao, Tao Liang, Tianhao Hu, Tianze Chen, Wei Liu, Wei Shi, Wei Wang, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Wentao Chen, Wentao Shi, Xi Su, Xiandi Ma, Xiangcheng Liu, Xiangyu Xi, Xiangyuan Liu, Xiangzhou Huang, Xiao Liu, Xiaodong Cai, Xiaolong Chen, Xiaowei Shi, Xiaoyu Li, Xin Chen, Xingchen Liu, Xuan Huang, Xuezhi Cao, Xunliang Cai, Yan Chen, Yang Bai, Yang Liu, Yang Yang, Yang Zheng, Yanyu Chen, Yaoming Wang, Yaoming Zhu, Yaorui Shi, Yaqi Huo, Yerui Sun, Yi Zhang, Yi-Kai Zhang, Yifan Lu, Yifan Zhao, Yihao Chen, Yitao Zhai, Yongjing Yin, Yongwei Zhou, Youshao Xiao, Yu Wang, Yu Yang, Yuchen Xie, Yuchen Yu, Yuchuan Dai, Yue Xu, Yueqing Sun, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunke Zhao, Yuwei Jiang, Yuxin Bian, Yuxin Chen, Yuxin Liu, Zeyang Yu, Zhao Yang, Zhengsheng Huang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhimin Lin, Zhiyuan Yao, Zhuofan Chen, Zhuowen Han, Zijian Zhang, Ziran Li, Ziwen Wang, Ziyuan Zhuang</div>
<div class="meta-line">First: 2026-01-23T13:20:09+00:00 · Latest: 2026-02-01T16:43:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16725v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16725v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model&#x27;s strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LongCat-Flash-Thinking-2601 技术报告</div>
<div class="mono" style="margin-top:8px">我们推出 LongCat-Flash-Thinking-2601，这是一个拥有 5600 亿参数的开源专家混合推理模型，具备卓越的智能体推理能力。该模型在广泛的智能体基准测试中，包括智能体搜索、智能体工具使用和工具集成推理，均取得了开源模型中的最先进性能。除了基准测试表现，该模型还展现出对复杂工具交互的强大泛化能力，以及在嘈杂现实环境下的稳健行为。其先进能力源于一个统一的训练框架，该框架结合了领域并行专家训练与后续融合，以及从预训练到后训练阶段，涵盖数据构建、环境、算法和基础设施的端到端协同设计。特别是，模型在复杂工具使用方面的强大泛化能力，得益于我们对环境扩展和原则性任务构建的深入探索。为优化长尾、偏态生成和多轮智能体交互，并实现在跨越 20 多个领域的超过 10,000 个环境中的稳定训练，我们系统地扩展了异步强化学习框架 DORA，以实现稳定高效的大规模多环境训练。此外，认识到现实世界任务本质上是嘈杂的，我们对现实世界的噪声模式进行了系统分析和分解，并设计了有针对性的训练程序，明确地将此类不完美因素纳入训练过程，从而提升了现实世界应用的鲁棒性。为进一步增强复杂推理任务的性能，我们引入了深度思考模式，通过密集并行思考联合扩展推理深度和宽度，实现有效的测试时扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts model, motivated by the need for advanced agentic reasoning capabilities in complex, noisy real-world environments. The method employs a unified training framework featuring domain-parallel expert training with fusion, an end-to-end co-design of data and infrastructure, and an extended asynchronous reinforcement learning system (DORA) for stable multi-environment training across over 10,000 environments; it also incorporates systematic noise analysis and a Heavy Thinking mode for test-time scaling. Experimental results show state-of-the-art performance among open-source models on agentic benchmarks like search and tool use, with strong generalization to complex tool interactions and improved robustness in noisy settings.</div>
<div class="mono" style="margin-top:8px">本文介绍了LongCat-Flash-Thinking-2601，一个拥有5600亿参数的开源混合专家模型，其动机是为复杂、嘈杂的现实环境提供先进的智能体推理能力。方法采用统一的训练框架，包括领域并行专家训练与融合、从预训练到后训练的数据与基础设施端到端协同设计，以及扩展的异步强化学习系统（DORA）以在超过10,000个跨20多个领域的环境中实现稳定训练；同时系统分析现实噪声模式并针对性纳入训练过程，还引入了重型思考模式以通过并行思考扩展推理深度和宽度。实验结果表明，该模型在智能体搜索、工具使用等基准测试中达到了开源模型的领先水平，展现出对复杂工具交互的强大泛化能力以及在嘈杂环境中的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</div>
<div class="meta-line">Authors: Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Jian Zhang, Yanfeng Wang, Ya Zhang, Weidi Xie</div>
<div class="meta-line">First: 2025-08-21T17:42:47+00:00 · Latest: 2026-02-01T16:18:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.15746v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.15746v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://qiaoyu-zheng.github.io/Deep-DxSearch">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The integration of Large Language Models (LLMs) into healthcare is constrained by knowledge limitations, hallucinations, and a disconnect from Evidence-Based Medicine (EBM). While Retrieval-Augmented Generation (RAG) offers a solution, current systems often rely on static workflows that miss the iterative, hypothetico-deductive reasoning of clinicians. To address this, we introduce Deep-DxSearch, an agentic RAG system trained end-to-end via reinforcement learning (RL) for traceable diagnostic reasoning. Deep-DxSearch acts as an active investigator, treating the LLM as an agent within an environment of 16,000+ guideline-derived disease profiles, 150,000+ patient records for case-based reasoning, and over 27 million biomedical documents. Using soft verifiable rewards that co-optimize retrieval and reasoning, the model learns to formulate queries, evaluate evidence, and refine searches to close diagnostic gaps. Experiments show our end-to-end RL framework consistently outperforms prompt-engineering and training-free RAG methods. On in-distribution (ID) and out-of-distribution (OOD) benchmarks for common and rare diseases, Deep-DxSearch surpasses strong baselines-including GPT-4o, DeepSeek-R1, and medical-specific frameworks-achieving an average accuracy gain of 22.7% over the second-best model. In validation with 150 real-world cases, Deep-DxSearch boosts physicians&#x27; average diagnostic accuracy from 45.6% to 69.1%. These results indicate that evolving agentic systems to leverage statistical regularities in large-scale healthcare data is key for trustworthy diagnostic assistants. All data, code, and checkpoints are available at https://qiaoyu-zheng.github.io/Deep-DxSearch.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>端到端可追溯诊断推理的智能体化RAG系统训练</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在医疗领域的应用受限于知识局限、幻觉问题以及与循证医学（EBM）的脱节。检索增强生成（RAG）虽提供了一种解决方案，但现有系统多依赖静态工作流，未能模拟临床医生迭代式的假设-演绎推理。为此，我们提出了Deep-DxSearch——一种通过强化学习（RL）端到端训练的智能体化RAG系统，旨在实现可追溯的诊断推理。该系统作为主动调查者，将LLM置于包含16,000+个指南衍生病症档案、150,000+份病例推理记录及超过2700万份生物医学文档的环境中进行训练。通过采用协同优化检索与推理的软可验证奖励机制，模型学习构建查询、评估证据并优化搜索以填补诊断缺口。实验表明，我们的端到端RL框架在常见病与罕见病的分布内（ID）和分布外（OOD）基准测试中，持续优于提示工程及无训练RAG方法，平均准确率较次优模型（包括GPT-4o、DeepSeek-R1及医疗专用框架）提升22.7%。在150例真实病例验证中，Deep-DxSearch将医师平均诊断准确率从45.6%提升至69.1%。这些结果表明，发展智能体系统以利用大规模医疗数据中的统计规律，是构建可信诊断助手的关键。所有数据、代码及模型检查点均已公开：https://qiaoyu-zheng.github.io/Deep-DxSearch。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of LLMs in healthcare, such as knowledge gaps and hallucinations, and the need for systems that mimic clinicians&#x27; iterative reasoning, this paper introduces Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning for traceable diagnostic reasoning. The method treats the LLM as an agent that actively queries and evaluates evidence from a large environment of disease profiles, patient records, and biomedical documents, using soft verifiable rewards to co-optimize retrieval and reasoning. Experimental results show that the system consistently outperforms prompt-engineering and training-free RAG baselines, achieving an average accuracy gain of 22.7% on ID and OOD benchmarks and boosting physicians&#x27; diagnostic accuracy from 45.6% to 69.1% in real-world case validation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型在医疗领域应用中的知识局限、幻觉问题以及与循证医学脱节的挑战，提出了一种可追溯诊断推理的智能体RAG系统Deep-DxSearch。该方法通过端到端的强化学习训练，将LLM作为智能体，使其在包含疾病档案、患者记录和生物医学文档的大型环境中主动进行查询和证据评估，并利用可验证的软奖励协同优化检索与推理。实验结果表明，该系统在分布内和分布外基准测试中均优于基于提示工程和无训练的RAG方法，平均准确率提升22.7%，并在150个真实病例验证中将医生的平均诊断准确率从45.6%提高至69.1%。</div>
</details>
</div>
<div class="card">
<div class="title">Dichotomous Diffusion Policy Optimization</div>
<div class="meta-line">Authors: Ruiming Liang, Yinan Zheng, Kexin Zheng, Tianyi Tan, Jianxiong Li, Liyuan Mao, Zhihao Wang, Guang Chen, Hangjun Ye, Jingjing Liu, Jinqiao Wang, Xianyuan Zhan</div>
<div class="meta-line">First: 2025-12-31T16:56:56+00:00 · Latest: 2026-02-01T16:02:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00898v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00898v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>二分扩散策略优化</div>
<div class="mono" style="margin-top:8px">基于扩散的策略因其在推理过程中卓越的表达能力和可控生成特性，在解决广泛决策任务中日益受到青睐。然而，利用强化学习有效训练大规模扩散策略仍具挑战。现有方法或因直接最大化价值目标导致训练不稳定，或因依赖粗略的高斯似然近似而面临计算问题，后者需要大量足够小的去噪步骤。本文提出DIPOLE（二分扩散策略改进），一种专为稳定可控扩散策略优化设计的新型强化学习算法。我们首先重新审视强化学习中的KL正则化目标，该目标为扩散策略提取提供了理想的加权回归目标，但常难以平衡贪婪性与稳定性。随后，我们构建了一种贪婪化策略正则化方案，自然地将最优策略分解为一对稳定学习的二分策略：一个致力于奖励最大化，另一个专注于奖励最小化。在此设计下，优化动作可通过在推理过程中线性组合二分策略的分数生成，从而实现对贪婪程度的灵活控制。在ExORL和OGBench的离线及离线到在线强化学习设置中的评估验证了本方法的有效性。我们还使用DIPOLE训练了一个大型视觉-语言-动作模型，用于端到端自动驾驶，并在大规模真实世界自动驾驶基准NAVSIM上进行评估，突显了其在复杂现实应用中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the training challenges of large diffusion policies in reinforcement learning, which often suffer from instability or computational inefficiency. The proposed method, DIPOLE, introduces a greedified policy regularization scheme that decomposes the optimal policy into two stably learned dichotomous policies—one for reward maximization and one for minimization—allowing optimized actions to be generated by linearly combining their scores during inference for flexible greediness control. Experimental results show effectiveness in offline and offline-to-online RL settings on ExORL and OGBench, and the method successfully trains a vision-language-action model for autonomous driving, demonstrating strong performance on the NAVSIM benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决强化学习中大型扩散策略训练面临的挑战，这些策略常因不稳定或计算效率低下而难以优化。所提出的DIPOLE方法采用了一种贪婪化策略正则化方案，将最优策略分解为两个稳定学习的二分策略——一个用于奖励最大化，另一个用于奖励最小化——从而在推理时通过线性组合其分数来生成优化动作，实现贪婪程度的灵活控制。实验结果表明，该方法在ExORL和OGBench的离线和离线到在线强化学习设置中表现有效，并成功训练了一个用于端到端自动驾驶的视觉-语言-动作模型，在NAVSIM大规模真实世界基准测试中展现了优异性能。</div>
</details>
</div>
<div class="card">
<div class="title">From Intents to Actions: Agentic AI in Autonomous Networks</div>
<div class="meta-line">Authors: Burak Demirel, Pablo Soldati, Yu Wang</div>
<div class="meta-line">First: 2026-02-01T15:01:57+00:00 · Latest: 2026-02-01T15:01:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01271v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01271v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从意图到行动：自主网络中的智能体化人工智能</div>
<div class="mono" style="margin-top:8px">电信网络日益需要自主运行，同时支持具有多样化且常相互冲突意图的异构服务——即各服务特有的性能目标、约束与需求。然而，将高层意图（如超低延迟、高吞吐量或能效）转化为具体控制行动（即低层执行器指令），现有启发式方法仍无法实现。本研究提出一种用于意图驱动自主网络的智能体化人工智能系统，其架构围绕三个专用智能体构建：由语言模型驱动的监督解析智能体，既执行意图到可执行优化模板的词法解析，也基于反馈、约束可行性与动态网络条件进行认知优化；优化器智能体将这些模板转化为可处理的优化问题，分析权衡关系并推导跨目标偏好；最后，基于多目标强化学习的偏好驱动控制器智能体，利用这些偏好在网络性能的帕累托前沿附近运行，以最优满足原始意图。这些智能体协同工作，使网络能够以可扩展方式自主解析、推理、适应并响应多样化意图与网络条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of autonomously translating high-level service intents, such as low latency or high throughput, into concrete control actions in telecommunication networks, a task beyond existing heuristic methods. The proposed solution is an Agentic AI system comprising three specialized agents: a supervisory interpreter that uses language models to parse and refine intents into optimization templates, an optimizer that formulates tractable problems and analyzes trade-offs, and a preference-driven controller based on multi-objective reinforcement learning to operate near the Pareto frontier. Experimental results demonstrate that this system enables networks to autonomously interpret, adapt to, and act upon diverse intents and dynamic conditions in a scalable manner, effectively bridging the gap from intents to actions.</div>
<div class="mono" style="margin-top:8px">本文针对电信网络中如何将高层服务意图（如低延迟或高吞吐量）自主转化为具体控制动作的挑战，现有启发式方法难以解决这一问题。提出的解决方案是一个智能体AI系统，包含三个专门代理：一个使用语言模型进行解析和精炼意图为优化模板的监督解释器，一个构建可处理优化问题并分析权衡的优化器，以及一个基于多目标强化学习的偏好驱动控制器，以在帕累托前沿附近运行。实验结果表明，该系统使网络能够以可扩展的方式自主解释、适应并响应多样化的意图和动态条件，有效实现了从意图到行动的转化。</div>
</details>
</div>
<div class="card">
<div class="title">Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics</div>
<div class="meta-line">Authors: Boxuan Zhang, Weipu Zhang, Zhaohan Feng, Wei Xiao, Jian Sun, Jie Chen, Gang Wang</div>
<div class="meta-line">First: 2026-02-01T15:00:30+00:00 · Latest: 2026-02-01T15:00:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01270v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01270v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界模型混合：通过模块化潜在动态扩展多任务强化学习</div>
<div class="mono" style="margin-top:8px">多任务强化学习（MTRL）在视觉领域面临的核心挑战是，当任务在观测和动态特性上存在显著异质性时，如何实现样本高效性。基于模型的强化学习通过世界模型为提高样本效率提供了可行路径，但传统的单一架构难以捕捉多样化的任务动态，导致重建和预测精度不佳。我们提出了世界模型混合（MoW），这是一种可扩展的架构，它结合了用于任务自适应视觉压缩的模块化变分自编码器、基于混合Transformer的动态模型（包含任务条件专家模块和共享主干网络），以及基于梯度的任务聚类策略以实现高效参数分配。在Atari 100k基准测试中，单个MoW智能体在26款Atari游戏上训练一次后，平均人类标准化得分达到110.4%，与由26个任务专用模型组成的集成方法STORM的114.2%得分相当，同时参数数量减少50%。在Meta-World环境中，MoW在30万环境步数内实现了74.5%的平均成功率，创造了新的技术标杆。这些结果表明，MoW为通用世界模型提供了可扩展且参数高效的基础框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of sample efficiency in multi-task reinforcement learning with diverse visual observations and dynamics by proposing Mixture-of-World Models (MoW), which integrates modular variational autoencoders for task-adaptive compression, a hybrid Transformer dynamics model with task-conditioned experts, and a gradient-based clustering strategy for parameter allocation. Experimentally, on the Atari 100k benchmark, a single MoW agent trained on 26 games achieved a mean human-normalized score of 110.4%, rivaling an ensemble of task-specific models while using 50% fewer parameters, and on Meta-World, it set a new state-of-the-art with a 74.5% average success rate within 300k steps, demonstrating scalable and parameter-efficient performance.</div>
<div class="mono" style="margin-top:8px">本文针对多任务强化学习中视觉观测与动态多样导致的样本效率问题，提出了混合世界模型（MoW），该方法结合了用于任务自适应压缩的模块化变分自编码器、具有任务条件专家和共享骨干的混合Transformer动态模型，以及基于梯度的任务聚类策略以实现参数高效分配。实验结果表明，在Atari 100k基准测试中，单个MoW代理在26款游戏上训练后取得了110.4%的平均人类标准化分数，与任务特定模型集成性能相当且参数减少50%；在Meta-World环境中，它在30万步内实现了74.5%的平均成功率，创造了新的技术标杆，证明了其可扩展性和参数效率。</div>
</details>
</div>
<div class="card">
<div class="title">Sample Efficient Active Algorithms for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Soumyadeep Roy, Shashwat Kushwaha, Ambedkar Dukkipati</div>
<div class="meta-line">First: 2026-02-01T14:38:07+00:00 · Latest: 2026-02-01T14:38:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01260v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01260v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线强化学习中样本高效主动算法研究</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）支持从静态数据中学习策略，但常面临状态-动作空间覆盖不足和分布偏移问题。通过允许有限的在线交互来选择性优化学习价值函数的不确定区域（即主动强化学习，ActiveRL），可有效缓解该问题。尽管已有实证成功案例，但文献中缺乏理论分析。本研究通过高斯过程（GP）不确定性建模视角，对ActiveRL进行严格的样本复杂度分析以填补这一空白。我们提出一种算法，结合GP集中不等式和信息增益边界，推导出高概率保证：仅需${\mathcal{O}}(1/ε^2)$次主动状态转移即可学习$ε$最优策略，优于纯离线方法的$Ω(1/ε^2(1-γ)^4)$收敛速率。结果表明，ActiveRL实现了近乎最优的信息效率——基于不确定性的引导机制能以最少在线数据加速价值函数收敛。该分析融合了GP集中不等式与信息增益边界，搭建了贝叶斯非参数回归与强化学习理论的桥梁。我们通过多组实验验证了算法与理论结论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of offline reinforcement learning, such as poor state-action space coverage and distributional shift, by introducing Active Reinforcement Learning (ActiveRL), which uses limited online interactions to refine uncertain regions of the value function. The method employs Gaussian Process uncertainty modeling to guide data collection, with theoretical analysis based on GP concentration inequalities and information-gain bounds, proving that an ε-optimal policy can be learned with O(1/ε²) active transitions, improving over purely offline methods. Experimental results validate the algorithm&#x27;s efficiency, showing that uncertainty-driven active learning accelerates value-function convergence with minimal online data, bridging Bayesian nonparametric regression and reinforcement learning theories.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中状态-动作空间覆盖不足和分布偏移等问题，提出了主动强化学习方法，通过有限的在线交互来优化价值函数的不确定区域。该方法采用高斯过程不确定性建模指导数据收集，基于高斯过程集中不等式和信息增益界限进行理论分析，证明能以O(1/ε²)的主动转移学习到ε-最优策略，优于纯离线方法。实验结果验证了算法的有效性，表明不确定性驱动的主动学习能以最少的在线数据加速价值函数收敛，连接了贝叶斯非参数回归与强化学习理论。</div>
</details>
</div>
<div class="card">
<div class="title">AI-Driven Low-Altitude Economy: Spectrum, Mobility, and Validation</div>
<div class="meta-line">Authors: Kürşat Tekbıyık, Amir Hossein Fahim Raouf, İsmail Güvenç, Mingzhe Chen, Güneş Karabulut Kurt, Antoine Lesage-Landry</div>
<div class="meta-line">First: 2025-06-02T07:12:44+00:00 · Latest: 2026-02-01T12:12:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.01378v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.01378v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Low Altitude Economy (LAE) network, with its transformative capabilities, is a candidate to become one of the major technological developments of the next decade for air mobility. However, the expected unprecedented density, mobility, and heterogeneity pose challenges and require new approaches, as it renders traditional rule-based approaches inadequate. To address these challenges, this study introduces artificial intelligence (AI)-based approaches and validation frameworks for transitioning AI-enabled technologies from simulation-based studies to practical and deployable systems. This study discusses essential enablers for intelligent LAE networks. First, AI-based spectrum sensing and coexistence utilizing the distributed nature of LAE nodes is introduced. Then, joint resource allocation and trajectory optimization driven by reinforcement learning is discussed. Bridging the gap between simulation and deployment through experimental platforms such as Aerial Experiments and Research Platform for Advanced Wireless (AERPAW), which are critical for validating models under realistic and non-stationary airspace conditions, is also addressed. The study concludes by highlighting open issues and outlining a forward-looking roadmap for the development of efficient, interoperable, and scalable AI-driven LAE ecosystems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人工智能驱动的低空经济：频谱、移动性与验证</div>
<div class="mono" style="margin-top:8px">低空经济网络凭借其变革性能力，有望成为未来十年航空移动领域的主要技术发展方向之一。然而，其预期达到的前所未有的密度、移动性和异构性带来了挑战，需要新的解决方案，因为传统的基于规则的方法已显不足。为应对这些挑战，本研究引入了基于人工智能的方法和验证框架，以推动人工智能技术从基于仿真的研究向实用化、可部署系统过渡。本研究探讨了智能低空经济网络的关键使能要素：首先介绍了利用低空节点分布式特性的人工智能频谱感知与共存技术；随后讨论了基于强化学习的联合资源分配与轨迹优化方法；同时阐述了通过实验平台（如AERPAW空中先进无线实验研究平台）弥合仿真与部署之间差距的重要性，这类平台对在真实非稳态空域条件下验证模型至关重要。研究最后强调了待解决的开放性问题，并展望了构建高效、互操作、可扩展的人工智能驱动低空经济生态系统的发展路线图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inadequacy of traditional rule-based methods for the high-density, mobile, and heterogeneous Low Altitude Economy (LAE) networks, this study proposes AI-driven solutions and validation frameworks to bridge the gap from simulation to real-world deployment. The method introduces AI-based spectrum sensing and coexistence leveraging distributed LAE nodes, along with reinforcement learning for joint resource allocation and trajectory optimization, validated through experimental platforms like AERPAW under realistic conditions. The main experimental results demonstrate the feasibility of these approaches for creating efficient, interoperable, and scalable LAE ecosystems, while also highlighting open challenges and a forward-looking roadmap.</div>
<div class="mono" style="margin-top:8px">本研究针对传统规则方法难以应对低空经济网络的高密度、高移动性和异构性挑战，提出了人工智能驱动的解决方案和验证框架，旨在弥合仿真与实际部署之间的差距。方法上引入了基于人工智能的分布式频谱感知与共存技术，以及利用强化学习进行联合资源分配和轨迹优化，并通过AERPAW等实验平台在真实非平稳空域条件下进行验证。主要实验结果验证了这些方法对于构建高效、互操作和可扩展的低空经济生态系统的可行性，同时指出了开放性问题并概述了未来发展路线图。</div>
</details>
</div>
<div class="card">
<div class="title">PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning</div>
<div class="meta-line">Authors: Shunpeng Yang, Ben Liu, Hua Chen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-01T11:08:09+00:00 · Latest: 2026-02-01T11:08:09+00:00</div>
<div class="meta-line">Comments: Submitted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01156v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01156v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#x27;s ability to capture richer multimodal action distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PolicyFlow：强化学习中基于连续归一化流的策略优化方法</div>
<div class="mono" style="margin-top:8px">在在线策略强化学习算法中，近端策略优化（PPO）因其简洁性、数值稳定性和强大的实证性能而广受青睐。标准PPO依赖于通过重要性比率定义的替代目标，这需要评估策略似然——当策略建模为高斯分布时通常较为直接。然而，将PPO扩展到更具表达能力的高容量策略模型（如连续归一化流，亦称流匹配模型）具有挑战性，因为沿完整流轨迹的似然评估计算成本高昂且常数值不稳定。为解决此问题，我们提出PolicyFlow，一种新颖的基于在线策略CNF的强化学习算法，它将表达能力强的CNF策略与PPO式目标相结合，无需沿完整流路径进行似然评估。PolicyFlow通过沿简单插值路径的速度场变化来近似重要性比率，在保持训练稳定性的同时降低了计算开销。为进一步防止模式崩溃并促进行为多样性，我们提出布朗正则化器——一种受布朗运动启发的隐式策略熵正则化器，其概念优雅且计算轻量。在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多种环境任务上的实验表明，与使用高斯策略的PPO及基于流的基线方法（如FPO和DPPO）相比，PolicyFlow取得了竞争性或更优的性能。值得注意的是，在MultiGoal任务上的结果突显了PolicyFlow捕捉更丰富多峰动作分布的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for PolicyFlow arises from the challenge of integrating expressive continuous normalizing flow (CNF) policies with Proximal Policy Optimization (PPO), as standard PPO&#x27;s reliance on policy likelihood evaluation becomes computationally expensive and unstable for CNFs. The method introduces PolicyFlow, an on-policy algorithm that approximates importance ratios using velocity field variations along a simple interpolation path, avoiding full flow trajectory likelihood evaluation, and incorporates a Brownian Regularizer to prevent mode collapse by encouraging diverse behaviors. Experimental results across MultiGoal, PointMaze, IsaacLab, and MuJoCo Playground environments demonstrate that PolicyFlow achieves competitive or superior performance compared to PPO with Gaussian policies and other flow-based baselines, notably capturing richer multimodal action distributions in tasks like MultiGoal.</div>
<div class="mono" style="margin-top:8px">PolicyFlow的提出动机源于将表达能力强的连续归一化流（CNF）策略与近端策略优化（PPO）结合的挑战，因为标准PPO依赖策略似然评估，这在CNF中计算成本高且不稳定。该方法引入了PolicyFlow这一在线策略算法，通过沿简单插值路径的流速场变化来近似重要性比率，避免了完整流轨迹的似然评估，并采用布朗正则化器来防止模式崩溃，以鼓励行为多样性。在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多种环境中的实验结果表明，PolicyFlow相比使用高斯策略的PPO及其他基于流的基线方法，取得了竞争性或更优的性能，尤其在MultiGoal任务中展现了捕捉更丰富多模态动作分布的能力。</div>
</details>
</div>
<div class="card">
<div class="title">UMM-RM: An Upcycle-and-Merge MoE Reward Model for Mitigating Reward Hacking</div>
<div class="meta-line">Authors: Lingling Fu, Yongfu Xue</div>
<div class="meta-line">First: 2025-11-30T04:36:37+00:00 · Latest: 2026-02-01T10:49:57+00:00</div>
<div class="meta-line">Comments: 8 pages,14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00724v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00724v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are a critical component of reinforcement learning from human feedback (RLHF). However, conventional dense RMs are susceptible to exploitation by policy models through biases or spurious correlations, resulting in reward hacking: RM scores increase during training while alignment with human preferences deteriorates, a problem that is further exacerbated under distribution shift.To address this issue, we propose UMM-RM (Upcycle-and-Merge MoE Reward Model). UMM-RM first upscales the feed-forward layers of a dense backbone into a mixture-of-experts (MoE) reward model with shared experts. The shared experts are always activated to capture instruction-agnostic preference signals, while the remaining experts model fine-grained preferences across instructions or task regimes. After training, the experts are consolidated into a single dense RM via learnable merging weights.This design retains the robustness and exploitation resistance provided by expert diversity while avoiding the inference overhead of MoE architectures or explicit ensembles. Experiments across multiple base models and preference datasets show that, compared with standard dense RMs, UMM-RM improves accuracy on preference data, reduces reward hacking during PPO training, and yields more stable preference alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UMM-RM：一种用于缓解奖励攻击的升级合并专家混合奖励模型</div>
<div class="mono" style="margin-top:8px">奖励模型（RMs）是基于人类反馈的强化学习（RLHF）的关键组成部分。然而，传统的密集奖励模型容易受到策略模型通过偏见或虚假相关性的利用，导致奖励攻击：在训练过程中RM分数上升，而与人类偏好的一致性却下降，这一问题在分布偏移下进一步加剧。为解决此问题，我们提出了UMM-RM（升级合并专家混合奖励模型）。UMM-RM首先将密集骨干网络的前馈层升级为具有共享专家的专家混合（MoE）奖励模型。共享专家始终被激活以捕获与指令无关的偏好信号，而其余专家则建模跨指令或任务领域的细粒度偏好。训练后，通过可学习的合并权重将专家整合为单个密集RM。该设计保留了专家多样性提供的鲁棒性和抗攻击性，同时避免了MoE架构或显式集成带来的推理开销。在多个基础模型和偏好数据集上的实验表明，与标准密集RM相比，UMM-RM提高了偏好数据的准确性，减少了PPO训练期间的奖励攻击，并实现了更稳定的偏好对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of reward hacking in reinforcement learning from human feedback, where conventional dense reward models are exploited by policy models, leading to deteriorating alignment despite increasing scores. The proposed method, UMM-RM, transforms a dense backbone into a mixture-of-experts model with shared experts to capture instruction-agnostic preferences and task-specific signals, then merges experts into a single dense model to maintain robustness without inference overhead. Experimental results across multiple models and datasets demonstrate that UMM-RM improves preference accuracy, reduces reward hacking during PPO training, and enhances alignment stability compared to standard dense reward models.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习中奖励模型被策略模型利用导致奖励黑客攻击的问题，即分数上升但人类偏好对齐性下降。提出的UMM-RM方法将稠密骨干网络升级为混合专家模型，其中共享专家捕获指令无关的偏好信号，其余专家建模细粒度任务偏好，训练后通过可学习的合并权重将专家整合为单一稠密模型以保持鲁棒性且避免推理开销。在多个基础模型和偏好数据集上的实验表明，相比标准稠密奖励模型，UMM-RM提高了偏好数据准确性，减少了PPO训练中的奖励黑客现象，并实现了更稳定的偏好对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Unleashing the Potential of Differential Evolution through Individual-Level Strategy Diversity</div>
<div class="meta-line">Authors: Chenchen Feng, Minyang Chen, Zhuozhao Li, Ran Cheng</div>
<div class="meta-line">First: 2026-02-01T10:45:44+00:00 · Latest: 2026-02-01T10:45:44+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TEVC</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01147v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01147v1">PDF</a> · <a href="https://github.com/EMI-Group/istratde">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since Differential Evolution (DE) is sensitive to strategy choice, most existing variants pursue performance through adaptive mechanisms or intricate designs. While these approaches focus on adjusting strategies over time, the structural benefits that static strategy diversity may bring remain largely unexplored. To bridge this gap, we study the impact of individual-level strategy diversity on DE&#x27;s search dynamics and performance, and introduce iStratDE (DE with individual-level strategies), a minimalist variant that assigns mutation and crossover strategies independently to each individual at initialization and keeps them fixed throughout the evolutionary process. By injecting diversity at the individual level without adaptation or feedback, iStratDE cultivates persistent behavioral heterogeneity that is especially effective with large populations. Moreover, its communication-free construction possesses intrinsic concurrency, thereby enabling efficient parallel execution and straightforward scaling for GPU computing. We further provide a convergence analysis of iStratDE under standard reachability assumptions, which establishes the almost-sure convergence of the best-so-far fitness. Extensive experiments on the CEC2022 benchmark suite and robotic control tasks demonstrate that iStratDE matches or surpasses established adaptive DE variants. These results highlight individual-level strategy assignment as a straightforward yet effective mechanism for enhancing DE&#x27;s performance. The source code of iStratDE is publicly accessible at: https://github.com/EMI-Group/istratde.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过个体级策略多样性释放差分进化的潜力</div>
<div class="mono" style="margin-top:8px">由于差分进化（DE）对策略选择敏感，现有变体大多通过自适应机制或复杂设计来提升性能。这些方法侧重于随时间调整策略，而静态策略多样性可能带来的结构性优势尚未得到充分探索。为填补这一空白，本研究探讨了个体级策略多样性对DE搜索动态与性能的影响，并提出iStratDE（个体级策略差分进化）——一种极简变体，在初始化时为每个个体独立分配变异与交叉策略，并在进化过程中保持固定。通过无需自适应或反馈的个体级多样性注入，iStratDE培育出持久的行为异质性，在大规模种群中尤为有效。其无通信架构具有内在并发性，可实现高效并行执行及对GPU计算的直接扩展。我们在标准可达性假设下进一步给出iStratDE的收敛性分析，证明当前最优适应度的几乎必然收敛性。基于CEC2022基准测试集与机器人控制任务的实验表明，iStratDE达到或超越了现有自适应DE变体的性能。这些结果揭示了个体级策略分配作为一种简洁有效的DE性能增强机制。iStratDE源代码已公开于：https://github.com/EMI-Group/istratde。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sensitivity of Differential Evolution (DE) to strategy choice and the underexplored potential of static strategy diversity, this paper introduces iStratDE, a minimalist variant that assigns fixed mutation and crossover strategies to each individual at initialization to foster persistent behavioral heterogeneity. The method employs individual-level strategy diversity without adaptation, enabling communication-free, intrinsically concurrent execution suitable for parallel and GPU computing, and is supported by a convergence analysis under standard assumptions. Experimental results on the CEC2022 benchmark and robotic control tasks show that iStratDE matches or outperforms established adaptive DE variants, highlighting individual-level strategy assignment as a straightforward and effective performance enhancer.</div>
<div class="mono" style="margin-top:8px">本文针对差分进化算法对策略选择敏感且静态策略多样性潜力未充分探索的问题，提出了iStratDE这一简约变体，通过在初始化时为每个个体分配固定的变异和交叉策略，以培养持久的行为异质性。该方法采用个体级策略多样性，无需自适应机制，实现了无通信、内在并行的执行，适合并行和GPU计算，并在标准假设下提供了收敛性分析。在CEC2022基准测试和机器人控制任务上的实验结果表明，iStratDE匹配或超越了现有自适应差分进化变体，凸显了个体级策略分配作为一种简单有效的性能提升机制。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Generative Adversarial Fine-Tuning for Large Language Models</div>
<div class="meta-line">Authors: Shiguang Wu, Yaqing Wang, Quanming Yao</div>
<div class="meta-line">First: 2026-02-01T10:20:27+00:00 · Latest: 2026-02-01T10:20:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01137v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型的自生成对抗微调</div>
<div class="mono" style="margin-top:8px">大语言模型的对齐微调通常依赖于监督微调或基于人类反馈的强化学习，两者均受限于高质量标注的高成本和稀缺性。近期的自博弈与合成数据方法虽降低了这种依赖，但常基于启发式假设或无依据的自我评估，可能导致偏差累积和性能漂移。本文提出自生成对抗大语言模型，这是一种统一的微调框架，将对齐问题建模为单一模型内的生成对抗博弈。该模型无需外部奖励模型，即可协同进化生成与判别能力。理论与实证结果表明，该方法实现了最先进的性能，既可作为有效的对齐算法，也能作为鲁棒的合成数据生成引擎。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the high cost and scarcity of human annotations for aligning large language models by proposing Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single model, eliminating the need for external reward models. The method jointly evolves generation and discrimination capabilities through self-play, avoiding heuristic assumptions and ungrounded self-evaluation to mitigate bias accumulation and performance drift. Experimental results show that SGALM achieves state-of-the-art performance, serving as both an effective alignment algorithm and a robust synthetic data engine.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型对齐中高质量人工标注成本高且稀缺的问题，提出了自生成对抗大语言模型（SGALM），这是一个统一的微调框架，将对齐任务构建为单个模型内的生成对抗博弈，无需外部奖励模型。该方法通过自我博弈联合演化生成与判别能力，避免了启发式假设和无根据的自我评估，从而减少偏差累积和性能漂移。实验结果表明，SGALM实现了最先进的性能，既可作为一种有效的对齐算法，也能作为稳健的合成数据生成引擎。</div>
</details>
</div>
<div class="card">
<div class="title">Meta-Learning Reinforcement Learning for Crypto-Return Prediction</div>
<div class="meta-line">Authors: Junqiao Wang, Zhaoyang Guan, Guanyu Liu, Tianze Xia, Xianzhi Li, Shuo Yin, Xinyuan Song, Chuhan Cheng, Tianyu Shi, Alex Lee</div>
<div class="meta-line">First: 2025-09-11T14:20:45+00:00 · Latest: 2026-02-01T10:15:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09751v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.09751v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Predicting cryptocurrency returns is notoriously difficult: price movements are driven by a fast-shifting blend of on-chain activity, news flow, and social sentiment, while labeled training data are scarce and expensive. In this paper, we present Meta-RL-Crypto, a unified transformer-based architecture that unifies meta-learning and reinforcement learning (RL) to create a fully self-improving trading agent. Starting from a vanilla instruction-tuned LLM, the agent iteratively alternates between three roles-actor, judge, and meta-judge-in a closed-loop architecture. This learning process requires no additional human supervision. It can leverage multimodal market inputs and internal preference feedback. The agent in the system continuously refines both the trading policy and evaluation criteria. Experiments across diverse market regimes demonstrate that Meta-RL-Crypto shows good performance on the technical indicators of the real market and outperforming other LLM-based baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于元学习强化学习的加密货币收益预测</div>
<div class="mono" style="margin-top:8px">预测加密货币收益极为困难：价格波动由快速变化的链上活动、新闻流和社交情绪共同驱动，且标注训练数据稀缺且成本高昂。本文提出Meta-RL-Crypto，一种基于Transformer的统一架构，融合元学习与强化学习，构建了完全自优化的交易智能体。该智能体以基础指令调优的大语言模型为起点，在闭环架构中迭代切换三种角色——执行者、评判者和元评判者。此学习过程无需额外人工监督，可融合多模态市场输入与内部偏好反馈，持续优化交易策略与评估标准。多样化市场环境下的实验表明，Meta-RL-Crypto在真实市场技术指标上表现良好，并优于其他基于大语言模型的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of predicting cryptocurrency returns, which is difficult due to volatile market drivers like on-chain data, news, and sentiment, coupled with scarce labeled data. The method introduces Meta-RL-Crypto, a transformer-based architecture that combines meta-learning and reinforcement learning to create a self-improving trading agent; this agent iteratively cycles through actor, judge, and meta-judge roles in a closed loop, using multimodal inputs without human supervision. Experimental results show that the agent performs well on real-market technical indicators and surpasses other LLM-based baselines across various market conditions.</div>
<div class="mono" style="margin-top:8px">本文针对加密货币回报预测的挑战展开研究，该挑战因链上数据、新闻和情绪等市场驱动因素波动剧烈以及标注数据稀缺而难以解决。方法上提出了Meta-RL-Crypto，一种基于Transformer的架构，结合元学习和强化学习来创建自我改进的交易代理；该代理在闭环中迭代循环执行行动者、评判者和元评判者角色，利用多模态输入且无需人工监督。实验结果表明，该代理在真实市场的技术指标上表现良好，并在不同市场环境下优于其他基于大语言模型的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Parallel Training in Spiking Neural Networks</div>
<div class="meta-line">Authors: Yanbin Huang, Man Yao, Yuqi Pan, Changze Lv, Siyuan Xu, Xiaoqing Zheng, Bo Xu, Guoqi Li</div>
<div class="meta-line">First: 2026-02-01T10:10:47+00:00 · Latest: 2026-02-01T10:10:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The bio-inspired integrate-fire-reset mechanism of spiking neurons constitutes the foundation for efficient processing in Spiking Neural Networks (SNNs). Recent progress in large models demands that spiking neurons support highly parallel computation to scale efficiently on modern GPUs. This work proposes a novel functional perspective that provides general guidance for designing parallel spiking neurons. We argue that the reset mechanism, which induces complex temporal dependencies and hinders parallel training, should be removed. However, any such modification should satisfy two principles: 1) preserving the functions of reset as a core biological mechanism; and 2) enabling parallel training without sacrificing the serial inference ability of spiking neurons, which underpins their efficiency at test time. To this end, we identify the functions of the reset and analyze how to reconcile parallel training with serial inference, upon which we propose a dynamic decay spiking neuron. We conduct comprehensive testing of our method in terms of: 1) Training efficiency and extrapolation capability. On 16k-length sequences, we achieve a 25.6x training speedup over the pioneering parallel spiking neuron, and our models trained on 2k-length can stably perform inference on sequences as long as 30k. 2) Generality. We demonstrate the consistent effectiveness of the proposed method across five task categories (image classification, neuromorphic event processing, time-series forecasting, language modeling, and reinforcement learning), three network architectures (spiking CNN/Transformer/SSMs), and two spike activation modes (spike/integer activation). 3) Energy consumption. The spiking firing of our neuron is lower than that of vanilla and existing parallel spiking neurons.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>脉冲神经网络的并行训练</div>
<div class="mono" style="margin-top:8px">脉冲神经元的仿生整合-发放-重置机制构成了脉冲神经网络高效处理的基础。大模型的近期进展要求脉冲神经元支持高度并行计算，以在现代GPU上高效扩展。本研究提出了一种新颖的功能视角，为设计并行脉冲神经元提供通用指导。我们认为，重置机制会引入复杂的时间依赖性并阻碍并行训练，应当被移除。然而，任何此类修改需满足两个原则：1）保留重置作为核心生物机制的功能；2）在不牺牲脉冲神经元串行推理能力的前提下实现并行训练，该能力是其测试时效率的基石。为此，我们识别了重置的功能，分析了如何协调并行训练与串行推理，并在此基础上提出了一种动态衰减脉冲神经元。我们从以下方面对方法进行了全面测试：1）训练效率与外推能力。在16k长度序列上，相比开创性并行脉冲神经元，我们实现了25.6倍的训练加速，且基于2k长度训练的模型能稳定执行长达30k序列的推理。2）通用性。我们在五类任务（图像分类、神经形态事件处理、时间序列预测、语言建模和强化学习）、三种网络架构（脉冲CNN/Transformer/SSMs）及两种脉冲激活模式（脉冲/整数激活）中验证了方法的持续有效性。3）能耗。我们神经元的脉冲发放率低于原始及现有并行脉冲神经元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for Spiking Neural Networks (SNNs) to scale efficiently on modern GPUs through parallel training, this paper addresses the hindrance posed by the traditional reset mechanism in spiking neurons, which creates complex temporal dependencies. The method proposes a functional perspective to guide the design of parallel spiking neurons, arguing for the removal of the reset while preserving its biological functions and maintaining serial inference efficiency; this leads to the introduction of a dynamic decay spiking neuron. Experimental results demonstrate significant improvements: a 25.6x training speedup on long sequences, stable inference on sequences up to 30k length, consistent effectiveness across diverse tasks and architectures, and reduced energy consumption compared to existing neurons.</div>
<div class="mono" style="margin-top:8px">本文的动机是使脉冲神经网络（SNN）能够在现代GPU上通过并行训练高效扩展，而传统脉冲神经元中的复位机制因引入复杂时间依赖性阻碍了这一点。方法上，提出了一种功能视角来指导并行脉冲神经元的设计，主张在保留复位机制生物学功能并维持串行推理效率的前提下移除它，从而引入了动态衰减脉冲神经元。实验结果表明，该方法在长序列上实现了25.6倍的训练加速，能在长达30k的序列上稳定推理，在多种任务和架构中均表现一致有效，且能耗低于现有脉冲神经元。</div>
</details>
</div>
<div class="card">
<div class="title">Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach</div>
<div class="meta-line">Authors: Yue Zhong, Jiawen Kang, Yongju Tong, Hong-Ning Dai, Dong In Kim, Abbas Jamalipour, Shengli Xie</div>
<div class="meta-line">First: 2026-02-01T10:01:07+00:00 · Latest: 2026-02-01T10:01:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01131v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向低空经济的李雅普诺夫稳定性感知斯塔克尔伯格博弈：一种基于控制导向剪枝的深度强化学习方法</div>
<div class="mono" style="margin-top:8px">随着低空经济的快速发展，无人机作为关键空中基站，需为用户提供从时延敏感关键任务到带宽密集型数据流的多类服务。然而，这类异构网络的效能常因机载资源有限与严格稳定性需求间的冲突而受限。本文突破传统以吞吐量为核心的设计思路，提出一种感知-通信-计算-控制闭环框架，显式建模通信时延对物理控制稳定性的影响。为保障任务可靠性，我们利用李雅普诺夫稳定性理论，推导控制系统状态演化与通信约束间的内在映射，将抽象稳定性需求转化为可量化的资源边界。进而将资源分配问题建模为斯塔克尔伯格博弈：无人机作为领导者动态定价资源以平衡负载并确保稳定性，用户作为跟随者则依据服务紧迫性优化请求。针对标准深度强化学习在能量受限边缘平台上计算开销过高的问题，本文提出一种新型轻量级剪枝近端策略优化算法。通过集成动态结构化剪枝机制，该算法在训练过程中显著压缩神经网络规模，使无人机能以最低推理时延快速逼近博弈均衡。仿真结果表明，所提方案在动态低空环境中能有效保障控制回路稳定性，同时最大化系统效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the conflict between limited UAV resources and stringent stability requirements in the low-altitude economy, this paper proposes a Sensing-Communication-Computing-Control framework that uses Lyapunov stability theory to map control system stability to quantifiable communication constraints. The method formulates resource allocation as a Stackelberg game between UAVs and users and introduces a lightweight, pruning-based Proximal Policy Optimization algorithm to reduce computational overhead for efficient equilibrium approximation. Experimental simulations show the scheme successfully ensures control loop stability while maximizing system utility in dynamic environments.</div>
<div class="mono" style="margin-top:8px">本文针对低空经济中无人机有限资源与严格稳定性要求之间的冲突，提出了一个感知-通信-计算-控制闭环框架，利用李雅普诺夫稳定性理论将控制系统稳定性映射为可量化的通信约束。该方法将资源分配建模为无人机与用户间的斯塔克尔伯格博弈，并引入一种轻量级的、基于剪枝的近端策略优化算法以降低计算开销，从而高效逼近博弈均衡。仿真结果表明，该方案在动态低空环境中能有效保障控制回路稳定性，同时最大化系统效用。</div>
</details>
</div>
<div class="card">
<div class="title">Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions</div>
<div class="meta-line">Authors: Jian-Qiao Zhu, Hanbo Xie, Dilip Arumugam, Robert C. Wilson, Thomas L. Griffiths</div>
<div class="meta-line">First: 2025-05-16T18:22:05+00:00 · Latest: 2026-02-01T09:58:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.11614v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.11614v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall short in offering interpretable explanations of the cognitive processes they capture. In this work, we explore the potential of pretrained large language models (LLMs) to serve as dual-purpose cognitive models--capable of both accurate prediction and interpretable explanation in natural language. Specifically, we employ reinforcement learning with outcome-based rewards to guide LLMs toward generating explicit reasoning traces for explaining human risky choices. Our findings demonstrate that this approach produces high-quality explanations alongside strong quantitative predictions of human decisions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>运用强化学习训练大语言模型解释人类决策</div>
<div class="mono" style="margin-top:8px">认知建模的核心目标是开发不仅能预测人类行为，还能揭示潜在认知机制的模型。虽然基于大规模行为数据训练的神经网络模型通常具有强大的预测性能，但在提供可解释的认知过程说明方面往往不足。本研究探索了预训练大语言模型作为双重用途认知模型的潜力——既能进行准确预测，又能用自然语言提供可解释的说明。具体而言，我们采用基于结果的奖励强化学习，引导大语言模型生成显式推理轨迹来解释人类风险决策。研究结果表明，该方法在实现人类决策强定量预测的同时，还能产生高质量的解释说明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need for cognitive models that not only predict human behavior accurately but also offer interpretable insights into the underlying cognitive processes, a capability often lacking in traditional neural network models. The method involves using reinforcement learning with outcome-based rewards to fine-tune pretrained large language models, guiding them to generate explicit natural language reasoning traces that explain human risky choices. The main experimental results show that this approach yields both high-quality, interpretable explanations and strong quantitative predictions of human decisions, positioning LLMs as effective dual-purpose cognitive models.</div>
<div class="mono" style="margin-top:8px">这项研究的动机在于开发既能准确预测人类行为又能提供对潜在认知机制可解释见解的认知模型，而传统神经网络模型通常缺乏后一能力。方法上，该研究采用基于结果的奖励进行强化学习，对预训练的大语言模型进行微调，引导其生成解释人类风险选择的显式自然语言推理轨迹。主要实验结果表明，该方法不仅能产生高质量、可解释的说明，还能对人类决策做出强有力的定量预测，从而将大语言模型定位为有效的双用途认知模型。</div>
</details>
</div>
<div class="card">
<div class="title">Anchored Supervised Fine-Tuning</div>
<div class="meta-line">Authors: He Zhu, Junyou Su, Peng Lai, Ren Ma, Wenjia Zhang, Linyi Yang, Guanhua Chen</div>
<div class="meta-line">First: 2025-09-28T08:58:12+00:00 · Latest: 2026-02-01T09:44:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23753v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23753v3">PDF</a> · <a href="https://github.com/zhuchichi56/ASFT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training of large language models involves a fundamental trade-off between supervised fine-tuning (SFT), which efficiently mimics demonstrations but tends to memorize, and reinforcement learning (RL), which achieves better generalization at higher computational cost. Dynamic Fine-Tuning (DFT) recently emerged as a promising middle ground, reweighting SFT objectives with token probabilities and achieving improvements in certain reasoning domains, though it exhibits instability in other tasks. We provide a analysis of DFT through the reward-weighted regression (RWR) framework, revealing that it corresponds to a specific auxiliary distribution choice that yields provably tighter RL bounds than standard SFT. However, our analysis also uncovers a critical limitation: this construction lacks distributional anchoring, leading to progressive drift that undermines training stability. To address this, we propose Anchored Supervised Fine-Tuning (ASFT), which augments DFT&#x27;s reweighting with lightweight KL regularization to preserve tightness while ensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT across mathematical reasoning, medical knowledge grounding, and code generation, achieving substantial improvements with minimal computational overhead. Our RWR framework provides a systematic lens for understanding post-training methods and demonstrates that principled theoretical analysis leads to both stronger guarantees and practical gains. The code is available at https://github.com/zhuchichi56/ASFT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>锚定监督微调</div>
<div class="mono" style="margin-top:8px">大语言模型的后训练面临监督微调（SFT）与强化学习（RL）之间的根本权衡：SFT能高效模仿演示但易导致记忆化，RL虽泛化能力更强但计算成本更高。动态微调（DFT）作为折中方案，通过词元概率重加权SFT目标，在部分推理领域取得改进，但在其他任务中表现不稳定。我们通过奖励加权回归（RWR）框架分析DFT，揭示其对应特定辅助分布选择，可证明比标准SFT获得更紧的RL边界。但分析同时发现关键缺陷：该构造缺乏分布锚定，导致渐进性漂移破坏训练稳定性。为此，我们提出锚定监督微调（ASFT），通过轻量KL正则化增强DFT的重加权机制，在保持边界紧致性的同时确保稳定性。实验表明，ASFT在数学推理、医学知识落地和代码生成任务中均稳定优于SFT与DFT，以极小计算开销实现显著提升。RWR框架为理解后训练方法提供系统性视角，证明理论分析既能带来更强理论保证，也能产生实际性能增益。代码已开源：https://github.com/zhuchichi56/ASFT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the trade-off in post-training large language models between supervised fine-tuning (SFT), which can over-memorize demonstrations, and reinforcement learning (RL), which generalizes better but is computationally expensive. Motivated by the instability of Dynamic Fine-Tuning (DFT), a middle-ground approach, the authors analyze DFT through a reward-weighted regression framework, revealing it lacks distributional anchoring and leads to drift. They propose Anchored Supervised Fine-Tuning (ASFT), which adds lightweight KL regularization to DFT&#x27;s reweighting to maintain stability while preserving theoretical tightness. Experiments show ASFT consistently outperforms SFT and DFT across mathematical reasoning, medical knowledge grounding, and code generation tasks, achieving significant improvements with minimal computational overhead.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型后训练中监督微调（SFT）易过度记忆演示、而强化学习（RL）泛化更好但计算成本高的权衡问题展开研究。受动态微调（DFT）方法不稳定的启发，作者通过奖励加权回归框架分析DFT，发现其缺乏分布锚定会导致训练漂移。为此，他们提出锚定监督微调（ASFT），在DFT的重新加权基础上加入轻量级KL正则化，以保持稳定性同时不牺牲理论紧致性。实验结果表明，在数学推理、医学知识基础和代码生成任务上，ASFT均一致优于SFT和DFT，以极小的计算开销实现了显著性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization</div>
<div class="meta-line">Authors: Junhao Qiu, Xin Chen, Liang Ge, Liyong Lin, Zhichao Lu, Qingfu Zhang</div>
<div class="meta-line">First: 2026-01-25T16:31:07+00:00 · Latest: 2026-02-01T09:38:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17899v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17899v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neighborhood search operators are critical to the performance of Multi-Objective Evolutionary Algorithms (MOEAs) and rely heavily on expert design. Although recent LLM-based Automated Heuristic Design (AHD) methods have made notable progress, they primarily optimize individual heuristics or components independently, lacking explicit exploration and exploitation of dynamic coupling relationships between operators. In this paper, multi-operator optimization in MOEAs is formulated as a Markov decision process, enabling the improvement of interdependent operators through sequential decision-making. To address this, we propose the Evolution of Operator Combination (E2OC) framework for MOEAs, which achieves the co-evolution of design strategies and executable codes. E2OC employs Monte Carlo Tree Search to progressively search combinations of operator design strategies and adopts an operator rotation mechanism to identify effective operator configurations while supporting the integration of mainstream AHD methods as the underlying designer. Experimental results across AHD tasks with varying objectives and problem scales show that E2OC consistently outperforms state-of-the-art AHD and other multi-heuristic co-design frameworks, demonstrating strong generalization and sustained optimization capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的多目标组合优化中相互依赖算子的协同演化</div>
<div class="mono" style="margin-top:8px">邻域搜索算子的设计对多目标进化算法（MOEAs）的性能至关重要，且高度依赖专家经验。尽管近期基于大语言模型的自动启发式设计（AHD）方法取得显著进展，但这些方法主要独立优化单一启发式或组件，缺乏对算子间动态耦合关系的显式探索与利用。本文将MOEAs中的多算子优化建模为马尔可夫决策过程，通过序列化决策实现相互依赖算子的协同改进。为此，我们提出面向MOEAs的算子组合演化（E2OC）框架，实现设计策略与可执行代码的协同进化。E2OC采用蒙特卡洛树搜索渐进式探索算子设计策略组合，并引入算子轮换机制以识别高效算子配置，同时支持集成主流AHD方法作为底层设计器。在不同目标与问题规模的AHD任务实验中，E2OC均持续优于当前最先进的AHD方法及其他多启发式协同设计框架，展现出强大的泛化能力与持续优化性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing LLM-based Automated Heuristic Design (AHD) methods, which optimize operators independently without exploiting their dynamic interdependencies, this paper introduces the Evolution of Operator Combination (E2OC) framework for Multi-Objective Evolutionary Algorithms (MOEAs). The method formulates multi-operator optimization as a Markov decision process, using Monte Carlo Tree Search to co-evolve design strategies and executable codes through sequential decision-making and an operator rotation mechanism. Experimental results across various AHD tasks demonstrate that E2OC consistently outperforms state-of-the-art AHD and multi-heuristic co-design frameworks, showing strong generalization and sustained optimization capabilities.</div>
<div class="mono" style="margin-top:8px">针对现有基于大语言模型的自动化启发式设计方法独立优化算子、未能利用其动态耦合关系的局限，本文提出了面向多目标进化算法的算子组合演化框架。该方法将多算子优化建模为马尔可夫决策过程，利用蒙特卡洛树搜索通过序贯决策和算子轮换机制，共同演化设计策略与可执行代码。在不同目标和问题规模的自动化启发式设计任务上的实验结果表明，该框架持续优于最先进的自动化启发式设计及其他多启发式协同设计框架，展现出强大的泛化能力和持续优化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Probing RLVR training instability through the lens of objective-level hacking</div>
<div class="meta-line">Authors: Yiming Dong, Kun Fu, Haoyu Li, Xinyuan Zhu, Yurou Liu, Lijing Shao, Jieping Ye, Zheng Wang</div>
<div class="meta-line">First: 2026-02-01T08:55:27+00:00 · Latest: 2026-02-01T08:55:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01103v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01103v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从目标层面破解视角探究RLVR训练不稳定性</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）已被证明能持续提升大语言模型的推理能力，但其训练过程常出现不稳定性，尤其在混合专家（MoE）架构中。训练不稳定性严重制约模型能力提升，但其内在成因与机制尚不明确。本研究提出一个理论框架，通过目标层面破解的视角理解RLVR不稳定性。与源于可被利用验证器的奖励破解不同，目标层面破解产生于令牌级信用错位，并表现为优化目标中的系统级伪信号。基于该框架，结合对300亿参数MoE模型的大量实验，我们追溯了MoE模型中关键病态训练动态——训练-推断差异异常增长现象的起源并形式化其机制，该现象虽广泛关联不稳定性却长期缺乏机理阐释。这些发现为MoE模型不稳定性的训练动态提供了具体因果解释，为设计稳定的RLVR算法提供了指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the training instability in reinforcement learning with verifiable rewards (RLVR), particularly for Mixture-of-Experts (MoE) models, where instability hinders capability improvement. The authors propose a framework attributing this instability to &#x27;objective-level hacking,&#x27; a phenomenon distinct from reward hacking, arising from token-level credit misalignment that creates spurious optimization signals. Through experiments on a 30B MoE model, they trace and formalize the mechanism behind a key pathological dynamic—abnormal growth of the training-inference discrepancy—providing a causal explanation for instability and guidance for designing more stable RLVR algorithms.</div>
<div class="mono" style="margin-top:8px">本文研究了可验证奖励强化学习（RLVR）中的训练不稳定性问题，特别是在混合专家（MoE）模型中，这种不稳定性阻碍了模型能力的提升。作者提出了一个框架，将不稳定性归因于“目标级黑客攻击”，这是一种不同于奖励黑客攻击的现象，源于令牌级信用错位，从而在优化目标中产生虚假信号。通过对一个300亿参数的MoE模型进行实验，他们追踪并形式化了一个关键病理动态背后的机制——训练与推理差异的异常增长，为不稳定性提供了因果解释，并为设计更稳定的RLVR算法提供了指导。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
