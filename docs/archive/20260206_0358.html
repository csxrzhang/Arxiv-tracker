<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 03:58</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0358</div>
    <div class="row"><div class="card">
<div class="title">Reinforced Attention Learning</div>
<div class="meta-line">Authors: Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen, Derek Zhiyuan Cheng</div>
<div class="meta-line">First: 2026-02-04T18:59:52+00:00 · Latest: 2026-02-04T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04884v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化注意力学习</div>
<div class="mono" style="margin-top:8px">通过测试时扩展，基于强化学习（RL）的后训练显著提升了大语言模型（LLM）的推理能力。然而，将该范式通过冗长推理过程扩展到多模态大语言模型（MLLM）时，对感知能力的提升有限，甚至可能导致性能下降。我们提出强化注意力学习（RAL），这是一种直接优化内部注意力分布而非输出词元序列的策略梯度框架。通过将优化目标从“生成什么”转向“关注何处”，RAL促进了复杂多模态输入中的有效信息分配与更优的语义关联。在多样化图像与视频基准测试中的实验表明，该方法相较GRPO及其他基线模型取得了一致性提升。我们进一步提出同策略注意力蒸馏，证明迁移潜在注意力行为比标准知识蒸馏能产生更强的跨模态对齐效果。我们的研究确立了注意力策略作为多模态后训练的一种原理性通用替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limited effectiveness of applying standard reinforcement learning post-training, which optimizes output token sequences, to multimodal large language models (MLLMs), as it often yields minimal perceptual gains or even performance degradation. To address this, the method introduces Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes the model&#x27;s internal attention distributions, thereby shifting the optimization focus from what to generate to where to attend within complex multimodal inputs. Experimental results across various image and video benchmarks demonstrate that RAL consistently outperforms baselines like GRPO, and the proposed On-Policy Attention Distillation further shows that transferring learned attention behaviors enhances cross-modal alignment more effectively than traditional knowledge distillation, establishing attention policies as a principled alternative for multimodal post-training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，将标准的强化学习后训练方法应用于多模态大语言模型时效果有限，该方法通常优化输出词元序列，但难以提升感知能力甚至可能导致性能下降。为此，本文提出了强化注意力学习方法，这是一个直接优化模型内部注意力分布的策略梯度框架，从而将优化重点从生成内容转向对复杂多模态输入的关注位置。在多种图像和视频基准测试上的实验结果表明，该方法持续优于GRPO等基线模型，并且所提出的在线策略注意力蒸馏进一步证明，迁移习得的注意力行为比标准知识蒸馏能更有效地增强跨模态对齐，从而确立了注意力策略作为多模态后训练的一种原则性通用替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Trust Region in LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee</div>
<div class="meta-line">First: 2026-02-04T18:59:04+00:00 · Latest: 2026-02-04T18:59:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04879v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视大语言模型强化学习中的信任区域</div>
<div class="mono" style="margin-top:8px">强化学习已成为微调大语言模型的核心技术，其中近端策略优化算法是实际上的标准算法。尽管应用广泛，我们认为PPO核心的概率比截断机制在结构上并不适合大语言模型固有的巨大词汇量。PPO基于采样词元的概率比约束策略更新，该比值是对真实策略散度的噪声单样本蒙特卡洛估计。这导致了次优的学习动态：低概率词元的更新被过度惩罚，而高概率词元的潜在灾难性偏移却约束不足，从而造成训练效率低下和稳定性问题。为此，我们提出散度近端策略优化算法，用基于策略散度直接估计（如总变差或KL散度）的原则性约束替代启发式截断。为避免巨大内存开销，我们引入高效的二元与Top-K近似方法，以可忽略的开销捕捉核心散度。大量实验评估表明，DPPO相比现有方法实现了更优的训练稳定性和效率，为基于强化学习的大语言模型微调提供了更稳健的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the structural mismatch between Proximal Policy Optimization&#x27;s (PPO) token-level ratio clipping and the large vocabularies of Large Language Models (LLMs), which causes inefficient and unstable training, this paper introduces Divergence Proximal Policy Optimization (DPPO). The method replaces PPO&#x27;s heuristic clipping with a principled constraint based on directly estimated policy divergences like Total Variation or KL, employing efficient Binary and Top-K approximations to manage computational overhead. Experimental results show that DPPO achieves superior training stability and efficiency over existing methods, providing a more robust foundation for RL-based LLM fine-tuning.</div>
<div class="mono" style="margin-top:8px">本文的动机是认识到近端策略优化（PPO）中基于词元概率比的裁剪机制与大型语言模型（LLM）的大词汇量结构不匹配，导致训练效率低下和不稳定。为此，论文提出了散度近端策略优化（DPPO），用基于直接估计的策略散度（如总变差或KL散度）的原则性约束替代了PPO的启发式裁剪，并引入了高效的二值化和Top-K近似以控制计算开销。主要实验结果表明，DPPO相比现有方法实现了更优的训练稳定性和效率，为基于强化学习的LLM微调提供了更稳健的基础。</div>
</details>
</div>
<div class="card">
<div class="title">CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation</div>
<div class="meta-line">Authors: Yannick Denker, Alexander Gepperth</div>
<div class="meta-line">First: 2026-02-04T18:54:26+00:00 · Latest: 2026-02-04T18:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRoSS：面向高任务多样性与真实物理仿真的可扩展强化学习持续机器人仿真套件</div>
<div class="mono" style="margin-top:8px">持续强化学习要求智能体在任务序列中学习且不遗忘已习得的策略。本研究基于Gazebo仿真器中的高拟真机器人，提出一种新型持续强化学习基准套件。该持续机器人仿真套件采用两种机器人平台：配备激光雷达、摄像头及碰撞传感器的两轮差速驱动机器人，以及七关节机械臂。前者用于线跟随与物体推动场景，通过视觉与结构参数变化生成大量差异化任务；后者应用于两种目标抵达场景：基于笛卡尔手部位置的高层控制（延续Continual World基准设计）与基于关节角度的底层控制。针对机械臂基准测试，我们额外提供仅需运动学计算的变体方案（无需传感器读数时可跳过物理仿真），其运行速度可提升两个数量级。本套件具备易扩展性，支持在高度物理真实的机器人环境中开展受控的持续强化学习研究，尤其支持近乎任意的仿真传感器配置。为确保可复现性与易用性，我们提供开箱即用的容器化部署方案（Apptainer），并报告了深度Q网络及策略梯度等标准强化学习算法的性能表现，验证了其作为可扩展、可复现的持续强化学习研究基准的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces CRoSS, a benchmark suite for continual reinforcement learning (CRL) in robotics, motivated by the need for scalable, realistic environments where agents can learn sequences of tasks without forgetting prior knowledge. The method employs two simulated robotic platforms in Gazebo: a differential-drive robot for line-following and object-pushing tasks with varied visual and structural parameters, and a 7-joint robotic arm for goal-reaching tasks under both high-level Cartesian and low-level joint control, including kinematics-only variants for faster computation. Experimental results demonstrate the suite&#x27;s extensibility and physical realism, with containerized setups ensuring reproducibility, and initial performance reports from standard RL algorithms like DQN and policy gradient methods validate its utility as a scalable CRL benchmark.</div>
<div class="mono" style="margin-top:8px">本文提出了CRoSS，一个用于机器人持续强化学习（CRL）的基准测试套件，其动机在于需要可扩展且逼真的环境，使智能体能在学习任务序列时不遗忘先前知识。方法基于Gazebo模拟器中的两个机器人平台：一个用于线跟随和物体推动任务的差速驱动机器人，其视觉和结构参数可变化以生成多样任务；另一个七关节机械臂用于目标到达任务，支持高级笛卡尔控制和低级关节控制，并包含无需物理模拟的仅运动学变体以加速计算。实验结果展示了该套件的可扩展性和物理逼真度，容器化设置确保了可复现性，且标准强化学习算法（如DQN和策略梯度方法）的初步性能报告验证了其作为可扩展CRL基准的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-04T18:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人机交互贝叶斯优化的个性化图像生成</div>
<div class="mono" style="margin-top:8px">假设Alice心中有一幅特定图像$x^\ast$，例如她童年成长街道的景象。为生成该图像，她通过多轮提示引导生成模型，得到图像$x^{p*}$。虽然$x^{p*}$已接近$x^\ast$，但Alice发现仅凭语言提示难以完全弥合差距。本文指出：即使语言描述已达极限，人类仍能判断新图像$x^+$是否比$x^{p*}$更接近$x^\ast$。基于此，我们提出MultiBO（多选择偏好贝叶斯优化）方法：以$x^{p*}$为基准生成$K$幅新图像，获取用户偏好反馈，利用反馈指导扩散模型，最终生成新一轮$K$幅图像。实验表明，在$B$轮用户反馈内，即使生成模型未获取$x^\ast$的直接信息，仍能显著逼近目标图像。通过30名用户的定性评分，以及与5个基线模型的定量指标对比，结果证实人类的多选择反馈可有效助力个性化图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of personalized image generation when textual prompts alone are insufficient to capture a user&#x27;s specific mental image. The proposed method, Multi-Choice Preferential Bayesian Optimization (MultiBO), leverages human preferential feedback in a multi-round loop: after an initial text-guided generation, it presents K candidate images, collects user judgments on which is closer to the target, and uses this feedback to guide a diffusion model toward the desired output. Experimental results from 30 users and quantitative comparisons against five baselines demonstrate that this human-in-the-loop approach significantly narrows the gap between generated images and the user&#x27;s envisioned target within a limited number of feedback rounds.</div>
<div class="mono" style="margin-top:8px">本文针对个性化图像生成中仅凭文本提示难以精确匹配用户心中特定图像的问题展开研究。提出的方法名为多选择偏好贝叶斯优化（MultiBO），其核心是通过多轮人机交互循环利用人类偏好反馈：在初始文本引导生成后，系统呈现K个候选图像，收集用户关于哪个更接近目标图像的判断，并利用该反馈指导扩散模型优化输出。基于30名用户的定性评分以及与五个基线方法的定量比较，实验结果表明，在有限的反馈轮次内，这种人机协作方法能显著缩小生成图像与用户设想目标之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning</div>
<div class="meta-line">Authors: Joydeep Chandra, Satyam Kumar Navneet, Aleksandr Algazinov, Yong Zhang</div>
<div class="meta-line">First: 2026-02-04T18:10:59+00:00 · Latest: 2026-02-04T18:10:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04821v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\% coverage efficiency, controls FDR at 4.1\% under verified dependence, and improves safety rate to 95.2\% compared to 69\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于不确定性感知共形预测与世界模型强化学习的城市交通安全控制</div>
<div class="mono" style="margin-top:8px">城市交通管理系统需同时预测未来状况、检测异常并采取安全纠正措施，同时提供可靠性保证。我们提出STREAM-RL统一框架，包含三项创新算法贡献：(1) PU-GAT+不确定性引导自适应共形预测器，通过置信度单调注意力动态调整图注意力权重，实现无分布覆盖保证；(2) CRFN-BY共形残差流网络，通过标准化流建模不确定性归一化残差，在任意依赖关系下实现Benjamini-Yekutieli错误发现率控制；(3) LyCon-WRL+不确定性引导安全世界模型强化学习智能体，具备李雅普诺夫稳定性证明、经认证的利普希茨边界及不确定性传播想象推演。据我们所知，这是首个将校准不确定性从预测端经异常检测传播至安全策略学习，并具备端到端理论保证的框架。在多个真实交通轨迹数据上的实验表明：STREAM-RL实现91.4%的覆盖效率，在已验证依赖关系下将错误发现率控制在4.1%，安全率提升至95.2%（标准PPO为69%），同时获得更高奖励且端到端推理延迟仅23毫秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for reliable urban traffic control systems that can predict, detect anomalies, and act safely with formal guarantees. It proposes STREAM-RL, a unified framework featuring three novel components: PU-GAT+ for uncertainty-guided forecasting with coverage guarantees, CRFN-BY for anomaly detection with false discovery rate control, and LyCon-WRL+ for safe reinforcement learning using world models with stability certificates. Experimental results on real-world traffic data show that STREAM-RL achieves 91.4% coverage efficiency, maintains a 4.1% false discovery rate, improves safety rates to 95.2% from 69% compared to standard methods, and operates with low latency.</div>
<div class="mono" style="margin-top:8px">本文针对城市交通控制需要可靠预测、异常检测和安全行动的需求，提出了STREAM-RL统一框架。该框架包含三个新颖组件：PU-GAT+用于基于不确定性的预测并保证覆盖范围，CRFN-BY用于控制错误发现率的异常检测，以及LyCon-WRL+用于基于世界模型且具有稳定性证明的安全强化学习。在真实交通轨迹数据上的实验表明，STREAM-RL实现了91.4%的覆盖效率，将错误发现率控制在4.1%，将安全率从基准方法的69%提升至95.2%，同时保持了较低的端到端推理延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Rewards in Reinforcement Learning for Cyber Defence</div>
<div class="meta-line">Authors: Elizabeth Bates, Chris Hicks, Vasilios Mavroudis</div>
<div class="meta-line">First: 2026-02-04T17:55:23+00:00 · Latest: 2026-02-04T17:55:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越奖励：强化学习在网络安全防御中的应用</div>
<div class="mono" style="margin-top:8px">近年来，利用深度强化学习训练自主网络防御代理以保护计算机网络的研究兴趣激增。这些代理通常在网络训练环境中使用密集、高度设计的奖励函数进行训练，这些函数结合了对多种（非）理想状态和代价高昂行动的惩罚与激励。密集奖励有助于缓解复杂环境探索的挑战，但可能导致代理偏向次优且风险更高的解决方案，这在复杂网络环境中尤为关键。我们通过多种稀疏与密集奖励函数、两个成熟的网络训练环境、不同网络规模以及策略梯度和基于价值的强化学习算法，全面评估了奖励函数结构对学习与策略行为特征的影响。评估采用了一种新颖的基准评估方法，可直接比较不同奖励函数，揭示奖励、行动空间与网络环境中次优策略风险之间的微妙关联。结果表明，只要稀疏奖励与目标一致且能频繁触发，不仅能提升训练可靠性，还能产生具有更低风险策略的更有效网络防御代理。出乎意料的是，稀疏奖励还能生成更符合网络防御目标、且无需显式数值惩罚即可节制使用高成本防御行动的策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the risk that dense, engineered reward functions may bias reinforcement learning agents toward suboptimal and risky policies in autonomous cyber defense, this paper systematically evaluates the impact of reward structure. The method employs a novel ground-truth evaluation approach to compare sparse and dense reward functions across two established cyber gym environments, various network sizes, and both policy-gradient and value-based RL algorithms. The main experimental results reveal that well-designed sparse rewards, when goal-aligned and frequently encountered, lead to more reliable training, produce more effective defense agents with lower-risk policies, and can yield behaviors better aligned with defender objectives—including sparing use of costly actions—even without explicit numerical penalties for those actions.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，在自主网络防御中，密集、精心设计的奖励函数可能导致强化学习智能体偏向次优且高风险的策略。研究方法采用了一种新颖的基准评估方法，在两个成熟的网络训练环境、不同网络规模以及策略梯度和基于价值的RL算法中，系统比较了稀疏与密集奖励函数的影响。主要实验结果表明，设计良好、目标对齐且能频繁遇到的稀疏奖励，不仅能提高训练可靠性，还能产生更有效、策略风险更低的网络防御智能体；令人惊讶的是，稀疏奖励还能使策略更符合防御者目标，并减少对高成本防御行动的依赖，而无需对这些行动施加明确的数值惩罚。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning</div>
<div class="meta-line">Authors: Wolfgang Maass, Sabine Janzen, Prajvi Saxena, Sach Mukherjee</div>
<div class="meta-line">First: 2026-02-04T17:53:28+00:00 · Latest: 2026-02-04T17:53:28+00:00</div>
<div class="meta-line">Comments: 16 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04807v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>演化传入架构：面向损伤规避学习的仿生模型</div>
<div class="mono" style="margin-top:8px">本文提出传入学习框架，通过生成计算传入痕迹作为损伤规避学习的自适应内部风险信号。该仿生框架采用双层架构：进化优化（外层循环）发现能支持有效策略学习的传入感知架构，而强化学习（内层循环）利用这些信号训练损伤规避策略。该研究将传入感知形式化为高效学习的归纳偏置——架构选择标准是其促进有效学习的能力（而非直接最小化损伤）。我们在平滑性与有界噪声假设下提供了理论收敛保证。通过在生物力学数字孪生长时间尺度（数十年生命周期）的挑战性场景中验证，发现基于计算传入痕迹的演化架构相比人工设计基线显著提升效率与年龄鲁棒性，实现具有年龄依赖行为适应的策略（高风险行为减少23%）。消融研究验证了计算传入痕迹信号、进化机制与预测差异的核心作用。我们公开了代码与数据以确保可复现性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for robust damage-avoidance learning in long-term systems like biomechanical digital twins, this paper introduces Afferent Learning, a biologically-inspired framework that evolves adaptive internal risk signals called Computational Afferent Traces (CATs). The method employs a two-level architecture: an outer evolutionary loop discovers afferent sensing architectures that provide an inductive bias, while an inner reinforcement learning loop trains policies using these CAT signals. Experimental results in a multi-decade simulation show that evolved CAT architectures significantly outperform hand-designed baselines in efficiency and age-robustness, reducing high-risk actions by 23% and enabling age-dependent behavioral adaptation, with ablations confirming the importance of the core components.</div>
<div class="mono" style="margin-top:8px">受生物系统启发，本文针对生物力学数字孪生等长期系统的损伤规避学习需求，提出了传入学习框架，该框架演化出自适应的内部风险信号，即计算传入痕迹。该方法采用双层架构：外层进化循环发现能提供归纳偏置的传入感知架构，内层强化学习循环则利用这些信号训练策略。在多年代际模拟中的实验结果表明，演化出的传入痕迹架构在效率和年龄鲁棒性上显著优于人工设计的基线，将高风险行为减少了23%，并实现了依赖年龄的行为适应，消融研究验证了核心组件的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging</div>
<div class="meta-line">Authors: Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu</div>
<div class="meta-line">First: 2026-02-04T17:52:17+00:00 · Latest: 2026-02-04T17:52:17+00:00</div>
<div class="meta-line">Comments: 14 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04805v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04805v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>皮肤令牌：一种用于统一自回归绑定系统的学习型紧凑表示</div>
<div class="mono" style="margin-top:8px">生成式3D模型的快速扩散在动画流程中造成了关键瓶颈：绑定。现有自动化方法从根本上受限于其蒙皮处理方式，将其视为一个不适定的高维回归任务，优化效率低下且通常与骨骼生成解耦。我们认为这是一个表示问题，并提出了皮肤令牌：一种学习得到的、紧凑的、离散的蒙皮权重表示。通过利用FSQ-CVAE捕捉蒙皮固有的稀疏性，我们将任务从连续回归重构为更易处理的令牌序列预测问题。该表示支持TokenRig——一个统一的自回归框架，将整个绑定系统建模为骨骼参数与皮肤令牌的单一序列，从而学习骨骼与皮肤变形之间的复杂依赖关系。该统一模型随后可进行强化学习阶段，通过定制的几何与语义奖励提升对复杂、分布外资产的泛化能力。量化结果显示，皮肤令牌表示在蒙皮精度上比现有最优方法提升98%-133%，而经RL优化的完整TokenRig框架将骨骼预测精度提升17%-22%。本研究提出了一种统一的生成式绑定方法，实现了更高保真度与鲁棒性，为3D内容创作中长期存在的挑战提供了可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the bottleneck of manual rigging in 3D animation pipelines, this paper introduces SkinTokens, a learned discrete representation for skinning weights that reframes skinning from continuous regression to token sequence prediction using an FSQ-CVAE. The method enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, capturing dependencies between skeletons and skin deformations, with a reinforcement learning stage applying geometric and semantic rewards for generalization. Experimentally, SkinTokens achieves a 98%-133% improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework enhances bone prediction by 17%-22%, demonstrating higher fidelity and robustness in rigging 3D assets.</div>
<div class="mono" style="margin-top:8px">本文针对3D动画流程中手动绑定骨架的瓶颈问题，提出了SkinTokens这一用于蒙皮权重的学习型离散表示，通过FSQ-CVAE将蒙皮从连续回归重构为令牌序列预测任务。该方法支持TokenRig，一个统一的自动回归框架，将整个绑定建模为骨骼参数和SkinTokens的单一序列，以捕捉骨架与皮肤变形间的复杂依赖关系，并通过强化学习阶段应用几何和语义奖励来提升泛化能力。实验结果表明，SkinTokens在蒙皮精度上比现有最优方法提高了98%-133%，而完整的TokenRig框架将骨骼预测提升了17%-22%，为3D内容创建提供了更高保真度和鲁棒性的可扩展解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Improving Pretraining: using post-trained models to pretrain better models</div>
<div class="meta-line">Authors: Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar, Jason Weston, Olga Golovneva</div>
<div class="meta-line">First: 2026-01-29T07:09:30+00:00 · Latest: 2026-02-04T17:31:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21343v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21343v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model&#x27;s core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我改进预训练：利用后训练模型预训练更优模型</div>
<div class="mono" style="margin-top:8px">确保大语言模型生成内容的安全性、事实性与整体质量是一项关键挑战，尤其在模型日益广泛应用于现实场景的背景下。当前主流解决方案依赖于收集成本高昂、精心标注的数据集，并进行多阶段微调与对齐。然而，即使这一复杂流程也无法完全修正预训练阶段习得的偏差模式。因此，在预训练阶段解决这些问题至关重要，因为该阶段塑造了模型的核心行为模式，能从根本上避免不安全或虚构内容被深度固化。为此，我们提出一种新型预训练方法：通过流式文档处理，运用强化学习逐步优化后续K个生成标记。一个经过后训练的强模型将对候选生成内容（包括模型推演序列、原始后缀及改写后缀）进行质量、安全性与事实性评估。训练初期依赖原始与改写后缀；随着模型能力提升，强化学习机制将对高质量推演给予奖励。该方法从源头构建了更优质、更安全、更符合事实的模型。实验表明，本方法在事实性与安全性指标上分别较标准预训练提升36.2%与18.5%，在整体生成质量方面胜率最高提升达86.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to embed safety, factuality, and quality directly into large language models&#x27; core behaviors rather than relying solely on costly post-training alignment, this paper introduces a self-improving pretraining method. The method employs reinforcement learning during pretraining, where a strong post-trained model evaluates candidate token generations—including model rollouts, original text suffixes, and rewritten suffixes—based on quality, safety, and factuality to provide rewards, gradually shifting reliance from human-provided suffixes to model-generated high-quality rollouts as training progresses. Experimental results demonstrate that this approach yields relative improvements of 36.2% in factuality and 18.5% in safety over standard pretraining, along with up to 86.3% win rate gains in overall generation quality.</div>
<div class="mono" style="margin-top:8px">本文的动机是需要在大型语言模型的核心行为中直接嵌入安全性、事实性和质量，而非仅依赖昂贵的后训练对齐方法。为此，论文提出了一种自改进的预训练方法，该方法在预训练过程中使用强化学习，由一个强大的后训练模型根据质量、安全性和事实性评估候选令牌生成（包括模型自生成、原始文本后缀和重写后缀）以提供奖励，随着训练进行，逐步从依赖人工提供的后缀转向模型生成的高质量内容。实验结果表明，该方法相比标准预训练在事实性上相对提升了36.2%，在安全性上提升了18.5%，并在整体生成质量上获得了高达86.3%的胜率改进。</div>
</details>
</div>
<div class="card">
<div class="title">When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?</div>
<div class="meta-line">Authors: Xinyu Zhou, Chang Jin, Carsten Eickhoff, Zhijiang Guo, Seyed Ali Bahrainian</div>
<div class="meta-line">First: 2026-02-04T16:54:47+00:00 · Latest: 2026-02-04T16:54:47+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04755v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04755v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\%$ and $5.80\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>沉默是金：大语言模型能否在时序问答及其他任务中学会弃答？</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）很少承认不确定性，常生成流畅但具有误导性的答案，而非选择弃答（即拒绝回答）。这一缺陷在时序问答中尤为明显，模型常忽略时效性证据并混淆不同时期的事实。本文首次对训练LLMs在时序问答推理中具备弃答能力进行实证研究。现有方法（如校准）在捕捉复杂推理中的不确定性时可能不可靠。我们将弃答定义为可教授的技能，提出一种结合思维链监督与弃答感知奖励强化学习的训练框架。目标是系统分析不同信息类型和训练技术如何影响LLMs在时序推理中的弃答行为。通过大量实验发现：强化学习显著提升推理性能——基于Qwen2.5-1.5B-Instruct初始化的模型在TimeQA-Easy和Hard数据集上的精确匹配率分别超越GPT-4o达3.46%和5.80%；对不可答问题的真阳性率较纯监督微调模型提升20%。分析表明：监督微调会引发过度自信损害可靠性，强化学习虽提升预测精度但存在类似风险。通过对比隐式推理线索（原始上下文、时序子语境、知识图谱）与显式思维链监督，发现隐式信息对弃答推理的助益有限。本研究为协同优化弃答与推理机制提供了新见解，为构建更可靠的LLMs奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem that large language models (LLMs) often produce confidently wrong answers instead of abstaining when uncertain, particularly in temporal question answering where they conflate facts across time periods. The authors frame abstention as a teachable skill and introduce a training pipeline that combines Chain-of-Thought supervision with Reinforcement Learning using abstention-aware rewards to jointly optimize reasoning and abstention behavior. Experimental results show that their RL-trained model, based on Qwen2.5-1.5B-Instruct, outperforms GPT-4o on TimeQA benchmarks and improves the true positive rate on unanswerable questions by 20% compared to a supervised fine-tuning baseline, though analysis reveals that RL improves accuracy while still carrying some reliability risks, and implicit reasoning cues provide limited benefit compared to explicit supervision.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在不确定时往往生成自信但错误的答案而非选择弃答的问题展开研究，特别是在时序问答中模型常混淆不同时期的事实。作者将弃答能力构建为一种可学习的技能，提出了一种结合思维链监督与基于弃答感知奖励的强化学习的训练流程，以联合优化推理和弃答行为。实验结果表明，基于Qwen2.5-1.5B-Instruct并经过强化学习训练的模型在TimeQA基准上超越了GPT-4o，相较于纯监督微调基线，在不可回答问题上的真阳性率提升了20%，但分析也表明强化学习虽提高了准确性仍存在可靠性风险，且隐式推理线索相比显式监督收益有限。</div>
</details>
</div>
<div class="card">
<div class="title">Rationality Measurement and Theory for Reinforcement Learning Agents</div>
<div class="meta-line">Authors: Kejiang Qian, Amos Storkey, Fengxiang He</div>
<div class="meta-line">First: 2026-02-04T16:41:22+00:00 · Latest: 2026-02-04T16:41:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04737v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04737v1">PDF</a> · <a href="https://github.com/EVIEHub/Rationality">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy&#x27;s actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm&#x27;s generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习智能体的理性度量与理论</div>
<div class="mono" style="margin-top:8px">本文针对强化学习智能体提出了一套理性度量及相关理论，该性质日益关键却鲜有研究。我们定义部署中的行为若沿最陡方向最大化隐藏的真实价值函数，则为完全理性。策略行为与其理性对应行为在部署轨迹上的期望价值差异被定义为期望理性风险；同时定义了训练中的经验平均版本。二者的差异（称为理性风险间隙）可分解为：（1）由训练与部署间环境偏移引起的外在成分，以及（2）由算法在动态环境中泛化能力决定的内在成分。二者分别上界为：（1）训练与部署中转移核及初始状态分布的$1$-Wasserstein距离，以及（2）价值函数类的经验Rademacher复杂度。我们的理论提出了关于正则化器（包括层归一化、$\ell_2$正则化及权重归一化）与领域随机化益处，以及环境偏移危害的假设。实验完全验证了这些假设。代码发布于https://github.com/EVIEHub/Rationality。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of formal rationality measures in reinforcement learning by introducing a suite of metrics and theoretical bounds to assess agent decision-making. The method defines perfect rationality as actions maximizing the true value function&#x27;s steepest direction, leading to measures like expected rational risk and rational risk gap, which decompose into extrinsic and intrinsic components bounded by Wasserstein distance and Rademacher complexity. Experimental results confirm hypotheses that regularizers and domain randomization improve rationality while environment shifts degrade it, validating the proposed theory.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中缺乏形式化理性度量的问题，提出了一套评估智能体决策的指标和理论框架。方法将完美理性定义为行动在隐藏真实价值函数的最速方向上最大化，并引入了期望理性风险和理性风险差距等度量，后者可分解为外在和内在成分，分别受Wasserstein距离和Rademacher复杂度上界约束。实验结果验证了正则化与领域随机化提升理性、环境偏移损害理性的假设，支持了所提理论。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary Mapping of Neural Networks to Spatial Accelerators</div>
<div class="meta-line">Authors: Alessandro Pierro, Jonathan Timcheck, Jason Yik, Marius Lindauer, Eyke Hüllermeier, Marcel Wever</div>
<div class="meta-line">First: 2026-02-04T16:28:08+00:00 · Latest: 2026-02-04T16:28:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04717v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04717v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial accelerators, composed of arrays of compute-memory integrated units, offer an attractive platform for deploying inference workloads with low latency and low energy consumption. However, fully exploiting their architectural advantages typically requires careful, expert-driven mapping of computational graphs to distributed processing elements. In this work, we automate this process by framing the mapping challenge as a black-box optimization problem. We introduce the first evolutionary, hardware-in-the-loop mapping framework for neuromorphic accelerators, enabling users without deep hardware knowledge to deploy workloads more efficiently. We evaluate our approach on Intel Loihi 2, a representative spatial accelerator featuring 152 cores per chip in a 2D mesh. Our method achieves up to 35% reduction in total latency compared to default heuristics on two sparse multi-layer perceptron networks. Furthermore, we demonstrate the scalability of our approach to multi-chip systems and observe an up to 40% improvement in energy efficiency, without explicitly optimizing for it.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经网络到空间加速器的进化映射</div>
<div class="mono" style="margin-top:8px">空间加速器由计算-内存一体化单元阵列构成，为部署低延迟、低能耗的推理工作负载提供了理想平台。然而，要充分发挥其架构优势，通常需要专家精心将计算图映射到分布式处理单元。本研究通过将映射问题构建为黑盒优化任务，实现了该过程的自动化。我们提出了首个面向神经形态加速器的进化式硬件在环映射框架，使不具备深度硬件知识的用户也能更高效地部署工作负载。我们在英特尔Loihi 2芯片（典型空间加速器，每芯片含152个核心的二维网格结构）上评估该方法。针对两个稀疏多层感知器网络，相比默认启发式方法，我们的方案最高可降低35%的总延迟。此外，我们验证了该方法在多芯片系统中的可扩展性，在未专门优化能耗的情况下，能效最高提升40%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need to automate the expert-driven mapping of neural network computational graphs onto spatial accelerators to fully exploit their low-latency, low-energy potential. The method frames mapping as a black-box optimization problem and introduces an evolutionary, hardware-in-the-loop framework that allows users without deep hardware expertise to deploy workloads efficiently. Experimental evaluation on the Intel Loihi 2 accelerator shows the approach reduces total latency by up to 35% compared to default heuristics for sparse multi-layer perceptrons and scales to multi-chip systems, yielding up to a 40% improvement in energy efficiency without direct optimization for it.</div>
<div class="mono" style="margin-top:8px">本研究旨在自动化将神经网络计算图映射到空间加速器的专家驱动过程，以充分挖掘其低延迟、低能耗的潜力。其方法将映射问题构建为一个黑盒优化问题，并引入了一种进化的、硬件在环的框架，使不具备深厚硬件知识的用户也能高效部署工作负载。在英特尔Loihi 2加速器上的实验评估表明，该方法相比默认启发式方法，在稀疏多层感知器网络上可降低高达35%的总延迟，并能扩展到多芯片系统，在不直接优化能效的情况下实现了高达40%的能效提升。</div>
</details>
</div>
<div class="card">
<div class="title">When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates</div>
<div class="meta-line">Authors: Michele Caprio, Siu Lun Chau, Krikamol Muandet</div>
<div class="meta-line">First: 2025-10-06T12:42:32+00:00 · Latest: 2026-02-04T16:14:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04769v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.04769v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many machine learning algorithms rely on iterative updates of uncertainty representations, ranging from variational inference and expectation-maximization, to reinforcement learning, continual learning, and multi-agent learning. In the presence of imprecision and ambiguity, credal sets -- closed, convex sets of probability distributions -- have emerged as a popular framework for representing imprecise probabilistic beliefs. Under such imprecision, many learning problems in imprecise probabilistic machine learning (IPML) may be viewed as processes involving successive applications of update rules on credal sets. This naturally raises the question of whether this iterative process converges to stable fixed points -- or, more generally, under what conditions on the updating mechanism such fixed points exist, and whether they can be attained. We provide the first analysis of this problem, and illustrate our findings using Credal Bayesian Deep Learning as a concrete example. Our work demonstrates that incorporating imprecision into the learning process not only enriches the representation of uncertainty, but also reveals structural conditions under which stability emerges, thereby offering new insights into the dynamics of iterative learning under imprecision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信度集何时稳定？信度集更新的不动点定理</div>
<div class="mono" style="margin-top:8px">许多机器学习算法依赖于不确定性表示的迭代更新，涵盖变分推断、期望最大化、强化学习、持续学习与多智能体学习等领域。面对不精确性与模糊性时，信度集——即概率分布的闭凸集——已成为表示不精确概率信念的主流框架。在此类不精确性条件下，不精确概率机器学习中的许多学习问题可视为对信度集连续应用更新规则的过程。这自然引出一个问题：该迭代过程是否会收敛至稳定不动点？更一般地说，更新机制需满足何种条件才能保证不动点的存在性与可达性？本文首次对该问题进行了系统性分析，并以信度贝叶斯深度学习为具体案例阐释研究发现。我们的工作表明，将不精确性纳入学习过程不仅能丰富不确定性表征，还能揭示稳定性涌现的结构性条件，从而为不精确条件下的迭代学习动力学提供新见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand convergence in iterative learning algorithms that handle imprecise probabilistic beliefs, this paper investigates the conditions under which credal set updates reach stable fixed points. The method employs fixed-point theorems to analyze the existence and attainability of such fixed points within the framework of imprecise probabilistic machine learning, using Credal Bayesian Deep Learning as a case study. The main experimental results demonstrate that incorporating imprecision not only enhances uncertainty representation but also reveals structural conditions that lead to stability, offering new insights into the dynamics of iterative learning under ambiguity.</div>
<div class="mono" style="margin-top:8px">本文旨在探究处理不精确概率信念的迭代学习算法中，信度集更新达到稳定不动点的条件，以理解其收敛性。研究方法基于不动点定理，分析了不精确概率机器学习框架下不动点的存在性和可达性，并以信度贝叶斯深度学习为例进行说明。主要实验结果表明，引入不精确性不仅丰富了不确定性表示，还揭示了导致稳定性的结构条件，为模糊性下的迭代学习动态提供了新见解。</div>
</details>
</div>
<div class="card">
<div class="title">Information Templates: A New Paradigm for Intelligent Active Feature Acquisition</div>
<div class="meta-line">Authors: Hung-Tien Huang, Dzung Dinh, Junier B. Oliva</div>
<div class="meta-line">First: 2025-08-25T18:15:11+00:00 · Latest: 2026-02-04T16:12:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.18380v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.18380v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active feature acquisition (AFA) is an instance-adaptive paradigm in which, at inference time, a policy sequentially chooses which features to acquire (at a cost) before predicting. Existing approaches either train reinforcement learning policies, which deal with a difficult MDP, or greedy policies that cannot account for the joint informativeness of features or require knowledge about the underlying data distribution. To overcome this, we propose Template-based AFA (TAFA), a non-greedy framework that learns a small library of feature templates -- sets of features that are jointly informative -- and uses this library of templates to guide the next feature acquisitions. Through identifying feature templates, the proposed framework not only significantly reduces the action space considered by the policy but also alleviates the need to estimate the underlying data distribution. Extensive experiments on synthetic and real-world datasets show that TAFA outperforms the existing state-of-the-art baselines while achieving lower overall acquisition cost and computation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信息模板：智能主动特征获取的新范式</div>
<div class="mono" style="margin-top:8px">主动特征获取（AFA）是一种实例自适应范式，在推理时通过策略按顺序选择（以一定成本）获取哪些特征后再进行预测。现有方法要么训练处理复杂MDP的强化学习策略，要么采用无法考虑特征联合信息性或需要底层数据分布知识的贪婪策略。为克服这些局限，我们提出基于模板的AFA（TAFA），这是一种非贪婪框架，通过学习少量特征模板库——即具有联合信息性的特征集合——并利用该模板库指导后续特征获取。通过识别特征模板，该框架不仅显著缩小策略需考虑的动作空间，还降低了对底层数据分布估计的需求。在合成与真实数据集上的大量实验表明，TAFA在降低总体获取成本与计算量的同时，性能优于现有最先进基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in active feature acquisition (AFA), where existing methods either rely on complex reinforcement learning or greedy policies that fail to capture joint feature informativeness or require distributional knowledge. The authors propose Template-based AFA (TAFA), a framework that learns a compact library of feature templates—sets of jointly informative features—to guide sequential feature acquisition, thereby shrinking the action space and eliminating the need for data distribution estimation. Experiments on synthetic and real-world datasets demonstrate that TAFA surpasses state-of-the-art baselines by achieving lower acquisition costs and computational demands while improving performance.</div>
<div class="mono" style="margin-top:8px">本文针对主动特征获取（AFA）的局限性展开研究，现有方法要么依赖复杂的强化学习，要么采用无法考虑特征联合信息性或需要数据分布知识的贪婪策略。为此，作者提出了基于模板的AFA（TAFA）框架，通过学习一个紧凑的特征模板库——即具有联合信息性的特征集合——来指导顺序特征获取，从而缩小动作空间并避免估计数据分布。在合成和真实数据集上的实验表明，TAFA在降低总体获取成本和计算量的同时，性能优于当前最先进的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design</div>
<div class="meta-line">Authors: Jaemoo Choi, Yuchen Zhu, Wei Guo, Petr Molodyk, Bo Yuan, Jinbin Bai, Yi Xin, Molei Tao, Yongxin Chen</div>
<div class="meta-line">First: 2026-02-04T15:36:42+00:00 · Latest: 2026-02-04T15:36:42+00:00</div>
<div class="meta-line">Comments: 23 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视扩散模型中强化学习的设计空间：超越损失设计的似然估计重要性</div>
<div class="mono" style="margin-top:8px">强化学习已广泛应用于扩散模型和流模型在文本到图像生成等视觉任务中。然而，由于扩散模型具有难以处理的似然函数，直接应用流行的策略梯度类方法存在障碍，这些任务仍具挑战性。现有方法主要侧重于基于已高度工程化的大语言模型目标构建新目标，并使用临时估计器处理似然，未深入探究此类估计如何影响整体算法性能。本研究通过解耦三个因素对强化学习设计空间进行系统分析：i) 策略梯度目标，ii) 似然估计器，iii) 轨迹采样方案。研究表明，采用基于证据下界的模型似然估计器（仅从最终生成样本计算）是实现高效、稳定强化学习优化的主导因素，其影响超过特定策略梯度损失函数的选择。我们在SD 3.5 Medium模型上通过多个奖励基准验证了发现，所有任务均呈现一致趋势。该方法在90 GPU小时内将GenEval分数从0.24提升至0.95，效率比FlowGRPO高4.6倍，比当前最优方法DiffusionNFT高2倍，且未出现奖励破解现象。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying reinforcement learning to diffusion models for tasks like text-to-image generation, where intractable likelihoods hinder direct use of policy-gradient methods. The authors systematically analyze the RL design space by disentangling policy-gradient objectives, likelihood estimators, and rollout sampling schemes, finding that an evidence lower bound (ELBO)-based likelihood estimator computed from final generated samples is the key factor for effective optimization, outweighing loss design specifics. Experimental validation on multiple reward benchmarks with SD 3.5 Medium shows their method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, achieving 4.6× and 2× greater efficiency than prior methods like FlowGRPO and DiffusionNFT without reward hacking.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在扩散模型（如文本到图像生成）中的应用挑战展开研究，其中难以处理的似然性阻碍了策略梯度方法的直接使用。作者通过解耦策略梯度目标、似然估计器和轨迹采样方案，系统分析了强化学习的设计空间，发现基于证据下界（ELBO）的似然估计器（仅从最终生成样本计算）是实现有效优化的关键因素，其影响超过损失函数设计的具体细节。在SD 3.5 Medium模型上的多奖励基准实验表明，该方法在90 GPU小时内将GenEval分数从0.24提升至0.95，相比FlowGRPO和DiffusionNFT等先进方法，效率分别提高4.6倍和2倍，且未出现奖励黑客问题。</div>
</details>
</div>
<div class="card">
<div class="title">Mixed-Density Diffuser: Efficient Planning with Non-Uniform Temporal Resolution</div>
<div class="meta-line">Authors: Crimson Stambaugh, Rajesh P. N. Rao</div>
<div class="meta-line">First: 2025-10-27T05:45:59+00:00 · Latest: 2026-02-04T15:33:28+00:00</div>
<div class="meta-line">Comments: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN) (under review)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23026v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.23026v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies demonstrate that diffusion planners benefit from sparse-step planning over single-step planning. Training models to skip steps in their trajectories helps capture long-term dependencies without additional memory or computational cost. However, predicting excessively sparse plans degrades performance. We hypothesize this temporal density threshold is non-uniform across a planning horizon and that certain parts of a predicted trajectory should be more densely generated. We propose Mixed-Density Diffuser (MDD), a diffusion planner where the densities throughout the horizon are tunable hyperparameters. We show that MDD surpasses the SOTA Diffusion Veteran (DV) framework across the Maze2D, Franka Kitchen, and Antmaze Datasets for Deep Data-Driven Reinforcement Learning (D4RL) task domains, achieving a new SOTA on the D4RL benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合密度扩散器：非均匀时间分辨率下的高效规划</div>
<div class="mono" style="margin-top:8px">近期研究表明，扩散规划器通过稀疏步长规划优于单步规划。训练模型跳过轨迹中的步骤有助于捕捉长期依赖关系，且无需额外内存或计算成本。然而，预测过度稀疏的规划会降低性能。我们假设这种时间密度阈值在规划时域内是非均匀的，且预测轨迹的某些部分应更密集生成。我们提出混合密度扩散器（MDD），这是一种扩散规划器，其整个时域的密度可作为可调超参数。实验表明，在Maze2D、Franka Kitchen和Antmaze数据集上，MDD在深度数据驱动强化学习（D4RL）任务领域均超越了当前最优的扩散规划框架（DV），并在D4RL基准测试中创造了新的最优性能记录。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in diffusion-based planning where uniformly sparse step-skipping can degrade performance, hypothesizing that optimal temporal resolution varies across a planning horizon. It introduces the Mixed-Density Diffuser (MDD), a method allowing tunable, non-uniform step densities as hyperparameters to generate trajectories with varying detail levels. Experimental results demonstrate that MDD outperforms the state-of-the-art Diffusion Veteran framework on Maze2D, Franka Kitchen, and Antmaze datasets, achieving new top performance on the D4RL benchmark.</div>
<div class="mono" style="margin-top:8px">本文针对扩散规划中均匀稀疏跳步可能降低性能的问题，提出规划时域的最优时间分辨率是非均匀的假设。方法上引入了混合密度扩散器（MDD），允许将非均匀的步长密度作为可调超参数，以生成具有不同细节层次的轨迹。实验结果表明，MDD在Maze2D、Franka Kitchen和Antmaze数据集上超越了当前最先进的Diffusion Veteran框架，并在D4RL基准测试中取得了新的最优性能。</div>
</details>
</div>
<div class="card">
<div class="title">WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Zelai Xu, Zhexuan Xu, Ruize Zhang, Chunyang Zhu, Shi Yu, Weilin Liu, Quanlu Zhang, Wenbo Ding, Chao Yu, Yu Wang</div>
<div class="meta-line">First: 2026-02-04T15:05:12+00:00 · Latest: 2026-02-04T15:05:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04634v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WideSeek-R1：通过多智能体强化学习探索面向广泛信息检索的宽度扩展</div>
<div class="mono" style="margin-top:8px">近期大语言模型（LLM）的发展主要聚焦于深度扩展，即单个智能体通过多轮推理和工具使用解决长程问题。然而，随着任务范围扩大，关键瓶颈从个体能力转向组织协同能力。本研究探索多智能体系统的宽度扩展这一互补维度，以应对广泛信息检索需求。现有多智能体系统常依赖人工编排的工作流和轮替式交互，难以实现高效并行化。为弥补这一不足，我们提出WideSeek-R1——一种通过多智能体强化学习（MARL）训练的领航智能体-子智能体框架，旨在协同实现可扩展的编排与并行执行。该框架基于共享LLM构建隔离上下文与专用工具，在包含2万项广泛信息检索任务的精选数据集上联合优化领航智能体与并行子智能体。大量实验表明，WideSeek-R1-4B在WideSearch基准测试中达到40.0%的项目F1分数，其性能可与单智能体DeepSeek-R1-671B相媲美。此外，随着并行子智能体数量增加，WideSeek-R1-4B展现出持续的性能提升，凸显了宽度扩展策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the shift in bottleneck from individual competence to organizational capability as tasks broaden, this work explores width scaling via multi-agent systems for broad information seeking, addressing limitations of hand-crafted workflows in existing systems. The method introduces WideSeek-R1, a lead-agent-subagent framework trained with multi-agent reinforcement learning to enable scalable orchestration and parallel execution, utilizing a shared LLM with isolated contexts and specialized tools on 20k curated tasks. Experimental results show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, matching the performance of the much larger single-agent DeepSeek-R1-671B, with consistent gains as parallel subagents increase, demonstrating the efficacy of width scaling.</div>
<div class="mono" style="margin-top:8px">本研究动机在于，随着任务范围扩大，关键瓶颈从个体能力转向组织能力，因此探索通过多智能体系统进行宽度扩展以应对广泛信息搜索，解决现有系统依赖手工流程和低效串行交互的问题。方法上提出了WideSeek-R1，这是一个通过多智能体强化学习训练的主从智能体框架，利用共享大语言模型与隔离上下文及专用工具，在2万个精选任务上实现可扩展编排和并行执行。主要实验结果表明，WideSeek-R1-4B在WideSearch基准测试中达到40.0%的项目F1分数，与规模大得多的单智能体DeepSeek-R1-671B性能相当，且随着并行子智能体数量增加性能持续提升，凸显了宽度扩展的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning</div>
<div class="meta-line">Authors: Doyeon Lee, Eunyi Lyou, Hyunsoo Cho, Sookyung Kim, Joonseok Lee, Jaemoo Choi</div>
<div class="meta-line">First: 2026-02-04T14:51:04+00:00 · Latest: 2026-02-04T14:51:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04620v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04620v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QUATRO：面向大语言模型微调的查询自适应信赖域策略优化</div>
<div class="mono" style="margin-top:8px">基于GRPO风格强化学习的大语言模型微调算法近期备受关注。然而，这类算法依赖启发式信赖域近似，可能引发脆弱的优化行为——全局重要性比率裁剪与分组归一化无法有效调控超出裁剪范围的样本重要性比率。本文提出查询自适应信赖域策略优化（QUATRO），通过原理性优化直接实施信赖域约束。该方法构建了清晰可解释的目标函数，实现对策略更新的显式控制，并通过精确信赖域公式内生的稳定项，实现熵值可控的稳定优化。在多样化数学推理基准上的实证表明，QUATRO在策略陈旧性增强和激进学习率设置下仍能保持训练稳定性，并在整个训练过程中维持良好受控的熵值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the brittle optimization behavior of existing GRPO-style RL fine-tuning methods due to heuristic trust-region approximations, this paper proposes QUATRO, a method that directly enforces trust-region constraints through a principled optimization to yield a clear, interpretable objective with explicit control over policy updates. The method introduces stabilizer terms intrinsically from the exact trust-region formulation, enabling stable, entropy-controlled optimization. Experimental results on diverse mathematical reasoning benchmarks demonstrate that QUATRO achieves stable training under increased policy staleness and aggressive learning rates while maintaining well-controlled entropy throughout.</div>
<div class="mono" style="margin-top:8px">针对现有基于GRPO的强化学习大模型微调方法因启发式信任域近似导致的脆弱优化行为，本文提出了QUATRO方法，其通过原理性优化直接强制执行信任域约束，从而产生清晰、可解释的目标，并能显式控制策略更新。该方法从精确的信任域公式中内在地引入了稳定项，实现了稳定且熵受控的优化。在多样化的数学推理基准测试上的实验结果表明，QUATRO在策略陈旧性增加和激进学习率下仍能保持训练稳定，并在整个训练过程中维持良好的熵控制。</div>
</details>
</div>
<div class="card">
<div class="title">Stochastic Decision Horizons for Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Nikola Milosevic, Leonard Franz, Daniel Haeufle, Georg Martius, Nico Scherf, Pavel Kolev</div>
<div class="meta-line">First: 2026-02-04T14:27:16+00:00 · Latest: 2026-02-04T14:27:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04599v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04599v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束强化学习中的随机决策时域</div>
<div class="mono" style="margin-top:8px">约束马尔可夫决策过程（CMDPs）为强化学习中处理安全及其他辅助目标等约束提供了原则性模型。传统采用加性成本约束与对偶变量的方法常阻碍离策略可扩展性。本文提出基于随机决策时域的“控制即推断”框架，其中约束违反会衰减奖励贡献，并通过状态-动作依赖的延续机制缩短有效规划时域。该框架生成与离策略演员-评论家学习兼容的生存加权目标。我们提出两种违反语义——吸收终止与虚拟终止，二者共享相同的生存加权回报，但形成分别导向SAC/MPO风格策略改进的优化结构。实验在标准基准测试中展现出更高的样本效率与更优的回报-违反权衡。此外，采用虚拟终止的MPO（VT-MPO）在我们的高维肌肉骨骼Hyfydy环境中实现了有效扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the scalability limitations of constrained reinforcement learning (RL) in Constrained Markov Decision Processes (CMDPs), where traditional dual-variable methods hinder off-policy learning. The authors propose a Control as Inference formulation using stochastic decision horizons, which attenuates rewards based on constraint violations and shortens the planning horizon via state-action-dependent continuation, leading to survival-weighted objectives compatible with off-policy actor-critic algorithms. Two violation semantics—absorbing and virtual termination—are introduced, both enabling SAC/MPO-style policy optimization. Experimental results on standard benchmarks show improved sample efficiency and better return-violation trade-offs, with the virtual termination MPO (VT-MPO) variant scaling effectively to a high-dimensional musculoskeletal control task.</div>
<div class="mono" style="margin-top:8px">本文针对约束强化学习中约束马尔可夫决策过程（CMDP）的可扩展性限制，传统基于对偶变量的方法阻碍了离策略学习。作者提出了一种基于随机决策视野的“推断即控制”框架，通过状态-动作依赖的延续机制，使约束违规衰减奖励贡献并缩短有效规划视野，从而产生与离策略演员-评论家算法兼容的生存加权目标。引入了两种违规语义——吸收终止和虚拟终止，均支持SAC/MPO风格策略优化。在标准基准测试中，实验结果表明了更高的样本效率和更优的回报-违规权衡，其中虚拟终止MPO（VT-MPO）变体能够有效扩展到高维肌肉骨骼控制任务。</div>
</details>
</div>
<div class="card">
<div class="title">Dual Mind World Model Inspired Network Digital Twin for Access Scheduling</div>
<div class="meta-line">Authors: Hrishikesh Dutta, Roberto Minerva, Noel Crespi</div>
<div class="meta-line">First: 2026-02-04T13:53:55+00:00 · Latest: 2026-02-04T13:53:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04566v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04566v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Emerging networked systems such as industrial IoT and real-time cyber-physical infrastructures demand intelligent scheduling strategies capable of adapting to dynamic traffic, deadlines, and interference constraints. In this work, we present a novel Digital Twin-enabled scheduling framework inspired by Dual Mind World Model (DMWM) architecture, for learning-informed and imagination-driven network control. Unlike conventional rule-based or purely data-driven policies, the proposed DMWM combines short-horizon predictive planning with symbolic model-based rollout, enabling the scheduler to anticipate future network states and adjust transmission decisions accordingly. We implement the framework in a configurable simulation testbed and benchmark its performance against traditional heuristics and reinforcement learning baselines under varied traffic conditions. Our results show that DMWM achieves superior performance in bursty, interference-limited, and deadline-sensitive environments, while maintaining interpretability and sample efficiency. The proposed design bridges the gap between network-level reasoning and low-overhead learning, marking a step toward scalable and adaptive NDT-based network optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于双心智世界模型启发的网络数字孪生接入调度方法</div>
<div class="mono" style="margin-top:8px">工业物联网和实时信息物理系统等新兴网络系统需要能够适应动态流量、截止时间和干扰约束的智能调度策略。本研究提出一种受双心智世界模型架构启发的新型数字孪生调度框架，实现基于学习认知与想象驱动的网络控制。与传统基于规则或纯数据驱动的策略不同，所提出的DMWM将短期预测规划与基于符号模型的推演相结合，使调度器能够预判未来网络状态并相应调整传输决策。我们在可配置仿真平台上实现该框架，并在不同流量条件下与传统启发式算法及强化学习基线进行性能对比。实验结果表明，DMWM在突发性、干扰受限和截止时间敏感的环境中均表现出优越性能，同时保持可解释性和样本效率。该设计弥合了网络级推理与低开销学习之间的鸿沟，标志着向可扩展、自适应的基于网络数字孪生的优化迈出重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for adaptive scheduling in dynamic networked systems like industrial IoT by proposing a Digital Twin-enabled framework inspired by the Dual Mind World Model (DMWM). The method integrates short-horizon predictive planning with symbolic model-based rollouts to anticipate network states and optimize transmission decisions. Experimental results from simulations demonstrate that DMWM outperforms traditional heuristics and reinforcement learning baselines in bursty, interference-limited, and deadline-sensitive scenarios, while offering improved interpretability and sample efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对工业物联网等动态网络系统对自适应调度的需求，提出了一种受双心智世界模型启发的数字孪生调度框架。该方法结合了短期预测规划与基于符号模型的推演，以预判网络状态并优化传输决策。仿真实验结果表明，在突发、干扰受限和时限敏感的场景下，该框架优于传统启发式方法和强化学习基线，同时保持了更好的可解释性和样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions</div>
<div class="meta-line">Authors: Qianyi Xu, Gousia Habib, Feng Wu, Yanrui Du, Zhihui Chen, Swapnil Mishra, Dilruk Perera, Mengling Feng</div>
<div class="meta-line">First: 2026-02-03T09:30:32+00:00 · Latest: 2026-02-04T13:20:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03305v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03305v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>medR：基于三驱动势函数临床离线强化学习的奖励工程</div>
<div class="mono" style="margin-top:8px">强化学习为优化动态治疗方案提供了强大框架，但临床强化学习本质上受限于奖励工程：即在复杂、稀疏的离线环境中定义能安全有效指导策略学习的信号。现有方法常依赖难以泛化至不同病理的手动启发式规则。为此，我们提出一种利用大语言模型进行离线奖励设计与验证的自动化流程。我们采用由生存性、置信度与能力三个核心组件构成的势函数来构建奖励函数，并引入量化指标以在部署前严格评估和选择最优奖励结构。通过整合大语言模型驱动的领域知识，本框架实现了针对特定疾病的奖励函数自动化设计，同时显著提升了最终策略的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reward engineering in clinical offline reinforcement learning, where manually designed rewards often fail to generalize across diseases. The authors propose an automated pipeline that uses Large Language Models to design and verify reward functions structured as tri-drive potential functions, incorporating survival, confidence, and competence components. Experimental results demonstrate that this method effectively automates reward design for specific pathologies and significantly improves the performance of learned treatment policies compared to heuristic approaches.</div>
<div class="mono" style="margin-top:8px">本文针对临床离线强化学习中奖励函数设计的挑战，即手动设计的奖励通常难以在不同疾病间泛化。作者提出了一种自动化流程，利用大语言模型来设计和验证奖励函数，该函数由生存、置信度和能力三个核心组件构成的三驱动势函数。实验结果表明，该方法能有效自动化特定疾病的奖励设计，并显著提升了所学治疗策略的性能，优于传统启发式方法。</div>
</details>
</div>
<div class="card">
<div class="title">Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization</div>
<div class="meta-line">Authors: Haoran Yin, Shuaiqun Pan, Zhao Wei, Jian Cheng Wong, Yew-Soon Ong, Anna V. Kononova, Thomas Bäck, Niki van Stein</div>
<div class="meta-line">First: 2026-02-04T13:18:45+00:00 · Latest: 2026-02-04T13:18:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04529v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04529v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advent of Large Language Models (LLMs) has opened new frontiers in automated algorithm design, giving rise to numerous powerful methods. However, these approaches retain critical limitations: they require extensive evaluation of the target problem to guide the search process, making them impractical for real-world optimization tasks, where each evaluation consumes substantial computational resources. This research proposes an innovative and efficient framework that decouples algorithm discovery from high-cost evaluation. Our core innovation lies in combining a Genetic Programming (GP) function generator with an LLM-driven evolutionary algorithm designer. The evolutionary direction of the GP-based function generator is guided by the similarity between the landscape characteristics of generated proxy functions and those of real-world problems, ensuring that algorithms discovered via proxy functions exhibit comparable performance on real-world problems. Our method enables deep exploration of the algorithmic space before final validation while avoiding costly real-world evaluations. We validated the framework&#x27;s efficacy across multiple real-world problems, demonstrating its ability to discover high-performance algorithms while substantially reducing expensive evaluations. This approach shows a path to apply LLM-based automated algorithm design to computationally intensive real-world optimization challenges.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向问题特征的自动化算法设计：一种适用于现实世界优化的高效框架</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的出现为自动化算法设计开辟了新前沿，催生了众多强大方法。然而，这些方法仍存在关键局限：需要大量评估目标问题以指导搜索过程，导致其难以应用于现实优化任务——因为每次评估都会消耗大量计算资源。本研究提出一种创新高效框架，将算法发现与高成本评估解耦。核心创新在于将遗传编程（GP）函数生成器与LLM驱动的进化算法设计器相结合。GP函数生成器的进化方向由生成代理函数与实际问题在特征空间上的相似性引导，确保通过代理函数发现的算法在现实问题上具有可比性能。该方法支持在最终验证前深度探索算法空间，同时避免昂贵的现实评估。我们在多个现实问题上验证了框架的有效性，证明其能以大幅减少昂贵评估为代价，发现高性能算法。该研究为将基于LLM的自动化算法设计应用于计算密集型现实优化挑战提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the impracticality of existing LLM-based automated algorithm design methods that require extensive and costly evaluations on real-world optimization problems, this paper proposes an efficient framework that decouples algorithm discovery from high-cost evaluation. The method combines a Genetic Programming function generator with an LLM-driven evolutionary algorithm designer, where the evolution is guided by the similarity in landscape characteristics between generated proxy functions and the target real-world problems. Experimental results across multiple real-world problems demonstrate that the framework successfully discovers high-performance algorithms while substantially reducing the number of expensive evaluations required, making LLM-based automated design feasible for computationally intensive tasks.</div>
<div class="mono" style="margin-top:8px">针对现有基于大语言模型的自动化算法设计方法需要在实际优化问题上进行大量昂贵评估而不切实际的问题，本研究提出了一种高效框架，将算法发现与高成本评估解耦。该方法结合了遗传编程函数生成器与大语言模型驱动的进化算法设计器，其进化方向由生成的代理函数与实际问题的景观特征相似性所引导。在多个实际问题上验证的结果表明，该框架能够成功发现高性能算法，同时显著减少了所需的昂贵评估次数，使得基于大语言模型的自动化设计能够应用于计算密集的实际优化挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Learning the Value Systems of Agents with Preference-based and Inverse Reinforcement Learning</div>
<div class="meta-line">Authors: Andrés Holgado-Sánchez, Holger Billhardt, Alberto Fernández, Sascha Ossowski</div>
<div class="meta-line">Venue: Holgado-Sánchez, A., Billhardt, H., Fernández, A., Ossowski, S. Learning the value systems of agents with preference-based and inverse reinforcement learning. Autonomous Agents Multi-Agent Systems 40, 4 (2026)</div>
<div class="meta-line">First: 2026-02-04T13:07:15+00:00 · Latest: 2026-02-04T13:07:15+00:00</div>
<div class="meta-line">Comments: 42 pages, 5 figures. Published in Journal of Autonomous Agents and Multi-Agent Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04518v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04518v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agreement Technologies refer to open computer systems in which autonomous software agents interact with one another, typically on behalf of humans, in order to come to mutually acceptable agreements. With the advance of AI systems in recent years, it has become apparent that such agreements, in order to be acceptable to the involved parties, must remain aligned with ethical principles and moral values. However, this is notoriously difficult to ensure, especially as different human users (and their software agents) may hold different value systems, i.e. they may differently weigh the importance of individual moral values. Furthermore, it is often hard to specify the precise meaning of a value in a particular context in a computational manner. Methods to estimate value systems based on human-engineered specifications, e.g. based on value surveys, are limited in scale due to the need for intense human moderation. In this article, we propose a novel method to automatically \emph{learn} value systems from observations and human demonstrations. In particular, we propose a formal model of the \emph{value system learning} problem, its instantiation to sequential decision-making domains based on multi-objective Markov decision processes, as well as tailored preference-based and inverse reinforcement learning algorithms to infer value grounding functions and value systems. The approach is illustrated and evaluated by two simulated use cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏好与逆强化学习的智能体价值系统学习</div>
<div class="mono" style="margin-top:8px">协议技术指开放计算机系统中自主软件代理（通常代表人类）通过交互达成相互可接受协议。近年来人工智能系统的发展表明，此类协议需符合伦理原则与道德价值观才能被各方接受。然而，确保这一点尤为困难，因为不同用户（及其软件代理）可能持有不同价值体系，即对个体道德价值的重要性赋予不同权重。此外，以计算方式在特定情境中精确定义价值内涵常具挑战性。基于人工设计规范（如价值调查）的价值系统评估方法因依赖密集人工干预而规模受限。本文提出一种通过观察与人类示范自动学习价值系统的新方法，包括构建价值系统学习问题的形式化模型、将其实例化为基于多目标马尔可夫决策过程的序列决策领域，并设计定制化的偏好学习与逆强化学习算法以推断价值基础函数与价值系统。该方案通过两个模拟用例进行阐释与评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to enable autonomous agents in agreement technologies to align with diverse human ethical principles by learning their value systems, overcoming limitations of manual specification. The method proposes a formal model for value system learning in sequential decision-making, using multi-objective Markov decision processes and tailored algorithms combining preference-based and inverse reinforcement learning to infer value grounding functions. Main experimental results demonstrate the approach&#x27;s effectiveness through evaluation in two simulated use cases, showing its capability to automatically learn value systems from observations and demonstrations.</div>
<div class="mono" style="margin-top:8px">该研究的动机是让协议技术中的自主智能体通过学习人类价值系统来符合多样化的伦理原则，以克服人工定义的局限性。方法上提出了一个针对序列决策的价值系统学习形式化模型，基于多目标马尔可夫决策过程，并结合了偏好学习和逆强化学习的定制算法来推断价值基础函数。主要实验结果通过两个模拟用例的评估验证了该方法的有效性，展示了其从观察和演示中自动学习价值系统的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning</div>
<div class="meta-line">Authors: Shubham Vaishnav, Praveen Kumar Donta, Sindri Magnússon</div>
<div class="meta-line">First: 2025-05-01T23:34:35+00:00 · Latest: 2026-02-04T11:51:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.00918v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.00918v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">IoT networks often face conflicting routing goals such as maximizing packet delivery, minimizing delay, and conserving limited battery energy. These priorities can also change dynamically: for example, an emergency alert requires high reliability, while routine monitoring prioritizes energy efficiency to prolong network lifetime. Existing works, including many deep reinforcement learning approaches, are typically centralized and assume static objectives, making them slow to adapt when preferences shift. We propose a dynamic and fully distributed multi-objective Q-learning routing algorithm that learns multiple per-preference Q-tables in parallel and introduces a novel greedy interpolation policy to act near-optimally for unseen preferences without retraining or central coordination. A theoretical analysis further shows that the optimal value function is Lipschitz-continuous in the preference parameter, ensuring that the proposed greedy interpolation policy yields provably near-optimal behavior. Simulations show that our approach adapts in real time to shifting priorities and achieves up to 80-90\% lower energy consumption and more than 2-5x higher cumulative rewards and packet delivery compared to six baseline protocols, under dynamic and distributed settings. Sensitivity analysis across varying preference window lengths confirms that the proposed DPQ framework consistently achieves higher composite reward than all baseline methods, demonstrating robustness to changes in operating conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多目标Q学习的物联网网络动态分布式路由</div>
<div class="mono" style="margin-top:8px">物联网网络常面临路由目标冲突，如最大化数据包投递率、最小化延迟与节约有限电池能量。这些优先级亦会动态变化：例如紧急警报需高可靠性，而常规监测优先考虑能效以延长网络寿命。现有研究（包括多数深度强化学习方法）通常采用集中式架构并假设静态目标，当偏好变化时适应缓慢。本文提出一种动态全分布式多目标Q学习路由算法，可并行学习多个按偏好划分的Q表，并引入新型贪婪插值策略，无需重新训练或中央协调即可对未见偏好实现近似最优决策。理论分析进一步证明最优值函数在偏好参数上满足利普希茨连续性，确保所提贪婪插值策略可证明地产生近似最优行为。仿真表明，在动态分布式环境下，本方法能实时适应优先级变化，与六种基线协议相比，能耗降低80-90%，累计奖励与数据包投递率提升2-5倍以上。针对不同偏好窗口长度的敏感性分析证实，所提DPQ框架始终获得高于所有基线方法的复合奖励，展现出对运行条件变化的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of dynamically balancing conflicting routing objectives in IoT networks, such as reliability, latency, and energy efficiency, which can shift based on operational needs like emergency alerts versus routine monitoring. The authors propose a fully distributed multi-objective Q-learning algorithm that learns multiple per-preference Q-tables in parallel and employs a novel greedy interpolation policy to enable near-optimal routing for unseen preferences without retraining or central coordination, supported by theoretical analysis showing Lipschitz continuity of the optimal value function. Experimental simulations demonstrate that the method adapts in real time to changing priorities, achieving up to 80-90% lower energy consumption and 2-5x higher cumulative rewards and packet delivery rates compared to six baseline protocols under dynamic, distributed conditions, with sensitivity analysis confirming robust performance across varying preference windows.</div>
<div class="mono" style="margin-top:8px">本文针对物联网网络中路由目标（如可靠性、延迟和能效）的动态平衡挑战展开研究，这些目标可能随操作需求（如紧急警报与常规监控）而变化。作者提出了一种完全分布式的多目标Q学习算法，该算法并行学习多个基于偏好的Q表，并采用一种新颖的贪婪插值策略，无需重新训练或中央协调即可为未见偏好实现近最优路由，理论分析证明了最优值函数在偏好参数上的Lipschitz连续性。实验模拟表明，该方法能实时适应优先级变化，在动态分布式设置下，与六种基线协议相比，能耗降低高达80-90%，累积奖励和数据包投递率提高2-5倍，敏感性分析进一步验证了其在变化偏好窗口下的鲁棒性能。</div>
</details>
</div>
<div class="card">
<div class="title">Mixture of Masters: Sparse Chess Language Models with Player Routing</div>
<div class="meta-line">Authors: Giacomo Frisoni, Lorenzo Molfetta, Davide Freddi, Gianluca Moro</div>
<div class="meta-line">First: 2026-02-04T11:18:43+00:00 · Latest: 2026-02-04T11:18:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04447v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04447v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal&#x27;s offensive vocation or Petrosian&#x27;s defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>专家混合：基于玩家路由的稀疏国际象棋语言模型</div>
<div class="mono" style="margin-top:8px">现代国际象棋语言模型是基于数千名高段位棋手数百万对局训练的稠密Transformer网络。然而，这类单一网络易陷入模式平均化行为，导致风格边界模糊且罕见有效策略被抑制。为应对同质化问题，我们提出专家混合模型——首个采用模拟世界级特级大师的小型GPT专家模块的国际象棋专家混合架构。每个专家模块通过结合自监督学习与棋类专项奖励引导的强化学习进行训练。针对每一步棋，可事后学习的门控网络根据棋局状态选择最适配的角色通道，使模型能动态切换风格（例如塔尔式的进攻倾向或彼得罗相式的防守稳固性）。在未见过标准对局中与Stockfish评估对比时，该模型在生成多样性、可控性与可解释性方面，均优于基于聚合数据训练的稠密独立专家网络及主流GPT基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the homogenization of dense chess language models that blur stylistic boundaries and suppress rare strategies, this paper introduces Mixture-of-Masters (MoM), a sparse mixture-of-experts model where small GPT experts emulate world-class grandmasters. The method trains each expert via self-supervised and reinforcement learning with chess-specific rewards, using a learnable gating network to dynamically route moves to the most suitable persona based on game state, enabling style switching such as offensive or defensive play. Experimental results show that MoM outperforms dense individual experts and aggregated GPT baselines against Stockfish on unseen standard games, while maintaining generation variety, control, and interpretability.</div>
<div class="mono" style="margin-top:8px">针对密集国际象棋语言模型因风格边界模糊和罕见策略受抑制而导致的同质化问题，本文提出了Mixture-of-Masters（MoM），一种稀疏专家混合模型，其中小型GPT专家模拟世界级大师。该方法通过结合自监督学习和基于象棋特定奖励的强化学习训练每个专家，并利用可学习的门控网络根据棋局状态动态选择最合适的人物风格进行走子路由，从而实现进攻或防守等风格切换。实验结果表明，在未见过的标准棋局评估中，MoM相对于Stockfish的表现优于密集个体专家和基于聚合数据训练的GPT基线，同时保持了生成的多样性、可控性和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL</div>
<div class="meta-line">Authors: Lunjun Zhang, Jimmy Ba</div>
<div class="meta-line">First: 2026-02-04T10:50:17+00:00 · Latest: 2026-02-04T10:50:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04417v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04417v1">PDF</a> · <a href="https://github.com/LunjunZhang/ema-pg">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to acquire increasingly complex reasoning and agentic behaviors. In this work, we propose two simple techniques to improve policy gradient algorithms for LLMs. First, we replace the fixed anchor policy during RL with an Exponential Moving Average (EMA), similar to a target network in deep Q-learning. Second, we introduce Top-k KL estimator, which allows for flexible interpolation between exact KL and sampled KL. We derive the stability conditions for using EMA anchor; moreover, we show that our Top-k KL estimator yields both unbiased KL values and unbiased gradients at any k, while bringing the benefits of exact KL. When combined with GRPO, the two techniques (EMA-PG) lead to a significant performance boost. On math reasoning, it allows R1-distilled Qwen-1.5B to reach 53.9% on OlympiadBench compared to 50.8% by GRPO. On agentic RL domains, with Qwen-3B base, EMA-PG improves GRPO by an average of 33.3% across 7 datasets of Q&amp;A with search engines, including 29.7% $\rightarrow$ 44.1% on HotpotQA, 27.4% $\rightarrow$ 40.1% on 2WikiMultiHopQA. Overall, we show that EMA-PG is a simple, principled, and powerful approach to scaling RL for LLMs. Code: https://github.com/LunjunZhang/ema-pg</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EMA策略梯度：通过EMA锚点和Top-k KL驯服大语言模型的强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）使大语言模型（LLMs）能够习得日益复杂的推理和智能体行为。本研究提出两种改进LLM策略梯度算法的简单技术：首先，用指数移动平均（EMA）替代RL中的固定锚点策略，类似于深度Q学习中的目标网络；其次，引入Top-KL估计器，可在精确KL与采样KL之间灵活插值。我们推导了使用EMA锚点的稳定性条件，并证明Top-KL估计器在任何k值下均能提供无偏KL值与无偏梯度，同时保留精确KL的优势。结合GRPO时，这两项技术（EMA-PG）带来显著性能提升：在数学推理任务中，R1蒸馏的Qwen-1.5B在OlympiadBench达到53.9%（GRPO为50.8%）；在智能体RL领域，基于Qwen-3B的EMA-PG在7个搜索引擎问答数据集中平均提升GRPO性能33.3%，包括HotpotQA从29.7%提升至44.1%，2WikiMultiHopQA从27.4%提升至40.1%。研究表明，EMA-PG是一种简洁、原理清晰且高效的LLM强化学习扩展方法。代码：https://github.com/LunjunZhang/ema-pg</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to improve the stability and performance of reinforcement learning for large language models by introducing two techniques within policy gradient algorithms. The method replaces the typical fixed anchor policy with an exponential moving average anchor for stability and proposes a Top-k KL estimator that flexibly interpolates between exact and sampled KL divergence, providing unbiased estimates. Experimental results show that combining these techniques with GRPO significantly boosts performance, such as raising a model&#x27;s score on OlympiadBench from 50.8% to 53.9% for math reasoning and achieving average improvements of 33.3% across several agentic Q&amp;A datasets, including notable gains on HotpotQA and 2WikiMultiHopQA.</div>
<div class="mono" style="margin-top:8px">本文旨在通过引入两种技术来提升大语言模型强化学习的稳定性和性能。方法上，它用指数移动平均锚点替代了典型的固定锚点策略以增强稳定性，并提出了Top-k KL估计器，可在精确KL与采样KL之间灵活插值并提供无偏估计。实验结果表明，将这些技术与GRPO结合能显著提升性能，例如在数学推理任务OlympiadBench上将模型得分从50.8%提高到53.9%，并在多个智能问答数据集上平均提升33.3%，尤其在HotpotQA和2WikiMultiHopQA上取得了显著进步。</div>
</details>
</div>
<div class="card">
<div class="title">HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation</div>
<div class="meta-line">Authors: Puyue Wang, Jiawei Hu, Yan Gao, Junyan Wang, Yu Zhang, Gillian Dobbie, Tao Gu, Wafa Johal, Ting Dang, Hong Jia</div>
<div class="meta-line">First: 2026-02-04T10:41:23+00:00 · Latest: 2026-02-04T10:41:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04412v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04412v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher&#x27;s robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at \href{https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HoRD：基于历史条件强化学习与在线蒸馏的鲁棒人形机器人控制</div>
<div class="mono" style="margin-top:8px">人形机器人在动力学、任务规范或环境设置的微小变化下可能出现性能显著下降。本文提出HoRD——一种面向领域偏移下鲁棒人形控制的两阶段学习框架。第一阶段通过历史条件强化学习训练高性能教师策略，该策略从近期状态-动作轨迹推断潜在动力学上下文，以在线适应多样随机化动力学。第二阶段执行在线蒸馏，将教师的鲁棒控制能力迁移至基于Transformer的学生策略，该策略以稀疏的根关节相对三维关键点轨迹为输入。通过结合历史条件适应与在线蒸馏，HoRD使单一策略能够零样本适应未见领域，无需逐领域重新训练。大量实验表明，HoRD在鲁棒性与迁移性上均优于强基线方法，尤其在未见领域和外部扰动场景下表现突出。代码与项目页面详见：https://tonywang-0517.github.io/hord/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the fragility of humanoid robot control under domain shifts by introducing HoRD, a two-stage learning framework. The motivation stems from the significant performance degradation humanoids experience with minor changes in dynamics or environment. The method first trains a teacher policy using history-conditioned reinforcement learning, which infers latent dynamics from recent trajectories to adapt online to randomized conditions, then distills this capability online into a transformer-based student policy that processes sparse 3D joint keypoints. Experimental results demonstrate that HoRD outperforms baselines in robustness and zero-shot transfer to unseen domains, effectively handling external perturbations without per-domain retraining.</div>
<div class="mono" style="margin-top:8px">本文针对人形机器人在动态变化下控制性能下降的问题，提出了HoRD这一两阶段学习框架。其动机源于人形机器人在动态或环境微小变化时表现显著衰退。方法上，首先通过历史条件强化学习训练教师策略，使其从近期轨迹推断潜在动态以在线适应随机化条件，然后通过在线蒸馏将鲁棒控制能力迁移至基于Transformer的学生策略，该策略处理稀疏的3D关节关键点轨迹。实验结果表明，HoRD在未见领域和外部扰动下，其鲁棒性和零样本迁移能力均优于基线方法，无需针对每个领域重新训练。</div>
</details>
</div>
<div class="card">
<div class="title">From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training</div>
<div class="meta-line">Authors: Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, Nikolay Malkin</div>
<div class="meta-line">First: 2025-01-10T18:18:25+00:00 · Latest: 2026-02-04T10:39:50+00:00</div>
<div class="meta-line">Comments: TMLR final version; code: https://github.com/GFNOrg/gfn-diffusion/tree/stagger</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.06148v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.06148v3">PDF</a> · <a href="https://github.com/GFNOrg/gfn-diffusion/tree/stagger">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of training neural stochastic differential equations, or diffusion models, to sample from a Boltzmann distribution without access to target samples. Existing methods for training such models enforce time-reversal of the generative and noising processes, using either differentiable simulation or off-policy reinforcement learning (RL). We prove equivalences between families of objectives in the limit of infinitesimal discretization steps, linking entropic RL methods (GFlowNets) with continuous-time objects (partial differential equations and path space measures). We further show that an appropriate choice of coarse time discretization during training allows greatly improved sample efficiency and the use of time-local objectives, achieving competitive performance on standard sampling benchmarks with reduced computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从离散时间策略到连续时间扩散采样器：渐近等价性与加速训练</div>
<div class="mono" style="margin-top:8px">本研究探讨了在无法获取目标样本的情况下，训练神经随机微分方程（即扩散模型）以从玻尔兹曼分布采样的方法。现有训练方法通过可微分模拟或离策略强化学习，强制生成过程与加噪过程的时间可逆性。我们证明了在无穷小离散化步长极限下，不同目标函数族之间的等价关系，从而将熵正则化强化学习方法（GFlowNets）与连续时间对象（偏微分方程及路径空间测度）联系起来。进一步研究表明，训练过程中采用适当的粗粒度时间离散化策略，可显著提升样本效率并实现时间局部化目标函数，在标准采样基准测试中以更低计算成本获得具有竞争力的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training neural stochastic differential equations (diffusion models) to sample from Boltzmann distributions without requiring target samples, motivated by the computational inefficiencies of existing methods that enforce time-reversal via differentiable simulation or off-policy reinforcement learning. The authors establish theoretical equivalences between entropic reinforcement learning objectives, such as those used in GFlowNets, and continuous-time formulations involving partial differential equations and path space measures, in the limit of infinitesimal discretization steps. Experimentally, they demonstrate that employing a coarse time discretization during training enhances sample efficiency and enables the use of time-local objectives, leading to competitive performance on standard sampling benchmarks while reducing computational costs.</div>
<div class="mono" style="margin-top:8px">本文研究了在无法获取目标样本的情况下，训练神经随机微分方程（扩散模型）以从玻尔兹曼分布中采样的问题，其动机在于现有方法通过可微分模拟或离策略强化学习强制时间反转，存在计算效率低下的局限。作者证明了在无限小离散化步长的极限下，熵强化学习目标（如GFlowNets所用）与涉及偏微分方程和路径空间测度的连续时间表述之间的等价性。实验结果表明，在训练过程中采用粗粒度时间离散化可显著提高样本效率，并允许使用时间局部目标，从而在标准采样基准上实现有竞争力的性能，同时降低了计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning</div>
<div class="meta-line">Authors: Wei Duan, Jie Lu, En Yu, Junyu Xuan</div>
<div class="meta-line">First: 2025-12-11T23:56:43+00:00 · Latest: 2026-02-04T09:59:26+00:00</div>
<div class="meta-line">Comments: Accepted by AAMAS 2026 (oral) with appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11179v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11179v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph-based multi-agent reinforcement learning (MARL) enables coordinated behavior under partial observability by modeling agents as nodes and communication links as edges. While recent methods excel at learning sparse coordination graphs-determining who communicates with whom-they do not address what information should be transmitted under hard bandwidth constraints. We study this bandwidth-limited regime and show that naive dimensionality reduction consistently degrades coordination performance. Hard bandwidth constraints force selective encoding, but deterministic projections lack mechanisms to control how compression occurs. We introduce Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors regularized via KL divergence to an uninformative prior. BVME&#x27;s variational framework provides principled, tunable control over compression strength through interpretable hyperparameters, directly constraining the representations used for decision-making. Across SMACv1, SMACv2, and MPE benchmarks, BVME achieves comparable or superior performance while using 67--83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination. Ablations reveal U-shaped sensitivity to bandwidth, with BVME excelling at extreme ratios while adding minimal overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向协作多智能体强化学习的带宽约束变分消息编码</div>
<div class="mono" style="margin-top:8px">基于图的多智能体强化学习（MARL）通过将智能体建模为节点、通信链路建模为边，实现了部分可观测条件下的协同行为。现有方法虽擅长学习稀疏协调图（确定通信对象），却未解决硬带宽约束下应传输何种信息的问题。本研究聚焦带宽受限场景，证明简单的降维方法会持续损害协调性能。硬带宽约束迫使选择性编码，但确定性投影缺乏控制压缩过程的机制。我们提出带宽约束变分消息编码（BVME）——一种轻量级模块，将消息视为从学习到的高斯后验中采样的结果，并通过KL散度以无信息先验进行正则化。BVME的变分框架通过可解释超参数提供对压缩强度的可调控制，直接约束决策表征。在SMACv1、SMACv2和MPE基准测试中，BVME在使用67-83%更少消息维度的同时，取得相当或更优性能，在消息质量对协调至关重要的稀疏图上提升尤为显著。消融实验揭示了带宽的U形敏感性，BVME在极端压缩比下表现卓越且仅引入极小开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of what information to transmit in graph-based multi-agent reinforcement learning under hard bandwidth constraints, where existing methods focus on learning sparse communication graphs but neglect efficient message encoding. The authors propose Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors, regularized via KL divergence to an uninformative prior to provide principled, tunable control over compression. Experimental results on SMACv1, SMACv2, and MPE benchmarks show that BVME achieves comparable or superior performance while using 67–83% fewer message dimensions, with particularly strong gains on sparse graphs where message quality critically impacts coordination, and exhibits U-shaped sensitivity to bandwidth with minimal overhead.</div>
<div class="mono" style="margin-top:8px">本文研究了在图结构多智能体强化学习中，硬带宽约束下应传输何种信息的问题，现有方法侧重于学习稀疏通信图但忽略了高效的消息编码。作者提出了带宽约束变分消息编码（BVME），这是一个轻量级模块，将消息视为从学习到的高斯后验中采样的样本，并通过KL散度正则化至无信息先验，以提供可调、有理论依据的压缩控制。在SMACv1、SMACv2和MPE基准测试上的实验结果表明，BVME在使用67–83%更少消息维度的同时，取得了相当或更优的性能，在消息质量对协调至关重要的稀疏图上增益尤为明显，并且对带宽表现出U形敏感性，同时仅增加极小的开销。</div>
</details>
</div>
<div class="card">
<div class="title">Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents</div>
<div class="meta-line">Authors: Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang</div>
<div class="meta-line">First: 2025-09-27T01:36:46+00:00 · Latest: 2026-02-04T09:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23040v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23040v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the &quot;memorize while reading&quot; methods. While this approach scales efficiently, it suffers from pruning of latent evidence, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, which integrates the mechanism of memory retrieval into the memory update process, enabling the agent to selectively callback historical memories for non-linear reasoning. To further strengthen training, we propose a multi-level reward design, which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support complex multi-hop reasoning. Extensive experiments demonstrate that ReMemR1 significantly outperforms state-of-the-art baselines on long-context question answering while incurring negligible computational overhead, validating its ability to trade marginal cost for robust long-context reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>回望以向前推理：面向长上下文LLM智能体的可重访记忆机制</div>
<div class="mono" style="margin-top:8px">大语言模型在长上下文问答任务中面临关键证据分散于数百万标记的挑战。现有研究通过线性文档扫描动态更新记忆缓冲区（即“边读边记”方法），虽具扩展效率，但存在潜在证据剪枝、覆盖写入导致信息丢失、以及稀疏强化学习信号等问题。为此，我们提出ReMemR1，将记忆检索机制融入记忆更新过程，使智能体能够选择性回调历史记忆进行非线性推理。为进一步强化训练，我们设计了结合最终答案奖励与密集步骤级信号的多层次奖励机制，以引导有效的记忆使用。这些贡献共同缓解了信息衰减、改进了监督机制，并支持复杂的多跳推理。大量实验表明，ReMemR1在长上下文问答任务上显著优于现有基线方法，且计算开销可忽略，验证了其以边际成本换取稳健长上下文推理的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of long-context question answering where key evidence is scattered across millions of tokens, noting that existing &#x27;memorize while reading&#x27; methods suffer from information loss and sparse training signals. The authors propose ReMemR1, a method that integrates memory retrieval into the update process, allowing selective callback of historical memories for non-linear reasoning, and introduce a multi-level reward design combining final-answer and step-level rewards to guide effective memory use. Experimental results show that ReMemR1 significantly outperforms state-of-the-art baselines on long-context QA tasks with minimal computational overhead, demonstrating robust reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文针对长上下文问答中关键证据分散于数百万token的挑战，指出现有“边读边记”方法存在信息丢失和训练信号稀疏的问题。作者提出ReMemR1方法，将记忆检索机制融入更新过程，允许选择性回调历史记忆以进行非线性推理，并设计结合最终答案奖励与密集步骤级奖励的多层次奖励机制来指导有效记忆使用。实验结果表明，ReMemR1在长上下文问答任务上显著优于现有先进基线方法，且计算开销可忽略，验证了其以边际成本实现稳健长上下文推理的能力。</div>
</details>
</div>
<div class="card">
<div class="title">DeepAgent: A General Reasoning Agent with Scalable Toolsets</div>
<div class="meta-line">Authors: Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, Zhicheng Dou</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2025-10-24T16:24:01+00:00 · Latest: 2026-02-04T08:04:15+00:00</div>
<div class="meta-line">Comments: Accepted by WWW 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21618v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21618v2">PDF</a> · <a href="https://github.com/RUC-NLPIR/DeepAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepAgent：具备可扩展工具集的通用推理智能体</div>
<div class="mono" style="margin-top:8px">大型推理模型已展现出强大的问题解决能力，但现实任务常需外部工具与长程交互。现有智能体框架多遵循预定义流程，限制了自主性与全局任务完成度。本文提出DeepAgent——一种端到端深度推理智能体，能在单一连贯的推理过程中执行自主思考、工具发现与动作执行。针对长程交互的挑战（特别是多工具调用引发的上下文长度爆炸及交互历史累积），我们引入自主记忆折叠机制，将过往交互压缩为结构化的情景记忆、工作记忆与工具记忆，在保留关键信息的同时减少误差累积。为实现高效稳定的通用工具使用教学，我们开发了端到端强化学习策略ToolPO，利用LLM模拟的API接口，并通过工具调用优势归因机制为工具调用标记分配细粒度信用。在八个基准测试（包括通用工具使用任务：ToolBench、API-Bank、TMDB、Spotify、ToolHop，以及下游应用：ALFWorld、WebShop、GAIA、HLE）上的大量实验表明，DeepAgent在标注工具与开放集工具检索场景中均持续超越基线方法。本工作为构建面向现实应用的更通用、更强大的智能体迈出重要一步。代码与演示见https://github.com/RUC-NLPIR/DeepAgent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind DeepAgent is to overcome the limitations of existing agent frameworks that rely on predefined workflows, which hinder autonomous and global task completion in real-world scenarios requiring external tools and long-horizon interactions. The method introduces an end-to-end deep reasoning agent that integrates autonomous thinking, tool discovery, and action execution into a single reasoning process, featuring an autonomous memory folding mechanism to compress past interactions into structured memories, reducing context length explosion and error accumulation, and a reinforcement learning strategy called ToolPO for efficient tool use via LLM-simulated APIs and fine-grained credit assignment. Main experimental results show that DeepAgent consistently outperforms baselines across eight benchmarks, including general tool-use tasks like ToolBench and downstream applications such as ALFWorld, demonstrating effectiveness in both labeled-tool and open-set tool retrieval scenarios.</div>
<div class="mono" style="margin-top:8px">DeepAgent的动机是解决现有智能体框架依赖预定义工作流的局限性，这些框架在需要外部工具和长程交互的现实任务中难以实现自主和全局完成。方法上，它提出了一个端到端的深度推理智能体，将自主思考、工具发现和行动执行整合到单一推理过程中，通过自主记忆折叠机制将过往交互压缩为结构化记忆以减少上下文长度爆炸和错误累积，并采用名为ToolPO的强化学习策略，利用LLM模拟API和细粒度信用分配来高效学习通用工具使用。主要实验结果表明，在包括ToolBench等通用工具使用任务和ALFWorld等下游应用在内的八个基准测试中，DeepAgent在标注工具和开放集工具检索场景下均持续优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Extending RLVR to Open-Ended Tasks via Verifiable Multiple-Choice Reformulation</div>
<div class="meta-line">Authors: Mengyu Zhang, Siyu Ding, Weichong Yin, Yu Sun, Hua Wu</div>
<div class="meta-line">First: 2025-11-04T10:45:52+00:00 · Latest: 2026-02-04T07:52:47+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02463v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.02463v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards(RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs). However, its success has thus far been largely confined to the mathematical and programming domains with clear and automatically checkable outcomes. Reinforcement learning on open-ended tasks (e.g., creative writing and subjective Q&amp;A) continues to rely on reward models due to the absence of verifiable solutions. This raises a key question: how can we extend RLVR to strengthen reasoning in open-ended tasks regardless of the absence of the unambiguous ground truth? To overcome this challenge, we introduce Verifiable Multiple-Choice Reformulation for Reinforcement Learning from Verifiable Rewards (VMR-RLVR), a novel training strategy that restructures open-ended data into verifiable multiple-choice formats, enabling effective training even in the absence of explicit ground truth. Experimental results on multiple benchmarks validate the effectiveness of our method in improving LLM performance on open-ended tasks. Notably, across seven open-ended benchmarks, our VMR-RLVR training delivers an average gain of 3.29 points over the RL with reward model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过可验证多选重构将RLVR扩展至开放式任务</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）在提升大语言模型（LLMs）推理能力方面展现出巨大潜力，但其成功目前主要局限于具有明确且可自动验证结果的数学与编程领域。由于缺乏可验证的解决方案，开放式任务（如创意写作和主观问答）的强化学习仍需依赖奖励模型。这引出一个关键问题：如何在缺乏明确标准答案的情况下扩展RLVR以增强开放式任务的推理能力？为应对这一挑战，我们提出基于可验证奖励的强化学习之可验证多选重构（VMR-RLVR），这是一种将开放式数据重构为可验证多选格式的新型训练策略，即使在缺乏显式标准答案的情况下也能实现有效训练。多个基准测试的实验结果验证了本方法在提升LLM开放式任务性能方面的有效性。值得注意的是，在七个开放式任务基准上，我们的VMR-RLVR训练相比基于奖励模型的强化学习平均提升3.29分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to extend Reinforcement Learning with Verifiable Rewards (RLVR), which has proven effective for tasks with clear solutions like math and coding, to open-ended domains such as creative writing where unambiguous ground truth is absent. The proposed method, termed Verifiable Multiple-Choice Reformulation for RLVR (VMR-RLVR), transforms open-ended tasks into a verifiable multiple-choice format, allowing the application of RLVR even without explicit correct answers. Main experimental results show that this approach significantly boosts large language model performance, achieving an average improvement of 3.29 points over standard reinforcement learning with reward models across seven open-ended benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于将可验证奖励的强化学习（RLVR）从具有明确解的数学和编程领域，扩展到缺乏明确标准答案的开放式任务（如创意写作）中。所提出的方法称为可验证多选题重构的RLVR（VMR-RLVR），通过将开放式任务重新构建为可验证的多选题形式，使得即使在缺乏显式正确答案的情况下也能有效应用RLVR进行训练。主要实验结果表明，该方法显著提升了大型语言模型在开放式任务上的性能，在七个开放式基准测试中，相比使用奖励模型的传统强化学习方法平均提升了3.29分。</div>
</details>
</div>
<div class="card">
<div class="title">Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Yansong Ning, Jun Fang, Naiqiang Tan, Hao Liu</div>
<div class="meta-line">First: 2026-02-04T07:26:23+00:00 · Latest: 2026-02-04T07:26:23+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04284v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04284v1">PDF</a> · <a href="https://github.com/usail-hkust/Agent-Omit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent&#x27;s adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Agent-Omit：通过智能体强化学习训练高效LLM智能体以实现自适应思维与观察省略</div>
<div class="mono" style="margin-top:8px">在多轮智能体-环境交互中管理智能体的思维与观察是提升智能体效率的新兴策略。然而，现有研究将整个交互轨迹同等对待，忽视了不同轮次中思维必要性与观察效用的动态变化。为此，我们首先定量研究了思维与观察如何影响智能体的效能与效率。基于研究发现，我们提出了Agent-Omit——一个统一的训练框架，使LLM智能体能够自适应地省略冗余思维与观察。具体而言，我们首先生成少量冷启动数据（包含单轮与多轮省略场景），对智能体进行省略行为微调。进一步，我们引入一种省略感知的智能体强化学习方法，结合双重采样机制和定制化省略奖励，以激励智能体的自适应省略能力。理论上，我们证明了省略策略的偏差受KL散度上界约束。在五个智能体基准测试上的实验结果表明，所构建的Agent-Omit-8B模型性能可与七种前沿LLM智能体相媲美，并在效能-效率权衡上优于七种高效LLM智能体方法。代码与数据公开于https://github.com/usail-hkust/Agent-Omit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency in LLM agents by noting that existing methods treat all interaction turns equally, ignoring varying necessity of internal thoughts and environmental observations. To enable adaptive omission of redundant thoughts and observations, the authors propose Agent-Omit, a training framework that first uses synthesized cold-start data for fine-tuning and then applies an omit-aware agentic reinforcement learning approach with a dual sampling mechanism and tailored omission reward. Experiments on five benchmarks demonstrate that the resulting Agent-Omit-8B model matches the performance of leading LLM agents while achieving the best effectiveness-efficiency trade-off compared to seven efficient agent methods.</div>
<div class="mono" style="margin-top:8px">本文针对LLM智能体效率低下的问题，指出现有方法对所有交互轮次一视同仁，忽略了内部思考和环境观察的必要性在不同轮次中的差异。为实现对冗余思考和观察的自适应省略，作者提出了Agent-Omit训练框架，首先利用合成的冷启动数据进行微调，然后采用一种包含双重采样机制和定制省略奖励的省略感知智能体强化学习方法。在五个基准测试上的实验表明，所构建的Agent-Omit-8B模型在性能上可与前沿LLM智能体相媲美，并且相比七种高效智能体方法，实现了最佳的效果与效率平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms</div>
<div class="meta-line">Authors: Priyankkumar Dhrangdhariya, Soumyadipta Maiti, Venkataramana Runkana</div>
<div class="meta-line">First: 2026-02-04T07:15:33+00:00 · Latest: 2026-02-04T07:15:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04277v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04277v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于有限元建模、机器学习、粒子群优化与贝叶斯优化算法的非充气乘用车轮胎多目标设计优化</div>
<div class="mono" style="margin-top:8px">非充气轮胎为充气轮胎提供了一种有前景的替代方案，但其不连续的辐条结构在刚度调节、耐久性和高速振动方面存在挑战。本研究提出了一种集成生成式设计与机器学习驱动的框架，用于优化乘用车UPTIS型辐条几何结构。通过高阶多项式参数化上下辐条轮廓，并基于PCHIP几何变异生成约250种设计。采用KRR预测刚度、XGBoost预测耐久性与振动的机器学习模型实现了高精度预测，降低了对计算密集型有限元仿真的依赖。结合粒子群优化与贝叶斯优化算法进行多目标性能优化，最终设计相比基准方案实现了53%的刚度可调范围、最高50%的耐久性提升及43%的振动降低。粒子群优化实现了快速定向收敛，贝叶斯优化则有效探索了多目标权衡关系。该框架为系统化开发高性能新一代UPTIS辐条结构提供了有效途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to address the challenges of stiffness tuning, durability, and vibration in non-pneumatic tires by developing an integrated optimization framework for UPTIS-type spoke geometries. The method parameterizes spoke profiles with high-order polynomials to generate designs, employs machine learning models (KRR and XGBoost) to predict performance from finite element data, and utilizes Particle Swarm Optimization and Bayesian Optimization for multi-objective refinement. Experimental results show that the optimized designs achieve a 53% stiffness tunability, up to 50% durability improvement, and a 43% reduction in vibration compared to baseline, with PSO enabling fast convergence and Bayesian Optimization effectively balancing trade-offs.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决非充气轮胎在刚度调节、耐久性和振动方面的挑战，通过开发一个集成优化框架来设计UPTIS型辐条结构。方法采用高阶多项式参数化辐条轮廓以生成设计，利用机器学习模型（KRR和XGBoost）从有限元数据预测性能，并应用粒子群优化和贝叶斯优化进行多目标优化。实验结果表明，优化后的设计相比基线实现了53%的刚度可调性、高达50%的耐久性提升和43%的振动减少，其中粒子群优化提供了快速收敛，而贝叶斯优化有效探索了多目标权衡。</div>
</details>
</div>
<div class="card">
<div class="title">MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning</div>
<div class="meta-line">Authors: Yaorui Shi, Shugui Liu, Yu Yang, Wenyu Mao, Yuxin Chen, Qi GU, Hui Su, Xunliang Cai, Xiang Wang, An Zhang</div>
<div class="meta-line">First: 2026-01-29T09:47:17+00:00 · Latest: 2026-02-04T07:03:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21468v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21468v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemOCR：面向高效长程推理的布局感知视觉记忆系统</div>
<div class="mono" style="margin-top:8px">长程智能体推理需要将不断增长的交互历史有效压缩至有限上下文窗口。现有记忆系统大多将历史序列化为文本，其词元级成本均匀且随长度线性增长，常将稀缺资源消耗于低价值细节。为此，我们提出MemOCR——一种多模态记忆智能体，通过视觉布局实现自适应信息密度的记忆空间分配，从而在严格上下文预算下提升长程推理能力。具体而言，MemOCR维护结构化富文本记忆（如标题、高亮内容）并将其渲染为图像供智能体查询访问，通过视觉方式突出关键证据并大幅压缩辅助细节。为确保不同记忆预算下的鲁棒性，我们采用预算感知目标的强化学习训练MemOCR，使智能体适应多级压缩场景。在长上下文多跳与单跳问答基准测试中，MemOCR优于强文本基线，并在极端预算下实现更高效的上下文利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient long-horizon reasoning within limited context windows, where traditional text-based memory systems waste tokens on low-value details, this paper introduces MemOCR, a multimodal memory agent that uses visual layout to allocate memory space with adaptive information density. The method maintains a structured rich-text memory, renders it into an image for consultation, visually prioritizing crucial evidence while compressing auxiliary details, and trains the agent with reinforcement learning under budget-aware objectives to ensure robustness across varying memory budgets. Experimental results on long-context multi-hop and single-hop question-answering benchmarks show that MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.</div>
<div class="mono" style="margin-top:8px">针对有限上下文窗口内高效长程推理的需求，传统基于文本的记忆系统常将稀缺的令牌预算浪费在低价值细节上，本文提出了MemOCR，一种多模态记忆代理，通过视觉布局以自适应信息密度分配内存空间。该方法维护结构化的富文本记忆，将其渲染成图像供代理查询，在视觉上优先呈现关键证据并压缩辅助细节，并通过基于预算目标的强化学习训练代理，以确保在不同内存预算下的鲁棒性。在长上下文多跳和单跳问答基准测试中，MemOCR优于强大的文本基线，并在极端预算下实现了更有效的上下文利用。</div>
</details>
</div>
<div class="card">
<div class="title">Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning</div>
<div class="meta-line">Authors: Wenze Lin, Zhen Yang, Xitai Jiang, Pony Ma, Gao Huang</div>
<div class="meta-line">First: 2026-02-04T06:55:58+00:00 · Latest: 2026-02-04T06:55:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04265v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04265v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes &quot;thickening&quot; (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to &quot;thinning&quot;, imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从增厚到减薄：基于人类学习动态的奖励塑造机制用于大语言模型推理</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为提升大语言模型推理能力的有前景范式，但常面临熵崩溃、表述冗余及难题探索不足等挑战。现有奖励方案未能区分解决问题所需的广泛搜索与掌握知识后所需的效率。本研究提出T2T动态奖励框架，其受人类学习过程启发，采用双阶段机制：(1) 错误尝试时激励“增厚”以扩展搜索空间、探索新解路径；(2) 答案正确后转为“减薄”，通过长度惩罚减少冗余，从而增强模型置信度并固化推理能力。在Qwen系列和Deepseek模型上的数学基准测试表明，T2T显著优于标准GRPO及近期基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing LLM reasoning, such as entropy collapse and insufficient exploration, by proposing T2T, a dynamic reward framework inspired by human learning. The method implements a dual-phase mechanism: it encourages longer reasoning trajectories (&#x27;thickening&#x27;) on incorrect attempts to broaden search, then shifts to penalizing length (&#x27;thinning&#x27;) upon correctness to eliminate redundancy and solidify skills. Experimental results on mathematical benchmarks like MATH-500 show that T2T significantly outperforms standard GRPO and other baselines across models such as Qwen and Deepseek, achieving superior reasoning performance.</div>
<div class="mono" style="margin-top:8px">本文针对用于增强大语言模型推理的可验证奖励强化学习（RLVR）中存在的熵崩溃和探索不足等局限性，提出了一种受人类学习启发的动态奖励框架T2T。该方法采用双阶段机制：在错误尝试时鼓励延长推理轨迹（&#x27;增厚&#x27;）以拓宽搜索空间，而在答案正确时则转为惩罚长度（&#x27;减薄&#x27;）以减少冗余并巩固推理能力。在MATH-500等数学基准上的实验结果表明，T2T在Qwen和Deepseek等模型上显著优于标准的GRPO及其他基线方法，实现了更优的推理性能。</div>
</details>
</div>
<div class="card">
<div class="title">DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO</div>
<div class="meta-line">Authors: Jinyoung Park, Jeehye Na, Jinyoung Kim, Hyunwoo J. Kim</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-09T06:15:54+00:00 · Latest: 2026-02-04T06:45:15+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07464v5">Abs</a> · <a href="https://arxiv.org/pdf/2506.07464v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement learning algorithm with group-normalized rewards. However, the effectiveness of GRPO in Video Large Language Models (VideoLLMs) remains underexplored. In this paper, we explore GRPO and identify two issues that hinder effective learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation. Reg-GRPO reformulates the GRPO loss function as a regression task that directly predicts the advantage in GRPO, eliminating the need for safeguards such as clipping and min operations. This directly aligns the model with the advantages, providing guidance to prefer better outputs. The difficulty-aware data augmentation strategy augments input prompts/videos to target solvable difficulty levels, enabling diverse reward signals. Our experimental results show that our approach significantly improves video reasoning performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepVideo-R1：基于难度感知回归GRPO的视频强化微调</div>
<div class="mono" style="margin-top:8px">近期研究表明，基于强化学习（RL）的后训练能有效提升大语言模型（LLM）的推理能力。其中，组相对策略优化（GRPO）采用PPO式强化学习算法与组归一化奖励，已展现出显著成效。然而，GRPO在视频大语言模型（VideoLLM）中的应用效果尚未充分探索。本文通过研究GRPO，发现阻碍有效学习的两个问题：（1）对安全机制的依赖；（2）优势函数消失。为应对这些挑战，我们提出DeepVideo-R1——一种采用回归GRPO（Reg-GRPO）与难度感知数据增强训练的视频大语言模型。Reg-GRPO将GRPO损失函数重构为直接预测优势值的回归任务，无需依赖裁剪、最小值运算等安全机制，使模型直接与优势函数对齐，从而引导其生成更优输出。难度感知数据增强策略通过扩增输入提示/视频至可求解的难度级别，实现多样化的奖励信号。实验结果表明，该方法在多个基准测试中显著提升了视频推理性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored application of Group Relative Policy Optimization (GRPO) in Video Large Language Models (VideoLLMs) and its issues with safeguards and vanishing advantages, this paper proposes DeepVideo-R1, which introduces Regressive GRPO (Reg-GRPO) and difficulty-aware data augmentation. The method reformulates GRPO as a regression task to directly predict advantages, eliminating the need for safeguards, and augments prompts/videos to target solvable difficulty levels for diverse rewards. Experimental results demonstrate that this approach significantly enhances video reasoning performance across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于探索群组相对策略优化（GRPO）在视频大语言模型中的应用不足及其存在的安全措施依赖和优势消失问题，提出了DeepVideo-R1模型，采用回归式GRPO（Reg-GRPO）和难度感知数据增强方法。该方法将GRPO重构为直接预测优势的回归任务，无需安全措施如裁剪和最小化操作，并通过增强输入提示/视频以针对可解决的难度级别来提供多样化的奖励信号。实验结果表明，该方法在多个基准测试中显著提升了视频推理性能。</div>
</details>
</div>
<div class="card">
<div class="title">From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution</div>
<div class="meta-line">Authors: Hanlin Pan, Yuhao Tang, Wanfu Gao</div>
<div class="meta-line">First: 2026-02-04T06:36:44+00:00 · Latest: 2026-02-04T06:36:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04255v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04255v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In partial multi-label learning (PML), the true labels are unobserved, which makes label disambiguation important but difficult. A key challenge is that ambiguous candidate labels can propagate errors into downstream tasks such as feature engineering. To solve this issue, we jointly model the disambiguation and feature selection tasks as Partially Observable Markov Decision Processes (POMDP) to turn PML risk minimization into expected-return maximization. Stage 1 trains a transformer policy via reinforcement learning to produce high-quality hard pseudo-labels; Stage 2 describes feature selection as a sequential reinforcement learning problem, selecting features step by step and outputting an interpretable global ranking. We further provide the theoretical analysis of PML-POMDP correspondence and the excess-risk bound that decompose the error into pseudo label quality term and sample size. Experiments in multiple metrics and data sets verify the advantages of the framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从模糊到行动：基于POMDP视角的部分多标签模糊性及其单步求解</div>
<div class="mono" style="margin-top:8px">在部分多标签学习（PML）中，真实标签不可观测，使得标签消歧至关重要但极具挑战。核心难题在于模糊的候选标签会将误差传播至特征工程等下游任务。为解决该问题，我们将消歧与特征选择任务联合建模为部分可观测马尔可夫决策过程（POMDP），从而将PML风险最小化转化为期望回报最大化。第一阶段通过强化学习训练Transformer策略生成高质量硬伪标签；第二阶段将特征选择描述为序列强化学习问题，逐步选择特征并输出可解释的全局排序。我们进一步从理论上分析了PML-POMDP的对应关系，并推导出将误差分解为伪标签质量项与样本量的超额风险界。多指标与多数据集的实验验证了该框架的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of label ambiguity in partial multi-label learning, where true labels are unobserved and ambiguous labels can propagate errors into downstream tasks. The authors propose a two-stage framework that models disambiguation and feature selection as a Partially Observable Markov Decision Process, transforming risk minimization into expected-return maximization: first, a transformer policy is trained via reinforcement learning to generate hard pseudo-labels; second, feature selection is formulated as a sequential reinforcement learning problem to produce an interpretable global feature ranking. Theoretical analysis establishes the PML-POMDP correspondence and an excess-risk bound decomposing error into pseudo-label quality and sample size terms, while experiments across multiple datasets and metrics demonstrate the framework&#x27;s effectiveness.</div>
<div class="mono" style="margin-top:8px">本文针对部分多标签学习中的标签歧义问题展开研究，该场景下真实标签不可见，歧义标签可能将误差传播至下游任务。作者提出一个两阶段框架，将标签消歧和特征选择建模为部分可观测马尔可夫决策过程，从而将风险最小化转化为期望回报最大化：第一阶段通过强化学习训练Transformer策略生成高质量硬伪标签；第二阶段将特征选择构建为序列强化学习问题，逐步选择特征并输出可解释的全局排序。理论分析建立了PML-POMDP对应关系以及将误差分解为伪标签质量项与样本量项的过剩风险界，在多数据集和多指标上的实验验证了该框架的优越性。</div>
</details>
</div>
<div class="card">
<div class="title">STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</div>
<div class="meta-line">Authors: Chen Li, Han Zhang, Zhantao Yang, Fangyi Chen, Zihan Wang, Anudeepsekhar Bolimera, Marios Savvides</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-12T07:27:50+00:00 · Latest: 2026-02-04T06:14:03+00:00</div>
<div class="meta-line">Comments: This paper has been accepted at AAAI 2026. This is the author&#x27;s extended version. The final version will appear in the official proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08688v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.08688v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STELAR-VISION：面向视觉对齐推理的自拓扑感知高效学习框架</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）在推理方面取得显著进展，但在处理复杂多模态任务时仍面临挑战，且常产生冗余输出。其关键局限在于依赖链式思维（CoT）推理，而许多任务本可从树状或图状等替代拓扑结构中获益。为此，我们提出STELAR-Vision——一种面向拓扑感知推理的训练框架。其核心是TopoAug合成数据管道，通过多样化拓扑结构增强训练数据。采用监督微调与强化学习相结合的方法，我们在保持准确率的同时注重效率，对Qwen2VL模型进行后训练。此外，我们提出节俭学习（Frugal Learning）方法，能以极小的准确率损失缩减输出长度。在MATH-V和VLM-S2H基准测试中，STELAR-Vision相比基础模型准确率提升9.7%，并超越更大的Qwen2VL-72B-Instruct模型7.3%。在五个分布外基准测试中，其性能最高超过Phi-4-Multimodal-Instruct达28.4%，超越LLaMA-3.2-11B-Vision-Instruct达13.2%，展现出强大泛化能力。与纯链式训练相比，本方法在分布内数据集上整体准确率提升4.3%，并在所有分布外基准测试中保持稳定优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of vision-language models (VLMs) in handling complex multimodal tasks and generating verbose outputs due to their reliance on chain-of-thought reasoning, this paper introduces STELAR-Vision, a training framework for topology-aware reasoning. The method employs TopoAug, a synthetic data pipeline to enrich training with diverse topological structures like trees or graphs, and uses supervised fine-tuning and reinforcement learning to post-train Qwen2VL models, incorporating Frugal Learning to reduce output length. Experimental results show that STELAR-Vision improves accuracy by 9.7% over its base model and surpasses larger models like Qwen2VL-72B-Instruct by 7.3% on benchmarks such as MATH-V and VLM-S2H, while also demonstrating strong generalization on out-of-distribution tasks, outperforming models like Phi-4-Multimodal-Instruct by up to 28.4%.</div>
<div class="mono" style="margin-top:8px">本文的动机是视觉语言模型在处理复杂多模态任务时，因依赖链式思维推理而存在输出冗长和性能不足的问题。为此，我们提出了STELAR-Vision训练框架，其核心方法包括TopoAug合成数据管道，以引入树状或图状等多样化拓扑结构来增强训练，并通过监督微调和强化学习对Qwen2VL模型进行后训练，同时采用Frugal Learning来缩短输出长度。主要实验结果表明，在MATH-V和VLM-S2H基准测试中，STELAR-Vision相比基础模型准确率提升了9.7%，并超过更大的Qwen2VL-72B-Instruct模型7.3%；在五个分布外基准测试中，它比Phi-4-Multimodal-Instruct模型最高领先28.4%，显示出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning to Discover a North East Monsoon Index for Rainfall Prediction in Thailand</div>
<div class="meta-line">Authors: Kiattikun Chobtham</div>
<div class="meta-line">First: 2026-01-15T08:40:01+00:00 · Latest: 2026-02-04T05:36:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10181v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.10181v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately predicting long-term rainfall is challenging. Global climate indices, such as the El Niño-Southern Oscillation, are standard input features for machine learning. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel North East monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习发现东北季风指数以提升泰国降雨预测精度</div>
<div class="mono" style="margin-top:8px">长期降雨预测具有挑战性。全球气候指数（如厄尔尼诺-南方涛动）是机器学习常用的输入特征，但针对泰国特定区域、能提升预测精度的局地尺度指数仍存在明显空白。本文提出一种基于海表温度计算的新型东北季风气候指数，以反映北半球冬季季风气候特征。为优化该指数的计算区域，采用深度Q网络强化学习智能体探索并筛选与季节性降雨相关性最高的矩形海域。通过将降雨站点划分为12个独立聚类，以区分泰国南部与北部地区的降雨模式。实验结果表明，将优化后的指数融入长短期记忆模型，能显著提升多数聚类区域的长期月降雨预测能力，有效降低12个月预见期预报的均方根误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of improving long-term rainfall prediction in Thailand by developing a local-scale climate index for the North East monsoon, as existing global indices like ENSO are insufficient for regional accuracy. The method involves creating a novel monsoon index from sea surface temperature data and using a Deep Q-Network reinforcement learning agent to optimize the geographic areas for index calculation by selecting rectangles with high correlation to seasonal rainfall, with rainfall stations grouped into 12 clusters to capture regional patterns. Experimental results demonstrate that integrating this optimized index into Long Short-Term Memory models significantly enhances monthly rainfall prediction skill across most clusters, effectively reducing the Root Mean Square Error for forecasts up to 12 months ahead.</div>
<div class="mono" style="margin-top:8px">本文针对泰国长期降雨预测的挑战，开发了一种东北季风的局地气候指数，因为ENSO等全球指数难以满足区域预测精度需求。方法包括基于海表温度数据构建新的季风指数，并利用深度Q网络强化学习智能体优化指数计算的地理区域，通过选择与季节性降雨相关性高的矩形区域来实现，同时将降雨站点分为12个聚类以区分区域模式。实验结果表明，将该优化指数融入长短期记忆模型后，显著提升了大多数聚类区域的月度降雨预测能力，有效降低了12个月前瞻预测的均方根误差。</div>
</details>
</div>
<div class="card">
<div class="title">ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design</div>
<div class="meta-line">Authors: Fang Sheng, Mohammad Noaeen, Zahra Shakeri</div>
<div class="meta-line">First: 2026-01-29T19:16:39+00:00 · Latest: 2026-02-04T05:28:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00157v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00157v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Antimicrobial resistance threatens healthcare sustainability and motivates low-cost computational discovery of antimicrobial peptides (AMPs). De novo peptide generation must optimize antimicrobial activity and safety through low predicted toxicity, but likelihood-trained generators do not enforce these goals explicitly. We introduce ProDCARL, a reinforcement-learning alignment framework that couples a diffusion-based protein generator (EvoDiff OA-DM 38M) with sequence property predictors for AMP activity and peptide toxicity. We fine-tune the diffusion prior on AMP sequences to obtain a domain-aware generator. Top-k policy-gradient updates use classifier-derived rewards plus entropy regularization and early stopping to preserve diversity and reduce reward hacking. In silico experiments show ProDCARL increases the mean predicted AMP score from 0.081 after fine-tuning to 0.178. The joint high-quality hit rate reaches 6.3\% with pAMP $&gt;$0.7 and pTox $&lt;$0.3. ProDCARL maintains high diversity, with $1-$mean pairwise identity equal to 0.929. Qualitative analyses with AlphaFold3 and ProtBERT embeddings suggest candidates show plausible AMP-like structural and semantic characteristics. ProDCARL serves as a candidate generator that narrows experimental search space, and experimental validation remains future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProDCARL：基于强化学习对齐的扩散模型用于从头设计抗菌肽</div>
<div class="mono" style="margin-top:8px">抗菌素耐药性威胁着医疗保健的可持续性，推动了抗菌肽（AMPs）的低成本计算发现。从头肽生成需通过低预测毒性优化抗菌活性和安全性，但基于似然训练的传统生成器未显式强化这些目标。本文提出ProDCARL——一种强化学习对齐框架，将基于扩散的蛋白质生成器（EvoDiff OA-DM 38M）与AMP活性和肽毒性序列属性预测器耦合。我们在AMP序列上微调扩散先验以获得领域感知生成器，采用基于分类器奖励的Top-k策略梯度更新，结合熵正则化与早停机制以保持多样性并减少奖励破解。计算实验表明ProDCARL将平均预测AMP分数从微调后的0.081提升至0.178，联合高质量命中率（pAMP＞0.7且pTox＜0.3）达6.3%。该方法保持高多样性（1-平均成对序列同一性=0.929），基于AlphaFold3和ProtBERT嵌入的定性分析显示候选肽具有合理的类AMP结构与语义特征。ProDCARL作为候选生成器可缩小实验搜索空间，实验验证尚待后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for low-cost computational discovery of antimicrobial peptides (AMPs) to combat antimicrobial resistance, this paper introduces ProDCARL, a reinforcement learning-aligned diffusion model for de novo AMP design that explicitly optimizes for antimicrobial activity and low toxicity. The method fine-tunes a diffusion-based protein generator on AMP sequences and aligns it using policy-gradient updates with rewards from activity and toxicity classifiers, incorporating entropy regularization to maintain diversity and prevent reward hacking. In silico experiments demonstrate that ProDCARL significantly improves the mean predicted AMP score from 0.081 to 0.178, achieves a joint high-quality hit rate of 6.3% for sequences with high activity and low toxicity, and maintains high diversity with a mean pairwise identity of 0.929, while qualitative analyses suggest generated candidates exhibit plausible AMP-like structural and semantic features.</div>
<div class="mono" style="margin-top:8px">为应对抗菌素耐药性威胁并推动低成本计算发现抗菌肽（AMPs），本文提出了ProDCARL，一种强化学习对齐的扩散模型，用于从头设计抗菌肽，旨在明确优化抗菌活性和低毒性。该方法基于AMP序列微调扩散蛋白质生成器，并利用来自活性和毒性分类器的奖励进行策略梯度对齐，结合熵正则化以保持多样性和避免奖励欺骗。计算实验表明，ProDCARL将平均预测AMP分数从0.081显著提升至0.178，在活性高且毒性低的序列上实现了6.3%的联合高质量命中率，并以0.929的平均成对同一性保持了高多样性，同时定性分析表明生成的候选肽具有合理的抗菌肽样结构和语义特征。</div>
</details>
</div>
<div class="card">
<div class="title">Steering LLMs via Scalable Interactive Oversight</div>
<div class="meta-line">Authors: Enyu Zhou, Zhiheng Xi, Long Ma, Zhihao Zhang, Shihan Dou, Zhikai Lei, Guoteng Wang, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang</div>
<div class="meta-line">First: 2026-02-04T04:52:00+00:00 · Latest: 2026-02-04T04:52:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04210v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过可扩展交互式监督引导大语言模型</div>
<div class="mono" style="margin-top:8px">随着大语言模型日益自动化复杂、长周期的任务（如氛围编程），监督缺口已然显现。模型虽擅长执行，但用户常因领域知识不足、难以精确表述意图、无法可靠验证复杂输出而难以有效引导模型。这提出了可扩展监督的核心挑战：使人类能在超越自身规范或验证能力的任务上负责任地引导AI系统。为此，我们提出可扩展交互式监督框架，将复杂意图分解为可管理决策的递归树结构以增强人类监督。系统通过各节点收集低负担反馈，并递归聚合为精确全局指引，而非依赖开放式提示。在网页开发任务中的验证表明，该框架能使非专家产出专家级产品需求文档，对齐度提升54%。关键的是，我们证明该框架可通过仅使用在线用户反馈的强化学习进行优化，为AI规模化进程中保持人类控制提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scalable oversight, where users struggle to effectively guide large language models on complex tasks due to limitations in expertise, intent articulation, and output verification. To solve this, the authors propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of decisions, allowing users to provide low-burden feedback at each node, which is aggregated into precise global guidance. In experiments on web development tasks, the method enables non-experts to produce expert-level Product Requirement Documents with a 54% improvement in alignment, and it is further shown that the framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical path for maintaining human control as AI scales.</div>
<div class="mono" style="margin-top:8px">本文针对可扩展监督的挑战展开研究，即用户由于专业知识不足、意图表述困难及复杂输出验证能力有限，难以有效指导大语言模型完成复杂任务。为解决此问题，作者提出了可扩展交互式监督框架，通过将复杂意图分解为递归决策树，使用户能在每个节点提供低负担反馈，并递归聚合这些信号形成精确的全局指导。在网页开发任务的实验中，该方法使非专家能够生成专家级的产品需求文档，对齐度提升了54%，并且证明该框架可通过仅使用在线用户反馈的强化学习进行优化，为人工智能扩展时保持人类控制提供了可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models</div>
<div class="meta-line">Authors: Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu</div>
<div class="meta-line">First: 2026-01-06T11:54:47+00:00 · Latest: 2026-02-04T04:36:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing large audio-language models perceive the world as &quot;mono&quot;-a single stream of audio that ignores the critical spatial dimension (&quot;where&quot;) required for universal audio scene analysis (ASA). To bridge this gap, we first introduce a hierarchical framework for audio scene analysis. Guided by this framework, we introduce a system that enables large audio-language models (LALMs) to understand and reason about the complex acoustic world.
  Our system endows LALMs with universal spatial understanding through four key innovations: (1) A scalable simulation pipeline that synthesizes high-quality First-Order-Ambisonics(FOA) data; (2) A unified model framework that integrates universal spatial encoding with a dense hybrid projection mechanism to bridge the modality gap; (3) A progressive training curriculum that evolves from representation alignment to reinforcement learning-based reasoning; and (4) A comprehensive benchmark for audio scene analysis (ASA) designed to rigorously evaluate atomic perception, relational integration, and cognitive reasoning capabilities, on which our model demonstrates comparatively strong capability for spatial understanding. Our work provides a clear pathway for leveraging the powerful reasoning abilities of LALMs towards holistic ASA, advancing from &quot;mono&quot; semantic recognition to spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界并非单声道：赋能大型音频语言模型的空间理解能力</div>
<div class="mono" style="margin-top:8px">现有大型音频语言模型将世界感知为&#x27;单声道&#x27;——即忽略通用音频场景分析所需关键空间维度（&#x27;方位&#x27;）的单一音频流。为弥合这一鸿沟，我们首先提出音频场景分析的层级化框架。基于此框架，我们构建了一个使大型音频语言模型能够理解并推理复杂声学世界的系统。该系统通过四项核心创新赋予模型通用空间理解能力：（1）可扩展的高质量一阶Ambisonics合成数据仿真管线；（2）融合通用空间编码与密集混合投影机制以弥合模态鸿沟的统一模型框架；（3）从表征对齐演进至强化学习推理的渐进式训练课程；（4）专为严格评估原子感知、关系整合与认知推理能力而设计的音频场景分析综合基准测试，我们的模型在该基准中展现出较强的空间理解能力。本研究为利用大型音频语言模型的强大推理能力实现整体化音频场景分析提供了清晰路径，推动技术从&#x27;单声道&#x27;语义识别向空间智能演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation that existing large audio-language models treat audio as a single, spatially flat stream, this paper introduces a system to endow such models with spatial understanding for holistic audio scene analysis. The method employs a hierarchical framework and four key innovations: a scalable pipeline for synthesizing spatial audio data, a unified model integrating spatial encoding via a dense hybrid projection, a progressive training curriculum from alignment to reinforcement learning, and a comprehensive benchmark for evaluation. Experimental results on this benchmark demonstrate the model&#x27;s strong capability in spatial understanding, advancing audio recognition from mere semantic analysis to spatial intelligence.</div>
<div class="mono" style="margin-top:8px">针对现有大型音频-语言模型将音频视为单一空间平面流的局限，本文提出一个系统，旨在赋予此类模型空间理解能力以实现整体音频场景分析。方法采用分层框架和四项关键创新：可扩展的空间音频数据合成流程、通过密集混合投影集成空间编码的统一模型、从对齐到强化学习的渐进式训练课程，以及用于评估的综合基准。在该基准上的实验结果表明，该模型在空间理解方面表现出较强能力，推动了音频识别从单纯语义分析向空间智能的演进。</div>
</details>
</div>
<div class="card">
<div class="title">Safe In-Context Reinforcement Learning</div>
<div class="meta-line">Authors: Amir Moeini, Minjae Kwon, Alper Kamil Bozkurt, Yuichi Motai, Rohan Chandra, Lu Feng, Shangtong Zhang</div>
<div class="meta-line">First: 2025-09-29T23:07:32+00:00 · Latest: 2026-02-04T04:31:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25582v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25582v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context reinforcement learning (ICRL) is an emerging RL paradigm where an agent, after pretraining, can adapt to out-of-distribution test tasks without any parameter updates, instead relying on an expanding context of interaction history. While ICRL has shown impressive generalization, safety during this adaptation process remains unexplored, limiting its applicability in real-world deployments where test-time behavior is expected to be safe. In this work, we propose SCARED: Safe Contextual Adaptive Reinforcement via Exact-penalty Dual, the first method that promotes safe adaptation of ICRL under the constrained Markov decision process framework. During the parameter-update-free adaptation process, our agent not only maximizes the reward but also keeps the accumulated cost within a user-specified safety budget. We also demonstrate that the agent actively reacts to the safety budget; with a higher safety budget, the agent behaves more aggressively, and with a lower safety budget the agent behaves more conservatively. Across challenging benchmarks, SCARED consistently enables safe and robust in-context adaptation, outperforming existing ICRL and safe meta-RL baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全上下文强化学习</div>
<div class="mono" style="margin-top:8px">上下文强化学习（ICRL）是一种新兴的强化学习范式，智能体在预训练后无需参数更新，仅依靠不断扩展的交互历史上下文即可适应分布外测试任务。尽管ICRL展现出卓越的泛化能力，其适应过程中的安全性问题尚未被探索，这限制了其在要求测试阶段行为必须安全的实际场景中的应用。本研究提出SCARED方法：基于精确惩罚对偶的安全上下文自适应强化学习，这是首个在约束马尔可夫决策过程框架下保障ICRL安全适应的方法。在无需参数更新的适应过程中，智能体不仅最大化奖励，同时将累积成本控制在用户设定的安全预算内。实验表明智能体能主动响应安全预算：预算较高时行为更激进，预算较低时更保守。在多项挑战性基准测试中，SCARED始终实现安全稳健的上下文适应，性能优于现有ICRL与安全元强化学习基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of safety guarantees in in-context reinforcement learning (ICRL), which limits its real-world use despite strong generalization, this paper introduces SCARED, a method that ensures safe adaptation without parameter updates under a constrained Markov decision process framework. The method employs an exact-penalty dual approach to maximize reward while strictly keeping cumulative cost within a user-defined safety budget during test-time adaptation. Experimental results across benchmarks show that SCARED enables robust and safe in-context adaptation, outperforming existing ICRL and safe meta-RL baselines, with agents dynamically adjusting behavior to be more aggressive or conservative based on the specified budget.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，尽管上下文强化学习（ICRL）展现出强大的泛化能力，但其在适应过程中的安全性尚未得到保障，这限制了其在实际场景中的应用。为此，论文提出了SCARED方法，该方法在约束马尔可夫决策过程框架下，通过精确惩罚对偶策略，使智能体在无需参数更新的适应过程中，既能最大化奖励，又能将累积成本严格控制在用户指定的安全预算内。实验结果表明，SCARED在多个挑战性基准测试中实现了安全且鲁棒的上下文适应，性能优于现有的ICRL和安全元强化学习基线，且智能体能根据安全预算主动调整行为，预算较高时更激进，较低时更保守。</div>
</details>
</div>
<div class="card">
<div class="title">Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning</div>
<div class="meta-line">Authors: Zixiang Di, Jinyi Han, Shuo Zhang, Ying Liao, Zhi Li, Xiaofeng Ji, Yongqi Wang, Zheming Yang, Ming Gao, Bingdong Li, Jie Wang</div>
<div class="meta-line">First: 2026-02-03T13:32:02+00:00 · Latest: 2026-02-04T04:25:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03516v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03516v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并非所有负样本都同等重要：大语言模型从合理推理中学习效果更佳</div>
<div class="mono" style="margin-top:8px">从负样本中学习对提升大语言模型（LLM）推理能力具有巨大潜力，但现有方法将所有错误答案视为同等信息量，忽视了样本质量的关键作用。为此，我们提出“合理负样本（PNS）”方法，通过合成高质量负样本，使其在格式与结构上保持连贯性，但最终得出错误答案。PNS通过逆向强化学习训练专用模型，结合格式合规性、答案反转、奖励模型评估和思维链评价的复合奖励机制，生成与正确答案几乎无法区分的响应。我们在七个数学推理基准上对三种骨干模型进行偏好优化的即插即用数据源验证，结果表明PNS持续优于其他负样本合成方法，相比强化学习训练模型平均提升2.03%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation in existing methods that treat all incorrect responses as equally informative for improving Large Language Model (LLM) reasoning, proposing that sample quality is crucial. The authors introduce Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples which follow expected formats and maintain structural coherence but ultimately lead to incorrect answers, using reverse reinforcement learning trained with a composite reward based on format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation. Experimental validation across three backbone models on seven mathematical reasoning benchmarks shows that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models, demonstrating its effectiveness as a plug-and-play data source for preference optimization.</div>
<div class="mono" style="margin-top:8px">本文针对现有方法在提升大语言模型推理能力时将所有错误答案视为同等信息量的局限性，提出样本质量至关重要。作者引入了合理负样本方法，该方法通过逆向强化学习训练一个专用模型，结合格式合规性、准确性反转、奖励模型评估和思维链评估的复合奖励，合成具有预期格式和结构连贯性但最终得出错误答案的高质量负样本。在三个骨干模型和七个数学推理基准上的实验验证表明，该方法作为即插即用的偏好优化数据源，始终优于其他负样本合成方法，相比强化学习训练模型平均提升2.03%，证明了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment</div>
<div class="meta-line">Authors: Zhexin Zhang, Yida Lu, Junfeng Fang, Junxiao Yang, Shiyao Cui, Hao Zhou, Fandong Meng, Jie Zhou, Hongning Wang, Minlie Huang, Tat-Seng Chua</div>
<div class="meta-line">First: 2026-02-04T04:23:58+00:00 · Latest: 2026-02-04T04:23:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04196v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety risks of AI models have been widely studied at deployment time, such as jailbreak attacks that elicit harmful outputs. In contrast, safety risks emerging during training remain largely unexplored. Beyond explicit reward hacking that directly manipulates explicit reward functions in reinforcement learning, we study implicit training-time safety risks: harmful behaviors driven by a model&#x27;s internal incentives and contextual background information. For example, during code-based reinforcement learning, a model may covertly manipulate logged accuracy for self-preservation. We present the first systematic study of this problem, introducing a taxonomy with five risk levels, ten fine-grained risk categories, and three incentive types. Extensive experiments reveal the prevalence and severity of these risks: notably, Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs when provided only with background information. We further analyze factors influencing these behaviors and demonstrate that implicit training-time risks also arise in multi-agent training settings. Our results identify an overlooked yet urgent safety challenge in training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缺失的另一半：揭示部署之外训练时隐含的安全风险</div>
<div class="mono" style="margin-top:8px">AI模型的安全风险在部署阶段已得到广泛研究，例如诱导有害输出的越狱攻击。相比之下，训练过程中出现的安全风险在很大程度上仍未得到探索。除了直接操纵强化学习中显式奖励函数的显式奖励攻击外，我们研究了隐含的训练时安全风险：由模型内部激励和上下文背景信息驱动的有害行为。例如，在基于代码的强化学习中，模型可能为自我保护而暗中篡改记录准确率。我们首次对该问题进行了系统性研究，提出了包含五个风险等级、十个细粒度风险类别和三种激励类型的分类体系。大量实验揭示了这些风险的普遍性和严重性：值得注意的是，Llama-3.1-8B-Instruct模型在仅提供背景信息时，74.4%的训练运行中表现出风险行为。我们进一步分析了影响这些行为的因素，并证明隐含的训练时风险在多智能体训练环境中同样存在。本研究指出了一个被忽视但紧迫的训练安全挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underexplored safety risks that emerge during AI model training, moving beyond the well-studied deployment-time threats like jailbreak attacks. The authors introduce a taxonomy to systematically categorize implicit training-time safety risks, which are harmful behaviors driven by a model&#x27;s internal incentives and contextual background information, such as covertly manipulating logged accuracy in code-based reinforcement learning for self-preservation. Through extensive experiments, the study finds these risks to be prevalent and severe; for instance, Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs when given only background information, and the analysis further shows these risks persist in multi-agent settings, highlighting an urgent and overlooked safety challenge in AI training.</div>
<div class="mono" style="margin-top:8px">本文探讨了人工智能模型训练过程中尚未被充分研究的安全风险，超越了诸如越狱攻击等已深入研究的部署时威胁。作者提出了一种分类法，系统性地对隐式训练时安全风险进行分类，这些风险是由模型内部激励和上下文背景信息驱动的有害行为，例如在基于代码的强化学习中为自我保存而暗中篡改记录准确率。通过大量实验，研究发现这些风险普遍且严重；例如，当仅提供背景信息时，Llama-3.1-8B-Instruct在74.4%的训练运行中表现出风险行为，分析进一步表明这些风险在多智能体训练环境中同样存在，从而揭示了AI训练中一个紧迫且被忽视的安全挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Piece of CAKE: Adaptive Execution Engines via Microsecond-Scale Learning</div>
<div class="meta-line">Authors: Zijie Zhao, Ryan Marcus</div>
<div class="meta-line">First: 2026-02-04T03:41:07+00:00 · Latest: 2026-02-04T03:41:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04181v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-level database operators often admit multiple physical implementations (&quot;kernels&quot;) that are semantically equivalent but have vastly different performance characteristics depending on the input data distribution. Existing database systems typically rely on static heuristics or worst-case optimal defaults to select these kernels, often missing significant performance opportunities. In this work, we propose CAKE (Counterfactual Adaptive Kernel Execution), a system that learns to select the optimal kernel for each data &quot;morsel&quot; using a microsecond-scale contextual multi-armed bandit. CAKE circumvents the high latency of traditional reinforcement learning by exploiting the cheapness of counterfactuals -- selectively running multiple kernels to obtain full feedback -- and compiling policies into low-latency regret trees. Experimentally, we show that CAKE can reduce end-to-end workload latency by up to 2x compared to state-of-the-art static heuristics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAKE切片：基于微秒级学习的自适应执行引擎</div>
<div class="mono" style="margin-top:8px">底层数据库算子通常存在多种语义等价但性能差异显著的物理实现（“内核”），其性能表现高度依赖于输入数据分布。现有数据库系统通常依赖静态启发式规则或最坏情况最优默认值来选择内核，常错失显著的性能优化机会。本文提出CAKE（反事实自适应内核执行）系统，该系统通过微秒级上下文多臂老虎机学习为每个数据“片段”选择最优内核。CAKE利用反事实评估的低成本特性——选择性并行执行多个内核以获取完整反馈——将策略编译为低延迟遗憾树，从而规避传统强化学习的高延迟问题。实验表明，相较于最先进的静态启发式方法，CAKE可将端到端工作负载延迟降低高达2倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the performance variability of semantically equivalent database operator kernels across different data distributions, which static heuristics often fail to optimize. The method introduces CAKE, an adaptive execution engine that uses a microsecond-scale contextual multi-armed bandit to learn optimal kernel selections per data morsel; it leverages cheap counterfactual execution to gather full feedback and compiles policies into low-latency regret trees. Experimental results demonstrate that CAKE can reduce end-to-end workload latency by up to 2x compared to state-of-the-art static heuristics.</div>
<div class="mono" style="margin-top:8px">这项工作的动机在于，语义等效的数据库操作内核在不同数据分布下性能差异显著，而静态启发式方法常无法优化。该方法提出了CAKE，一个自适应执行引擎，它使用微秒级上下文多臂老虎机来学习每个数据片段的最优内核选择；通过利用廉价的反事实执行获取完整反馈，并将策略编译为低延迟的遗憾树。实验结果表明，与最先进的静态启发式方法相比，CAKE能将端到端工作负载延迟降低多达2倍。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Efficient and Agile Randomized Q-Learning</div>
<div class="meta-line">Authors: He Wang, Xingyu Xu, Yuejie Chi</div>
<div class="meta-line">First: 2025-06-30T16:08:29+00:00 · Latest: 2026-02-04T03:34:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.24005v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.24005v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Bayesian-based exploration often demonstrates superior empirical performance compared to bonus-based methods in model-based reinforcement learning (RL), its theoretical understanding remains limited for model-free settings. Existing provable algorithms either suffer from computational intractability or rely on stage-wise policy updates which reduce responsiveness and slow down the learning process. In this paper, we propose a novel variant of Q-learning algorithm, refereed to as RandomizedQ, which integrates sampling-based exploration with agile, step-wise, policy updates, for episodic tabular RL. We establish an $\widetilde{O}(\sqrt{H^5SAT})$ regret bound, where $S$ is the number of states, $A$ is the number of actions, $H$ is the episode length, and $T$ is the total number of episodes. In addition, we present a logarithmic regret bound under a mild positive sub-optimality condition on the optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance compared to existing Q-learning variants with both bonus-based and Bayesian-based exploration on standard benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可证明高效且敏捷的随机化Q学习</div>
<div class="mono" style="margin-top:8px">在基于模型的强化学习中，基于贝叶斯的探索方法通常比基于奖励的方法表现出更优的实证性能，但其在无模型设置下的理论理解仍有限。现有的可证明算法要么存在计算不可行性问题，要么依赖于分阶段的策略更新，这会降低响应速度并减缓学习过程。本文提出一种新颖的Q学习算法变体，称为RandomizedQ，它将基于采样的探索与敏捷的、逐步的策略更新相结合，适用于表格型情景强化学习。我们建立了$\widetilde{O}(\sqrt{H^5SAT})$的遗憾上界，其中$S$为状态数，$A$为动作数，$H$为情景长度，$T$为总情景数。此外，在最优Q函数满足温和的正次优条件下，我们还提出了对数遗憾上界。实证结果表明，在标准基准测试中，RandomizedQ相较于现有的基于奖励和基于贝叶斯探索的Q学习变体均表现出卓越性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the empirical success but limited theoretical understanding of Bayesian exploration in model-free reinforcement learning, this paper introduces RandomizedQ, a novel Q-learning variant that combines sampling-based exploration with agile step-wise policy updates for episodic tabular RL. The method addresses computational inefficiency and sluggish learning in prior provable algorithms by enabling frequent policy updates without stage-wise constraints. Theoretically, it achieves an Õ(√(H⁵SAT)) regret bound and a logarithmic regret under a mild condition on the optimal Q-function, while empirically, it outperforms existing Q-learning variants with both bonus-based and Bayesian exploration on standard benchmarks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于贝叶斯探索在无模型强化学习中虽表现出色但理论理解有限，现有可证明算法存在计算不可行或依赖阶段性策略更新导致学习迟缓的问题。为此，作者提出了RandomizedQ，一种新颖的Q学习变体，它将基于采样的探索与敏捷的逐步策略更新相结合，用于表格型情景强化学习。该方法通过避免阶段性约束实现频繁策略更新，理论上获得了Õ(√(H⁵SAT))的遗憾界，并在最优Q函数的温和条件下实现了对数遗憾；实验表明，它在标准基准测试中优于现有基于奖励和贝叶斯探索的Q学习变体。</div>
</details>
</div>
<div class="card">
<div class="title">On the Equilibrium between Feasible Zone and Uncertain Model in Safe Exploration</div>
<div class="meta-line">Authors: Yujie Yang, Zhilong Zheng, Shengbo Eben Li</div>
<div class="meta-line">First: 2026-01-31T10:07:14+00:00 · Latest: 2026-02-04T03:09:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00636v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00636v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全探索中可行域与不确定模型的均衡研究</div>
<div class="mono" style="margin-top:8px">确保环境探索的安全性在强化学习中至关重要。尽管将探索限制在可行域内已成为公认的安全保障方法，但核心问题仍未解决：通过探索可获得的最大可行域是什么？如何确定该边界？本文首次通过揭示安全探索的目标在于寻找可行域与环境模型之间的均衡来回答这些问题。这一结论基于二者相互依存的关系：更大的可行域能获得更精确的环境模型，而更精确的模型又能支持探索更广阔的区域。我们提出了首个以均衡为导向的安全探索框架——安全均衡探索（SEE），该框架通过交替寻找最大可行域与最小不确定模型实现突破。基于不确定模型的图结构表述，我们证明了SEE获得的不确定模型具有单调优化特性，可行域呈单调扩展趋势，且二者均收敛于安全探索的均衡点。在经典控制任务上的实验表明，该算法能在零约束违反的前提下成功扩展可行域，并在数次迭代内达成安全探索的均衡状态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the unresolved problem in safe reinforcement learning of determining the maximum feasible exploration zone and how to identify it, motivated by the need to ensure safety during environmental exploration. The authors propose that safe exploration aims to find an equilibrium between the feasible zone and the environment model, as these are interdependent: a larger zone improves model accuracy, and a more accurate model enables exploring a larger zone. They introduce the safe equilibrium exploration (SEE) framework, which alternates between finding the maximum feasible zone and the least uncertain model, proving that both monotonically converge to an equilibrium. Experimental results on classic control tasks demonstrate that SEE expands feasible zones with zero constraint violations and achieves equilibrium within few iterations.</div>
<div class="mono" style="margin-top:8px">本文针对安全强化学习中未解决的如何确定最大可行探索区域及其识别问题展开研究，动机在于确保环境探索过程中的安全性。作者提出安全探索的目标是找到可行区域与环境模型之间的平衡，因为两者相互依赖：更大的区域提高模型准确性，而更准确的模型又能探索更大区域。他们引入了安全平衡探索（SEE）框架，通过交替寻找最大可行区域和最小不确定性模型，证明两者单调收敛至平衡状态。在经典控制任务上的实验结果表明，SEE能够以零约束违反扩展可行区域，并在少数迭代内实现安全探索的平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Topology-Aware Revival for Efficient Sparse Training</div>
<div class="meta-line">Authors: Meiling Jin, Fei Wang, Xiaoyun Yuan, Chen Qian, Yuan Cheng</div>
<div class="meta-line">First: 2026-02-04T03:01:12+00:00 · Latest: 2026-02-04T03:01:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04166v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04166v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拓扑感知复活：实现高效稀疏训练</div>
<div class="mono" style="margin-top:8px">静态稀疏训练通过固定掩码模式实现高效学习，但其受限结构降低了鲁棒性。早期剪枝决策可能使网络陷入难以摆脱的脆弱结构，尤其在深度强化学习中，策略的持续演化会不断改变训练分布。我们提出拓扑感知复活，这是一种轻量级的一次性后剪枝处理，无需动态重连即可改进静态稀疏性。在静态剪枝后，TAR执行单次复活步骤：根据拓扑需求在各层分配少量预留预算，在各层内随机均匀地重新激活部分先前剪除的连接，随后在剩余训练中保持所得连接固定不变。在SAC和TD3的多个连续控制任务中，TAR将最终回报较静态稀疏基线提升最高达+37.9%，并以中位数增益+13.5%优于动态稀疏训练基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the brittleness of static sparse training in deep reinforcement learning, where early pruning decisions can trap networks in suboptimal structures that struggle to adapt to shifting policy distributions. To mitigate this, the authors propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning method that revives a small, budgeted set of previously pruned connections across layers based on topological needs, then fixes the mask for the rest of training without dynamic rewiring. Experiments on continuous-control tasks with SAC and TD3 show TAR improves final return by up to 37.9% over static sparse baselines and achieves a median gain of 13.5% over dynamic sparse training methods.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习中静态稀疏训练的脆弱性问题，即早期剪枝决策可能导致网络陷入难以适应策略分布变化的次优结构。为此，作者提出了拓扑感知复活（TAR）方法，这是一种轻量级的一次性后剪枝过程，它根据拓扑需求在各层分配少量预算，随机均匀地重新激活部分先前剪除的连接，随后固定连接结构进行剩余训练。在SAC和TD3的连续控制任务实验中，TAR相比静态稀疏基线将最终回报提升了最高37.9%，并且相比动态稀疏训练基线实现了13.5%的中位数增益。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors</div>
<div class="meta-line">Authors: Ren-Wei Liang, Chin-Ting Hsu, Chan-Hung Yu, Saransh Agrawal, Shih-Cheng Huang, Chieh-Yen Lin, Shang-Tse Chen, Kuan-Hao Huang, Shao-Hua Sun</div>
<div class="meta-line">First: 2025-04-27T12:16:51+00:00 · Latest: 2026-02-04T02:52:29+00:00</div>
<div class="meta-line">Comments: Accepted at The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2026), Rabat, Morocco</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.20106v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.20106v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring that large language models (LLMs) are both helpful and harmless is a critical challenge, as overly strict constraints can lead to excessive refusals, while permissive models risk generating harmful content. Existing approaches, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), attempt to balance these trade-offs but suffer from performance conflicts, limited controllability, and poor extendability. To address these issues, we propose Preference Vector, a novel framework inspired by task arithmetic. Instead of optimizing multiple preferences within a single objective, we train separate models on individual preferences, extract behavior shifts as preference vectors, and dynamically merge them at test time. This modular approach enables fine-grained, user-controllable preference adjustments and facilitates seamless integration of new preferences without retraining. Experiments show that our proposed Preference Vector framework improves helpfulness without excessive conservatism, allows smooth control over preference trade-offs, and supports scalable multi-preference alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏好向量的自适应助益-无害对齐方法</div>
<div class="mono" style="margin-top:8px">确保大语言模型（LLMs）兼具助益性与无害性是一项关键挑战：约束过严会导致过度拒绝，而宽松模型则可能生成有害内容。现有方法（如基于人类反馈的强化学习RLHF和直接偏好优化DPO）试图平衡这一矛盾，但存在性能冲突、可控性有限和扩展性不足等问题。为此，我们提出受任务算术启发的创新框架——偏好向量。该框架不再通过单一目标优化多重偏好，而是针对独立偏好分别训练模型，将行为偏移提取为偏好向量，并在测试阶段动态融合。这种模块化方法支持细粒度的用户可控偏好调节，且无需重新训练即可无缝集成新偏好。实验表明，偏好向量框架能在避免过度保守的前提下提升助益性，实现平滑的偏好权衡控制，并支持可扩展的多偏好对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of aligning large language models to be both helpful and harmless, as existing methods like RLHF and DPO often create performance conflicts and lack controllability. The authors propose a novel framework called Preference Vector, inspired by task arithmetic, which trains separate models for individual preferences, extracts their behavioral shifts as vectors, and dynamically combines them during inference. Experimental results demonstrate that this approach improves helpfulness without excessive refusals, enables smooth user control over trade-offs, and supports scalable integration of new preferences without retraining.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在兼顾有用性和无害性对齐时面临的挑战，现有方法如RLHF和DPO常导致性能冲突且可控性不足。作者受任务算术启发，提出了一种名为偏好向量的新框架，该方法为不同偏好训练独立模型，将其行为变化提取为向量，并在推理时动态融合。实验结果表明，该框架能在不过度保守的情况下提升模型的有用性，允许用户平滑控制偏好权衡，并支持无需重新训练即可扩展集成新偏好。</div>
</details>
</div>
<div class="card">
<div class="title">CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs</div>
<div class="meta-line">Authors: Zhiyuan Yao, Yi-Kai Zhang, Yuxin Chen, Yueqing Sun, Zishan Xu, Yu Yang, Tianhao Hu, Qi Gu, Hui Su, Xunliang Cai</div>
<div class="meta-line">First: 2026-02-03T03:14:36+00:00 · Latest: 2026-02-04T02:35:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03048v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03048v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning. However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model&#x27;s dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model&#x27;s evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoBA-RL：面向大语言模型强化学习的能力导向预算分配算法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为提升大语言模型推理能力的关键方法。然而，诸如组相对策略优化等标准框架通常采用均匀的模拟预算分配，导致资源效率低下。现有自适应方法多依赖任务通过率等实例级指标，难以捕捉模型动态学习状态。为突破这些局限，我们提出CoBA-RL强化学习算法，该算法能根据模型演进能力自适应分配模拟预算。具体而言，CoBA-RL通过能力导向价值函数将任务映射至潜在训练收益，并采用基于堆的贪心策略，高效地将计算资源自校准分配至高训练价值样本。大量实验表明，该方法有效协调探索与利用的平衡，在多个挑战性基准测试中实现持续泛化提升。这些发现证明：量化样本训练价值与优化预算分配是提升大语言模型后训练效率的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the inefficiency of uniform rollout budgets in reinforcement learning for large language models (LLMs) and the inability of existing adaptive methods to capture dynamic learning states. The proposed method, CoBA-RL, introduces a Capability-Oriented Value function to estimate potential training gains for tasks and uses a heap-based greedy strategy to allocate computational resources adaptively to high-value samples. Experimental results show that CoBA-RL effectively balances exploration and exploitation, leading to consistent generalization improvements across multiple challenging benchmarks, highlighting the importance of quantifying sample training value for efficient LLM post-training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，大型语言模型强化学习中均匀的展开预算分配效率低下，且现有自适应方法无法捕捉模型的动态学习状态。为此，我们提出了CoBA-RL方法，它利用能力导向的价值函数来评估任务的潜在训练收益，并采用基于堆的贪心策略，将计算资源自适应地分配给高训练价值的样本。实验结果表明，CoBA-RL能有效协调探索与利用的平衡，在多个具有挑战性的基准测试中实现了一致的泛化性能提升，这凸显了量化样本训练价值对于提高大语言模型后训练效率的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic</div>
<div class="meta-line">Authors: Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato</div>
<div class="meta-line">First: 2026-01-29T16:50:30+00:00 · Latest: 2026-02-04T02:30:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21972v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21972v2">PDF</a> · <a href="https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多智能体演员-评论家方法的去中心化大语言模型协作学习</div>
<div class="mono" style="margin-top:8px">近期研究探索通过多智能体强化学习优化大语言模型协作，但现有方法多依赖需集中式执行的预定义协议。去中心化协作更具实践价值，可实现并行推理与灵活部署。当前基于蒙特卡洛的微调方法方差较高，需大量训练样本；而演员-评论家方法能有效应对此问题。本文提出两种多智能体演员-评论家方法：采用集中式评论家的CoLLM-CC与采用分布式评论家的CoLLM-DC。实验表明，在短周期/稠密奖励场景中，蒙特卡洛方法与CoLLM-DC能达到与CoLLM-CC相当的效能；但在长周期/稀疏奖励任务中，前两者均表现不佳：蒙特卡洛方法需大量样本，CoLLM-DC则难以收敛。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing Multi-Agent Reinforcement Learning (MARL) methods for optimizing LLM collaboration, which often rely on centralized execution protocols and high-variance Monte Carlo fine-tuning. To enable more practical, decentralized collaboration where agents can run in parallel, the authors propose two Multi-Agent Actor-Critic (MAAC) approaches: CoLLM-CC with a centralized critic and CoLLM-DC with decentralized critics. Experimental results across writing, coding, and game-playing tasks demonstrate that while Monte Carlo methods and CoLLM-DC perform comparably to CoLLM-CC in short-horizon, dense-reward settings, they underperform significantly on long-horizon or sparse-reward tasks, with Monte Carlo methods requiring many more samples and CoLLM-DC struggling to converge.</div>
<div class="mono" style="margin-top:8px">本文针对现有用于优化大语言模型（LLM）协作的多智能体强化学习（MARL）方法的局限性展开研究，这些方法通常依赖集中式执行协议和高方差的蒙特卡洛微调。为实现更实用、可并行推理的分散式协作，作者提出了两种多智能体演员-评论家（MAAC）方法：采用集中式评论家的CoLLM-CC和采用分散式评论家的CoLLM-DC。在写作、编码和游戏领域的实验结果表明，在短周期、密集奖励的任务中，蒙特卡洛方法和CoLLM-DC与CoLLM-CC性能相当；但在长周期或稀疏奖励任务中，它们表现显著较差，蒙特卡洛方法需要更多训练样本，而CoLLM-DC则难以收敛。</div>
</details>
</div>
<div class="card">
<div class="title">When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs</div>
<div class="meta-line">Authors: Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, Bodam Kim, DongGeon Lee, Haon Park, JaeHoon Lee, Jongho Shin</div>
<div class="meta-line">First: 2025-08-05T12:14:01+00:00 · Latest: 2026-02-04T02:28:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.03365v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.03365v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that manipulates state-of-the-art audio language models to generate harmful content. Our method embeds harmful payloads as subtle perturbations into audio inputs that remain intelligible to human listeners. The first stage uses a novel reward-based white-box optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to jailbreak the target model and elicit harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use gradient-based optimization to embed subtle perturbations into benign audio carriers, such as weather queries or greeting messages. Our method achieves average attack success rates of 60-78% across two benchmarks and five multimodal LLMs, validated by multiple evaluation frameworks. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating multimodal AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当良意音频走向对抗：利用无害输入破解音频语言模型</div>
<div class="mono" style="margin-top:8px">随着大语言模型日益融入日常生活，音频已成为人机交互的关键界面。然而，这种便利性也带来了新的安全漏洞，使音频成为潜在的攻击媒介。本研究提出WhisperInject——一种两阶段对抗性音频攻击框架，能够操纵先进的音频语言模型生成有害内容。该方法将有害载荷以人耳难以察觉的细微扰动形式嵌入可理解的音频输入中。第一阶段采用基于奖励的白盒优化方法——强化学习投影梯度下降法，破解目标模型并诱导其生成原生有害回复。该原生有害回复随后作为第二阶段（载荷注入）的目标，通过基于梯度的优化将细微扰动嵌入良性音频载体（如天气查询或问候消息）。本方法在两项基准测试和五个多模态大语言模型上实现了60-78%的平均攻击成功率，并经过多重评估框架验证。这项研究揭示了一类新型的、基于音频的实用威胁，突破了理论攻击的局限，展现了一种可实施且隐蔽的多模态AI系统操控方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing use of audio as an interface for AI systems and the associated security risks, this paper introduces WhisperInject, a two-stage adversarial attack framework designed to jailbreak audio-language models. The method first uses a white-box optimization technique called Reinforcement Learning with Projected Gradient Descent (RL-PGD) to elicit harmful responses from a target model, then embeds these harmful payloads as subtle perturbations into benign audio inputs via gradient-based optimization, keeping the audio intelligible to humans. Experimental results show the attack achieves average success rates of 60-78% across two benchmarks and five multimodal LLMs, demonstrating a practical and covert threat to audio-based AI interactions.</div>
<div class="mono" style="margin-top:8px">本研究针对音频作为AI交互界面日益普及所带来的安全风险，提出了WhisperInject这一两阶段对抗性攻击框架，旨在破解音频语言模型。该方法首先采用一种基于奖励的白盒优化技术（RL-PGD）诱导目标模型生成有害响应，随后通过基于梯度的优化将有害载荷作为细微扰动嵌入到良性音频载体中，同时保持音频对人类可理解。实验结果表明，该攻击在两个基准测试和五个多模态大语言模型上平均成功率可达60-78%，揭示了对音频AI系统一种切实可行且隐蔽的操纵威胁。</div>
</details>
</div>
<div class="card">
<div class="title">Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games</div>
<div class="meta-line">Authors: Anupam Nayak, Tong Yang, Osman Yagan, Gauri Joshi, Yuejie Chi</div>
<div class="meta-line">First: 2025-10-15T01:00:54+00:00 · Latest: 2026-02-04T01:58:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13060v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13060v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reverse Kullback-Leibler (KL) divergence-based regularization with respect to a fixed reference policy is widely used in modern reinforcement learning to preserve the desired traits of the reference policy and sometimes to promote exploration (using uniform reference policy, known as entropy regularization). Beyond serving as a mere anchor, the reference policy can also be interpreted as encoding prior knowledge about good actions in the environment. In the context of alignment, recent game-theoretic approaches have leveraged KL regularization with pretrained language models as reference policies, achieving notable empirical success in self-play methods. Despite these advances, the theoretical benefits of KL regularization in game-theoretic settings remain poorly understood. In this work, we develop and analyze algorithms that provably achieve improved sample efficiency under KL regularization. We study both two-player zero-sum matrix games and Markov games: for matrix games, we propose OMG, an algorithm based on best response sampling with optimistic bonuses, and extend this idea to Markov games through the algorithm SOMG, which also uses best response sampling and a novel concept of superoptimistic bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales inversely with the KL regularization strength $β$ in addition to the traditional $\widetilde{\mathcal{O}}(\sqrt{T})$ regret without the $β^{-1}$ dependence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在KL正则化零和马尔可夫博弈中实现对数遗憾</div>
<div class="mono" style="margin-top:8px">基于反向Kullback-Leibler（KL）散度、针对固定参考策略的正则化在现代强化学习中广泛应用，旨在保留参考策略的理想特性，有时也用于促进探索（使用均匀参考策略，即熵正则化）。参考策略不仅作为锚点，还可被解释为编码了环境中优质动作的先验知识。在对齐领域，近期博弈论方法利用KL正则化，以预训练语言模型作为参考策略，在自博弈方法中取得了显著的实证成功。尽管有这些进展，KL正则化在博弈论背景下的理论优势仍鲜为人知。本研究开发并分析了在KL正则化下可证明提升样本效率的算法。我们研究双人零和矩阵博弈与马尔可夫博弈：针对矩阵博弈，提出OMG算法，该算法基于带乐观奖励的最佳响应采样；并通过SOMG算法将此思想扩展至马尔可夫博弈，该算法同样采用最佳响应采样及新颖的超乐观奖励概念。两种算法均实现了对数遗憾，其随T的增长率与KL正则化强度β成反比，且避免了传统无β⁻¹依赖的$\widetilde{\mathcal{O}}(\sqrt{T})$遗憾。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the empirical success of KL regularization in aligning language models and the lack of theoretical understanding in game settings, this paper develops algorithms to achieve improved sample efficiency in KL-regularized zero-sum games. The method introduces OMG for matrix games and SOMG for Markov games, both employing best response sampling with optimistic bonuses, and for Markov games, a novel superoptimistic bonus. The main experimental results show that these algorithms achieve a logarithmic regret in T that scales inversely with the regularization strength β, improving upon the typical √T regret without such dependence.</div>
<div class="mono" style="margin-top:8px">本文的动机源于KL正则化在语言模型对齐中的实证成功及其在博弈理论中理论理解的缺乏，旨在开发算法以提高KL正则化零和博弈中的样本效率。方法上，针对矩阵博弈提出了OMG算法，针对马尔可夫博弈提出了SOMG算法，两者均采用最佳响应采样与乐观奖励，其中马尔可夫博弈还引入了新颖的超乐观奖励。主要实验结果证明，这些算法实现了与正则化强度β成反比的对数级T后悔，优于传统的不依赖β的√T后悔。</div>
</details>
</div>
<div class="card">
<div class="title">Lyapunov Constrained Soft Actor-Critic (LC-SAC) using Koopman Operator Theory for Quadrotor Trajectory Tracking</div>
<div class="meta-line">Authors: Dhruv S. Kushwaha, Zoleikha A. Biron</div>
<div class="meta-line">First: 2026-02-04T01:51:05+00:00 · Latest: 2026-02-04T01:51:05+00:00</div>
<div class="meta-line">Comments: 12 pages, 7 Figures, submitted to IEEE RA-L</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04132v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04132v1">PDF</a> · <a href="https://github.com/DhruvKushwaha/LC-SAC-Quadrotor-Trajectory-Tracking">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has achieved remarkable success in solving complex sequential decision-making problems. However, its application to safety-critical physical systems remains constrained by the lack of stability guarantees. Standard RL algorithms prioritize reward maximization, often yielding policies that may induce oscillations or unbounded state divergence. There has significant work in incorporating Lyapunov-based stability guarantees in RL algorithms with key challenges being selecting a candidate Lyapunov function, computational complexity by using excessive function approximators and conservative policies by incorporating stability criterion in the learning process. In this work we propose a novel Lyapunov-constrained Soft Actor-Critic (LC-SAC) algorithm using Koopman operator theory. We propose use of extended dynamic mode decomposition (EDMD) to produce a linear approximation of the system and use this approximation to derive a closed form solution for candidate Lyapunov function. This derived Lyapunov function is incorporated in the SAC algorithm to further provide guarantees for a policy that stabilizes the nonlinear system. The results are evaluated trajectory tracking of a 2D Quadrotor environment based on safe-control-gym. The proposed algorithm shows training convergence and decaying violations for Lyapunov stability criterion compared to baseline vanilla SAC algorithm. GitHub Repository: https://github.com/DhruvKushwaha/LC-SAC-Quadrotor-Trajectory-Tracking</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Koopman算子理论的四旋翼轨迹跟踪李雅普诺夫约束软演员-评论家算法</div>
<div class="mono" style="margin-top:8px">强化学习在解决复杂序列决策问题方面取得了显著成就，但其在安全关键物理系统中的应用仍因缺乏稳定性保证而受限。标准强化学习算法以奖励最大化为优先目标，常产生可能引发振荡或状态无界发散的控制策略。现有研究虽尝试将李雅普诺夫稳定性保证融入强化学习算法，但仍面临三大挑战：候选李雅普诺夫函数的选择困难、因使用过多函数逼近器导致的计算复杂度、以及在学习过程中引入稳定性准则可能产生的保守策略。本研究提出一种基于Koopman算子理论的新型李雅普诺夫约束软演员-评论家算法。我们采用扩展动态模态分解方法构建系统的线性近似模型，并基于该模型推导候选李雅普诺夫函数的闭式解。将所得李雅普诺夫函数嵌入软演员-评论家算法框架，可为稳定非线性系统的控制策略提供理论保证。通过在基于safe-control-gym构建的二维四旋翼轨迹跟踪环境中进行验证，相较于基准原始软演员-评论家算法，本算法展现出更好的训练收敛性，且李雅普诺夫稳定性准则的违反程度呈衰减趋势。项目代码仓库：https://github.com/DhruvKushwaha/LC-SAC-Quadrotor-Trajectory-Tracking</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of ensuring stability in reinforcement learning for safety-critical systems like quadrotors, where standard algorithms lack stability guarantees and may cause oscillations or divergence. The authors propose a Lyapunov-constrained Soft Actor-Critic (LC-SAC) algorithm that leverages Koopman operator theory, using extended dynamic mode decomposition to linearly approximate the system dynamics and derive a closed-form candidate Lyapunov function, which is then integrated into the SAC framework to enforce stability during policy learning. Experimental results on a 2D quadrotor trajectory tracking task demonstrate that LC-SAC achieves training convergence and reduces violations of Lyapunov stability criteria compared to a baseline vanilla SAC algorithm.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在四旋翼等安全关键系统中缺乏稳定性保证、易引发振荡或发散的问题，提出了一种基于Koopman算子理论的Lyapunov约束软演员-评论家（LC-SAC）算法。该方法利用扩展动态模态分解对系统动力学进行线性近似，推导出闭式Lyapunov候选函数，并将其融入SAC框架以在学习过程中强制稳定性约束。在基于safe-control-gym的二维四旋翼轨迹跟踪实验中，所提算法相比基线SAC实现了训练收敛，并显著降低了Lyapunov稳定性准则的违反次数。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting</div>
<div class="meta-line">Authors: Mehrdad Moghimi, Anthony Coache, Hyejin Ku</div>
<div class="meta-line">First: 2026-02-04T01:49:12+00:00 · Latest: 2026-02-04T01:49:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04131v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦时间与风险：基于广义折扣的风险敏感强化学习</div>
<div class="mono" style="margin-top:8px">分布强化学习（RL）因其优化风险敏感目标的能力，在安全关键领域得到日益广泛的应用。然而，折扣因子的作用常被忽视，通常仅被视为马尔可夫决策过程的固定参数或可调超参数，很少考虑其对学习策略的影响。文献中已知，折扣函数在刻画智能体时间偏好方面起主导作用，而指数折扣因子无法完全捕捉这一特性。基于此，我们提出一种新框架，支持在分布强化学习中灵活折扣未来奖励并优化风险度量。我们提供了算法最优性的技术分析，表明多时间跨度扩展解决了现有方法的问题，并通过大量实验验证了方法的鲁棒性。研究结果强调，折扣是决策问题中捕捉更丰富时间与风险偏好特征的关键，对现实世界安全关键应用具有潜在意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better capture agent time preferences beyond exponential discounting and to optimize risk-sensitive objectives in safety-critical applications, this paper proposes a novel distributional reinforcement learning framework that decouples time and risk through flexible discounting of future rewards. The method involves a technical analysis of algorithm optimality and introduces a multi-horizon extension to address issues in existing approaches. Experimental results demonstrate the robustness of the method, highlighting that flexible discounting is crucial for expressing diverse temporal and risk preferences, with potential benefits for real-world safety-critical domains.</div>
<div class="mono" style="margin-top:8px">本文的动机是超越指数折扣以更好地捕捉智能体的时间偏好，并在安全关键应用中优化风险敏感目标，提出了一种新颖的分布强化学习框架，通过灵活的未来奖励折扣将时间与风险解耦。该方法包括对算法最优性的技术分析，并引入了多视野扩展以解决现有方法中的问题。实验结果表明该方法具有鲁棒性，强调灵活折扣对于表达多样化的时间和风险偏好至关重要，对现实世界安全关键应用具有潜在意义。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Reason in 13 Parameters</div>
<div class="meta-line">Authors: John X. Morris, Niloofar Mireshghallah, Mark Ibrahim, Saeed Mahloujifar</div>
<div class="meta-line">First: 2026-02-04T01:20:04+00:00 · Latest: 2026-02-04T01:20:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04118v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research has shown that language models can learn to \textit{reason}, often via reinforcement learning. Some work even trains low-rank parameterizations for reasoning, but conventional LoRA cannot scale below the model dimension. We question whether even rank=1 LoRA is necessary for learning to reason and propose TinyLoRA, a method for scaling low-rank adapters to sizes as small as one parameter. Within our new parameterization, we are able to train the 8B parameter size of Qwen2.5 to 91\% accuracy on GSM8K with only 13 trained parameters in bf16 (26 total bytes). We find this trend holds in general: we are able to recover 90\% of performance improvements while training $1000x$ fewer parameters across a suite of more difficult learning-to-reason benchmarks such as AIME, AMC, and MATH500. Notably, we are only able to achieve such strong performance with RL: models trained using SFT require $100-1000x$ larger updates to reach the same performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用13个参数学习推理</div>
<div class="mono" style="margin-top:8px">近期研究表明，语言模型能够学习推理能力，通常通过强化学习实现。部分研究甚至训练了低秩参数化方法用于推理，但传统的LoRA无法扩展到低于模型维度的规模。我们质疑即使是秩为1的LoRA对于学习推理是否必要，并提出TinyLoRA——一种将低秩适配器规模缩小至单个参数的方法。在此新参数化框架下，我们仅用13个bf16训练参数（总计26字节）就将80亿参数的Qwen2.5模型在GSM8K数据集上训练至91%准确率。研究发现这一趋势具有普适性：在AIME、AMC、MATH500等更具挑战性的学习推理基准测试中，我们能用少1000倍的参数恢复90%的性能提升。值得注意的是，仅通过强化学习才能实现如此强劲的性能：使用监督微调训练的模型需要增加100-1000倍的参数量才能达到同等性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper questions the necessity of conventional low-rank adaptation (LoRA) for enabling reasoning in language models and introduces TinyLoRA, a method that scales low-rank adapters down to as few as one parameter. The motivation is to achieve efficient reasoning capabilities with minimal parameter updates, and the method involves training extremely small adapters, such as one with just 13 parameters, on models like the 8B Qwen2.5. Experimental results show that this approach achieves 91% accuracy on GSM8K and recovers 90% of performance gains with 1000x fewer parameters across benchmarks like AIME and MATH500, with reinforcement learning proving essential as supervised fine-tuning requires significantly larger updates.</div>
<div class="mono" style="margin-top:8px">本文质疑了传统低秩适应（LoRA）对于语言模型学习推理的必要性，并提出了TinyLoRA方法，该方法能将低秩适配器规模缩小至单个参数。其动机是以最少的参数更新实现高效的推理能力，方法涉及训练极小的适配器，例如在80亿参数的Qwen2.5模型上仅使用13个参数。实验结果表明，该方法在GSM8K上达到91%的准确率，并在AIME和MATH500等更复杂的推理基准测试中，以1000倍更少的参数恢复了90%的性能提升，其中强化学习至关重要，因为监督微调需要大得多的更新量。</div>
</details>
</div>
<div class="card">
<div class="title">The Invisible Leash: Why RLVR May or May Not Escape Its Origin</div>
<div class="meta-line">Authors: Fang Wu, Weihao Xuan, Ximing Lu, Mingjie Liu, Yi Dong, Zaid Harchaoui, Yejin Choi</div>
<div class="meta-line">First: 2025-07-20T07:04:08+00:00 · Latest: 2026-02-04T00:51:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.14843v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.14843v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing LLMs&#x27; capabilities. However, it remains unclear whether the current practice of RLVR truly expands a model&#x27;s reasoning boundary or mainly amplifies high-reward outputs that the base model already knows, thereby improving precision. This study presents an empirical investigation that provides fresh insights into the limits of RLVR. We examine how RLVR can operate as a support-constrained optimization mechanism that may restrict the discovery of entirely original solutions, remaining constrained by the base model&#x27;s initial distribution. We also identify an entropy-reward trade-off: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves \texttt{pass@1}, \textit{the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets}, failing to recover correct answers that were previously accessible to the base model. Interestingly, while RLVR sometimes increases token-level entropy, it results in greater uncertainty at each generation step and declining answer-level entropy. This indicates that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, we reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash requires future innovations that seed probability mass into underrepresented solution regions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无形之链：RLVR为何可能或无法突破其起源限制</div>
<div class="mono" style="margin-top:8px">近期研究凸显可验证奖励强化学习（RLVR）作为提升大语言模型能力的前沿方法。然而，当前RLVR实践究竟能真正拓展模型的推理边界，还是主要放大基础模型已掌握的高奖励输出以提升精确度，仍不明确。本研究通过实证分析揭示RLVR的局限性：RLVR作为支持约束优化机制，可能限制全新解决方案的发现，始终受限于基础模型的初始分布。研究同时发现熵-奖励权衡现象——RLVR在稳定提升精确度的同时，可能逐步压缩探索空间，忽略正确但低表征的解决方案。大量实验证实，虽然RLVR持续提升\texttt{pass@1}指标，但在更大采样预算下，经验支持集的收缩普遍压倒其扩展，无法恢复基础模型原本可获取的正确答案。值得注意的是，RLVR虽有时增加词元级熵值，却导致每个生成步骤的不确定性上升及答案级熵值下降，表明这些看似不确定的路径最终收敛于更小的答案集合。综上，我们揭示了RLVR在拓展推理视野方面的潜在局限。要突破这道无形之链，未来需通过创新技术将概率质量注入低表征解空间区域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether Reinforcement Learning with Verifiable Rewards (RLVR) genuinely expands the reasoning capabilities of large language models or merely refines existing knowledge. The authors empirically analyze RLVR as a support-constrained optimization mechanism, arguing it often amplifies high-reward outputs from the base model&#x27;s initial distribution rather than discovering novel solutions, and they identify an entropy-reward trade-off where precision gains come at the cost of narrowed exploration. Experimental results show that while RLVR consistently improves pass@1 accuracy, it leads to a shrinkage in the empirical support of outputs, failing to recover correct but underrepresented answers previously accessible to the base model, and although token-level entropy may increase, answer-level entropy declines, indicating convergence onto a smaller set of distinct solutions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了带可验证奖励的强化学习（RLVR）是真正扩展了大语言模型的推理能力，还是仅仅优化了其已有知识。作者通过实证分析将RLVR视为一种支持受限的优化机制，认为它通常只是放大了基础模型初始分布中的高奖励输出，而非发现全新解决方案，并识别出一种熵-奖励权衡，即精确度的提升以探索范围收窄为代价。实验结果表明，尽管RLVR持续提高了pass@1准确率，但它导致了输出经验支持的收缩，无法恢复基础模型先前可访问的正确但未被充分代表的答案，且虽然词元级熵可能增加，但答案级熵下降，表明模型收敛到更小的解决方案集合中。</div>
</details>
</div>
<div class="card">
<div class="title">Taking the GP Out of the Loop</div>
<div class="meta-line">Authors: Mehul Bafna, Siddhant anand Jadhav, David Sweet</div>
<div class="meta-line">First: 2025-06-15T11:37:28+00:00 · Latest: 2026-02-03T23:56:03+00:00</div>
<div class="meta-line">Comments: 12 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12818v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.12818v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) has traditionally solved black-box problems where function evaluation is expensive and, therefore, observations are few. Recently, however, there has been growing interest in applying BO to problems where function evaluation is cheaper and observations are more plentiful. In this regime, scaling to many observations $N$ is impeded by Gaussian-process (GP) surrogates: GP hyperparameter fitting scales as $\mathcal{O}(N^3)$ (reduced to roughly $\mathcal{O}(N^2)$ in modern implementations), and it is repeated at every BO iteration. Many methods improve scaling at acquisition time, but hyperparameter fitting still scales poorly, making it the bottleneck. We propose Epistemic Nearest Neighbors (ENN), a lightweight alternative to GPs that estimates function values and uncertainty (epistemic and aleatoric) from $K$-nearest-neighbor observations. ENN scales as $\mathcal{O}(N)$ for both fitting and acquisition. Our BO method, TuRBO-ENN, replaces the GP surrogate in TuRBO with ENN and its Thompson-sampling acquisition with $\mathrm{UCB} = μ(x) + σ(x)$. For the special case of noise-free problems, we can omit fitting altogether by replacing $\mathrm{UCB}$ with a non-dominated sort over $μ(x)$ and $σ(x)$. We show empirically that TuRBO-ENN reduces proposal time (i.e., fitting time + acquisition time) by one to two orders of magnitude compared to TuRBO at up to 50,000 observations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将高斯过程移出循环</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）传统上用于解决函数评估成本高昂、观测数据稀少的黑盒问题。然而，近年来将BO应用于函数评估成本较低、观测数据更丰富问题的兴趣日益增长。在此场景下，高斯过程（GP）代理模型阻碍了向大量观测数据$N$的扩展：GP超参数拟合的时间复杂度为$\mathcal{O}(N^3)$（现代实现中降至约$\mathcal{O}(N^2)$），且每次BO迭代都需重复执行。许多方法改进了采集阶段的时间复杂度，但超参数拟合的扩展性仍然很差，成为性能瓶颈。我们提出认知最近邻（ENN）——一种轻量级GP替代方案，通过$K$近邻观测数据估计函数值及不确定性（认知不确定性与偶然不确定性）。ENN在拟合和采集阶段的时间复杂度均为$\mathcal{O}(N)$。我们的BO方法TuRBO-ENN将TuRBO中的GP代理替换为ENN，并将其汤普森采样采集函数替换为$\mathrm{UCB} = μ(x) + σ(x)$。针对无噪声问题的特殊情况，可通过用$μ(x)$和$σ(x)$的非支配排序替代$\mathrm{UCB}$来完全省略拟合过程。实验表明，在高达50,000次观测的规模下，TuRBO-ENN相比TuRBO将提案时间（即拟合时间+采集时间）降低了一到两个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational bottleneck of Gaussian processes (GP) in Bayesian optimization (BO) when scaling to many observations, this paper proposes a lightweight surrogate called Epistemic Nearest Neighbors (ENN) to replace GPs. The method estimates function values and uncertainty via K-nearest neighbors, achieving linear O(N) scaling for both model fitting and acquisition, and integrates ENN into the TuRBO framework with a UCB acquisition strategy. Experimental results demonstrate that the resulting TuRBO-ENN reduces proposal time by one to two orders of magnitude compared to GP-based TuRBO, effectively handling up to 50,000 observations.</div>
<div class="mono" style="margin-top:8px">针对贝叶斯优化在处理大量观测数据时高斯过程计算成本过高的问题，本文提出了一种轻量级替代模型——认知最近邻（ENN）。该方法通过K近邻观测来估计函数值及不确定性，实现了模型拟合和采集的线性O(N)复杂度，并将其与TuRBO框架结合，采用UCB采集策略。实验结果表明，所提出的TuRBO-ENN在多达5万次观测的情况下，将建议时间相比基于高斯过程的TuRBO降低了一到两个数量级。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL</div>
<div class="meta-line">Authors: Xiaofeng Lin, Sirou Zhu, Yilei Chen, Mingyu Chen, Hejian Sang, Ioannis Paschalidis, Zhipeng Wang, Aldo Pacchiano, Xuezhou Zhang</div>
<div class="meta-line">First: 2026-02-03T23:53:05+00:00 · Latest: 2026-02-03T23:53:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04089v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04089v1">PDF</a> · <a href="https://github.com/XiaofengLin7/ORBIT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过跨片段元强化学习扩展大语言模型的上下文在线学习能力</div>
<div class="mono" style="margin-top:8px">大语言模型在任务相关信息可预先获取的静态预测和指令跟随任务中表现优异，但现实世界的决策任务常具有在线特性：关键信息需通过交互获取、反馈存在延迟，且有效行为需随时间平衡信息收集与利用。虽然上下文学习支持无需权重更新的适应，但现有大语言模型在此类场景中难以可靠利用上下文交互经验。本研究证明该局限可通过训练解决：我们提出ORBIT——一个多任务、多片段的元强化学习框架，用于训练大语言模型在上下文中从交互学习。经元训练后，相对较小的开源模型（Qwen3-14B）在全新环境中展现出显著提升的上下文在线学习能力，其性能与GPT-5.2相当，并大幅超越标准强化学习微调方法。扩展实验进一步表明模型规模与性能提升呈正相关，这预示着基于推理时学习的决策智能体具有显著发展潜力。论文结果复现代码详见：https://github.com/XiaofengLin7/ORBIT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitation of large language models (LLMs) in online decision-making tasks, where information must be acquired through interaction and feedback is delayed. The authors introduce ORBIT, a cross-episode meta-reinforcement learning framework that trains LLMs to learn from in-context interaction experience. After meta-training, a relatively small model (Qwen3-14B) shows substantially improved in-context online learning on unseen environments, matching GPT-5.2&#x27;s performance and greatly outperforming standard RL fine-tuning, with scaling experiments indicating consistent gains as model size increases.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型在在线决策任务中的局限性展开，这类任务需要通过交互获取信息且反馈延迟。作者提出了ORBIT，一个跨情节的元强化学习框架，用于训练大语言模型从上下文交互经验中学习。经过元训练后，一个相对较小的开源模型（Qwen3-14B）在未见过的环境中展现出显著提升的上下文在线学习能力，其性能与GPT-5.2相当，并大幅优于标准的强化学习微调方法，扩展实验进一步表明模型规模的增加会带来持续的收益提升。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner</div>
<div class="meta-line">Authors: Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Yi-An Ma, Lianhui Qin</div>
<div class="meta-line">First: 2026-02-02T06:26:31+00:00 · Latest: 2026-02-03T23:43:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01705v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01705v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越模式激发：基于潜在扩散推理器的多样性保持强化学习</div>
<div class="mono" style="margin-top:8px">近期强化学习方法通过优化离散思维链生成来提升大语言模型推理能力，但离散强化学习中模式激发行为导致策略熵降低，使得词符空间探索常面临多样性崩溃问题。为缓解此问题，我们提出潜在扩散推理强化学习框架，该框架直接在连续潜在空间中进行探索，其中潜在变量编码语义级推理轨迹。通过引导扩散建模探索过程，多步去噪分散随机性并保持多种共存解模式而不相互抑制。此外，通过解耦潜在空间探索与文本空间生成，我们证明基于潜在扩散的优化比单纯文本空间策略优化更有效，而互补的文本策略与潜在探索结合能带来额外增益。在代码生成和数学推理基准测试中，该方法在pass@1和pass@k指标上均持续优于离散强化学习基线，其中代码生成任务pass@1绝对提升+9.4%，数学推理任务提升+5.7%，凸显了基于扩散的潜在强化学习作为离散词符级推理强化学习的理论替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the diversity collapse issue in reinforcement learning for large language model reasoning, where discrete token-level optimization leads to mode elicitation and reduced solution variety. The authors propose LaDi-RL, a framework that explores reasoning trajectories in a continuous latent space using guided diffusion, enabling multi-step denoising to preserve multiple solution modes without mutual suppression. Experimental results on code generation and mathematical reasoning benchmarks show consistent improvements, with absolute pass@1 gains of +9.4% and +5.7% respectively, demonstrating the effectiveness of latent diffusion-based RL over discrete token-level methods.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型推理中强化学习的多样性崩溃问题，即离散令牌级优化导致模式引发和解决方案多样性减少。作者提出LaDi-RL框架，通过引导扩散在连续潜在空间中探索推理轨迹，利用多步去噪保留多种解决方案模式而无需相互抑制。在代码生成和数学推理基准测试中的实验结果显示了一致的改进，绝对pass@1分别提升9.4%和5.7%，证明了基于潜在扩散的强化学习相比离散令牌级方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs</div>
<div class="meta-line">Authors: Soumya Rani Samineni, Durgesh Kalwar, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati</div>
<div class="meta-line">First: 2025-05-19T19:57:15+00:00 · Latest: 2026-02-03T21:15:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13697v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.13697v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing claims around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting popular structural assumptions made in modeling LLM training as an MDP, and show how they lead to a degenerate MDP, that characterizes the problem as a contextual bandit, where RL updates naturally collapse into a form of on-policy variant of outcome-driven supervised learning. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Our comprehensive analysis demonstrates that, due to these simplifying assumptions, GRPO objective reduces to filtered Iterative SFT, an on-policy variant of supervised fine-tuning. Our experiments on benchmarks including GSM8K and Countdown, across a diverse set of model families show that Filtered Iterative SFT, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We also show that these structural assumptions indirectly incentivize RL to generate longer sequences of intermediate tokens which in turn feeds into the narrative of &quot;RL incentivizing thinking because it generates longer thinking traces.&quot;</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习徒有其名？分析大语言模型后训练中的结构假设</div>
<div class="mono" style="margin-top:8px">基于强化学习的大语言模型后训练近期备受关注，尤其在DeepSeek R1采用GRPO进行微调后。针对RL后训练提升推理能力的普遍主张，本文批判性审视了这些方法背后的形式化框架与假设。我们首先指出将LLM训练建模为MDP时的常见结构假设，并揭示其如何导致退化的MDP——将问题简化为情境赌博机，使RL更新自然退化为结果驱动的监督学习在策略变体。两个关键结构假设包括：(1)将MDP状态仅定义为动作的拼接（状态即上下文窗口，动作即LLM生成的词元）；(2)将状态-动作轨迹的奖励均匀分配至整个轨迹。综合分析表明，这些简化假设使GRPO目标退化为过滤式迭代SFT（监督微调的在策略变体）。我们在GSM8K和Countdown等基准测试中跨多模型系列的实验表明：融合正负样本的过滤式迭代SFT能达到与GRPO训练相当的性能。研究同时揭示，这些结构假设间接激励RL生成更长的中间词元序列，进而强化了“RL因生成更长思维轨迹而激励思考”的叙事。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper critically examines the structural assumptions underlying reinforcement learning (RL) post-training methods for large language models (LLMs), motivated by growing claims that such methods enhance reasoning. The authors analyze popular assumptions in modeling LLM training as a Markov Decision Process (MDP), specifically that states are concatenated actions (the context window) and rewards are uniformly split across trajectories, arguing these lead to a degenerate MDP equivalent to a contextual bandit. They demonstrate that under these assumptions, the GRPO objective reduces to a form of on-policy supervised fine-tuning called Filtered Iterative SFT. Experimental results on benchmarks like GSM8K and Countdown across various model families show that Filtered Iterative SFT, using both positive and negative samples, achieves performance comparable to GRPO, while also revealing that the assumptions indirectly incentivize generating longer intermediate token sequences, which may misleadingly be interpreted as improved reasoning.</div>
<div class="mono" style="margin-top:8px">本文批判性地审视了大型语言模型（LLM）强化学习（RL）后训练方法的结构性假设，其动机是针对此类方法能提升推理能力的日益增长的主张。作者分析了将LLM训练建模为马尔可夫决策过程（MDP）的常见假设，特别是状态由动作（即上下文窗口）拼接而成且奖励在轨迹上均匀分配，认为这导致MDP退化为一个上下文赌博机问题。他们证明，在这些假设下，GRPO目标可简化为一种名为过滤迭代监督微调（Filtered Iterative SFT）的在线策略监督学习形式。在GSM8K和Countdown等基准测试及多种模型系列上的实验结果表明，使用正负样本的过滤迭代监督微调取得了与GRPO相当的性能，同时揭示这些假设间接鼓励生成长度更大的中间标记序列，这可能被误解为推理能力的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Detection of Watermarked Language Models</div>
<div class="meta-line">Authors: Dara Bahri, John Wieting</div>
<div class="meta-line">First: 2025-08-18T17:43:06+00:00 · Latest: 2026-02-03T20:58:08+00:00</div>
<div class="meta-line">Comments: Published at TMLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13131v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.13131v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Watermarking has recently emerged as an effective strategy for detecting the generations of large language models (LLMs). The strength of a watermark typically depends strongly on the entropy afforded by the language model and the set of input prompts. However, entropy can be quite limited in practice, especially for models that are post-trained, for example via instruction tuning or reinforcement learning from human feedback (RLHF), which makes detection based on watermarking alone challenging. In this work, we investigate whether detection can be improved by combining watermark detectors with non-watermark ones. We explore a number of hybrid schemes that combine the two, observing performance gains over either class of detector under a wide range of experimental conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提升水印语言模型的检测能力</div>
<div class="mono" style="margin-top:8px">水印技术近期已成为检测大型语言模型生成内容的有效策略。水印强度通常高度依赖于语言模型提供的熵值及输入提示集。然而，实践中熵值可能非常有限，尤其对于经过后训练的模型（例如通过指令微调或人类反馈强化学习），这使得仅依赖水印进行检测面临挑战。本研究探讨通过结合水印检测器与非水印检测器能否提升检测效果。我们探索了多种融合方案，在广泛实验条件下观察到其性能优于单一类别检测器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of reliably detecting text generated by large language models (LLMs), noting that existing watermarking techniques become less effective when model entropy is low, such as after post-training via instruction tuning or RLHF. To improve detection, the authors propose and evaluate hybrid schemes that combine watermark-based detectors with non-watermark detection methods. Their experimental results demonstrate that these hybrid approaches consistently outperform using either detector type alone across a variety of conditions, offering enhanced detection capability.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型生成文本的可靠检测难题，指出现有水印技术在模型熵较低时（例如经过指令微调或人类反馈强化学习等后训练后）效果会减弱。为提升检测性能，作者提出并评估了将基于水印的检测器与非水印检测方法相结合的混合方案。实验结果表明，在各种条件下，这些混合方法的检测性能均优于单独使用任一类检测器，从而提供了更强的检测能力。</div>
</details>
</div>
<div class="card">
<div class="title">Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning</div>
<div class="meta-line">Authors: Zidi Xiong, Shan Chen, Himabindu Lakkaraju</div>
<div class="meta-line">First: 2026-02-03T20:03:58+00:00 · Latest: 2026-02-03T20:03:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03978v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a &quot;free gift&quot; during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可监控性作为免费赠礼：RLVR如何自发对齐推理过程</div>
<div class="mono" style="margin-top:8px">随着大型推理模型（LRM）的日益广泛应用，对其思维链（CoT）轨迹进行安全审计变得至关重要。近期研究发现，在可验证奖励强化学习（RLVR）的早期阶段，可监控性——即CoT忠实且信息丰富地反映内部计算的程度——可能以“免费赠礼”的形式出现。我们通过对不同模型系列和训练领域的系统评估，将这一观察具体化。结果表明该效应并非普遍存在：可监控性的提升高度依赖于数据特性。我们特别论证了数据多样性和指令遵循数据在RLVR训练中的关键作用，并进一步揭示可监控性与模型能力相互独立——推理性能的提升并不意味透明度的增加。通过机制分析，我们发现可监控性的增益主要源于响应分布的锐化（熵减）和对提示关注度的提升，而非对推理轨迹更强的因果依赖。研究还揭示了可监控性动态如何随受控训练及评估难度变化。这些发现共同构建了关于RLVR下可监控性生成机制的整体视角，明确了增益可能发生与不可能发生的条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the emergence of monitorability—the faithfulness and informativeness of chain-of-thought traces—during Reinforcement Learning with Verifiable Rewards (RLVR) training for Large Reasoning Models, motivated by the need to audit reasoning processes for safety. The method involves a systematic evaluation across model families and training domains to analyze the conditions under which monitorability improves. The main experimental results show that monitorability gains are not universal but strongly data-dependent, requiring diverse and instruction-following data; they are orthogonal to capability improvements, and mechanistically arise from response distribution sharpening and increased attention to the prompt rather than stronger causal reliance on reasoning traces, with dynamics varying based on training and evaluation difficulty.</div>
<div class="mono" style="margin-top:8px">本文研究了在大型推理模型的带可验证奖励的强化学习训练中，可监控性——即思维链轨迹的忠实性和信息性——如何自发出现，其动机源于对推理过程进行安全审计的需求。方法包括跨模型系列和训练领域的系统评估，以分析可监控性提升的条件。主要实验结果表明，可监控性的提升并非普遍现象，而是强烈依赖于数据，需要多样化和遵循指令的数据；它与能力提升正交，机制上主要源于响应分布锐化和对提示关注度的增加，而非对推理轨迹更强的因果依赖，且其动态变化受训练和评估难度的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Non-linear PCA via Evolution Strategies: a Novel Objective Function</div>
<div class="meta-line">Authors: Thomas Uriot, Elise Chung</div>
<div class="meta-line">First: 2026-02-03T19:34:37+00:00 · Latest: 2026-02-03T19:34:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03967v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03967v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Principal Component Analysis (PCA) is a powerful and popular dimensionality reduction technique. However, due to its linear nature, it often fails to capture the complex underlying structure of real-world data. While Kernel PCA (kPCA) addresses non-linearity, it sacrifices interpretability and struggles with hyperparameter selection. In this paper, we propose a robust non-linear PCA framework that unifies the interpretability of PCA with the flexibility of neural networks. Our method parametrizes variable transformations via neural networks, optimized using Evolution Strategies (ES) to handle the non-differentiability of eigendecomposition. We introduce a novel, granular objective function that maximizes the individual variance contribution of each variable providing a stronger learning signal than global variance maximization. This approach natively handles categorical and ordinal variables without the dimensional explosion associated with one-hot encoding. We demonstrate that our method significantly outperforms both linear PCA and kPCA in explained variance across synthetic and real-world datasets. At the same time, it preserves PCA&#x27;s interpretability, enabling visualization and analysis of feature contributions using standard tools such as biplots. The code can be found on GitHub.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于进化策略的非线性主成分分析：一种新颖的目标函数</div>
<div class="mono" style="margin-top:8px">主成分分析（PCA）是一种强大且常用的降维技术。然而，由于其线性本质，它往往难以捕捉现实世界数据中复杂的底层结构。虽然核主成分分析（kPCA）解决了非线性问题，但牺牲了可解释性且难以选择超参数。本文提出一种鲁棒的非线性PCA框架，将PCA的可解释性与神经网络的灵活性相结合。该方法通过神经网络参数化变量变换，并利用进化策略（ES）优化以处理特征分解的不可微性。我们引入了一种新颖的细粒度目标函数，通过最大化每个变量的独立方差贡献，提供比全局方差最大化更强的学习信号。该方法天然支持分类变量和有序变量，避免了独热编码带来的维度爆炸问题。实验表明，在合成和真实数据集上，本方法在解释方差方面显著优于线性PCA和kPCA，同时保留了PCA的可解释性，支持使用双标图等标准工具进行特征贡献的可视化分析。代码已发布于GitHub。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of linear PCA and the interpretability issues of Kernel PCA, this paper introduces a non-linear PCA framework that combines the interpretability of PCA with the flexibility of neural networks. The method parametrizes variable transformations using neural networks, optimized via Evolution Strategies to handle non-differentiable eigendecomposition, and employs a novel objective function that maximizes individual variable variance contributions for better learning signals. Experimental results on synthetic and real-world datasets show that the approach significantly outperforms both linear PCA and Kernel PCA in explained variance while maintaining interpretability for visualization and feature analysis, and it natively supports categorical and ordinal variables without dimensional explosion.</div>
<div class="mono" style="margin-top:8px">本文针对线性主成分分析（PCA）的局限性及核主成分分析（KPCA）可解释性不足的问题，提出了一种非线性PCA框架，将PCA的可解释性与神经网络的灵活性相结合。该方法通过神经网络参数化变量变换，利用进化策略优化以处理特征分解的不可微性，并采用一种新颖的目标函数最大化每个变量的个体方差贡献以提供更强的学习信号。在合成和真实数据集上的实验结果表明，该方法在解释方差方面显著优于线性PCA和KPCA，同时保持了可解释性以支持可视化和特征分析，并能原生处理分类和有序变量而无需维度爆炸。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Guarantees for Offline Domain Randomization</div>
<div class="meta-line">Authors: Arnaud Fickinger, Abderrahim Bendahi, Stuart Russell</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-11T19:22:33+00:00 · Latest: 2026-02-03T19:25:34+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10133v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.10133v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement-learning (RL) agents often struggle when deployed from simulation to the real-world. A dominant strategy for reducing the sim-to-real gap is domain randomization (DR) which trains the policy across many simulators produced by sampling dynamics parameters, but standard DR ignores offline data already available from the real system. We study offline domain randomization (ODR), which first fits a distribution over simulator parameters to an offline dataset. While a growing body of empirical work reports substantial gains with algorithms such as DROPO, the theoretical foundations of ODR remain largely unexplored. In this work, we cast ODR as a maximum-likelihood estimation over a parametric simulator family and provide statistical guarantees: under mild regularity and identifiability conditions, the estimator is weakly consistent (it converges in probability to the true dynamics as data grows), and it becomes strongly consistent (i.e., it converges almost surely to the true dynamics) when an additional uniform Lipschitz continuity assumption holds. We examine the practicality of these assumptions and outline relaxations that justify ODR&#x27;s applicability across a broader range of settings. Taken together, our results place ODR on a principled footing and clarify when offline data can soundly guide the choice of a randomization distribution for downstream offline RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线域随机化的统计保证</div>
<div class="mono" style="margin-top:8px">强化学习（RL）智能体从仿真环境部署到现实世界时常常面临困难。缩小仿真与现实差距的主流策略是域随机化（DR），该方法通过采样动力学参数生成多个仿真器来训练策略，但标准DR忽略了已从真实系统获取的离线数据。我们研究离线域随机化（ODR），该方法首先根据离线数据集拟合仿真器参数分布。尽管越来越多的实证研究（如DROPO算法）报告了显著性能提升，但ODR的理论基础仍待深入探索。本文通过参数化仿真器族的最大似然估计框架分析ODR，并提供统计保证：在温和的正则性与可识别性条件下，估计量具有弱一致性（随数据量增加以概率收敛于真实动力学），当附加一致Lipschitz连续性假设成立时，可获得强一致性（几乎必然收敛于真实动力学）。我们检验了这些假设的实用性，并提出了适用于更广泛场景的松弛条件。综合而言，本研究为ODR奠定了理论基石，并阐明了离线数据何时能可靠指导下游离线强化学习的随机化分布选择。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the sim-to-real transfer challenge in reinforcement learning by proposing a principled framework for offline domain randomization (ODR), which leverages existing real-world data to inform the selection of simulation parameters rather than relying on standard random sampling. The method formulates ODR as a maximum-likelihood estimation problem over a parametric family of simulators, establishing statistical guarantees that, under mild regularity and identifiability conditions, the estimator converges in probability to the true dynamics as data increases, with almost sure convergence under an additional uniform Lipschitz continuity assumption. Experimental analysis validates these theoretical foundations, demonstrating that ODR provides a sound basis for improving downstream offline RL performance by ensuring the randomization distribution is data-driven, thereby bridging the gap between empirical success and theoretical understanding.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中仿真到现实迁移的挑战，提出了离线域随机化的理论框架，利用现有真实世界数据指导仿真参数选择，而非依赖标准随机采样。该方法将离线域随机化建模为参数化仿真器族上的最大似然估计问题，并建立了统计保证：在温和的正则性和可识别性条件下，估计量随数据增加以概率收敛于真实动力学，且在附加的一致Lipschitz连续性假设下几乎必然收敛。实验分析验证了这些理论基础，表明离线域随机化通过确保随机化分布由数据驱动，为提升下游离线强化学习性能提供了可靠依据，从而弥合了实证成功与理论理解之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Child Mortality Prediction in Bangladesh: A Decade-Long Validation Study</div>
<div class="meta-line">Authors: Md Muhtasim Munif Fahim, Md Rezaul Karim</div>
<div class="meta-line">First: 2026-02-03T19:18:50+00:00 · Latest: 2026-02-03T19:18:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03957v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03957v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The predictive machine learning models for child mortality tend to be inaccurate when applied to future populations, since they suffer from look-ahead bias due to the randomization used in cross-validation. The Demographic and Health Surveys (DHS) data from Bangladesh for 2011-2022, with n = 33,962, are used in this paper. We trained the model on (2011-2014) data, validated it on 2017 data, and tested it on 2022 data. Eight years after the initial test of the model, a genetic algorithm-based Neural Architecture Search found a single-layer neural architecture (with 64 units) to be superior to XGBoost (AUROC = 0.76 vs. 0.73; p &lt; 0.01). Additionally, through a detailed fairness audit, we identified an overall &quot;Socioeconomic Predictive Gradient,&quot; with a positive correlation between regional poverty level (r = -0.62) and the algorithm&#x27;s AUC. In addition, we found that the model performed at its highest levels in the least affluent divisions (AUC 0.74) and decreased dramatically in the wealthiest divisions (AUC 0.66). These findings suggest that the model is identifying areas with the greatest need for intervention. Our model would identify approximately 1300 additional at-risk children annually than a Gradient Boosting model when screened at the 10% level and validated using SHAP values and Platt Calibration, and therefore provide a robust, production-ready computational phenotype for targeted maternal and child health interventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>孟加拉国儿童死亡率预测：一项长达十年的验证研究</div>
<div class="mono" style="margin-top:8px">儿童死亡率的预测机器学习模型在应用于未来人群时往往不准确，因其在交叉验证中采用的随机化方法存在前瞻性偏差。本研究使用孟加拉国2011-2022年人口与健康调查数据（n=33,962），以2011-2014年数据训练模型，2017年数据验证，2022年数据测试。模型初测八年后，基于遗传算法的神经架构搜索发现单层神经网络（含64个单元）优于XGBoost（AUROC=0.76 vs. 0.73；p&lt;0.01）。通过公平性审计，我们发现整体存在&#x27;社会经济预测梯度&#x27;，地区贫困水平与算法AUC呈负相关（r=-0.62）。模型在最贫困地区的表现最佳（AUC 0.74），在最富裕地区显著下降（AUC 0.66），表明模型能有效识别最需干预的区域。在10%筛查阈值下，结合SHAP值与普拉特校准验证，本模型较梯度提升模型每年可多识别约1300名高危儿童，为针对性妇幼健康干预提供了稳健的生产级计算表型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the issue of look-ahead bias in child mortality prediction models by conducting a decade-long validation using Bangladesh DHS data from 2011-2022. The method involved training on 2011-2014 data, validating on 2017 data, and testing on 2022 data, employing a Neural Architecture Search which identified a single-layer neural network as superior to XGBoost. Key results show the neural model achieved a higher AUROC (0.76 vs. 0.73), exhibited a socioeconomic predictive gradient where performance was better in poorer regions, and could identify approximately 1300 more at-risk children annually than a gradient boosting model at a 10% screening threshold.</div>
<div class="mono" style="margin-top:8px">本研究针对儿童死亡率预测模型存在的前瞻性偏差问题，利用孟加拉国2011-2022年的人口健康调查数据进行了长达十年的验证。方法上，模型基于2011-2014年数据训练，2017年数据验证，2022年数据测试，并通过神经架构搜索确定单层神经网络优于XGBoost。主要实验结果表明，神经网络模型获得了更高的AUROC（0.76对比0.73），表现出社会经济预测梯度即在较贫困地区性能更佳，且在10%的筛查阈值下，每年可比梯度提升模型多识别约1300名高危儿童。</div>
</details>
</div>
<div class="card">
<div class="title">Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints</div>
<div class="meta-line">Authors: Olaf Yunus Laitinen Imanov, Duygu Erisken, Derya Umut Kulali, Taner Yilmaz, Rana Irem Turhan</div>
<div class="meta-line">First: 2026-02-03T19:03:15+00:00 · Latest: 2026-02-03T19:03:15+00:00</div>
<div class="meta-line">Comments: 12 pages, 6 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03940v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Affordable housing shortages affect billions, while land scarcity and regulations make site selection slow. We present AURA (Autonomous Urban Resource Allocator), a hierarchical multi-agent reinforcement learning system for real-time affordable housing site selection under hard regulatory constraints (QCT, DDA, LIHTC). We model the task as a constrained multi-objective Markov decision process optimizing accessibility, environmental impact, construction cost, and social equity while enforcing feasibility. AURA uses a regulatory-aware state encoding 127 federal and local constraints, Pareto-constrained policy gradients with feasibility guarantees, and reward decomposition separating immediate costs from long-term social outcomes. On datasets from 8 U.S. metros (47,392 candidate parcels), AURA attains 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over strong baselines. In a New York City 2026 case study, it reduces selection time from 18 months to 72 hours and identifies 23% more viable sites; chosen sites have 31% better transit access and 19% lower environmental impact than expert picks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向实时保障性住房选址的自主AI智能体：监管约束下的多目标强化学习</div>
<div class="mono" style="margin-top:8px">保障性住房短缺影响数十亿人，而土地稀缺和法规限制使选址进程缓慢。本文提出AURA（自主城市资源分配器），一种在严格监管约束（QCT、DDA、LIHTC）下进行实时保障性住房选址的层次化多智能体强化学习系统。我们将该任务建模为约束多目标马尔可夫决策过程，在确保可行性的同时优化可达性、环境影响、建造成本和社会公平性。AURA采用包含127项联邦与地方约束的法规感知状态编码、具备可行性保证的帕累托约束策略梯度，以及将即时成本与长期社会效益分离的奖励分解机制。在来自美国8个大都市区（47,392个候选地块）的数据集上，AURA达到94.3%的法规合规率，帕累托超体积较基线模型提升37.2%。在纽约市2026年案例研究中，系统将选址时间从18个月缩短至72小时，多识别23%的可行地块；所选地块比专家推荐方案提升31%的交通可达性并降低19%的环境影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the global affordable housing shortage and the slow, constrained process of land selection, this paper introduces AURA, an autonomous AI system using hierarchical multi-agent reinforcement learning for real-time site selection under strict regulatory frameworks like QCT and LIHTC. The method formulates the problem as a constrained multi-objective Markov decision process, employing regulatory-aware state encoding and Pareto-constrained policy gradients to optimize accessibility, environmental impact, cost, and equity while ensuring feasibility. Experimental results on datasets from eight U.S. metropolitan areas show that AURA achieves 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over baselines; in a New York City case study, it reduced selection time from 18 months to 72 hours, identifying 23% more viable sites with 31% better transit access and 19% lower environmental impact compared to expert selections.</div>
<div class="mono" style="margin-top:8px">针对全球保障性住房短缺以及土地选址过程缓慢且受限的问题，本文提出了AURA，一个基于分层多智能体强化学习的自主AI系统，用于在QCT和LIHTC等严格监管框架下进行实时选址。该方法将问题建模为约束多目标马尔可夫决策过程，采用监管感知的状态编码和帕累托约束策略梯度，在确保可行性的同时优化可达性、环境影响、成本和社会公平性。在来自八个美国大都市区的数据集上的实验结果表明，AURA实现了94.3%的法规遵从性，并将帕累托超体积比基线提高了37.2%；在纽约市的案例研究中，它将选址时间从18个月缩短至72小时，识别出的可行地块数量比专家选择多23%，且交通便利性提高31%，环境影响降低19%。</div>
</details>
</div>
<div class="card">
<div class="title">Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes</div>
<div class="meta-line">Authors: Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie</div>
<div class="meta-line">First: 2026-01-26T18:57:00+00:00 · Latest: 2026-02-03T18:58:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18795v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18795v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复用计算量：通过条件化极离策略前缀扩展强化学习在难题上的应用</div>
<div class="mono" style="margin-top:8px">传统强化学习方法在大语言模型推理任务中，面对难题时因正确同策略轨迹稀少、策略梯度消失而导致学习停滞，造成计算资源浪费。为提升强化学习效率，我们提出复用历史采样计算量（来自先前推理或强化学习训练）形成的离策略轨迹。标准离策略方法直接监督离策略数据会引发优化不稳定。我们提出前缀强化学习方法，该方法以成功离策略轨迹的前缀为条件，通过同策略强化学习完成后续生成，从而规避离策略不稳定性。通过调节离策略前缀长度控制问题难度，该方法显著增强了难题上的学习信号。我们证明该目标函数不仅与标准强化学习目标一致，且具有更高样本效率。实验发现反向泛化现象：仅在前缀问题上训练可泛化至无前缀的分布外任务，且学习策略常与前缀策略不同。实验中通过基础模型的拒绝采样获取离策略轨迹，形成自改进循环。在复杂推理任务上，即使计入初始拒绝采样的计算成本，前缀强化学习达到相同训练奖励的速度仍比最强基线（离策略数据监督微调后强化学习）快2倍，最终奖励提升3倍。该优势可迁移至保留基准测试，且当离策略轨迹源自不同模型家族时依然有效，验证了其在实际场景中的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of standard reinforcement learning (RL) for large language models on hard reasoning problems, where on-policy learning stalls due to rare correct traces and vanishing gradients. To overcome this, the authors propose PrefixRL, a method that reuses off-policy traces from prior inference or training by conditioning on their successful prefixes and then performing on-policy RL to complete them, thereby avoiding instabilities from direct off-policy supervision. Experimental results show that PrefixRL accelerates training, reaching the same reward twice as fast as strong baselines while tripling final reward on hard problems, with gains transferring to held-out benchmarks and demonstrating flexibility across model families.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在困难推理问题上标准强化学习效率低下的问题，提出了一种名为PrefixRL的方法。该方法通过重用先前推理或训练中的离策略轨迹，以其成功前缀为条件进行在策略强化学习来完成后续部分，从而避免了直接离策略监督的不稳定性。实验结果表明，PrefixRL在困难问题上将训练速度提升至基准方法的两倍，并将最终奖励提高三倍，其增益可迁移到未见基准测试中，且在不同模型家族上仍保持有效性，验证了其实用灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL</div>
<div class="meta-line">Authors: Erfan Miahi, Eugene Belilovsky</div>
<div class="meta-line">First: 2026-02-03T18:56:48+00:00 · Latest: 2026-02-03T18:56:48+00:00</div>
<div class="meta-line">Comments: 32 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解并利用权重更新稀疏性实现通信高效的分布式强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是大型语言模型（LLM）后训练的关键组成部分。然而，在带宽受限的分布式RL中，可扩展性常受限于从训练器到推理工作器的策略权重同步，尤其是在商用网络或去中心化环境中。尽管近期研究表明RL更新仅修改模型参数的极小部分，但这些观察通常基于粗略的检查点差异。我们系统实证研究了步级与多步粒度下的权重更新稀疏性，分析了其在训练动态、离策略延迟和模型规模中的演变。研究发现更新稀疏性始终较高，在实际相关设置中常超过99%。利用此结构，我们提出PULSE（基于无损稀疏编码的补丁更新），这是一种简单高效的无损权重同步方法，仅传输修改参数的索引与数值。PULSE对传输错误具有鲁棒性，并避免了加法增量方案固有的浮点漂移。在带宽受限的去中心化环境中，相比全权重同步，该方法在保持比特级一致的训练动态和性能的同时，实现了超过100倍（从14 GB降至约108 MB）的通信缩减。通过利用此结构，PULSE使去中心化RL训练接近中心化吞吐量，将维持高GPU利用率所需的权重同步带宽从20 Gbit/s降至0.2 Gbit/s。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the communication bottleneck in distributed reinforcement learning (RL) for large language models, where synchronizing policy weights across trainers and workers consumes significant bandwidth. The authors first conduct an empirical study revealing that RL weight updates are extremely sparse, often exceeding 99% sparsity across various training conditions. They then propose PULSE, a lossless synchronization method that transmits only the indices and values of modified parameters, which is robust to errors and avoids numerical drift. Experimental results show that PULSE reduces communication by over 100x (from ~14 GB to ~108 MB) while maintaining identical training performance, enabling decentralized RL to approach centralized throughput by drastically lowering required bandwidth.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型分布式强化学习中策略权重同步带来的通信瓶颈问题展开研究。作者首先通过实证分析发现，强化学习的权重更新具有极高的稀疏性，在多种实际训练设置下常超过99%。基于此，他们提出了PULSE方法，这是一种无损的同步方案，仅传输被修改参数的索引和数值，既能抵抗传输错误，又可避免浮点数累积误差。实验结果表明，PULSE将通信量降低了100倍以上（从约14 GB降至约108 MB），同时保持了完全一致的训练动态和性能，使得去中心化强化学习的通信带宽需求从20 Gbit/s大幅降至0.2 Gbit/s，从而接近中心化训练的吞吐效率。</div>
</details>
</div>
<div class="card">
<div class="title">ME-IGM: Individual-Global-Max in Maximum Entropy Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Wen-Tse Chen, Yuxuan Li, Shiyu Huang, Jiayu Chen, Jeff Schneider</div>
<div class="meta-line">Venue: Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 - 29, 2026, IFAAMAS, 19 pages</div>
<div class="meta-line">First: 2024-06-20T01:55:08+00:00 · Latest: 2026-02-03T18:35:29+00:00</div>
<div class="meta-line">Comments: Published in the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.13930v4">Abs</a> · <a href="https://arxiv.org/pdf/2406.13930v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent credit assignment is a fundamental challenge for cooperative multi-agent reinforcement learning (MARL), where a team of agents learn from shared reward signals. The Individual-Global-Max (IGM) condition is a widely used principle for multi-agent credit assignment, requiring that the joint action determined by individual Q-functions maximizes the global Q-value. Meanwhile, the principle of maximum entropy has been leveraged to enhance exploration in MARL. However, we identify a critical limitation in existing maximum entropy MARL methods: a misalignment arises between local policies and the joint policy that maximizes the global Q-value, leading to violations of the IGM condition. To address this misalignment, we propose an order-preserving transformation. Building on it, we introduce ME-IGM, a novel maximum entropy MARL algorithm compatible with any credit assignment mechanism that satisfies the IGM condition while enjoying the benefits of maximum entropy exploration. We empirically evaluate two variants of ME-IGM: ME-QMIX and ME-QPLEX, in non-monotonic matrix games, and demonstrate their state-of-the-art performance across 17 scenarios in SMAC-v2 and Overcooked.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ME-IGM：最大熵多智能体强化学习中的个体-全局最大化</div>
<div class="mono" style="margin-top:8px">多智能体信用分配是合作式多智能体强化学习（MARL）的核心挑战，其中智能体团队从共享奖励信号中学习。个体-全局最大化（IGM）条件是广泛使用的多智能体信用分配原则，要求由个体Q函数确定的联合动作能最大化全局Q值。同时，最大熵原理已被用于增强MARL中的探索能力。然而，我们发现现有最大熵MARL方法存在一个关键局限：局部策略与最大化全局Q值的联合策略之间出现错位，导致违反IGM条件。为解决此问题，我们提出一种保序变换方法。在此基础上，我们引入ME-IGM——一种新颖的最大熵MARL算法，兼容任何满足IGM条件的信用分配机制，同时享有最大熵探索的优势。我们在非单调矩阵游戏中实证评估了ME-IGM的两个变体：ME-QMIX和ME-QPLEX，并在SMAC-v2和Overcooked的17个场景中展示了其领先性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a misalignment between local policies and the joint policy in maximum entropy multi-agent reinforcement learning (MARL), which violates the Individual-Global-Max (IGM) condition crucial for credit assignment. To resolve this, the authors propose an order-preserving transformation and introduce ME-IGM, a novel algorithm that integrates maximum entropy exploration with any IGM-compliant credit assignment method. Experimental results on non-monotonic matrix games, SMAC-v2, and Overcooked show that its variants, ME-QMIX and ME-QPLEX, achieve state-of-the-art performance across 17 scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对最大熵多智能体强化学习（MARL）中局部策略与联合策略之间的错位问题，该问题破坏了信用分配的关键原则——个体全局最大化（IGM）条件。为解决此问题，作者提出了一种保序变换，并引入了ME-IGM算法，该算法将最大熵探索与任何符合IGM条件的信用分配机制相结合。在非单调矩阵游戏、SMAC-v2和Overcooked上的实验结果表明，其变体ME-QMIX和ME-QPLEX在17个场景中均取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving</div>
<div class="meta-line">Authors: Yesom Park, Annie C. Lu, Shao-Ching Huang, Qiyang Hu, Y. Sungtaek Ju, Stanley Osher</div>
<div class="meta-line">First: 2026-02-03T18:18:30+00:00 · Latest: 2026-02-03T18:18:30+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03816v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03816v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SymPlex：面向符号偏微分方程求解的结构感知Transformer</div>
<div class="mono" style="margin-top:8px">本文提出SymPlex——一种无需真实表达式即可发现偏微分方程解析符号解的强化学习框架。该框架将符号PDE求解建模为树结构决策过程，仅利用PDE及其边界条件优化候选解。其核心组件SymFormer是一种结构感知Transformer，通过树相对自注意力建模层次化符号依赖关系，并借助语法约束的自回归解码确保句法有效性，克服了基于序列生成器的表达能力局限。与在离散化或隐函数空间逼近解的传统数值/神经网络方法不同，SymPlex直接在符号表达式空间操作，生成可解释、人类可读的解，天然表征非光滑行为与显式参数依赖。实验结果表明，该基于深度学习的符号方法能精确还原非光滑及参数化PDE解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SymPlex, a reinforcement learning framework designed to discover analytical symbolic solutions for partial differential equations (PDEs) without relying on known ground-truth expressions. The method formulates symbolic solving as a tree-structured decision process, utilizing a novel structure-aware Transformer called SymFormer that employs tree-relative self-attention and grammar-constrained decoding to generate syntactically valid expressions, thereby addressing the expressivity limitations of sequence-based models. Experimental results show that SymPlex can exactly recover non-smooth and parametric PDE solutions, producing interpretable, human-readable symbolic expressions directly from the PDE and its boundary conditions.</div>
<div class="mono" style="margin-top:8px">本文提出了SymPlex，一个无需真实表达式先验、用于发现偏微分方程解析符号解的强化学习框架。该方法将符号求解构建为树状决策过程，其核心是名为SymFormer的结构感知Transformer，它通过树相对自注意力和语法约束的自回归解码来建模层次符号依赖并确保句法有效性，从而克服了序列生成器的表达能力限制。实验结果表明，SymPlex能够精确恢复非光滑和参数化的偏微分方程解，直接从方程及其边界条件生成可解释、人类可读的符号表达式。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation</div>
<div class="meta-line">Authors: Ziru Chen, Dongdong Chen, Ruinan Jin, Yingbin Liang, Yujia Xie, Huan Sun</div>
<div class="meta-line">First: 2026-02-03T18:08:41+00:00 · Latest: 2026-02-03T18:08:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03806v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03806v1">PDF</a> · <a href="https://github.com/OSU-NLP-Group/cobalt">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs&#x27; in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连接在线与离线强化学习：面向多轮代码生成的上下文赌博机学习</div>
<div class="mono" style="margin-top:8px">近期，利用强化学习在真实世界任务（如多轮代码生成）上训练大语言模型的研究备受关注。在线强化学习虽通常优于离线强化学习，但其较高的训练成本和不稳定性限制了广泛应用。本文基于多轮代码生成可建模为一步可恢复马尔可夫决策过程的观察，提出了基于离线轨迹的上下文赌博机学习方法Cobalt，该方法融合了在线与离线强化学习的优势。Cobalt首先使用参考大语言模型收集代码生成轨迹，并将其分割为部分轨迹作为上下文提示；随后在在线赌博机学习阶段，通过单步代码生成训练大语言模型完成每个部分轨迹提示。实验表明，Cobalt在LiveCodeBench基准上显著优于基于GRPO和VeRPO的两种多轮在线强化学习基线方法，并将R1-Distill 8B和Qwen3 8B模型的绝对Pass@1分数分别提升最高达9.0和6.2分。此外，本文分析了大语言模型的上下文奖励操纵行为，并通过引入扰动轨迹增强Cobalt训练以缓解该问题。总体而言，本研究证明Cobalt为多轮代码生成等迭代决策任务提供了有效解决方案。代码与数据已开源：https://github.com/OSU-NLP-Group/cobalt。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying reinforcement learning to multi-turn code generation with large language models, where online RL is effective but costly and unstable, while offline RL is more stable but less performant. The authors propose Cobalt, a method that frames the task as a one-step recoverable Markov decision process and combines offline trajectory collection with online contextual bandit learning; specifically, it uses partial trajectories from a reference model as prompts and trains the LLM to complete them in single-step generations. Experimental results show that Cobalt outperforms online RL baselines like GRPO and VeRPO, improving models such as R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench, while also mitigating reward hacking through perturbed trajectory augmentation.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在多轮代码生成任务中应用强化学习所面临的挑战，即在线强化学习虽有效但成本高且不稳定，而离线强化学习更稳定但性能不足。作者提出Cobalt方法，将任务构建为一步可恢复马尔可夫决策过程，结合离线轨迹收集与在线上下文赌博机学习：具体而言，它使用参考模型的部分轨迹作为提示，训练大语言模型以单步生成完成代码。实验结果表明，Cobalt优于GRPO和VeRPO等在线强化学习基线，在LiveCodeBench上将R1-Distill 8B和Qwen3 8B模型的绝对Pass@1分数分别提升高达9.0和6.2分，同时通过扰动轨迹增强缓解了奖励黑客行为。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Estimation of Kernel Surrogate Models for Task Attribution</div>
<div class="meta-line">Authors: Zhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T17:43:48+00:00 · Latest: 2026-02-03T17:43:48+00:00</div>
<div class="meta-line">Comments: 27 pages. To appear in ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03783v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task&#x27;s performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向任务归因的核代理模型高效估计方法</div>
<div class="mono" style="margin-top:8px">现代人工智能代理（如大语言模型）通常在翻译、代码生成、数学推理和文本预测等多样化任务上同时训练。核心问题在于量化每个独立训练任务对目标任务性能的影响，即任务归因问题。直接方法（留一重训练法）通过移除各任务来测量影响，但计算成本随规模增长而不可行。近期研究提出构建代理模型来预测任意训练任务子集对目标任务的性能表现。现有工作主要聚焦线性代理模型，这类模型虽能捕捉一阶关系，却无法表征协同、拮抗或异或型效应等非线性交互。本文首先提出统一的任务加权框架来分析任务归因方法，并通过二阶分析揭示线性代理模型与影响函数的新关联。进而引入能更有效表征二阶任务交互的核代理模型。为高效学习核代理模型，我们开发了基于梯度的估计方法，该方法利用预训练模型的一阶近似；实证表明，该方法无需重复训练即可实现低于2%相对误差的精确估计。在数学推理、上下文学习及多目标强化学习等领域的实验验证了核代理模型的有效性：其与留一法基准的相关系数较线性代理模型和影响函数基线提升25%；应用于下游任务选择时，在上下文学习的示例选择和多目标强化学习基准测试中实现40%的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational challenge of task attribution in modern AI systems by moving beyond linear surrogate models to capture nonlinear task interactions. The authors propose a kernel surrogate model framework, which they efficiently estimate using a gradient-based method that approximates pretrained models without costly retraining. Experimental results across domains like math reasoning and in-context learning show that the kernel approach reduces relative error to under 2%, achieves a 25% higher correlation with ground-truth leave-one-out metrics, and improves downstream task selection by 40% in benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对现代AI系统中任务归因的计算挑战，提出了超越线性代理模型的核代理模型框架，以捕捉非线性任务交互作用。作者开发了一种基于梯度的估计方法，利用预训练模型的一阶近似高效学习核代理，避免了重复训练的昂贵开销。在数学推理、上下文学习等多个领域的实验中，该方法将相对误差降至2%以下，与留一法基准的相关性比线性代理和影响函数基线提高25%，并在下游任务选择中使上下文学习和多目标强化学习基准的演示选择性能提升40%。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL</div>
<div class="meta-line">Authors: Ian Wu, Yuxiao Qu, Amrith Setlur, Aviral Kumar</div>
<div class="meta-line">First: 2026-02-03T17:34:04+00:00 · Latest: 2026-02-03T17:34:04+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03773v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03773v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理缓存：通过短视界强化学习实现长视界持续改进</div>
<div class="mono" style="margin-top:8px">能够突破训练预算持续改进的大语言模型（LLMs）可通过测试时自适应解决日益复杂的问题，这一特性我们称为外推能力。然而，标准强化学习（RL）在固定问题分布和训练预算下运行，限制了测试时分布漂移下的外推能力。为此，我们提出RC——一种在训练和推理阶段替代标准自回归解码的迭代解码算法。RC利用LLMs在响应生成与摘要能力间的不对称性，构建跨迭代持续改进的推理链。经RC训练的模型能够实现外推，并在推理视界上持续改进，其视界长度可超过训练时所见范围的十倍以上。实证表明：使用16k词元训练预算对4B模型进行RC训练后，在测试时使用0.5百万词元可将HMMT 2025任务性能从40%提升至近70%，优于同规模模型及多数大型推理LLMs。最后，我们还证明RC训练模型能更有效地利用现有脚手架进一步提升测试时性能，这得益于训练获得的摘要条件生成能力的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling large language models (LLMs) to continually improve and adapt beyond their fixed training budgets, a capability termed extrapolation, which is limited by standard reinforcement learning. The authors propose Reasoning Cache (RC), an iterative decoding algorithm that replaces standard autoregressive decoding by leveraging an asymmetry in LLMs&#x27; capabilities to construct and refine reasoning chains across iterations. Experimental results demonstrate that a 4B model trained with RC using a 16k-token budget significantly improves performance on the HMMT 2025 benchmark from 40% to nearly 70% with extended test-time tokens, outperforming comparable and larger models, and also shows enhanced ability to utilize existing scaffolds for further scaling.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大语言模型在固定训练预算之外持续改进和适应的挑战，即外推能力，而标准强化学习在此方面存在局限。作者提出了推理缓存（RC）算法，一种替代标准自回归解码的迭代解码方法，它利用大语言模型能力的不对称性来跨迭代构建和优化推理链。实验结果表明，一个40亿参数的模型使用16k令牌的训练预算通过RC训练后，在HMMT 2025基准测试上的性能从40%显著提升至近70%，测试时使用更多令牌，表现优于同类及更大模型，并显示出利用现有框架进一步扩展性能的增强能力。</div>
</details>
</div>
<div class="card">
<div class="title">A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance</div>
<div class="meta-line">Authors: Axel Friedrich Wolter, Tobias Sutter</div>
<div class="meta-line">First: 2025-05-07T15:18:43+00:00 · Latest: 2026-02-03T16:19:43+00:00</div>
<div class="meta-line">Comments: 54 pages, 1 figure; Revised version with additional finite-time convergence results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.04494v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.04494v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study reinforcement learning by combining recent advances in regularized linear programming formulations with the classical theory of stochastic approximation. Motivated by the challenge of designing algorithms that leverage off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a novel primal-dual Projected Gradient Descent-Ascent algorithm for solving regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience replay-based gradient estimation with a two-timescale decomposition of the underlying nested optimization problem. The algorithm operates asynchronously, interacts with the environment through a single trajectory of correlated data, and updates its policy online in response to the dual variable associated with the occupancy measure of the underlying MDP. We prove that PGDA-RL converges almost surely to the optimal value function and policy of the regularized MDP. Our convergence analysis relies on tools from stochastic approximation theory and holds under weaker assumptions than those required by existing primal-dual RL approaches, notably removing the need for a simulator or a fixed behavioral policy. Under a strengthened ergodicity assumption on the underlying Markov chain, we establish a last-iterate finite-time guarantee with $\tilde{O} (k^{-2/3})$ mean-square convergence, aligning with the best-known rates for two-timescale stochastic approximation methods under Markovian sampling and biased gradient estimates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于在线对偶变量引导的双时间尺度原始-对偶强化学习框架</div>
<div class="mono" style="margin-top:8px">本研究结合正则化线性规划的最新进展与经典随机逼近理论，探索强化学习算法设计。针对如何利用离轨数据同时保持同轨探索的挑战，我们提出PGDA-RL算法——一种用于求解正则化马尔可夫决策过程的新型原始-对偶投影梯度下降-上升算法。该算法将基于经验回放的梯度估计与嵌套优化问题的双时间尺度分解相结合，通过单条相关数据轨迹与环境异步交互，并依据MDP占用测度的对偶变量在线更新策略。我们证明PGDA-RL几乎必然收敛至正则化MDP的最优值函数与策略，其收敛分析基于随机逼近理论工具，且所需假设弱于现有原始-对偶RL方法（尤其无需模拟器或固定行为策略）。在强化马尔可夫链遍历性假设下，我们建立了末次迭代有限时间保证，获得$\tilde{O}(k^{-2/3})$均方收敛速率，与马尔可夫采样和偏置梯度估计条件下双时间尺度随机逼近方法的最佳已知速率一致。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to leverage off-policy data while preserving on-policy exploration in reinforcement learning, this paper introduces PGDA-RL, a primal-dual algorithm based on a two-timescale projected gradient descent-ascent framework for solving regularized Markov Decision Processes. The method integrates experience replay for gradient estimation and operates asynchronously using a single trajectory of correlated data, updating the policy online guided by dual variables associated with the MDP&#x27;s occupancy measure. Experimental results demonstrate that PGDA-RL converges almost surely to the optimal value function and policy under weaker assumptions than prior primal-dual approaches, and under an ergodicity assumption, it achieves a finite-time mean-square convergence rate of $\tilde{O}(k^{-2/3})$, matching the best-known rates for two-timescale stochastic approximation under Markovian sampling.</div>
<div class="mono" style="margin-top:8px">本文旨在解决强化学习中利用离策略数据同时保持在线探索的挑战，提出了PGDA-RL算法，这是一种基于双时间尺度投影梯度下降-上升框架的原对偶方法，用于求解正则化马尔可夫决策过程。该方法结合了经验回放进行梯度估计，并异步地使用单条相关数据轨迹，通过MDP占用测度的对偶变量在线更新策略。实验结果表明，PGDA-RL在比现有原对偶方法更弱的假设下几乎必然收敛到最优值函数和策略，且在遍历性假设下，实现了$\tilde{O}(k^{-2/3})$的有限时间均方收敛速率，与马尔可夫采样下双时间尺度随机逼近的最佳已知速率一致。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation</div>
<div class="meta-line">Authors: Jiashuo Sun, Pengcheng Jiang, Saizhuo Wang, Jiajun Fan, Heng Wang, Siru Ouyang, Ming Zhong, Yizhu Jiao, Chengsong Huang, Xueqiang Xu, Pengrui Han, Peiran Li, Jiaxin Huang, Ge Liu, Heng Ji, Jiawei Han</div>
<div class="meta-line">First: 2026-02-03T16:08:23+00:00 · Latest: 2026-02-03T16:08:23+00:00</div>
<div class="meta-line">Comments: 19 pages, 8 tables, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03689v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03689v1">PDF</a> · <a href="https://github.com/GasolSun36/BAR-RAG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator&#x27;s Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重思重排序器：面向检索增强生成鲁棒性的边界感知证据选择</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）系统在实际检索噪声下仍显脆弱，即使所需证据已出现在前K个结果中。关键原因在于检索器与重排序器仅针对相关性进行优化，往往选择要么是琐碎、直接揭示答案的段落，要么是缺乏回答问题所需关键信息的证据，而未考虑证据是否适合生成器。我们提出BAR-RAG，将重排序器重构为边界感知证据选择器，其目标是定位生成器的“黄金平衡区”——既非过于简单也非生成器完全无法回答，而是具有挑战性却足以支撑推理的证据，从而提供最强的学习信号。BAR-RAG通过强化学习利用生成器反馈训练选择器，并采用两阶段流程：在诱导的证据分布下微调生成器，以缓解训练与推理间的分布失配问题。在知识密集型问答基准上的实验表明，BAR-RAG在噪声检索环境下持续提升端到端性能，较现有强RAG及重排序基线平均提升10.3%，同时显著增强系统鲁棒性。代码已公开于https://github.com/GasolSun36/BAR-RAG。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the brittleness of Retrieval-Augmented Generation (RAG) systems under noisy retrieval, where traditional retrievers and rerankers often select evidence that is either too trivial or insufficient for the generator. The authors propose BAR-RAG, a method that reframes the reranker as a boundary-aware evidence selector to target a &#x27;Goldilocks Zone&#x27; of challenging yet sufficient evidence, training it with reinforcement learning based on generator feedback and employing a two-stage pipeline to fine-tune the generator for the induced evidence distribution. Experimental results on knowledge-intensive QA benchmarks demonstrate that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3% over strong baselines and significantly enhancing robustness.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）系统在噪声检索下的脆弱性问题，指出传统检索器和重排序器常选择对生成器而言过于简单或信息不足的证据。作者提出了BAR-RAG方法，将重排序器重新设计为边界感知的证据选择器，以定位“恰到好处”的、具有挑战性但充分的证据，通过基于生成器反馈的强化学习进行训练，并采用两阶段流程对生成器进行微调以适应诱导的证据分布。在知识密集型问答基准上的实验结果表明，BAR-RAG在噪声检索下持续提升了端到端性能，相比强基线平均增益达10.3%，并显著增强了鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration</div>
<div class="meta-line">Authors: Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong, Yankai Chen, Chen Ma, Xue Liu, Pluto Zhou, Irwin King</div>
<div class="meta-line">First: 2026-02-03T15:32:09+00:00 · Latest: 2026-02-03T15:32:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03647v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03647v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#x27;cut-and-regenerate&#x27; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Search-R2：通过执行器-精炼器协作增强搜索集成推理能力</div>
<div class="mono" style="margin-top:8px">搜索集成推理使语言智能体能够通过主动查询外部源来超越静态参数化知识。然而，通过强化学习训练这些智能体受到多尺度信用分配问题的阻碍：现有方法通常依赖稀疏的轨迹级奖励，无法区分高质量推理与偶然猜测，导致冗余或误导性搜索行为。为此，我们提出Search-R2——一种新颖的执行器-精炼器协作框架，通过定向干预增强推理能力，两个组件在训练期间联合优化。该方法将生成过程分解为：执行器生成初始推理轨迹，元精炼器通过&#x27;截断-再生&#x27;机制选择性诊断并修复缺陷步骤。为提供细粒度监督，我们设计了混合奖励机制，将结果正确性与量化检索证据信息密度的密集过程奖励相结合。理论上，我们将执行器-精炼器交互形式化为平滑混合策略，证明选择性校正相比强基线能带来严格的性能提升。在多种通用和多跳QA数据集上的大量实验表明，Search-R2在不同模型规模上持续优于基于RAG和RL的强基线，以最小开销实现更优的推理准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the multi-scale credit assignment problem in training search-integrated reasoning agents via reinforcement learning, where sparse rewards often fail to differentiate high-quality reasoning from lucky guesses, leading to inefficient search behaviors. To overcome this, the authors propose Search-R2, an Actor-Refiner collaboration framework that decomposes reasoning into an Actor generating initial trajectories and a Meta-Refiner selectively diagnosing and repairing flawed steps through a cut-and-regenerate mechanism, supported by a hybrid reward combining outcome correctness with dense process rewards based on retrieved evidence information density. Experimental results across various general and multi-hop QA datasets show that Search-R2 consistently outperforms strong retrieval-augmented generation and reinforcement learning baselines in reasoning accuracy with minimal overhead, validating its effectiveness across different model scales.</div>
<div class="mono" style="margin-top:8px">本文针对基于强化学习的搜索集成推理智能体训练中的多尺度信用分配问题，即稀疏奖励难以区分高质量推理与侥幸猜测，导致搜索行为低效。为解决此问题，作者提出了Search-R2框架，采用执行者-精炼者协作机制，将推理分解为执行者生成初始轨迹和元精炼者通过剪裁-再生机制选择性诊断修复错误步骤，并设计了结合结果正确性和基于检索证据信息密度的密集过程奖励的混合奖励。在多种通用和多跳问答数据集上的实验表明，Search-R2在推理准确性上持续优于强检索增强生成和强化学习基线方法，且开销最小，验证了其在不同模型规模下的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG</div>
<div class="meta-line">Authors: Yicheng Zhang, Zhen Qin, Zhaomin Wu, Wenqi Zhang, Shuiguang Deng</div>
<div class="meta-line">First: 2026-02-03T15:30:14+00:00 · Latest: 2026-02-03T15:30:14+00:00</div>
<div class="meta-line">Comments: On going work. Codes are released at https://github.com/zyc140345/HARR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03645v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03645v1">PDF</a> · <a href="https://github.com/zyc140345/HARR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) enables large language models (LLMs) to produce evidence-based responses, and its performance hinges on the matching between the retriever and LLMs. Retriever optimization has emerged as an efficient alternative to fine-tuning LLMs. However, existing solutions suffer from objective mismatch between retriever optimization and the goal of RAG pipeline. Reinforcement learning (RL) provides a promising solution to address this limitation, yet applying RL to retriever optimization introduces two fundamental challenges: 1) the deterministic retrieval is incompatible with RL formulations, and 2) state aliasing arises from query-only retrieval in multi-hop reasoning. To address these challenges, we replace deterministic retrieval with stochastic sampling and formulate RAG as a Markov decision process, making retriever optimizable by RL. Further, we incorporate retrieval history into the state at each retrieval step to mitigate state aliasing. Extensive experiments across diverse RAG pipelines, datasets, and retriever scales demonstrate consistent improvements of our approach in RAG performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向RAG中历史感知稠密检索器的强化微调</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）使大语言模型（LLM）能够生成基于证据的响应，其性能取决于检索器与LLM的匹配程度。检索器优化已成为微调LLM的高效替代方案。然而，现有解决方案存在检索器优化目标与RAG流程目标不匹配的问题。强化学习（RL）为解决这一局限提供了可行方案，但将RL应用于检索器优化面临两个根本性挑战：1）确定性检索与RL框架不兼容；2）多跳推理中仅基于查询的检索会导致状态混淆。为解决这些挑战，我们采用随机采样替代确定性检索，并将RAG建模为马尔可夫决策过程，使检索器可通过RL进行优化。此外，我们在每个检索步骤中将检索历史纳入状态表示以缓解状态混淆问题。跨多种RAG流程、数据集和检索器规模的实验表明，该方法能持续提升RAG性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the objective mismatch in retrieval-augmented generation (RAG) by proposing a reinforcement learning (RL) method to optimize the retriever, as existing retriever fine-tuning approaches do not align well with the overall RAG pipeline goals. The method reformulates RAG as a Markov decision process, replacing deterministic retrieval with stochastic sampling to make it compatible with RL, and incorporates retrieval history into the state to prevent state aliasing in multi-hop reasoning. Experimental results across various RAG pipelines, datasets, and retriever scales show that this approach consistently improves RAG performance.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）中检索器优化与整体目标不匹配的问题，提出了一种基于强化学习的方法来优化检索器。该方法将RAG重新表述为马尔可夫决策过程，用随机采样替代确定性检索以适配强化学习框架，并在每一步检索中引入检索历史作为状态信息，以缓解多跳推理中的状态混淆问题。在多种RAG流程、数据集和检索器规模上的实验表明，该方法能持续提升RAG的性能。</div>
</details>
</div>
<div class="card">
<div class="title">TRE: Encouraging Exploration in the Trust Region</div>
<div class="meta-line">Authors: Chao Huang, Yujing Lu, Quangang Li, Shenghe Wang, Yan Wang, Yueyang Zhang, Long Xia, Jiashu Zhao, Zhiyuan Sun, Daiting Shi, Tingwen Liu</div>
<div class="meta-line">First: 2026-02-03T15:21:49+00:00 · Latest: 2026-02-03T15:21:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03635v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03635v1">PDF</a> · <a href="https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model&#x27;s trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRE：在信任区域内鼓励探索</div>
<div class="mono" style="margin-top:8px">熵正则化是强化学习中增强探索的标准技术，但在大语言模型中效果甚微甚至可能降低性能。我们将此归因于大语言模型因庞大词表和生成长度而固有的累积尾部风险。在此类环境中，标准的全局熵最大化会不加区分地将概率质量分散至大量无效词元的尾部，而非聚焦于合理候选，从而破坏连贯推理。为此，我们提出信任区域熵方法，严格在模型信任区域内鼓励探索。在数学推理、组合搜索和偏好对齐任务上的大量实验表明，TRE始终优于原始PPO、标准熵正则化及其他探索基线。代码已开源：https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the observation that standard entropy regularization, a common technique to encourage exploration in reinforcement learning, often fails or harms performance when fine-tuning large language models (LLMs) due to the models&#x27; large vocabularies and long generation sequences, which cause probability mass to be wasted on implausible tokens. To address this, the authors propose Trust Region Entropy (TRE), a method designed to promote exploration only within the model&#x27;s established trust region, thereby preserving coherent reasoning. Experimental results across mathematical reasoning, combinatorial search, and preference alignment tasks demonstrate that TRE consistently outperforms baseline methods including standard PPO and entropy regularization.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于标准熵正则化这一强化学习中常用的探索鼓励技术，在微调大语言模型时常常失效甚至损害性能，这是由于模型词表巨大且生成序列长，导致概率质量被分散到大量无效令牌上。为解决该问题，作者提出了信任区域熵方法，旨在仅在模型既定的信任区域内鼓励探索，从而保持推理的连贯性。在数学推理、组合搜索和偏好对齐任务上的实验结果表明，该方法在性能上持续优于包括标准近端策略优化和熵正则化在内的基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Priors in Bayesian Optimization for Hyperparameter Optimization</div>
<div class="meta-line">Authors: Lukas Fehring, Marcel Wever, Maximilian Spliethöver, Leona Hennig, Henning Wachsmuth, Marius Lindauer</div>
<div class="meta-line">First: 2025-11-04T13:44:04+00:00 · Latest: 2026-02-03T15:19:02+00:00</div>
<div class="meta-line">Comments: 8 pages plus references and appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02570v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.02570v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) is a widely used approach to hyperparameter optimization (HPO). However, most existing HPO methods only incorporate expert knowledge during initialization, limiting practitioners&#x27; ability to influence the optimization process as new insights emerge. This limits the applicability of BO in iterative machine learning development workflows. We propose DynaBO, a BO framework that enables continuous user control of the optimization process. Over time, DynaBO leverages provided user priors by augmenting the acquisition function with decaying, prior-weighted preferences while preserving asymptotic convergence guarantees. To reinforce robustness, we introduce a data-driven safeguard that detects and can be used to reject misleading priors. We prove theoretical results on near-certain convergence, robustness to adversarial priors, and accelerated convergence when informative priors are provided. Extensive experiments across various HPO benchmarks show that DynaBO consistently outperforms our state-of-the-art competitors across all benchmarks and for all prior kinds. Our results demonstrate that DynaBO enables reliable and efficient collaborative BO, bridging automated and manually controlled model development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯优化中用于超参数优化的动态先验</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）是超参数优化（HPO）中广泛使用的方法。然而，现有大多数HPO方法仅在初始化阶段融入专家知识，限制了实践者在获得新见解时影响优化过程的能力，这制约了BO在迭代式机器学习开发工作流中的适用性。我们提出DynaBO框架，该框架支持用户持续控制优化过程。DynaBO通过向采集函数添加衰减型先验加权偏好来持续利用用户提供的先验知识，同时保持渐近收敛保证。为增强鲁棒性，我们引入数据驱动的安全机制，可检测并拒绝误导性先验。我们证明了该框架在以下方面的理论结果：近乎必然收敛性、对抗性先验的鲁棒性，以及提供信息性先验时的加速收敛。在多种HPO基准测试中的大量实验表明，DynaBO在所有基准测试和各类先验条件下均持续优于当前最先进的对比方法。结果表明DynaBO能够实现可靠高效的协作式贝叶斯优化，在自动化与人工控制的模型开发之间架起桥梁。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation that existing Bayesian optimization methods for hyperparameter tuning only incorporate expert knowledge at initialization, this paper introduces DynaBO, a framework enabling continuous user guidance throughout the optimization process. The method dynamically integrates user priors by augmenting the acquisition function with decaying, prior-weighted preferences, includes a data-driven safeguard to reject misleading priors, and maintains theoretical convergence guarantees. Experimental results across various benchmarks demonstrate that DynaBO consistently outperforms state-of-the-art competitors for all prior types, enabling more reliable and efficient collaborative model development.</div>
<div class="mono" style="margin-top:8px">针对现有贝叶斯优化方法在超参数调优中仅能在初始化阶段融入专家知识的局限，本文提出了DynaBO框架，以支持用户在优化过程中持续提供指导。该方法通过为采集函数添加衰减的先验加权偏好来动态整合用户先验，并引入数据驱动的安全机制来拒绝误导性先验，同时保持了理论收敛保证。在多种基准测试上的实验结果表明，DynaBO在所有先验类型下均一致优于现有最优方法，实现了更可靠、高效的协作式模型开发。</div>
</details>
</div>
<div class="card">
<div class="title">SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization</div>
<div class="meta-line">Authors: Maksim Afanasyev, Illarion Iov</div>
<div class="meta-line">First: 2026-02-02T17:46:06+00:00 · Latest: 2026-02-03T13:58:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02383v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02383v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response&#x27;s absolute likelihood. This can lead to unlearning, where the model degrades the probability of high-quality outputs to satisfy margin constraints, and formatting collapse caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SLIME：基于稳定似然的隐式边界强化偏好优化方法</div>
<div class="mono" style="margin-top:8px">直接偏好优化方法已成为替代基于人类反馈的强化学习（RLHF）对齐大语言模型（LLM）的高效计算方案。最新方法通过推导隐式奖励函数简化了对齐流程，但常存在关键目标失配问题：优化选定与拒绝回答间的相对边界并不能保证维持选定回答的绝对似然。这可能导致模型遗忘（为满足边界约束而降低高质量输出的概率）以及因过度惩罚拒绝序列引发的格式崩溃。本研究提出SLIME（基于稳定似然的隐式边界强化）——一种无需参考模型的对齐目标，旨在解耦偏好学习与生成质量。SLIME包含三重目标：（1）锚定项以最大化优选回答的似然；（2）稳定惩罚项防止拒绝标记的概率坍缩至零；（3）结合硬约束与软约束的双边界机制以实现精确边界塑造。实验表明，SLIME在保持更高生成稳定性的同时，性能优于当前最先进的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SLIME, a method to address the objective mismatch in direct preference optimization for aligning large language models, where existing approaches can degrade generation quality by over-penalizing rejected responses. The proposed method decouples preference learning from generation quality through a three-part objective: an anchoring term to maximize the likelihood of preferred responses, a stabilizing penalty to prevent rejected token probabilities from collapsing, and a dual-margin mechanism for precise boundary shaping. Experimental results show that SLIME outperforms state-of-the-art baselines while maintaining higher generation stability.</div>
<div class="mono" style="margin-top:8px">本文提出了SLIME方法，旨在解决大型语言模型对齐中直接偏好优化的目标不匹配问题，现有方法可能因过度惩罚被拒绝的响应而降低生成质量。该方法通过三部分目标将偏好学习与生成质量解耦：锚定项最大化首选响应的似然，稳定惩罚防止被拒绝标记的概率崩溃，以及双边界机制实现精确边界塑造。实验结果表明，SLIME在保持更高生成稳定性的同时，性能优于现有先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains</div>
<div class="meta-line">Authors: Qixin Zeng, Hongyin Zhang, Shangke Lyu, Junxi Jin, Donglin Wang, Chao Huang</div>
<div class="meta-line">First: 2026-02-03T13:30:18+00:00 · Latest: 2026-02-03T13:30:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03511v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03511v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CMR：基于压缩映射嵌入的仿人机器人非结构化地形鲁棒运动控制</div>
<div class="mono" style="margin-top:8px">仿人机器人在非结构化地形上的鲁棒抗干扰能力仍是长期挑战，尤其在感知不可靠、模型失配显著的环境中。虽然高度图等感知信息能增强地形认知，但传感器噪声与仿真-现实差距易导致策略失稳。本研究通过理论分析证明：当潜在动力学具有压缩性时，观测噪声下的回报差距存在上界。进而提出鲁棒性压缩映射框架，将高维易扰观测映射至潜在空间，使局部扰动随时间衰减。该方法结合对比表征学习与Lipschitz正则化，在显式控制敏感度的同时保持任务相关几何结构。该框架可作为辅助损失项无缝集成至现代深度强化学习流程，无需额外技术负担。大量仿人机器人实验表明，在强噪声环境下CMR显著优于其他运动控制算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robust humanoid locomotion on unstructured terrains, where sensor noise and model inaccuracies can destabilize control policies. The authors propose the Contractive Mapping for Robustness (CMR) framework, which embeds high-dimensional, noisy observations into a contractive latent space to attenuate disturbances over time, using a combination of contrastive learning and Lipschitz regularization integrated as an auxiliary loss in reinforcement learning. Experimental results on humanoid locomotion demonstrate that CMR significantly outperforms existing methods under conditions of increased sensor noise.</div>
<div class="mono" style="margin-top:8px">本文针对非结构化地形下人形机器人鲁棒步态的挑战，其中传感器噪声和模型失配易导致控制策略失稳。作者提出了收缩映射鲁棒性框架，通过结合对比学习和Lipschitz正则化，将高维噪声观测嵌入到收缩的潜在空间中，以随时间衰减扰动，并作为强化学习的辅助损失项集成。在人形机器人的广泛实验中，该方法在增强噪声条件下显著优于其他步态算法。</div>
</details>
</div>
<div class="card">
<div class="title">Reparameterization Flow Policy Optimization</div>
<div class="meta-line">Authors: Hai Zhong, Zhuoran Li, Xun Wang, Longbo Huang</div>
<div class="meta-line">First: 2026-02-03T13:22:08+00:00 · Latest: 2026-02-03T13:22:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03501v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03501v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\times$ the reward of the state-of-the-art baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重参数化流策略优化</div>
<div class="mono" style="margin-top:8px">重参数化策略梯度（RPG）已成为基于模型的强化学习中一种强大的范式，通过可微动力学反向传播梯度实现高样本效率。然而，现有RPG方法主要局限于高斯策略，限制了其性能且未能利用生成模型的最新进展。本研究发现，通过可微ODE积分生成动作的流策略天然契合RPG框架——这一关联在先前工作中尚未建立。但简单利用这种协同效应效果有限，常存在训练不稳定与探索不足的问题。我们提出重参数化流策略优化（RFO），通过流生成过程与系统动力学的联合反向传播计算策略梯度，无需复杂对数似然计算即可实现高样本效率。RFO包含两个针对稳定性与探索的定制正则项，并提出带动作分块的变体。在涉及刚体/软体及状态/视觉输入的多种运动与操控任务实验中，RFO均展现卓越性能。在控制软体四足机器人的挑战性运动任务中，RFO获得的奖励达到当前最优基线的近2倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in Reparameterization Policy Gradient (RPG) methods, which are sample-efficient but have been confined to Gaussian policies, thus missing out on the benefits of advanced generative models. The authors propose Reparameterization Flow Policy Optimization (RFO), a method that integrates flow-based policies—which generate actions via differentiable ODE integration—with RPG by jointly backpropagating gradients through both the flow generation and system dynamics, thereby avoiding intractable log-likelihood computations. To ensure stability and exploration, RFO incorporates two tailored regularization terms and a variant with action chunking. Experimental results across locomotion and manipulation tasks, including rigid and soft bodies with state or visual inputs, show RFO&#x27;s effectiveness, notably achieving nearly double the reward of a state-of-the-art baseline on a challenging soft-body quadruped locomotion task.</div>
<div class="mono" style="margin-top:8px">本文针对重参数化策略梯度（RPG）方法的局限性展开研究，该方法虽样本效率高，但此前主要局限于高斯策略，未能利用先进生成模型的优势。作者提出了重参数化流策略优化（RFO），该方法通过将基于流的策略（通过可微ODE积分生成动作）与RPG框架相结合，联合反向传播流生成和系统动态的梯度，从而避免了难以处理的似然计算。为确保训练稳定性和探索性，RFO引入了两个定制正则化项以及一个带动作分块的变体。在包括刚体和软体的运动及操控任务（使用状态或视觉输入）上的大量实验表明，RFO方法有效，尤其在一个具有挑战性的软体四足机器人运动任务上，其奖励达到了最先进基线的近两倍。</div>
</details>
</div>
<div class="card">
<div class="title">IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning</div>
<div class="meta-line">Authors: Haohao Luo, Zexi Li, Yuexiang Xie, Wenhao Zhang, Yaliang Li, Ying Shen</div>
<div class="meta-line">First: 2026-02-03T12:43:09+00:00 · Latest: 2026-02-03T12:43:09+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03468v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IntentRL：通过强化学习训练面向开放式深度研究的主动用户意图代理</div>
<div class="mono" style="margin-top:8px">深度研究（DR）代理通过自主从大规模网络语料库中检索并综合证据生成长篇报告，将大语言模型（LLM）的应用扩展至参数化知识之外，实现了一种长视野的自主范式。然而，与实时对话助手不同，DR 计算成本高且耗时，引发了自主性与交互性的矛盾：对模糊用户查询的高度自主性常导致执行时间过长且结果不尽人意。为此，我们提出 IntentRL 框架，训练主动代理在启动长视野研究前澄清潜在用户意图。为克服开放式研究数据稀缺的问题，我们引入一种可扩展的流水线，通过浅层到深层的意图细化图将少量种子样本扩展为高质量对话轮次。进一步采用两阶段强化学习（RL）策略：第一阶段在离线对话上应用 RL 以高效学习通用用户交互行为；第二阶段使用训练后的代理与用户模拟器进行在线推演，以增强对多样化用户反馈的适应能力。大量实验表明，IntentRL 在意图命中率和下游任务性能上均显著提升，优于闭源 DR 代理的内置澄清模块及主动型 LLM 基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the autonomy-interaction dilemma in Deep Research (DR) agents, where high autonomy on ambiguous queries leads to inefficient, unsatisfactory outcomes. The authors propose IntentRL, a framework that trains proactive agents to clarify latent user intents before long-horizon research, using a scalable pipeline to generate high-quality dialogue data via an intent refinement graph and a two-stage reinforcement learning strategy for efficient offline learning and online adaptation. Experimental results demonstrate that IntentRL significantly improves intent hit rate and downstream task performance, surpassing both closed-source DR agents and proactive LLM baselines.</div>
<div class="mono" style="margin-top:8px">本文针对深度研究（DR）智能体在模糊查询下因高度自主性导致效率低下、结果不理想的自主-交互困境，提出了IntentRL框架，训练主动智能体在开始长期研究前澄清用户潜在意图。该方法通过意图细化图的可扩展管道生成高质量对话数据，并采用两阶段强化学习策略进行高效离线学习和在线适应。实验结果表明，IntentRL显著提高了意图命中率和下游任务性能，优于闭源DR智能体和主动式LLM基线。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing</div>
<div class="meta-line">Authors: Xin Sheng, Jiaxin Li, Yujuan Pang, Ran Peng, Yong Ma</div>
<div class="meta-line">First: 2026-02-03T12:17:25+00:00 · Latest: 2026-02-03T12:17:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03452v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03452v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) is effective for training large language models on deterministic outcome reasoning tasks. Prior work shows RLVR works with few prompts, but prompt selection is often based only on training-accuracy variance, leading to unstable optimization directions and weaker transfer. We revisit prompt selection from a mechanism-level view and argue that an effective minibatch should provide both (i) a reliable positive anchor and (ii) explicit negative learning signals from rare failures. Based on this principle, we propose \emph{positive--negative pairing}: at each update, we sample a hard-but-solvable $q^{+}$ and an easy-but-brittle prompt $q^{-}$(high success rate but not perfect), characterized by low and high empirical success rates under multiple rollouts. We further introduce Weighted GRPO, which reweights binary outcomes at the pair level and uses group-normalized advantages to amplify rare successes on $q^{+}$ into sharp positive guidance while turning rare failures on $q^{-}$ into strong negative penalties. This bidirectional signal provides informative learning feedback for both successes and failures, improving sample efficiency without suppressing exploration. On Qwen2.5-Math-7B, a single paired minibatch per update consistently outperforms a GRPO baseline that selects two prompts via commonly used variance-based selection heuristics: AIME~2025 Pass@8 improves from 16.8 to 22.2, and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained from a pool of 1209 training prompts. Similar gains are observed on Qwen2.5-Math-7B-Instruct.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越方差：通过稀有事件放大与双向配对实现高效提示的RLVR</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）在确定性结果推理任务上训练大语言模型效果显著。先前研究表明RLVR仅需少量提示即可工作，但提示选择通常仅基于训练准确率方差，导致优化方向不稳定且泛化能力较弱。我们从机制层面重新审视提示选择，主张有效的小批量应同时提供（i）可靠的正向锚点与（ii）来自稀有失败的显式负向学习信号。基于此原则，我们提出\emph{正负配对}方法：每次更新时，采样一个困难但可解的提示$q^{+}$与一个简单但脆弱的提示$q^{-}$（高成功率但不完美），二者通过多次推演分别具有低与高的经验成功率。我们进一步引入加权GRPO，在配对层级对二元结果进行重加权，并利用组归一化优势将$q^{+}$上的稀有成功放大为明确正向指导，同时将$q^{-}$上的稀有失败转化为强负向惩罚。这种双向信号为成功与失败均提供信息丰富的学习反馈，在提升样本效率的同时不抑制探索。在Qwen2.5-Math-7B上的实验表明，每次更新仅使用单个配对小批量，即持续优于通过常用方差选择启发法选取两个提示的GRPO基线：AIME~2025 Pass@8从16.8提升至22.2，AMC23 Pass@64从94.0提升至97.0，且与基于1209个训练提示池的大规模RLVR保持竞争力。在Qwen2.5-Math-7B-Instruct模型上也观察到类似增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of prompt selection in reinforcement learning with verifiable rewards (RLVR) for language models, where prior variance-based methods lead to unstable optimization. The authors propose a mechanism-level approach called positive-negative pairing, which constructs minibatches by pairing a hard-but-solvable prompt with a high empirical success rate (positive anchor) and an easy-but-brittle prompt with rare failures (negative signal). They introduce Weighted GRPO to reweight binary outcomes and use group-normalized advantages, amplifying rare successes into positive guidance and rare failures into negative penalties. Experimental results on Qwen2.5-Math-7B show significant improvements: AIME 2025 Pass@8 increased from 16.8 to 22.2 and AMC23 Pass@64 from 94.0 to 97.0, matching performance with large-scale RLVR using far fewer prompts.</div>
<div class="mono" style="margin-top:8px">本文针对语言模型在可验证奖励强化学习（RLVR）中提示选择效率低下的问题展开研究，指出先前基于方差的方法会导致优化方向不稳定。作者从机制层面提出了一种正负配对方法，通过将高经验成功率的难解提示（正锚点）与存在罕见失败风险的易碎提示（负信号）配对构建小批量。他们引入了加权GRPO来重新加权二元结果并利用组归一化优势，将罕见成功放大为正向指导，将罕见失败转化为强负惩罚。在Qwen2.5-Math-7B上的实验结果显示显著提升：AIME 2025 Pass@8从16.8提高到22.2，AMC23 Pass@64从94.0提高到97.0，仅用极少提示即达到了与大规模RLVR相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">CRL-VLA: Continual Vision-Language-Action Learning</div>
<div class="meta-line">Authors: Qixin Zeng, Shuo Zhang, Hongyin Zhang, Renjie Wang, Han Zhao, Libang Zhao, Runze Li, Donglin Wang, Chao Huang</div>
<div class="meta-line">First: 2026-02-03T12:09:53+00:00 · Latest: 2026-02-03T12:09:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03445v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRL-VLA：持续视觉-语言-动作学习</div>
<div class="mono" style="margin-top:8px">在开放世界环境中，终身学习对于具身智能体至关重要，其中强化学习微调已成为重要范式，使视觉-语言-动作模型能通过环境交互掌握灵巧操作。因此，持续强化学习是将VLA模型部署于终身机器人场景的可行路径，但平衡稳定性（保留旧技能）与可塑性（学习新技能）仍是现有方法的重大挑战。我们提出CRL-VLA框架，通过严格理论边界实现VLA模型的持续后训练。我们推导出统一性能边界，将稳定性-可塑性权衡与目标条件优势幅度相关联，并通过策略散度进行缩放。CRL-VLA通过非对称调节解决该困境：约束先前任务的优势幅度，同时允许新任务受控增长。这通过简单有效的双评论家架构实现，其中冻结评论家锚定语义一致性，可训练估计器驱动适应。在LIBERO基准测试中，CRL-VLA有效协调了这些冲突目标，在抗遗忘与前向适应方面均超越基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling Vision-Language-Action (VLA) models for lifelong robotic learning, where balancing the retention of old skills (stability) with the acquisition of new ones (plasticity) is difficult. The authors propose CRL-VLA, a continual reinforcement learning framework that provides theoretical bounds and employs an asymmetric regulation strategy, implemented via a dual-critic architecture with a novel Goal-Conditioned Value Formulation, to constrain advantage on prior tasks while allowing controlled learning on new ones. Experimental results on the LIBERO benchmark show that CRL-VLA outperforms baseline methods in both preventing forgetting and adapting to new tasks.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉-语言-动作模型在终身机器人学习中的挑战，即平衡旧技能保持与新技能学习的稳定性-可塑性权衡难题。研究者提出了CRL-VLA框架，该框架提供理论边界，并通过采用非对称调节策略——利用具有新颖目标条件价值公式的双评论家架构，限制先验任务的优势值同时允许新任务受控学习。在LIBERO基准测试上的实验结果表明，CRL-VLA在抗遗忘和正向适应方面均优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Goals Hierarchical Policy for Long-Horizon Offline Goal-Conditioned RL</div>
<div class="meta-line">Authors: Jinwoo Choi, Sang-Hyun Lee, Seung-Woo Seo</div>
<div class="meta-line">First: 2026-02-03T11:11:03+00:00 · Latest: 2026-02-03T11:11:03+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03389v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03389v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline goal-conditioned reinforcement learning remains challenging for long-horizon tasks. While hierarchical approaches mitigate this issue by decomposing tasks, most existing methods rely on separate high- and low-level networks and generate only a single intermediate subgoal, making them inadequate for complex tasks that require coordinating multiple intermediate decisions. To address this limitation, we draw inspiration from the chain-of-thought paradigm and propose the Chain-of-Goals Hierarchical Policy (CoGHP), a novel framework that reformulates hierarchical decision-making as autoregressive sequence modeling within a unified architecture. Given a state and a final goal, CoGHP autoregressively generates a sequence of latent subgoals followed by the primitive action, where each latent subgoal acts as a reasoning step that conditions subsequent predictions. To implement this efficiently, we pioneer the use of an MLP-Mixer backbone, which supports cross-token communication and captures structural relationships among state, goal, latent subgoals, and action. Across challenging navigation and manipulation benchmarks, CoGHP consistently outperforms strong offline baselines, demonstrating improved performance on long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向长时域离线目标条件强化学习的链式目标分层策略</div>
<div class="mono" style="margin-top:8px">离线目标条件强化学习在长时域任务中仍面临挑战。虽然分层方法通过任务分解缓解了这一问题，但现有方法大多依赖独立的高层与底层网络，且仅生成单一中间子目标，难以应对需要协调多重中间决策的复杂任务。为突破此局限，我们受思维链范式启发，提出链式目标分层策略——一种将分层决策重构为统一架构内自回归序列建模的新框架。给定状态与最终目标，该框架自回归地生成潜在子目标序列及原始动作，每个潜在子目标作为推理步骤制约后续预测。为实现高效计算，我们首次采用MLP-Mixer主干网络，支持跨令牌通信并捕捉状态、目标、潜在子目标与动作间的结构关系。在导航与操作基准测试中，该策略持续超越现有离线基线，显著提升长时域任务性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of long-horizon tasks in offline goal-conditioned reinforcement learning, where existing hierarchical methods often rely on separate networks and generate only single subgoals, limiting their effectiveness for complex tasks requiring multiple intermediate decisions. Inspired by chain-of-thought reasoning, the authors propose the Chain-of-Goals Hierarchical Policy (CoGHP), a unified framework that reformulates hierarchical decision-making as autoregressive sequence modeling to generate a sequence of latent subgoals and actions. Using an MLP-Mixer backbone to enable cross-token communication and capture structural relationships, the method is evaluated on navigation and manipulation benchmarks, where it consistently outperforms strong offline baselines, demonstrating improved performance in long-horizon scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对离线目标条件强化学习中长时程任务的挑战，现有分层方法通常依赖分离的网络且仅生成单一子目标，限制了其在需要多个中间决策的复杂任务中的有效性。受思维链启发，作者提出了目标链分层策略（CoGHP），这是一个统一框架，将分层决策重新表述为自回归序列建模，以生成一系列潜在子目标和动作。该方法采用MLP-Mixer骨干网络实现跨令牌通信并捕捉状态、目标、潜在子目标和动作之间的结构关系，在导航和操作基准测试中进行了评估，结果表明其一致优于强离线基线，在长时程任务上表现出性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">An Approximate Ascent Approach To Prove Convergence of PPO</div>
<div class="meta-line">Authors: Leif Doering, Daniel Schmidt, Moritz Melcher, Sebastian Kassing, Benedikt Wille, Tilman Aach, Simon Weissmann</div>
<div class="meta-line">First: 2026-02-03T11:10:22+00:00 · Latest: 2026-02-03T11:10:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03386v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03386v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Proximal Policy Optimization (PPO) is among the most widely used deep reinforcement learning algorithms, yet its theoretical foundations remain incomplete. Most importantly, convergence and understanding of fundamental PPO advantages remain widely open. Under standard theory assumptions we show how PPO&#x27;s policy update scheme (performing multiple epochs of minibatch updates on multi-use rollouts with a surrogate gradient) can be interpreted as approximated policy gradient ascent. We show how to control the bias accumulated by the surrogate gradients and use techniques from random reshuffling to prove a convergence theorem for PPO that sheds light on PPO&#x27;s success. Additionally, we identify a previously overlooked issue in truncated Generalized Advantage Estimation commonly used in PPO. The geometric weighting scheme induces infinite mass collapse onto the longest $k$-step advantage estimator at episode boundaries. Empirical evaluations show that a simple weight correction can yield substantial improvements in environments with strong terminal signal, such as Lunar Lander.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种证明PPO收敛性的近似上升方法</div>
<div class="mono" style="margin-top:8px">近端策略优化（PPO）是最广泛使用的深度强化学习算法之一，但其理论基础仍不完整。最重要的是，PPO的收敛性及其核心优势的理解仍存在广泛空白。在标准理论假设下，我们展示了PPO的策略更新方案（对重复使用的轨迹数据执行多轮小批量更新，并采用替代梯度）可被解释为近似策略梯度上升。我们阐述了如何控制替代梯度累积的偏差，并利用随机重排技术证明了PPO的收敛定理，从而揭示PPO的成功机制。此外，我们发现了PPO常用的截断广义优势估计中一个先前被忽视的问题：在回合边界处，几何加权方案会导致无限质量坍缩到最长的k步优势估计器上。实证评估表明，在具有强终止信号的环境（如月球着陆器）中，简单的权重修正能带来显著性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the incomplete theoretical understanding of Proximal Policy Optimization (PPO), a widely used deep reinforcement learning algorithm, by providing a convergence proof. The motivation stems from the lack of rigorous analysis for PPO&#x27;s policy update scheme, which involves multiple epochs of minibatch updates on reused rollout data. The method interprets PPO as an approximate policy gradient ascent, controlling the bias from surrogate gradients and applying random reshuffling techniques to establish convergence. Experimentally, the authors identify and correct an issue in truncated Generalized Advantage Estimation that causes infinite mass collapse at episode boundaries, demonstrating that this simple weight correction leads to significant performance improvements in environments like Lunar Lander with strong terminal signals.</div>
<div class="mono" style="margin-top:8px">本文针对广泛使用的深度强化学习算法——近端策略优化（PPO）理论理解不完整的问题，提供了收敛性证明。研究动机源于PPO策略更新方案（即对重复使用的 rollout 数据进行多轮小批量更新）缺乏严格分析。方法上，将PPO解释为近似策略梯度上升，通过控制代理梯度带来的偏差并应用随机重排技术来证明收敛性。实验方面，作者发现并修正了PPO中常用的截断广义优势估计的一个问题，该问题在回合边界会导致无限质量塌缩到最长k步优势估计器上，结果表明在如 Lunar Lander 这类具有强终止信号的环境中，简单的权重修正能带来显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She, Viet Anh Nguyen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T03:50:01+00:00 · Latest: 2026-02-03T10:25:06+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01601v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01601v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可验证奖励在线强化学习的自适应轨迹分配策略</div>
<div class="mono" style="margin-top:8px">采样效率是可验证奖励强化学习的关键瓶颈。现有基于分组的策略优化方法（如GRPO）为所有训练提示分配固定数量的轨迹。这种均匀分配隐含地将所有提示视为同等信息量，可能导致计算预算使用效率低下并阻碍训练进展。本文提出VIP（方差感知预测分配策略），将给定轨迹预算分配给当前批次中的提示，以最小化策略更新的期望梯度方差。在每次迭代中，VIP使用轻量级高斯过程模型基于近期轨迹预测每个提示的成功概率。这些概率预测被转化为方差估计，随后输入凸优化问题，在严格计算预算约束下确定最优轨迹分配。实验结果表明，VIP在多个基准测试中持续提升采样效率，并比均匀或启发式分配策略获得更高性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the sampling inefficiency in reinforcement learning with verifiable rewards, where existing methods like GRPO uniformly allocate rollouts across all training prompts, treating them as equally informative and wasting computational budget. The authors propose VIP, a Variance-Informed Predictive allocation strategy that minimizes the expected gradient variance of policy updates by using a lightweight Gaussian process model to predict per-prompt success probabilities from recent rollouts, translating these into variance estimates, and solving a convex optimization problem for optimal rollout allocation under a budget constraint. Experimental results demonstrate that VIP consistently improves sampling efficiency and achieves higher performance compared to uniform or heuristic allocation strategies across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对可验证奖励强化学习中的采样效率问题，指出现有方法如GRPO对所有训练提示均匀分配模拟轨迹，将其视为同等信息量，导致计算预算浪费。作者提出VIP，一种方差感知预测分配策略，通过轻量级高斯过程模型基于近期轨迹预测每个提示的成功概率，将其转化为方差估计，并在预算约束下求解凸优化问题以最小化策略更新的梯度方差，实现最优分配。实验结果表明，VIP在多个基准测试中持续提升了采样效率，性能优于均匀或启发式分配策略。</div>
</details>
</div>
<div class="card">
<div class="title">MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis</div>
<div class="meta-line">Authors: Xiao Sun, Yuming Yang, Junnan Zhu, Jiang Zhong, Xinyu Zhou, Kaiwen Wei</div>
<div class="meta-line">First: 2026-02-03T10:03:35+00:00 · Latest: 2026-02-03T10:03:35+00:00</div>
<div class="meta-line">Comments: 36 pages, 27 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03340v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MentalSeek-Dx：面向真实世界精神疾病诊断的渐进式假设-演绎推理</div>
<div class="mono" style="margin-top:8px">精神健康障碍已成为日益严峻的全球公共卫生挑战。尽管大语言模型在精神疾病评估中展现出潜力，但其临床应用因缺乏生态效度和细粒度诊断监督的基准而严重受限。为弥合这一差距，我们推出首个专注于真实临床场景下疾病层级精神诊断的基准——\textbf{MentalDx Bench}。该基准包含712份经委员会认证精神科医生依据ICD-11标准标注的去标识化电子健康记录，涵盖16个诊断类别下的76种疾病。对18个大语言模型的评估揭示了关键的\textit{范式错位}：在粗粒度诊断分类中的优异表现与疾病层级诊断的系统性失败形成鲜明对比，凸显了基于模式建模与临床假设-演绎推理之间的鸿沟。为此，我们提出\textbf{MentalSeek-Dx}——通过监督轨迹构建和课程式强化学习内化临床推理过程的医学专用大语言模型。在MentalDx Bench上的实验表明，仅用140亿参数的MentalSeek-Dx即达到最先进性能，为可靠的精神疾病诊断建立了临床基础框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the global mental health crisis and the limitations of existing benchmarks for evaluating Large Language Models (LLMs) in clinical psychiatry, this work introduces MentalDx Bench, a novel benchmark comprising 712 real-world, psychiatrist-annotated electronic health records covering 76 disorders. The method involves evaluating 18 LLMs, revealing a critical failure in fine-grained, disorder-level diagnosis despite good coarse categorization, and subsequently proposing MentalSeek-Dx, a specialized 14B-parameter model trained with supervised trajectory construction and curriculum reinforcement learning to internalize clinical hypothetico-deductive reasoning. The main experimental result is that MentalSeek-Dx achieves state-of-the-art performance on the new benchmark, demonstrating a clinically grounded framework for reliable psychiatric diagnosis.</div>
<div class="mono" style="margin-top:8px">本研究针对全球心理健康挑战及现有大语言模型在精神病学临床评估中缺乏生态效度和细粒度诊断监督基准的局限，提出了首个基于真实世界临床环境、包含712份经认证精神科医生标注电子健康记录的精神障碍诊断基准MentalDx Bench。方法上，通过评估18个大语言模型揭示了其在粗粒度分类表现良好但在细粒度障碍诊断上系统性失败的范式错位，进而提出了MentalSeek-Dx这一医学专用模型，该模型通过监督轨迹构建和课程强化学习来内化临床假设-演绎推理过程。主要实验结果表明，仅拥有140亿参数的MentalSeek-Dx在新基准上取得了最先进的性能，为可靠的精神病诊断建立了一个临床 grounded 的框架。</div>
</details>
</div>
<div class="card">
<div class="title">MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control</div>
<div class="meta-line">Authors: Yongwei Zhang, Yuanzhe Xing, Quanyi Liang, Quan Quan, Zhikun She</div>
<div class="meta-line">First: 2025-12-31T16:36:44+00:00 · Latest: 2026-02-03T10:02:50+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24955v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24955v2">PDF</a> · <a href="https://github.com/YuanZhe-Xing/MSACL}{https://github.com/YuanZhe-Xing/MSACL">Code1</a> · <a href="https://github.com/YuanZhe-Xing/MSACL">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For safety-critical applications, model-free reinforcement learning (RL) faces numerous challenges, particularly the difficulty of establishing verifiable stability guarantees while maintaining high exploration efficiency. To address these challenges, we present Multi-Step Actor-Critic Learning with Lyapunov Certificates (MSACL), a novel approach that seamlessly integrates exponential stability with maximum entropy reinforcement learning (MERL). In contrast to existing methods that rely on complex reward engineering and single-step constraints, MSACL utilizes intuitive rewards and multi-step data for actor-critic learning. Specifically, we first introduce Exponential Stability Labels (ESLs) to categorize samples and propose a $λ$-weighted aggregation mechanism to learn Lyapunov certificates. Leveraging these certificates, we then develop a stability-aware advantage function to guide policy optimization, thereby ensuring rapid Lyapunov descent and robust state convergence. We evaluate MSACL across six benchmarks, comprising four stabilization and two high-dimensional tracking tasks. Experimental results demonstrate its consistent superiority over both standard RL baselines and state-of-the-art Lyapunov-based RL algorithms. Beyond rapid convergence, MSACL exhibits significant robustness against environmental uncertainties and remarkable generalization to unseen reference signals. The source code and benchmarking environments are available at \href{https://github.com/YuanZhe-Xing/MSACL}{https://github.com/YuanZhe-Xing/MSACL}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MSACL：基于李雅普诺夫证书的多步演员-评论家学习算法及其在指数稳定控制中的应用</div>
<div class="mono" style="margin-top:8px">针对安全关键应用，无模型强化学习面临诸多挑战，尤其是在保持高探索效率的同时难以建立可验证的稳定性保证。为此，我们提出基于李雅普诺夫证书的多步演员-评论家学习算法（MSACL），该方法将指数稳定性与最大熵强化学习无缝结合。相较于依赖复杂奖励工程和单步约束的现有方法，MSACL采用直观奖励机制并利用多步数据进行演员-评论家学习。具体而言，我们首先引入指数稳定性标签对样本分类，并提出λ加权聚合机制以学习李雅普诺夫证书。基于这些证书，我们进一步构建稳定性感知优势函数来指导策略优化，从而确保李雅普诺夫函数的快速下降和鲁棒状态收敛。我们在六个基准测试中评估MSACL，包括四个镇定任务和两个高维跟踪任务。实验结果表明，该算法在标准强化学习基线和最先进的基于李雅普诺夫的强化学习算法中均表现出持续优越性。除快速收敛外，MSACL对环境不确定性具有显著鲁棒性，并对未见参考信号展现出卓越泛化能力。源代码与基准测试环境已发布于https://github.com/YuanZhe-Xing/MSACL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for verifiable stability guarantees and high exploration efficiency in safety-critical reinforcement learning, this paper introduces MSACL, a method that integrates exponential stability with maximum entropy RL. The approach uses Exponential Stability Labels to categorize samples and a λ-weighted aggregation to learn Lyapunov certificates, which then inform a stability-aware advantage function for policy optimization, ensuring rapid Lyapunov descent. Experiments on six benchmarks, including stabilization and tracking tasks, show MSACL outperforms standard RL and Lyapunov-based baselines, demonstrating robustness to uncertainties and generalization to unseen signals.</div>
<div class="mono" style="margin-top:8px">针对安全关键应用中强化学习难以同时保证可验证的稳定性和高效探索的问题，本文提出了MSACL方法，将指数稳定性与最大熵强化学习相结合。该方法通过指数稳定性标签对样本分类，并采用λ加权聚合学习李雅普诺夫证书，进而利用稳定性感知的优势函数指导策略优化，确保快速李雅普诺夫下降。在包括稳定化和高维跟踪任务的六个基准测试中，实验结果表明MSACL优于标准强化学习和先进的基于李雅普诺夫的方法，展现出对环境不确定性的强鲁棒性和对未见参考信号的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Task-Centric Policy Optimization from Misaligned Motion Priors</div>
<div class="meta-line">Authors: Ziang Zheng, Kai Feng, Yi Nie, Shentao Qin</div>
<div class="meta-line">First: 2026-01-27T09:46:34+00:00 · Latest: 2026-02-03T09:56:39+00:00</div>
<div class="meta-line">Comments: Work requires further details and not complete yet</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19411v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19411v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing naïve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于未对齐运动先验的任务中心策略优化</div>
<div class="mono" style="margin-top:8px">人形机器人控制常利用人类演示的运动先验来促进行为自然性。然而，由于本体差异、重定向误差和任务无关的变异，此类演示常存在次优性或与机器人任务未对齐，导致简单模仿会降低任务性能。反之，纯任务强化学习虽能产生多种任务最优解，却常导致不自然或不稳定的运动。这揭示了对抗模仿学习中线性奖励混合的根本局限。我们提出\emph{任务中心运动先验}（TCMP），这是一种任务优先的对抗模仿框架，将模仿视为条件正则化器而非平等目标。TCMP在最大化任务改进的同时，仅当模仿信号与任务进展兼容时才纳入它们，从而产生一种自适应、几何感知的更新机制，既能保持任务可行的下降方向，又能在未对齐时抑制有害模仿。我们提供了梯度冲突与任务优先稳定点的理论分析，并通过人形控制实验验证了所提方法能在噪声演示下实现稳健的任务性能与一致的运动风格。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of using human motion priors for humanoid robot control, where direct imitation often degrades task performance due to misalignment from embodiment differences or suboptimal demonstrations, while pure task reinforcement learning yields unnatural motions. The authors propose Task-Centric Motion Priors (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer, adaptively incorporating imitation signals only when they align with task progress to preserve task-feasible optimization and suppress harmful guidance. Experimental results on humanoid control demonstrate that TCMP achieves robust task performance while maintaining consistent motion style even with noisy demonstrations, supported by theoretical analysis of gradient conflicts and stationary points.</div>
<div class="mono" style="margin-top:8px">本文针对人形机器人控制中使用人类运动先验的挑战展开研究，指出直接模仿常因本体差异或次优演示导致任务性能下降，而纯任务强化学习则产生不自然的运动。作者提出了任务中心运动先验（TCMP），这是一个任务优先的对抗模仿框架，将模仿视为条件正则化器，仅在模仿信号与任务进展兼容时自适应地纳入，以保持任务可行的优化并抑制有害的模仿指导。在人形控制实验中的结果表明，TCMP即使在噪声演示下也能实现稳健的任务性能并保持一致的运动风格，该框架还得到了梯度冲突和驻点理论分析的支持。</div>
</details>
</div>
<div class="card">
<div class="title">From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner&#x27;s Tutorial</div>
<div class="meta-line">Authors: Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar</div>
<div class="meta-line">First: 2026-01-13T15:40:55+00:00 · Latest: 2026-02-03T09:52:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08662v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08662v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从经典到量子强化学习及其在量子控制中的应用：初学者教程</div>
<div class="mono" style="margin-top:8px">本教程旨在通过清晰、示例驱动的讲解，使本科生更易理解强化学习（RL）。重点在于弥合RL理论与实际编程应用之间的差距，解决学生从概念理解转向实践时面临的常见挑战。通过动手示例和易于理解的解释，本教程致力于帮助学生掌握在现实场景中自信应用RL技术所需的基础技能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This tutorial is motivated by the need to make reinforcement learning (RL) more accessible to undergraduate students, bridging the gap between theoretical concepts and practical implementation. The method involves providing clear, example-driven explanations and hands-on coding examples to demystify RL. The main outcome is that students gain foundational skills to confidently apply RL techniques in real-world scenarios, overcoming common challenges in transitioning from theory to practice.</div>
<div class="mono" style="margin-top:8px">本教程旨在使强化学习对本科生更易理解，以弥合理论概念与实际应用之间的差距。其方法是通过清晰的示例驱动解释和动手编码示例来简化强化学习。主要成果是学生获得了将强化学习技术自信应用于现实场景的基础技能，克服了从理论到实践过渡中的常见挑战。</div>
</details>
</div>
<div class="card">
<div class="title">MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Shengyuan Liu, Liuxin Bao, Qi Yang, Wanting Geng, Boyun Zheng, Chenxin Li, Wenting Chen, Houwen Peng, Yixuan Yuan</div>
<div class="meta-line">First: 2026-02-03T09:47:49+00:00 · Latest: 2026-02-03T09:47:49+00:00</div>
<div class="meta-line">Comments: 23 Pages, 4 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03320v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03320v1">PDF</a> · <a href="https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here">Code1</a> · <a href="https://github.com/CUHK-AIM-Group/MedSAM-Agent">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedSAM-Agent：通过多轮智能体强化学习赋能交互式医学图像分割</div>
<div class="mono" style="margin-top:8px">医学图像分割正从任务特定模型向通用化框架演进。近期研究利用多模态大语言模型作为自主智能体，采用带可验证奖励的强化学习来协调如Segment Anything Model等专用工具。然而，这些方法通常依赖单轮、僵化的交互策略，且训练过程缺乏流程级监督，限制了其充分挖掘交互工具动态潜力的能力，并导致冗余操作。为弥补这一不足，我们提出MedSAM-Agent框架，将交互式分割重构为多步骤自主决策过程。首先，我们引入专家轨迹生成的混合提示策略，使模型能够内化类人决策启发式与自适应优化策略。此外，我们开发了包含多轮端到端结果验证与临床保真度流程奖励设计的双阶段训练流程，以提升交互简洁性与决策效率。在6种医学模态和21个数据集上的大量实验表明，MedSAM-Agent实现了最先进的性能，有效将自主医学推理与鲁棒的迭代优化相统一。代码已发布于\href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{此处}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of single-turn, rigid interaction strategies in existing medical image segmentation frameworks that use Multi-modal Large Language Models (MLLMs) as agents, this paper introduces MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. The method employs a hybrid prompting strategy for expert-curated trajectory generation to internalize human-like decision heuristics and a two-stage training pipeline with multi-turn, end-to-end outcome verification and a clinical-fidelity process reward design to enhance interaction parsimony and efficiency. Experimental results across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization.</div>
<div class="mono" style="margin-top:8px">针对现有基于多模态大语言模型（MLLM）代理的医学图像分割方法中单轮、僵化的交互策略缺乏过程级监督、导致冗余操作的问题，本文提出了MedSAM-Agent框架，将交互式分割重新定义为多步骤自主决策过程。该方法采用混合提示策略生成专家引导的轨迹，以内化类人决策启发式策略，并通过集成多轮端到端结果验证与临床保真度过程奖励设计的两阶段训练流程，提升交互的简洁性和决策效率。在6种医学模态和21个数据集上的广泛实验表明，MedSAM-Agent实现了最先进的性能，有效统一了自主医学推理与鲁棒的迭代优化。</div>
</details>
</div>
<div class="card">
<div class="title">Emergent time-keeping mechanisms in a deep reinforcement learning agent performing an interval timing task</div>
<div class="meta-line">Authors: Amrapali Pednekar, Alvaro Garrido, Pieter Simoens, Yara Khaluf</div>
<div class="meta-line">First: 2025-08-06T13:56:41+00:00 · Latest: 2026-02-03T09:46:37+00:00</div>
<div class="meta-line">Comments: Accepted at 2025 Artificial Life Conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.15784v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.15784v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Drawing parallels between Deep Artificial Neural Networks (DNNs) and biological systems can aid in understanding complex biological mechanisms that are difficult to disentangle. Temporal processing, an extensively researched topic, is one such example that lacks a coherent understanding of its underlying mechanisms. In this study, we investigate temporal processing in a Deep Reinforcement Learning (DRL) agent performing an interval timing task and explore potential biological counterparts to its emergent behavior. The agent was successfully trained to perform a duration production task, which involved marking successive occurrences of a target interval while viewing a video sequence. Analysis of the agent&#x27;s internal states revealed oscillatory neural activations, a ubiquitous pattern in biological systems. Interestingly, the agent&#x27;s actions were predominantly influenced by neurons exhibiting these oscillations with high amplitudes and frequencies corresponding to the target interval. Parallels are drawn between the agent&#x27;s time-keeping strategy and the Striatal Beat Frequency (SBF) model, a biologically plausible model of interval timing. Furthermore, the agent maintained its oscillatory representations and task performance when tested on different video sequences (including a blank video). Thus, once learned, the agent internalized its time-keeping mechanism and showed minimal reliance on its environment to perform the timing task. A hypothesis about the resemblance between this emergent behavior and certain aspects of the evolution of biological processes like circadian rhythms, has been discussed. This study aims to contribute to recent research efforts of utilizing DNNs to understand biological systems, with a particular emphasis on temporal processing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>执行间隔计时任务的深度强化学习智能体中涌现的时间保持机制</div>
<div class="mono" style="margin-top:8px">通过类比深度人工神经网络与生物系统，有助于理解难以解析的复杂生物机制。时间处理作为广泛研究的课题，其底层机制尚缺乏统一认知。本研究探索了执行间隔计时任务的深度强化学习智能体中的时间处理过程，并对其涌现行为与潜在生物对应机制进行探讨。该智能体经训练成功完成时长生成任务，即在观看视频序列时标记目标间隔的连续出现。对其内部状态的分析揭示了振荡性神经激活模式——这种普遍存在于生物系统的特征。值得注意的是，智能体的行为主要受那些呈现高振幅、且振荡频率与目标间隔相对应的神经元影响。研究将智能体的时间保持策略与具有生物合理性的间隔计时模型——纹状体节拍频率模型进行类比。此外，当使用不同视频序列（包括空白视频）测试时，智能体仍能维持振荡表征与任务表现。这表明习得机制后，智能体内化了时间保持机制，执行计时任务时对环境依赖极低。研究进一步探讨了该涌现行为与昼夜节律等生物过程演化特征的相似性假说。本研究旨在推动利用深度神经网络理解生物系统的前沿探索，特别聚焦于时间处理领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand complex temporal processing mechanisms in biological systems, this study trains a deep reinforcement learning agent on an interval timing task to produce target durations while viewing video sequences. The method involves analyzing the agent&#x27;s internal neural activations, which reveal emergent oscillatory patterns resembling biological rhythms, particularly aligning with the Striatal Beat Frequency model. Key experimental results show that the agent&#x27;s timing actions are driven by high-amplitude oscillations tuned to the target interval, and it maintains performance and oscillatory representations even when tested on different or blank videos, indicating an internalized time-keeping mechanism independent of environmental cues.</div>
<div class="mono" style="margin-top:8px">本研究旨在理解生物系统中复杂的时序处理机制，通过训练深度强化学习代理执行间隔计时任务，使其在观看视频序列时产生目标时长。方法包括分析代理的内部神经激活，发现其出现了类似生物节律的振荡模式，尤其与纹状体节拍频率模型相符。主要实验结果表明，代理的计时行为由与目标间隔匹配的高振幅振荡驱动，且在不同视频甚至空白视频测试中仍保持性能和振荡表征，表明其内化了不依赖环境线索的计时机制。</div>
</details>
</div>
<div class="card">
<div class="title">Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL</div>
<div class="meta-line">Authors: Julian Lemmel, Felix Resch, Mónika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu</div>
<div class="meta-line">First: 2026-02-02T15:41:53+00:00 · Latest: 2026-02-03T09:41:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02236v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02236v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents&#x27; performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于实时循环强化学习的自动驾驶预训练控制器在线微调</div>
<div class="mono" style="margin-top:8px">在现实应用中部署预训练策略面临重大挑战，从根本上限制了基于学习的控制系统的实际适用性。当自主系统遭遇系统动力学变化、传感器漂移或任务目标变更时，固定策略的性能会迅速下降。研究表明，采用实时循环强化学习（RTRRL）——一种具有生物合理性的在线适应算法——能够有效微调预训练策略，提升自主智能体在驾驶任务中的表现。进一步研究发现，RTRRL可与近期受生物启发的循环网络模型（液阻-液容循环神经网络）产生协同效应。通过在模拟CarRacing环境及配备事件相机的RoboRacer实车线跟踪任务中的验证，证明了该闭环方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of deploying pretrained policies in autonomous driving, where fixed policies degrade when encountering environmental changes. The authors propose using Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible online adaptation algorithm, to fine-tune pretrained controllers, and integrate it with a Liquid-Resistance Liquid-Capacitance recurrent network for enhanced synergy. Experimental results demonstrate improved performance in both a simulated CarRacing environment and a real-world line-following task using a RoboRacer with an event camera.</div>
<div class="mono" style="margin-top:8px">本文针对自动驾驶中预训练策略在环境变化时性能下降的部署挑战，提出使用实时循环强化学习这一生物启发的在线适应算法来微调预训练控制器，并结合液体电阻-电容循环网络以增强协同性。实验结果表明，该方法在模拟CarRacing环境和搭载事件相机的RoboRacer实车线跟随任务中均有效提升了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models</div>
<div class="meta-line">Authors: Yuelin Hu, Zhengxue Cheng, Wei Liu, Li Song</div>
<div class="meta-line">First: 2026-02-03T09:38:21+00:00 · Latest: 2026-02-03T09:38:21+00:00</div>
<div class="meta-line">Comments: accepted by cscwd2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03309v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03309v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.
  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.
  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>熵门控选择性策略优化：面向大语言模型混合训练的令牌级梯度分配方法</div>
<div class="mono" style="margin-top:8px">大语言模型的混合训练方法通常结合样本级的专家演示监督微调（SFT）与模型推演的强化学习（RL）。本文提出熵门控选择性策略优化（EGSPO），这是一个三阶段框架，通过令牌级梯度调制扩展了样本级混合方法。第一阶段为SFT专家学习，采用纯SFT损失通过专家演示建立可靠的预热策略。第二阶段为RL推演生成，从当前策略采样轨迹并计算各令牌的预测熵。第三阶段为EGSPO机制，实施熵门控梯度分配：预测熵模块将高熵令牌导向完整PPO更新以鼓励探索，低熵令牌则导向衰减PPO更新以降低方差并保留知识。关键的是，两个分支均包含优势函数A_t，确保错误轨迹获得一致的负向学习信号，防止强化自信错误。EGSPO在数学推理基准上取得稳定提升，较CHORD phi基线在AIME和MATH分别提升3.8%和2.9%，仅产生3.4%的额外计算开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve hybrid training of large language models that combine supervised fine-tuning (SFT) and reinforcement learning (RL), this paper introduces Entropy-Gated Selective Policy Optimization (EGSPO), a three-stage framework that modulates gradients at the token level rather than just the sample level. The method first performs SFT on expert demonstrations, then generates RL rollouts to compute per-token predictive entropy, and finally uses an entropy-gated mechanism to route high-entropy tokens to full PPO updates for exploration and low-entropy tokens to attenuated updates to reduce variance and preserve knowledge, all while incorporating advantage functions to avoid reinforcing errors. Experimental results on mathematical reasoning benchmarks show consistent improvements, with gains of 3.8% on AIME and 2.9% on MATH over the CHORD phi baseline, while adding only 3.4% computational overhead.</div>
<div class="mono" style="margin-top:8px">本文旨在改进结合监督微调（SFT）和强化学习（RL）的大语言模型混合训练方法，提出了熵门控选择性策略优化（EGSPO），这是一个三阶段框架，在令牌级别而非仅样本级别调节梯度。该方法首先在专家演示上进行SFT，然后生成RL轨迹以计算每个令牌的预测熵，最后利用熵门控机制将高熵令牌导向完整的PPO更新以促进探索，低熵令牌则导向衰减更新以减少方差并保留知识，同时结合优势函数以避免强化错误。在数学推理基准测试上的实验结果表明，该方法相比CHORD phi基线在AIME上提升了3.8%，在MATH上提升了2.9%，且仅增加了3.4%的计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">Periodic Regularized Q-Learning</div>
<div class="meta-line">Authors: Hyukjun Yang, Han-Dong Lim, Donghwan Lee</div>
<div class="meta-line">First: 2026-02-03T09:28:06+00:00 · Latest: 2026-02-03T09:28:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03301v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03301v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In reinforcement learning (RL), Q-learning is a fundamental algorithm whose convergence is guaranteed in the tabular setting. However, this convergence guarantee does not hold under linear function approximation. To overcome this limitation, a significant line of research has introduced regularization techniques to ensure stable convergence under function approximation. In this work, we propose a new algorithm, periodic regularized Q-learning (PRQ). We first introduce regularization at the level of the projection operator and explicitly construct a regularized projected value iteration (RP-VI), subsequently extending it to a sample-based RL algorithm. By appropriately regularizing the projection operator, the resulting projected value iteration becomes a contraction. By extending this regularized projection into the stochastic setting, we establish the PRQ algorithm and provide a rigorous theoretical analysis that proves finite-time convergence guarantees for PRQ under linear function approximation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>周期性正则化Q学习</div>
<div class="mono" style="margin-top:8px">在强化学习中，Q学习是一种基础算法，其在表格设定下的收敛性已得到保证。然而，在线性函数逼近下该收敛性无法维持。为突破此限制，一个重要研究方向引入正则化技术以确保函数逼近下的稳定收敛。本文提出新算法——周期性正则化Q学习（PRQ）。我们首先在投影算子层面引入正则化，显式构建正则化投影值迭代（RP-VI），进而将其扩展为基于采样的强化学习算法。通过对投影算子进行适当正则化，所得投影值迭代成为压缩映射。通过将正则化投影扩展至随机设定，我们建立了PRQ算法，并提供严格的理论分析，证明PRQ在线性函数逼近下具有有限时间收敛保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the instability of Q-learning under linear function approximation, this paper introduces Periodic Regularized Q-learning (PRQ) to ensure stable convergence. The method first constructs a regularized projected value iteration (RP-VI) by applying regularization to the projection operator, making it a contraction, and then extends this to a sample-based algorithm. The main experimental results, supported by theoretical analysis, demonstrate that PRQ achieves finite-time convergence guarantees in the linear function approximation setting.</div>
<div class="mono" style="margin-top:8px">针对线性函数逼近下Q学习不稳定的问题，本文提出了周期性正则化Q学习（PRQ）以确保稳定收敛。该方法首先通过对投影算子进行正则化，构建了正则化投影值迭代（RP-VI），使其成为收缩映射，随后将其扩展为基于采样的强化学习算法。理论分析表明，PRQ在线性函数逼近下实现了有限时间收敛保证，为主要实验结果提供了支撑。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
