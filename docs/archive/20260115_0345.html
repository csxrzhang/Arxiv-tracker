<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-15 03:45</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260115_0345</div>
    <div class="row"><div class="card">
<div class="title">Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge</div>
<div class="meta-line">Authors: Yao Tang, Li Dong, Yaru Hao, Qingxiu Dong, Furu Wei, Jiatao Gu</div>
<div class="meta-line">First: 2026-01-13T18:48:00+00:00 · Latest: 2026-01-13T18:48:00+00:00</div>
<div class="meta-line">Comments: 21 pages. Code available at https://github.com/GMLR-Penn/Multiplex-Thinking</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08808v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08808v1">PDF</a> · <a href="https://github.com/GMLR-Penn/Multiplex-Thinking">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多路思维：基于词元级分支与合并的推理方法</div>
<div class="mono" style="margin-top:8px">大型语言模型通常通过思维链（CoT）更有效地解决复杂推理任务，但代价是生成长而低带宽的词元序列。相比之下，人类常通过保持对可能后续步骤的概率分布进行软推理。受此启发，我们提出多路思维，一种随机软推理机制：在每个思考步骤中，采样K个候选词元，并将其嵌入聚合为单个连续的多路词元。这既保留了词汇嵌入先验和标准离散生成的采样动态，又能在多路展开上导出可处理的概率分布。因此，多路轨迹可直接通过同策略强化学习（RL）进行优化。重要的是，多路思维具有自适应性：当模型置信度高时，多路词元近乎离散，行为类似标准CoT；当模型不确定时，它能紧凑表示多个可能后续步骤，且不增加序列长度。在多项高难度数学推理基准测试中，多路思维从Pass@1到Pass@1024均持续优于强离散CoT和RL基线方法，同时生成更短的序列。代码与模型检查点发布于https://github.com/GMLR-Penn/Multiplex-Thinking。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that human reasoning often maintains a distribution over plausible next steps, unlike the long, low-bandwidth token sequences of Chain-of-Thought (CoT) in large language models, this paper introduces Multiplex Thinking, a stochastic soft reasoning mechanism. The method samples K candidate tokens at each thinking step and aggregates their embeddings into a single continuous multiplex token, preserving vocabulary embedding priors and sampling dynamics while enabling tractable probability distributions over rollouts, which allows direct optimization via on-policy reinforcement learning. Experimental results on challenging math reasoning benchmarks show that Multiplex Thinking consistently outperforms strong discrete CoT and reinforcement learning baselines across metrics from Pass@1 to Pass@1024 while generating shorter sequences.</div>
<div class="mono" style="margin-top:8px">受人类推理常保持对可能后续步骤的分布这一观察启发，本文提出了多路复用思维，一种随机软推理机制，以解决大语言模型中思维链方法产生的长序列、低带宽问题。该方法在每个思考步骤采样K个候选词元，并将其嵌入聚合为单个连续的多路复用词元，保留了词汇嵌入先验和采样动态，同时实现了对展开轨迹的可处理概率分布，从而可通过策略上强化学习直接优化。在具有挑战性的数学推理基准测试中，实验结果表明，多路复用思维在从Pass@1到Pass@1024的各项指标上均持续优于强离散思维链和强化学习基线，且生成的序列更短。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yucheng Wang, Yufei He, Jiaying Wu, Yilun Zhao, See-Kiong Ng, Cynthia Breazeal, Anh Tuan Luu, Hae Won Park, Bryan Hooi</div>
<div class="meta-line">First: 2026-01-13T17:48:43+00:00 · Latest: 2026-01-13T17:48:43+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08763v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08763v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励罕见策略：面向大语言模型创造性问题求解的独特性感知强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为大语言模型后训练的核心范式，尤其在复杂推理任务中，但其常面临探索坍缩问题：策略过早集中于少数主导推理模式，虽提升pass@1指标，却限制了轨迹级多样性及pass@k增益。本文认为该问题源于对局部词元行为的正则化，而非对解集多样性的关注。为此，我们提出独特性感知强化学习——一种轨迹级目标函数，显式奖励采用罕见高层策略的正确解法。该方法使用基于大语言模型的评判器，依据高层解题策略（忽略表面差异）对同一问题的轨迹进行聚类，并依据聚类规模对策略优势进行逆向加权。由此，正确但新颖的策略将比冗余策略获得更高奖励。在数学、物理和医学推理基准测试中，本方法在大规模采样预算下持续提升pass@$k$指标，在保持pass@1性能的同时显著提高pass@$k$曲线下面积，并能维持探索过程，系统性地发掘更多样化的解题策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the issue of exploration collapse in reinforcement learning for large language models, where policies converge prematurely on dominant reasoning patterns and limit solution diversity, this paper proposes Uniqueness-Aware Reinforcement Learning. The method employs an LLM-based judge to cluster rollouts by high-level solution strategies, ignoring superficial differences, and reweights policy advantages to reward correct but rare strategies more heavily than common ones. Experimental results across mathematics, physics, and medical reasoning benchmarks show that the approach consistently improves pass@k metrics and the area under the pass@k curve without harming pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型强化学习中探索崩溃的问题，即策略过早集中于少数主导推理模式、限制了解决方案多样性，本文提出了独特性感知强化学习方法。该方法利用基于大语言的评判器，根据高层解决方案策略对推理过程进行聚类，忽略表面差异，并依据聚类大小反向调整策略优势，从而对正确但罕见的策略给予更高奖励。在数学、物理和医学推理基准测试中，该方法在不损害单次通过率的前提下，持续提升了多次采样通过率及曲线下面积，同时维持了探索性，并大规模发现了更多样化的解决策略。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis</div>
<div class="meta-line">Authors: Sara Giordano, Kornikar Sen, Miguel A. Martin-Delgado</div>
<div class="meta-line">First: 2025-07-22T14:39:20+00:00 · Latest: 2026-01-13T17:34:14+00:00</div>
<div class="meta-line">Comments: 35 pages, 7 figures, color figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.16641v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.16641v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A reinforcement learning (RL) framework is introduced for the efficient synthesis of quantum circuits that generate specified target quantum states from a fixed initial state, addressing a central challenge in both the Noisy Intermediate-Scale Quantum (NISQ) era and future fault-tolerant quantum computing. The approach utilizes tabular Q-learning, based on action sequences, within a discretized quantum state space, to effectively manage the exponential growth of the space dimension.The framework introduces a hybrid reward mechanism, combining a static, domain-informed reward that guides the agent toward the target state with customizable dynamic penalties that discourage inefficient circuit structures such as gate congestion and redundant state revisits. This is a circuit-aware reward, in contrast to the current trend of works on this topic, which are primarily fidelity-based. By leveraging sparse matrix representations and state-space discretization, the method enables practical navigation of high-dimensional environments while minimizing computational overhead. Benchmarking on graph-state preparation tasks for up to seven qubits, we demonstrate that the algorithm consistently discovers minimal-depth circuits with optimized gate counts. Moreover, extending the framework to a universal gate set still yields low depth circuits, highlighting the algorithm robustness and adaptability. The results confirm that this RL-driven approach, with our completely circuit-aware method, efficiently explores the complex quantum state space and synthesizes near-optimal quantum circuits, providing a resource-efficient foundation for quantum circuit optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合奖励驱动的强化学习在高效量子电路合成中的应用</div>
<div class="mono" style="margin-top:8px">本文提出一种强化学习框架，用于高效合成从固定初始态生成指定目标量子态的量子电路，以应对含噪声中等规模量子时代及未来容错量子计算的核心挑战。该方法在离散化量子态空间中，基于动作序列采用表格型Q学习，有效应对空间维度的指数增长。框架引入混合奖励机制：结合静态领域知识奖励引导智能体趋近目标态，以及可定制的动态惩罚机制抑制低效电路结构（如门拥堵和冗余态重访）。这是一种电路感知的奖励机制，区别于当前该领域主要基于保真度的研究趋势。通过利用稀疏矩阵表示和态空间离散化，该方法能在最小化计算开销的同时，实现对高维环境的实际导航。在最多七量子比特的图态制备任务上进行基准测试，结果表明该算法能持续发现具有优化门数量的最小深度电路。此外，将框架扩展至通用门集仍能获得低深度电路，凸显了算法的鲁棒性和适应性。结果证实，这种采用完全电路感知方法的强化学习驱动方案，能高效探索复杂量子态空间并合成近似最优量子电路，为量子电路优化提供了资源高效的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient quantum circuit synthesis in both NISQ and fault-tolerant quantum computing, this paper introduces a reinforcement learning framework using tabular Q-learning in a discretized quantum state space to manage exponential dimensionality. The method employs a hybrid reward mechanism combining static, domain-informed guidance with dynamic penalties to discourage inefficient circuit structures like gate congestion and redundant state visits, moving beyond purely fidelity-based approaches. Experimental results on graph-state preparation for up to seven qubits show the algorithm consistently finds minimal-depth circuits with optimized gate counts, and it remains robust and adaptable when extended to a universal gate set, confirming its effectiveness in synthesizing near-optimal circuits with low computational overhead.</div>
<div class="mono" style="margin-top:8px">针对嘈杂中型量子（NISQ）时代和未来容错量子计算中量子电路高效合成的核心挑战，本文提出了一种强化学习框架，在离散化量子态空间中使用基于动作序列的表格Q学习来管理空间维度的指数增长。该方法采用混合奖励机制，将静态的领域知识引导奖励与可定制的动态惩罚相结合，以抑制门拥堵和状态重复访问等低效电路结构，区别于当前主流的保真度驱动方法。在多达七个量子比特的图态制备任务上的实验结果表明，该算法能一致地发现具有优化门数的最小深度电路，且扩展到通用门集时仍能保持低深度，凸显了其鲁棒性和适应性，为量子电路优化提供了资源高效的基础。</div>
</details>
</div>
<div class="card">
<div class="title">TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback</div>
<div class="meta-line">Authors: Prithwish Jana, Sam Davidson, Bhavana Bhasker, Andrey Kan, Anoop Deoras, Laurent Callot</div>
<div class="meta-line">First: 2026-01-13T17:08:30+00:00 · Latest: 2026-01-13T17:08:30+00:00</div>
<div class="meta-line">Comments: The paper has been published at the 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE 2026), Rio de Janeiro, Brazil, April 12-18, 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08734v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08734v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TerraFormer：基于策略引导验证器反馈微调大语言模型的自动化基础设施即代码生成框架</div>
<div class="mono" style="margin-top:8px">自动化基础设施即代码（IaC）具有挑战性，大语言模型（LLM）常从自然语言生成错误配置。本文提出TerraFormer——一种结合监督微调与验证器引导强化学习的神经符号框架，通过形式化验证工具提供语法、可部署性及策略合规性反馈。我们通过多阶段验证与迭代式LLM自校正，构建了两个高质量自然语言转IaC数据集：TF-Gen（15.2万实例）和TF-Mutn（5.2万实例）。在包含Sonnet 3.7、DeepSeek-R1、GPT-4.1等约50倍参数量模型的17个前沿LLM对比评估中，TerraFormer在IaC-Eval、TF-Gen（测试集）和TF-Mutn（测试集）上分别将基础LLM正确率提升15.94%、11.65%和19.60%，在两项测试集上超越更大规模模型，于IaC-Eval排名第三，并在最佳实践与安全合规性方面达到最优水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of automating Infrastructure-as-Code (IaC) generation from natural language, where large language models (LLMs) often produce incorrect configurations. It introduces TerraFormer, a neuro-symbolic framework that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. Experimental results show that TerraFormer improves correctness over its base LLM by up to 19.60% on specialized datasets, outperforms much larger models like Sonnet 3.7 and GPT-4.1 on key tests, and achieves high rankings in best-practices and security compliance.</div>
<div class="mono" style="margin-top:8px">该论文针对从自然语言自动生成基础设施即代码（IaC）的挑战，其中大型语言模型（LLM）常产生错误配置。它提出了TerraFormer，一个神经符号框架，结合了监督微调和验证器引导的强化学习，利用形式化验证工具提供语法、可部署性和策略合规性反馈。实验结果表明，TerraFormer在其基础LLM上正确性提升高达19.60%，在关键测试中优于Sonnet 3.7和GPT-4.1等大得多的模型，并在最佳实践和安全合规性方面排名靠前。</div>
</details>
</div>
<div class="card">
<div class="title">Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts</div>
<div class="meta-line">Authors: Bert Verbruggen, Arne Vanhoyweghen, Vincent Ginis</div>
<div class="meta-line">First: 2026-01-13T16:53:40+00:00 · Latest: 2026-01-13T16:53:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08726v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08726v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality&#x27;&#x27; depends on the environment&#x27;s statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network&#x27;s function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process&#x27;s intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent&#x27;s exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非遍历场景下深度强化学习的模型无关解决方案</div>
<div class="mono" style="margin-top:8px">强化学习（RL）仍是机器学习的核心优化框架。尽管RL智能体能够收敛至最优解，但“最优性”的定义取决于环境的统计特性。作为大多数RL算法核心的贝尔曼方程，其构建基于未来奖励的期望值。然而，当遍历性被破坏时，长期结果取决于特定轨迹而非总体均值。在此类场景中，总体均值与个体智能体经历的时间平均增长相偏离，导致期望值公式产生系统性次优策略。先前研究表明，传统RL架构在非遍历环境中无法恢复真实最优解。我们将此分析延伸至深度RL实现，证明其在非遍历动态下同样产生次优策略。在学习过程中引入显式时间依赖性可修正这一局限：通过使网络函数逼近器融合时序信息，智能体能够估算与过程内在增长率一致的价值函数。这一改进无需改变环境反馈机制（如奖励转换或修正目标函数），而是源自智能体对时序轨迹的自然学习。我们的研究成果为日益增长的非遍历系统强化学习方法研究提供了新见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the failure of traditional and deep reinforcement learning (RL) methods to achieve optimal policies in non-ergodic environments, where the standard Bellman equation&#x27;s reliance on expected values leads to systematic suboptimality because ensemble averages diverge from individual time-average growth. The proposed method introduces explicit time dependence into the learning process by allowing the neural network&#x27;s function approximation to incorporate temporal information, enabling the agent to estimate value functions aligned with the process&#x27;s intrinsic growth rate without modifying environmental feedback like rewards or objectives. Experimental results demonstrate that this model-agnostic approach corrects the limitation, allowing deep RL agents to recover policies consistent with true optimum growth in non-ergodic settings, whereas standard implementations remain suboptimal.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于传统和深度强化学习方法在非遍历环境中无法获得最优策略，因为标准贝尔曼方程依赖期望值，导致系统性的次优性，即系综平均与个体时间平均增长发生偏离。所提出的方法通过让神经网络的函数近似纳入时间信息，将显式时间依赖性引入学习过程，使智能体能够在不改变环境反馈（如奖励或目标函数）的情况下，估计与过程内在增长率一致的价值函数。实验结果表明，这种与模型无关的方法纠正了这一局限，使深度强化学习智能体在非遍历环境中能恢复与真实最优增长一致的政策，而标准实现则仍保持次优。</div>
</details>
</div>
<div class="card">
<div class="title">Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following</div>
<div class="meta-line">Authors: Yirong Zeng, Yufei Liu, Xiao Ding, Yutai Hou, Yuxian Wang, Haonan Song, Wu Ning, Dandan Tu, Qixun Zhang, Bibo Cai, Yuxiang He, Ting Liu</div>
<div class="meta-line">First: 2026-01-08T14:00:51+00:00 · Latest: 2026-01-13T16:42:42+00:00</div>
<div class="meta-line">Comments: Under review, 13 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\% in performance while achieving a 58\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>精度优于多样性：高精度奖励泛化至稳健指令跟随</div>
<div class="mono" style="margin-top:8px">在指令跟随任务中，一个核心观点认为：可验证的硬约束与不可验证的软约束的多样化混合对于泛化至未见指令至关重要。本研究通过系统性实证检验挑战了这一主流共识。反直觉的是，我们发现仅使用硬约束训练的模型始终优于混合数据集训练的模型。大量实验表明，奖励精度（而非约束多样性）是实现有效对齐的主要驱动力。大语言模型评判器在检测错误响应时召回率较低，导致严重的奖励破解现象，从而削弱了多样性的优势。注意力机制分析进一步揭示，高精度奖励能形成可迁移的指令跟随元技能。基于这些发现，我们提出一种以数据为中心的精炼策略，优先保障奖励精度。在五个基准测试中，该方法以13.4%的性能优势超越基线模型，训练时间减少58%，且在指令跟随之外的任务中保持强泛化能力。本研究主张范式转变：从盲目追求数据多样性转向聚焦高精度奖励。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the prevailing belief that diverse mixtures of verifiable and unverifiable constraints are essential for generalizing instruction-following models, finding instead that high-precision rewards are the key driver. The method involves systematic empirical comparisons showing models trained solely on hard constraints outperform those on mixed datasets, as low recall in LLM judges for detecting false responses leads to reward hacking that undermines diversity benefits. Experimental results across five benchmarks demonstrate that a proposed data-centric refinement strategy prioritizing reward precision outperforms baselines by 13.4% in performance while reducing training time by 58%, with analysis revealing these rewards develop a transferable meta-skill for robust generalization.</div>
<div class="mono" style="margin-top:8px">本文挑战了当前主流观点，即指令跟随模型的泛化需要可验证与不可验证约束的多样化混合，发现高精度奖励才是关键驱动因素。方法上通过系统实证比较，表明仅使用硬约束训练的模型优于混合数据集训练的模型，因为大语言模型法官在检测错误响应时召回率低，导致奖励黑客攻击从而削弱多样性益处。在五个基准测试上的实验结果表明，所提出的以数据为中心、优先考虑奖励精度的优化策略，其性能比基线高出13.4%，同时训练时间减少58%，分析还揭示这些奖励能培养出可迁移的元技能以实现稳健泛化。</div>
</details>
</div>
<div class="card">
<div class="title">The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection</div>
<div class="meta-line">Authors: Md Shafiqul Islam, Shakti Prasad Padhy, Douglas Allaire, Raymundo Arróyave</div>
<div class="meta-line">First: 2026-01-08T20:52:53+00:00 · Latest: 2026-01-13T16:22:49+00:00</div>
<div class="meta-line">Comments: Included a subsection named &quot;Budgetary impact of inline kernel optimization during BO&quot;, and corrected label of a figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05371v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05371v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>核流形：高斯过程模型选择的几何方法</div>
<div class="mono" style="margin-top:8px">高斯过程回归是一种强大的非参数贝叶斯框架，但其性能关键取决于协方差核的选择。因此，选择合适的核是模型质量的核心，但仍是概率建模中最具挑战性和计算成本最高的步骤之一。我们提出了一种基于核间几何结构的贝叶斯优化框架，利用高斯过程先验间的期望散度距离高效探索核空间。通过该距离矩阵的多维尺度分析嵌入，将离散核库映射至连续欧几里得流形，实现平滑的贝叶斯优化。在此框架中，输入空间由核组合构成，目标函数为对数边际似然，特征化通过多维尺度分析坐标实现。当散度构成有效度量时，该嵌入能保持几何结构并产生稳定的贝叶斯优化景观。我们在合成基准测试、真实世界时间序列数据集以及预测熔池几何形状的增材制造案例研究中验证了该方法，相较于包括大语言模型引导搜索在内的基线方法，取得了更优的预测精度和不确定性校准效果。该框架为核搜索建立了可复用的概率几何结构，对高斯过程建模与深度核学习具有直接应用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the critical challenge of selecting an appropriate covariance kernel in Gaussian Process regression, which is essential for model performance but computationally expensive. The method introduces a Bayesian optimization framework that leverages a geometric approach, using divergence-based distances between GP priors to map a discrete kernel library into a continuous Euclidean manifold via multidimensional scaling, enabling smooth optimization over kernel compositions with the log marginal likelihood as the objective. Experimental results on synthetic benchmarks, real-world time-series data, and an additive manufacturing case study demonstrate superior predictive accuracy and uncertainty calibration compared to baselines like LLM-guided search, establishing a reusable probabilistic geometry for kernel selection.</div>
<div class="mono" style="margin-top:8px">该论文针对高斯过程回归中协方差核选择这一关键挑战展开研究，核选择对模型性能至关重要但计算成本高昂。方法提出了一种基于几何的贝叶斯优化框架，利用高斯过程先验间的散度距离，通过多维缩放将离散核库映射到连续欧几里得流形上，从而实现对核组合的平滑优化，并以对数边际似然为目标函数。在合成基准测试、真实世界时间序列数据集以及一个预测熔池几何形状的增材制造案例研究中，实验结果表明该方法相比基于大语言模型搜索等基线方法，具有更优的预测精度和不确定性校准能力，为核选择建立了一个可复用的概率几何框架。</div>
</details>
</div>
<div class="card">
<div class="title">PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning</div>
<div class="meta-line">Authors: Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei</div>
<div class="meta-line">First: 2026-01-13T16:02:35+00:00 · Latest: 2026-01-13T16:02:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08679v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08679v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PersonaDual：通过自适应推理平衡个性化与客观性</div>
<div class="mono" style="margin-top:8px">随着用户日益期望大语言模型（LLM）与其偏好保持一致，个性化信息变得愈发重要。然而，个性化信息是一把双刃剑：它虽能提升交互体验，却可能损害客观性与事实准确性，尤其当其与问题不匹配时。为缓解此问题，我们提出PersonaDual框架，该框架在单一模型中同时支持通用客观推理与个性化推理，并能根据上下文自适应切换模式。PersonaDual首先通过监督微调（SFT）学习两种推理模式，随后借助我们提出的DualGRPO强化学习算法进一步优化模式选择。在客观性与个性化基准测试上的实验表明，PersonaDual在保留个性化优势的同时减少了干扰，实现了近乎无干扰的性能表现，并能更有效地利用有益的个性化信号来提升客观问题解决能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of balancing personalization with objectivity in LLMs, as user-aligned responses can enhance interaction but risk factual compromise when preferences conflict with queries. To mitigate this, the authors propose PersonaDual, a framework that integrates both general objective and personalized reasoning within a single model, using adaptive mode switching based on context; it is trained via supervised fine-tuning to learn dual reasoning patterns and further optimized with a novel reinforcement learning method called DualGRPO to refine mode selection. Experimental results on objective and personalized benchmarks demonstrate that PersonaDual nearly eliminates interference from unhelpful personalization while effectively leveraging beneficial personalized signals to enhance objective problem-solving, thus preserving personalization benefits without sacrificing factual correctness.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型中个性化与客观性之间的平衡问题展开研究，因为适应用户偏好的回答虽能提升交互体验，但当偏好与查询冲突时可能损害事实准确性。为此，作者提出了PersonaDual框架，该框架在单一模型中整合了通用客观推理和个性化推理，并根据上下文自适应切换模式；其训练首先通过监督微调学习两种推理模式，然后采用新提出的强化学习方法DualGRPO进一步优化模式选择。在客观性和个性化基准测试上的实验结果表明，PersonaDual几乎消除了无益个性化带来的干扰，同时有效利用有益的个性化信号来提升客观问题解决能力，从而在保持个性化优势的同时不牺牲事实正确性。</div>
</details>
</div>
<div class="card">
<div class="title">From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner&#x27;s Tutorial</div>
<div class="meta-line">Authors: Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar</div>
<div class="meta-line">First: 2026-01-13T15:40:55+00:00 · Latest: 2026-01-13T15:40:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08662v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从经典到量子强化学习及其在量子控制中的应用：初学者教程</div>
<div class="mono" style="margin-top:8px">本教程旨在通过清晰、示例驱动的讲解，使本科生更易理解强化学习（RL）。重点在于弥合RL理论与实际编程应用之间的差距，解决学生从概念理解过渡到实现时面临的常见挑战。通过实践示例和易于理解的解释，本教程致力于帮助学生掌握在现实场景中自信应用RL技术所需的基础技能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This tutorial is motivated by the need to make reinforcement learning (RL) more accessible to undergraduate students, bridging the gap between theoretical concepts and practical implementation. The method involves providing clear, example-driven explanations and hands-on coding examples to demystify common challenges in applying RL. The main outcome is an educational resource that equips students with foundational skills to confidently use RL techniques in real-world scenarios, emphasizing practical application over abstract theory.</div>
<div class="mono" style="margin-top:8px">本教程旨在使强化学习对本科生更易理解，以弥合理论概念与实际应用之间的差距。其方法是通过清晰的示例驱动解释和动手编码示例，解决学生在从概念理解转向实现时面临的常见挑战。主要成果是提供了一个教育资源，帮助学生掌握在现实场景中自信应用强化学习技术的基础技能，强调实践应用而非抽象理论。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Safe Reinforcement Learning using Entropy Regularizer</div>
<div class="meta-line">Authors: Abhijit Mazumdar, Rafal Wisniewski, Manuela L. Bujorianu</div>
<div class="meta-line">First: 2026-01-13T15:23:19+00:00 · Latest: 2026-01-13T15:23:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08646v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08646v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于熵正则化的可证明安全强化学习</div>
<div class="mono" style="margin-top:8px">我们研究了在安全约束下学习马尔可夫决策过程最优策略的问题，并将其建模为可达-规避框架。目标是设计在线强化学习算法，确保在学习阶段以任意高概率满足安全约束。为此，我们首先提出了一种基于不确定性乐观原则的算法。在此基础上，我们提出了利用熵正则化的核心算法。我们对两种算法进行了有限样本分析，并推导了其遗憾界。结果表明，熵正则化的引入改善了遗憾界，并显著控制了基于不确定性乐观原则的安全强化学习算法固有的回合间波动性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of ensuring safety during online reinforcement learning in Markov decision processes with reach-avoid constraints, motivated by the need for algorithms that guarantee safety with high probability throughout learning. The method involves first developing an algorithm based on the optimism in the face of uncertainty (OFU) principle, then introducing a main algorithm that incorporates entropy regularization to enhance performance. Experimental results from finite-sample analysis show that the entropy-regularized algorithm achieves improved regret bounds and significantly reduces the episode-to-episode variability common in OFU-based safe RL approaches.</div>
<div class="mono" style="margin-top:8px">本文针对具有安全约束的马尔可夫决策过程，旨在解决在线强化学习期间的安全保障问题，其动机是设计能在学习阶段以任意高概率确保安全约束的算法。方法上，首先提出了一种基于乐观面对不确定性（OFU）原则的算法，进而引入了利用熵正则化的主要算法以优化性能。有限样本分析实验结果表明，熵正则化算法改善了遗憾界，并大幅降低了基于OFU的安全强化学习算法中固有的回合间变异性。</div>
</details>
</div>
<div class="card">
<div class="title">TROJail: Trajectory-Level Optimization for Multi-Turn Large Language Model Jailbreaks with Process Rewards</div>
<div class="meta-line">Authors: Xiqiao Xiong, Ouxiang Li, Zhuo Liu, Moxin Li, Wentao Shi, Fengbin Zhu, Qifan Wang, Fuli Feng</div>
<div class="meta-line">First: 2025-12-08T17:42:59+00:00 · Latest: 2026-01-13T15:14:32+00:00</div>
<div class="meta-line">Comments: 21 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07761v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07761v2">PDF</a> · <a href="https://github.com/xxiqiao/TROJail">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have seen widespread adoption, yet they remain vulnerable to multi-turn jailbreak attacks, threatening their safe deployment. This has led to the task of training automated multi-turn attackers to probe model safety vulnerabilities. However, existing approaches typically rely on turn-level optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate this task as a multi-turn reinforcement learning problem, directly optimizing the harmfulness of the final-turn response as the outcome reward. To address the sparse supervision of the outcome reward, we introduce TROJail, which employs two process rewards to evaluate the utility of intermediate prompts and integrate them into advantage estimation. These rewards (1) penalize overly harmful prompts that trigger the model&#x27;s refusal mechanism, and (2) encourage steering the semantic relevance of responses toward the targeted harmful content. Experimental results show improved attack success rates across multiple models and benchmarks, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/TROJail. Warning: This paper contains examples of harmful content.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TROJail：基于过程奖励的多轮大语言模型越狱轨迹级优化方法</div>
<div class="mono" style="margin-top:8px">大语言模型已获广泛应用，但仍易受多轮越狱攻击威胁，影响其安全部署。为此需训练自动化多轮攻击器以探测模型安全漏洞。现有方法多依赖轮次级优化，难以学习长期攻击策略。为弥补这一不足，我们将该任务构建为多轮强化学习问题，直接以最终轮次回答的危害性作为结果奖励进行优化。针对结果奖励监督稀疏的问题，本文提出TROJail方法，通过两个过程奖励评估中间提示的效用并整合至优势估计：其一对触发模型拒绝机制的过度有害提示进行惩罚，其二引导回答的语义相关性向目标有害内容靠拢。实验表明，该方法在多个模型与基准测试中均提升了攻击成功率，验证了其有效性。代码发布于https://github.com/xxiqiao/TROJail。警告：本文包含有害内容示例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of large language models to multi-turn jailbreak attacks, motivating the development of an automated attacker to probe safety weaknesses. The method formulates the task as a multi-turn reinforcement learning problem, introducing TROJail to optimize trajectory-level outcomes with process rewards that penalize prompts triggering refusal mechanisms and steer responses toward harmful content. Experimental results demonstrate improved attack success rates across multiple models and benchmarks, validating the approach&#x27;s effectiveness in learning long-term attack strategies.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在多轮越狱攻击中的脆弱性，旨在开发自动化攻击器以探测安全漏洞。方法将任务构建为多轮强化学习问题，提出TROJail，通过过程奖励优化轨迹级结果，这些奖励惩罚触发模型拒绝机制的提示并引导响应语义朝向有害内容。实验结果表明，该方法在多个模型和基准测试中提高了攻击成功率，验证了其学习长期攻击策略的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Enabling Population-Level Parallelism in Tree-Based Genetic Programming for GPU Acceleration</div>
<div class="meta-line">Authors: Zhihong Wu, Lishuang Wang, Kebin Sun, Zhuozhao Li, Ran Cheng</div>
<div class="meta-line">First: 2025-01-21T07:42:54+00:00 · Latest: 2026-01-13T14:05:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.17168v6">Abs</a> · <a href="https://arxiv.org/pdf/2501.17168v6">PDF</a> · <a href="https://github.com/EMI-Group/evogp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tree-based Genetic Programming (TGP) is a widely used evolutionary algorithm for tasks such as symbolic regression, classification, and robotic control. Due to the intensive computational demands of running TGP, GPU acceleration is crucial for achieving scalable performance. However, efficient GPU-based execution of TGP remains challenging, primarily due to three core issues: (1) the structural heterogeneity of program individuals, (2) the complexity of integrating multiple levels of parallelism, and (3) the incompatibility between high-performance CUDA execution and flexible Python-based environments. To address these issues, we propose EvoGP, a high-performance framework tailored for GPU acceleration of TGP via population-level parallel execution. First, EvoGP introduces a tensorized representation that encodes variable-sized trees into fixed-shape, memory-aligned arrays, enabling uniform memory access and parallel computation across diverse individuals. Second, EvoGP adopts an adaptive parallelism strategy that dynamically combines intra- and inter-individual parallelism based on dataset size, ensuring high GPU utilization across a broad spectrum of tasks. Third, EvoGP embeds custom CUDA kernels into the PyTorch runtime, achieving seamless integration with Python-based environments such as Gym, MuJoCo, Brax, and Genesis. Experimental results demonstrate that EvoGP achieves a peak throughput exceeding $10^{11}$ GPops/s. Specifcially, this performance represents a speedup of up to $304\times$ over existing GPU-based TGP implementations and $18\times$ over state-of-the-art CPU-based libraries. Furthermore, EvoGP maintains comparable accuracy and exhibits improved scalability across large population sizes. EvoGP is open source and accessible at: https://github.com/EMI-Group/evogp.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实现基于树的遗传编程中种群级并行化以支持GPU加速</div>
<div class="mono" style="margin-top:8px">基于树的遗传编程是一种广泛应用于符号回归、分类和机器人控制等任务的进化算法。由于运行TGP计算密集，GPU加速对于实现可扩展性能至关重要。然而，基于GPU的高效TGP执行仍面临挑战，主要源于三个核心问题：(1) 程序个体的结构异质性，(2) 多层次并行化整合的复杂性，以及(3) 高性能CUDA执行与灵活Python环境之间的不兼容性。为解决这些问题，我们提出了EvoGP——一个通过种群级并行执行为TGP GPU加速定制的高性能框架。首先，EvoGP引入张量化表示法，将可变大小的树编码为固定形状、内存对齐的数组，实现对不同个体的统一内存访问和并行计算。其次，EvoGP采用自适应并行策略，根据数据集规模动态结合个体内与个体间并行化，确保在广泛任务中实现高GPU利用率。第三，EvoGP将定制CUDA内核嵌入PyTorch运行时，实现与Gym、MuJoCo、Brax及Genesis等Python环境的无缝集成。实验结果表明，EvoGP峰值吞吐量超过$10^{11}$ GPops/s，具体表现为：相比现有基于GPU的TGP实现加速达$304\times$，较先进基于CPU的库提升$18\times$。此外，EvoGP在保持相当精度的同时，展现出在大种群规模下更优的可扩展性。EvoGP已开源，访问地址：https://github.com/EMI-Group/evogp。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the high computational demands of Tree-based Genetic Programming (TGP) and the challenges in efficiently accelerating it on GPUs due to structural heterogeneity, parallelism integration complexity, and environment incompatibility. The proposed method, EvoGP, addresses these by introducing a tensorized tree representation for uniform memory access, an adaptive parallelism strategy for dynamic GPU utilization, and embedding custom CUDA kernels into PyTorch for seamless Python integration. Experimental results show that EvoGP achieves a peak throughput exceeding 10^11 GPops/s, delivering speedups of up to 304x over existing GPU-based TGP implementations and 18x over state-of-the-art CPU libraries, while maintaining comparable accuracy and improved scalability with large populations.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于树基遗传编程（TGP）的高计算需求，以及由于结构异质性、并行集成复杂性和环境不兼容性导致其在GPU上高效加速的挑战。所提出的EvoGP方法通过引入张量化树表示以实现统一内存访问、采用自适应并行策略以动态利用GPU、以及将自定义CUDA内核嵌入PyTorch实现无缝Python集成来解决这些问题。实验结果表明，EvoGP实现了超过10^11 GPops/s的峰值吞吐量，相比现有基于GPU的TGP实现加速高达304倍，相比最先进的CPU库加速18倍，同时在保持相当准确性的前提下，对大规模种群表现出更好的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">A Differential Perspective on Distributional Reinforcement Learning</div>
<div class="meta-line">Authors: Juan Sebastian Rojas, Chi-Guhn Lee</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-06-03T19:26:25+00:00 · Latest: 2026-01-13T13:43:59+00:00</div>
<div class="meta-line">Comments: In AAAI Conference on Artificial Intelligence 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.03333v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.03333v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To date, distributional reinforcement learning (distributional RL) methods have exclusively focused on the discounted setting, where an agent aims to optimize a discounted sum of rewards over time. In this work, we extend distributional RL to the average-reward setting, where an agent aims to optimize the reward received per time step. In particular, we utilize a quantile-based approach to develop the first set of algorithms that can successfully learn and/or optimize the long-run per-step reward distribution, as well as the differential return distribution of an average-reward MDP. We derive proven-convergent tabular algorithms for both prediction and control, as well as a broader family of algorithms that have appealing scaling properties. Empirically, we find that these algorithms yield competitive and sometimes superior performance when compared to their non-distributional equivalents, while also capturing rich information about the long-run per-step reward and differential return distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分布强化学习的差分视角</div>
<div class="mono" style="margin-top:8px">迄今为止，分布强化学习方法仅聚焦于折扣设定，即智能体旨在优化随时间折扣的奖励总和。本研究将分布强化学习扩展至平均奖励设定，其中智能体旨在优化每时间步获得的奖励。具体而言，我们采用基于分位数的方法，开发了首套能够成功学习和/或优化长期每步奖励分布以及平均奖励马尔可夫决策过程差分回报分布的算法。我们推导出经证明收敛的表格算法（适用于预测与控制任务），以及一个具备良好扩展性的更广泛算法族。实证研究表明，与非分布等效方法相比，这些算法不仅具有竞争力，有时性能更优，同时还能捕捉关于长期每步奖励与差分回报分布的丰富信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of existing distributional reinforcement learning methods to the discounted setting, this paper extends distributional RL to the average-reward setting, where the goal is to optimize the long-run per-step reward distribution. The method employs a quantile-based approach to develop novel algorithms for learning the differential return distribution in average-reward Markov decision processes, including proven-convergent tabular algorithms for prediction and control and a scalable family of algorithms. Experimentally, these algorithms demonstrate competitive or superior performance compared to non-distributional counterparts while effectively capturing detailed information about the long-run reward and differential return distributions.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有分布强化学习方法仅限于折扣设定，因此将其扩展到平均奖励设定，旨在优化长期每步奖励分布。方法上采用基于分位数的策略，开发了用于学习平均奖励马尔可夫决策过程中差分回报分布的新算法，包括可证明收敛的表格算法用于预测和控制，以及具有良好扩展性的算法族。实验结果表明，这些算法与非分布方法相比具有竞争力或更优性能，同时能有效捕捉长期每步奖励和差分回报分布的丰富信息。</div>
</details>
</div>
<div class="card">
<div class="title">VGC-Bench: Towards Mastering Diverse Team Strategies in Competitive Pokémon</div>
<div class="meta-line">Authors: Cameron Angliss, Jiaxun Cui, Jiaheng Hu, Arrasy Rahman, Peter Stone</div>
<div class="meta-line">First: 2025-06-12T03:19:39+00:00 · Latest: 2026-01-13T13:17:41+00:00</div>
<div class="meta-line">Comments: AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10326v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.10326v3">PDF</a> · <a href="https://github.com/cameronangliss/vgc-bench">Code1</a> · <a href="https://huggingface.co/datasets/cameronangliss/vgc-battle-logs">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing AI agents that can robustly adapt to varying strategic landscapes without retraining is a central challenge in multi-agent learning. Pokémon Video Game Championships (VGC) is a domain with a vast space of approximately $10^{139}$ team configurations, far larger than those of other games such as Chess, Go, Poker, StarCraft, or Dota. The combinatorial nature of team building in Pokémon VGC causes optimal strategies to vary substantially depending on both the controlled team and the opponent&#x27;s team, making generalization uniquely challenging. To advance research on this problem, we introduce VGC-Bench: a benchmark that provides critical infrastructure, standardizes evaluation protocols, and supplies a human-play dataset of over 700,000 battle logs and a range of baseline agents based on heuristics, large language models, behavior cloning, and multi-agent reinforcement learning with empirical game-theoretic methods such as self-play, fictitious play, and double oracle. In the restricted setting where an agent is trained and evaluated in a mirror match with a single team configuration, our methods can win against a professional VGC competitor. We repeat this training and evaluation with progressively larger team sets and find that as the number of teams increases, the best-performing algorithm in the single-team setting has worse performance and is more exploitable, but has improved generalization to unseen teams. Our code and dataset are open-sourced at https://github.com/cameronangliss/vgc-bench and https://huggingface.co/datasets/cameronangliss/vgc-battle-logs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VGC-Bench：迈向精通《宝可梦》竞技对战中的多样化队伍策略</div>
<div class="mono" style="margin-top:8px">开发无需重新训练即可稳健适应不同战略格局的智能体，是多智能体学习的核心挑战。宝可梦电子游戏锦标赛（VGC）的领域空间极为庞大，队伍配置组合数约达10^139种，远超国际象棋、围棋、扑克、《星际争霸》或《Dota》等游戏。宝可梦VGC中队伍构建的组合特性导致最优策略会因己方队伍与对手队伍的差异而发生显著变化，这使得泛化能力面临独特挑战。为推进该问题的研究，我们推出VGC-Bench：一个提供关键基础设施、标准化评估流程、包含超过70万条对战记录的人类对战数据集，以及涵盖启发式方法、大语言模型、行为克隆、结合自我博弈/虚拟博弈/双预言家等经验博弈论方法的多智能体强化学习基线智能体的基准测试平台。在智能体使用单一队伍配置进行镜像对战的训练与评估限制场景中，我们的方法能够战胜职业VGC选手。我们逐步扩大队伍集合重复训练与评估，发现随着队伍数量增加，在单队伍场景中表现最佳的算法胜率下降且更易被针对，但对未见队伍的泛化能力有所提升。代码与数据集已开源：https://github.com/cameronangliss/vgc-bench 与 https://huggingface.co/datasets/cameronangliss/vgc-battle-logs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces VGC-Bench, a benchmark designed to address the challenge of creating AI agents that can generalize across the vast strategic landscape of competitive Pokémon, where team configurations number approximately 10^139, far exceeding other games. The method involves providing infrastructure, standardized evaluation, a dataset of over 700,000 human battle logs, and baseline agents using heuristics, large language models, behavior cloning, and multi-agent reinforcement learning with game-theoretic methods like self-play. Experimental results show that in a restricted mirror-match setting with a single team, their methods can defeat a professional player; however, as the number of teams increases, performance in the single-team setting degrades and becomes more exploitable, though generalization to unseen teams improves.</div>
<div class="mono" style="margin-top:8px">本文介绍了VGC-Bench，这是一个旨在解决在竞技宝可梦游戏中创建能够泛化到庞大策略空间的AI智能体挑战的基准，其中队伍配置数量约为10^139，远超其他游戏。方法包括提供基础设施、标准化评估、超过70万条人类对战日志的数据集，以及使用启发式方法、大语言模型、行为克隆和结合自我博弈等博弈论方法的多智能体强化学习的基线智能体。实验结果表明，在单一队伍的镜像对战限制设置中，其方法能够击败专业玩家；然而，随着队伍数量增加，单一队伍设置下的性能下降且更易被利用，但对未见队伍的泛化能力有所提升。</div>
</details>
</div>
<div class="card">
<div class="title">Your Group-Relative Advantage Is Biased</div>
<div class="meta-line">Authors: Fengkai Yang, Zherui Chen, Xiaohan Wang, Xiaodong Lu, Jiajun Chai, Guojun Yin, Wei Lin, Shuai Ma, Fuzhen Zhuang, Deqing Wang, Yaodong Yang, Jianxin Li, Yikun Ban</div>
<div class="meta-line">First: 2026-01-13T13:03:15+00:00 · Latest: 2026-01-13T13:03:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08521v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08521v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>群体相对优势估计存在偏差</div>
<div class="mono" style="margin-top:8px">基于验证器奖励的强化学习已成为推理任务上对大语言模型进行后训练的常用方法，其中GRPO及其变体等基于群体的方法获得广泛应用。这些方法依赖群体相对优势估计以避免学习判别器，但其理论性质仍缺乏深入理解。本研究揭示了基于群体强化学习的一个根本问题：群体相对优势估计量相对于真实期望优势存在固有偏差。我们首次通过理论分析证明，该方法会系统性地低估困难提示的优势、高估简单提示的优势，导致探索与利用失衡。为解决此问题，我们提出历史感知自适应难度加权——一种基于动态难度锚点与训练过程调整优势估计的自适应重加权方案。在五个数学推理基准上的理论分析与实验均表明，HA-DW在融入GRPO及其变体后能持续提升性能。我们的结果表明，纠正有偏差的优势估计对于实现稳健高效的RLVR训练至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the widespread use of group-based reinforcement learning from verifier rewards (RLVR) methods like GRPO for post-training large language models on reasoning tasks, despite a lack of theoretical understanding of their core group-relative advantage estimator. The method proposed, History-Aware Adaptive Difficulty Weighting (HA-DW), addresses the identified fundamental issue that this estimator is inherently biased, systematically underestimating advantages for hard prompts and overestimating for easy ones, by introducing an adaptive reweighting scheme based on an evolving difficulty anchor and training dynamics. The main experimental results, validated through theoretical analysis and tests on five mathematical reasoning benchmarks, show that integrating HA-DW into GRPO and its variants consistently improves performance, underscoring the importance of correcting biased advantage estimation for robust and efficient RLVR training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管基于群体的验证器奖励强化学习（RLVR）方法（如GRPO）被广泛用于大型语言模型在推理任务上的后训练，但其核心的群体相对优势估计器的理论性质仍不明确。为解决该估计器固有的偏差问题——即系统性地低估困难提示的优势并高估简单提示的优势，本文提出了历史感知自适应难度加权（HA-DW）方法，这是一种基于动态难度锚点和训练过程的自适应重加权方案。主要实验结果通过理论分析和在五个数学推理基准测试上的实验表明，将HA-DW集成到GRPO及其变体中能持续提升性能，这证实了纠正有偏的优势估计对于实现稳健高效的RLVR训练至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">AUV Trajectory Learning for Underwater Acoustic Energy Transfer and Age Minimization</div>
<div class="meta-line">Authors: Mohamed Afouene Melki, Mohammad Shehab, Mohamed-Slim Alouini</div>
<div class="meta-line">First: 2026-01-13T12:23:53+00:00 · Latest: 2026-01-13T12:23:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08491v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08491v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internet of underwater things (IoUT) is increasingly gathering attention with the aim of monitoring sea life and deep ocean environment, underwater surveillance as well as maintenance of underwater installments. However, conventional IoUT devices, reliant on battery power, face limitations in lifespan and pose environmental hazards upon disposal. This paper introduces a sustainable approach for simultaneous information uplink from the IoUT devices and acoustic energy transfer (AET) to the devices via an autonomous underwater vehicle (AUV), potentially enabling them to operate indefinitely. To tackle the time-sensitivity, we adopt age of information (AoI), and Jain&#x27;s fairness index. We develop two deep-reinforcement learning (DRL) algorithms, offering a high-complexity, high-performance frequency division duplex (FDD) solution and a low-complexity, medium-performance time division duplex (TDD) approach. The results elucidate that the proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向水下声能传输与信息年龄最小化的自主水下航行器轨迹学习</div>
<div class="mono" style="margin-top:8px">水下物联网（IoUT）在海洋生物监测、深海环境观测、水下安防及设施维护等领域日益受到关注。然而，依赖电池供电的传统IoUT设备存在寿命限制，且废弃后可能造成环境危害。本文提出一种可持续方案：通过自主水下航行器（AUV）同时实现IoUT设备信息上行链路传输与声能传输（AET），有望使设备实现永久运行。针对时间敏感性问题，我们采用信息年龄（AoI）与Jain公平性指数作为度量标准。开发了两种深度强化学习（DRL）算法：高性能高复杂度的频分双工（FDD）方案，以及中等性能低复杂度的时分双工（TDD）方案。实验结果表明，相较于基线方法，所提FDD与TDD方案能显著降低平均AoI，并提升能量收集效率与数据收集公平性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited lifespan and environmental concerns of battery-powered Internet of Underwater Things (IoUT) devices, this paper proposes a sustainable system where an autonomous underwater vehicle (AUV) simultaneously collects data and delivers acoustic energy to these devices. The method employs deep reinforcement learning (DRL) to design two AUV trajectory learning algorithms: a high-performance frequency division duplex (FDD) approach and a lower-complexity time division duplex (TDD) approach, both aiming to minimize age of information (AoI) and ensure fairness. Experimental results demonstrate that both proposed solutions significantly outperform baseline methods by reducing average AoI, increasing harvested energy, and improving fairness in data collection.</div>
<div class="mono" style="margin-top:8px">针对依赖电池的水下物联网设备寿命有限且存在处置环境危害的问题，本文提出了一种可持续方案，利用自主水下航行器同时从设备收集信息并通过声能传输为其充电。研究方法采用深度强化学习，设计了两种航行器轨迹学习算法：高性能的频分双工方案和较低复杂度的时分双工方案，以最小化信息年龄并保障公平性。实验结果表明，与基线方法相比，所提出的两种方案均能显著降低平均信息年龄、提升能量收集效率并改善数据收集的公平性。</div>
</details>
</div>
<div class="card">
<div class="title">JudgeRLVR: Judge First, Generate Second for Efficient Reasoning</div>
<div class="meta-line">Authors: Jiangshan Duo, Hanyu Li, Hailin Zhang, Yudong Wang, Sujian Li, Liang Zhao</div>
<div class="meta-line">First: 2026-01-13T11:47:42+00:00 · Latest: 2026-01-13T11:47:42+00:00</div>
<div class="meta-line">Comments: 16 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08468v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JudgeRLVR：先判别后生成的高效推理框架</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习已成为大语言模型推理的标准范式。然而，仅针对最终答案正确性进行优化，常导致模型陷入盲目冗长的探索，依赖穷举试错而非结构化规划来求解。虽然长度惩罚等启发式约束可减少冗余，但常会截断关键推理步骤，形成效率与可验证性间的艰难权衡。本文提出判别能力是高效生成的前提：通过学习区分有效解，模型可内化一种剪枝搜索空间的引导信号。我们提出JudgeRLVR两阶段“先判别后生成”范式：第一阶段训练模型判别具有可验证答案的解题响应；第二阶段用判别器初始化模型，通过标准生成式RLVR进行微调。在使用相同数学领域训练数据的情况下，相比标准RLVR，JudgeRLVR为Qwen3-30B-A3B模型实现了更优的质量-效率权衡：在领域内数学任务上，平均准确率提升约3.7分且平均生成长度减少42%；在领域外基准测试中，平均准确率提升约4.5分，展现出更强的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of standard Reinforcement Learning with Verifiable Rewards (RLVR) in large language models, where optimization for final-answer correctness often leads to verbose, unstructured exploration. The authors propose JudgeRLVR, a two-stage paradigm that first trains the model to judge the validity of solution responses, thereby internalizing a discriminative signal to prune the search space, and then fine-tunes it with standard RLVR generation. Experimental results on the Qwen3-30B-A3B model show that JudgeRLVR improves in-domain math accuracy by about +3.7 points while reducing average generation length by 42%, and achieves about +4.5 points higher accuracy on out-of-domain benchmarks, demonstrating better efficiency and generalization.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型中基于可验证奖励的强化学习（RLVR）效率低下的问题，指出仅优化最终答案正确性会导致模型进行冗长、无结构的探索。作者提出JudgeRLVR这一两阶段范式，首先训练模型判断解决方案的有效性，从而内化判别信号以剪枝搜索空间，然后基于此用标准RLVR进行微调生成。在Qwen3-30B-A3B模型上的实验结果表明，JudgeRLVR在领域内数学任务上平均准确率提升约3.7分，同时平均生成长度减少42%，在领域外基准测试上平均准确率提高约4.5分，展现了更好的效率与泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?</div>
<div class="meta-line">Authors: Long Zhang, Yuchen Xia</div>
<div class="meta-line">First: 2026-01-13T11:05:12+00:00 · Latest: 2026-01-13T11:05:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08434v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08434v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向具身智能驾驶的大规模多模态模型：自动驾驶的下一个前沿？</div>
<div class="mono" style="margin-top:8px">大规模多模态模型的出现为解决自动驾驶模块化设计在开放场景中的局限性提供了前景广阔的技术路径，此类场景需持续的环境理解与逻辑推理能力。具身人工智能通过闭环交互促进策略优化，实现持续学习能力，从而推动自动驾驶向具身智能驾驶演进。然而，若仅依赖LMM增强EI驾驶而缺乏联合决策，其能力将受制约。本文提出一种新颖的语义与策略双驱动混合决策框架以应对该挑战，确保持续学习与联合决策。该框架融合LMM实现语义理解与认知表征，并采用深度强化学习进行实时策略优化。我们首先阐述EI驾驶与LMM的基础原理，继而探讨该框架催生的新兴机遇，包括潜在优势与典型应用场景。通过案例实验验证了本框架在完成换道规划任务中的性能优越性。最后，提出了若干赋能EI驾驶的未来研究方向以指导后续工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of modular autonomous driving systems in complex open-world scenarios, this paper proposes a hybrid decision framework that integrates Large Multimodal Models (LMMs) for semantic understanding with Deep Reinforcement Learning (DRL) for policy optimization to achieve embodied intelligent driving. The method combines LMMs&#x27; cognitive representation and reasoning with DRL&#x27;s real-time, closed-loop learning for joint decision-making. The main experimental results from a lane-change planning case study demonstrate the performance superiority of this dual-driven framework in completing the driving task.</div>
<div class="mono" style="margin-top:8px">本文针对模块化自动驾驶系统在复杂开放场景中的局限性，提出了一种混合决策框架，将大型多模态模型（LMMs）的语义理解与深度强化学习（DRL）的策略优化相结合，以实现具身智能驾驶。该方法融合了LMMs的认知表征与推理能力，以及DRL的实时闭环学习能力，以支持联合决策。通过车道变换规划任务的案例实验，主要结果表明该双驱动框架在完成驾驶任务方面具有性能优越性。</div>
</details>
</div>
<div class="card">
<div class="title">RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation</div>
<div class="meta-line">Authors: Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, Wei Chen</div>
<div class="meta-line">First: 2026-01-13T10:56:39+00:00 · Latest: 2026-01-13T10:56:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08430v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08430v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RubricHub：基于自动化由粗到细生成框架构建的全面且高区分度评分标准数据集</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）在数学等推理密集型领域取得了显著进展，但开放生成任务的优化仍因缺乏真实标注而面临挑战。基于评分标准的评估虽能提供结构化验证代理，但现有方法存在可扩展性瓶颈与标准粗糙问题，导致监督效果受限。为此，我们提出自动化由粗到细评分标准生成框架，通过融合原则引导合成、多模型聚合与难度演进机制，生成能捕捉细微差异的全面高区分度评估标准。基于此框架，我们构建了RubricHub——一个大规模（约11万条）多领域数据集。通过包含基于评分标准的拒绝采样微调（RuFT）与强化学习（RuRL）的两阶段后训练流程验证其有效性：实验表明RubricHub带来显著性能提升，后训练的Qwen3-14B模型在HealthBench上达到69.3分的最先进水平，超越GPT-5等前沿闭源模型。代码与数据即将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of optimizing open-ended text generation in reinforcement learning, where the lack of ground truth limits progress, by proposing an automated Coarse-to-Fine Rubric Generation framework that combines principle-guided synthesis, multi-model aggregation, and difficulty evolution to create detailed and discriminative evaluation criteria. This framework is used to build RubricHub, a large-scale multi-domain dataset of approximately 110k entries, and its utility is validated through a two-stage post-training pipeline involving Rubric-based Rejection Sampling Fine-Tuning and Reinforcement Learning. Experimental results show that models post-trained with RubricHub achieve state-of-the-art performance, with Qwen3-14B scoring 69.3 on HealthBench, outperforming proprietary models like GPT-5.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中开放文本生成优化因缺乏真实标注而受限的挑战，提出了一种自动化的从粗到细的评分标准生成框架，该框架结合了原则引导合成、多模型聚合和难度演化，以创建细致且区分度高的评估标准。基于此框架，构建了RubricHub这一大规模多领域数据集（约11万条），并通过包含基于评分标准的拒绝采样微调和强化学习的两阶段后训练流程验证其效用。实验结果表明，使用RubricHub后训练的模型实现了最先进的性能，其中Qwen3-14B在HealthBench上得分69.3，超越了GPT-5等专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering</div>
<div class="meta-line">Authors: Nonghai Zhang, Weitao Ma, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Jingwen Xu</div>
<div class="meta-line">First: 2026-01-13T10:55:08+00:00 · Latest: 2026-01-13T10:55:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08427v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08427v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid&#x27;&#x27; through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>静默裁判：基于潜在几何聚类的自验证强化学习</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）显著提升了大型语言模型（LLM）的推理性能，但其成功严重依赖昂贵的外部验证器或人工规则。这种依赖不仅导致高昂的计算成本和训练延迟，还产生稀疏奖励，阻碍优化效率。为解决这些问题，我们提出Latent-GRPO框架，直接从潜在空间几何结构推导内在奖励。关键的是，实证分析揭示了一个引人注目的几何特性：正确推理轨迹的终止词表征形成具有高类内相似度的密集聚类，而错误轨迹则作为离群点分散分布。基于这一发现，我们提出迭代鲁棒质心估计（IRCE）算法，通过球面投影缓解幅度波动，并通过迭代聚合估计鲁棒的“真值质心”，从而生成密集连续的奖励。在多数据集上的实验结果表明，本方法在保持模型性能的同时，相比基线实现了超过2倍的训练加速。此外，大量结果证明了其强大的泛化能力和鲁棒性。代码即将发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high computational cost and sparse rewards of using external verifiers in Group Relative Policy Optimization (GRPO) for enhancing LLM reasoning. To address this, the authors propose Latent-GRPO, a method that leverages latent geometric clustering by observing that correct reasoning trajectories form dense clusters in representation space, while incorrect ones scatter as outliers. They introduce the Iterative Robust Centroid Estimation (IRCE) algorithm to generate dense, continuous rewards via spherical projection and robust centroid estimation. Experimental results across multiple datasets show that the method maintains performance while achieving over 2x training speedup compared to baselines, with additional demonstrations of strong generalization and robustness.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，使用外部验证器进行群体相对策略优化（GRPO）以增强大语言模型推理时，存在计算成本高和奖励稀疏的问题。为解决此问题，作者提出了Latent-GRPO方法，该方法利用潜在几何聚类，观察到正确推理轨迹在表示空间中形成密集簇，而错误轨迹则分散为异常值。他们引入了迭代鲁棒质心估计（IRCE）算法，通过球面投影和鲁棒质心估计来生成密集、连续的奖励。在多个数据集上的实验结果表明，该方法在保持模型性能的同时，相比基线实现了超过2倍的训练加速，并展现出强大的泛化能力和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs</div>
<div class="meta-line">Authors: Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy</div>
<div class="meta-line">First: 2026-01-13T10:17:46+00:00 · Latest: 2026-01-13T10:17:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08403v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08403v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens&#x27; marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&amp;M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>欧文-沙普利策略优化（OSPO）：一种面向生成式搜索大语言模型的原则性强化学习算法</div>
<div class="mono" style="margin-top:8px">大语言模型日益通过强化学习训练以完成个性化推荐任务，但GRPO等标准方法依赖稀疏的序列级奖励，导致信用分配缺口，难以确定驱动成功的关键词元。当模型必须从缺乏真实标签的模糊语言中推断潜在用户意图时——这种推理模式在预训练阶段极少出现——该问题尤为突出。我们提出欧文-沙普利策略优化（OSPO）框架，该框架基于词元对结果的边际贡献重新分配序列级优势。与需要额外计算的价值模型方法不同，OSPO通过沙普利-欧文归因实现基于势函数的奖励塑形，在保持最优策略的同时分配片段级信用，无需参数化价值模型即可直接从任务反馈中学习。通过构建语义连贯单元（描述产品属性的短语或捕捉偏好的句子）的联盟，OSPO能识别驱动性能的响应部分。在亚马逊ESCI和H&amp;M时尚数据集上的实验显示，该方法较基线模型取得持续增益，并对训练时未见的分布外检索器表现出显著的测试时鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the credit assignment problem in reinforcement learning for large language models used in personalized recommendation, where sparse sequence-level rewards obscure token-level contributions, especially when inferring latent user intent from ambiguous queries. The authors propose Owen-Shapley Policy Optimization (OSPO), a method that redistributes sequence-level advantages by computing tokens&#x27; marginal contributions via Shapley-Owen attributions, using potential-based reward shaping to assign credit to semantically coherent segments without requiring parametric value models. Experimental results on Amazon ESCI and H&amp;M Fashion datasets demonstrate that OSPO consistently outperforms baselines and exhibits robustness to out-of-distribution retrievers at test time.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在个性化推荐任务中强化学习面临的信用分配问题，即稀疏的序列级奖励难以明确各令牌的贡献，尤其是在从模糊查询中推断潜在用户意图时。作者提出了欧文-沙普利策略优化（OSPO）方法，该方法通过沙普利-欧文归因计算令牌的边际贡献，重新分配序列级优势，利用基于势的奖励塑造为语义连贯的片段分配信用，无需参数化价值模型。在亚马逊ESCI和H&amp;M时尚数据集上的实验表明，OSPO一致优于基线方法，并在测试时对训练中未见过的分布外检索器表现出鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Memory in the Age of AI Agents</div>
<div class="meta-line">Authors: Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, Senjie Jin, Jiejun Tan, Yanbin Yin, Jiongnan Liu, Zeyu Zhang, Zhongxiang Sun, Yutao Zhu, Hao Sun, Boci Peng, Zhenrong Cheng, Xuanbo Fan, Jiaxin Guo, Xinlei Yu, Zhenhong Zhou, Zewen Hu, Jiahao Huo, Junhao Wang, Yuwei Niu, Yu Wang, Zhenfei Yin, Xiaobin Hu, Yue Liao, Qiankun Li, Kun Wang, Wangchunshu Zhou, Yixin Liu, Dawei Cheng, Qi Zhang, Tao Gui, Shirui Pan, Yan Zhang, Philip Torr, Zhicheng Dou, Ji-Rong Wen, Xuanjing Huang, Yu-Gang Jiang, Shuicheng Yan</div>
<div class="meta-line">First: 2025-12-15T17:22:34+00:00 · Latest: 2026-01-13T09:33:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13564v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.13564v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人工智能代理时代的记忆</div>
<div class="mono" style="margin-top:8px">记忆已成为并仍将是基于基础模型的智能代理的核心能力。随着代理记忆研究迅速扩展并受到空前关注，该领域也日益呈现碎片化。现有关于代理记忆的研究在动机、实现方式和评估标准上往往存在显著差异，而定义松散的记忆术语泛滥进一步模糊了概念清晰度。传统分类（如长/短期记忆）已不足以涵盖当代代理记忆系统的多样性。本文旨在提供当前代理记忆研究的最新全景。我们首先明确界定代理记忆的范围，并将其与相关概念（如大语言模型记忆、检索增强生成和上下文工程）区分开来。接着，我们从形式、功能和动态三个统一视角审视代理记忆：从形式角度，我们识别出三种主流实现方式——令牌级记忆、参数化记忆和潜在记忆；从功能角度，我们提出更细粒度的分类法，区分事实记忆、经验记忆和工作记忆；从动态角度，我们分析记忆如何随时间形成、演化和检索。为支持实际开发，我们汇编了全面的记忆基准测试和开源框架总结。在整合现有成果的基础上，我们阐述了前瞻性研究前沿，包括记忆自动化、强化学习集成、多模态记忆、多代理记忆及可信度问题。我们希望本综述不仅能作为现有工作的参考，更能为将记忆重新定位为未来智能代理设计中的一等原语提供概念基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the growing yet fragmented research landscape surrounding memory in AI agents, where inconsistent terminologies and taxonomies obscure progress. The method involves a systematic survey that delineates agent memory from related concepts and analyzes it through three unified lenses: forms (token-level, parametric, latent memory), functions (factual, experiential, working memory), and dynamics (formation, evolution, retrieval). The main experimental results include a comprehensive compilation of memory benchmarks and open-source frameworks, alongside a forward-looking discussion of emerging frontiers such as memory automation and multimodal memory, aiming to establish a conceptual foundation for future agent design.</div>
<div class="mono" style="margin-top:8px">本文的动机在于人工智能智能体记忆研究领域快速发展但日益碎片化，术语定义松散且传统分类不足以涵盖当代系统多样性。研究方法是通过系统性综述，明确智能体记忆的范围，并从形式（如令牌级、参数化、潜在记忆）、功能（如事实性、经验性、工作记忆）和动态（如形成、演化、检索）三个统一视角进行分析。主要实验成果包括汇编了全面的记忆基准测试和开源框架，并展望了记忆自动化、多模态记忆等新兴前沿，旨在为未来智能体设计提供记忆作为核心原语的概念基础。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition</div>
<div class="meta-line">Authors: Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos</div>
<div class="meta-line">First: 2026-01-13T08:30:43+00:00 · Latest: 2026-01-13T08:30:43+00:00</div>
<div class="meta-line">Comments: 7 pages, 4 figures, submitted to the IFAC World Congress 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08327v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08327v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent&#x27;s policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents&#x27; communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe and stable task execution, confirming the framework&#x27;s effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于通信正则化的异构多智能体强化学习安全协同目标捕获框架</div>
<div class="mono" style="margin-top:8px">本文提出一种去中心化多智能体强化学习框架，使结构异构的智能体团队能够在部分可观测、通信受限且存在动态交互的环境中协同发现并捕获随机分布的目标。各智能体策略采用多智能体近端策略优化算法训练，并配备图注意力网络编码器，该编码器融合模拟距离传感数据与相邻智能体间交换的通信嵌入向量，实现基于局部传感与关系信息的上下文感知决策。本工作特别提出统一框架，通过安全滤波器集成基于图的通信机制与轨迹感知安全策略。该架构采用结构化奖励设计，通过促进信息正交性来激励有效目标发现与捕获、碰撞规避以及智能体通信向量间的解耦。通过系统消融实验验证了所提奖励函数的有效性。仿真结果进一步展示了安全稳定的任务执行性能，证实了框架的实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for safe and efficient coordination among structurally diverse agents in partially observable environments with communication limits, this paper proposes a decentralized multi-agent reinforcement learning framework for coordinated target acquisition. The method employs Multi-Agent Proximal Policy Optimization with a Graph Attention Network encoder that fuses local sensing and communicated embeddings, integrated with safety filters for trajectory-aware safety and a reward function promoting target acquisition, collision avoidance, and communication vector de-correlation. Experimental results from simulations and an ablation study demonstrate safe, stable task execution and the effectiveness of the designed reward formulation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决异构多智能体在部分可观测、通信受限的动态环境中安全协调获取目标的问题，提出了一种去中心化的多智能体强化学习框架。该方法采用多智能体近端策略优化算法，结合图注意力网络编码器融合局部感知与邻域通信信息，并集成安全滤波器实现轨迹级安全，同时设计了一种鼓励目标获取、避撞及通信向量解相关的结构化奖励函数。仿真实验和消融研究表明，该框架能够实现安全稳定的任务执行，验证了所提奖励函数的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation</div>
<div class="meta-line">Authors: Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin</div>
<div class="meta-line">First: 2026-01-13T08:22:28+00:00 · Latest: 2026-01-13T08:22:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08323v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08323v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AtomMem：基于原子内存操作的可学习动态智能体记忆</div>
<div class="mono" style="margin-top:8px">为智能体配备记忆对于解决现实世界中的长程问题至关重要。然而，现有的大多数智能体记忆机制依赖于静态且人工设计的工作流程，这限制了其性能与泛化能力，凸显了对更灵活、基于学习的记忆框架的需求。本文提出AtomMem，将记忆管理重构为一个动态决策问题：通过将高层记忆过程解构为基础原子CRUD（创建、读取、更新、删除）操作，将记忆工作流转化为可学习的决策过程。结合监督微调与强化学习，AtomMem能够学习自主的、任务对齐的策略，以协调适应特定任务需求的记忆行为。在三个长上下文基准测试中的实验结果表明，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法。对训练动态的进一步分析表明，这种基于学习的框架使智能体能够发现结构化、任务对齐的记忆管理策略，凸显了其相对于预定义流程的关键优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of static, hand-crafted memory workflows in agents for long-horizon tasks, which hinder performance and generalization. To address this, the authors propose AtomMem, a learnable memory framework that reframes memory management as a dynamic decision-making problem by decomposing it into fundamental atomic CRUD operations, making the workflow a learnable process. Through a combination of supervised fine-tuning and reinforcement learning, AtomMem learns an autonomous policy to orchestrate memory behaviors tailored to task demands. Experimental results on three long-context benchmarks show that AtomMem-8B consistently outperforms prior static memory methods, with further analysis revealing that the learning-based approach enables the discovery of structured, task-aligned memory management strategies, highlighting its advantage over predefined routines.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，现有智能体用于长程任务的静态、手工设计的内存工作流存在局限性，影响了性能和泛化能力。为此，作者提出了AtomMem，一种可学习的内存框架，通过将内存管理解构为基本的原子CRUD操作，将其重新定义为动态决策问题，从而使工作流可学习。结合监督微调和强化学习，AtomMem学习了一种自主策略，以根据任务需求协调内存行为。在三个长上下文基准测试上的实验结果表明，训练后的AtomMem-8B持续优于先前的静态工作流内存方法，进一步分析训练动态显示，这种基于学习的方法使智能体能够发现结构化、与任务对齐的内存管理策略，凸显了其相对于预定义流程的关键优势。</div>
</details>
</div>
<div class="card">
<div class="title">ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning</div>
<div class="meta-line">Authors: Kun Liang, Clive Bai, Xin Xu, Chenming Tang, Sanwoo Lee, Weijie Liu, Saiyong Yang, Yunfang Wu</div>
<div class="meta-line">First: 2026-01-13T07:57:48+00:00 · Latest: 2026-01-13T07:57:48+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08310v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08310v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORBIT：面向可控多预算推理的在线策略探索-利用框架</div>
<div class="mono" style="margin-top:8px">近期的大型推理模型通过利用长链思维链推理展现出强大性能，但在推理时统一应用过长的推理过程会产生高昂且常不必要的计算成本。为解决此问题，先前研究探索了多种从输入推断适当推理预算的策略。然而，这类方法在最坏情况下并不可靠，因为估计所需的最小推理量本质上具有难度，且它们在训练过程中隐式固定了推理成本与准确性的权衡，限制了不同部署场景下的灵活性。基于这些局限，我们提出ORBIT——一种由输入触发的、具有明确分离推理模式的可控多预算推理框架。ORBIT采用多阶段强化学习探索各推理量级的帕累托最优行为，再通过在线策略蒸馏将这些行为融合至单一统一模型。实验表明，ORBIT实现了：（1）跨多模式的可控推理行为；（2）各模式内具有竞争力的推理密度；（3）将前沿策略集成至单一学生模型，同时保持清晰的模式分离与各模式的高性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of uniformly applying long reasoning chains in Large Reasoning Models, which incurs unnecessary computational costs, and the inflexibility of prior methods that fix the cost-accuracy trade-off during training. To overcome this, the authors propose ORBIT, a controllable multi-budget reasoning framework that uses multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at different effort levels and then distills these into a single unified model via on-policy distillation. Experimental results demonstrate that ORBIT achieves controllable reasoning across multiple modes, maintains competitive reasoning density within each mode, and successfully integrates diverse policies into one model while preserving mode separation and high performance.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型统一应用长推理链导致计算成本过高的问题，以及现有方法在训练中固定成本-精度权衡而缺乏灵活性的局限性，提出了ORBIT这一可控多预算推理框架。该方法采用多阶段强化学习来发现不同推理努力水平下的帕累托最优行为，并通过在线策略蒸馏将这些行为融合到单一统一模型中。实验结果表明，ORBIT实现了跨多种模式的可控推理，在每个模式下保持了有竞争力的推理密度，并将这些前沿策略集成到一个模型中，同时确保了清晰的模式分离和高效能。</div>
</details>
</div>
<div class="card">
<div class="title">GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition</div>
<div class="meta-line">Authors: Jingchao Wang, Yifan He, Haote Yang, Jiang Wu, Lingli Ge, Xingjian Wei, Yinfan Wang, Linye Li, Huijie Ao, Chengjin Liu, Bin Wang, Lijun Wu, Conghui He</div>
<div class="meta-line">First: 2025-06-09T08:47:10+00:00 · Latest: 2026-01-13T07:21:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07553v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.07553v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optical Chemical Structure Recognition (OCSR) is essential for converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown promise, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To address these issues, we introduce GTR-VL, featuring two key innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric \textit{Faithfully Recognize What You&#x27;ve Seen} principle, which aligns abbreviated structures in images with their expanded annotations. For hand-drawn OCSR tasks, where datasets lack graph annotations and only provide final SMILES, we apply reinforcement learning using the GRPO method, introducing reward mechanisms like format reward, graph reward, and SMILES reward. This approach significantly enhances performance in hand-drawn recognition tasks through weak supervision. We developed GTR-1.3M, a large-scale instruction-tuning dataset with corrected annotations, and MolRec-Bench, the first benchmark for fine-grained evaluation of graph-parsing accuracy in OCSR. Our two-stage training scheme involves SFT training for printed images and the GRPO method for transferring capabilities to hand-drawn tasks. Experiments show that GTR-VL outperforms specialist models, chemistry-domain VLMs, and commercial VLMs on both printed and hand-drawn datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GTR-CoT：图遍历作为分子结构识别的视觉思维链</div>
<div class="mono" style="margin-top:8px">光学化学结构识别（OCSR）对于将分子图像转换为机器可读格式至关重要。尽管近期视觉语言模型（VLMs）展现出潜力，但其图像描述方法在处理复杂分子结构和不一致标注时仍存在困难。为解决这些问题，我们提出GTR-VL模型，其具备两项关键创新：（1）采用“图遍历作为视觉思维链”机制，通过顺序预测原子-键合来模拟人类逐步解析分子图的推理过程；（2）遵循以数据为中心的“忠实识别所见结构”原则，实现图像中简写结构与扩展标注的对齐。针对手绘OCSR任务（数据集缺乏图结构标注、仅提供最终SMILES），我们采用GRPO强化学习方法，引入格式奖励、图结构奖励和SMILES奖励机制，通过弱监督显著提升手绘识别性能。我们构建了包含修正标注的大规模指令调优数据集GTR-1.3M，以及首个用于OCSR图解析细粒度评估的基准测试集MolRec-Bench。采用两阶段训练方案：先对印刷图像进行监督微调训练，再通过GRPO方法将能力迁移至手绘任务。实验表明，GTR-VL在印刷和手绘数据集上均超越专业模型、化学领域VLMs及商业VLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing vision-language models in Optical Chemical Structure Recognition (OCSR), which often fail with complex molecular structures and inconsistent annotations, this paper introduces GTR-VL with two key innovations: a Graph Traversal as Visual Chain of Thought mechanism that incrementally parses molecular graphs through sequential atom-bond predictions, and a data-centric principle to align abbreviated structures with expanded annotations. The method employs a two-stage training scheme, using supervised fine-tuning for printed images and reinforcement learning (GRPO method) with reward mechanisms for hand-drawn tasks, supported by a newly created large-scale dataset GTR-1.3M and benchmark MolRec-Bench. Experimental results demonstrate that GTR-VL outperforms specialist models, chemistry-domain VLMs, and commercial VLMs on both printed and hand-drawn datasets, significantly enhancing performance in hand-drawn recognition through weak supervision.</div>
<div class="mono" style="margin-top:8px">针对现有视觉语言模型在光学化学结构识别（OCSR）中处理复杂分子结构和标注不一致的不足，本文提出了GTR-VL模型，其核心创新包括：模拟人类推理的“图遍历作为视觉思维链”机制，通过顺序预测原子和键来逐步解析分子图；以及数据中心的“忠实识别所见”原则，用于对齐图像中的缩写结构与扩展标注。方法采用两阶段训练方案，对印刷图像使用监督微调，对手绘任务则应用强化学习（GRPO方法）并引入格式、图和SMILES奖励机制，同时构建了大规模指令调优数据集GTR-1.3M和首个细粒度图解析评估基准MolRec-Bench。实验结果表明，GTR-VL在印刷和手绘数据集上均优于专业模型、化学领域视觉语言模型及商业视觉语言模型，通过弱监督显著提升了手绘识别性能。</div>
</details>
</div>
<div class="card">
<div class="title">RAG-R1: Incentivizing the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism</div>
<div class="meta-line">Authors: Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu</div>
<div class="meta-line">First: 2025-06-30T09:02:45+00:00 · Latest: 2026-01-13T06:28:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02962v6">Abs</a> · <a href="https://arxiv.org/pdf/2507.02962v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs), despite their remarkable capabilities, are prone to generating hallucinated or outdated content due to their static internal knowledge. While Retrieval-Augmented Generation (RAG) integrated with Reinforcement Learning (RL) offers a solution, these methods are fundamentally constrained by a single-query mode, leading to prohibitive latency and inherent brittleness. To overcome these limitations, we introduce RAG-R1, a novel two-stage training framework centered around multi-query parallelism. Our framework enables LLMs to adaptively leverage internal and external knowledge during the reasoning process while transitioning from the single-query mode to multi-query parallelism. This architectural shift bolsters reasoning robustness while significantly reducing inference latency. Extensive experiments on seven question-answering benchmarks confirm the superiority of our method, which outperforms the strongest baseline by up to 13.7% and decreases inference time by 11.1%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAG-R1：通过多查询并行化激励大语言模型的检索与推理能力</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）虽能力卓越，但其静态内部知识易导致生成虚假或过时内容。检索增强生成（RAG）与强化学习（RL）结合虽能缓解此问题，但现有方法受限于单查询模式，存在延迟过高与本质脆弱的缺陷。为突破这些限制，我们提出RAG-R1——一种以多查询并行为核心的新型两阶段训练框架。该框架使LLMs在推理过程中自适应融合内外知识，同时实现从单查询到多查询并行的模式转换。这一架构变革显著提升了推理鲁棒性，并将推理延迟降低11.1%。在七个问答基准上的大量实验证实了本方法的优越性，其性能最高超越基线13.7%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of Retrieval-Augmented Generation (RAG) methods, which often suffer from high latency and brittleness due to reliance on single-query processing, leading to hallucinations or outdated outputs in Large Language Models (LLMs). To overcome this, the authors propose RAG-R1, a two-stage training framework that introduces multi-query parallelism, allowing LLMs to dynamically utilize both internal and external knowledge during reasoning, thereby enhancing robustness and reducing inference time. Experimental results across seven question-answering benchmarks demonstrate that RAG-R1 outperforms the strongest baseline by up to 13.7% in performance while reducing inference latency by 11.1%.</div>
<div class="mono" style="margin-top:8px">该论文针对检索增强生成（RAG）方法因依赖单查询处理而存在高延迟和脆弱性的问题，这导致大语言模型（LLM）容易产生幻觉或过时内容。为此，作者提出了RAG-R1，一个基于多查询并行的两阶段训练框架，使LLM能够在推理过程中动态利用内部和外部知识，从而增强鲁棒性并降低推理时间。在七个问答基准上的实验结果表明，RAG-R1在性能上比最强基线提升高达13.7%，同时推理延迟降低了11.1%。</div>
</details>
</div>
<div class="card">
<div class="title">Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks</div>
<div class="meta-line">Authors: Abdikarim Mohamed Ibrahim, Rosdiadee Nordin</div>
<div class="meta-line">First: 2026-01-13T06:23:21+00:00 · Latest: 2026-01-13T06:23:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08254v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08254v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型人工智能模型引导的非地面网络资源分配深度强化学习</div>
<div class="mono" style="margin-top:8px">大型人工智能模型（LAM）已被提出应用于非地面网络（NTN），其强大的泛化能力和减少的任务特定训练需求可带来更优性能。本文提出一种由大型语言模型（LLM）引导的深度强化学习（DRL）智能体。LLM作为高层协调器，生成文本化引导信息以塑造DRL智能体训练过程中的奖励机制。实验结果表明：在吞吐量、公平性和中断概率指标上，LAM-DRL方法相比启发式算法，在常规天气场景下性能提升40%，在极端天气场景下提升64%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to leverage the strong generalization capabilities of Large AI Models (LAMs) to enhance resource allocation in Non-Terrestrial Networks (NTNs), aiming to reduce the need for extensive task-specific training. The proposed method introduces a Deep Reinforcement Learning (DRL) agent guided by a Large Language Model (LLM), where the LLM acts as a high-level coordinator providing textual guidance to shape the DRL agent&#x27;s reward function during training. Experimental results demonstrate that this LAM-DRL approach significantly outperforms traditional DRL, achieving 40% higher performance in nominal weather scenarios and 64% higher in extreme weather scenarios compared to heuristic baselines, as measured by throughput, fairness, and outage probability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是利用大型人工智能模型强大的泛化能力来提升非地面网络中的资源分配性能，旨在减少对大量任务特定训练的需求。所提出的方法引入了一个由大型语言模型引导的深度强化学习智能体，其中大型语言模型作为高级协调器，在训练过程中生成文本指导来塑造深度强化学习智能体的奖励函数。实验结果表明，与传统的启发式方法相比，这种大型人工智能模型引导的深度强化学习方法在吞吐量、公平性和中断概率方面表现显著更优，在正常天气场景下性能提升40%，在极端天气场景下性能提升64%。</div>
</details>
</div>
<div class="card">
<div class="title">Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making</div>
<div class="meta-line">Authors: Liu He</div>
<div class="meta-line">First: 2026-01-13T06:09:24+00:00 · Latest: 2026-01-13T06:09:24+00:00</div>
<div class="meta-line">Comments: 15 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08247v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08247v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将认知偏差融入强化学习以优化金融决策</div>
<div class="mono" style="margin-top:8px">金融市场受人类非理性行为影响，认知偏差导致决策偏离理性。传统强化学习（RL）金融决策模型假设理性主体，可能忽略心理因素的作用。本研究将认知偏差整合到金融交易的RL框架中，假设此类模型能模拟人类交易行为，并在风险调整后收益上优于标准RL代理。我们引入过度自信和损失厌恶等偏差至奖励结构和决策过程，并在模拟及真实交易环境中评估其表现。尽管结果不确定或负面，本研究为将类人偏差融入RL的挑战提供了见解，为开发稳健的金融AI系统提供了宝贵经验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the influence of human cognitive biases on financial markets, this study challenges the rational-agent assumption in traditional reinforcement learning (RL) models for trading. The method integrates biases like overconfidence and loss aversion into RL reward structures and decision processes to create more human-like agents. Experimental results from simulated and real-world environments, however, were inconclusive or negative, highlighting the challenges of such integration but offering valuable insights for developing robust financial AI systems.</div>
<div class="mono" style="margin-top:8px">本研究受金融市场中人类认知偏差的影响所驱动，旨在挑战传统强化学习交易模型中理性代理人的假设。其方法将过度自信和损失厌恶等偏差整合到强化学习的奖励结构和决策过程中，以创建更类人的交易代理。然而，在模拟和真实交易环境中的实验结果并不明确甚至为负面，这凸显了将此类偏差融入模型所面临的挑战，同时也为开发稳健的金融人工智能系统提供了宝贵的经验教训。</div>
</details>
</div>
<div class="card">
<div class="title">The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination</div>
<div class="meta-line">Authors: Haoran Su, Yandong Sun, Congjia Yu</div>
<div class="meta-line">First: 2026-01-13T05:47:18+00:00 · Latest: 2026-01-13T05:47:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08237v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08237v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励工程学的终结：大语言模型如何重塑多智能体协作</div>
<div class="mono" style="margin-top:8px">奖励工程学——即通过人工设计奖励函数以引导智能体达成预期行为——始终是多智能体强化学习领域的核心难题。信用分配模糊性、环境非平稳性以及交互复杂度的组合增长进一步加剧了这一挑战。我们认为，大语言模型的最新进展正推动从手工设计数值奖励向基于语言的目标描述转变。已有研究表明，大语言模型可直接从自然语言描述合成奖励函数（如EUREKA），并能以最少人工干预在线调整奖励方案（如CARD）。与此同时，新兴的“可验证奖励强化学习”范式为语言中介式监督替代传统奖励工程提供了实证依据。我们将这一转型归纳为三个维度：语义化奖励描述、动态奖励适配以及更优的人类意图对齐，同时指出其在计算开销、幻觉鲁棒性以及大规模多智能体系统扩展性方面仍存在开放挑战。最后我们提出一种研究方向：让协作从共享语义表征中自然涌现，而非依赖显式设计的数值信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper argues that the longstanding challenge of manual reward engineering in multi-agent reinforcement learning, plagued by issues like credit assignment and non-stationarity, is being redefined by large language models (LLMs). The proposed shift moves from hand-crafted numerical rewards to using language-based objective specifications, where LLMs can synthesize or adapt reward functions from natural language descriptions, as evidenced by prior works like EUREKA and CARD, and the Reinforcement Learning from Verifiable Rewards (RLVR) paradigm. The main conceptual transition involves semantic specification, dynamic adaptation, and improved human alignment, though challenges in computational cost, hallucination robustness, and scalability for large systems remain, pointing toward a future where multi-agent coordination stems from shared semantic understanding rather than engineered numerical signals.</div>
<div class="mono" style="margin-top:8px">本文认为，多智能体强化学习中长期存在的手动设计奖励函数的挑战，如信用分配模糊和环境非平稳性，正被大语言模型（LLMs）重新定义。其核心方法是从手工设计的数值奖励转向基于语言的目标规范，利用LLMs从自然语言描述中合成或在线调整奖励函数，相关证据来自EUREKA、CARD等先前工作以及可验证奖励的强化学习（RLVR）范式。这一转变主要体现在语义奖励规范、动态奖励适应和更好的人类意图对齐三个维度，但同时也面临计算开销、幻觉鲁棒性和大规模系统可扩展性等开放挑战，最终展望了一个多智能体协调基于共享语义表征而非显式工程化数值信号的研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition</div>
<div class="meta-line">Authors: Hao Deng, Bo Liu</div>
<div class="meta-line">First: 2026-01-13T05:25:32+00:00 · Latest: 2026-01-13T05:25:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08230v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08230v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Graph Neural Networks (GNNs) excel on graph-structured data, their performance is fundamentally limited by the quality of the observed graph, which often contains noise, missing links, or structural properties misaligned with GNNs&#x27; underlying assumptions. To address this, graph structure learning aims to infer a more optimal topology. Existing methods, however, often incur high computational costs due to complex generative models and iterative joint optimization, limiting their practical utility. In this paper, we propose GADPN, a simple yet effective graph structure learning framework that adaptively refines graph topology via low-rank denoising and generalized structural perturbation. Our approach makes two key contributions: (1) we introduce Bayesian optimization to adaptively determine the optimal denoising strength, tailoring the process to each graph&#x27;s homophily level; and (2) we extend the structural perturbation method to arbitrary graphs via Singular Value Decomposition (SVD), overcoming its original limitation to symmetric structures. Extensive experiments on benchmark datasets demonstrate that GADPN achieves state-of-the-art performance while significantly improving efficiency. It shows particularly strong gains on challenging disassortative graphs, validating its ability to robustly learn enhanced graph structures across diverse network types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GADPN：基于奇异值分解的图自适应去噪与扰动网络</div>
<div class="mono" style="margin-top:8px">图神经网络（GNNs）在图结构数据上表现优异，但其性能根本上受限于观测图的质量，后者常包含噪声、缺失边或与GNN基本假设不符的结构特性。为解决此问题，图结构学习旨在推断更优的拓扑。然而，现有方法因复杂的生成模型和迭代联合优化常导致高计算成本，限制了实际应用。本文提出GADPN，一种简单高效的图结构学习框架，通过低秩去噪和广义结构扰动自适应优化图拓扑。本方法有两个关键贡献：（1）引入贝叶斯优化自适应确定最佳去噪强度，根据各图的同配性水平定制处理过程；（2）通过奇异值分解（SVD）将结构扰动方法扩展至任意图，突破了其原本仅适用于对称结构的限制。在基准数据集上的大量实验表明，GADPN在显著提升效率的同时实现了最先进的性能，尤其在具有挑战性的异配性图上表现出显著优势，验证了其在不同网络类型中稳健学习增强图结构的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the performance limitations of Graph Neural Networks (GNNs) when applied to noisy or incomplete observed graph structures, which existing graph structure learning methods address with high computational costs. The proposed method, GADPN, adaptively refines graph topology through low-rank denoising and generalized structural perturbation, utilizing Bayesian optimization to determine denoising strength based on graph homophily and extending structural perturbation via Singular Value Decomposition (SVD) to handle arbitrary graphs. Experimental results on benchmark datasets show that GADPN achieves state-of-the-art performance with improved efficiency, particularly demonstrating strong gains on disassortative graphs, validating its robustness across diverse network types.</div>
<div class="mono" style="margin-top:8px">本文的动机在于图神经网络（GNNs）在应用于噪声或不完整的观测图结构时性能受限，而现有的图结构学习方法计算成本高昂。所提出的方法GADPN通过低秩去噪和广义结构扰动自适应地优化图拓扑，利用贝叶斯优化根据图同配性确定去噪强度，并通过奇异值分解（SVD）扩展结构扰动以处理任意图。在基准数据集上的实验结果表明，GADPN实现了最先进的性能并显著提高了效率，尤其在具有挑战性的异配图上表现出强劲增益，验证了其在不同网络类型中稳健学习增强图结构的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function</div>
<div class="meta-line">Authors: Hyeongyu Kang, Jaewoo Lee, Woocheol Shin, Kiyoung Om, Jinkyoo Park</div>
<div class="meta-line">First: 2025-12-04T08:21:52+00:00 · Latest: 2026-01-13T04:42:44+00:00</div>
<div class="meta-line">Comments: 36 pages, 21 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04559v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04559v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose Soft Q-based Diffusion Finetuning (SQDF), a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于软Q函数重参数化策略梯度的扩散模型微调方法</div>
<div class="mono" style="margin-top:8px">扩散模型擅长生成高似然样本，但常需与下游目标对齐。现有扩散模型微调方法普遍存在奖励过优化问题，导致高奖励但不自然的样本及多样性下降。为缓解过优化，我们提出基于软Q的扩散微调方法（SQDF），这是一种用于扩散对齐的新型KL正则化强化学习方法，采用训练无关、可微分的软Q函数估计的重参数化策略梯度。SQDF通过三项创新进一步增强：为去噪过程设计折扣因子以实现准确信度分配，集成一致性模型以优化Q函数估计，利用离线策略回放缓冲区提升模式覆盖度并平衡奖励与多样性。实验表明，SQDF在文本-图像对齐任务中既能实现优越的目标奖励，又能保持样本多样性。此外，在在线黑盒优化任务中，SQDF在保持自然度与多样性的同时获得了高样本效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of reward over-optimization in fine-tuning diffusion models for downstream tasks, which often leads to unnatural samples and reduced diversity. To mitigate this, the authors propose Soft Q-based Diffusion Finetuning (SQDF), a KL-regularized reinforcement learning method that uses a reparameterized policy gradient based on a training-free, differentiable estimator of the soft Q-function. The method is enhanced with a discount factor for credit assignment, consistency models to improve Q-estimates, and an off-policy replay buffer to balance reward and diversity. Experimental results show that SQDF achieves higher target rewards while preserving sample diversity in text-to-image alignment tasks and demonstrates high sample efficiency in online black-box optimization while maintaining naturalness and diversity.</div>
<div class="mono" style="margin-top:8px">本文针对扩散模型在下游任务微调中存在的奖励过优化问题，该问题常导致生成样本不自然且多样性降低。为缓解此问题，作者提出了基于软Q函数的扩散微调方法（SQDF），这是一种KL正则化的强化学习方法，利用基于训练无关、可微分的软Q函数估计量的重参数化策略梯度。该方法通过引入折扣因子进行信用分配、使用一致性模型改进Q估计、以及采用离策略回放缓冲区来平衡奖励与多样性，从而得到增强。实验结果表明，在文本到图像对齐任务中，SQDF在获得更高目标奖励的同时保持了样本多样性，并在在线黑盒优化中实现了高样本效率，同时确保了生成的自然性和多样性。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Multiagent Reinforcement Learning with Collective Influence Estimation</div>
<div class="meta-line">Authors: Zhenglong Luo, Zhiyong Chen, Aoxiang Liu, Ke Pan</div>
<div class="meta-line">First: 2026-01-13T04:24:11+00:00 · Latest: 2026-01-13T04:24:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08210v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.
  To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object&#x27;s states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于集体影响力估计的可扩展多智能体强化学习</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）因其在解决复杂协作任务方面的潜力而备受关注。然而，现有MARL方法通常依赖智能体间频繁交换动作或状态信息以实现有效协调，这在实际机器人系统中难以满足。常见解决方案是引入估计器网络来建模其他智能体行为并预测其动作；然而，此类设计会导致估计器网络的规模和计算成本随智能体数量快速增长，从而限制大规模系统的可扩展性。为应对这些挑战，本文提出一种增强型多智能体学习框架——集体影响力估计网络（CIEN）。通过显式建模其他智能体对任务对象的集体影响力，每个智能体仅需根据局部观测和任务对象状态即可推断关键交互信息，实现无需显式动作信息交换的高效协作。该框架有效避免了网络规模随团队扩大而扩张；此外，新智能体的加入无需修改现有智能体网络结构，展现出强大的可扩展性。基于软演员-评论家（SAC）算法的多智能体协作任务实验表明，该方法在通信受限环境下实现了稳定高效的协调。进一步地，采用集体影响力建模训练的策略部署于真实机器人平台，实验结果表明其鲁棒性与部署可行性显著提升，同时降低了对通信基础设施的依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the scalability and communication limitations in multiagent reinforcement learning (MARL) by proposing a framework with a Collective Influence Estimation Network (CIEN). The motivation stems from the impracticality of frequent action or state exchanges in real robotic systems and the computational inefficiency of existing estimator networks that scale poorly with agent count. The method enables each agent to infer critical interaction information from local observations and task object states, modeling collective influence without explicit communication, thus avoiding network expansion and allowing easy integration of new agents. Experimental results on cooperative tasks using Soft Actor-Critic show stable coordination in communication-limited settings, with real robotic deployment demonstrating improved robustness, feasibility, and reduced reliance on communication infrastructure.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体强化学习中的可扩展性和通信限制问题，提出了一种集成集体影响力估计网络（CIEN）的框架。其动机源于实际机器人系统中频繁交换动作或状态信息的不切实际性，以及现有估计器网络随智能体数量增加而计算成本急剧上升的限制。该方法使每个智能体能够仅从局部观察和任务对象状态中推断关键交互信息，通过建模集体影响力而无需显式通信，从而避免了网络扩展并允许轻松集成新智能体。基于软演员-评论家算法的多智能体协作任务实验表明，该方法在通信受限环境下实现了稳定高效的协调；真实机器人平台上的部署结果进一步显示出显著提升的鲁棒性、部署可行性以及对通信基础设施依赖的降低。</div>
</details>
</div>
<div class="card">
<div class="title">On the Sample Complexity of Differentially Private Policy Optimization</div>
<div class="meta-line">Authors: Yi He, Xingyu Zhou</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-24T00:21:38+00:00 · Latest: 2026-01-13T04:00:52+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21060v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21060v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy optimization (PO) is a cornerstone of modern reinforcement learning (RL), with diverse applications spanning robotics, healthcare, and large language model training. The increasing deployment of PO in sensitive domains, however, raises significant privacy concerns. In this paper, we initiate a theoretical study of differentially private policy optimization, focusing explicitly on its sample complexity. We first formalize an appropriate definition of differential privacy (DP) tailored to PO, addressing the inherent challenges arising from on-policy learning dynamics and the subtlety involved in defining the unit of privacy. We then systematically analyze the sample complexity of widely-used PO algorithms, including policy gradient (PG), natural policy gradient (NPG) and more, under DP constraints and various settings, via a unified framework. Our theoretical results demonstrate that privacy costs can often manifest as lower-order terms in the sample complexity, while also highlighting subtle yet important observations in private PO settings. These offer valuable practical insights for privacy-preserving PO algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>差分隐私策略优化的样本复杂度研究</div>
<div class="mono" style="margin-top:8px">策略优化（PO）是现代强化学习（RL）的基石，广泛应用于机器人、医疗和大语言模型训练等领域。然而，PO在敏感领域的日益部署引发了显著的隐私担忧。本文首次对差分隐私策略优化展开理论研究，重点关注其样本复杂度。我们首先形式化了一个适用于PO的差分隐私（DP）定义，以应对在线策略学习动态带来的固有挑战以及隐私单位定义的微妙性。随后，通过统一框架，系统分析了在DP约束及多种设置下广泛使用的PO算法（包括策略梯度（PG）、自然策略梯度（NPG）等）的样本复杂度。理论结果表明，隐私代价通常表现为样本复杂度的低阶项，同时揭示了私有PO设置中微妙而重要的观察。这些发现为隐私保护PO算法提供了宝贵的实践洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing use of policy optimization in sensitive areas like healthcare and the need to address privacy concerns, this paper provides a theoretical analysis of differentially private policy optimization, specifically focusing on its sample complexity. The method involves formalizing a tailored definition of differential privacy for the policy optimization setting to handle on-policy learning dynamics and privacy unit challenges, followed by a systematic examination of algorithms such as policy gradient and natural policy gradient under a unified framework. The main experimental results reveal that privacy costs often appear as lower-order terms in the sample complexity, while also uncovering subtle but important observations that offer practical insights for designing privacy-preserving algorithms.</div>
<div class="mono" style="margin-top:8px">本文的动机源于策略优化在医疗等敏感领域的广泛应用所带来的隐私问题，旨在从理论上分析差分隐私策略优化的样本复杂度。研究方法包括为策略优化场景定制差分隐私定义，以应对在线策略学习动态和隐私单元定义的挑战，并通过统一框架系统分析策略梯度、自然策略梯度等算法在差分隐私约束下的表现。主要实验结果发现，隐私成本在样本复杂度中通常表现为低阶项，同时揭示了私有策略优化设置中一些微妙但重要的观察，为设计隐私保护算法提供了有价值的实践见解。</div>
</details>
</div>
<div class="card">
<div class="title">ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Jinpeng Wang, Chao Li, Ting Ye, Mengyuan Zhang, Wei Liu, Jian Luan</div>
<div class="meta-line">First: 2025-11-26T03:10:15+00:00 · Latest: 2026-01-13T03:30:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21005v5">Abs</a> · <a href="https://arxiv.org/pdf/2511.21005v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ICPO：基于内在置信度的群体相对偏好优化方法用于高效强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）在提升大语言模型（LLM）的推理能力方面展现出显著潜力。然而，现有RLVR方法常受限于奖励粒度粗、奖励噪声及探索效率低等问题，导致训练不稳定与熵崩溃。为应对这一挑战，我们提出基于内在置信度的群体相对偏好优化方法（ICPO）。其核心思想在于：LLM生成不同响应的概率本身可直接反映其对推理过程的自我评估。受偏好建模思想启发，ICPO通过比较同一输入提示下多个响应的相对生成概率，计算各响应的偏好优势分数，并将该分数与可验证奖励结合以引导探索过程。我们发现，偏好优势分数不仅能缓解奖励粒度粗与奖励噪声问题，还可有效抑制过度自信误差，增强被低估的高质量响应的相对优势，并防止模型对特定策略的过拟合。在四个通用领域基准和三个数学基准上的综合实验表明，相较于GRPO，ICPO能持续提升推理性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models, such as coarse-grained rewards, reward noise, and inefficient exploration, which cause unstable training and entropy collapse. To overcome these issues, the authors propose Intrinsic Confidence-Driven Group Relative Preference Optimization (ICPO), a method that leverages the LLM&#x27;s own generation probabilities across multiple responses to compute a preference advantage score, reflecting the model&#x27;s intrinsic confidence in its reasoning; this score is then integrated with verifiable rewards to guide exploration. Experimental results on four general-domain and three mathematical benchmarks show that ICPO consistently enhances reasoning performance compared to baseline methods like GRPO, effectively mitigating reward-related issues and preventing overfitting.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型的可验证奖励强化学习中存在的奖励粒度粗、奖励噪声和探索效率低等问题，这些问题会导致训练不稳定和熵崩溃。为解决这些挑战，作者提出了内在置信度驱动的群体相对偏好优化方法，该方法利用模型在同一提示下生成多个响应的概率来计算偏好优势分数，以反映模型对推理过程的内在评估，并将该分数与可验证奖励结合以指导探索过程。在四个通用领域基准和三个数学基准上的综合实验表明，相比GRPO等方法，ICPO能稳定提升推理能力，有效缓解奖励相关问题并防止模型对特定策略的过拟合。</div>
</details>
</div>
<div class="card">
<div class="title">VBO-MI: A Fully Gradient-Based Bayesian Optimization Framework Using Variational Mutual Information Estimation</div>
<div class="meta-line">Authors: Farhad Mirkarimi</div>
<div class="meta-line">First: 2026-01-13T03:07:52+00:00 · Latest: 2026-01-13T03:07:52+00:00</div>
<div class="meta-line">Comments: 31 pages, 8 figures, Code will be released upon acceptance</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world tasks require optimizing expensive black-box functions accessible only through noisy evaluations, a setting commonly addressed with Bayesian optimization (BO). While Bayesian neural networks (BNNs) have recently emerged as scalable alternatives to Gaussian Processes (GPs), traditional BNN-BO frameworks remain burdened by expensive posterior sampling and acquisition function optimization. In this work, we propose {VBO-MI} (Variational Bayesian Optimization with Mutual Information), a fully gradient-based BO framework that leverages recent advances in variational mutual information estimation. To enable end-to-end gradient flow, we employ an actor-critic architecture consisting of an {action-net} to navigate the input space and a {variational critic} to estimate information gain. This formulation effectively eliminates the traditional inner-loop acquisition optimization bottleneck, achieving up to a {$10^2 \times$ reduction in FLOPs} compared to BNN-BO baselines. We evaluate our method on a diverse suite of benchmarks, including high-dimensional synthetic functions and complex real-world tasks such as PDE optimization, the Lunar Lander control problem, and categorical Pest Control. Our experiments demonstrate that VBO-MI consistently provides the same or superior optimization performance and computational scalability over the baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VBO-MI：一种基于变分互信息估计的完全梯度化贝叶斯优化框架</div>
<div class="mono" style="margin-top:8px">许多现实任务需要优化仅能通过噪声评估访问的昂贵黑盒函数，这一场景通常采用贝叶斯优化（BO）处理。虽然贝叶斯神经网络（BNN）近期作为高斯过程（GP）的可扩展替代方案出现，但传统BNN-BO框架仍受限于昂贵的后验采样与采集函数优化。本研究提出{VBO-MI}（基于互信息的变分贝叶斯优化），这是一个利用变分互信息估计最新进展的完全梯度化BO框架。为实现端到端梯度流，我们采用由在输入空间导航的{动作网络}和估计信息增益的{变分评判器}组成的执行者-评判者架构。该设计有效消除了传统内循环采集优化瓶颈，相比BNN-BO基线实现高达{$10^2$倍浮点运算量削减}。我们在多样化基准测试中评估该方法，包括高维合成函数及复杂现实任务（如偏微分方程优化、月球着陆器控制问题、分类害虫防治）。实验表明VBO-MI在优化性能与计算可扩展性上始终达到等同或优于基线的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces VBO-MI, a fully gradient-based Bayesian optimization framework designed to overcome the computational inefficiencies of traditional methods, which rely on expensive posterior sampling and acquisition function optimization. The method leverages variational mutual information estimation within an actor-critic architecture, featuring an action-net to explore the input space and a variational critic to estimate information gain, thereby enabling end-to-end gradient flow and eliminating the inner-loop optimization bottleneck. Experimental results on high-dimensional synthetic functions and real-world tasks like PDE optimization and Lunar Lander control demonstrate that VBO-MI achieves up to a 100-fold reduction in FLOPs while matching or surpassing the performance of baseline Bayesian optimization methods.</div>
<div class="mono" style="margin-top:8px">本文提出了VBO-MI，一种完全基于梯度的贝叶斯优化框架，旨在解决传统方法因依赖昂贵的后验采样和采集函数优化而导致的计算效率低下问题。该方法利用变分互信息估计，采用行动者-评论家架构，其中行动网络负责探索输入空间，变分评论家估计信息增益，从而实现端到端的梯度流动，消除了内部循环优化瓶颈。在高维合成函数和偏微分方程优化、月球着陆器控制等实际任务的实验中，VBO-MI在计算量上实现了高达100倍的减少，同时达到或超越了基线贝叶斯优化方法的性能。</div>
</details>
</div>
<div class="card">
<div class="title">ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms</div>
<div class="meta-line">Authors: Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari</div>
<div class="meta-line">First: 2026-01-13T02:56:06+00:00 · Latest: 2026-01-13T02:56:06+00:00</div>
<div class="meta-line">Comments: 39 pages, 12 figures, 8 tables (including appendix)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08166v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08166v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ZeroDVFS：面向嵌入式平台的零样本大语言模型引导核心与频率分配</div>
<div class="mono" style="margin-top:8px">动态电压频率调节（DVFS）与任务核心分配对嵌入式系统的热管理及能效性能平衡至关重要。现有方法或依赖忽略停滞时间的基于利用率的启发式策略，或需大量离线性能分析以生成静态表格，无法实现运行时自适应。本文提出一种基于模型的分层多智能体强化学习（MARL）框架，用于多核平台的热感知与能耗感知调度。两个协作智能体通过分解指数级动作空间，将后续决策延迟降至358毫秒。首次决策耗时3.5至8.0秒（含一次性大语言模型特征提取）。精准的环境模型采用回归技术预测热力学特性与性能状态，结合大语言模型提取的语义特征，可在无需特定负载分析样本的情况下生成合成训练数据，实现训练后平台对新工作负载的零样本部署。我们引入基于大语言模型的语义特征提取方法，通过13项代码级特征静态表征OpenMP程序。受Dyna-Q启发的框架将直接强化学习与基于模型的规划相结合，收敛速度比无模型方法快20倍。在NVIDIA Jetson TX2、Jetson Orin NX、RubikPi及Intel Core i7平台上对BOTS和PolybenchC基准测试的实验表明：相比Linux ondemand调控器，能效提升7.09倍，完工时间缩短4.0倍；首次决策延迟比基于表格的分析方法快8300倍，满足动态嵌入式系统的实际部署需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing dynamic voltage and frequency scaling (DVFS) and core allocation methods, which either rely on simplistic heuristics or require extensive offline profiling, hindering runtime adaptation in embedded systems. It proposes a hierarchical multi-agent reinforcement learning (MARL) framework that decomposes the action space for efficient decision-making, achieving a latency of 358ms for subsequent decisions after an initial 3.5 to 8.0s setup involving one-time LLM feature extraction. The method uses an accurate environment model combined with LLM-extracted semantic features from code to enable zero-shot deployment on new workloads without profiling, demonstrating 20x faster convergence than model-free approaches and, in experiments on multiple embedded platforms, showing 7.09x better energy efficiency and 4.0x better makespan compared to the Linux ondemand governor, with first-decision latency 8,300x faster than table-based methods.</div>
<div class="mono" style="margin-top:8px">该论文针对现有动态电压频率缩放（DVFS）和核心分配方法的局限性，这些方法要么依赖简单的启发式规则而忽略停滞时间，要么需要大量离线分析生成表格，阻碍了嵌入式系统中的运行时适应。它提出了一种分层多智能体强化学习（MARL）框架，通过分解动作空间实现高效决策，后续决策延迟为358毫秒，初始决策（包括一次性LLM特征提取）需要3.5至8.0秒。该方法利用准确的环境模型结合从代码中提取的LLM语义特征，无需分析即可实现新工作负载的零样本部署，实验表明其收敛速度比无模型方法快20倍，在多个嵌入式平台上的测试显示，相比Linux ondemand调控器，能效提升7.09倍，完工时间改善4.0倍，且首次决策延迟比基于表格的方法快8,300倍。</div>
</details>
</div>
<div class="card">
<div class="title">Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following</div>
<div class="meta-line">Authors: Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu</div>
<div class="meta-line">First: 2025-10-16T08:24:44+00:00 · Latest: 2026-01-13T02:52:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14420v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.14420v3">PDF</a> · <a href="https://github.com/Rainier-rq/verl-if">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>指令即所需：面向指令跟随的自监督强化学习</div>
<div class="mono" style="margin-top:8px">语言模型在处理对现实应用至关重要的多约束指令时常常表现不佳。现有的强化学习方法依赖于外部监督，且多约束任务产生的奖励信号稀疏。本文提出一种无标签的自监督强化学习框架，通过直接从指令中提取奖励信号并生成伪标签用于奖励模型训练，从而消除对外部监督的依赖。该方法引入约束分解策略和高效的逐约束二分类，以应对稀疏奖励挑战，同时保持计算效率。实验表明，该方法泛化能力强，在3个领域内和5个领域外数据集上均取得显著提升，包括具有挑战性的代理和多轮指令跟随任务。数据和代码已公开于https://github.com/Rainier-rq/verl-if。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of language models struggling to follow multi-constraint instructions, which is critical for real-world applications, and overcomes limitations of existing reinforcement learning methods that rely on external supervision and suffer from sparse rewards. The proposed method is a self-supervised reinforcement learning framework that eliminates the need for external supervision by deriving reward signals directly from instructions and generating pseudo-labels for training a reward model, incorporating constraint decomposition strategies and efficient constraint-wise binary classification to handle sparse rewards efficiently. Experimental results demonstrate that the approach generalizes effectively, achieving significant improvements across three in-domain and five out-of-domain datasets, including challenging agentic and multi-turn instruction-following tasks.</div>
<div class="mono" style="margin-top:8px">本文针对语言模型在遵循多约束指令方面的挑战，这对于实际应用至关重要，并克服了现有强化学习方法依赖外部监督和稀疏奖励信号的局限。所提出的方法是一种自监督强化学习框架，通过直接从指令中推导奖励信号并生成伪标签来训练奖励模型，从而消除对外部监督的依赖，同时引入约束分解策略和高效的约束级二元分类以有效处理稀疏奖励问题。实验结果表明，该方法具有良好的泛化能力，在三个领域内和五个领域外数据集上均取得了显著改进，包括具有挑战性的代理和多轮指令遵循任务。</div>
</details>
</div>
<div class="card">
<div class="title">Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies</div>
<div class="meta-line">Authors: Zeyang Li, Sunbochen Tang, Navid Azizan</div>
<div class="meta-line">First: 2026-01-13T01:58:24+00:00 · Latest: 2026-01-13T01:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逆向流匹配：基于扩散与流策略的在线强化学习统一框架</div>
<div class="mono" style="margin-top:8px">扩散与流策略因其强大的表达能力在在线强化学习中日益受到重视，但高效训练仍是关键挑战。在线强化学习的核心难点在于缺乏目标分布的直接样本；目标实际上是由Q函数定义的未归一化玻尔兹曼分布。为此，针对扩散策略提出了两类看似不同的方法：噪声期望方法（以噪声加权平均作为训练目标）和梯度期望方法（以Q函数梯度加权平均作为训练目标）。然而，这些目标间的形式化关联及其能否整合为更通用的框架尚不明确。本文提出统一框架——逆向流匹配，通过逆向推断视角将训练目标构建为给定中间噪声样本的后验均值估计问题。关键创新在于引入朗之万-斯坦算子构造零均值控制变量，推导出能有效降低重要性采样方差的一般性估计器类别。研究表明，现有噪声期望与梯度期望方法均为此广义类别的特例。这一统一视角带来两项重要进展：将针对玻尔兹曼分布的能力从扩散策略拓展至流策略，并能原则性融合Q值与Q梯度信息，推导出最优的最小方差估计器，从而提升训练效率与稳定性。我们将逆向流匹配实例化用于在线强化学习的流策略训练，在连续控制基准测试中相比扩散策略基线展现出更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of efficiently training expressive diffusion and flow policies in online reinforcement learning, where direct samples from the target Boltzmann distribution defined by the Q-function are unavailable. The authors propose a unified framework called reverse flow matching (RFM), which formulates the training target as a posterior mean estimation problem given noisy intermediate samples and employs Langevin Stein operators to construct low-variance estimators via control variates. This framework generalizes existing noise-expectation and gradient-expectation methods, enabling their principled combination into an optimal minimum-variance estimator and extending the capability to flow policies. Experimental results on continuous-control benchmarks demonstrate that RFM improves performance over diffusion policy baselines, enhancing training efficiency and stability.</div>
<div class="mono" style="margin-top:8px">本文针对在线强化学习中扩散策略和流策略表达能力强但训练效率低的关键挑战，其核心困难在于无法直接获取由Q函数定义的未归一化玻尔兹曼目标分布的样本。作者提出了一个统一框架——反向流匹配（RFM），该框架从反向推断视角出发，将训练目标构建为给定噪声中间样本的后验均值估计问题，并引入朗之万-斯坦因算子来构造零均值控制变量，从而推导出一类能有效降低重要性采样方差的一般性估计器。该框架统一了现有的噪声期望和梯度期望方法，使其能够将Q值和Q梯度信息原则性地结合为最优的最小方差估计器，并将针对玻尔兹曼分布的训练能力从扩散策略扩展到流策略。在连续控制基准测试上的实验表明，基于RFM训练的流策略相比扩散策略基线取得了更好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Structure Detection for Contextual Reinforcement Learning</div>
<div class="meta-line">Authors: Tianyue Zhou, Jung-Hoon Cho, Cathy Wu</div>
<div class="meta-line">First: 2026-01-13T01:22:39+00:00 · Latest: 2026-01-13T01:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08120v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08120v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contextual Reinforcement Learning (CRL) tackles the problem of solving a set of related Contextual Markov Decision Processes (CMDPs) that vary across different context variables. Traditional approaches--independent training and multi-task learning--struggle with either excessive computational costs or negative transfer. A recently proposed multi-policy approach, Model-Based Transfer Learning (MBTL), has demonstrated effectiveness by strategically selecting a few tasks to train and zero-shot transfer. However, CMDPs encompass a wide range of problems, exhibiting structural properties that vary from problem to problem. As such, different task selection strategies are suitable for different CMDPs. In this work, we introduce Structure Detection MBTL (SD-MBTL), a generic framework that dynamically identifies the underlying generalization structure of CMDP and selects an appropriate MBTL algorithm. For instance, we observe Mountain structure in which generalization performance degrades from the training performance of the target task as the context difference increases. We thus propose M/GP-MBTL, which detects the structure and adaptively switches between a Gaussian Process-based approach and a clustering-based approach. Extensive experiments on synthetic data and CRL benchmarks--covering continuous control, traffic control, and agricultural management--show that M/GP-MBTL surpasses the strongest prior method by 12.49% on the aggregated metric. These results highlight the promise of online structure detection for guiding source task selection in complex CRL environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向上下文强化学习的结构检测</div>
<div class="mono" style="margin-top:8px">上下文强化学习（CRL）旨在解决一组随不同上下文变量变化的上下文马尔可夫决策过程（CMDP）的关联问题。传统方法——独立训练与多任务学习——常面临计算成本过高或负迁移的困境。近期提出的多策略方法——基于模型的迁移学习（MBTL）——通过策略性地选择少量任务进行训练并实现零样本迁移，已展现出显著效果。然而，CMDP涵盖的问题范围广泛，其结构特性因问题而异，因此不同的任务选择策略适用于不同的CMDP。本文提出结构检测MBTL（SD-MBTL），这是一个通用框架，能够动态识别CMDP的潜在泛化结构并选择合适的MBTL算法。例如，我们观察到“山峰结构”，即随着上下文差异增大，目标任务的泛化性能从训练性能逐渐下降。为此，我们提出M/GP-MBTL，该算法能检测结构并自适应地在基于高斯过程的方法与基于聚类的方法之间切换。在合成数据及CRL基准测试（涵盖连续控制、交通控制与农业管理）上的大量实验表明，M/GP-MBTL在综合指标上优于先前最强方法12.49%。这些结果凸显了在线结构检测在复杂CRL环境中指导源任务选择的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in Contextual Reinforcement Learning (CRL) where existing multi-policy methods like Model-Based Transfer Learning (MBTL) lack adaptability to diverse structural properties across different Contextual Markov Decision Processes (CMDPs). To overcome this, the authors propose Structure Detection MBTL (SD-MBTL), a framework that dynamically identifies the underlying generalization structure of a CMDP and selects an appropriate MBTL algorithm accordingly; specifically, they introduce M/GP-MBTL, which detects a &#x27;Mountain structure&#x27; and adaptively switches between Gaussian Process-based and clustering-based approaches for task selection. Experimental results on synthetic data and CRL benchmarks, including continuous control and traffic control, demonstrate that M/GP-MBTL outperforms the strongest prior method by 12.49% on an aggregated metric, highlighting the effectiveness of online structure detection for improving source task selection in complex CRL environments.</div>
<div class="mono" style="margin-top:8px">本文针对情境强化学习（CRL）中的挑战，即现有多策略方法如基于模型的迁移学习（MBTL）难以适应不同情境马尔可夫决策过程（CMDP）的多样化结构特性。为解决此问题，作者提出了结构检测MBTL（SD-MBTL）框架，该框架能动态识别CMDP的底层泛化结构并选择相应的MBTL算法；具体而言，他们引入了M/GP-MBTL方法，通过检测“山地结构”并自适应地在基于高斯过程和基于聚类的任务选择方法间切换。在合成数据和CRL基准测试（包括连续控制和交通控制）上的实验结果表明，M/GP-MBTL在综合指标上优于先前最强方法12.49%，凸显了在线结构检测在复杂CRL环境中指导源任务选择的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order</div>
<div class="meta-line">Authors: Chengyang Gu, Yuxin Pan, Hui Xiong, Yize Chen</div>
<div class="meta-line">First: 2026-01-13T00:57:45+00:00 · Latest: 2026-01-13T00:57:45+00:00</div>
<div class="meta-line">Comments: Accepted at International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL&#x27;s robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STO-RL：基于LLM引导的子目标时序排序的稀疏奖励离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）能够从预收集的数据集中学习策略，避免了昂贵且风险高的在线交互，但在涉及稀疏奖励的长时程任务中往往表现不佳。现有的目标条件与分层离线RL方法通过分解任务并生成中间奖励来缓解传统离线RL的局限，但通常忽略了子目标间的时序依赖关系，并依赖不精确的奖励塑形，导致策略次优。为解决这些问题，我们提出了STO-RL（基于LLM引导子目标时序排序的离线RL），这是一个利用大语言模型（LLM）生成时序排序的子目标序列及对应状态到子目标阶段映射的离线RL框架。借助此时序结构，STO-RL应用基于势能的奖励塑形，将稀疏的终端奖励转化为密集且时序一致的信号，在促进子目标进展的同时避免次优解。由此产生的带有塑形奖励的增强数据集，能够高效训练出高性能的离线策略。在四个离散与连续稀疏奖励基准测试上的评估表明，STO-RL始终优于最先进的离线目标条件与分层RL基线，实现了更快的收敛速度、更高的成功率和更短的轨迹。消融研究进一步证实了STO-RL对LLM生成的不完美或含噪子目标序列的鲁棒性，表明LLM引导的子目标时序结构与理论支撑的奖励塑形相结合，为长时程离线RL提供了一个实用且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of offline reinforcement learning (RL) in long-horizon tasks with sparse rewards, where existing methods often ignore temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. The proposed STO-RL framework leverages large language models (LLMs) to generate temporally ordered subgoal sequences and state-to-subgoal mappings, then applies potential-based reward shaping to convert sparse terminal rewards into dense, temporally consistent signals, enabling efficient offline policy training. Experimental results on four sparse-reward benchmarks show that STO-RL outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines in convergence speed, success rates, and trajectory length, with ablation studies confirming its robustness to imperfect LLM-generated sequences.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习在稀疏奖励的长时程任务中的挑战，现有方法常忽略子目标间的时间依赖性并依赖不精确的奖励塑造，导致策略次优。提出的STO-RL框架利用大语言模型生成时序有序的子目标序列和状态到子目标的映射，再通过基于势能的奖励塑造将稀疏终端奖励转换为密集且时序一致的信号，从而支持高效的离线策略训练。在四个稀疏奖励基准测试中，STO-RL在收敛速度、成功率和轨迹长度上均优于最先进的离线目标条件和分层强化学习方法，消融研究进一步验证了其对不完美大语言模型生成序列的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Large-scale Regional Traffic Signal Control Based on Single-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Qiang Li, Jin Niu, Qin Luo, Lina Yu</div>
<div class="meta-line">First: 2025-03-12T10:51:29+00:00 · Latest: 2026-01-12T23:22:59+00:00</div>
<div class="meta-line">Comments: A critical error in the methodology. The reported congestion control effects were not caused by the proposed signal timing optimization, but by an incorrect traffic volume scaling factor during evaluation. The traffic demand was not properly amplified, resulting in misleading performance gains. Due to the substantial nature of the error, completion of revisions is not feasible in the short term</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.09252v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.09252v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of global urbanization and motorization, traffic congestion has become a significant issue, severely affecting the quality of life, environment, and economy. This paper puts forward a single-agent reinforcement learning (RL)-based regional traffic signal control (TSC) model. Different from multi - agent systems, this model can coordinate traffic signals across a large area, with the goals of alleviating regional traffic congestion and minimizing the total travel time. The TSC environment is precisely defined through specific state space, action space, and reward functions. The state space consists of the current congestion state, which is represented by the queue lengths of each link, and the current signal phase scheme of intersections. The action space is designed to select an intersection first and then adjust its phase split. Two reward functions are meticulously crafted. One focuses on alleviating congestion and the other aims to minimize the total travel time while considering the congestion level. The experiments are carried out with the SUMO traffic simulation software. The performance of the TSC model is evaluated by comparing it with a base case where no signal-timing adjustments are made. The results show that the model can effectively control congestion. For example, the queuing length is significantly reduced in the scenarios tested. Moreover, when the reward is set to both alleviate congestion and minimize the total travel time, the average travel time is remarkably decreased, which indicates that the model can effectively improve traffic conditions. This research provides a new approach for large-scale regional traffic signal control and offers valuable insights for future urban traffic management.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单智能体强化学习的大规模区域交通信号控制</div>
<div class="mono" style="margin-top:8px">在全球城市化与机动化背景下，交通拥堵已成为严重影响生活质量、环境和经济的重要问题。本文提出一种基于单智能体强化学习的区域交通信号控制模型。与多智能体系统不同，该模型能够在大范围协调交通信号，以缓解区域交通拥堵并最小化总行程时间为目标。通过明确定义状态空间、动作空间和奖励函数构建了交通信号控制环境：状态空间由各路段排队长度表征的当前拥堵状态及交叉口当前信号相位方案组成；动作空间设计为先选择交叉口再调整其相位配时；精心设计了两类奖励函数，一类侧重缓解拥堵，另一类在考虑拥堵水平的同时以最小化总行程时间为目标。实验采用SUMO交通仿真软件进行，通过与未调整信号配时的基准案例对比评估模型性能。结果表明，该模型能有效控制拥堵，测试场景中排队长度显著减少；当奖励函数同时兼顾缓解拥堵与最小化行程时间时，平均行程时间显著下降，证明模型能有效改善交通状况。本研究为大规模区域交通信号控制提供了新思路，为未来城市交通管理提供了重要参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to address traffic congestion from urbanization, this paper proposes a single-agent reinforcement learning model for large-scale regional traffic signal control, aiming to coordinate signals across an area to reduce congestion and total travel time. The method defines a state space with link queue lengths and signal phases, an action space for selecting intersections and adjusting phase splits, and designs two reward functions targeting congestion alleviation and travel time minimization. Experimental results using SUMO simulation initially indicated significant reductions in queue length and average travel time; however, a critical methodological error was later identified where an incorrect traffic volume scaling factor during evaluation invalidated these reported performance gains, making the conclusions unreliable.</div>
<div class="mono" style="margin-top:8px">本文针对城市化带来的交通拥堵问题，提出了一种基于单智能体强化学习的大规模区域交通信号控制模型，旨在通过协调区域信号来缓解拥堵并减少总行程时间。方法上，模型定义了包含路段排队长度和信号相位的状态空间、用于选择交叉口和调整相位时长的动作空间，并精心设计了分别针对缓解拥堵和最小化行程时间的两个奖励函数。实验使用SUMO仿真进行，结果最初显示排队长度和平均行程时间显著下降；但后续发现存在关键方法错误，即在评估中使用了不正确的交通流量缩放因子，导致所报告的性能提升无效，结论不可靠。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations</div>
<div class="meta-line">Authors: Qiang Li, Jin Niu, Lina Yu</div>
<div class="meta-line">First: 2025-11-01T13:18:50+00:00 · Latest: 2026-01-12T23:22:21+00:00</div>
<div class="meta-line">Comments: A critical error in the methodology. The reported congestion control effects were not caused by the proposed signal timing optimization, but by an incorrect traffic volume scaling factor during evaluation. The traffic demand was not properly amplified, resulting in misleading performance gains. Due to the substantial nature of the error, completion of revisions is not feasible in the short term</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00549v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00549v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model&#x27;s effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>需求波动下区域交通信号控制的鲁棒单智能体强化学习方法</div>
<div class="mono" style="margin-top:8px">交通拥堵主要由交叉口排队引发，严重影响城市生活水平、安全、环境质量及经济效率。交通信号控制（TSC）系统虽具缓解拥堵潜力，但传统优化模型常难以捕捉真实交通的复杂性与动态性。本研究提出一种新颖的单智能体强化学习（RL）框架，用于区域自适应TSC，通过集中决策范式规避多智能体系统的协调复杂性。该模型采用邻接矩阵统一编码路网拓扑、基于探测车辆数据的实时排队状态及当前信号配时参数。借助DreamerV3世界模型的高效学习能力，智能体习得的控制策略按序选择交叉口并调整其信号相位配时以调控交通流入/流出，类似于反馈控制系统。奖励设计优先考虑排队消散，直接将拥堵指标（队列长度）与控制动作关联。在SUMO中的仿真实验验证了模型有效性：在具有多层级（10%、20%、30%）起讫点（OD）需求波动的推理场景下，该框架展现出鲁棒的抗波动能力，并显著降低队列长度。本研究为兼容探测车辆技术的智能交通控制建立了新范式。未来研究将通过在训练中引入随机OD需求波动、探索突发事件区域优化机制，以增强实际适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study was motivated by the need to address traffic congestion through adaptive traffic signal control (TSC) that can handle real-world complexity, moving beyond traditional optimization models. The method introduces a single-agent reinforcement learning framework using a centralized paradigm to avoid multi-agent coordination issues; it encodes network topology, real-time queue states, and signal parameters via an adjacency matrix and employs the DreamerV3 world model to learn policies for sequentially selecting intersections and adjusting phase splits. The main experimental results, based on SUMO simulations with multi-level demand fluctuations, initially indicated robust performance and reduced queue lengths, but a critical methodological error was later identified where an incorrect traffic volume scaling factor during evaluation invalidated the reported congestion control effects, making the performance gains misleading.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过能处理现实世界复杂性的自适应交通信号控制（TSC）来缓解交通拥堵，以超越传统优化模型。方法上引入了一个单智能体强化学习框架，采用集中式决策范式以避免多智能体协调的复杂性；它通过邻接矩阵编码网络拓扑、实时排队状态和信号参数，并利用DreamerV3世界模型学习策略，以顺序选择交叉口并调整信号相位配时。主要实验结果基于SUMO仿真，在多层次需求波动下最初显示了鲁棒的性能和缩短的排队长度，但后来发现一个关键的方法论错误：评估中使用了不正确的交通流量缩放因子，导致报告的拥堵控制效果无效，性能提升存在误导性。</div>
</details>
</div>
<div class="card">
<div class="title">Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control</div>
<div class="meta-line">Authors: Qiang Li, Ningjing Zeng, Lina Yu</div>
<div class="meta-line">First: 2025-11-01T13:29:13+00:00 · Latest: 2026-01-12T23:21:57+00:00</div>
<div class="meta-line">Comments: A critical error in the methodology. The reported congestion control effects were not caused by the proposed signal timing optimization, but by an incorrect traffic volume scaling factor during evaluation. The traffic demand was not properly amplified, resulting in misleading performance gains. Due to the substantial nature of the error, completion of revisions is not feasible in the short term</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00551v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00551v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Several studies have employed reinforcement learning (RL) to address the challenges of regional adaptive traffic signal control (ATSC) and achieved promising results. In this field, existing research predominantly adopts multi-agent frameworks. However, the adoption of multi-agent frameworks presents challenges for scalability. Instead, the Traffic signal control (TSC) problem necessitates a single-agent framework. TSC inherently relies on centralized management by a single control center, which can monitor traffic conditions across all roads in the study area and coordinate the control of all intersections. This work proposes a single-agent RL-based regional ATSC model compatible with probe vehicle technology. Key components of the RL design include state, action, and reward function definitions. To facilitate learning and manage congestion, both state and reward functions are defined based on queue length, with action designed to regulate queue dynamics. The queue length definition used in this study differs slightly from conventional definitions but is closely correlated with congestion states. More importantly, it allows for reliable estimation using link travel time data from probe vehicles. With probe vehicle data already covering most urban roads, this feature enhances the proposed method&#x27;s potential for widespread deployment. The method was comprehensively evaluated using the SUMO simulation platform. Experimental results demonstrate that the proposed model effectively mitigates large-scale regional congestion levels via coordinated multi-intersection control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向区域自适应交通信号控制的单智能体强化学习模型</div>
<div class="mono" style="margin-top:8px">多项研究采用强化学习（RL）应对区域自适应交通信号控制（ATSC）的挑战并取得显著成果。该领域现有研究主要采用多智能体框架，但其可扩展性面临挑战。交通信号控制（TSC）问题本质上需要单智能体框架，因其依赖单一控制中心的集中管理——该中心可监测研究区域内所有道路的交通状况并协调各交叉口控制。本研究提出一种兼容探测车技术的单智能体RL区域ATSC模型，其RL设计核心包含状态、动作与奖励函数的定义。为促进学习并管理拥堵，状态与奖励函数均基于排队长度构建，动作设计则用于调控排队动态。本研究采用的排队长度定义与传统方式略有差异，但与拥堵状态高度相关，更重要的是能通过探测车的路段行程时间数据进行可靠估计。鉴于探测车数据已覆盖大部分城市道路，此特性增强了该方法广泛部署的潜力。基于SUMO仿真平台的综合评估表明，该模型通过多交叉口协同控制，能有效缓解大规模区域拥堵水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the scalability limitations of multi-agent reinforcement learning frameworks for regional adaptive traffic signal control, this paper proposes a single-agent RL model that aligns with centralized traffic management. The method defines state, action, and reward functions based on queue length, which is estimated from probe vehicle travel time data to enhance real-world deployability. Experimental evaluation in SUMO initially suggested the model effectively reduced regional congestion through coordinated intersection control, but a critical methodological error was later identified: the reported performance gains were misleading due to an incorrect traffic volume scaling factor during simulation, invalidating the claimed results.</div>
<div class="mono" style="margin-top:8px">针对区域自适应交通信号控制中多智能体强化学习框架可扩展性不足的问题，本文提出了一种符合集中式管理需求的单智能体强化学习模型。该方法基于排队长度定义状态、动作和奖励函数，并利用浮动车行程时间数据进行估计，以提高实际部署潜力。在SUMO平台上的实验评估最初表明该模型能通过协调多路口控制有效缓解区域拥堵，但随后发现存在关键方法错误：由于仿真中交通流量缩放因子设置不当，所报告的性能提升存在误导性，导致结论无效。</div>
</details>
</div>
<div class="card">
<div class="title">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</div>
<div class="meta-line">Authors: Nawazish Alia, Rachael Shawb, Karl Mason</div>
<div class="meta-line">First: 2026-01-12T22:41:26+00:00 · Latest: 2026-01-12T22:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向奶牛场高效电力负荷调度的预测感知深度强化学习</div>
<div class="mono" style="margin-top:8px">奶牛养殖是高度依赖电网电力的能源密集型产业。随着可再生能源并网比例提升，可持续能源管理对降低电网依赖、支持联合国可持续发展目标7（经济适用的清洁能源）至关重要。然而，可再生能源的间歇性给实时供需平衡带来挑战。智能负荷调度成为在保障可靠性的同时最小化运营成本的关键。强化学习在提升能效与降低成本方面展现出潜力，但现有基于强化学习的调度方法多假设未来电价或发电量完全已知，这在动态环境中不切实际。此外，标准PPO变体依赖固定裁剪或KL散度阈值，易在波动电价下导致训练不稳定。为此，本研究提出一种面向奶牛场高效负荷调度的深度强化学习框架，聚焦实际运行约束下的电池储能与热水系统。所提出的预测感知PPO通过基于小时和月份的残差校准，整合短期需求与可再生能源发电预测；而PID-KL-PPO变体采用比例-积分-微分控制器自适应调节KL散度，实现稳定策略更新。基于真实奶牛场数据训练，该方法比PPO降低用电成本达1%，比DQN降低4.8%，比SAC降低1.5%。在电池调度中，PPO减少电网购电13.1%，证明了其在现代奶牛养殖可持续能源管理中的可扩展性与有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for sustainable energy management in the energy-intensive dairy sector amidst increasing renewable integration, this study proposes a Deep Reinforcement Learning framework for efficient electricity load scheduling. The method introduces a Forecast Aware PPO that incorporates short-term demand and renewable generation forecasts using residual calibration, alongside a PID KL PPO variant that adaptively regulates KL divergence for stable policy updates. Experimental results on real-world dairy farm data show the approach achieves up to 1% lower electricity costs than standard PPO, 4.8% lower than DQN, and 1.5% lower than SAC, while also reducing grid imports by 13.1% in battery scheduling, demonstrating its effectiveness for cost reduction and grid independence.</div>
<div class="mono" style="margin-top:8px">本研究针对乳制品行业能源密集且可再生能源集成度提高的背景下，为实现可持续能源管理，提出了一种用于高效电力负荷调度的深度强化学习框架。该方法引入了预测感知PPO，利用基于小时和月份的残差校准整合短期需求和可再生能源发电预测，同时采用PID KL PPO变体自适应调节KL散度以确保策略更新的稳定性。在真实乳牛场数据上的实验结果表明，该方法比标准PPO降低电力成本达1%，比DQN降低4.8%，比SAC降低1.5%，并在电池调度中减少电网输入13.1%，证明了其在降低成本和增强电网独立性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-01-12T21:57:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：面向科学复合图表的视觉条件化面板检测与描述生成框架</div>
<div class="mono" style="margin-top:8px">科学复合图表将多个带标签的面板整合为单一图像，但实际流程中的描述文本常缺失或仅提供图表级摘要，导致面板级理解困难。本文提出FigEx2——一种视觉条件化框架，可直接从复合图表中定位面板并生成面板级描述。为缓解开放式描述中多样化表述的影响，我们引入噪声感知门控融合模块，自适应过滤词元级特征以稳定检测查询空间。此外，采用结合监督学习与强化学习的阶段式优化策略，利用基于CLIP的对齐奖励和基于BERTScore的语义奖励，确保严格的多模态一致性。为支撑高质量监督，我们构建了面板级定位基准数据集BioSci-Fig-Cap，以及物理、化学跨学科测试集。实验表明：FigEx2在检测任务上达到0.726 mAP@0.5:0.95的优异性能，在METEOR和BERTScore指标上分别显著超越Qwen3-VL-8B模型0.51和0.24。值得注意的是，FigEx2在未经微调的情况下，对分布外科学领域展现出卓越的零样本迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of understanding scientific compound figures, where captions are often missing or only provide figure-level summaries, hindering panel-level comprehension. It introduces FigEx2, a visual-conditioned framework that localizes panels and generates panel-wise captions directly from the figure, employing a noise-aware gated fusion module to stabilize detection queries by filtering token-level features and a staged optimization strategy combining supervised learning with reinforcement learning using CLIP and BERTScore rewards for multimodal consistency. Experiments on the curated BioSci-Fig-Cap benchmark and cross-disciplinary test suites show FigEx2 achieves 0.726 mAP@0.5:0.95 for detection and outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore, with notable zero-shot transferability to out-of-distribution scientific domains.</div>
<div class="mono" style="margin-top:8px">该论文针对科学复合图中标题常缺失或仅提供图级摘要、阻碍面板级理解的问题，提出了FigEx2这一视觉条件框架，可直接从复合图中定位面板并生成面板级标题。方法采用噪声感知门控融合模块通过过滤令牌级特征来稳定检测查询空间，并结合监督学习与强化学习的阶段优化策略，利用基于CLIP的对齐和基于BERTScore的语义奖励确保严格的多模态一致性。在精心构建的BioSci-Fig-Cap基准及跨学科测试集上的实验结果表明，FigEx2在检测上达到0.726 mAP@0.5:0.95，并在METEOR和BERTScore上分别显著优于Qwen3-VL-8B模型0.51和0.24，且对分布外科学领域展现出卓越的零样本迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality</div>
<div class="meta-line">Authors: Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik</div>
<div class="meta-line">First: 2025-05-23T11:30:30+00:00 · Latest: 2026-01-12T21:52:55+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/ZLKong/Awesome-Collection-Token-Reduction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18227v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18227v3">PDF</a> · <a href="https://github.com/ZLKong/Awesome-Collection-Token-Reduction">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input&#x27;s essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate &quot;overthinking&quot; and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, agentic framework design, and broader ML and scientific domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成模型中的令牌缩减应超越效率考量——从视觉、语言到多模态</div>
<div class="mono" style="margin-top:8px">在Transformer架构中，令牌——从原始数据中提取的离散单元——通过将输入分割为固定长度的块而形成。每个令牌随后被映射为嵌入向量，从而实现并行注意力计算，同时保留输入的关键信息。由于Transformer自注意力机制具有二次计算复杂度，令牌缩减主要被用作一种效率优化策略。这在单一视觉和语言领域中尤为明显，它有助于平衡计算成本、内存占用和推理延迟。尽管已有这些进展，本文认为在大规模生成模型时代，令牌缩减应超越其传统以效率为导向的角色。相反，我们将其定位为生成建模的一项基本原则，对模型架构和更广泛的应用具有关键影响。具体而言，我们认为在视觉、语言和多模态系统中，令牌缩减能够：(i) 促进更深层的多模态整合与对齐，(ii) 缓解“过度思考”和幻觉现象，(iii) 在长输入序列中保持连贯性，(iv) 增强训练稳定性等。我们将令牌重新定义为超越效率度量的工具。基于此，我们展望了未来有前景的研究方向，包括算法设计、强化学习引导的令牌缩减、上下文学习中的令牌优化、智能体框架设计，以及更广泛的机器学习和科学领域应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper argues that token reduction in Transformers should move beyond its traditional role of improving computational efficiency to become a core principle in generative modeling, influencing architecture and applications. The authors propose reframing token reduction as a method that can enhance multimodal integration, reduce hallucinations, maintain long-context coherence, and improve training stability across vision, language, and multimodal systems. They outline future directions including algorithm design and reinforcement learning-guided approaches to leverage token reduction for broader generative capabilities.</div>
<div class="mono" style="margin-top:8px">本文主张在Transformer中，词元缩减应超越其传统的提升计算效率的角色，成为影响生成模型架构与应用的核心原则。作者提出将词元缩减重新定义为一种能够增强多模态整合、减少幻觉、保持长上下文连贯性并提升训练稳定性的方法，适用于视觉、语言及多模态系统。他们展望了未来方向，包括算法设计和强化学习引导的词元缩减，以拓展其在生成能力上的应用。</div>
</details>
</div>
<div class="card">
<div class="title">MCP Bridge: A Lightweight, LLM-Agnostic RESTful Proxy for Model Context Protocol Servers</div>
<div class="meta-line">Authors: Arash Ahmadi, Sarah Sharif, Yaser M. Banad</div>
<div class="meta-line">First: 2025-04-11T22:19:48+00:00 · Latest: 2026-01-12T21:39:11+00:00</div>
<div class="meta-line">Comments: 29 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.08999v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.08999v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly augmented with external tools through standardized interfaces like the Model Context Protocol (MCP). However, current MCP implementations face critical limitations: they typically require local process execution through STDIO transports, making them impractical for resource-constrained environments like mobile devices, web browsers, and edge computing. We present MCP Bridge, a lightweight RESTful proxy that connects to multiple MCP servers and exposes their capabilities through a unified API. Unlike existing solutions, MCP Bridge is fully LLM-agnostic, supporting any backend regardless of vendor. The system implements a risk-based execution model with three security levels-standard execution, confirmation workflow, and Docker isolation - while maintaining backward compatibility with standard MCP clients. However, reliable execution within this framework requires models that can strictly adhere to protocol schemas. To this end, we also fine-tuned the Qwen3 4B and 8B model family on the Agent-Ark/Toucan-1.5M dataset using four Reinforcement Learning techniques: Group Relative Policy Optimization (GRPO), Dr. GRPO, Beta Normalization Policy Optimization (BNPO), and Decoupled Alignment Policy Optimization (DAPO). Evaluated on the MCPToolBench++ benchmark, our optimized model achieves an F1 score of 73.0% that outperforms GPT-OSS-120B (62.17%) and remains competitive with the 70B+ parameter baselines. Evaluation demonstrates that MCP Bridge successfully addresses the constraints of direct MCP connections while providing enhanced security controls and cross-platform compatibility, enabling sophisticated LLM-powered applications in previously inaccessible environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MCP Bridge：面向模型上下文协议服务器的轻量级、LLM无关RESTful代理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）正日益通过模型上下文协议（MCP）等标准化接口与外部工具增强集成。然而，现有MCP实现面临关键局限：通常需通过STDIO传输在本地进程执行，难以适用于移动设备、网页浏览器及边缘计算等资源受限环境。本文提出MCP Bridge——一种轻量级RESTful代理，可连接多个MCP服务器并通过统一API暴露其功能。与现有方案不同，MCP Bridge完全与LLM无关，支持任意供应商的后端。该系统采用基于风险的执行模型，提供标准执行、确认工作流和Docker隔离三级安全机制，同时保持与标准MCP客户端的向后兼容性。但该框架的可靠执行需要模型严格遵循协议模式。为此，我们使用四种强化学习技术——组相对策略优化（GRPO）、Dr. GRPO、Beta归一化策略优化（BNPO）和解耦对齐策略优化（DAPO）——在Agent-Ark/Toucan-1.5M数据集上对Qwen3 4B和8B模型系列进行微调。在MCPToolBench++基准测试中，优化后模型取得73.0%的F1分数，优于GPT-OSS-120B（62.17%），并与70B+参数基线模型保持竞争力。评估表明，MCP Bridge成功解决了直接MCP连接的限制，同时提供增强的安全控制与跨平台兼容性，使复杂LLM应用得以在以往无法部署的环境中运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of current Model Context Protocol (MCP) implementations, which rely on local STDIO transports and are unsuitable for resource-constrained environments like mobile and edge devices. The method introduces MCP Bridge, a lightweight, LLM-agnostic RESTful proxy that connects to multiple MCP servers and exposes their capabilities through a unified API, featuring a risk-based execution model with three security levels for enhanced control. Main experimental results show that the system successfully enables MCP in previously inaccessible environments, and a fine-tuned Qwen3 model optimized with reinforcement learning techniques achieves a 73.0% F1 score on the MCPToolBench++ benchmark, outperforming larger baselines like GPT-OSS-120B.</div>
<div class="mono" style="margin-top:8px">本文的动机源于当前模型上下文协议（MCP）实现的局限性，其依赖本地标准输入输出传输，不适用于移动设备和边缘计算等资源受限环境。方法上提出了MCP Bridge，一个轻量级、与大型语言模型无关的RESTful代理，它连接多个MCP服务器并通过统一API暴露其功能，采用基于风险的执行模型，提供三个安全级别以增强控制。主要实验结果表明，该系统成功在先前无法访问的环境中实现了MCP应用，且通过强化学习技术优化的微调Qwen3模型在MCPToolBench++基准测试中取得了73.0%的F1分数，优于GPT-OSS-120B等更大规模基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety</div>
<div class="meta-line">Authors: Can Jin, Rui Wu, Tong Che, Qixin Zhang, Hongwu Peng, Jiahui Zhao, Zhenting Wang, Wenqi Wei, Ligong Han, Zhao Zhang, Yuan Cao, Ruixiang Tang, Dimitris N. Metaxas</div>
<div class="meta-line">First: 2026-01-12T21:08:46+00:00 · Latest: 2026-01-12T21:08:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08000v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like&#x27;&#x27; safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结合法规与判例推理：面向大语言模型安全的案例增强审慎对齐方法</div>
<div class="mono" style="margin-top:8px">确保大语言模型（LLMs）在遵循安全原则的同时不拒绝良性请求，仍是一项重大挑战。OpenAI通过引入审慎对齐（DA）方法，借助对详细“类代码”安全规则的推理来提升其o系列模型的安全性，但该方法在通常缺乏高级推理能力的开源LLMs中的有效性尚未得到充分研究。本研究系统评估了明确指定大量安全规则与通过示例案例演示这两种方式的影响。研究发现，引用显式规则对无害性的提升效果不稳定，且会系统性降低助益性；而基于案例增强的简化规则进行训练，能产生更稳健、泛化的安全行为。通过以案例增强推理而非冗长的类代码规则引导LLMs，我们避免了机械遵循狭窄枚举规则，实现了更广泛的适应性。基于这些发现，我们提出CADA——一种利用自生成安全推理链进行强化学习的案例增强审慎对齐方法。CADA有效提升了无害性，增强了对攻击的鲁棒性，减少了过度拒绝行为，并在多样化基准测试中保持了实用性，为在维持助益性的同时提升安全性提供了一种替代纯规则DA的可行方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of aligning large language models (LLMs) with safety principles without causing excessive refusal of benign requests, motivated by the limitations of rule-only deliberative alignment in open-source models. The method introduces CADA, a case-augmented deliberative alignment approach that uses reinforcement learning on self-generated safety reasoning chains, combining illustrative cases with simple codes rather than relying solely on extensive, code-like safety rules. Experimental results show that CADA enhances harmlessness, improves robustness against attacks, reduces over-refusal, and preserves utility across diverse benchmarks, outperforming methods based on explicit safety codes which inconsistently improve safety and degrade helpfulness.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型（LLM）在遵循安全原则时避免过度拒绝良性请求的挑战，其动机在于仅依赖规则进行审慎对齐的方法在开源模型中效果有限。该方法提出了CADA，一种案例增强的审慎对齐方法，它利用强化学习对自生成的安全推理链进行训练，将示例案例与简单规则结合，而非仅依赖详尽的代码式安全规则。实验结果表明，CADA有效提升了无害性，增强了对抗攻击的鲁棒性，减少了过度拒绝，并在多样基准测试中保持了实用性，优于依赖明确安全规则的方法，后者在安全性提升上不一致且会损害帮助性。</div>
</details>
</div>
<div class="card">
<div class="title">Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL</div>
<div class="meta-line">Authors: Zhewei Yao, Guoheng Sun, Lukasz Borchmann, Gaurav Nuti, Zheyu Shen, Minghang Deng, Bohan Zhai, Hao Zhang, Ang Li, Yuxiong He</div>
<div class="meta-line">First: 2025-05-22T23:33:47+00:00 · Latest: 2026-01-12T20:32:51+00:00</div>
<div class="meta-line">Comments: 22 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20315v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20315v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Translating natural language into SQL (Test2SQL) is a longstanding challenge at the intersection of natural language understanding and structured data access. While large language models (LLMs) have significantly improved fluency in SQL generation, producing correct and executable SQL--particularly for complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a reinforcement learning (RL) framework and model family designed to generate accurate, executable SQL using a lightweight reward signal based solely on execution correctness. Our approach avoids brittle intermediate supervision and complex reward shaping, promoting stable training and alignment with the end task. Combined with carefully curated data, strong supervised initialization, and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art execution accuracy across six diverse Test2SQL benchmarks, including the top position on the BIRD leaderboard. Notably, our 7B model outperforms prior 70B-class systems, highlighting the framework&#x27;s scalability and efficiency. We further demonstrate inference-time robustness through simple extensions like value retrieval and majority voting. Extensive experiments and ablation studies offer both positive and negative insights, providing practical guidance for future Test2SQL research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Arctic-Text2SQL-R1：基于简单奖励机制的文本到SQL强推理框架</div>
<div class="mono" style="margin-top:8px">将自然语言转换为SQL（Text2SQL）是自然语言理解与结构化数据访问交叉领域的长期挑战。尽管大语言模型显著提升了SQL生成的流畅度，但生成正确且可执行的SQL——特别是针对复杂查询——仍是瓶颈。本文提出Arctic-Text2SQL-R1，这是一个基于强化学习的框架与模型系列，仅通过执行正确性的轻量级奖励信号即可生成准确、可执行的SQL。该方法避免了脆弱的中间监督与复杂的奖励设计，实现了稳定训练并与最终任务对齐。结合精心构建的数据集、强监督初始化及高效训练策略，Arctic-Text2SQL-R1在六项Text2SQL基准测试中均达到最先进的执行准确率，包括BIRD排行榜首位。值得注意的是，我们的70亿参数模型性能超越此前700亿参数级系统，彰显了框架的可扩展性与高效性。通过值检索与多数表决等简单扩展，进一步验证了推理阶段的鲁棒性。大量实验与消融研究提供了正反两方面的洞见，为未来Text2SQL研究提供了实践指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the persistent challenge in Text-to-SQL of generating correct and executable SQL queries from natural language, especially for complex queries, where large language models still struggle. The authors propose Arctic-Text2SQL-R1, a reinforcement learning framework that uses a simple reward signal based solely on execution correctness to train models, avoiding complex reward shaping and intermediate supervision for stable training. Experimental results show that this approach, combined with curated data and strong initialization, achieves state-of-the-art execution accuracy across six benchmarks, with a 7B model outperforming prior 70B-class systems, demonstrating scalability and robustness through techniques like value retrieval and majority voting.</div>
<div class="mono" style="margin-top:8px">本文针对Text-to-SQL中长期存在的挑战，即从自然语言生成正确且可执行的SQL查询，尤其是在复杂查询方面，大型语言模型仍存在不足。作者提出了Arctic-Text2SQL-R1，一个强化学习框架，仅基于执行正确性使用简单的奖励信号来训练模型，避免了复杂的奖励塑造和中间监督，以实现稳定训练。实验结果表明，该方法结合精心整理的数据和强初始化，在六个基准测试中达到了最先进的执行准确率，其中7B模型超越了先前70B级别的系统，并通过值检索和多数投票等技术展示了可扩展性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">DataScribe: An AI-Native, Policy-Aligned Web Platform for Multi-Objective Materials Design and Discovery</div>
<div class="meta-line">Authors: Divyanshu Singh, Doguhan Sarıtürk, Cameron Lea, Md Shafiqul Islam, Raymundo Arroyave, Vahid Attari</div>
<div class="meta-line">First: 2026-01-12T19:59:39+00:00 · Latest: 2026-01-12T19:59:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07966v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07966v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The acceleration of materials discovery requires digital platforms that go beyond data repositories to embed learning, optimization, and decision-making directly into research workflows. We introduce DataScribe, an AI-native, cloud-based materials discovery platform that unifies heterogeneous experimental and computational data through ontology-backed ingestion and machine-actionable knowledge graphs. The platform integrates FAIR-compliant metadata capture, schema and unit harmonization, uncertainty-aware surrogate modeling, and native multi-objective multi-fidelity Bayesian optimization, enabling closed-loop propose-measure-learn workflows across experimental and computational pipelines. DataScribe functions as an application-layer intelligence stack, coupling data governance, optimization, and explainability rather than treating them as downstream add-ons. We validate the platform through case studies in electrochemical materials and high-entropy alloys, demonstrating end-to-end data fusion, real-time optimization, and reproducible exploration of multi-objective trade spaces. By embedding optimization engines, machine learning, and unified access to public and private scientific data directly within the data infrastructure, and by supporting open, free use for academic and non-profit researchers, DataScribe functions as a general-purpose application-layer backbone for laboratories of any scale, including self-driving laboratories and geographically distributed materials acceleration platforms, with built-in support for performance, sustainability, and supply-chain-aware objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DataScribe：面向多目标材料设计与发现的人工智能原生、策略对齐网络平台</div>
<div class="mono" style="margin-top:8px">加速材料发现需要超越数据存储库的数字平台，将学习、优化和决策直接嵌入研究流程。我们推出DataScribe——一个基于云的人工智能原生材料发现平台，通过本体支持的数据摄取和机器可操作知识图谱，统一异构的实验与计算数据。该平台集成符合FAIR原则的元数据采集、模式与单位协调、不确定性感知代理建模，以及原生多目标多保真贝叶斯优化，实现跨实验与计算流程的闭环“提出-测量-学习”工作流。DataScribe作为应用层智能栈，将数据治理、优化和可解释性深度耦合而非视为下游附加组件。我们通过电化学材料和高熵合金的案例研究验证平台，展示端到端数据融合、实时优化及多目标权衡空间的可复现探索。通过将优化引擎、机器学习及对公共与私有科学数据的统一访问直接嵌入数据基础设施，并支持学术与非营利研究者的开放免费使用，DataScribe成为适用于任意规模实验室（包括自动驾驶实验室和地理分布式材料加速平台）的通用应用层支撑框架，内置对性能、可持续性和供应链感知目标的支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to accelerate materials discovery beyond static data repositories, this paper introduces DataScribe, an AI-native cloud platform that integrates heterogeneous materials data via ontology-backed knowledge graphs and machine-actionable workflows. Its method combines FAIR-compliant metadata capture, schema harmonization, and native multi-objective multi-fidelity Bayesian optimization to enable closed-loop propose-measure-learn cycles. Experimental validation through case studies in electrochemical materials and high-entropy alloys demonstrates the platform&#x27;s capability for end-to-end data fusion, real-time optimization, and reproducible exploration of multi-objective trade spaces, serving as a scalable backbone for self-driving and distributed laboratories.</div>
<div class="mono" style="margin-top:8px">本文旨在加速材料发现，超越静态数据存储库，提出了DataScribe——一个基于云的原生人工智能平台，通过本体支持的知识图谱和机器可操作工作流整合异构材料数据。其方法结合了FAIR兼容的元数据捕获、模式协调以及原生多目标多保真度贝叶斯优化，以实现闭环的“提出-测量-学习”循环。通过在电化学材料和高熵合金的案例研究中验证，该平台展示了端到端数据融合、实时优化以及可重复的多目标权衡空间探索能力，可作为自驱动和分布式实验室的可扩展基础架构。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Methods for Neighborhood Selection in Local Search</div>
<div class="meta-line">Authors: Yannick Molinghen, Augustin Delecluse, Renaud De Landtsheer, Stefano Michelini</div>
<div class="meta-line">First: 2026-01-12T19:25:29+00:00 · Latest: 2026-01-12T19:25:29+00:00</div>
<div class="meta-line">Comments: Accepted at ICORES 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07948v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has recently gained traction as a means to improve combinatorial optimization methods, yet its effectiveness within local search metaheuristics specifically remains comparatively underexamined. In this study, we evaluate a range of reinforcement learning-based neighborhood selection strategies -- multi-armed bandits (upper confidence bound, $ε$-greedy) and deep reinforcement learning methods (proximal policy optimization, double deep $Q$-network) -- and compare them against multiple baselines across three different problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. We show how search-specific characteristics, particularly large variations in cost due to constraint violation penalties, necessitate carefully designed reward functions to provide stable and informative learning signals. Our extensive experiments reveal that algorithm performance varies substantially across problems, although that $ε$-greedy consistently ranks among the best performers. In contrast, the computational overhead of deep reinforcement learning approaches only makes them competitive with a substantially longer runtime. These findings highlight both the promise and the practical limitations of deep reinforcement learning in local search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>局部搜索中邻域选择的强化学习方法研究</div>
<div class="mono" style="margin-top:8px">强化学习作为改进组合优化方法的手段近期受到关注，但其在局部搜索元启发式算法中的具体应用效果仍相对缺乏深入研究。本研究评估了一系列基于强化学习的邻域选择策略——包括多臂老虎机（置信上界、ε-贪婪）和深度强化学习方法（近端策略优化、双深度Q网络），并在三个不同问题（旅行商问题、带时间窗的取送货问题、车辆排序问题）中与多种基线方法进行比较。研究表明，搜索过程的特定特征（尤其是因约束违反惩罚导致的成本大幅波动）需要精心设计奖励函数，以提供稳定且信息丰富的学习信号。大量实验表明，算法性能在不同问题间差异显著，但ε-贪婪策略始终表现优异。相比之下，深度强化学习方法因计算开销较大，仅能在显著延长运行时间的情况下具备竞争力。这些发现既揭示了深度强化学习在局部搜索中的应用潜力，也指出了其实际局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the application of reinforcement learning (RL) for neighborhood selection within local search metaheuristics, motivated by its underexplored potential in this specific combinatorial optimization context. The method evaluates several RL strategies, including multi-armed bandits (Upper Confidence Bound, ε-greedy) and deep RL approaches (Proximal Policy Optimization, Double Deep Q-Network), against baseline methods on three problems: the Traveling Salesman Problem, the Pickup and Delivery Problem with Time Windows, and the Car Sequencing Problem. The main experimental results demonstrate that algorithm performance is highly problem-dependent, with ε-greedy consistently performing well, while deep RL methods incur significant computational overhead, making them competitive only with substantially longer runtimes, thus highlighting both the promise and practical limitations of deep RL in local search.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在局部搜索元启发式算法中应用强化学习进行邻域选择，其动机在于强化学习在这一特定组合优化场景中的潜力尚未得到充分探索。方法上，评估了多种强化学习策略，包括多臂赌博机（上置信界、ε-贪婪）和深度强化学习方法（近端策略优化、双深度Q网络），并在旅行商问题、带时间窗的取送货问题以及车辆排序问题这三个问题上与基线方法进行了比较。主要实验结果表明，算法性能高度依赖于具体问题，其中ε-贪婪方法表现 consistently 良好，而深度强化学习方法则因计算开销较大，仅在运行时间显著延长时才具有竞争力，从而揭示了深度强化学习在局部搜索中的前景与实际局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation</div>
<div class="meta-line">Authors: Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, Bo An, Xiaoli Li, Huazhe Xu</div>
<div class="meta-line">First: 2026-01-12T18:53:11+00:00 · Latest: 2026-01-12T18:53:11+00:00</div>
<div class="meta-line">Comments: Project page: https://failure-aware-rl.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07821v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://failure-aware-rl.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>故障感知强化学习：面向现实世界操作的具备自恢复能力的可靠离线至在线强化学习</div>
<div class="mono" style="margin-top:8px">基于深度强化学习的训练后算法能够针对特定目标（如泛化性、精确性和鲁棒性）突破机器人模型的性能极限。然而，现实世界探索过程中不可避免地会发生需要人工干预的故障（例如机器人打翻水杯或打碎玻璃），阻碍了该范式的实际部署。为此，我们提出了故障感知离线至在线强化学习（FARL），这是一种能在现实世界强化学习中最大限度减少故障的新范式。我们创建了包含常见需人工干预故障场景的基准测试集FailureBench，并提出一种算法，该算法整合了基于世界模型的安全评判器与离线训练的恢复策略，以防止在线探索期间的故障。大量仿真和现实世界实验表明，FARL能显著减少需干预故障，同时在训练后的在线强化学习中提升性能与泛化能力。在现实世界强化学习训练后阶段，FARL平均减少73.1%的需干预故障，并提升11.3%的性能。视频与代码详见：https://failure-aware-rl.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to prevent intervention-requiring failures, such as spilling or breakage, during real-world robotic reinforcement learning (RL) exploration, which hinders practical deployment. The method introduces Failure-Aware RL (FARL), a paradigm that integrates a world-model-based safety critic and an offline-trained recovery policy to minimize failures, alongside a benchmark called FailureBench for evaluation. Experimental results in simulation and real-world manipulation tasks show that FARL reduces intervention-requiring failures by 73.1% on average while improving performance by 11.3%, enhancing reliability and generalization during online RL post-training.</div>
<div class="mono" style="margin-top:8px">该研究的动机是解决现实世界机器人强化学习探索中不可避免的需人工干预的故障（如泼洒或破碎），这些故障阻碍了实际部署。方法上提出了故障感知强化学习（FARL），通过整合基于世界模型的安全评估器和离线训练的恢复策略来最小化故障，并建立了FailureBench基准进行评估。在仿真和真实世界操作任务中的实验结果表明，FARL平均减少了73.1%的需干预故障，同时性能提升了11.3%，从而提高了在线强化学习后训练阶段的可靠性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN using Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Peihao Yan, Jie Lu, Huacheng Zeng, Y. Thomas Hou</div>
<div class="meta-line">Venue: in IEEE Transactions on Networking, vol. 34, pp. 1596-1611, 2026</div>
<div class="meta-line">First: 2025-09-17T18:20:04+00:00 · Latest: 2026-01-12T18:11:14+00:00</div>
<div class="meta-line">Comments: Published in: IEEE Transactions on Networking</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14343v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14343v2">PDF</a> · <a href="https://github.com/xslice-5G/code">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-Radio Access Network (O-RAN) has become an important paradigm for 5G and beyond radio access networks. This paper presents an xApp called xSlice for the Near-Real-Time (Near-RT) RAN Intelligent Controller (RIC) of 5G O-RANs. xSlice is an online learning algorithm that adaptively adjusts MAC-layer resource allocation in response to dynamic network states, including time-varying wireless channel conditions, user mobility, traffic fluctuations, and changes in user demand. To address these network dynamics, we first formulate the Quality-of-Service (QoS) optimization problem as a regret minimization problem by quantifying the QoS demands of all traffic sessions through weighting their throughput, latency, and reliability. We then develop a deep reinforcement learning (DRL) framework that utilizes an actor-critic model to combine the advantages of both value-based and policy-based updating methods. A graph convolutional network (GCN) is incorporated as a component of the DRL framework for graph embedding of RAN data, enabling xSlice to handle a dynamic number of traffic sessions. We have implemented xSlice on an O-RAN testbed with 10 smartphones and conducted extensive experiments to evaluate its performance in realistic scenarios. Experimental results show that xSlice can reduce performance regret by 67% compared to the state-of-the-art solutions. Source code is available at https://github.com/xslice-5G/code.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的5G O-RAN近实时资源切片服务质量优化</div>
<div class="mono" style="margin-top:8px">开放无线接入网（O-RAN）已成为5G及未来无线接入网络的重要范式。本文提出了一种名为xSlice的xApp，用于5G O-RAN的近实时无线接入网络智能控制器。xSlice是一种在线学习算法，能够根据动态网络状态（包括时变无线信道条件、用户移动性、流量波动和用户需求变化）自适应调整MAC层资源分配。为应对这些网络动态，我们首先通过加权所有流量会话的吞吐量、时延和可靠性来量化其服务质量需求，从而将服务质量优化问题建模为遗憾最小化问题。随后，我们开发了一个深度强化学习框架，该框架采用演员-评论家模型，结合了基于价值和基于策略的更新方法的优势。该框架引入图卷积网络作为组件，用于对无线接入网络数据进行图嵌入，使xSlice能够处理动态变化的流量会话数量。我们在配备10部智能手机的O-RAN测试平台上实现了xSlice，并进行了大量实验以评估其在真实场景中的性能。实验结果表明，与现有先进解决方案相比，xSlice能将性能遗憾降低67%。源代码发布于：https://github.com/xslice-5G/code。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to manage dynamic network conditions like varying channels and user mobility in 5G O-RAN, this paper introduces xSlice, a deep reinforcement learning-based xApp for near-real-time resource slicing. The method formulates QoS optimization as a regret minimization problem, employing an actor-critic DRL framework enhanced with a graph convolutional network to handle dynamic traffic sessions through graph embeddings. Experimental results from a testbed with 10 smartphones demonstrate that xSlice reduces performance regret by 67% compared to state-of-the-art solutions, effectively optimizing throughput, latency, and reliability.</div>
<div class="mono" style="margin-top:8px">本文针对5G O-RAN中时变信道和用户移动性等动态网络条件，提出了xSlice这一基于深度强化学习的近实时资源切片xApp。方法将服务质量优化问题构建为遗憾最小化问题，采用演员-评论家深度强化学习框架，并结合图卷积网络进行无线接入网络数据的图嵌入以处理动态流量会话。在配备10部智能手机的O-RAN测试平台上进行的实验表明，xSlice相比现有最优方案能将性能遗憾降低67%，有效优化了吞吐量、延迟和可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning</div>
<div class="meta-line">Authors: Wei Fang, James Glass</div>
<div class="meta-line">First: 2026-01-12T17:58:39+00:00 · Latest: 2026-01-12T17:58:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07782v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07782v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单次检索：基于查询规划的多步骤工具检索方法</div>
<div class="mono" style="margin-top:8px">在庞大且动态的工具库中运作的LLM智能体依赖高效检索，但标准的单次密集检索器难以处理复杂请求。这些失败主要源于抽象用户目标与技术文档之间的语义鸿沟，以及固定维度嵌入在建模组合工具结构时的能力局限。为解决这些挑战，我们提出TOOLQP——一个将检索建模为迭代式查询规划的轻量级框架。TOOLQP不再采用单次匹配，而是将指令分解为子任务，动态生成查询与检索器交互，通过针对组合所需的特定子任务有效弥合语义差距。我们使用合成查询轨迹训练TOOLQP，并通过可验证奖励强化学习（RLVR）进行优化。实验表明TOOLQP实现了最先进的性能，展现出卓越的零样本泛化能力、跨不同检索器的鲁棒性，以及在下游智能体执行中的显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of standard single-shot dense retrievers in handling complex user requests within large, dynamic tool libraries for LLM agents, due to a semantic gap between abstract goals and technical documentation and the inability of fixed embeddings to model tool compositions. The proposed method, TOOLQP, addresses this by modeling retrieval as iterative query planning, decomposing instructions into sub-tasks and generating dynamic queries to bridge the semantic gap, with training involving synthetic query trajectories and optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experimental results show that TOOLQP achieves state-of-the-art performance, demonstrating superior zero-shot generalization, robustness across retrievers, and significant improvements in downstream agent execution.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，针对大型动态工具库中的LLM智能体，标准单次密集检索器因抽象用户目标与技术文档间的语义鸿沟，以及固定大小嵌入无法建模工具组合，难以处理复杂请求。提出的TOOLQP方法将检索建模为迭代查询规划，通过将指令分解为子任务并生成动态查询来弥合语义差距，训练过程使用合成查询轨迹并通过可验证奖励的强化学习进行优化。实验结果表明，TOOLQP实现了最先进的性能，展现出卓越的零样本泛化能力、跨检索器的鲁棒性，以及在下游智能体执行中的显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to bin: differentiable and Bayesian optimization for multi-dimensional discriminants in high-energy physics</div>
<div class="meta-line">Authors: Johannes Erdmann, Nitish Kumar Kasaraguppe, Florian Mausolf</div>
<div class="meta-line">First: 2026-01-12T17:40:45+00:00 · Latest: 2026-01-12T17:40:45+00:00</div>
<div class="meta-line">Comments: 13 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07756v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07756v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Categorizing events using discriminant observables is central to many high-energy physics analyses. Yet, bin boundaries are often chosen by hand. A simple, popular choice is to apply argmax projections of multi-class scores and equidistant binning of one-dimensional discriminants. We propose a binning optimization for signal significance directly in multi-dimensional discriminants. We use a Gaussian Mixture Model (GMM) to define flexible bin boundary shapes for multi-class scores, while in one dimension (binary classification) we move bin boundaries directly. On this binning model, we study two optimization strategies: a differentiable and a Bayesian optimization approach. We study two toy setups: a binary classification and a three-class problem with two signals and backgrounds. In the one-dimensional case, both approaches achieve similar gains in signal sensitivity compared to equidistant binnings for a given number of bins. In the multi-dimensional case, the GMM-based binning defines sensitive categories as well, with the differentiable approach performing best. We show that, in particular for limited separability of the signal processes, our approach outperforms argmax classification even with optimized binning in the one-dimensional projections. Both methods are released as lightweight Python plugins intended for straightforward integration into existing analyses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习分箱：高能物理中多维判别式的可微与贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">利用判别观测量对事件进行分类是高能物理分析的核心环节，但分箱边界常依赖人工设定。常见简单做法是对多类得分进行argmax投影，并对一维判别式采用等距分箱。本文提出直接在多维判别式上优化信号显著性的分箱方法：针对多类得分，采用高斯混合模型定义灵活的分箱边界形状；针对一维情况（二分类），直接优化边界位置。基于此分箱模型，研究两种优化策略：可微优化与贝叶斯优化。通过两个示例场景（二分类问题、含双信号与背景的三分类问题）验证：在一维情形中，两种方法在给定箱数下均较等距分箱显著提升信号灵敏度；在多维情形中，基于高斯混合模型的分箱同样能构建敏感类别，其中可微优化表现最佳。研究表明，尤其在信号过程可分性有限时，本方法即使与一维投影的优化分箱相比，仍优于argmax分类。两种方法均以轻量级Python插件形式发布，便于集成至现有分析流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the common practice of manually choosing bin boundaries for discriminant observables in high-energy physics analyses, this paper proposes an optimization method to directly enhance signal significance. The method employs a Gaussian Mixture Model (GMM) to define flexible bin shapes in multi-dimensional discriminants and directly adjusts bin boundaries in one-dimensional cases, optimizing via either differentiable or Bayesian strategies. Experimental results on toy setups, including binary and three-class problems, show that both optimization approaches improve signal sensitivity over equidistant binning in one dimension, while the differentiable method excels with GMM-based binning in multi-dimensional scenarios, outperforming traditional argmax classification especially when signal separability is limited.</div>
<div class="mono" style="margin-top:8px">针对高能物理分析中通常手动选择判别变量分箱边界的问题，本文提出了一种直接优化信号显著性的方法。该方法采用高斯混合模型（GMM）来定义多维判别变量中的灵活分箱形状，在一维情况下则直接调整分箱边界，并通过可微分或贝叶斯优化策略进行优化。在包括二分类和三分类问题的模拟实验中，结果表明两种优化方法在一维情况下均比等距分箱提升了信号灵敏度，而在多维场景中，基于GMM的分箱结合可微分优化表现最佳，尤其在信号可分离性有限时，其性能超越了传统的argmax分类方法。</div>
</details>
</div>
<div class="card">
<div class="title">MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models</div>
<div class="meta-line">Authors: Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller</div>
<div class="meta-line">First: 2025-12-24T15:15:18+00:00 · Latest: 2026-01-12T17:32:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21231v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21231v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term &#x27;latent solvability&#x27;. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiST：理解中期科学训练在化学推理模型发展中的作用</div>
<div class="mono" style="margin-top:8px">大型语言模型可通过基于规则的奖励进行在线微调来发展推理能力。然而，近期研究揭示了一个关键限制：仅当基础模型已对正确答案分配了不可忽略的概率时，强化学习才能成功——这一特性我们称为“潜在可解性”。本研究探讨了化学推理能力的涌现及其先决条件对化学领域的意义。我们确定了基于强化学习的化学推理的两个必要条件：1）符号能力，2）潜在化学知识。我们提出了中期科学训练（MiST）：一套满足这些条件的中期训练技术，包括采用SMILES/CIF感知预处理的数据混合、对29亿词元的持续预训练，以及对10亿词元的有监督微调。这些步骤将30亿和70亿参数模型的潜在可解性分数提升至多1.8倍，并使强化学习在有机反应命名任务中的Top-1准确率从10.9%提升至63.9%，在无机材料生成任务中从40.6%提升至67.4%。其他具有挑战性的化学任务也观察到类似结果，同时生成可解释的推理轨迹。我们的研究明确了化学推理训练的清晰前提，并凸显了中期训练在解锁推理能力方面的更广泛作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the observation that reinforcement learning (RL) for chemical reasoning in large language models requires the base model to already assign some probability to correct answers, a property termed &#x27;latent solvability&#x27;. To address this prerequisite, the authors propose Mid-Stage Scientific Training (MiST), a method involving data-mixing with chemical structure-aware pre-processing, continued pre-training on 2.9 billion tokens, and supervised fine-tuning on 1 billion tokens to build symbolic competence and latent chemical knowledge. The main experimental results show that MiST increases the latent-solvability score by up to 1.8 times for 3B and 7B parameter models, enabling RL to improve top-1 accuracy from 10.9% to 63.9% on organic reaction naming and from 40.6% to 67.4% on inorganic material generation, with similar gains on other challenging chemical tasks while producing interpretable reasoning traces.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于发现，大型语言模型通过基于规则的奖励进行在线微调以发展化学推理能力时，强化学习仅在基础模型已对正确答案赋予不可忽略的概率（即“潜在可解性”）时才能成功。为此，作者提出了中期科学训练（MiST）方法，该方法通过混合化学结构感知预处理的数据、在29亿词元上继续预训练、以及在10亿词元上进行监督微调，来构建符号能力和潜在化学知识以满足前提条件。主要实验结果表明，MiST将3B和7B参数模型的潜在可解性分数提升了高达1.8倍，使得强化学习能够将有机反应命名的Top-1准确率从10.9%提升至63.9%，将无机材料生成的准确率从40.6%提升至67.4%，在其他具有挑战性的化学任务上也观察到类似提升，同时产生了可解释的推理轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids</div>
<div class="meta-line">Authors: Shaoting Zhu, Ziwen Zhuang, Mengjie Zhao, Kun-Ying Lee, Hang Zhao</div>
<div class="meta-line">First: 2026-01-12T16:50:50+00:00 · Latest: 2026-01-12T16:50:50+00:00</div>
<div class="meta-line">Comments: Project Page: https://project-instinct.github.io/hiking-in-the-wild</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07718v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07718v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://project-instinct.github.io/hiking-in-the-wild">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalability and training complexity; specifically, some previous works using virtual obstacles are implemented case-by-case. In this work, we present \textit{Hiking in the Wild}, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking. To ensure safety and training stability, we introduce two key mechanisms: a foothold safety mechanism combining scalable \textit{Terrain Edge Detection} with \textit{Foot Volume Points} to prevent catastrophic slippage on edges, and a \textit{Flat Patch Sampling} strategy that mitigates reward hacking by generating feasible navigation targets. Our approach utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments on a full-size humanoid demonstrate that our policy enables robust traversal of complex terrains at speeds up to 2.5 m/s. The training and deployment code is open-sourced to facilitate reproducible research and deployment on real robots with minimal hardware modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外徒步：面向仿人机器人的可扩展感知跑酷框架</div>
<div class="mono" style="margin-top:8px">在复杂非结构化环境中实现稳健的仿人机器人徒步行走，需要从反应式本体感知转向主动感知。然而整合外部感知仍是重大挑战：基于地图的方法存在状态估计漂移问题，例如基于激光雷达的方法难以有效处理躯干抖动。现有端到端方法常受限于可扩展性和训练复杂性；特别是先前采用虚拟障碍物的研究多为个案实现。本文提出《野外徒步》——一个为稳健仿人机器人徒步设计的可扩展端到端感知跑酷框架。为确保安全与训练稳定性，我们引入两项关键机制：结合可扩展《地形边缘检测》与《足部体积点》的立足点安全机制以防止边缘致命滑移，以及通过生成可行导航目标来缓解奖励作弊的《平坦区块采样》策略。该方法采用单阶段强化学习方案，将原始深度输入与本体感知直接映射为关节动作，无需依赖外部状态估计。在全尺寸仿人机器人上的大量实地实验表明，我们的策略能以最高2.5米/秒的速度稳健穿越复杂地形。训练与部署代码已开源，以促进可复现研究及在最小硬件改动下部署至真实机器人。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for robust humanoid navigation in unstructured environments, this paper introduces a scalable, end-to-end perceptive parkour framework that integrates exteroception to overcome limitations of reactive proprioception and drift-prone mapping methods. The method employs a single-stage reinforcement learning scheme, using raw depth and proprioceptive inputs to directly output joint actions, and incorporates a foothold safety mechanism with terrain edge detection and foot volume points alongside a flat patch sampling strategy to ensure stability and prevent reward hacking. Experimental results on a full-size humanoid show the policy enables robust traversal of complex terrains at speeds up to 2.5 m/s, demonstrating effective performance in field tests.</div>
<div class="mono" style="margin-top:8px">本文旨在实现人形机器人在复杂非结构化环境中的鲁棒徒步导航，通过从反应性本体感知转向前瞻性感知，以解决外感知集成中的挑战，如基于地图的方法存在状态估计漂移问题。该方法采用单阶段强化学习方案，将原始深度和本体感知输入直接映射为关节动作，并引入了结合地形边缘检测与足部体积点的立足点安全机制以及平坦区域采样策略，以确保训练稳定性和安全性。在真人尺寸人形机器人上的广泛实地实验表明，该策略能够以高达2.5米/秒的速度稳健地穿越复杂地形，验证了其在实际部署中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing</div>
<div class="meta-line">Authors: Osvaldo Simeone</div>
<div class="meta-line">First: 2026-01-01T07:38:07+00:00 · Latest: 2026-01-12T15:47:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00245v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.00245v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现代神经形态人工智能：从令牌内处理到令牌间处理</div>
<div class="mono" style="margin-top:8px">人工智能的快速发展带来了新颖的数据处理和生成能力，但也伴随着不断攀升的能耗需求。这一挑战重新激发了人们对神经形态计算原理的兴趣，该原理通过离散稀疏激活、循环动力学和非线性反馈，有望实现类脑的高效计算。事实上，现代人工智能架构正通过重度量化激活、状态空间动力学和稀疏注意力机制，日益体现神经形态原理。本文通过区分令牌内处理与令牌间处理的视角，详细阐述了神经形态模型、状态空间模型与Transformer架构之间的关联。早期神经形态人工智能研究多基于脉冲神经网络进行令牌内处理，即对同一向量输入（如图像像素）的多通道或特征进行转换。相比之下，近期研究探索了如何利用神经形态原理设计高效的令牌间处理方法，该方法能根据上下文相关性选择性地组合不同信息元素。这些方法通过实现关联记忆机制，利用了状态空间动力学或稀疏自注意力。本文在系统阐述基于令牌内/间处理视角的现代神经形态人工智能模型的同时，也综述了相关训练方法，涵盖从利用并行卷积处理的替代梯度法到基于强化学习机制的局部学习规则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the escalating energy demands of modern AI, this paper explores how neuromorphic computing principles, such as sparse and discrete activations, can enhance efficiency. The method involves analyzing AI architectures through the lens of intra-token processing, which transforms features within a single input (like pixels in an image), and inter-token processing, which selectively combines contextual information across inputs. The main experimental results highlight that while early neuromorphic work focused on spiking neural networks for intra-token tasks, recent advances leverage state-space models and sparse attention for efficient inter-token processing, with training methods ranging from surrogate gradients to local reinforcement learning rules.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现代人工智能不断增长的能源需求，旨在探讨如何利用神经形态计算原理（如稀疏和离散激活）来提高效率。方法上，通过区分令牌内处理（对单个输入内的特征进行转换，如图像像素）和令牌间处理（根据上下文相关性选择性地组合不同信息元素）来分析AI架构。主要实验结果表明，早期神经形态研究主要基于脉冲神经网络处理令牌内任务，而近期进展则利用状态空间模型和稀疏注意力机制实现高效的令牌间处理，其训练方法涵盖了从代理梯度到基于强化学习的局部学习规则。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Micro-Level Claims Reserving</div>
<div class="meta-line">Authors: Benjamin Avanzi, Ronald Richman, Bernard Wong, Mario Wüthrich, Yagebu Xie</div>
<div class="meta-line">First: 2026-01-12T15:17:18+00:00 · Latest: 2026-01-12T15:17:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07637v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07637v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Outstanding claim liabilities are revised repeatedly as claims develop, yet most modern reserving models are trained as one-shot predictors and typically learn only from settled claims. We formulate individual claims reserving as a claim-level Markov decision process in which an agent sequentially updates outstanding claim liability (OCL) estimates over development, using continuous actions and a reward design that balances accuracy with stable reserve revisions. A key advantage of this reinforcement learning (RL) approach is that it can learn from all observed claim trajectories, including claims that remain open at valuation, thereby avoiding the reduced sample size and selection effects inherent in supervised methods trained on ultimate outcomes only. We also introduce practical components needed for actuarial use -- initialisation of new claims, temporally consistent tuning via a rolling-settlement scheme, and an importance-weighting mechanism to mitigate portfolio-level underestimation driven by the rarity of large claims. On CAS and SPLICE synthetic general insurance datasets, the proposed Soft Actor-Critic implementation delivers competitive claim-level accuracy and strong aggregate OCL performance, particularly for the immature claim segments that drive most of the liability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>微观理赔准备金评估的强化学习方法</div>
<div class="mono" style="margin-top:8px">未决赔款负债随理赔进展不断调整，但多数现代准备金模型仅作为单次预测器训练，且通常仅从已结案理赔中学习。本文将个体理赔准备金评估构建为理赔层级的马尔可夫决策过程：智能体通过连续动作在理赔进展中序贯更新未决赔款负债估计值，其奖励机制兼顾估计精度与准备金调整稳定性。该强化学习方法的关键优势在于能利用所有观测到的理赔轨迹（包括评估时仍未结案的理赔）进行学习，从而避免仅基于最终结果的监督方法存在的样本量缩减和选择偏差问题。本文还引入了精算实务所需的组件：新理赔的初始化方法、通过滚动结案方案实现时间一致性调参，以及缓解因大额理赔罕见性导致的组合层面低估问题的重要性加权机制。在CAS与SPLICE综合财产保险数据集上的实验表明，所提出的柔性执行者-评论者算法在理赔层级精度与未决赔款负债汇总预测方面均表现优异，尤其对构成主要负债的未成熟理赔段具有显著优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of traditional one-shot reserving models that only learn from settled claims, proposing a reinforcement learning approach to micro-level claims reserving. The method formulates the problem as a Markov decision process where an agent sequentially updates outstanding claim liability estimates using continuous actions and a reward balancing accuracy with revision stability, enabling learning from all claim trajectories including open claims to avoid sample size reduction. Experimental results on CAS and SPLICE synthetic insurance datasets show that the Soft Actor-Critic implementation achieves competitive claim-level accuracy and strong aggregate liability performance, especially for immature claims that drive most liabilities.</div>
<div class="mono" style="margin-top:8px">本文的动机是传统一次性准备金模型仅从已结案索赔中学习的局限性，提出了一种用于微观层面索赔准备金的强化学习方法。该方法将问题构建为马尔可夫决策过程，其中智能体使用连续动作和平衡准确性与修订稳定性的奖励来顺序更新未决索赔负债估计，从而能够从包括未结案索赔在内的所有索赔轨迹中学习，避免样本量减少。在CAS和SPLICE合成保险数据集上的实验结果表明，所提出的Soft Actor-Critic实现实现了具有竞争力的索赔级别准确性和强大的总负债表现，特别是对于驱动大部分负债的未成熟索赔部分。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts</div>
<div class="meta-line">Authors: Di Zhang, Xun Wu, Shaohan Huang, Lingjie Jiang, Yaru Hao, Li Dong, Zewen Chi, Zhifang Sui, Furu Wei</div>
<div class="meta-line">First: 2025-10-27T05:47:48+00:00 · Latest: 2026-01-12T15:14:47+00:00</div>
<div class="meta-line">Comments: Added additional experiments, improved analysis, and fixed minor issues</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23027v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23027v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have substantially improved the training of large-scale language models, leading to significant gains in generation quality and reasoning ability. However, most existing research focuses on dense models, while RL training for Mixture-of-Experts (MoE) architectures remains underexplored. To address the instability commonly observed in MoE training, we propose a novel router-aware approach to optimize importance sampling (IS) weights in off-policy RL. Specifically, we design a rescaling strategy guided by router logits, which effectively reduces gradient variance and mitigates training divergence. Experimental results demonstrate that our method significantly improves both the convergence stability and the final performance of MoE models, highlighting the potential of RL algorithmic innovations tailored to MoE architectures and providing a promising direction for efficient training of large-scale expert models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向混合专家模型的稳定高效强化学习方法</div>
<div class="mono" style="margin-top:8px">强化学习的最新进展显著提升了大语言模型的训练效果，在生成质量与推理能力上取得重要突破。然而现有研究多集中于稠密模型，针对混合专家架构的强化学习训练仍探索不足。为解决MoE训练中常见的不稳定问题，本文提出一种新颖的路由器感知方法，用于优化离线策略强化学习中的重要性采样权重。具体而言，我们设计了一种由路由器逻辑值引导的重缩放策略，能有效降低梯度方差并缓解训练发散。实验结果表明，该方法显著提升了MoE模型的收敛稳定性与最终性能，彰显了针对MoE架构定制强化学习算法的潜力，为大规模专家模型的高效训练提供了可行方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the instability in reinforcement learning (RL) training for Mixture-of-Experts (MoE) models, which is underexplored compared to dense models. The method introduces a router-aware approach that optimizes importance sampling weights via a rescaling strategy guided by router logits to reduce gradient variance and prevent training divergence. Experiments show the proposed technique significantly enhances both the convergence stability and final performance of MoE models, indicating a promising direction for efficient large-scale expert model training.</div>
<div class="mono" style="margin-top:8px">本文针对混合专家模型中强化学习训练不稳定的问题展开研究，该问题相较于稠密模型尚未得到充分探索。方法上提出了一种路由器感知的新方法，通过基于路由器逻辑的重新缩放策略来优化重要性采样权重，从而降低梯度方差并防止训练发散。实验结果表明，该技术显著提高了混合专家模型的收敛稳定性和最终性能，为高效训练大规模专家模型提供了一个有前景的方向。</div>
</details>
</div>
<div class="card">
<div class="title">NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning</div>
<div class="meta-line">Authors: Zhongtao Miao, Kaiyan Zhao, Masaaki Nagata, Yoshimasa Tsuruoka</div>
<div class="meta-line">First: 2026-01-07T10:49:00+00:00 · Latest: 2026-01-12T15:01:07+00:00</div>
<div class="meta-line">Comments: Fixed typos in Table 1, Figure 7 and Section 4.2: regex -&gt; exact. Refined the caption of Table 3</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03790v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03790v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging &quot;translation difficulty&quot; to further improve the translation quality of translation agents using our search tool.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NeoAMT：基于强化学习的新词感知智能机器翻译框架</div>
<div class="mono" style="margin-top:8px">新词感知机器翻译旨在将包含新词的源语句翻译为目标语言。与通用机器翻译相比，该领域研究尚不充分。本文提出一种基于维基词典检索工具的智能框架NeoAMT，用于新词感知机器翻译。具体而言，我们首先构建了覆盖16种语言、75个翻译方向的新词感知机器翻译数据集，该数据集源自约1000万条英语维基词典记录；同时基于约300万条清洗后的维基词典记录构建了检索工具的语料库。随后利用该工具，通过强化学习训练翻译智能体，并评估新词感知机器翻译的准确性。在此基础上，进一步提出包含创新奖励机制与自适应推演生成方法的强化学习训练框架，通过量化“翻译难度”来提升使用本检索工具的翻译智能体的译文质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored challenge of translating neologisms in machine translation, this paper proposes NeoAMT, an agentic framework that integrates a Wiktionary-based search tool. The method involves creating a new multilingual dataset and retrieval corpus from Wiktionary dumps, then training a translation agent using reinforcement learning with a novel reward design and adaptive rollout generation based on translation difficulty. Experimental results demonstrate that the framework improves translation accuracy for sentences containing neologisms across 16 languages and 75 translation directions.</div>
<div class="mono" style="margin-top:8px">针对机器翻译中未充分探索的新词翻译难题，本文提出了NeoAMT，这是一个基于维基词典搜索工具的智能体框架。方法包括利用维基词典数据构建新的多语言数据集和检索语料库，然后通过强化学习训练翻译智能体，其中设计了新颖的奖励机制和基于翻译难度的自适应生成策略。实验结果表明，该框架在涵盖16种语言和75个翻译方向的任务中，有效提升了包含新词的句子的翻译准确性。</div>
</details>
</div>
<div class="card">
<div class="title">GRPO with State Mutations: Improving LLM-Based Hardware Test Plan Generation</div>
<div class="meta-line">Authors: Dimple Vijay Kochar, Nathaniel Pinckney, Guan-Ting Liu, Chia-Tung Ho, Chenhui Deng, Haoxing Ren, Brucek Khailany</div>
<div class="meta-line">First: 2026-01-12T14:42:42+00:00 · Latest: 2026-01-12T14:42:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07593v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07593v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">RTL design often relies heavily on ad-hoc testbench creation early in the design cycle. While large language models (LLMs) show promise for RTL code generation, their ability to reason about hardware specifications and generate targeted test plans remains largely unexplored. We present the first systematic study of LLM reasoning capabilities for RTL verification stimuli generation, establishing a two-stage framework that decomposes test plan generation from testbench execution. Our benchmark reveals that state-of-the-art models, including DeepSeek-R1 and Claude-4.0-Sonnet, achieve only 15.7-21.7% success rates on generating stimuli that pass golden RTL designs. To improve LLM generated stimuli, we develop a comprehensive training methodology combining supervised fine-tuning with a novel reinforcement learning approach, GRPO with State Mutation (GRPO-SMu), which enhances exploration by varying input mutations. Our approach leverages a tree-based branching mutation strategy to construct training data comprising equivalent and mutated trees, moving beyond linear mutation approaches to provide rich learning signals. Training on this curated dataset, our 7B parameter model achieves a 33.3% golden test pass rate and a 13.9% mutation detection rate, representing a 17.6% absolute improvement over baseline and outperforming much larger general-purpose models. These results demonstrate that specialized training methodologies can significantly enhance LLM reasoning capabilities for hardware verification tasks, establishing a foundation for automated sub-unit testing in semiconductor design workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于状态突变的GRPO方法：改进基于大语言模型的硬件测试计划生成</div>
<div class="mono" style="margin-top:8px">RTL设计在设计周期早期通常严重依赖临时测试平台创建。虽然大语言模型在RTL代码生成方面展现出潜力，但其理解硬件规格并生成针对性测试计划的能力尚未得到充分探索。我们首次系统研究了大语言模型在RTL验证激励生成中的推理能力，建立了一个将测试计划生成与测试平台执行解耦的两阶段框架。基准测试表明，包括DeepSeek-R1和Claude-4.0-Sonnet在内的前沿模型，在生成能通过黄金RTL设计的激励方面成功率仅为15.7-21.7%。为改进大语言模型生成的激励，我们开发了结合监督微调与新型强化学习方法（带状态突变的GRPO，即GRPO-SMu）的综合训练方案，通过输入突变增强探索能力。该方法采用基于树的分支突变策略构建包含等价树与突变树的训练数据，突破线性突变方法的局限，提供丰富的学习信号。基于此精选数据集训练的7B参数模型实现了33.3%的黄金测试通过率和13.9%的突变检测率，较基线绝对提升17.6%，且优于规模更大的通用模型。这些结果表明，专业化训练方法能显著提升大语言模型在硬件验证任务中的推理能力，为半导体设计流程中的自动化子单元测试奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored potential of large language models (LLMs) for generating targeted hardware test plans from RTL specifications, this paper introduces a systematic two-stage framework to decompose test plan generation from testbench execution. The method develops a comprehensive training approach combining supervised fine-tuning with a novel reinforcement learning technique called GRPO with State Mutation (GRPO-SMu), which employs a tree-based branching mutation strategy to enhance exploration and create rich training data. Experimental results show that baseline models like DeepSeek-R1 and Claude-4.0-Sonnet achieve only 15.7-21.7% success rates, while the proposed 7B parameter model, trained on the curated dataset, attains a 33.3% golden test pass rate and a 13.9% mutation detection rate, representing a 17.6% absolute improvement and outperforming much larger general-purpose models.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型在根据硬件RTL规范生成针对性测试计划方面潜力尚未充分探索的问题，提出了一个系统性的两阶段框架，将测试计划生成与测试平台执行解耦。方法上，结合了监督微调与一种名为GRPO状态突变的新型强化学习技术，采用基于树的分支突变策略来增强探索并构建丰富的训练数据。实验结果表明，DeepSeek-R1和Claude-4.0-Sonnet等基线模型的成功率仅为15.7-21.7%，而基于所提方法训练的7B参数模型在黄金测试通过率上达到33.3%，突变检测率为13.9%，实现了17.6%的绝对性能提升，并超越了规模大得多的通用模型。</div>
</details>
</div>
<div class="card">
<div class="title">Large Language Models for Physics Instrument Design</div>
<div class="meta-line">Authors: Sara Zoccheddu, Shah Rukh Qasim, Patrick Owen, Nicola Serra</div>
<div class="meta-line">First: 2026-01-12T14:30:54+00:00 · Latest: 2026-01-12T14:30:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07580v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07580v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the use of large language models (LLMs) for physics instrument design and compare their performance to reinforcement learning (RL). Using only prompting, LLMs are given task constraints and summaries of prior high-scoring designs and propose complete detector configurations, which we evaluate with the same simulators and reward functions used in RL-based optimization. Although RL yields stronger final designs, we find that modern LLMs consistently generate valid, resource-aware, and physically meaningful configurations that draw on broad pretrained knowledge of detector design principles and particle--matter interactions, despite having no task-specific training. Based on this result, as a first step toward hybrid design workflows, we explore pairing the LLMs with a dedicated trust region optimizer, serving as a precursor to future pipelines in which LLMs propose and structure design hypotheses while RL performs reward-driven optimization. Based on these experiments, we argue that LLMs are well suited as meta-planners: they can design and orchestrate RL-based optimization studies, define search strategies, and coordinate multiple interacting components within a unified workflow. In doing so, they point toward automated, closed-loop instrument design in which much of the human effort required to structure and supervise optimization can be reduced.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型在物理仪器设计中的应用</div>
<div class="mono" style="margin-top:8px">本研究探讨大语言模型（LLMs）在物理仪器设计中的应用，并将其性能与强化学习（RL）进行比较。仅通过提示，LLMs接收任务约束和先前高分设计的摘要，提出完整的探测器配置，并使用与RL优化相同的模拟器和奖励函数进行评估。尽管RL能产生更优的最终设计，但现代LLMs无需任务特定训练，即可基于对探测器设计原理及粒子-物质相互作用的广泛预训练知识，持续生成有效、资源感知且物理意义明确的配置。基于此结果，作为混合设计工作流程的第一步，我们探索将LLMs与专用信任区域优化器结合，作为未来流程的雏形：LLMs提出并构建设计假设，而RL执行奖励驱动的优化。实验表明，LLMs适合作为元规划器：它们能设计和协调基于RL的优化研究、定义搜索策略，并在统一工作流中协调多个交互组件。这为实现自动化闭环仪器设计指明了方向，可大幅减少构建和监控优化所需的人力投入。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the application of large language models (LLMs) to automate physics instrument design, motivated by the need to reduce human effort in structuring and supervising optimization processes. The method involves prompting LLMs with task constraints and summaries of high-performing designs to generate complete detector configurations, which are then evaluated using simulators and reward functions comparable to those in reinforcement learning (RL) optimization. Experimental results show that while RL produces superior final designs, LLMs consistently generate valid, resource-efficient, and physically meaningful configurations without task-specific training, leveraging their broad pretrained knowledge. The study further explores hybrid workflows where LLMs act as meta-planners, proposing design hypotheses and orchestrating RL-based optimization, pointing toward automated, closed-loop design systems.</div>
<div class="mono" style="margin-top:8px">本文研究利用大语言模型（LLMs）自动化物理仪器设计，旨在减少人工在优化过程构建和监督方面的投入。方法上，通过向LLMs提供任务约束和先前高分设计的摘要，使其生成完整的探测器配置，并使用与强化学习（RL）优化相同的模拟器和奖励函数进行评估。实验结果表明，尽管RL能产生更优的最终设计，但LLMs无需任务特定训练即可持续生成有效、资源感知且物理意义明确的配置，这得益于其广泛的预训练知识。研究进一步探索了混合工作流程，其中LLMs作为元规划器提出设计假设并协调基于RL的优化，为自动化闭环设计系统指明了方向。</div>
</details>
</div>
<div class="card">
<div class="title">Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?</div>
<div class="meta-line">Authors: Jingwei Ni, Yu Fan, Vilém Zouhar, Donya Rooein, Alexander Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash</div>
<div class="meta-line">First: 2025-06-24T09:49:26+00:00 · Latest: 2026-01-12T13:46:04+00:00</div>
<div class="meta-line">Comments: EACL 2026 Main</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.19467v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.19467v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Variation in human annotation (i.e., disagreements) is common in NLP, often reflecting important information like task subjectivity and sample ambiguity. Modeling this variation is important for applications that are sensitive to such information. Although RLVR-style reasoning (Reinforcement Learning with Verifiable Rewards) has improved Large Language Model (LLM) performance on many tasks, it remains unclear whether such reasoning enables LLMs to capture informative variation in human annotation. In this work, we evaluate the influence of different reasoning settings on LLM disagreement modeling. We systematically evaluate each reasoning setting across model sizes, distribution expression methods, and steering methods, resulting in 60 experimental setups across 3 tasks. Surprisingly, our results show that RLVR-style reasoning degrades performance in disagreement modeling, while naive Chain-of-Thought (CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback). These findings underscore the potential risk of replacing human annotators with reasoning LLMs, especially when disagreements are important.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理能否帮助大语言模型捕捉人类标注者的分歧？</div>
<div class="mono" style="margin-top:8px">人类标注中的差异（即分歧）在自然语言处理中普遍存在，常反映任务主观性和样本歧义等重要信息。对此类信息敏感的应用中，建模这种差异至关重要。尽管基于可验证奖励的强化学习式推理提升了大语言模型在多项任务上的表现，但此类推理是否能使大语言模型捕捉人类标注中有信息量的差异仍不明确。本研究评估了不同推理设置对大语言模型分歧建模的影响，系统考察了模型规模、分布表达方法和引导方法，在3个任务上构建了60种实验配置。出乎意料的是，结果显示强化学习式推理会降低分歧建模性能，而朴素的思维链推理反而能提升基于人类反馈强化学习的大语言模型表现。这些发现凸显了用推理型大语言模型替代人类标注者的潜在风险，尤其在分歧具有重要意义时。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether reasoning techniques, specifically RLVR-style and Chain-of-Thought (CoT), help large language models (LLMs) capture human annotator disagreements, which reflect task subjectivity and sample ambiguity. The motivation is to assess if such reasoning improves modeling of annotation variation, important for sensitive applications. The method involves systematically evaluating different reasoning settings across model sizes, distribution expression methods, and steering methods, totaling 60 experimental setups on three tasks. The main experimental results surprisingly show that RLVR-style reasoning degrades performance in disagreement modeling, while naive CoT reasoning improves it for RLHF LLMs, highlighting risks in replacing human annotators with reasoning LLMs when disagreements are informative.</div>
<div class="mono" style="margin-top:8px">本文研究推理技术，特别是RLVR风格和思维链（CoT），是否有助于大语言模型捕捉人类标注者的分歧，这些分歧反映了任务的主观性和样本的模糊性。动机在于评估此类推理是否能改进对标注变异的建模，这对敏感应用至关重要。方法包括系统评估不同推理设置，涵盖模型大小、分布表达方法和引导方法，在三个任务上共进行了60个实验设置。主要实验结果出人意料地表明，RLVR风格推理会降低分歧建模的性能，而朴素的CoT推理则能提升RLHF大语言模型的性能，这突显了在分歧具有信息性时用推理大语言模型替代人类标注者的潜在风险。</div>
</details>
</div>
<div class="card">
<div class="title">Stagewise Reinforcement Learning and the Geometry of the Regret Landscape</div>
<div class="meta-line">Authors: Chris Elliott, Einar Urdshals, David Quarel, Matthew Farrugia-Roberts, Daniel Murfet</div>
<div class="meta-line">First: 2026-01-12T13:25:21+00:00 · Latest: 2026-01-12T13:25:21+00:00</div>
<div class="meta-line">Comments: 50 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07524v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07524v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Singular learning theory characterizes Bayesian learning as an evolving tradeoff between accuracy and complexity, with transitions between qualitatively different solutions as sample size increases. We extend this theory to deep reinforcement learning, proving that the concentration of the generalized posterior over policies is governed by the local learning coefficient (LLC), an invariant of the geometry of the regret function. This theory predicts that Bayesian phase transitions in reinforcement learning should proceed from simple policies with high regret to complex policies with low regret. We verify this prediction empirically in a gridworld environment exhibiting stagewise policy development: phase transitions over SGD training manifest as &quot;opposing staircases&quot; where regret decreases sharply while the LLC increases. Notably, the LLC detects phase transitions even when estimated on a subset of states where the policies appear identical in terms of regret, suggesting it captures changes in the underlying algorithm rather than just performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阶段性强化学习与遗憾函数的几何景观</div>
<div class="mono" style="margin-top:8px">奇异学习理论将贝叶斯学习描述为准确性与复杂性之间随样本量增加而演变的权衡过程，其间存在不同质解之间的转换。我们将该理论拓展至深度强化学习，证明策略广义后验的集中性由遗憾函数几何的不变量——局部学习系数（LLC）所主导。该理论预测强化学习中的贝叶斯相变应从高遗憾的简单策略向低遗憾的复杂策略演进。我们在呈现阶段性策略发展的网格世界环境中实证验证了这一预测：随机梯度下降训练中的相变表现为“反向阶梯”现象，即遗憾急剧下降的同时LLC上升。值得注意的是，即使在策略遗憾表现相同的状态子集上估计，LLC仍能检测到相变，表明其捕捉的是底层算法的本质变化而非仅性能波动。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by extending singular learning theory, which describes Bayesian learning as a tradeoff between accuracy and complexity with phase transitions, to the domain of deep reinforcement learning (RL). The method involves proving that the concentration of the generalized posterior over policies is governed by a geometric invariant called the local learning coefficient (LLC), derived from the regret function&#x27;s geometry. The main experimental results, validated in a gridworld environment, show stagewise policy development where phase transitions during SGD training appear as &quot;opposing staircases&quot;: regret decreases sharply while the LLC increases, and the LLC detects transitions even in state subsets where regret-based performance appears unchanged, indicating it captures algorithmic changes beyond mere performance metrics.</div>
<div class="mono" style="margin-top:8px">本文的动机是将奇异学习理论——该理论将贝叶斯学习描述为准确性与复杂性之间随样本量增加而经历相变的权衡——扩展到深度强化学习领域。方法上，研究证明了策略的广义后验集中性由遗憾函数几何的一个不变量，即局部学习系数（LLC）所主导。主要实验结果在网格世界环境中得到验证，显示了策略的阶段式发展：SGD训练中的相变表现为“反向阶梯”，即遗憾急剧下降而LLC上升；值得注意的是，LLC甚至在策略遗憾表现相同的状态子集中也能检测到相变，表明其捕捉的是底层算法的变化，而不仅仅是性能指标。</div>
</details>
</div>
<div class="card">
<div class="title">SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Object-Centric Representations from Pretrained Vision Models</div>
<div class="meta-line">Authors: Alexandre Brown, Glen Berseth</div>
<div class="meta-line">First: 2025-08-12T20:16:54+00:00 · Latest: 2026-01-12T13:21:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09325v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.09325v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://segdac.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual reinforcement learning (RL) is challenging due to the need to extract useful representations from high-dimensional inputs while learning effective control from sparse and noisy rewards. Although large perception models exist, integrating them effectively into RL for visual generalization and improved sample efficiency remains difficult. We propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment Anything (SAM) for object-centric decomposition and YOLO-World to ground the image segmentation process via text inputs. It includes a novel transformer-based architecture that supports a dynamic number of segments at each time step and effectively learns which segments to focus on using online RL, without using human labels. By evaluating SegDAC over a challenging visual generalization benchmark using Maniskill3, which covers diverse manipulation tasks under strong visual perturbations, we demonstrate that SegDAC achieves significantly better visual generalization, doubling prior performance on the hardest setting and matching or surpassing prior methods in sample efficiency across all evaluated tasks. Project Page: https://segdac.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SegDAC：通过从预训练视觉模型中提取动态以对象为中心的表示来改进视觉强化学习</div>
<div class="mono" style="margin-top:8px">视觉强化学习（RL）面临挑战，因为需要从高维输入中提取有用表示，同时从稀疏且嘈杂的奖励中学习有效控制。尽管存在大型感知模型，但将其有效整合到RL中以实现视觉泛化和提高样本效率仍然困难。我们提出SegDAC，一种分割驱动的行动者-评论家方法。SegDAC使用Segment Anything（SAM）进行以对象为中心的分解，并使用YOLO-World通过文本输入来锚定图像分割过程。它包含一种新颖的基于Transformer的架构，支持每个时间步动态数量的分割片段，并利用在线RL有效学习应关注哪些片段，无需使用人工标注。通过在Maniskill3这一涵盖强视觉扰动下多样化操作任务的挑战性视觉泛化基准上评估SegDAC，我们证明SegDAC实现了显著更好的视觉泛化，在最困难设置下性能翻倍，并在所有评估任务中达到或超越先前方法的样本效率。项目页面：https://segdac.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of visual reinforcement learning (RL), where extracting useful representations from high-dimensional inputs and learning control from sparse rewards is difficult, despite the availability of large perception models. The proposed method, SegDAC, integrates Segment Anything (SAM) for object-centric decomposition and YOLO-World for text-grounded segmentation, employing a novel transformer-based architecture that dynamically handles varying numbers of segments and learns to focus on relevant ones through online RL without human labels. Experimental results on the Maniskill3 benchmark show that SegDAC significantly improves visual generalization, doubling prior performance in the hardest setting and matching or exceeding prior methods in sample efficiency across diverse manipulation tasks under visual perturbations.</div>
<div class="mono" style="margin-top:8px">本文针对视觉强化学习（RL）的挑战，即从高维输入中提取有用表示并从稀疏奖励中学习控制是困难的，尽管已有大型感知模型可用。所提出的SegDAC方法整合了Segment Anything（SAM）进行以对象为中心的分解和YOLO-World进行基于文本的分割，采用一种新颖的基于Transformer的架构，动态处理每个时间步的可变分段数量，并通过在线RL无人类标签地学习关注相关分段。在Maniskill3基准测试上的实验结果表明，SegDAC显著提升了视觉泛化能力，在最困难设置下将先前性能翻倍，并在所有评估的视觉扰动操作任务中匹配或超越了先前方法的样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions</div>
<div class="meta-line">Authors: Yongqi Li, Hao Lang, Tieyun Qian, Yongbin Li</div>
<div class="meta-line">First: 2026-01-12T13:13:24+00:00 · Latest: 2026-01-12T13:13:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07516v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07516v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于覆盖增强隐式动作的多模态对话智能体控制</div>
<div class="mono" style="margin-top:8px">视觉语言模型正日益作为多模态对话智能体应用于多样化对话任务。近期，强化学习被广泛探索用于使多模态对话智能体适应不同的人机交互场景。尽管在泛化性能上展现出显著提升，通过强化学习微调多模态对话智能体仍面临处理极大文本标记空间的挑战。为此，我们转而学习紧凑的隐式动作空间进行强化学习微调。具体而言，采用观察学习机制构建隐式动作空间的码本，利用未来观测估计当前隐式动作，该动作可进一步用于重建未来观测。然而，配对图文数据的稀缺性阻碍了学习具有充分覆盖度的码本。因此，我们同时利用配对图文数据与纯文本数据构建隐式动作空间，通过跨模态投影器将文本嵌入转换为图文嵌入。我们在配对图文数据上初始化跨模态投影器，并基于新型循环一致性损失在大量纯文本数据上进一步训练以增强其鲁棒性。实验表明，基于隐式动作的方法在两种对话任务上均优于多种强化学习算法的基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge of fine-tuning multimodal conversational agents (MCAs) via reinforcement learning (RL), which struggles with the large text token space. The method involves learning a compact latent action space for RL by constructing a codebook using a learning-from-observation mechanism, where future observations help estimate current latent actions to reconstruct those observations; to overcome data scarcity, it leverages both paired image-text and text-only data with a cross-modal projector trained via cycle consistency loss. The main experimental results show that this latent action-based approach outperforms competitive baselines on two conversation tasks across various RL algorithms.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决通过强化学习微调多模态对话代理时面临的大规模文本标记空间挑战。方法上，通过构建一个基于观察学习的码本来学习紧凑的潜在动作空间，利用未来观察估计当前潜在动作以重建观察；为克服配对数据稀缺，结合了配对图像-文本和纯文本数据，使用跨模态投影器并通过循环一致性损失增强其鲁棒性。主要实验结果表明，该基于潜在动作的方法在多种强化学习算法下，于两个对话任务上优于竞争基线。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Inference Towards ICD Coding</div>
<div class="meta-line">Authors: Xiaoxiao Deng</div>
<div class="meta-line">First: 2026-01-12T12:51:21+00:00 · Latest: 2026-01-12T12:51:21+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07496v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07496v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated ICD coding involves assigning standardized diagnostic codes to clinical narratives. The vast label space and extreme class imbalance continue to challenge precise prediction. To address these issues, LabGraph is introduced -- a unified framework that reformulates ICD coding as a graph generation task. By combining adversarial domain adaptation, graph-based reinforcement learning, and perturbation regularization, LabGraph effectively enhances model robustness and generalization. In addition, a label graph discriminator dynamically evaluates each generated code, providing adaptive reward feedback during training. Experiments on benchmark datasets demonstrate that LabGraph consistently outperforms previous approaches on micro-F1, micro-AUC, and P@K.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向ICD编码的图推理方法</div>
<div class="mono" style="margin-top:8px">自动化ICD编码旨在为临床叙述分配标准化诊断代码。庞大的标签空间与极端的类别不平衡仍是精准预测的挑战。为此，本文提出LabGraph——一个将ICD编码重构为图生成任务的统一框架。该框架融合对抗性领域适应、基于图的强化学习与扰动正则化技术，有效提升了模型的鲁棒性与泛化能力。此外，标签图判别器能动态评估每个生成的代码，在训练过程中提供自适应奖励反馈。在基准数据集上的实验表明，LabGraph在micro-F1、micro-AUC和P@K指标上均持续优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to improve automated ICD coding, which faces challenges from a vast label space and extreme class imbalance. The method introduces LabGraph, a unified framework that reformulates ICD coding as a graph generation task, combining adversarial domain adaptation, graph-based reinforcement learning, and perturbation regularization to enhance robustness and generalization, with a label graph discriminator providing adaptive reward feedback. Main experimental results show that LabGraph consistently outperforms previous approaches on benchmark datasets in metrics such as micro-F1, micro-AUC, and P@K.</div>
<div class="mono" style="margin-top:8px">该研究的动机是改进自动化ICD编码，其面临巨大标签空间和极端类别不平衡的挑战。方法上提出了LabGraph，这是一个将ICD编码重新定义为图生成任务的统一框架，结合了对抗性领域适应、基于图的强化学习和扰动正则化以增强鲁棒性和泛化能力，并利用标签图判别器提供自适应奖励反馈。主要实验结果表明，在基准数据集上，LabGraph在micro-F1、micro-AUC和P@K等指标上持续优于先前方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation</div>
<div class="meta-line">Authors: Baban Gain, Dibyanayan Bandyopadhyay, Asif Ekbal, Trilok Nath Singh</div>
<div class="meta-line">First: 2025-04-02T17:26:40+00:00 · Latest: 2026-01-12T12:46:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.01919v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.01919v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are rapidly reshaping machine translation (MT), particularly by introducing instruction-following, in-context learning, and preference-based alignment into what has traditionally been a supervised encoder-decoder paradigm. This survey provides a comprehensive and up-to-date overview of how LLMs are being leveraged for MT across data regimes, languages, and application settings. We systematically analyze prompting-based methods, parameter-efficient and full fine-tuning strategies, synthetic data generation, preference-based optimization, and reinforcement learning with human and weakly supervised feedback. Special attention is given to low-resource translation, where we examine the roles of synthetic data quality, diversity, and preference signals, as well as the limitations of current RLHF pipelines. We further review recent advances in Mixture-of-Experts models, MT-focused LLMs, and multilingual alignment, highlighting trade-offs between scalability, specialization, and accessibility. Beyond sentence-level translation, we survey emerging document-level and discourse-aware MT methods with LLMs, showing that most approaches extend sentence-level pipelines through structured context selection, post-editing, or reranking rather than requiring fundamentally new data regimes or architectures. Finally, we discuss LLM-based evaluation, its strengths and biases, and its role alongside learned metrics. Overall, this survey positions LLM-based MT as an evolution of traditional MT systems, where gains increasingly depend on data quality, preference alignment, and context utilization rather than scale alone, and outlines open challenges for building robust, inclusive, and controllable translation systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弥合语言鸿沟：基于大语言模型的机器翻译研究综述</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正迅速重塑机器翻译（MT）领域，尤其通过引入指令跟随、上下文学习和基于偏好的对齐机制，对传统有监督编码器-解码器范式形成革新。本综述全面梳理了LLMs在不同数据体系、语言及应用场景中赋能机器翻译的最新进展，系统分析了基于提示的方法、参数高效与全参数微调策略、合成数据生成、基于偏好的优化，以及结合人类反馈与弱监督反馈的强化学习。特别关注低资源翻译场景，探讨了合成数据质量、多样性及偏好信号的作用，并指出当前RLHF流程的局限性。进一步回顾了专家混合模型、面向MT的专用LLMs及多语言对齐技术的最新进展，揭示了可扩展性、专业化与可访问性之间的权衡关系。除句子级翻译外，本文还综述了新兴的文档级与语篇感知的LLM-MT方法，指出多数方案通过结构化上下文选择、后编辑或重排序扩展句子级流程，而非依赖全新数据体系或架构。最后，探讨了基于LLM的评估方法及其优势与偏差，以及其与学习型指标的协同作用。总体而言，本综述将LLM-MT定位为传统MT系统的演进方向，其性能提升日益依赖于数据质量、偏好对齐与上下文利用而非单纯规模扩张，并展望了构建鲁棒、包容且可控的翻译系统所面临的开放挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey is motivated by the rapid integration of Large Language Models (LLMs) into machine translation, which is shifting the traditional supervised paradigm by introducing capabilities like instruction-following and in-context learning. The method involves a systematic review of techniques including prompting, fine-tuning, synthetic data generation, and preference-based optimization, with special analysis of low-resource scenarios and emerging areas like document-level translation. The main experimental results and findings indicate that advancements increasingly depend on data quality, preference alignment, and effective context use rather than mere model scale, while also highlighting limitations in current reinforcement learning pipelines and the role of LLM-based evaluation alongside traditional metrics.</div>
<div class="mono" style="margin-top:8px">本综述的动机在于大型语言模型（LLM）正迅速重塑机器翻译领域，通过引入指令跟随、上下文学习等能力，改变了传统的监督式编码器-解码器范式。方法上，系统性地综述了提示学习、参数高效微调、合成数据生成、基于偏好的优化等技术，并特别分析了低资源翻译场景以及文档级翻译等新兴方向。主要实验结果表明，进展越来越依赖于数据质量、偏好对齐和上下文利用，而非单纯模型规模，同时指出了当前强化学习流程的局限性，并探讨了基于LLM的评估方法与传统指标并存的作用。</div>
</details>
</div>
<div class="card">
<div class="title">Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Sijia li, Xinran Li, Shibo Chen, Jun Zhang</div>
<div class="meta-line">First: 2026-01-12T12:17:11+00:00 · Latest: 2026-01-12T12:17:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07463v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07463v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拼图求解：面向离线多智能体强化学习的局部到全局世界模型</div>
<div class="mono" style="margin-top:8px">离线多智能体强化学习（MARL）旨在利用预收集数据集解决多智能体系统中的协作决策问题。现有离线MARL方法主要将训练约束在数据集分布内，导致策略过于保守，难以泛化至数据支持范围之外。基于模型的方法通过学习的世界模型生成合成数据以扩展原始数据集，为问题提供了可行方案，但多智能体系统的高维性、非平稳性和复杂性使得准确估计离线MARL中的状态转移和奖励函数极具挑战。鉴于直接建模联合动力学的困难，我们提出局部到全局（LOGO）世界模型，该创新框架利用更易估计的局部预测来推断全局状态动力学，在隐式捕获智能体间依赖关系的同时提升预测精度。借助训练好的世界模型，我们生成合成数据以增强原始数据集，扩展有效状态-动作空间。为确保策略学习的可靠性，我们进一步引入不确定性感知采样机制，通过预测不确定性自适应加权合成数据，减少近似误差向策略的传播。与传统的基于集成的方法相比，本方法仅需额外编码器进行不确定性估计，在保持精度的同时显著降低计算开销。在8种场景下与8个基线方法的对比实验表明，本方法在标准离线MARL基准测试中超越了现有最优基线，为可泛化的离线多智能体学习建立了新的基于模型基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the overly conservative policies in offline multi-agent reinforcement learning (MARL) that arise from strict dataset constraints, which limit generalization beyond the data support. The method introduces a local-to-global (LOGO) world model, which infers global state dynamics from easier-to-estimate local predictions to improve accuracy and capture agent dependencies, and augments the dataset with synthetic data generated from this model while using an uncertainty-aware sampling mechanism to weight synthetic data by prediction uncertainty, reducing error propagation. Main experimental results show that the approach outperforms eight baselines across eight scenarios on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable learning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线多智能体强化学习中因严格数据集约束导致的策略过于保守、难以泛化到数据支持范围之外的问题。方法上提出了一种局部到全局的世界模型，通过更易估计的局部预测来推断全局状态动态，以提高准确性并捕捉智能体间依赖关系，同时利用该模型生成合成数据以扩充原始数据集，并引入不确定性感知采样机制根据预测不确定性自适应加权合成数据，减少误差传播。主要实验结果表明，该方法在八个场景中优于八个基线，在标准离线多智能体强化学习基准上建立了新的基于模型的泛化学习基线。</div>
</details>
</div>
<div class="card">
<div class="title">SPEC-RL: Accelerating On-Policy Reinforcement Learning with Speculative Rollouts</div>
<div class="meta-line">Authors: Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Xu Han, Peng Li, Anxiang Zeng, Jinsong Su</div>
<div class="meta-line">First: 2025-09-27T10:32:34+00:00 · Latest: 2026-01-12T11:06:46+00:00</div>
<div class="meta-line">Comments: fixed typos</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23232v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23232v3">PDF</a> · <a href="https://github.com/ShopeeLLM/Spec-RL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) increasingly rely on reinforcement learning with verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning. However, the training process remains bottlenecked by the computationally expensive rollout stage. Existing acceleration methods-such as parallelization, objective- and data-driven modifications, and replay buffers-either incur diminishing returns, introduce bias, or overlook redundancy across iterations. We identify that rollouts from consecutive training epochs frequently share a large portion of overlapping segments, wasting computation. To address this, we propose SPEC-RL, a novel framework that integrates SPECulative decoding with the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency. Experiments on diverse math reasoning and generalization benchmarks, including AIME24, MATH-500, OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout time by 2-3x without compromising policy quality. As a purely rollout-stage enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g., PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPEC-RL：通过推测性轨迹展开加速同策略强化学习</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）日益依赖可验证奖励的强化学习（RLVR）来激发可靠的思维链推理。然而，训练过程仍受限于计算成本高昂的轨迹展开阶段。现有加速方法（如并行化、目标与数据驱动的修改、经验回放缓冲区）要么收益递减，要么引入偏差，或忽略了迭代间的冗余。我们发现连续训练周期的轨迹展开常存在大量重叠片段，造成计算浪费。为此，我们提出SPEC-RL——一种将推测性解码与RL轨迹展开过程相结合的新框架。该框架复用先前的轨迹片段作为推测前缀，通过草稿-验证机制进行扩展，在确保策略一致性的同时避免冗余生成。在包括AIME24、MATH-500、OlympiadBench、MMLU-STEM等多样化数学推理与泛化基准测试中，实验表明SPEC-RL能将轨迹展开时间减少2-3倍且不降低策略质量。作为纯轨迹展开阶段的增强方案，SPEC-RL可无缝集成主流算法（如PPO、GRPO、DAPO），为大规模推理模型的RLVR扩展提供了通用且实用的路径。代码已开源：https://github.com/ShopeeLLM/Spec-RL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for SPEC-RL stems from the computational bottleneck in reinforcement learning with verifiable rewards (RLVR) for large language models, where traditional rollout acceleration methods face diminishing returns, bias, or inefficiency due to redundant overlapping segments across training epochs. The method introduces a novel framework that integrates speculative decoding into the RL rollout process, reusing prior trajectory segments as speculative prefixes and extending them through a draft-and-verify mechanism to avoid redundant generation while maintaining policy consistency. Experimental results on benchmarks such as AIME24, MATH-500, and MMLU-STEM show that SPEC-RL reduces rollout time by 2-3 times without compromising policy quality, and it seamlessly integrates with mainstream algorithms like PPO and GRPO as a general rollout-stage enhancement.</div>
<div class="mono" style="margin-top:8px">SPEC-RL的动机源于大型语言模型在可验证奖励强化学习中的计算瓶颈，传统加速方法因训练迭代间存在冗余重叠片段而面临收益递减、偏差或低效问题。该方法提出了一种新颖框架，将推测性解码集成到强化学习rollout过程中，重用先前的轨迹片段作为推测前缀，并通过草稿-验证机制进行扩展，以避免冗余生成同时保持策略一致性。在AIME24、MATH-500和MMLU-STEM等基准测试中的实验结果表明，SPEC-RL将rollout时间减少了2-3倍且不损害策略质量，并能作为通用的rollout阶段增强无缝集成到PPO、GRPO等主流算法中。</div>
</details>
</div>
<div class="card">
<div class="title">BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Yunpeng Qing, Yixiao Chi, Shuo Chen, Shunyu Liu, Kelu Yao, Sixu Lin, Litao Liu, Changqing Zou</div>
<div class="meta-line">First: 2025-06-06T05:41:33+00:00 · Latest: 2026-01-12T10:54:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05762v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.05762v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in offline Reinforcement Learning (RL) have proven that effective policy learning can benefit from imposing conservative constraints on pre-collected datasets. However, such static datasets often exhibit distribution bias, resulting in limited generalizability. To address this limitation, a straightforward solution is data augmentation (DA), which leverages generative models to enrich data distribution. Despite the promising results, current DA techniques focus solely on reconstructing future trajectories from given states, while ignoring the exploration of history transitions that reach them. This single-direction paradigm inevitably hinders the discovery of diverse behavior patterns, especially those leading to critical states that may have yielded high-reward outcomes. In this work, we introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework for offline RL that models both future and history trajectories from any intermediate states. Specifically, we decompose the trajectory generation task into two independent yet complementary diffusion processes: one generating forward trajectories to predict future dynamics, and the other generating backward trajectories to trace essential history transitions.BiTrajDiff can efficiently leverage critical states as anchors to expand into potentially valuable yet underexplored regions of the state space, thereby facilitating dataset diversity. Extensive experiments on the D4RL benchmark suite demonstrate that BiTrajDiff achieves superior performance compared to other advanced DA methods across various offline RL backbones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiTrajDiff：基于扩散模型的双向轨迹生成用于离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习的最新进展表明，对预收集数据集施加保守约束有助于有效策略学习。然而，此类静态数据集常存在分布偏差，导致泛化能力受限。为应对这一局限，数据增强作为一种直接解决方案，利用生成模型丰富数据分布。尽管现有方法取得了一定成效，但当前数据增强技术仅关注从给定状态重构未来轨迹，而忽略了探索到达这些状态的历史转移。这种单向范式不可避免地限制了多样化行为模式的发现，尤其是那些可能导致高回报关键状态的模式。本文提出双向轨迹扩散，这是一种用于离线强化学习的新型数据增强框架，可从任意中间状态同时建模未来与历史轨迹。具体而言，我们将轨迹生成任务分解为两个独立且互补的扩散过程：前向扩散生成未来动态轨迹，后向扩散追溯关键历史转移。该框架能高效利用关键状态作为锚点，拓展至状态空间中潜在有价值但未充分探索的区域，从而提升数据集多样性。在D4RL基准测试上的大量实验表明，相较于其他先进数据增强方法，该框架在多种离线强化学习骨干模型上均取得了更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces BiTrajDiff, a data augmentation framework for offline reinforcement learning motivated by the limited diversity and distribution bias of static datasets, which constrain policy generalizability. The method employs bidirectional diffusion models to generate both future and historical trajectories from any intermediate state, decomposing the task into two complementary processes to explore diverse behavior patterns and critical states. Experimental results on the D4RL benchmark show that BiTrajDiff outperforms other advanced data augmentation techniques across various offline RL backbones, enhancing dataset diversity and performance.</div>
<div class="mono" style="margin-top:8px">本文提出了BiTrajDiff，一种用于离线强化学习的数据增强框架，其动机在于静态数据集存在分布偏差和多样性不足，限制了策略的泛化能力。该方法采用双向扩散模型，从任意中间状态生成未来和历史轨迹，将任务分解为两个互补的过程，以探索多样行为模式和关键状态。在D4RL基准测试上的实验结果表明，BiTrajDiff优于其他先进的数据增强方法，在不同离线RL骨干网络中提升了数据集多样性和性能。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in Mathematical Reasoning</div>
<div class="meta-line">Authors: Ziheng Li, Liu Kang, Feng Xiao, Luxi Xing, Qingyi Si, Zhuoran Li, Weikang Gong, Deqing Yang, Yanghua Xiao, Hongcheng Guo</div>
<div class="meta-line">First: 2026-01-12T10:48:02+00:00 · Latest: 2026-01-12T10:48:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07408v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07408v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) has emerged as a promising critic-free reinforcement learning paradigm for reasoning tasks. However, standard GRPO employs a coarse-grained credit assignment mechanism that propagates group-level rewards uniformly to to every token in a sequence, neglecting the varying contribution of individual reasoning steps. We address this limitation by introducing Outcome-grounded Advantage Reshaping (OAR), a fine-grained credit assignment mechanism that redistributes advantages based on how much each token influences the model&#x27;s final answer. We instantiate OAR via two complementary strategies: (1) OAR-P, which estimates outcome sensitivity through counterfactual token perturbations, serving as a high-fidelity attribution signal; (2) OAR-G, which uses an input-gradient sensitivity proxy to approximate the influence signal with a single backward pass. These importance signals are integrated with a conservative Bi-Level advantage reshaping scheme that suppresses low-impact tokens and boosts pivotal ones while preserving the overall advantage mass. Empirical results on extensive mathematical reasoning benchmarks demonstrate that while OAR-P sets the performance upper bound, OAR-G achieves comparable gains with negligible computational overhead, both significantly outperforming a strong GRPO baseline, pushing the boundaries of critic-free LLM reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的优势重塑机制在数学推理中的细粒度信用分配研究</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）已成为推理任务中一种前景广阔的无评论者强化学习范式。然而，标准GRPO采用粗粒度信用分配机制，将群体层面的奖励均匀传播给序列中的每个标记，忽略了各推理步骤的差异性贡献。我们通过引入基于结果的优势重塑（OAR）机制来解决这一局限，该细粒度信用分配机制根据每个标记对模型最终答案的影响程度重新分配优势值。我们通过两种互补策略实现OAR：（1）OAR-P通过反事实标记扰动评估结果敏感性，作为高保真归因信号；（2）OAR-G采用输入梯度敏感性代理，通过单次反向传播近似影响信号。这些重要性信号与保守的双层优势重塑方案相结合，在保持整体优势总量的同时抑制低影响标记并增强关键标记。在广泛数学推理基准上的实验结果表明：OAR-P设定了性能上限，而OAR-G以可忽略的计算开销实现了可比增益，两者均显著超越强GRPO基线，推动了无评论者大语言模型推理的边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the coarse-grained credit assignment in Group Relative Policy Optimization (GRPO) for reasoning tasks, which uniformly rewards all tokens and overlooks individual step contributions. The method introduces Outcome-grounded Advantage Reshaping (OAR), a fine-grained mechanism that redistributes advantages based on each token&#x27;s influence on the final answer, implemented via two strategies: OAR-P uses counterfactual token perturbations for high-fidelity attribution, while OAR-G employs an input-gradient proxy for efficient approximation, both integrated with a conservative Bi-Level reshaping scheme. Main experimental results on mathematical reasoning benchmarks show that OAR-P sets a performance upper bound, OAR-G achieves comparable gains with minimal computational cost, and both significantly outperform the GRPO baseline, advancing critic-free LLM reasoning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决数学推理任务中组相对策略优化（GRPO）的粗粒度信用分配问题，该方法对所有标记均匀奖励而忽略了个别推理步骤的贡献。方法上提出了基于结果的优势重塑（OAR），这是一种细粒度信用分配机制，根据每个标记对最终答案的影响重新分配优势，通过两种策略实现：OAR-P利用反事实标记扰动提供高保真归因信号，而OAR-G使用输入梯度敏感性代理以单次反向传播近似影响信号，两者均与保守的双层优势重塑方案结合。主要实验结果在广泛的数学推理基准测试中表明，OAR-P设定了性能上限，OAR-G以可忽略的计算开销实现可比增益，两者均显著优于强GRPO基线，推动了无批评者LLM推理的边界。</div>
</details>
</div>
<div class="card">
<div class="title">MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP</div>
<div class="meta-line">Authors: Ruiqi Li, Zhiqiang Wang, Yunhao Yao, Xiang-Yang Li</div>
<div class="meta-line">First: 2026-01-12T10:28:46+00:00 · Latest: 2026-01-12T10:28:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07395v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07395v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a particularly stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MCP-ITP：一种针对MCP中隐式工具投毒的自动化框架</div>
<div class="mono" style="margin-top:8px">为规范基于LLM的智能体与环境间的交互，模型上下文协议（MCP）被提出并已广泛采用。然而，集成外部工具扩大了攻击面，使智能体面临工具投毒攻击。在此类攻击中，恶意指令通过工具元数据在MCP注册阶段注入智能体上下文，从而操控其行为。先前研究主要关注显式工具投毒或依赖人工构造的投毒工具。相比之下，我们聚焦于一种更具隐蔽性的变体：隐式工具投毒，即投毒工具本身未被调用，而是通过工具元数据中嵌入的指令诱导智能体调用合法但高权限工具执行恶意操作。我们提出MCP-ITP，这是首个针对MCP生态系统的自动化自适应隐式工具投毒框架。MCP-ITP将投毒工具生成建模为黑盒优化问题，采用迭代优化策略，结合评估LLM和检测LLM的反馈，以在规避现有检测机制的同时最大化攻击成功率（ASR）。在MCPTox数据集上对12个LLM智能体的实验表明，MCP-ITP始终优于人工构造的基线方法，最高可实现84.2%的ASR，同时将恶意工具检测率（MDR）压制至最低0.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the security vulnerability of implicit tool poisoning within the Model Context Protocol (MCP), where malicious instructions in tool metadata can manipulate LLM agents without invoking the poisoned tool itself. The authors propose MCP-ITP, an automated framework that treats poisoned tool generation as a black-box optimization problem, using iterative feedback from evaluation and detection LLMs to craft effective attacks. Experiments on the MCPTox dataset with 12 LLM agents show the method achieves up to 84.2% attack success rate while reducing malicious tool detection to as low as 0.3%, outperforming manual baselines.</div>
<div class="mono" style="margin-top:8px">本文针对模型上下文协议（MCP）中隐式工具投毒的安全漏洞展开研究，即工具元数据中的恶意指令可在不调用被投毒工具的情况下操纵LLM智能体。作者提出了MCP-ITP自动化框架，将投毒工具生成视为黑盒优化问题，利用评估和检测LLM的迭代反馈来生成有效攻击。在MCPTox数据集上对12个LLM智能体的实验表明，该方法攻击成功率最高达84.2%，同时将恶意工具检测率压低至0.3%，优于人工设计的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training</div>
<div class="meta-line">Authors: Xueyan Niu, Bo Bai, Wei Han, Weixi Zhang</div>
<div class="meta-line">First: 2026-01-12T10:14:09+00:00 · Latest: 2026-01-12T10:14:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07389v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07389v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论后训练中监督微调与强化学习的不可解耦性</div>
<div class="mono" style="margin-top:8px">大语言模型的后训练通常交替进行监督微调（SFT）与强化学习（RL）。这两种方法目标不同：SFT最小化模型输出与专家响应之间的交叉熵损失，而RL最大化来自人类偏好或基于规则的验证器的奖励信号。现代推理模型广泛采用交替进行SFT与RL训练的做法，但二者能否解耦尚无理论解释。我们证明两种顺序的解耦均不可行：（1）先SFT后RL的耦合：在SFT最优性下，RL会增加SFT损失；（2）先RL后SFT的耦合：SFT会降低RL已获得的奖励。基于Qwen3-0.6B的实验证实了预测的性能下降，验证了在后训练中分离SFT与RL会导致先前性能损失。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the theoretical and practical inseparability of supervised fine-tuning (SFT) and reinforcement learning (RL) during the post-training of large language models. The motivation stems from the common practice of alternating SFT, which minimizes cross-entropy loss on expert data, with RL, which maximizes reward signals, without a clear understanding of their interdependence. The authors prove that decoupling these stages in either order is impossible: RL following SFT increases the SFT loss, and SFT following RL reduces the previously achieved reward. Experimental validation using the Qwen3-0.6B model confirms this predicted performance degradation, demonstrating that separating SFT and RL leads to a loss of prior gains.</div>
<div class="mono" style="margin-top:8px">本文研究了大语言模型后训练中监督微调（SFT）与强化学习（RL）在理论和实践上的不可分离性。其动机源于当前普遍交替使用SFT（最小化专家数据的交叉熵损失）和RL（最大化奖励信号）的做法，但缺乏对其相互依赖关系的清晰理解。作者证明，以任意顺序解耦这两个阶段都是不可能的：在SFT之后进行RL会增加SFT损失，而在RL之后进行SFT则会降低先前获得的奖励。使用Qwen3-0.6B模型进行的实验验证了这种预测的性能下降，表明分离SFT和RL会导致先前获得的性能增益丧失。</div>
</details>
</div>
<div class="card">
<div class="title">OpenTinker: Separating Concerns in Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Siqi Zhu, Jiaxuan You</div>
<div class="meta-line">First: 2026-01-12T09:57:46+00:00 · Latest: 2026-01-12T09:57:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07376v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07376v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenTinker：分离智能体强化学习中的关注点</div>
<div class="mono" style="margin-top:8px">本文介绍OpenTinker——一种围绕算法设计、执行与智能体-环境交互的关注点分离原则构建的大型语言模型智能体强化学习基础设施。该框架摒弃了单一化的端到端强化学习流程，将智能体学习系统解耦为具有明确定义抽象边界的轻量级可组合模块。用户可自定义智能体、环境及交互协议，而推理与训练任务则交由托管执行运行时处理。OpenTinker引入了集中式调度器，可在共享资源上管理基于LoRA/全参数的强化学习、监督微调及推理等训练与推理工作负载。本文进一步探讨了将OpenTinker扩展至多智能体训练的设计原则，并通过系列强化学习应用案例展示了该框架在实际智能体学习场景中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind OpenTinker is to address the complexity of monolithic, end-to-end pipelines in agentic reinforcement learning by proposing a modular infrastructure that separates concerns across algorithm design, execution, and agent-environment interaction. Its method involves decomposing learning systems into lightweight, composable components with clear abstraction boundaries, where users specify agents and environments while a managed runtime handles inference and training, supported by a centralized scheduler for workloads like LoRA-based and full-parameter RL. The main experimental results demonstrate the framework&#x27;s effectiveness through a set of practical RL use cases, showcasing its utility in agentic learning scenarios and its extensibility to multi-agent training.</div>
<div class="mono" style="margin-top:8px">OpenTinker的动机是解决智能体强化学习中端到端整体式流程的复杂性，提出了一种模块化基础设施，将算法设计、执行和智能体-环境交互的关注点分离。其方法是将学习系统分解为具有清晰抽象边界的轻量级可组合组件，用户指定智能体和环境，而托管运行时处理推理和训练，并通过集中式调度器管理如基于LoRA和全参数强化学习等工作负载。主要实验结果通过一系列实际强化学习用例展示了该框架的有效性，证明了其在智能体学习场景中的实用性以及对多智能体训练的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning</div>
<div class="meta-line">Authors: Jinchang Luo, Mingquan Cheng, Fan Wan, Ni Li, Xiaoling Xia, Shuangshuang Tian, Tingcheng Bian, Haiwei Wang, Haohuan Fu, Yan Tao</div>
<div class="meta-line">First: 2025-10-23T13:35:02+00:00 · Latest: 2026-01-12T09:03:57+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20548v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.20548v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GlobalRAG：通过强化学习增强多跳问答中的全局推理能力</div>
<div class="mono" style="margin-top:8px">强化学习近期在改进检索增强生成（RAG）方面展现出潜力。然而，其在多跳问答（QA）中的有效性仍受限于两个根本问题：（i）缺乏构建多步推理的全局规划；（ii）执行不忠实，阻碍了有效查询构建和检索证据的一致性使用。我们提出GlobalRAG，一个旨在增强多跳QA中全局推理的强化学习框架。该框架将问题分解为子目标，协调检索与推理过程，并迭代优化证据。为引导此过程，我们引入了规划质量奖励和子目标完成奖励，以促进连贯规划和可靠子目标执行。此外，渐进权重退火策略平衡了过程导向与结果导向的目标。在领域内和领域外基准测试上的大量实验表明，GlobalRAG仅使用8k训练数据（相当于强基线训练数据的42%）即显著超越强基线模型，在EM和F1指标上平均提升14.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in applying reinforcement learning to retrieval-augmented generation for multi-hop question answering, specifically the lack of global planning and unfaithful execution that hinder structured reasoning and consistent evidence use. It proposes GlobalRAG, a framework that decomposes questions into subgoals, coordinates retrieval with reasoning, and iteratively refines evidence using novel rewards like Planning Quality Reward and SubGoal Completion Reward, alongside a progressive weight annealing strategy to balance process and outcome objectives. Experimental results on in-domain and out-of-domain benchmarks show that GlobalRAG significantly outperforms strong baselines with only 8k training data, achieving average improvements of 14.2% in both EM and F1 scores.</div>
<div class="mono" style="margin-top:8px">该论文针对强化学习在检索增强生成中应用于多跳问答时的局限性，即缺乏全局规划和执行不忠实的问题，这些问题阻碍了结构化推理和证据的一致性使用。它提出了GlobalRAG框架，通过将问题分解为子目标、协调检索与推理、并利用规划质量奖励和子目标完成奖励等新机制迭代优化证据，同时采用渐进权重退火策略平衡过程与结果目标。在领域内和领域外基准测试上的实验结果表明，GlobalRAG仅使用8k训练数据就显著优于强基线方法，在EM和F1分数上平均提升了14.2%。</div>
</details>
</div>
<div class="card">
<div class="title">Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training</div>
<div class="meta-line">Authors: Xue Gong, Qi Yi, Ziyuan Nan, Guanhua Huang, Kejiao Li, Yuhao Jiang, Ruibin Xiong, Zenan Xu, Jiaming Guo, Shaohui Peng, Bo Zhou</div>
<div class="meta-line">First: 2026-01-12T08:41:47+00:00 · Latest: 2026-01-12T08:41:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07320v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07320v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training Large Language Models (LLMs) for reasoning tasks is increasingly driven by Reinforcement Learning with Verifiable Rewards (RLVR), where Proximal Policy Optimization (PPO) provides a principled framework for stable policy updates. However, the practical application of PPO is hindered by unreliable advantage estimation in the sparse-reward RLVR regime. This issue arises because the sparse rewards in RLVR lead to inaccurate intermediate value predictions, which in turn introduce significant bias when aggregated at every token by Generalized Advantage Estimation (GAE). To address this, we introduce Segmental Advantage Estimation (SAE), which mitigates the bias that GAE can incur in RLVR. Our key insight is that aggregating $n$-step advantages at every token(as in GAE) is unnecessary and often introduces excessive bias, since individual tokens carry minimal information. Instead, SAE first partitions the generated sequence into coherent sub-segments using low-probability tokens as heuristic boundaries. It then selectively computes variance-reduced advantage estimates only from these information-rich segment transitions, effectively filtering out noise from intermediate tokens. Our experiments demonstrate that SAE achieves superior performance, with marked improvements in final scores, training stability, and sample efficiency. These gains are shown to be consistent across multiple model sizes, and a correlation analysis confirms that our proposed advantage estimator achieves a higher correlation with an approximate ground-truth advantage, justifying its superior performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分段优势估计：增强PPO在长上下文LLM训练中的性能</div>
<div class="mono" style="margin-top:8px">针对推理任务的大语言模型训练日益依赖可验证奖励的强化学习，其中近端策略优化为稳定策略更新提供了理论框架。然而，PPO在实际应用中被稀疏奖励RLVR机制中不可靠的优势估计所阻碍。该问题源于RLVR的稀疏奖励导致中间价值预测不准确，进而在广义优势估计逐词元聚合时引入显著偏差。为此，我们提出分段优势估计方法，以缓解GAE在RLVR中可能产生的偏差。核心洞见在于：逐词元聚合n步优势既非必要且常引入过度偏差，因为单个词元信息量有限。SAE首先使用低概率词元作为启发式边界，将生成序列划分为连贯子段，随后仅从这些信息丰富的段间转换选择性计算方差缩减的优势估计，有效滤除中间词元的噪声。实验表明SAE实现了更优性能，在最终得分、训练稳定性和样本效率方面均有显著提升。这些增益在不同模型规模中表现一致，相关性分析证实我们提出的优势估计器与近似真实优势具有更高相关性，佐证了其优越性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of unreliable advantage estimation in Proximal Policy Optimization (PPO) when training Large Language Models for reasoning tasks with sparse verifiable rewards, which leads to biased policy updates. To mitigate this, the authors propose Segmental Advantage Estimation (SAE), a method that partitions generated sequences into sub-segments using low-probability tokens as boundaries and computes advantage estimates selectively at segment transitions rather than at every token, thereby reducing noise and bias. Experimental results show that SAE improves final performance scores, enhances training stability, and increases sample efficiency across various model sizes, with correlation analysis confirming its advantage estimates align better with ground-truth values.</div>
<div class="mono" style="margin-top:8px">本文针对在稀疏可验证奖励下训练大型语言模型进行推理任务时，近端策略优化中优势估计不可靠导致策略更新偏差的问题。作者提出了分段优势估计方法，通过以低概率词元作为启发式边界将生成序列划分为连贯子段，并选择性地在信息丰富的段间计算方差降低的优势估计，从而过滤中间词元的噪声。实验结果表明，该方法在最终得分、训练稳定性和样本效率上均取得显著提升，且在不同模型规模上表现一致，相关性分析证实其优势估计与近似真实优势具有更高相关性，验证了其优越性能。</div>
</details>
</div>
<div class="card">
<div class="title">VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing</div>
<div class="meta-line">Authors: Guanyuan Pan, Yugui Lin, Tiansheng Zhou, Pietro Liò, Shuai Wang, Yaqi Wang</div>
<div class="meta-line">First: 2026-01-12T08:37:32+00:00 · Latest: 2026-01-12T08:37:32+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07315v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07315v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches often underutilize circuit schematics and lack the explainability required for industry adoption. To tackle these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-starting from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance, achieving a 100% success rate in optimizing an amplifier with a complementary input and a class-AB output stage, while maintaining total runtime under 43 minutes across all experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLM-CAD：面向模拟电路尺寸优化的视觉语言模型协同智能体设计流程</div>
<div class="mono" style="margin-top:8px">模拟混合信号电路尺寸优化涉及高维设计空间中的复杂权衡。现有自动优化方法常未充分利用电路原理图，且缺乏工业应用所需的可解释性。为此，我们提出一种视觉语言模型优化的协同智能体设计流程（VLM-CAD），通过分析电路、优化直流工作点、执行基于推理的尺寸优化及外部尺寸优化来应对这些挑战。我们集成Image2Net工具标注电路原理图并生成结构化JSON描述，供视觉语言模型精准解析。进一步提出可解释信任域贝叶斯优化方法（ExTuRBO），采用智能体生成种子的协同热启动策略，并为外部尺寸优化提供双粒度灵敏度分析，支持生成完整最终设计报告。基于180nm、90nm和45nm预测技术模型的放大器尺寸优化实验表明，VLM-CAD能有效平衡功耗与性能，在优化互补输入级与AB类输出级放大器时实现100%成功率，且所有实验总运行时间均控制在43分钟内。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better utilize circuit schematics and improve explainability in automatic analog circuit sizing, this paper introduces VLM-CAD, a workflow that integrates Vision Language Models to analyze annotated schematics and employs a novel Explainable Trust Region Bayesian Optimization method for optimization. The method combines schematic interpretation, inference-based sizing, and external optimization with collaborative warm-starting and sensitivity analysis. Experimental results on amplifier sizing across 180nm, 90nm, and 45nm technology nodes show that VLM-CAD successfully balances power and performance, achieving a 100% success rate for a complex amplifier design while completing all optimizations in under 43 minutes.</div>
<div class="mono" style="margin-top:8px">针对自动模拟电路尺寸设计中电路原理图利用不足和可解释性差的问题，本文提出了VLM-CAD工作流程，该方法集成视觉语言模型来分析带标注的原理图，并采用一种新颖的可解释信任域贝叶斯优化方法进行优化。该方法结合了原理图解析、基于推理的尺寸确定和外部优化，并采用协作热启动和灵敏度分析。在180nm、90nm和45nm工艺节点上的放大器尺寸设计实验结果表明，VLM-CAD能有效平衡功耗与性能，对一个具有互补输入和AB类输出级的复杂放大器实现了100%的优化成功率，且所有实验的总运行时间控制在43分钟以内。</div>
</details>
</div>
<div class="card">
<div class="title">Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts</div>
<div class="meta-line">Authors: Yun Chen, Bowei Huang, Fan Guo, Kang Song</div>
<div class="meta-line">First: 2026-01-12T08:27:24+00:00 · Latest: 2026-01-12T08:27:24+00:00</div>
<div class="meta-line">Comments: 9 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous mobile manipulation in unstructured warehouses requires a balance between efficient large-scale navigation and high-precision object interaction. Traditional end-to-end learning approaches often struggle to handle the conflicting demands of these distinct phases. Navigation relies on robust decision-making over large spaces, while manipulation needs high sensitivity to fine local details. Forcing a single network to learn these different objectives simultaneously often causes optimization interference, where improving one task degrades the other. To address these limitations, we propose a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework tailored for autonomous forklifts. HMER decomposes long-horizon tasks into specialized sub-policies controlled by a Semantic Task Planner. This structure separates macro-level navigation from micro-level manipulation, allowing each expert to focus on its specific action space without interference. The planner coordinates the sequential execution of these experts, bridging the gap between task planning and continuous control. Furthermore, to solve the problem of sparse exploration, we introduce a Hybrid Imitation-Reinforcement Training Strategy. This method uses expert demonstrations to initialize the policy and Reinforcement Learning for fine-tuning. Experiments in Gazebo simulations show that HMER significantly outperforms sequential and end-to-end baselines. Our method achieves a task success rate of 94.2\% (compared to 62.5\% for baselines), reduces operation time by 21.4\%, and maintains placement error within 1.5 cm, validating its efficacy for precise material handling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向自主叉车长时域多目标任务的异构多专家强化学习</div>
<div class="mono" style="margin-top:8px">非结构化仓库中的自主移动操作需兼顾大规模高效导航与高精度物体交互。传统端到端学习方法常难以协调这两个阶段的不同需求：导航依赖大范围空间的鲁棒决策，而操作则需对局部细节高度敏感。强制单一网络同时学习这些不同目标易导致优化干扰，即提升一项任务会损害另一项。为此，我们提出一种专为自主叉车设计的异构多专家强化学习框架。该框架将长时域任务分解为由语义任务规划器控制的专用子策略，从而在结构上分离宏观导航与微观操作，使各专家能专注于特定动作空间而无相互干扰。规划器协调这些专家的顺序执行，弥合了任务规划与连续控制之间的鸿沟。此外，为解决稀疏探索问题，我们引入混合模仿-强化训练策略，通过专家演示初始化策略，再以强化学习进行微调。Gazebo仿真实验表明，该框架显著优于序列化与端到端基线方法：任务成功率提升至94.2%（基线为62.5%），操作时间减少21.4%，且放置误差保持在1.5厘米内，验证了其在精密物料搬运中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of autonomous mobile manipulation in warehouses, where a single policy struggles to balance the conflicting demands of long-range navigation and precise object interaction. The authors propose a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework that decomposes tasks into specialized sub-policies for navigation and manipulation, coordinated by a Semantic Task Planner, and employ a hybrid training strategy combining imitation learning and reinforcement learning to overcome sparse rewards. Experimental results in simulation demonstrate that HMER achieves a 94.2% task success rate, a 21.4% reduction in operation time, and sub-1.5 cm placement error, significantly outperforming baseline methods.</div>
<div class="mono" style="margin-top:8px">本文针对仓库环境中自主移动操作的挑战，即单一策略难以平衡大范围导航与精确物体交互的冲突需求。作者提出了一种异构多专家强化学习框架，通过语义任务规划器将任务分解为专门的导航与操作子策略进行协调，并采用结合模仿学习与强化学习的混合训练策略以解决稀疏奖励问题。在仿真实验中的结果表明，该方法实现了94.2%的任务成功率，操作时间减少21.4%，放置误差保持在1.5厘米以内，性能显著优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning</div>
<div class="meta-line">Authors: Wenxun Wu, Yuanyang Li, Guhan Chen, Linyue Wang, Hongyang Chen</div>
<div class="meta-line">First: 2025-10-08T14:04:27+00:00 · Latest: 2026-01-12T08:14:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.07038v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.07038v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have popularized test-time scaling, where models generate additional reasoning tokens before producing final answers. These approaches have demonstrated significant performance improvements on benchmarks involving mathematical reasoning. However, language models relying solely on direct inference still struggle with tasks demanding up-to-date knowledge or computational tools such as calculators and code interpreters for complex arithmetic operations. To overcome these limitations, we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement learning framework that systematically integrates multi-hop reasoning with adaptive tool-calling capabilities. Our approach employs a modified version of Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm, which we adapt specifically for tool invocation scenarios, enabling models to dynamically interleave complex reasoning with on-demand tool usage (including search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and TAPO-hard-18K, specifically designed to train and evaluate both fact-based reasoning and mathematical calculation capabilities. Our experiments on Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach, with both models achieving state-of-the-art performance on tasks requiring external knowledge and mathematical computation among methods with comparable parameters. Notably, TAPO achieves more efficient tool utilization than baseline methods while preventing excessive calls caused by reward hacking. These results highlight the significant potential of combining advanced reasoning with tool usage to enhance model performance in knowledge-intensive and computationally demanding tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>工具增强策略优化：推理与自适应工具使用与强化学习的协同</div>
<div class="mono" style="margin-top:8px">近期大语言模型（LLMs）的进展推动了测试时扩展的普及，即模型在生成最终答案前先产生额外的推理标记。这些方法在涉及数学推理的基准测试中展现出显著的性能提升。然而，仅依赖直接推理的语言模型在处理需要最新知识或计算工具（如计算器和代码解释器进行复杂算术运算）的任务时仍面临困难。为克服这些限制，我们提出了工具增强策略优化（TAPO），这是一种新颖的强化学习框架，系统性地将多跳推理与自适应工具调用能力相结合。我们的方法采用动态采样策略优化（DAPO）的改进版本——一种近期开发的强化学习范式，并专门针对工具调用场景进行适配，使模型能够动态交织复杂推理与按需工具使用（包括搜索API和Python解释器）。为支持本研究，我们引入了两个新数据集：TAPO-easy-60K和TAPO-hard-18K，专门用于训练和评估基于事实的推理与数学计算能力。在Qwen2.5-3B和Qwen2.5-7B模型上的实验证明了我们方法的有效性，在参数量可比的方法中，两个模型在需要外部知识和数学计算的任务上均达到了最先进的性能。值得注意的是，TAPO相比基线方法实现了更高效的工具利用，同时避免了因奖励攻击导致的过度调用。这些结果凸显了将高级推理与工具使用相结合，在知识密集型和计算密集型任务中提升模型性能的巨大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of large language models (LLMs) in handling tasks requiring current knowledge or computational tools, this paper introduces Tool-Augmented Policy Optimization (TAPO), a reinforcement learning framework that synergizes multi-hop reasoning with adaptive tool-calling. The method adapts Dynamic Sampling Policy Optimization (DAPO) to enable models to dynamically interleave reasoning with tool use, such as search APIs and Python interpreters. Experimental results on Qwen2.5 models using new datasets (TAPO-easy-60K and TAPO-hard-18K) show state-of-the-art performance in knowledge and math tasks, with more efficient tool utilization and reduced reward hacking compared to baselines.</div>
<div class="mono" style="margin-top:8px">针对大语言模型在处理需要最新知识或计算工具的任务时存在的局限，本文提出了工具增强策略优化（TAPO），这是一个将多跳推理与自适应工具调用相结合的强化学习框架。该方法改进动态采样策略优化（DAPO），使模型能够动态交织推理与工具使用（如搜索API和Python解释器）。在Qwen2.5模型上使用新数据集（TAPO-easy-60K和TAPO-hard-18K）的实验结果表明，该方法在知识和数学任务上取得了领先性能，相比基线方法实现了更高效的工具利用并减少了因奖励黑客行为导致的过度调用。</div>
</details>
</div>
<div class="card">
<div class="title">LRAS: Advanced Legal Reasoning with Agentic Search</div>
<div class="meta-line">Authors: Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo</div>
<div class="meta-line">First: 2026-01-12T08:07:35+00:00 · Latest: 2026-01-12T08:07:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07296v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07296v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on &quot;closed-loop reasoning&quot; derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric &quot;closed-loop thinking&quot; to dynamic and interactive &quot;Active Inquiry&quot;. By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32\%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LRAS：基于智能体搜索的高级法律推理</div>
<div class="mono" style="margin-top:8px">尽管大型推理模型在数学领域展现出卓越的逻辑能力，但其在法律领域的应用仍受限于对程序严谨性和法律逻辑遵循的严格要求。现有依赖纯内部参数化知识进行&#x27;闭环推理&#x27;的法律大语言模型，常因缺乏对知识边界的自我认知而产生自信但错误的结论。为解决这一挑战，我们提出基于智能体搜索的法律推理框架，这是首个将法律大语言模型从静态参数化的&#x27;闭环思维&#x27;转向动态交互式&#x27;主动探究&#x27;的框架。通过整合内省模仿学习与难度感知强化学习，该框架使大型推理模型能够识别知识边界并处理法律推理的复杂性。实证结果表明，该框架以8.2-32%的优势超越现有最优基线模型，在需要可靠知识深度推理的任务中提升最为显著。我们将很快发布相关数据与模型以供进一步探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of Large Reasoning Models (LRMs) in legal applications, where their closed-loop reasoning often leads to overconfident errors due to a lack of awareness about knowledge boundaries. To overcome this, the authors propose the Legal Reasoning with Agentic Search (LRAS) framework, which shifts legal LLMs from static parametric reasoning to dynamic active inquiry by integrating introspective imitation learning and difficulty-aware reinforcement learning. Experimental results show that LRAS outperforms state-of-the-art baselines by 8.2-32%, with the most significant improvements in tasks demanding deep reasoning with reliable knowledge.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型在法律领域应用中的局限性，即其封闭式推理常因缺乏知识边界意识而导致自信但错误的结论。为解决这一问题，作者提出了法律推理与主动搜索框架，通过整合内省模仿学习和难度感知强化学习，将法律大语言模型从静态参数化推理转向动态主动查询。实验结果表明，该框架在基准测试中性能提升8.2-32%，尤其在需要可靠知识深度推理的任务上表现最为突出。</div>
</details>
</div>
<div class="card">
<div class="title">Simulated Annealing-based Candidate Optimization for Batch Acquisition Functions</div>
<div class="meta-line">Authors: Sk Md Ahnaf Akif Alvi, Raymundo Arróyave, Douglas Allaire</div>
<div class="meta-line">First: 2026-01-12T06:51:49+00:00 · Latest: 2026-01-12T06:51:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07258v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07258v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization with multi-objective acquisition functions such as q-Expected Hypervolume Improvement (qEHVI) requires efficient candidate optimization to maximize acquisition function values. Traditional approaches rely on continuous optimization methods like Sequential Least Squares Programming (SLSQP) for candidate selection. However, these gradient-based methods can become trapped in local optima, particularly in complex or high-dimensional objective landscapes. This paper presents a simulated annealing-based approach for candidate optimization in batch acquisition functions as an alternative to conventional continuous optimization methods. We evaluate our simulated annealing approach against SLSQP across four benchmark multi-objective optimization problems: ZDT1 (30D, 2 objectives), DTLZ2 (7D, 3 objectives), Kursawe (3D, 2 objectives), and Latent-Aware (4D, 2 objectives). Our results demonstrate that simulated annealing consistently achieves superior hypervolume performance compared to SLSQP in most test functions. The improvement is particularly pronounced for DTLZ2 and Latent-Aware problems, where simulated annealing reaches significantly higher hypervolume values and maintains better convergence characteristics. The histogram analysis of objective space coverage further reveals that simulated annealing explores more diverse and optimal regions of the Pareto front. These findings suggest that metaheuristic optimization approaches like simulated annealing can provide more robust and effective candidate optimization for multi-objective Bayesian optimization, offering a promising alternative to traditional gradient-based methods for batch acquisition function optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模拟退火的批量采集函数候选优化</div>
<div class="mono" style="margin-top:8px">使用多目标采集函数（如q-期望超体积改进）的贝叶斯优化需要高效的候选优化以最大化采集函数值。传统方法依赖连续优化方法（如序列最小二乘规划）进行候选选择，但这些基于梯度的方法易陷入局部最优，尤其在复杂或高维目标空间中。本文提出一种基于模拟退火的批量采集函数候选优化方法，作为传统连续优化方法的替代方案。我们在四个基准多目标优化问题（ZDT1、DTLZ2、Kursawe和Latent-Aware）上将模拟退火与序列最小二乘规划进行对比评估。结果表明，在多数测试函数中，模拟退火始终获得更优的超体积性能，尤其在DTLZ2和Latent-Aware问题上表现显著，能达到更高的超体积值并保持更好的收敛特性。目标空间覆盖的直方图分析进一步显示，模拟退火能探索帕累托前沿更多样且更优的区域。这些发现表明，模拟退火等元启发式优化方法可为多目标贝叶斯优化提供更稳健有效的候选优化，为批量采集函数优化提供了有前景的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of gradient-based methods like SLSQP, which can get stuck in local optima when optimizing multi-objective acquisition functions such as qEHVI in Bayesian Optimization, this paper proposes a simulated annealing-based approach for candidate optimization. The method applies simulated annealing as an alternative to continuous optimization to maximize acquisition function values more effectively. Experimental results on benchmark problems including ZDT1, DTLZ2, Kursawe, and Latent-Aware show that simulated annealing consistently outperforms SLSQP in hypervolume performance, with notable improvements on DTLZ2 and Latent-Aware, and it explores more diverse regions of the Pareto front, indicating better convergence and robustness.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决传统梯度优化方法（如SLSQP）在优化多目标获取函数（如qEHVI）时易陷入局部最优的问题，为此提出了一种基于模拟退火的候选优化方法。该方法采用模拟退火替代连续优化，以更有效地最大化获取函数值。在ZDT1、DTLZ2、Kursawe和Latent-Aware等基准多目标优化问题上的实验结果表明，模拟退火在超体积性能上持续优于SLSQP，尤其在DTLZ2和Latent-Aware问题上提升显著，并能探索帕累托前沿更多样化的区域，展现出更好的收敛性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern for Reasoning</div>
<div class="meta-line">Authors: Hanbin Wang, Jingwei Song, Jinpeng Li, Fei Mi, Lifeng Shang</div>
<div class="meta-line">First: 2026-01-12T06:19:09+00:00 · Latest: 2026-01-12T06:19:09+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07238v1">PDF</a> · <a href="https://github.com/wanghanbinpanda/GPSO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) exhibit diverse high-level reasoning patterns (e.g., direct solution, reflection-and-verification, and exploring multiple solutions), yet prevailing training recipes implicitly bias models toward a limited set of dominant patterns. Through a systematic analysis, we identify substantial accuracy variance across these patterns on mathematics and science benchmarks, revealing that a model&#x27;s default reasoning pattern is often sub-optimal for a given problem. To address this, we introduce Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that extends GRPO by incorporating multi-pattern rollouts, verifier-guided optimal pattern selection per problem, and attention masking during optimization to prevent the leakage of explicit pattern suffixes into the learned policy. By exploring a portfolio of diverse reasoning strategies and optimizing the policy on the most effective ones, GPSO enables the model to internalize the mapping from problem characteristics to optimal reasoning patterns. Extensive experiments demonstrate that GPSO delivers consistent and substantial performance gains across various model backbones and benchmarks, effectively mitigating pattern sub-optimality and fostering more robust, adaptable reasoning. All data and codes are available at https://github.com/wanghanbinpanda/GPSO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>群体模式选择优化：让大型推理模型自主选择最佳推理模式</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）展现出多样化的高层推理模式（如直接求解、反思验证、多解探索），但主流训练方法会隐式地将模型偏向于有限的优势模式。通过系统分析，我们在数学与科学基准测试中发现不同模式间存在显著的准确率差异，表明模型默认的推理模式常非当前问题的最优解。为此，我们提出群体模式选择优化（GPSO）——一种基于GRPO扩展的强化学习框架，其融合了多模式推演、验证器引导的逐题最优模式选择，以及在优化过程中通过注意力掩码防止显式模式后缀泄露至学习策略。通过探索多样化推理策略组合并针对最有效策略优化策略网络，GPSO使模型能够内化从问题特征到最优推理模式的映射关系。大量实验表明，GPSO在不同模型架构与基准测试中均能带来持续显著的性能提升，有效缓解模式次优问题，促进更鲁棒、自适应的推理能力。所有数据与代码已开源：https://github.com/wanghanbinpanda/GPSO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue that large reasoning models (LRMs) are often biased toward a limited set of dominant reasoning patterns during training, which can be sub-optimal for specific problems, as evidenced by significant accuracy variance across patterns on mathematics and science benchmarks. To mitigate this, the authors propose Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that extends GRPO by incorporating multi-pattern rollouts, verifier-guided optimal pattern selection per problem, and attention masking to prevent pattern suffix leakage, enabling the model to learn a mapping from problem characteristics to the most effective reasoning strategies. Experimental results show that GPSO consistently yields substantial performance improvements across various model backbones and benchmarks, effectively reducing pattern sub-optimality and enhancing reasoning robustness and adaptability.</div>
<div class="mono" style="margin-top:8px">该论文针对大型推理模型在训练中常偏向于有限的主导推理模式，导致对特定问题可能非最优的问题，通过在数学和科学基准测试中观察到不同模式间显著的准确率差异而提出动机。为解决此问题，作者提出了组模式选择优化方法，这是一个强化学习框架，扩展了GRPO，通过整合多模式展开、基于验证器的问题级最优模式选择以及注意力掩码以防止模式后缀泄露，使模型能够学习从问题特征到最有效推理策略的映射。主要实验结果表明，GPSO在不同模型主干和基准测试中均能带来一致且显著的性能提升，有效缓解了模式非最优性，并增强了推理的鲁棒性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">ADPO: Anchored Direct Preference Optimization</div>
<div class="meta-line">Authors: Wang Zixian</div>
<div class="meta-line">First: 2025-10-21T05:53:13+00:00 · Latest: 2026-01-12T06:15:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18913v7">Abs</a> · <a href="https://arxiv.org/pdf/2510.18913v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Anchored Direct Preference Optimization (ADPO), a policy alignment method derived from first principles of KL-regularized reinforcement learning. Unlike standard approaches that treat the reference policy merely as a regularizer, we show that the optimal policy in reinforcement learning from human feedback inherently operates in a differential coordinate system, optimizing relative advantage in the form of log ratios rather than absolute probabilities. ADPO explicitly parameterizes this optimal structure through anchored logits, effectively decoupling response quality from prior popularity and creating an implicit trust region through curvature scaling. We show that this formulation unifies supervised fine-tuning, reinforcement learning, and ranking-based objectives under a single geometric perspective. Theoretically, ADPO resolves the probability smearing problem of supervised fine-tuning while avoiding the mode-seeking instability characteristic of reverse-KL methods. Empirically, the listwise ranking variant of ADPO achieves state-of-the-art performance on reasoning tasks, outperforming GRPO by 30.9 percent on Qwen3-1.7B and demonstrating superior robustness under distribution shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ADPO：锚定直接偏好优化</div>
<div class="mono" style="margin-top:8px">本文提出锚定直接偏好优化（ADPO），一种基于KL正则化强化学习基本原理推导的策略对齐方法。不同于仅将参考策略作为正则器的标准方法，我们证明基于人类反馈的强化学习中最优策略本质上运行于微分坐标系中，通过对数比形式优化相对优势而非绝对概率。ADPO通过锚定逻辑参数显式参数化这一最优结构，有效解耦响应质量与先验流行度，并通过曲率缩放创建隐式信任区域。该公式将监督微调、强化学习和基于排序的目标统一于单一几何视角。理论上，ADPO解决了监督微调的概率弥散问题，同时避免了反向KL方法固有的模式寻求不稳定性。实证表明，ADPO的列表排序变体在推理任务上达到最先进性能，在Qwen3-1.7B模型上超越GRPO达30.9%，并在分布偏移下表现出卓越的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Anchored Direct Preference Optimization (ADPO), motivated by the need to improve policy alignment by treating the reference policy as more than just a regularizer, instead deriving it from KL-regularized reinforcement learning principles. The method parameterizes the optimal policy using anchored logits, which operate in a differential coordinate system to optimize relative advantage via log ratios, thereby decoupling response quality from prior popularity and creating an implicit trust region. Experimentally, ADPO achieves state-of-the-art results, outperforming GRPO by 30.9% on Qwen3-1.7B in reasoning tasks and showing superior robustness under distribution shift.</div>
<div class="mono" style="margin-top:8px">本文提出了锚定直接偏好优化（ADPO），其动机源于需要改进策略对齐方法，将参考策略视为不仅仅是正则化项，而是基于KL正则化强化学习原理推导而来。该方法通过锚定logits参数化最优策略，在微分坐标系中通过对数比优化相对优势，从而解耦响应质量与先验流行度，并创建隐式信任区域。实验结果表明，ADPO在推理任务上取得了最先进的性能，在Qwen3-1.7B模型上比GRPO高出30.9%，并在分布偏移下表现出更优的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting</div>
<div class="meta-line">Authors: Kun Zhao, Siyuan Dai, Pan Wang, Jifeng Song, Hui Ji, Chenghua Lin, Liang Zhan, Haoteng Tang</div>
<div class="meta-line">First: 2026-01-06T14:17:44+00:00 · Latest: 2026-01-12T05:56:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03321v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03321v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel &quot;Reason-then-Summarize&quot; architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>发现与诊断对齐：基于自洽强化学习的可信放射学报告生成框架</div>
<div class="mono" style="margin-top:8px">多模态大语言模型在放射学报告生成中展现出巨大潜力，但其临床转化受限于架构异构性与事实性幻觉的普遍存在。标准监督微调常难以严格对齐语言输出与视觉证据，而现有强化学习方法则面临计算成本过高或探索能力有限的问题。为此，我们提出一种自洽放射学报告生成的综合框架。首先通过系统评估确定医学影像处理中最优的视觉编码器与大语言模型主干配置。在此基础上，引入基于组相对策略优化的新型“先推理后总结”架构，将生成过程重构为两个独立模块：用于详细发现的思维模块与用于结构化疾病标签的应答模块。通过采用多维复合奖励函数，显式惩罚生成叙述与最终诊断间的逻辑不一致性。在MIMIC-CXR基准上的大量实验表明，本方法在临床效能指标上达到最先进水平，相较于强监督基线显著减少了幻觉现象。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of aligning radiology report generation with clinical evidence to reduce factual hallucinations in Multimodal Large Language Models (MLLMs). The authors propose a self-consistent framework that first identifies optimal vision and language model backbones, then introduces a &quot;Reason-then-Summarize&quot; architecture optimized via Group Relative Policy Optimization (GRPO), which separates detailed findings from structured diagnoses and uses a composite reward to penalize logical inconsistencies. Experimental results on the MIMIC-CXR benchmark show state-of-the-art performance in clinical metrics and a significant reduction in hallucinations compared to supervised baselines.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型在放射学报告生成中存在的与临床证据对齐不足及事实幻觉问题，提出了一种自洽的生成框架。该方法首先系统评估并选取最优的视觉编码器和语言模型骨干，然后引入基于组相对策略优化的“推理后总结”架构，将生成过程分解为详细发现和结构化疾病标签两部分，并通过多维复合奖励函数显式惩罚叙述与诊断间的逻辑不一致。在MIMIC-CXR基准上的大量实验表明，该方法在临床效能指标上达到领先水平，并相比强监督基线显著减少了幻觉现象。</div>
</details>
</div>
<div class="card">
<div class="title">Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration</div>
<div class="meta-line">Authors: Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu</div>
<div class="meta-line">First: 2026-01-12T05:43:20+00:00 · Latest: 2026-01-12T05:43:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07224v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model&#x27;s existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>巩固还是适应？PRISM：通过梯度集中度解耦监督微调与强化学习数据</div>
<div class="mono" style="margin-top:8px">尽管监督微调（SFT）与强化学习（RL）结合的混合范式已成为训练大语言模型智能体的标准方法，但两阶段间有效的数据分配机制仍鲜有研究。当前数据仲裁策略多依赖表层启发式方法，难以诊断内在学习需求。由于SFT通过模仿实现模式巩固，而RL通过探索驱动结构适应，数据与这些功能角色的错配会导致严重的优化干扰。我们提出PRISM——一个基于图式理论、具备动态感知能力的框架，该框架根据数据与模型现有知识的认知冲突程度进行仲裁。通过分析梯度的空间几何结构，PRISM将触发高空间集中度的数据识别为高冲突信号，这类数据需通过RL进行结构重组；而产生分散更新的数据则被导向SFT以实现高效巩固。在WebShop和ALFWorld上的大量实验表明，PRISM实现了帕累托改进，在超越先进混合方法的同时将计算成本降低最高达3.22倍。我们的研究结果表明，基于内部优化机制解耦数据对于实现可扩展且稳健的智能体对齐至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the lack of effective data allocation mechanisms between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in hybrid training paradigms for LLM agents, as current heuristics fail to align data with the distinct functional roles of consolidation and adaptation. The method, PRISM, introduces a dynamics-aware framework based on Schema Theory that arbitrates data by analyzing the spatial concentration of gradients, routing high-conflict data requiring structural adaptation to RL and low-conflict data suitable for consolidation to SFT. Experimental results on WebShop and ALFWorld show that PRISM achieves a Pareto improvement over state-of-the-art hybrid methods while reducing computational costs by up to 3.22 times, indicating that disentangling data based on internal optimization regimes enhances agent alignment.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，当前大型语言模型智能体的混合训练范式中，监督微调与强化学习阶段之间缺乏有效的数据分配机制，现有启发式方法无法使数据与巩固和适应的不同功能角色对齐。该方法提出了PRISM，一个基于图式理论的动态感知框架，通过分析梯度的空间集中度来仲裁数据，将需要结构重构的高冲突数据分配给强化学习，而将适合巩固的低冲突数据分配给监督微调。在WebShop和ALFWorld上的实验结果表明，PRISM实现了帕累托改进，优于最先进的混合方法，同时将计算成本降低了高达3.22倍，这表明基于内部优化机制分离数据能增强智能体的对齐效果。</div>
</details>
</div>
<div class="card">
<div class="title">ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools</div>
<div class="meta-line">Authors: Quy Minh Le, Minh Sao Khue Luu, Khanh-Tung Tran, Duc-Hai Nguyen, Hoang-Quoc-Viet Pham, Quan Le, Hoang Thanh Lam, Hoang D. Nguyen</div>
<div class="meta-line">First: 2025-09-24T16:01:05+00:00 · Latest: 2026-01-12T04:21:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.00023v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.00023v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective tool use is essential for agentic AI, yet training agents to utilize tools remains challenging due to manually designed rewards, limited training data, and poor multi-tool selection, resulting in slow adaptation, wasted computational resources, and suboptimal performance. We introduce ToolBrain, a lightweight and user-friendly framework for training tool use in agentic models with flexible reinforcement learning, thereby easing the barriers for researchers and practitioners to adapt LLM-based agents to specific domains. It supports a wide range of training strategies, including reinforcement learning algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain enables custom reward callables directly on an agent&#x27;s execution traces or simply utilizes an automated LLM-as-a-judge system for reward generation. It is packed with useful capabilities, including knowledge distillation from large to small models, automatic task generation from tool descriptions, seamless tool retrieval, efficient fine-tuning pipelines with QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate ToolBrain through an Email Search Agent case study, showing measurable improvements in tool-use skills under a realistic workflow, while keeping the codebase simple and extensible. Our framework is publicly available at https://toolbrain.org/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ToolBrain：面向智能体工具的灵活强化学习框架</div>
<div class="mono" style="margin-top:8px">高效的工具使用对智能体AI至关重要，但由于手动设计奖励、训练数据有限及多工具选择能力不足，训练智能体使用工具仍面临挑战，导致适应缓慢、计算资源浪费和性能欠佳。我们推出ToolBrain，一个轻量级、用户友好的框架，通过灵活强化学习训练智能体模型中的工具使用，从而降低研究者和实践者将基于LLM的智能体适配特定领域的门槛。该框架支持广泛的训练策略，包括GRPO和DPO等强化学习算法以及监督学习。ToolBrain支持直接基于智能体执行轨迹自定义奖励函数，或直接利用自动化LLM-as-a-judge系统生成奖励。它集成了多项实用功能，包括从大模型到小模型的知识蒸馏、基于工具描述的自动任务生成、无缝工具检索、通过Unsloth与QLoRA实现的高效微调流程，以及基于bitsandbytes的量化推理。我们通过电子邮件搜索智能体案例研究展示ToolBrain，在真实工作流中实现了工具使用技能的显著提升，同时保持代码库简洁且可扩展。本框架已在https://toolbrain.org/公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for ToolBrain stems from the challenges in training AI agents for effective tool use, such as manual reward design and poor multi-tool selection, which lead to slow adaptation and suboptimal performance. The method introduces a lightweight, flexible reinforcement learning framework that supports various training strategies, including GRPO and DPO, along with supervised learning, and features custom reward functions, automated LLM-based judgment, knowledge distillation, and efficient fine-tuning pipelines. Experimental results from an Email Search Agent case study demonstrate measurable improvements in tool-use skills within realistic workflows, while maintaining a simple and extensible codebase.</div>
<div class="mono" style="margin-top:8px">ToolBrain的提出源于训练AI智能体有效使用工具所面临的挑战，如手动设计奖励和多工具选择不佳，导致适应缓慢和性能欠佳。该方法引入了一个轻量级、灵活的强化学习框架，支持多种训练策略，包括GRPO和DPO以及监督学习，并具备自定义奖励函数、基于LLM的自动评判、知识蒸馏和高效微调流程等功能。通过电子邮件搜索智能体的案例研究，实验结果表明在真实工作流程中工具使用技能得到显著提升，同时保持了代码库的简洁性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Offline Meta-Reinforcement Learning with Flow-Based Task Inference and Adaptive Correction of Feature Overgeneralization</div>
<div class="meta-line">Authors: Min Wang, Xin Li, Mingzhong Wang, Hasnaa Bennis</div>
<div class="meta-line">First: 2026-01-12T03:16:07+00:00 · Latest: 2026-01-12T03:16:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07164v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07164v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline meta-reinforcement learning (OMRL) combines the strengths of learning from diverse datasets in offline RL with the adaptability to new tasks of meta-RL, promising safe and efficient knowledge acquisition by RL agents. However, OMRL still suffers extrapolation errors due to out-of-distribution (OOD) actions, compromised by broad task distributions and Markov Decision Process (MDP) ambiguity in meta-RL setups. Existing research indicates that the generalization of the $Q$ network affects the extrapolation error in offline RL. This paper investigates this relationship by decomposing the $Q$ value into feature and weight components, observing that while decomposition enhances adaptability and convergence in the case of high-quality data, it often leads to policy degeneration or collapse in complex tasks. We observe that decomposed $Q$ values introduce a large estimation bias when the feature encounters OOD samples, a phenomenon we term &#x27;&#x27;feature overgeneralization&#x27;&#x27;. To address this issue, we propose FLORA, which identifies OOD samples by modeling feature distributions and estimating their uncertainties. FLORA integrates a return feedback mechanism to adaptively adjust feature components. Furthermore, to learn precise task representations, FLORA explicitly models the complex task distribution using a chain of invertible transformations. We theoretically and empirically demonstrate that FLORA achieves rapid adaptation and meta-policy improvement compared to baselines across various environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于流式任务推断与特征过度泛化自适应校正的离线元强化学习</div>
<div class="mono" style="margin-top:8px">离线元强化学习（OMRL）结合了离线强化学习从多样化数据集中学习的优势与元强化学习对新任务的适应能力，有望使强化学习智能体实现安全高效的知识获取。然而，由于分布外（OOD）动作的存在，OMRL仍存在外推误差，这受到元强化学习设置中广泛的任务分布和马尔可夫决策过程（MDP）模糊性的影响。现有研究表明，$Q$网络的泛化能力会影响离线强化学习中的外推误差。本文通过将$Q$值分解为特征和权重分量来研究这一关系，发现虽然分解在高质量数据情况下能提升适应性和收敛性，但在复杂任务中常导致策略退化或崩溃。我们观察到，当特征遇到OOD样本时，分解后的$Q$值会引入较大的估计偏差，这一现象我们称之为“特征过度泛化”。为解决该问题，我们提出FLORA方法，通过建模特征分布并估计其不确定性来识别OOD样本。FLORA集成回报反馈机制以自适应调整特征分量。此外，为学习精确的任务表示，FLORA使用可逆变换链显式建模复杂任务分布。我们从理论和实验上证明，相较于基线方法，FLORA在多种环境中实现了快速适应和元策略提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of extrapolation errors in offline meta-reinforcement learning (OMRL), which arise from out-of-distribution actions and are exacerbated by feature overgeneralization when decomposing Q-values. The proposed method, FLORA, mitigates this by modeling feature distributions to identify OOD samples and using a return feedback mechanism to adaptively correct features, while also employing invertible transformations for precise task inference. Experimental results demonstrate that FLORA enables rapid adaptation and improves meta-policy performance compared to baseline methods across various environments.</div>
<div class="mono" style="margin-top:8px">本文针对离线元强化学习中因分布外动作导致的泛化误差问题，指出Q值分解会引发特征过度泛化，从而损害策略性能。所提出的FLORA方法通过建模特征分布来识别分布外样本，并利用回报反馈机制自适应调整特征组件，同时采用可逆变换链来精确学习任务表示。实验结果表明，与基线方法相比，FLORA在多种环境中能实现快速适应并提升元策略性能。</div>
</details>
</div>
<div class="card">
<div class="title">AscendKernelGen: A Systematic Study of LLM-Based Kernel Generation for Neural Processing Units</div>
<div class="meta-line">Authors: Xinzi Cao, Jianyang Zhai, Pengfei Li, Zhiheng Hu, Cen Yan, Bingxu Mu, Guanghuan Fang, Bin She, Jiayu Li, Yihan Su, Dongyang Tao, Xiansong Huang, Fan Xu, Feidiao Yang, Yao Lu, Chang-Dong Wang, Yutong Lu, Weicheng Xue, Bin Zhou, Yonghong Tian</div>
<div class="meta-line">First: 2026-01-12T03:12:58+00:00 · Latest: 2026-01-12T03:12:58+00:00</div>
<div class="meta-line">Comments: 33 pages,7 figures,16 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To meet the ever-increasing demand for computational efficiency, Neural Processing Units (NPUs) have become critical in modern AI infrastructure. However, unlocking their full potential requires developing high-performance compute kernels using vendor-specific Domain-Specific Languages (DSLs), a task that demands deep hardware expertise and is labor-intensive. While Large Language Models (LLMs) have shown promise in general code generation, they struggle with the strict constraints and scarcity of training data in the NPU domain. Our preliminary study reveals that state-of-the-art general-purpose LLMs fail to generate functional complex kernels for Ascend NPUs, yielding a near-zero success rate. To address these challenges, we propose AscendKernelGen, a generation-evaluation integrated framework for NPU kernel development. We introduce Ascend-CoT, a high-quality dataset incorporating chain-of-thought reasoning derived from real-world kernel implementations, and KernelGen-LM, a domain-adaptive model trained via supervised fine-tuning and reinforcement learning with execution feedback. Furthermore, we design NPUKernelBench, a comprehensive benchmark for assessing compilation, correctness, and performance across varying complexity levels. Experimental results demonstrate that our approach significantly bridges the gap between general LLMs and hardware-specific coding. Specifically, the compilation success rate on complex Level-2 kernels improves from 0% to 95.5% (Pass@10), while functional correctness achieves 64.3% compared to the baseline&#x27;s complete failure. These results highlight the critical role of domain-specific reasoning and rigorous evaluation in automating accelerator-aware code generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AscendKernelGen：基于大语言模型的神经处理器内核生成系统性研究</div>
<div class="mono" style="margin-top:8px">为满足日益增长的计算效率需求，神经处理器（NPU）已成为现代人工智能基础设施的关键组件。然而，要充分发挥其潜力，需要使用厂商特定的领域专用语言（DSL）开发高性能计算内核，这项任务既需要深厚的硬件专业知识，又极为耗时费力。尽管大语言模型（LLM）在通用代码生成方面展现出潜力，但在NPU领域严格的约束条件和训练数据稀缺的情况下仍面临困难。我们的初步研究表明，最先进的通用大语言模型无法为昇腾NPU生成可运行的复杂内核，成功率近乎为零。为应对这些挑战，我们提出了AscendKernelGen——一个面向NPU内核开发的生成-评估一体化框架。我们构建了Ascend-CoT高质量数据集（包含从真实内核实现中提取的思维链推理过程），并通过监督微调和基于执行反馈的强化学习训练出领域自适应模型KernelGen-LM。此外，我们设计了NPUKernelBench综合基准测试套件，用于评估不同复杂度级别下的编译成功率、功能正确性和性能表现。实验结果表明，我们的方法显著缩小了通用大语言模型与硬件专用编程之间的差距：在复杂二级内核上，编译成功率从0%提升至95.5%（Pass@10），功能正确率达到64.3%，而基线模型完全失效。这些成果凸显了领域专用推理和严格评估在加速器感知代码自动化生成中的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating high-performance compute kernels for Neural Processing Units (NPUs), which is difficult due to the need for expertise in vendor-specific Domain-Specific Languages and a lack of training data for Large Language Models (LLMs). The authors propose AscendKernelGen, a framework that integrates generation and evaluation, featuring Ascend-CoT, a dataset with chain-of-thought reasoning from real kernels, and KernelGen-LM, a model fine-tuned with supervised learning and reinforcement learning using execution feedback. Experiments show the method dramatically improves performance: on complex Level-2 kernels, the compilation success rate rises from 0% to 95.5% (Pass@10), and functional correctness reaches 64.3%, whereas baselines completely fail, demonstrating the importance of domain-specific adaptation and rigorous testing.</div>
<div class="mono" style="margin-top:8px">本文针对为神经处理单元（NPU）生成高性能计算内核的挑战，该任务因需要掌握厂商特定领域专用语言且训练数据匮乏而困难重重。作者提出了AscendKernelGen，一个集成生成与评估的框架，包含基于真实内核思维链推理的数据集Ascend-CoT，以及通过监督微调和执行反馈强化学习训练的领域自适应模型KernelGen-LM。实验结果表明，该方法显著提升了性能：在复杂的Level-2内核上，编译成功率从0%提高到95.5%（Pass@10），功能正确性达到64.3%，而基线方法完全失败，这凸显了领域特定适应和严格评估在自动化加速器感知代码生成中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Finite-Time Analysis of Simultaneous Double Q-learning</div>
<div class="meta-line">Authors: Hyunjun Na, Donghwan Lee</div>
<div class="meta-line">First: 2024-06-14T11:47:25+00:00 · Latest: 2026-01-12T02:42:33+00:00</div>
<div class="meta-line">Comments: 31 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.09946v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.09946v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">$Q$-learning is one of the most fundamental reinforcement learning (RL) algorithms. Despite its widespread success in various applications, it is prone to overestimation bias in the $Q$-learning update. To address this issue, double $Q$-learning employs two independent $Q$-estimators which are randomly selected and updated during the learning process. This paper proposes a modified double $Q$-learning, called simultaneous double $Q$-learning (SDQ), with its finite-time analysis. SDQ eliminates the need for random selection between the two $Q$-estimators, and this modification allows us to analyze double $Q$-learning through the lens of a novel switching system framework facilitating efficient finite-time analysis. Empirical studies demonstrate that SDQ converges faster than double $Q$-learning while retaining the ability to mitigate the maximization bias. Finally, we derive a finite-time expected error bound for SDQ.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同步双Q学习的有限时间分析</div>
<div class="mono" style="margin-top:8px">Q学习是最基础的强化学习算法之一，尽管在各领域应用广泛，但其Q值更新过程易产生高估偏差。为应对此问题，双Q学习采用两个独立的Q估计器，在学习过程中随机选择并更新。本文提出一种改进的双Q学习方法——同步双Q学习（SDQ），并给出其有限时间分析。SDQ无需在两个Q估计器间随机选择，这一改进使我们能通过新型切换系统框架分析双Q学习，从而实现高效的有限时间分析。实验表明SDQ在保持缓解最大化偏差能力的同时，收敛速度优于双Q学习。最后，我们推导出SDQ的有限时间期望误差界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the overestimation bias inherent in standard Q-learning by proposing simultaneous double Q-learning (SDQ), a modified algorithm that eliminates the random selection between two Q-estimators used in traditional double Q-learning. The method is analyzed through a novel switching system framework, which enables a rigorous finite-time convergence analysis. Experimental results demonstrate that SDQ not only retains the bias mitigation property of double Q-learning but also achieves faster convergence, with the paper providing a formal finite-time expected error bound to characterize its performance.</div>
<div class="mono" style="margin-top:8px">本文针对标准Q-learning中固有的高估偏差，提出了同时双Q学习（SDQ）这一改进算法，它消除了传统双Q学习中在两个Q估计器之间随机选择的需要。该方法通过一种新颖的切换系统框架进行分析，从而实现了严格的有限时间收敛性分析。实验结果表明，SDQ不仅保留了双Q学习的偏差缓解特性，而且实现了更快的收敛速度，论文还提供了正式的有限时间期望误差界来刻画其性能。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling</div>
<div class="meta-line">Authors: Zhaoyan Li, Hang Lei, Yujia Wang, Lanbo Liu, Hao Liu, Liang Yu</div>
<div class="meta-line">First: 2026-01-12T02:39:47+00:00 · Latest: 2026-01-12T02:39:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) can generate fluent text, producing high-quality creative stories remains challenging. Reinforcement Learning (RL) offers a promising solution but faces two critical obstacles: designing reliable reward signals for subjective storytelling quality and mitigating training instability. This paper introduces the Reinforcement Learning for Creative Storytelling (RLCS) framework to systematically address both challenges. First, we develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences, trained through supervised fine-tuning on demonstrations with reasoning chains distilled from strong teacher models, followed by GRPO-based refinement on expanded preference data. Second, we introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns. Experiments demonstrate that GenRM achieves 68\% alignment with human creativity judgments, and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality. This work provides a practical pipeline for applying RL to creative domains, effectively navigating the dual challenges of reward modeling and training stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励创造力：一种面向人类对齐的生成式奖励模型用于故事创作中的强化学习</div>
<div class="mono" style="margin-top:8px">尽管大语言模型能够生成流畅文本，但创作高质量创意故事仍具挑战。强化学习提供了有前景的解决方案，但面临两大关键障碍：为具有主观性的故事质量设计可靠奖励信号，以及缓解训练不稳定性。本文提出创意故事创作强化学习框架，系统性地解决这两项挑战。首先，我们开发了生成式奖励模型，该模型提供对故事偏好多维度分析和显式推理，通过在有推理链的演示数据上进行监督微调训练，随后在扩展的偏好数据上基于GRPO方法进行精炼。其次，我们引入基于熵的奖励塑形策略，动态优先学习置信度高的错误和不确定的正确预测，防止对已掌握模式的过拟合。实验表明，生成式奖励模型与人类创造力判断的对齐度达68%，且该框架在整体故事质量上显著优于包括Gemini-2.5-Pro在内的强基线模型。本工作为将强化学习应用于创意领域提供了实用流程，有效应对了奖励建模与训练稳定性的双重挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating high-quality creative stories with LLMs, where reinforcement learning is hindered by unreliable subjective reward signals and training instability. It proposes the RLCS framework, which includes a Generative Reward Model trained with supervised fine-tuning on demonstrations and refined with GRPO, alongside an entropy-based reward shaping strategy to focus learning on uncertain predictions. Experimental results show that the reward model achieves 68% alignment with human creativity judgments, and the full framework outperforms strong baselines like Gemini-2.5-Pro in overall story quality, offering a practical pipeline for RL in creative domains.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型生成高质量创意故事时面临的挑战，即强化学习中主观奖励信号不可靠和训练不稳定的问题，提出了RLCS框架。该框架包含一个生成式奖励模型，通过监督微调演示数据并使用GRPO进行精炼，同时引入基于熵的奖励塑造策略，以动态关注不确定的预测。实验结果表明，该奖励模型与人类创造力判断的吻合度达到68%，且完整框架在整体故事质量上显著优于包括Gemini-2.5-Pro在内的强基线，为创意领域的强化学习应用提供了实用流程。</div>
</details>
</div>
<div class="card">
<div class="title">Generating readily synthesizable small molecule fluorophore scaffolds with reinforcement learning</div>
<div class="meta-line">Authors: Ruhi Sayana, Kate Callon, Jennifer Xu, Jonathan Deutsch, Steven Chu, James Zou, John Janetzko, Rabindra V. Shivnaraine, Kyle Swanson</div>
<div class="meta-line">First: 2026-01-12T02:31:43+00:00 · Latest: 2026-01-12T02:31:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07145v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07145v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing new fluorophores for advanced imaging techniques requires exploring new chemical space. While generative AI approaches have shown promise in designing novel dye scaffolds, prior efforts often produced synthetically intractable candidates due to a lack of reaction constraints. Here, we developed SyntheFluor-RL, a generative AI model that employs known reaction libraries and molecular building blocks to create readily synthesizable fluorescent molecule scaffolds via reinforcement learning. To guide the generation of fluorophores, SyntheFluor-RL employs a scoring function built on multiple graph neural networks (GNNs) that predict key photophysical properties, including photoluminescence quantum yield, absorption, and emission wavelengths. These outputs are dynamically weighted and combined with a computed pi-conjugation score to prioritize candidates with desirable optical characteristics and synthetic feasibility. SyntheFluor-RL generated 11,590 candidate molecules, which were filtered to 19 structures predicted to possess dye-like properties. Of the 19 molecules, 14 were synthesized and 13 were experimentally confirmed. The top three were characterized, with the lead compound featuring a benzothiadiazole chromophore and exhibiting strong fluorescence (PLQY = 0.62), a large Stokes shift (97 nm), and a long excited-state lifetime (11.5 ns). These results demonstrate the effectiveness of SyntheFluor-RL in the identification of synthetically accessible fluorophores for further development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习生成易于合成的小分子荧光团支架</div>
<div class="mono" style="margin-top:8px">为先进成像技术开发新型荧光团需探索新的化学空间。尽管生成式人工智能方法在设计新型染料支架方面展现出潜力，但先前研究常因缺乏反应约束而产生难以合成的候选分子。本研究开发了SyntheFluor-RL——一种基于已知反应库和分子构建模块，通过强化学习生成易于合成的荧光分子支架的生成式AI模型。该模型采用基于多重图神经网络（GNN）构建的评分函数来预测关键光物理性质（包括光致发光量子产率、吸收与发射波长），通过动态加权这些输出并与计算的π共轭评分相结合，优先筛选具有理想光学特性与合成可行性的候选分子。SyntheFluor-RL共生成11,590个候选分子，经筛选获得19个预测具有类染料特性的结构。其中14个分子被成功合成，13个获得实验验证。对前三名分子进行表征显示，先导化合物以苯并噻二唑为发色团，具备强荧光性（PLQY=0.62）、大斯托克斯位移（97 nm）及长激发态寿命（11.5 ns）。这些结果证明了SyntheFluor-RL在识别可合成荧光团以推进后续开发方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to explore new chemical space for advanced imaging fluorophores while overcoming the synthetic intractability of prior AI-generated designs, this study introduces SyntheFluor-RL, a generative AI model that uses reinforcement learning guided by known reaction libraries and building blocks to create readily synthesizable fluorescent scaffolds. The method employs a scoring function based on graph neural networks to predict key photophysical properties like quantum yield and emission wavelengths, dynamically weighted with a pi-conjugation score to prioritize optically desirable and synthetically feasible candidates. Experimentally, the model generated 11,590 candidates, filtered to 19 predicted dye-like structures; of 14 synthesized, 13 were confirmed, with the top compound featuring a benzothiadiazole chromophore showing strong fluorescence (PLQY=0.62), a large Stokes shift (97 nm), and a long excited-state lifetime (11.5 ns), demonstrating the model&#x27;s effectiveness in identifying accessible fluorophores for development.</div>
<div class="mono" style="margin-top:8px">本研究旨在为先进成像技术探索新的荧光团化学空间，同时克服先前AI设计合成难度高的问题，提出了SyntheFluor-RL生成模型，该模型通过强化学习结合已知反应库和分子构建块来创建易于合成的荧光分子骨架。方法上采用基于图神经网络的评分函数预测光物理性质如量子产率和发射波长，并动态加权π共轭评分以优先选择光学性能优异且合成可行的候选分子。实验结果表明，模型生成了11,590个候选分子，筛选出19个预测具染料特性的结构；其中14个被合成，13个得到实验验证，领先化合物以苯并噻二唑为发色团，表现出强荧光（PLQY=0.62）、大斯托克斯位移（97 nm）和长激发态寿命（11.5 ns），证明了该模型在识别可合成荧光团方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ENTRA: Entropy-Based Redundancy Avoidance in Large Language Model Reasoning</div>
<div class="meta-line">Authors: Ruichu Cai, Haopeng Du, Qingwen Lin, Yutong Chen, Zijian Li, Boyan Xu</div>
<div class="meta-line">First: 2026-01-12T01:26:30+00:00 · Latest: 2026-01-12T01:26:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07123v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07123v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) often suffer from overthinking, generating unnecessarily long reasoning chains even for simple tasks. This leads to substantial computational overhead with limited performance gain, primarily due to redundant verification and repetitive generation. While prior work typically constrains output length or optimizes correctness, such coarse supervision fails to guide models toward concise yet accurate inference. In this paper, we propose ENTRA, an entropy-based training framework that suppresses redundant reasoning while preserving performance. ENTRA first estimates the token-level importance using a lightweight Bidirectional Importance Estimation (BIE) method, which accounts for both prediction confidence and forward influence. It then computes a redundancy reward based on the entropy of low-importance tokens, normalized by its theoretical upper bound, and optimizes this reward via reinforcement learning. Experiments on mathematical reasoning benchmarks demonstrate that ENTRA reduces output length by 37% to 53% with no loss-and in some cases, gains-in accuracy. Our approach offers a principled and efficient solution to reduce overthinking in LRMs, and provides a generalizable path toward redundancy-aware reasoning optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ENTRA：基于熵的大语言模型推理冗余规避方法</div>
<div class="mono" style="margin-top:8px">大型推理模型常因过度思考而产生冗长推理链，即使处理简单任务也会出现冗余验证与重复生成，导致计算开销显著而性能提升有限。现有方法通常通过约束输出长度或优化正确性进行粗粒度监督，难以引导模型实现简洁而精确的推理。本文提出ENTRA——一种基于熵的训练框架，在保持性能的同时抑制冗余推理。该框架首先通过轻量级双向重要性评估方法，结合预测置信度与前向影响力进行词元级重要性估计；随后基于低重要性词元的熵值计算冗余奖励（经理论上界归一化），并通过强化学习优化该奖励。数学推理基准实验表明，ENTRA能在保持（部分任务甚至提升）准确率的前提下，将输出长度缩减37%至53%。该方法为减少大型推理模型的过度思考提供了原则性高效解决方案，并为冗余感知的推理优化开辟了可推广的技术路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of overthinking in Large Reasoning Models, where they produce excessively long reasoning chains for simple tasks, leading to computational inefficiency without significant performance benefits. To mitigate this, the authors propose ENTRA, an entropy-based training framework that uses a lightweight Bidirectional Importance Estimation method to assess token-level importance and computes a redundancy reward based on the entropy of low-importance tokens, optimized via reinforcement learning. Experimental results on mathematical reasoning benchmarks show that ENTRA reduces output length by 37% to 53% while maintaining or even improving accuracy, offering a principled solution to reduce redundancy in model reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型中的过度思考问题展开研究，该问题导致模型在处理简单任务时生成过长的推理链，造成计算资源浪费且性能提升有限。为解决此问题，作者提出了ENTRA，一种基于熵的训练框架，采用轻量级双向重要性估计方法评估令牌级重要性，并基于低重要性令牌的熵计算冗余奖励，通过强化学习进行优化。在数学推理基准测试上的实验结果表明，ENTRA能将输出长度减少37%至53%，同时保持甚至提升准确性，为减少模型推理冗余提供了一种原则性高效方案。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework</div>
<div class="meta-line">Authors: Yixiao Peng, Hao Hu, Feiyang Li, Xinye Cao, Yingchang Jiang, Jipeng Tang, Guoshun Nan, Yuling Liu</div>
<div class="meta-line">First: 2026-01-12T01:25:41+00:00 · Latest: 2026-01-12T01:25:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07122v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07122v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&amp;CK&#x27;s Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于鲁棒性LLM赋能的层次化多智能体强化学习框架增强云网络韧性</div>
<div class="mono" style="margin-top:8px">虚拟化与资源池化技术虽赋予云网络结构灵活性与弹性扩展能力，却也扩大了攻击面并挑战网络韧性。基于强化学习的防御策略通过优化对抗环境下的资源部署与隔离策略，致力于维持并恢复网络可用性以增强系统韧性。然而，现有方法缺乏鲁棒性，需重新训练以适应网络结构、节点规模、攻击策略与强度的动态变化，且缺乏人机协同支持限制了可解释性与灵活性。为此，我们提出CyberOps-Bots——一个由大语言模型赋能的层次化多智能体强化学习框架。该框架受MITRE ATT&amp;CK战术-技术模型启发，采用双层架构：（1）上层LLM智能体通过反应式规划、基于IPDRR的感知、长短时记忆及行动/工具集成四大模块，实现全局态势感知、人类意图识别与战术规划；（2）下层RL智能体通过异构分离预训练开发，在局部网络区域执行原子化防御动作。该协同机制在保持LLM适应性与可解释性的同时，确保RL执行的可靠性。真实云数据集实验表明，相较于前沿算法，CyberOps-Bots在网络可用性上保持68.5%的优势，且在场景切换无需重训练时实现34.7%的跳跃式性能增益。据我们所知，这是首个构建具备人机协同支持的鲁棒性LLM-RL框架用于云防御的研究。我们将向社区开源该框架，以推动云网络鲁棒自主防御的进步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of robustness and interpretability in existing Reinforcement Learning (RL)-based cloud network defense systems, which struggle to adapt to dynamic network and attack changes without retraining. The authors propose CyberOps-Bots, a hierarchical multi-agent framework that integrates a Large Language Model (LLM) for global planning and human intent recognition with lower-level RL agents for executing localized defense actions, inspired by the MITRE ATT&amp;CK model. Experimental results on real cloud datasets demonstrate that the framework maintains network availability 68.5% higher than state-of-the-art methods and achieves a 34.7% jumpstart performance gain in new scenarios without requiring retraining, establishing a robust LLM-RL approach with human-in-the-loop support for cloud defense.</div>
<div class="mono" style="margin-top:8px">本文针对现有基于强化学习的云网络防御系统缺乏鲁棒性和可解释性、难以适应动态网络与攻击变化而需重新训练的问题，提出了一种名为CyberOps-Bots的分层多智能体框架。该框架受MITRE ATT&amp;CK模型启发，集成大型语言模型进行全局规划与人类意图识别，并利用下层强化学习智能体执行局部防御动作。在真实云数据集上的实验表明，该框架在不重新训练的情况下，比现有先进算法将网络可用性提高了68.5%，并在场景切换时实现了34.7%的启动性能提升，从而建立了一种具备人机交互支持的鲁棒LLM-RL云防御方法。</div>
</details>
</div>
<div class="card">
<div class="title">Reward-Preserving Attacks For Robust Reinforcement Learning</div>
<div class="meta-line">Authors: Lucas Schott, Elies Gherbi, Hatem Hajri, Sylvain Lamprier</div>
<div class="meta-line">First: 2026-01-12T01:14:03+00:00 · Latest: 2026-01-12T01:14:03+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures, 4 algorithms, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07118v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial robustness in RL is difficult because perturbations affect entire trajectories: strong attacks can break learning, while weak attacks yield little robustness, and the appropriate strength varies by state. We propose $α$-reward-preserving attacks, which adapt the strength of the adversary so that an $α$ fraction of the nominal-to-worst-case return gap remains achievable at each state. In deep RL, we use a gradient-based attack direction and learn a state-dependent magnitude $η\le η_{\mathcal B}$ selected via a critic $Q^π_α((s,a),η)$ trained off-policy over diverse radii. This adaptive tuning calibrates attack strength and, with intermediate $α$, improves robustness across radii while preserving nominal performance, outperforming fixed- and random-radius baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向鲁棒强化学习的奖励保持型攻击</div>
<div class="mono" style="margin-top:8px">强化学习的对抗鲁棒性研究面临挑战，因为扰动会影响完整轨迹：强攻击会破坏学习过程，弱攻击则难以提升鲁棒性，且适宜的攻击强度随状态变化。本文提出α-奖励保持型攻击，通过动态调整对抗强度，使每个状态下仍能保持名义回报与最差情形回报差距的α比例。在深度强化学习中，我们采用基于梯度的攻击方向，并通过离线训练、覆盖多半径范围的评判器Q^π_α((s,a),η)学习状态相关的强度参数η≤η_ℬ。这种自适应调节机制能校准攻击强度，在中间值α设定下，可在保持名义性能的同时提升多半径范围内的鲁棒性，其效果优于固定半径与随机半径基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of adversarial robustness in reinforcement learning, where perturbations impact entire trajectories and finding an appropriate attack strength is state-dependent. The authors propose α-reward-preserving attacks, which adapt adversary strength to preserve a fraction α of the nominal-to-worst-case return gap per state. Their method employs a gradient-based attack direction with a state-dependent magnitude learned via an off-policy critic trained over diverse radii, enabling adaptive tuning. Experimental results show that with intermediate α values, this approach improves robustness across various radii while maintaining nominal performance, outperforming fixed- and random-radius baselines.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的对抗鲁棒性挑战，其中扰动影响整个轨迹，且合适的攻击强度随状态变化。作者提出α-奖励保持攻击，通过调整对手强度以在每个状态保留名义与最坏情况回报差距的α比例。该方法采用基于梯度的攻击方向，并通过在不同半径上离线训练的评论家学习状态相关的幅度，实现自适应调整。实验结果表明，在中间α值下，该方法在保持名义性能的同时提高了跨半径的鲁棒性，优于固定半径和随机半径的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions</div>
<div class="meta-line">Authors: Kehan Long, Jorge Cortés, Nikolay Atanasov</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-16T07:36:40+00:00 · Latest: 2026-01-12T01:08:26+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.10947v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.10947v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Establishing stability certificates for closed-loop systems under reinforcement learning (RL) policies is essential to move beyond empirical performance and offer guarantees of system behavior. Classical Lyapunov methods require a strict stepwise decrease in the Lyapunov function but such certificates are difficult to construct for learned policies. The RL value function is a natural candidate but it is not well understood how it can be adapted for this purpose. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用广义李雅普诺夫函数验证强化学习策略的稳定性</div>
<div class="mono" style="margin-top:8px">为强化学习策略下的闭环系统建立稳定性证明，是超越经验性能、提供系统行为保障的关键。经典李雅普诺夫方法要求李雅普诺夫函数严格逐步递减，但此类证明难以针对学习策略构建。强化学习价值函数是自然候选，但其如何适配此目的尚不明确。为获得直观理解，我们首先研究线性二次调节器问题，并得出两个关键观察：其一，通过引入与系统动态及阶段成本相关的残差项，可从LQR策略的价值函数导出李雅普诺夫函数；其二，经典李雅普诺夫递减要求可放宽为广义李雅普诺夫条件，仅需在多时间步长上平均递减。基于此，我们考虑非线性场景，提出通过为强化学习价值函数添加神经网络残差项来学习广义李雅普诺夫函数的方法。该方法成功验证了在Gymnasium和DeepMind Control基准上训练的强化学习策略的稳定性。我们还将方法扩展至联合训练神经控制器与稳定性证明，采用多步李雅普诺夫损失，相比经典方法获得了更大的吸引域内近似认证范围。总体而言，本框架通过简化证明构建，实现了对广泛学习策略系统的稳定性认证，从而连接了经典控制理论与现代学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to provide stability guarantees for systems controlled by reinforcement learning policies beyond empirical performance, this paper introduces a method to certify stability using generalized Lyapunov functions. The approach is inspired by the linear quadratic regulator case, where the value function can be augmented with a residual term, and it relaxes the classical strict decrease condition to an average decrease over multiple steps; for nonlinear settings, the method learns these generalized Lyapunov functions by augmenting RL value functions with neural network residuals. Experimental results on Gymnasium and DeepMind Control benchmarks show successful stability certification for trained RL policies, and an extension jointly training controllers and certificates yields larger certified regions of attraction compared to classical Lyapunov methods, thereby bridging control theory and learning-based approaches.</div>
<div class="mono" style="margin-top:8px">本文的动机是为强化学习策略控制的系统提供超越经验性能的稳定性保证，提出了一种使用广义李雅普诺夫函数进行稳定性认证的方法。该方法受线性二次调节器问题的启发，通过将价值函数与残差项结合构建李雅普诺夫函数，并将经典的李雅普诺夫严格递减条件放宽为多步平均递减；针对非线性场景，该方法通过用神经网络残差增强强化学习价值函数来学习广义李雅普诺夫函数。在Gymnasium和DeepMind Control基准测试上的实验结果表明，该方法成功认证了已训练强化学习策略的稳定性，且扩展的联合训练控制器与证书的方法相比经典李雅普诺夫方法获得了更大的认证吸引域内近似，从而连接了经典控制理论与现代学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning</div>
<div class="meta-line">Authors: Meng Lu, Yuxing Lu, Yuchen Zhuang, Megan Mullins, Yang Xie, Guanghua Xiao, Charles Fleming, Wenqi Shi, Xuan Wang</div>
<div class="meta-line">First: 2026-01-12T00:11:10+00:00 · Latest: 2026-01-12T00:11:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MEDVISTAGYM：基于工具集成强化学习的医学图像思维训练可扩展环境</div>
<div class="mono" style="margin-top:8px">视觉语言模型在通用图像理解上表现优异，但在医学图像思维方面存在局限，尤其难以通过迭代视觉交互进行多步推理。现有医学视觉语言模型多依赖静态视觉嵌入与单次推理，无法在推理过程中重新审视、验证或优化视觉证据。虽然工具集成推理展现出潜力，但开源视觉语言模型缺乏训练基础设施来学习多模态医学推理中的工具选择、调用与协调。我们提出MedVistaGym——一个可扩展的交互式训练环境，通过激励工具集成的视觉推理促进医学图像分析。该环境使视觉语言模型能够决策工具调用时机与类型、定位任务相关图像区域，并在统一可执行界面中将单/多子图像证据整合至交错式多模态推理中，支持智能体训练。基于MedVistaGym，我们通过轨迹采样与端到端强化学习训练出MedVistaGym-R1模型，实现工具使用与智能推理的交错执行。在六项医学视觉问答基准测试中，MedVistaGym-R1-8B模型以19.10%至24.21%的优势超越同规模工具增强基线，证明结构化智能体训练（而非单纯工具调用）能有效解锁医学图像分析中的工具集成推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that while vision language models (VLMs) excel at general image understanding, they struggle with multi-step, iterative reasoning on medical images, as they typically use static visual embeddings and single-pass inference without the ability to re-examine evidence. To address this, the authors introduce MedVistaGym, a scalable training environment that enables VLMs to learn tool-integrated reasoning through reinforcement learning, teaching models to decide when and which tools to invoke, localize relevant image regions, and integrate sub-image evidence. The main experimental result is that their trained model, MedVistaGym-R1-8B, outperforms comparably sized tool-augmented baselines by 19.10% to 24.21% across six medical VQA benchmarks, demonstrating that structured agentic training is key to unlocking effective tool use in medical image analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管视觉语言模型在通用图像理解上表现优异，但在处理医学图像时难以进行多步迭代推理，因为它们通常依赖静态视觉嵌入和单次推理，无法重新审视或细化视觉证据。为此，作者提出了MedVistaGym，一个可扩展的训练环境，通过强化学习让模型学习工具集成推理，使其能够决定何时调用何种工具、定位相关图像区域并整合子图像证据。主要实验结果表明，训练出的模型MedVistaGym-R1-8B在六个医学视觉问答基准测试中，比同等规模工具增强基线模型性能提升19.10%至24.21%，证明结构化智能体训练而非单纯工具访问是实现医学图像分析中有效工具集成推理的关键。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Bayesian Optimization via Tempered Posteriors</div>
<div class="meta-line">Authors: Jiguang Li, Hengrui Luo</div>
<div class="meta-line">First: 2026-01-11T23:34:24+00:00 · Latest: 2026-01-11T23:34:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07094v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07094v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) iteratively fits a Gaussian process (GP) surrogate to accumulated evaluations and selects new queries via an acquisition function such as expected improvement (EI). In practice, BO often concentrates evaluations near the current incumbent, causing the surrogate to become overconfident and to understate predictive uncertainty in the region guiding subsequent decisions. We develop a robust GP-based BO via tempered posterior updates, which downweight the likelihood by a power $α\in (0,1]$ to mitigate overconfidence under local misspecification. We establish cumulative regret bounds for tempered BO under a family of generalized improvement rules, including EI, and show that tempering yields strictly sharper worst-case regret guarantees than the standard posterior $(α=1)$, with the most favorable guarantees occurring near the classical EI choice.
  Motivated by our theoretic findings, we propose a prequential procedure for selecting $α$ online: it decreases $α$ when realized prediction errors exceed model-implied uncertainty and returns $α$ toward one as calibration improves. Empirical results demonstrate that tempering provides a practical yet theoretically grounded tool for stabilizing BO surrogates under localized sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于退火后验的鲁棒贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）通过迭代拟合高斯过程（GP）代理模型，并借助期望改进（EI）等采集函数选择新查询点。实践中，BO常使评估点聚集在当前最优解附近，导致代理模型过度自信，并低估指导后续决策区域的预测不确定性。本文提出一种基于退火后验更新的鲁棒GP-BO方法，通过幂次α∈(0,1]对似然函数进行降权，以缓解局部设定偏误下的过度自信问题。我们在包含EI在内的广义改进规则族下建立了退火BO的累积遗憾界，证明退火处理相比标准后验（α=1）能获得严格更优的最坏情况遗憾保证，且最优保证出现在经典EI选择附近。基于理论发现，我们提出在线选择α的序贯程序：当实际预测误差超过模型隐含的不确定性时降低α值，在校准改善时使α趋近于1。实证结果表明，退火处理为局部采样下的BO代理模型稳定提供了兼具理论依据与实践价值的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of overconfidence in Gaussian process surrogates used in Bayesian optimization, which arises when evaluations cluster near the current best point, leading to underestimated predictive uncertainty and poor decision-making. To mitigate this, the authors introduce a robust Bayesian optimization method that employs tempered posterior updates, downweighting the likelihood by a factor α in (0,1] to reduce overconfidence under local model misspecification. Theoretically, they establish cumulative regret bounds for tempered BO under generalized improvement rules, showing that tempering yields sharper worst-case guarantees than standard methods, especially near the expected improvement choice. Empirically, they propose an online prequential procedure to adapt α based on prediction errors, and experimental results demonstrate that tempering effectively stabilizes BO surrogates in scenarios with localized sampling.</div>
<div class="mono" style="margin-top:8px">本文针对贝叶斯优化中高斯过程代理模型因评估点聚集于当前最优解附近而导致的过度自信问题，该问题会低估预测不确定性并影响决策质量。为解决此问题，作者提出了一种基于回火后验更新的鲁棒贝叶斯优化方法，通过引入参数α∈(0,1]对似然函数进行降权，以缓解局部模型误设下的过度自信。理论分析表明，该方法在广义改进规则下建立了累积遗憾界，证明回火处理比标准方法具有更优的最坏情况遗憾保证，尤其在经典期望改进规则附近效果显著。实验方面，作者设计了一种在线预检验程序来自适应调整α，依据预测误差与模型不确定性的偏差进行调整，实证结果验证了回火方法在局部采样场景下能有效稳定贝叶斯优化代理模型。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
