<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-17 03:37</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260117_0337</div>
    <div class="row"><div class="card">
<div class="title">MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching</div>
<div class="meta-line">Authors: Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin</div>
<div class="meta-line">First: 2026-01-15T18:59:23+00:00 · Latest: 2026-01-15T18:59:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10712v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10712v1">PDF</a> · <a href="https://github.com/quchangle1/MatchTIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MatchTIR：基于二分图匹配的细粒度监督工具集成推理</div>
<div class="mono" style="margin-top:8px">工具集成推理（TIR）通过将推理步骤与外部工具交互交织，赋能大语言模型处理复杂任务。然而，现有强化学习方法通常依赖结果级或轨迹级奖励，对轨迹内所有步骤分配统一的优势值。这种粗粒度的信用分配无法区分有效工具调用与冗余或错误调用，尤其在长视野多轮次场景中。为此，我们提出MatchTIR框架，通过基于二分图匹配的轮次级奖励分配与双层级优势估计引入细粒度监督。具体而言，我们将信用分配建模为预测轨迹与真实轨迹间的二分图匹配问题，采用两种分配策略生成密集的轮次级奖励。此外，为平衡局部步骤精度与全局任务成功率，我们引入融合轮次级与轨迹级信号的双层级优势估计方案，为每个交互轮次分配差异化优势值。在三个基准上的大量实验证明了MatchTIR的优越性。值得注意的是，我们的40亿参数模型在多数任务中超越了80亿参数竞品，尤其在长视野与多轮次任务中表现突出。代码已开源：https://github.com/quchangle1/MatchTIR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind MatchTIR is to address the coarse-grained credit assignment in existing reinforcement learning methods for Tool-Integrated Reasoning (TIR), which fail to distinguish effective tool calls from redundant or erroneous ones in long-horizon tasks. The method introduces fine-grained supervision via bipartite matching to formulate credit assignment as a matching problem between predicted and ground-truth traces, deriving dense turn-level rewards, and employs a dual-level advantage estimation scheme to balance local step precision with global task success. Experimental results on three benchmarks show that MatchTIR outperforms existing approaches, with a 4B model surpassing most 8B competitors, especially in long-horizon and multi-turn tasks.</div>
<div class="mono" style="margin-top:8px">MatchTIR的动机是解决现有工具集成推理（TIR）强化学习方法中粗粒度的信用分配问题，这些方法在长视野任务中无法区分有效工具调用与冗余或错误调用。该方法通过二分图匹配引入细粒度监督，将信用分配建模为预测轨迹与真实轨迹之间的匹配问题，以生成密集的回合级奖励，并采用双层优势估计方案来平衡局部步骤精度与全局任务成功。在三个基准测试上的实验结果表明，MatchTIR优于现有方法，其4B模型超越了大多数8B竞争对手，尤其在长视野和多回合任务中表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization</div>
<div class="meta-line">Authors: Minh Hieu Ha, Hung Phan, Tung Duy Doan, Tung Dao, Dao Tran, Huynh Thi Thanh Binh</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-07-28T15:26:43+00:00 · Latest: 2026-01-15T18:28:50+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20923v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.20923v3">PDF</a> · <a href="https://github.com/langkhachhoha/MPaGE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于帕累托网格引导的大语言模型实现多目标组合优化中快速高质量的启发式设计</div>
<div class="mono" style="margin-top:8px">多目标组合优化问题在实际应用中频繁出现，需要同时优化相互冲突的目标。传统进化算法虽有效，但通常依赖领域知识和重复参数调优，在处理未知问题实例时灵活性受限。近期，大语言模型与进化计算的融合为自动启发式生成开辟了新途径，利用其高级语言理解和代码生成能力。然而，现有方法多聚焦单目标任务，常忽视多目标场景下的运行效率和启发式多样性等关键因素。为填补这一空白，我们提出MPaGE方法——通过帕累托网格引导的LLM进化实现MOCOP多启发式生成。该方法作为简单进化多目标优化框架的增强版本，结合LLM与帕累托前沿网格技术，通过将目标空间网格化并保留最优候选方案来引导启发式生成。MPaGE利用LLM在变异过程中优先选择语义逻辑结构相异的启发式，从而提升种群多样性并减少冗余。大量实验表明，MPaGE性能优于现有基于LLM的框架，在运行速度显著提升的同时，与传统多目标进化算法获得相当的结果。代码已开源：https://github.com/langkhachhoha/MPaGE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of traditional evolutionary algorithms in multi-objective combinatorial optimization (MOCOP), which often require extensive domain expertise and parameter tuning, by integrating Large Language Models (LLMs) to automate heuristic design. The proposed method, MPaGE, enhances the Simple Evolutionary Multiobjective Optimization (SEMO) framework with a Pareto Front Grid technique to partition the objective space, guiding LLMs to generate diverse and high-quality heuristics by retaining top-performing candidates and prioritizing semantically distinct logical structures. Experimental results show that MPaGE outperforms existing LLM-based frameworks, achieves competitive performance with traditional multi-objective evolutionary algorithms, and operates with significantly faster runtime.</div>
<div class="mono" style="margin-top:8px">本文针对多目标组合优化问题中传统进化算法依赖领域知识和参数调优、灵活性不足的局限，提出利用大语言模型自动设计启发式方法。所提出的MPaGE方法在简单进化多目标优化框架基础上，引入帕累托前沿网格技术划分目标空间，通过保留高性能候选解引导大语言模型生成逻辑结构语义多样化的启发式规则，以提升种群多样性并减少冗余。实验结果表明，MPaGE在性能上优于现有基于大语言模型的框架，与传统多目标进化算法竞争力相当，且运行速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Uncertainty Guided Policy Learning for Multimodal Reasoning</div>
<div class="meta-line">Authors: Rui Liu, Dian Yu, Tong Zheng, Runpeng Dai, Zongxia Li, Wenhao Yu, Zhenwen Liang, Linfeng Song, Haitao Mi, Pratap Tokekar, Dong Yu</div>
<div class="meta-line">First: 2025-10-01T20:32:08+00:00 · Latest: 2026-01-15T17:51:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01444v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has advanced reasoning capabilities in multimodal large language models. However, existing methods typically treat visual inputs as deterministic, overlooking the perceptual ambiguity inherent to the visual modality. Consequently, they fail to distinguish whether a model&#x27;s uncertainty stems from complex reasoning or ambiguous perception, preventing the targeted allocation of exploration or learning signals. To address this gap, we introduce DUPL, a dual-uncertainty guided policy learning approach for multimodal RLVR that quantifies and leverages both perceptual uncertainty (via symmetric KL divergence) and output uncertainty (via policy entropy) to guide policy updates. By establishing an uncertainty-driven feedback loop and employing a dynamic branch prioritization mechanism, DUPL recalibrates the policy advantage to focus learning on states with high perceptual or decisional ambiguity, enabling effective targeted exploration beyond passive data augmentation. Implemented on top of GRPO and evaluated on six multimodal mathematical and general-domain reasoning benchmarks, DUPL improves Qwen2.5-VL 3B and 7B models, achieving accuracy gains of up to 11.2% on visual math tasks and up to 7.1% on general-domain reasoning tasks, while consistently outperforming GRPO. These results demonstrate that dual-uncertainty guided policy learning is an effective and generalizable approach for multimodal RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双不确定性引导的多模态推理策略学习</div>
<div class="mono" style="margin-top:8px">基于可验证奖励的强化学习（RLVR）提升了多模态大语言模型的推理能力。然而，现有方法通常将视觉输入视为确定性信息，忽略了视觉模态固有的感知模糊性，导致无法区分模型的不确定性是源于复杂推理还是模糊感知，从而难以针对性地分配探索或学习信号。为解决这一问题，我们提出了DUPL——一种双不确定性引导的多模态RLVR策略学习方法，通过量化并利用感知不确定性（基于对称KL散度）与输出不确定性（基于策略熵）来指导策略更新。通过建立不确定性驱动的反馈循环并采用动态分支优先级机制，DUPL重新校准策略优势，将学习聚焦于具有高感知或决策模糊性的状态，实现了超越被动数据增强的有效定向探索。在GRPO框架上实现并在六个多模态数学及通用领域推理基准测试中评估，DUPL显著提升了Qwen2.5-VL 3B和7B模型的性能，在视觉数学任务上准确率最高提升11.2%，在通用领域推理任务上最高提升7.1%，且持续优于GRPO。这些结果表明，双不确定性引导的策略学习是一种有效且可泛化的多模态RLVR方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the oversight of perceptual ambiguity in existing multimodal reinforcement learning with verifiable rewards (RLVR) methods, which treat visual inputs as deterministic and conflate reasoning and perceptual uncertainty, this paper introduces DUPL, a dual-uncertainty guided policy learning approach. The method quantifies perceptual uncertainty via symmetric KL divergence and output uncertainty via policy entropy, using them to establish an uncertainty-driven feedback loop and a dynamic branch prioritization mechanism that recalibrates the policy advantage to focus learning on ambiguous states. Experimental results on six multimodal reasoning benchmarks show that DUPL, implemented on GRPO, improves Qwen2.5-VL models, achieving accuracy gains of up to 11.2% on visual math tasks and up to 7.1% on general-domain tasks, consistently outperforming the baseline.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有基于可验证奖励的多模态强化学习方法通常将视觉输入视为确定性信息，忽略了其固有的感知模糊性，从而无法区分模型的不确定性是源于复杂推理还是模糊感知。为此，作者提出了DUPL方法，该方法通过对称KL散度量化感知不确定性，通过策略熵量化输出不确定性，并利用这些不确定性建立反馈循环和动态分支优先级机制，以重新校准策略优势，将学习重点集中在高模糊性状态上。在六个多模态数学和通用领域推理基准上的实验表明，基于GRPO实现的DUPL提升了Qwen2.5-VL 3B和7B模型的性能，在视觉数学任务上准确率最高提升11.2%，在通用推理任务上最高提升7.1%，且持续优于GRPO基线。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yucheng Wang, Yufei He, Jiaying Wu, Yilun Zhao, See-Kiong Ng, Cynthia Breazeal, Anh Tuan Luu, Hae Won Park, Bryan Hooi</div>
<div class="meta-line">First: 2026-01-13T17:48:43+00:00 · Latest: 2026-01-15T17:24:46+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08763v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08763v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励罕见策略：面向大语言模型创造性问题求解的独特性感知强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为大语言模型后训练的核心范式，尤其在复杂推理任务中，但其常面临探索坍缩问题：策略过早集中于少数主导推理模式，虽提升pass@1指标，却限制了轨迹级多样性及pass@k增益。本文指出该问题源于对局部词元行为的正则化而非解决方案集的多样性调控。为此，我们提出独特性感知强化学习——一种轨迹级优化目标，显式奖励采用罕见高层策略的正确解法。该方法基于大语言模型的评判器，将同一问题的求解轨迹按高层策略聚类（忽略表面差异），并依据聚类规模对策略优势进行逆向加权。由此，正确且新颖的策略将比冗余策略获得更高奖励。在数学、物理和医学推理基准测试中，本方法在大规模采样预算下持续提升pass@$k$指标，在保持pass@1性能的同时显著提高pass@$k$曲线下面积，并能维持探索性、大规模发掘更多样化的求解策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the exploration collapse in reinforcement learning (RL) for large language models (LLMs), where policies prematurely converge on a narrow set of reasoning patterns, limiting solution diversity and gains in pass@k metrics. The method introduces Uniqueness-Aware Reinforcement Learning, which explicitly rewards correct solutions that employ rare high-level strategies by using an LLM-based judge to cluster rollouts by strategy and reweighting policy advantages inversely with cluster size. Experimental results across mathematics, physics, and medical reasoning benchmarks show that this approach consistently improves pass@k and the area under the pass@k curve without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型强化学习中的探索崩溃问题，即策略过早集中于少数主导推理模式，限制了解决方案多样性和pass@k指标的提升。方法上提出了独特性感知强化学习，通过基于LLM的评判器将同一问题的解决方案按高层策略聚类，并依据聚类大小反向加权策略优势，从而对使用罕见策略的正确解给予更高奖励。在数学、物理和医学推理基准测试中，该方法在不牺牲pass@1的前提下，持续提升了pass@k和pass@k曲线下面积，同时保持了探索性并大规模发现了更多样化的解决策略。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park</div>
<div class="meta-line">First: 2026-01-14T17:57:43+00:00 · Latest: 2026-01-15T17:20:36+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09667v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09667v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作式多智能体测试时强化学习推理框架</div>
<div class="mono" style="margin-top:8px">多智能体系统已发展为适用于多种应用的实用LLM驱动协作体，其通过多样性与交叉验证获得鲁棒性。然而多智能体强化学习训练存在资源密集与不稳定的问题：智能体间的协同适应会引发非平稳性，且奖励信号通常稀疏且高方差。为此，我们提出\textbf{多智能体测试时强化学习框架}，该框架在推理阶段将结构化文本经验注入多智能体决策过程。该框架构建面向多轮讨论的专家团队，检索并整合测试时经验，最终通过共识机制完成决策。我们还研究了用于构建轮次级经验池的信用分配机制，并将其重新注入对话流程。在医学、数学与教育领域的挑战性基准测试中，该框架相比多智能体基线平均准确率提升3.67%，相比单智能体基线提升8.67%。消融实验检验了不同信用分配方案，并详细比较了其对训练结果的影响。该框架为无需调参的分布偏移鲁棒性多智能体推理提供了稳定、高效且可靠的实现路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the resource-intensive and unstable nature of multi-agent reinforcement learning (MARL) training, which suffers from non-stationarity and sparse rewards, this work introduces Multi-Agent Test-Time Reinforcement Learning (MATTRL). The method injects structured textual experience into multi-agent deliberation at inference time by forming a team of specialists, retrieving and integrating test-time experiences, and reaching consensus for decisions, while also studying credit assignment for constructing a turn-level experience pool. Experimental results across benchmarks in medicine, math, and education show that MATTRL improves accuracy by an average of 3.67% over a multi-agent baseline and 8.67% over single-agent baselines, with ablation studies analyzing different credit-assignment schemes.</div>
<div class="mono" style="margin-top:8px">针对多智能体强化学习训练资源消耗大、不稳定且存在非平稳性和稀疏奖励的问题，本研究提出了多智能体测试时强化学习框架。该方法在推理时通过组建专家团队、检索并整合测试时经验、达成共识进行决策，将结构化文本经验注入多智能体审议过程，并研究了用于构建轮次级经验池的信用分配机制。在医学、数学和教育等领域的基准测试中，该框架相比多智能体基线平均准确率提升3.67%，相比单智能体基线提升8.67%，消融实验还分析了不同信用分配方案对训练结果的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Combinatorial Optimization Augmented Machine Learning</div>
<div class="meta-line">Authors: Maximilian Schiffer, Heiko Hoppe, Yue Su, Louis Bouvier, Axel Parmentier</div>
<div class="meta-line">First: 2026-01-15T16:55:19+00:00 · Latest: 2026-01-15T16:55:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10583v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10583v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>组合优化增强机器学习</div>
<div class="mono" style="margin-top:8px">组合优化增强机器学习（COAML）作为一种将预测模型与组合决策相结合的新兴范式，通过将组合优化求解器嵌入学习流程，构建出既数据驱动又保持可行性的策略，从而连接了机器学习、运筹学与随机优化的传统领域。本文系统综述了COAML领域的最新进展：提出统一的COAML流程框架，阐述其方法论构成模块，并形式化其与经验成本最小化的关联；基于不确定性与决策结构建立问题分类体系，依此梳理静态与动态问题的算法路径，涵盖调度、车辆路径、随机规划及强化学习等跨领域应用，并从经验成本最小化、模仿学习与强化学习角度整合方法论贡献；最后指出关键研究前沿。本综述旨在为组合优化与机器学习的交叉领域提供入门导引与未来研究路线图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to integrate predictive machine learning models with structured decision-making under constraints, leading to the emerging paradigm of Combinatorial Optimization Augmented Machine Learning (COAML). The method involves embedding combinatorial optimization oracles into learning pipelines to create data-driven, feasibility-preserving policies, and the authors provide a comprehensive survey that introduces a unifying framework, a taxonomy based on uncertainty and decision structure, and reviews algorithmic approaches. The main experimental results are synthesized from a broad range of applications, including scheduling, vehicle routing, stochastic programming, and reinforcement learning, highlighting contributions in empirical cost minimization, imitation learning, and reinforcement learning, while identifying key research frontiers for future work.</div>
<div class="mono" style="margin-top:8px">本文的动机源于将预测性机器学习模型与约束下的结构化决策相结合的需求，从而催生了组合优化增强机器学习（COAML）这一新兴范式。其方法是将组合优化预言机嵌入学习流程，以构建数据驱动且保持可行性的策略，作者通过全面综述提出了一个统一框架、基于不确定性和决策结构的分类法，并回顾了相关算法方法。主要实验结果综合了调度、车辆路径规划、随机规划和强化学习等多个领域的应用，强调了在经验成本最小化、模仿学习和强化学习方面的贡献，同时指出了未来研究的关键前沿方向。</div>
</details>
</div>
<div class="card">
<div class="title">Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning</div>
<div class="meta-line">Authors: Nilin Abrahamsen</div>
<div class="meta-line">First: 2026-01-15T15:16:15+00:00 · Latest: 2026-01-15T15:16:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10498v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10498v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>投影微批次累积实现强化学习的无参考近端策略更新</div>
<div class="mono" style="margin-top:8px">本文提出投影微批次累积（PROMA），一种用于大语言模型微调的近端策略更新方法。PROMA通过在微批次聚合前投影掉序列维度的梯度分量，实现跨微批次的策略梯度累积。该投影在反向传播过程中逐层应用，无需额外前向或反向计算即可高效实现。实证表明，PROMA比GRPO能更严格地控制局部KL散度，从而获得更稳定的策略学习。与PPO和GRPO不同，PROMA无需依赖参考策略或似然比裁剪即可实现近端更新，且不会引发熵崩溃。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Projected Microbatch Accumulation (PROMA), a method motivated by the need for stable, reference-free proximal policy updates in reinforcement learning fine-tuning of large language models. PROMA&#x27;s approach involves accumulating policy gradients across microbatches after projecting out sequence-wise gradient components layer-wise during the backward pass, which avoids extra computational overhead. Experimental results show that PROMA provides tighter control of local KL divergence than GRPO, leading to more stable learning, and unlike PPO or GRPO, it prevents entropy collapse without relying on a reference policy or likelihood-ratio clipping.</div>
<div class="mono" style="margin-top:8px">本文提出了投影微批次累积（PROMA）方法，其动机是为大语言模型的强化学习微调提供稳定、无参考的近端策略更新。PROMA的方法是在反向传播过程中逐层投影掉序列级梯度分量，然后在微批次间累积策略梯度，从而实现高效计算而无需额外前向或反向传播。实验结果表明，与GRPO相比，PROMA能更严格地控制局部KL散度，使策略学习更稳定，并且不同于PPO或GRPO，它在不依赖参考策略或似然比裁剪的情况下避免了熵崩溃。</div>
</details>
</div>
<div class="card">
<div class="title">Urban Socio-Semantic Segmentation with Vision-Language Reasoning</div>
<div class="meta-line">Authors: Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li</div>
<div class="meta-line">First: 2026-01-15T15:00:36+00:00 · Latest: 2026-01-15T15:00:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10477v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10477v1">PDF</a> · <a href="https://github.com/AMAP-ML/SocioReasoner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach&#x27;s gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉-语言推理的城市社会语义分割</div>
<div class="mono" style="margin-top:8px">作为人类活动的枢纽，城市地表包含丰富的语义实体。从卫星图像中分割这些多样实体对一系列下游应用至关重要。当前先进的语义分割模型能可靠分割由物理属性定义的实体（如建筑、水体），但在处理社会定义类别（如学校、公园）时仍面临困难。本研究通过视觉-语言模型推理实现了社会语义分割。为此，我们构建了名为SocioSeg的城市社会语义分割数据集，该新型资源包含卫星影像、数字地图及按层级结构组织的社会语义实体像素级标注。此外，我们提出名为SocioReasoner的创新视觉-语言推理框架，通过跨模态识别与多阶段推理模拟人类识别标注社会语义实体的认知过程。采用强化学习优化这一不可微分流程，以激发视觉-语言模型的推理潜能。实验表明，该方法在性能上超越现有先进模型，并展现出强大的零样本泛化能力。数据集与代码已开源：https://github.com/AMAP-ML/SocioReasoner。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of segmenting socially defined urban entities like schools and parks from satellite imagery, which existing models struggle with, this paper introduces a novel vision-language reasoning framework called SocioReasoner. The method simulates human annotation processes through cross-modal recognition and multi-stage reasoning, optimized via reinforcement learning to handle non-differentiable steps, and is supported by a new dataset, SocioSeg, with hierarchical pixel-level labels. Experimental results show that this approach outperforms state-of-the-art models and exhibits strong zero-shot generalization capabilities.</div>
<div class="mono" style="margin-top:8px">针对现有模型难以从卫星图像中分割学校、公园等社会定义的城市实体的挑战，本文提出了一种名为SocioReasoner的新型视觉语言推理框架。该方法通过跨模态识别和多阶段推理模拟人类标注过程，并利用强化学习优化不可微分的步骤，同时引入了一个具有分层像素级标签的新数据集SocioSeg。实验结果表明，该方法优于现有最先进模型，并展现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models</div>
<div class="meta-line">Authors: Ziming Dai, Dabiao Ma, Jinle Tong, Mengyuan Han, Jian Yang, Haojun Fei</div>
<div class="meta-line">First: 2026-01-15T14:48:52+00:00 · Latest: 2026-01-15T14:48:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10457v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10457v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being &quot;non-intrusive&quot;. It treats the legacy model as a frozen model and performs targeted repairs on &quot;hard regions&quot; where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NSR-Boost：面向工业遗留模型的神经符号残差增强框架</div>
<div class="mono" style="margin-top:8px">尽管梯度提升决策树（GBDT）在工业表格应用中占据主导地位，但在高并发生产环境中升级遗留模型仍面临高昂的重训练成本和系统性风险。为解决此问题，我们提出NSR-Boost——一个专为工业场景设计的神经符号残差增强框架。其核心优势在于“非侵入性”：将遗留模型视为冻结模型，仅对预测失败的“困难区域”进行针对性修复。该框架包含三个关键阶段：首先通过残差定位困难区域；其次利用大语言模型生成符号代码结构，并通过贝叶斯优化微调参数，构建可解释的专家模块；最后通过轻量级聚合器动态整合专家模块与遗留模型的输出。我们报告了NSR-Boost在Qfin控股核心金融风控系统中的成功部署案例。该框架不仅在六个公共数据集和一个私有数据集上显著优于现有最优基线，更重要的是在实际在线数据中展现出卓越的性能提升。最终证明其能有效捕捉传统模型遗漏的长尾风险，为工业界提供安全、低成本的模型演进范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable safe and low-cost upgrades to legacy Gradient Boosted Decision Tree (GBDT) models in high-stakes industrial settings, where full retraining is prohibitively expensive and risky. The proposed method, NSR-Boost, is a non-intrusive neuro-symbolic residual boosting framework that freezes the legacy model and targets repairs on its erroneous &quot;hard regions&quot;. It identifies these regions via prediction residuals, generates interpretable symbolic code experts using a Large Language Model (LLM) fine-tuned with Bayesian optimization, and dynamically integrates them via a lightweight aggregator. The main experimental results demonstrate successful deployment in a financial risk control system, with the framework outperforming state-of-the-art baselines on six public and one private dataset, showing significant performance gains on real-world online data and effectively capturing long-tail risks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于为高风险工业场景中的传统梯度提升决策树（GBDT）模型提供一种安全、低成本的升级方案，以避免全量重新训练带来的高昂成本和系统风险。所提出的方法NSR-Boost是一个非侵入式的神经符号残差提升框架，它将遗留模型冻结，并针对其预测错误的“困难区域”进行针对性修复。该方法通过残差识别困难区域，利用大语言模型生成可解释的符号代码专家模块并通过贝叶斯优化微调参数，最后通过一个轻量级聚合器动态集成专家与遗留模型的输出。主要实验结果表明，该框架已成功部署于一个金融风控核心系统，在六个公共数据集和一个私有数据集上均优于现有先进基线，在真实在线数据上表现出显著的性能提升，并能有效捕捉传统模型遗漏的长尾风险。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching</div>
<div class="meta-line">Authors: Nadav Merlis</div>
<div class="meta-line">First: 2026-01-15T14:09:49+00:00 · Latest: 2026-01-15T14:09:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10418v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10418v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes (&#x27;fixed batching policies&#x27;), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应批处理的带多步前瞻信息的强化学习</div>
<div class="mono" style="margin-top:8px">我们研究具有多步前瞻信息的表格型强化学习问题。在行动前，学习者能观测到未来ℓ步的状态转移与奖励实现：即在任何可能行动路径下，智能体将到达的确切状态及将获得的奖励。尽管已有研究表明此类信息能显著提升价值，但寻找最优策略是NP难问题，通常采用两种可处理的启发式方法之一：按预定义尺寸分块处理前瞻信息（&#x27;固定批处理策略&#x27;）或模型预测控制。我们首先阐明这两种方法存在的问题，并提出以自适应（状态依赖）批处理方式利用前瞻信息；我们将此类策略称为自适应批处理策略（ABP）。我们推导了这些策略的最优贝尔曼方程，并设计了一种乐观的遗憾最小化算法，使得在与未知环境交互时能够学习最优ABP。我们的遗憾界在阶数上是最优的，仅可能受前瞻视野ℓ的影响（ℓ通常可视为小常数）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of leveraging multi-step lookahead information in tabular reinforcement learning, where observing future transitions and rewards can enhance value but optimal policy computation is NP-hard. The authors critique existing heuristics like fixed batching and model predictive control, proposing instead adaptive batching policies (ABPs) that use state-dependent batches to process lookahead more effectively. They derive optimal Bellman equations for ABPs and develop an optimistic regret-minimization algorithm to learn optimal policies in unknown environments, with experimental results showing regret bounds that are order-optimal up to a factor of the lookahead horizon, which is typically small.</div>
<div class="mono" style="margin-top:8px">本文研究了表格强化学习中利用多步前瞻信息的挑战，其中观察未来转移和奖励能提升价值，但最优策略计算是NP难的。作者批判了固定批处理和模型预测控制等现有启发式方法，提出了自适应批处理策略（ABPs），通过状态依赖的批次更有效地处理前瞻信息。他们推导了ABPs的最优贝尔曼方程，并设计了一种乐观的遗憾最小化算法来在未知环境中学习最优策略，实验结果表明遗憾界在除前瞻视野因子外是阶次最优的，而该因子通常较小。</div>
</details>
</div>
<div class="card">
<div class="title">CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Yuanjie Zhao, Junnan Qiu, Yue Ding, Jie Li</div>
<div class="meta-line">First: 2026-01-15T13:57:52+00:00 · Latest: 2026-01-15T13:57:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10407v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10407v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable Out-of-Distribution (OOD) triggers. In this paper, we propose CS-GBA (Critical Sample-based Gradient-guided Backdoor Attack), a novel framework designed to achieve high stealthiness and destructiveness under a strict budget. Leveraging the theoretical insight that samples with high Temporal Difference (TD) errors are pivotal for value function convergence, we introduce an adaptive Critical Sample Selection strategy that concentrates the attack budget on the most influential transitions. To evade OOD detection, we propose a Correlation-Breaking Trigger mechanism that exploits the physical mutual exclusivity of state features (e.g., 95th percentile boundaries) to remain statistically concealed. Furthermore, we replace the conventional label inversion with a Gradient-Guided Action Generation mechanism, which searches for worst-case actions within the data manifold using the victim Q-network&#x27;s gradient. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms state-of-the-art baselines, achieving high attack success rates against representative safety-constrained algorithms with a minimal 5% poisoning budget, while maintaining the agent&#x27;s performance in clean environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CS-GBA：一种基于关键样本的梯度引导后门攻击方法，针对离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）能够从静态数据集中优化策略，但本质上易受后门攻击。现有攻击策略通常因低效的随机投毒和易于检测的分布外（OOD）触发器，难以应对安全约束算法（如CQL）。本文提出CS-GBA（基于关键样本的梯度引导后门攻击），这是一种新颖框架，旨在严格预算下实现高隐蔽性与破坏性。基于高时序差分（TD）误差样本对价值函数收敛至关重要的理论洞见，我们引入自适应关键样本选择策略，将攻击预算集中于最具影响力的状态转移。为规避OOD检测，提出相关性破坏触发器机制，利用状态特征的物理互斥性（如95%分位数边界）保持统计隐蔽性。此外，我们以梯度引导动作生成机制替代传统的标签反转，利用受害者Q网络的梯度在数据流形中搜索最差动作。在D4RL基准测试上的实验结果表明，本方法显著优于现有基线，仅用5%的投毒预算即可对代表性安全约束算法实现高攻击成功率，同时在干净环境中保持智能体性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of offline reinforcement learning (RL) to backdoor attacks, particularly against safety-constrained algorithms like CQL, where existing methods suffer from inefficient random poisoning and easily detectable triggers. The proposed CS-GBA framework introduces three key innovations: an adaptive Critical Sample Selection strategy that focuses the attack budget on transitions with high Temporal Difference errors for maximum impact, a Correlation-Breaking Trigger mechanism that uses physical feature boundaries to avoid Out-of-Distribution detection, and a Gradient-Guided Action Generation method that finds worst-case actions via the victim Q-network&#x27;s gradients instead of simple label inversion. Experiments on D4RL benchmarks show the method achieves high attack success rates with only a 5% poisoning budget while preserving normal performance in clean environments, outperforming prior baselines.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中的后门攻击漏洞，特别是在面对CQL等安全约束算法时，现有方法因随机投毒效率低且触发器易被检测而效果不佳。提出的CS-GBA框架包含三项创新：自适应关键样本选择策略，将攻击预算集中于具有高时序差分误差的转换以最大化影响；相关性破坏触发器机制，利用物理特征边界避免分布外检测；以及梯度引导动作生成方法，通过受害者Q网络的梯度寻找最差动作而非简单标签反转。在D4RL基准测试上的实验表明，该方法仅用5%的投毒预算即实现高攻击成功率，同时保持干净环境下的正常性能，优于现有基线。</div>
</details>
</div>
<div class="card">
<div class="title">Bootstrap Off-policy with World Model</div>
<div class="meta-line">Authors: Guojian Zhan, Likun Wang, Xiangteng Zhang, Jiaxin Gao, Masayoshi Tomizuka, Shengbo Eben Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-01T06:33:04+00:00 · Latest: 2026-01-15T13:38:44+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00423v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00423v3">PDF</a> · <a href="https://github.com/molumitu/BOOM_MBRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy&#x27;s actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner&#x27;s non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner&#x27;s action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于世界模型的离线策略自举</div>
<div class="mono" style="margin-top:8px">在线规划在强化学习中已被证明能有效提升样本效率与最终性能。然而，将规划用于环境交互时，收集数据与策略实际行为之间必然产生偏差，从而损害模型学习与策略改进。为此，我们提出BOOM（基于世界模型的离线策略自举）框架，通过自举循环紧密整合规划与离线学习：策略初始化规划器，规划器通过行为对齐优化动作以自举策略。该循环由联合学习的世界模型支持，使规划器能模拟未来轨迹并提供价值目标以促进策略改进。BOOM的核心是无似然对齐损失，利用规划器的非参数动作分布自举策略，并结合软价值加权机制——该机制优先高回报行为，并缓解回放缓冲区内规划器动作质量的波动性。在DeepMind Control Suite和Humanoid-Bench等高维环境上的实验表明，BOOM在训练稳定性和最终性能上均达到最先进水平。代码发布于https://github.com/molumitu/BOOM_MBRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the divergence between data collected via online planning and the policy&#x27;s actual behaviors in reinforcement learning, which degrades model learning and policy improvement. It proposes BOOM, a framework that integrates planning and off-policy learning through a bootstrap loop, where the policy initializes the planner and the planner refines actions to bootstrap the policy via behavior alignment, supported by a jointly learned world model for trajectory simulation and value targets. Experiments on the DeepMind Control Suite and Humanoid-Bench demonstrate that BOOM achieves state-of-the-art results in training stability and final performance.</div>
<div class="mono" style="margin-top:8px">该论文针对强化学习中在线规划收集的数据与策略实际行为之间的偏差问题，这种偏差会降低模型学习和策略改进的效果。它提出了BOOM框架，通过一个引导循环紧密集成规划和离策略学习：策略初始化规划器，规划器优化动作以通过行为对齐引导策略，并辅以一个联合学习的世界模型进行轨迹模拟和价值目标提供。在DeepMind Control Suite和Humanoid-Bench上的实验表明，BOOM在训练稳定性和最终性能方面达到了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Probabilistic Insights for Efficient Exploration Strategies in Reinforcement Learning</div>
<div class="meta-line">Authors: Ernesto Garcia, Paola Bermolen, Matthieu Jonckheere, Seva Shneer</div>
<div class="meta-line">First: 2025-03-05T14:53:32+00:00 · Latest: 2026-01-15T13:24:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03565v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.03565v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate efficient exploration strategies of environments with unknown stochastic dynamics and sparse rewards. Specifically, we analyze first the impact of parallel simulations on the probability of reaching rare states within a finite time budget. Using simplified models based on random walks and Lévy processes, we provide analytical results that demonstrate a phase transition in reaching probabilities as a function of the number of parallel simulations. We identify an optimal number of parallel simulations that balances exploration diversity and time allocation. Additionally, we analyze a restarting mechanism that exponentially enhances the probability of success by redirecting efforts toward more promising regions of the state space. Our findings contribute to a more qualitative and quantitative theory of some exploration schemes in reinforcement learning, offering insights into developing more efficient strategies for environments characterized by rare events.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中高效探索策略的概率性洞见</div>
<div class="mono" style="margin-top:8px">本研究探讨了在未知随机动态与稀疏奖励环境下的高效探索策略。具体而言，我们首先分析了并行仿真对有限时间预算内到达稀有状态概率的影响。通过基于随机游走与莱维过程的简化模型，我们提供了理论结果，揭示了到达概率随并行仿真数量变化的相变现象。我们确定了平衡探索多样性与时间分配的最优并行仿真数量。此外，我们分析了一种重启机制，该机制通过将探索资源重新导向状态空间中更具潜力的区域，以指数级提升成功概率。这些发现为强化学习中部分探索方案提供了更定性与定量的理论支撑，并为开发针对稀有事件特征环境的高效策略提供了洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of efficient exploration in reinforcement learning environments with unknown stochastic dynamics and sparse rewards, where reaching rare states is critical. The method employs simplified models based on random walks and Lévy processes to analytically study the impact of parallel simulations and a restarting mechanism on exploration. The main experimental results reveal a phase transition in the probability of reaching rare states as the number of parallel simulations varies, identifying an optimal number that balances exploration diversity and time allocation, and show that the restarting mechanism exponentially boosts success probabilities by reallocating effort to promising state-space regions.</div>
<div class="mono" style="margin-top:8px">本文旨在解决具有未知随机动态和稀疏奖励的强化学习环境中高效探索的挑战，其中到达罕见状态至关重要。方法采用基于随机游走和Lévy过程的简化模型，分析性地研究了并行模拟和重启机制对探索的影响。主要实验结果揭示了到达罕见状态的概率随并行模拟数量变化而出现相变，确定了平衡探索多样性和时间分配的最优并行模拟数量，并表明重启机制通过将努力重新分配到有希望的状态空间区域，能指数级提升成功概率。</div>
</details>
</div>
<div class="card">
<div class="title">SuS: Strategy-aware Surprise for Intrinsic Exploration</div>
<div class="meta-line">Authors: Mark Kashirskiy, Ilya Makarov</div>
<div class="meta-line">First: 2026-01-15T12:48:59+00:00 · Latest: 2026-01-15T12:48:59+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures, 3 tables. Code available at https://github.com/mariklolik/sus</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10349v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10349v1">PDF</a> · <a href="https://github.com/mariklolik/sus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent&#x27;s current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SuS：面向内在探索的策略感知惊喜</div>
<div class="mono" style="margin-top:8px">本文提出策略感知惊喜（SuS），一种新颖的内在激励框架，利用前后预测失配作为强化学习中探索的新颖性信号。与传统仅依赖状态预测误差的好奇心驱动方法不同，SuS引入两个互补组件：策略稳定性（SS）与策略惊喜（SuS）。SS衡量时序步骤间行为策略的一致性，而SuS捕捉智能体当前策略表征下的意外结果。我们通过习得的加权系数融合两种信号构建组合奖励函数。在大语言模型的数学推理任务上评估SuS，结果显示其在准确性与解多样性方面均有显著提升。消融实验证实移除任一组件会导致至少10%的性能下降，验证了方法的协同效应。相比基线方法，SuS在Pass@1指标上提升17.4%，在Pass@5指标上提升26.4%，同时在训练全程保持更高的策略多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Strategy-aware Surprise (SuS), a novel intrinsic motivation framework designed to enhance exploration in reinforcement learning by moving beyond traditional state prediction error. The method incorporates two key components: Strategy Stability (SS), which measures the consistency of an agent&#x27;s behavioral strategy over time, and Strategy Surprise (SuS), which captures unexpected outcomes relative to the current strategy representation, with both signals adaptively weighted. Experimental results on mathematical reasoning tasks using large language models show that SuS significantly improves accuracy and solution diversity, achieving a 17.4% boost in Pass@1 and a 26.4% boost in Pass@5 over baselines, while ablation studies confirm that removing either component leads to at least a 10% performance drop, highlighting their synergistic effect.</div>
<div class="mono" style="margin-top:8px">本文提出了策略感知惊喜（SuS），这是一种新颖的内在激励框架，旨在通过超越传统的状态预测误差来增强强化学习中的探索。该方法包含两个关键组件：策略稳定性（SS），用于衡量智能体行为策略随时间的一致性；以及策略惊喜（SuS），用于捕捉相对于当前策略表示的意外结果，两者通过自适应权重系数结合。在基于大语言模型的数学推理任务上的实验结果表明，SuS显著提高了准确性和解决方案多样性，相比基线方法在Pass@1上提升了17.4%，在Pass@5上提升了26.4%，同时消融研究证实移除任一组件都会导致至少10%的性能下降，突显了它们的协同作用。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing Safe Mechanical Ventilation Using Offline RL With Hybrid Actions and Clinically Aligned Rewards</div>
<div class="meta-line">Authors: Muhammad Hamza Yousuf, Jason Li, Sahar Vahdati, Raphael Theilen, Jakob Wittenstein, Jens Lehmann</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-06-17T10:17:26+00:00 · Latest: 2026-01-15T12:24:48+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.14375v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.14375v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Invasive mechanical ventilation (MV) is a life-sustaining therapy commonly used in the intensive care unit (ICU) for patients with severe and acute conditions. These patients frequently rely on MV for breathing. Given the high risk of death in such cases, optimal MV settings can reduce mortality, minimize ventilator-induced lung injury, shorten ICU stays, and ease the strain on healthcare resources. However, optimizing MV settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for optimizing MV settings, current methods struggle with the hybrid (continuous and discrete) nature of MV settings. Discretizing continuous settings leads to exponential growth in the action space, which limits the number of optimizable settings. Converting the predictions back to continuous can cause a distribution shift, compromising safety and performance. To address this challenge, in the IntelliLung project, we are developing an AI-based approach where we constrain the action space and employ factored action critics. This approach allows us to scale to six optimizable settings compared to 2-3 in previous studies. We adapt SOTA offline RL algorithms to operate directly on hybrid action spaces, avoiding the pitfalls of discretization. We also introduce a clinically grounded reward function based on ventilator-free days and physiological targets. Using multiobjective optimization for reward selection, we show that this leads to a more equitable consideration of all clinically relevant objectives. Notably, we develop a system in close collaboration with healthcare professionals that is aligned with real-world clinical objectives and designed with future deployment in mind.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于混合动作与临床对齐奖励的离线强化学习推进安全机械通气</div>
<div class="mono" style="margin-top:8px">有创机械通气（MV）是重症监护室（ICU）中用于危重急症患者的生命支持疗法。这类患者常依赖MV维持呼吸。鉴于其高死亡率，优化MV参数可降低病死率、减少呼吸机相关性肺损伤、缩短ICU住院时间并缓解医疗资源压力。然而，由于患者个体差异，MV参数优化仍是复杂且易出错的过程。虽然离线强化学习（RL）在优化MV参数方面展现出潜力，但现有方法难以处理MV参数混合（连续与离散）的特性。离散化连续参数会导致动作空间指数级增长，限制可优化参数的数量；将预测结果转回连续值则可能引发分布偏移，影响安全性与性能。为应对这一挑战，IntelliLung项目团队正在开发一种基于人工智能的方法：通过约束动作空间并采用分解动作评价器，使可优化参数从既往研究的2-3个扩展至6个。我们改进前沿离线RL算法，使其可直接处理混合动作空间，避免离散化缺陷。同时，基于脱机天数和生理指标构建临床导向的奖励函数，通过多目标优化进行奖励选择，确保所有临床相关目标得到均衡考量。值得注意的是，该系统与医疗专业人员深度协作开发，契合真实临床目标，并为未来临床部署进行设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing invasive mechanical ventilation (MV) settings in intensive care units, where patient-specific variability makes manual optimization complex and error-prone. The authors propose an offline reinforcement learning approach within the IntelliLung project that handles hybrid (continuous and discrete) actions directly using constrained action spaces and factored action critics, avoiding the pitfalls of discretization and scaling to six optimizable settings. Experimental results demonstrate that their method, combined with a clinically grounded reward function based on ventilator-free days and physiological targets via multiobjective optimization, provides a more equitable consideration of clinical objectives and outperforms prior studies limited to 2-3 settings.</div>
<div class="mono" style="margin-top:8px">本文针对重症监护病房中有创机械通气（MV）参数优化的挑战，即患者特异性差异使得手动优化复杂且易出错。作者在IntelliLung项目中提出一种离线强化学习方法，通过约束动作空间和分解动作评价器直接处理混合（连续和离散）动作，避免了离散化的缺陷，并将可优化参数扩展至六个。实验结果表明，该方法结合基于脱机天数和生理目标、通过多目标优化设计的临床奖励函数，能更公平地考虑所有临床目标，性能优于先前仅能优化2-3个参数的研究。</div>
</details>
</div>
<div class="card">
<div class="title">Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning</div>
<div class="meta-line">Authors: Xin Guan, Zijian Li, Shen Huang, Pengjun Xie, Jingren Zhou, Jiuxin Cao</div>
<div class="meta-line">First: 2026-01-15T11:40:57+00:00 · Latest: 2026-01-15T11:40:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10306v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded &quot;lucky guesses,&quot; leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于证据增强策略优化与奖励协同进化的长上下文推理方法</div>
<div class="mono" style="margin-top:8px">尽管强化学习（RL）推动了大型语言模型的推理能力，但其在长上下文场景中的应用受限于结果奖励的稀疏性。这一局限未能有效惩罚缺乏依据的&#x27;侥幸猜测&#x27;，使得关键的海量信息证据检索过程缺乏有效监督。为此，我们提出EAPO（证据增强策略优化）框架。首先建立证据增强推理范式，通过树状结构证据抽样验证精确证据提取是长上下文推理的决定性瓶颈。基于此发现，EAPO设计了一种专用强化学习算法：通过奖励模型计算群体相对证据奖励，提供密集的过程监督以显式提升证据质量。为维持训练全程的精准监督，我们进一步引入自适应奖励-策略协同进化机制。该机制利用结果一致的推演轨迹迭代优化奖励模型，增强其判别能力以确保精确的过程指导。在八个基准测试上的综合评估表明，相较于当前最优基线方法，EAPO显著提升了长上下文推理性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of sparse outcome rewards in Reinforcement Learning for long-context reasoning with large language models, which fails to supervise the critical evidence retrieval process and may reward ungrounded guesses. The proposed method, Evidence-Augmented Policy Optimization (EAPO), first establishes an Evidence-Augmented Reasoning paradigm, identifying precise evidence extraction as the key bottleneck. It then introduces a specialized RL algorithm using a reward model to compute a dense Group-Relative Evidence Reward for process supervision, coupled with an Adaptive Reward-Policy Co-Evolution mechanism that iteratively refines the reward model using outcome-consistent rollouts to maintain accurate guidance. Experimental results across eight benchmarks show that EAPO significantly outperforms state-of-the-art baselines in long-context reasoning performance.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型长上下文推理中强化学习结果奖励稀疏的问题，该问题无法监督关键的证据检索过程并可能奖励无根据的猜测。提出的方法，即证据增强策略优化（EAPO），首先建立了证据增强推理范式，确定精确证据提取是关键瓶颈。随后引入一种专门的强化学习算法，使用奖励模型计算密集的组相对证据奖励以进行过程监督，并结合自适应奖励-策略协同进化机制，该机制利用结果一致的轨迹迭代优化奖励模型以保持精确指导。在八个基准测试上的综合评估表明，EAPO在长上下文推理性能上显著优于现有最先进的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Safe Reinforcement Learning for Stochastic Reach-Avoid Problems with Entropy Regularization</div>
<div class="meta-line">Authors: Abhijit Mazumdar, Rafal Wisniewski, Manuela L. Bujorianu</div>
<div class="meta-line">First: 2026-01-13T15:23:19+00:00 · Latest: 2026-01-15T11:31:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08646v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08646v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>熵正则化随机可达规避问题的可证明安全强化学习</div>
<div class="mono" style="margin-top:8px">我们研究带安全约束的马尔可夫决策过程最优策略学习问题，将其建模为可达规避框架。目标是设计在线强化学习算法，确保学习阶段以任意高概率满足安全约束。为此，首先提出基于不确定性乐观原则的算法，并在此基础上引入熵正则化构建核心算法。我们对两种算法进行有限样本分析，推导其遗憾界，证明熵正则化的引入能改善遗憾界，并显著抑制基于不确定性乐观原则的安全强化学习算法固有的回合间波动性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of ensuring safety during online reinforcement learning in Markov decision processes, specifically for reach-avoid problems where the agent must reach a goal while avoiding hazards. The motivation is to guarantee safety with high probability throughout the learning process. The method introduces two algorithms: an initial one based on the optimism in the face of uncertainty principle, followed by a main algorithm that incorporates entropy regularization to enhance stability and performance. Experimental results from finite-sample analysis show that the entropy-regularized algorithm achieves improved regret bounds and significantly reduces the high episode-to-episode variability typically associated with safe RL methods.</div>
<div class="mono" style="margin-top:8px">本文针对马尔可夫决策过程中的安全约束强化学习问题，特别是在到达-规避场景中，智能体需在抵达目标的同时避开危险区域。研究动机是在学习阶段以极高概率确保安全性。方法上提出了两种算法：首先是一种基于不确定性面前乐观原则的算法，随后是引入熵正则化的主算法以提升稳定性和性能。实验的有限样本分析结果表明，熵正则化算法改善了遗憾界，并显著控制了基于乐观原则的安全强化学习算法固有的回合间波动性。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</div>
<div class="meta-line">Authors: Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, Younggyo Seo</div>
<div class="meta-line">First: 2025-05-29T16:41:12+00:00 · Latest: 2026-01-15T11:24:14+00:00</div>
<div class="meta-line">Comments: 29 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00070v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.00070v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate Robot-R1, we also introduce a new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Robot-R1：基于强化学习的机器人具身推理增强框架</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型近期通过结合具身推理与机器人控制，在机器人领域展现出巨大潜力。常见方法采用监督微调对机器人控制相关的具身推理任务进行训练，但此类数据集常基于启发式构建，未针对机器人控制优化，且易导致灾难性遗忘与泛化性能下降。为此，我们提出Robot-R1——一种利用强化学习专门增强机器人控制具身推理能力的新框架。该框架通过当前场景图像与专家演示的环境元数据，学习预测任务完成所需的下一个关键点状态。受DeepSeek-R1启发，Robot-R1对基于推理的响应进行采样，并强化那些能产生更准确预测的响应。为系统评估Robot-R1，我们同时提出了一个需要多样化具身推理能力的新基准测试。实验表明，采用Robot-R1训练的模型在具身推理任务上优于监督微调方法。尽管仅拥有70亿参数，Robot-R1在空间与运动推理等底层动作控制相关任务上甚至超越了GPT-4o。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of Supervised Fine-Tuning (SFT) for training Large Vision-Language Models (LVLMs) in robotics, such as suboptimal dataset construction, catastrophic forgetting, and poor generalization. To address this, the authors introduce Robot-R1, a novel framework that uses reinforcement learning to enhance embodied reasoning for robot control; the method learns to predict the next keypoint state from current scene images and environment metadata, sampling and reinforcing reasoning-based responses that lead to accurate predictions. Experimental results on a new benchmark show that Robot-R1 outperforms SFT methods and, despite having only 7B parameters, surpasses GPT-4o on low-level action reasoning tasks like spatial and movement reasoning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，使用监督微调（SFT）训练大型视觉语言模型（LVLM）应用于机器人领域时存在局限性，如数据集构建欠佳、灾难性遗忘和泛化能力差。为此，作者提出了Robot-R1这一新颖框架，它利用强化学习来增强机器人控制中的具身推理能力；该方法基于当前场景图像和环境元数据学习预测完成任务所需的下一个关键点状态，并对导致准确预测的推理响应进行采样和强化。在一个新基准上的实验结果表明，Robot-R1在具身推理任务上优于SFT方法，并且尽管仅有70亿参数，其在空间和运动推理等低层动作控制相关任务上甚至超越了GPT-4o。</div>
</details>
</div>
<div class="card">
<div class="title">TranslateGemma Technical Report</div>
<div class="meta-line">Authors: Mara Finkelstein, Isaac Caswell, Tobias Domhan, Jan-Thorsten Peter, Juraj Juraska, Parker Riley, Daniel Deutsch, Cole Dilanni, Colin Cherry, Eleftheria Briakou, Elizabeth Nielsen, Jiaming Luo, Kat Black, Ryan Mullins, Sweta Agrawal, Wenda Xu, Erin Kats, Stephane Jaskiewicz, Markus Freitag, David Vilar</div>
<div class="meta-line">First: 2026-01-13T22:23:24+00:00 · Latest: 2026-01-15T10:43:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09012v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09012v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TranslateGemma技术报告</div>
<div class="mono" style="margin-top:8px">本文介绍TranslateGemma——一套基于Gemma 3基础模型的开源机器翻译模型。为增强Gemma 3在翻译任务中的多语言能力，我们采用两阶段微调流程：首先使用通过前沿模型生成的大规模高质量合成平行数据与人工翻译平行数据混合进行监督微调；随后通过强化学习阶段，采用包含MetricX-QE和AutoMQM的奖励模型集成优化翻译质量。我们在WMT25测试集的10个语言对上通过人工评估，以及在WMT24++基准的55个语言对上通过自动评估，验证了TranslateGemma的有效性。自动指标显示所有规模的模型均较基线Gemma 3模型取得持续显著提升。值得注意的是，较小规模的TranslateGemma模型常能达到与较大基线模型相当的性能，同时提升效率。我们还证明TranslateGemma模型保留了强大的多模态能力，在Vistra图像翻译基准上表现更优。开源TranslateGemma模型的发布旨在为研究社区提供强大且适应性强的机器翻译工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind TranslateGemma is to enhance the inherent multilingual capabilities of the Gemma 3 foundation models specifically for machine translation, creating open and powerful tools for the research community. The method employs a two-stage fine-tuning process: first, supervised fine-tuning using a mixture of high-quality synthetic and human-translated parallel data, followed by a reinforcement learning phase optimized with an ensemble of reward models like MetricX-QE and AutoMQM. Main experimental results from human evaluation on the WMT25 test set across 10 language pairs and automatic evaluation on the WMT24++ benchmark across 55 language pairs show consistent and substantial gains over baseline Gemma 3 models, with smaller TranslateGemma models often matching the performance of larger baselines for improved efficiency, while also retaining strong multimodal capabilities as demonstrated by enhanced performance on the Vistra image translation benchmark.</div>
<div class="mono" style="margin-top:8px">TranslateGemma 的动机是基于 Gemma 3 基础模型增强其固有的多语言能力，专门用于机器翻译任务，旨在为研究社区提供强大且可适配的开源工具。其方法采用两阶段微调流程：首先使用通过先进模型生成的大规模高质量合成平行数据与人工翻译平行数据的混合进行监督微调，随后进行强化学习阶段，利用包括 MetricX-QE 和 AutoMQM 在内的奖励模型集成来优化翻译质量。主要实验结果包括在 WMT25 测试集上对 10 种语言对进行的人工评估，以及在 WMT24++ 基准上对 55 种语言对进行的自动评估，结果显示其相比基线 Gemma 3 模型在所有规模上均取得一致且显著的性能提升，较小的 TranslateGemma 模型常能达到与较大基线模型相当的性能，从而提高了效率，同时模型还保留了强大的多模态能力，在 Vistra 图像翻译基准上表现出增强的性能。</div>
</details>
</div>
<div class="card">
<div class="title">An Ensemble of Evolutionary Algorithms With Both Crisscross Search and Sparrow Search for Processing Inferior Individuals</div>
<div class="meta-line">Authors: Mingxuan Du, Tingzhang Luo, Ziyang Wang, Chengjun Li</div>
<div class="meta-line">First: 2026-01-15T10:36:08+00:00 · Latest: 2026-01-15T10:36:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10263v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10263v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the field of artificial intelligence, real parameter single objective optimization is an important direction. Both the Differential Evolution (DE) and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) demonstrate good performance for real parameter single objective optimization. Nevertheless, there exist other types of evolutionary algorithm for the purpose. In recent years, researchers begin to study long-term search. EA4eig - an ensemble of three DE variants and CMA-ES - performs well for long-term search. In this paper, we introduce two types of evolutionary algorithm proposed recently - crisscross search and sparrow search - into EA4eig as secondary evolutionary algorithms to process inferior individuals. Thus, EA4eigCS is obtained. In our ensemble, the secondary evolutionary algorithms are expected to vary distribution of the population for breaking stagnation. Experimental results show that our EA4eigCS outperforms EA4eig and is competitive when compared with state-of-the-art algorithms. Code and supplementary material are available at:https://anonymous.4open.science/r/EA4eigCS-2A43.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>融合交叉搜索与麻雀搜索的进化算法集成框架及其在劣质个体处理中的应用</div>
<div class="mono" style="margin-top:8px">在人工智能领域，实参数单目标优化是一个重要方向。差分进化算法与协方差矩阵自适应进化策略在该领域均表现出良好性能，但还存在其他类型的进化算法。近年来，研究者开始关注长期搜索问题。EA4eig——集成三种差分进化变体与协方差矩阵自适应进化策略的算法——在长期搜索中表现优异。本文引入近期提出的交叉搜索与麻雀搜索两种进化算法作为EA4eig的次级进化算法，专门处理劣质个体，从而构建出EA4eigCS算法。在该集成框架中，次级进化算法通过改变种群分布以突破搜索停滞。实验结果表明，EA4eigCS性能优于EA4eig，并与前沿算法具有竞争力。代码及补充材料可通过以下链接获取：https://anonymous.4open.science/r/EA4eigCS-2A43。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance long-term search performance in real parameter single objective optimization, this paper introduces crisscross search and sparrow search as secondary evolutionary algorithms into the existing EA4eig ensemble, which combines three Differential Evolution variants and CMA-ES, to specifically process inferior individuals and vary population distribution to break stagnation. The method, termed EA4eigCS, integrates these algorithms to diversify the search process. Experimental results demonstrate that EA4eigCS outperforms the original EA4eig and is competitive with state-of-the-art algorithms.</div>
<div class="mono" style="margin-top:8px">本文旨在提升实数参数单目标优化中的长期搜索性能，通过将最近提出的交叉搜索和麻雀搜索作为辅助进化算法，引入到结合了三种差分进化变体和CMA-ES的EA4eig集成中，以专门处理劣质个体并改变种群分布来打破停滞。该方法称为EA4eigCS，通过整合这些算法来多样化搜索过程。实验结果表明，EA4eigCS优于原始EA4eig，并与先进算法具有竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">PRL: Process Reward Learning Improves LLMs&#x27; Reasoning Ability and Broadens the Reasoning Boundary</div>
<div class="meta-line">Authors: Jiarui Yao, Ruida Wang, Tong Zhang</div>
<div class="meta-line">First: 2026-01-15T09:01:53+00:00 · Latest: 2026-01-15T09:01:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10201v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs&#x27; reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRL：过程奖励学习提升大语言模型推理能力并拓展推理边界</div>
<div class="mono" style="margin-top:8px">提升大语言模型的推理能力是近期持续关注的研究方向，但现有工作多基于轨迹层面的结果奖励，缺乏对推理过程的细粒度监督。其他尝试融合过程信号的训练框架也严重依赖蒙特卡洛树搜索、独立奖励模型训练等繁琐步骤，损害训练效率，且过程信号设计缺乏严谨理论支撑，导致优化机制不透明。本文提出过程奖励学习方法，将熵正则化强化学习目标分解至中间步骤，并分配具有理论依据的过程奖励。我们从理论动机出发，推导出与“奖励最大化+策略模型与参考模型间KL散度惩罚项”目标等效的PRL公式。该方法能将结果奖励转化为过程监督信号，更有效指导强化学习优化中的探索。实验表明，PRL不仅能通过平均@n指标提升模型推理性能，还能通过改进通过率@n指标拓展推理边界。大量实验验证了PRL方法的有效性和泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Process Reward Learning (PRL), motivated by the need to provide fine-grained supervision during reasoning, as existing methods often rely on coarse outcome rewards or inefficient auxiliary steps like Monte Carlo Tree Search. The method decomposes an entropy-regularized reinforcement learning objective into intermediate steps, deriving rigorous process rewards that guide exploration without requiring separate reward models, thereby enhancing training efficiency. Experimental results demonstrate that PRL improves LLMs&#x27; average reasoning performance and broadens the reasoning boundary, as evidenced by gains in both average @ n and pass @ n metrics across extensive evaluations.</div>
<div class="mono" style="margin-top:8px">本文提出了过程奖励学习（PRL），其动机在于现有方法通常依赖粗粒度的结果奖励或低效的辅助步骤（如蒙特卡洛树搜索），缺乏对推理过程的细粒度监督。该方法将熵正则化的强化学习目标分解为中间步骤，推导出严格的过程奖励，从而在无需训练单独奖励模型的情况下指导探索，提高了训练效率。实验结果表明，PRL不仅提升了大型语言模型在平均@n指标上的推理性能，还通过改善pass@n指标拓宽了推理边界，大量实验验证了其有效性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning</div>
<div class="meta-line">Authors: Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin</div>
<div class="meta-line">First: 2026-01-15T08:45:54+00:00 · Latest: 2026-01-15T08:45:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10187v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10187v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively &quot;tames&quot; the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HOMURA：通过强化学习驯服沙漏模型实现时间受限的大语言模型翻译</div>
<div class="mono" style="margin-top:8px">大语言模型在多语言翻译领域取得显著进展，但其系统性的跨语言冗长偏差使其难以适用于字幕配音等严格时间受限的任务。现有提示工程方法难以平衡语义保真度与刚性时间可行性之间的矛盾。为填补这一空白，我们首先提出沙漏基准——专门用于评估音节级时长约束下的翻译性能。进一步，我们提出HOMURA强化学习框架，显式优化语义保持与时间合规性的权衡。通过采用KL正则化目标及创新的动态音节比例奖励机制，HOMURA有效实现了输出长度的精准调控。实验结果表明，该方法显著优于主流大语言模型基线，在保持语义充分性的同时，实现了尊重语言密度层级的精确长度控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of cross-lingual verbosity bias in Large Language Models (LLMs) for translation, which makes them unsuitable for time-constrained applications like subtitling where output length must strictly match source duration. To tackle this, the authors first introduce Sand-Glass, a benchmark for evaluating translation under syllable-level duration constraints, and then propose HOMURA, a reinforcement learning framework that optimizes the trade-off between semantic fidelity and temporal compliance using a KL-regularized objective with a dynamic syllable-ratio reward. Experimental results show that HOMURA significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density without compromising semantic adequacy.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在翻译中存在的跨语言冗长偏差问题展开研究，该偏差使其无法适用于字幕翻译等有严格时间约束的任务，即译文长度需与源语言时长精确匹配。为解决此问题，作者首先提出了Sand-Glass基准，用于评估音节级时长约束下的翻译质量；进而提出了HOMURA强化学习框架，该框架通过使用带有动态音节比奖励的KL正则化目标，显式优化语义保真度与时间合规性之间的权衡。实验结果表明，该方法显著优于强基线大语言模型，能够在保持语义充分性的同时，实现尊重语言密度层级的精确长度控制。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand</div>
<div class="meta-line">Authors: Kiattikun Chobtham</div>
<div class="meta-line">First: 2026-01-15T08:40:01+00:00 · Latest: 2026-01-15T08:40:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10181v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习发现东北季风指数以预测泰国月降雨量</div>
<div class="mono" style="margin-top:8px">气候预测因地球系统复杂的时空模式而面临挑战。全球气候指数（如厄尔尼诺-南方涛动）是长期降雨预测的标准输入特征。然而，在泰国特定区域能提升预测精度的局地尺度指数仍存在显著空白。本文提出一种基于海表温度计算的新型东北季风气候指数，以反映北半球冬季季风的气候特征。为优化该指数计算区域，深度Q网络强化学习智能体通过探索并选择与季节性降雨相关性最高的矩形区域来实现。降雨站点被划分为12个独立聚类，以区分泰国南部与北部地区的降雨模式。实验结果表明，将优化后的指数融入长短期记忆模型，能显著提升多数聚类区域的长期月降雨预测能力，有效降低12个月前瞻预测的均方根误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of improving long-term monthly rainfall prediction in Thailand by developing a localized climate index, as existing global indices like ENSO are insufficient for regional accuracy. The method involves creating a novel NorthEast monsoon index derived from sea surface temperature, with its calculation areas optimized using a Deep Q-Network reinforcement learning agent that selects rectangles based on correlation with seasonal rainfall; rainfall stations are grouped into 12 clusters to capture regional patterns. Experimental results demonstrate that integrating this optimized index into Long Short-Term Memory models significantly enhances prediction skill across most clusters, effectively reducing the Root Mean Square Error for 12-month-ahead forecasts.</div>
<div class="mono" style="margin-top:8px">本文针对泰国长期月降雨量预测的挑战，旨在开发一种本地化气候指数，因为ENSO等全球指数难以满足区域预测精度需求。方法上，通过海表温度计算新型东北季风指数，并利用深度Q网络强化学习代理基于与季节性降雨的相关性优化计算区域矩形选择，同时将降雨站分为12个聚类以区分区域模式。实验结果表明，将该优化指数融入长短期记忆模型后，显著提升了大多数聚类区域的预测能力，有效降低了12个月提前预测的均方根误差。</div>
</details>
</div>
<div class="card">
<div class="title">OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning</div>
<div class="meta-line">Authors: Zixun Huang, Jiayi Sheng, Zeyu Zheng</div>
<div class="meta-line">First: 2025-11-28T16:09:28+00:00 · Latest: 2026-01-15T08:06:30+00:00</div>
<div class="meta-line">Comments: 19 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.23310v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.23310v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OBLR-PO：一种稳定的强化学习理论框架</div>
<div class="mono" style="margin-top:8px">现有基于强化学习的大语言模型后训练方法发展迅速，但其设计主要依赖启发式经验而非系统化理论原则。这一局限阻碍了我们对梯度估计器特性及相关优化算法的理解，从而制约了提升训练稳定性与整体性能的潜力。本研究提出统一的理论框架，在温和假设下刻画常用策略梯度估计器的统计特性。通过分析建立无偏性、推导精确方差表达式，并给出优化损失上界，为学习动态提供理论依据。基于这些结果，我们证明收敛保证，并推导出由梯度信噪比调控的自适应学习率调度机制。进一步证明方差最优基线是梯度加权估计器，为方差缩减提供新原理，自然提升现有方法的稳定性。这些洞见催生了最优基线与学习率策略优化算法，该算法以理论为基础联合调整学习率与基线。在Qwen3-4B-Base和Qwen3-8B-Base上的实验表明，相比现有策略优化方法取得持续增益，验证了理论贡献可转化为大规模后训练的实际性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the heuristic-driven design of existing reinforcement learning (RL) methods for large language model post-training, which lacks systematic theoretical grounding and limits stability and performance understanding, this work introduces a unified theoretical framework to analyze policy-gradient estimators. The method establishes unbiasedness, derives exact variance expressions, and provides an optimization-loss upper bound, leading to convergence guarantees and an adaptive learning-rate schedule based on gradient signal-to-noise ratio (SNR), alongside a variance-optimal baseline formulated as a gradient-weighted estimator. Experimental results on Qwen3-4B-Base and Qwen3-8B-Base models show that the proposed OBLR-PO algorithm, which jointly adapts learning rates and baselines, consistently outperforms existing policy optimization methods, validating practical improvements in large-scale post-training.</div>
<div class="mono" style="margin-top:8px">现有基于强化学习的大语言模型后训练方法多依赖启发式设计，缺乏系统理论指导，限制了训练稳定性和性能提升的理解，为此本研究提出了一个统一的理论框架来分析策略梯度估计器。该方法在温和假设下建立了无偏性，推导了精确方差表达式，并给出了优化损失上界，从而获得收敛保证和基于梯度信噪比的自适应学习率调度，同时提出一种梯度加权的方差最优基线估计器以增强稳定性。在Qwen3-4B-Base和Qwen3-8B-Base模型上的实验表明，所提出的OBLR-PO算法能联合自适应调整学习率和基线，相比现有策略优化方法取得了稳定性能提升，验证了理论贡献在大规模后训练中的实际有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DecisionLLM: Large Language Models for Long Sequence Decision Exploration</div>
<div class="meta-line">Authors: Xiaowei Lv, Zhilin Zhang, Yijun Li, Yusen Huo, Siyuan Ju, Xuyan Li, Chunxiang Hong, Tianyu Wang, Yongcai Wang, Peng Sun, Chuan Yu, Jian Xu, Bo Zheng</div>
<div class="meta-line">First: 2026-01-15T07:42:02+00:00 · Latest: 2026-01-15T07:42:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10148v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10148v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs&#x27; inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DecisionLLM：面向长序列决策探索的大语言模型</div>
<div class="mono" style="margin-top:8px">长序列决策通常通过强化学习（RL）解决，是动态环境中优化战略操作（如计算广告中的实时竞价）的关键组成部分。决策变换器（DT）通过将RL构建为自回归序列建模问题，引入了一种强大的范式。与此同时，大语言模型（LLMs）在复杂推理与规划任务中展现出显著成功。这启发我们思考：共享相同Transformer架构但规模更大的LLMs，能否在长周期序列决策问题中实现性能的新突破。本研究探索LLMs在离线决策任务中的应用。该领域的一个根本挑战在于LLMs天生无法解析连续值，因为当数值以文本字符串表示时，模型缺乏对数值大小与顺序的固有理解。为此，我们提出将轨迹视为一种独立模态。通过学习将轨迹数据与自然语言任务描述对齐，我们的模型可在统一框架（称为DecisionLLM）中自回归预测未来决策。我们建立了该范式的扩展定律，证明性能取决于三个因素：模型规模、数据量与数据质量。在离线实验基准与竞价场景中，DecisionLLM均取得优异表现。具体而言，DecisionLLM-3B在Maze2D umaze-v1上超越传统决策变换器（DT）69.4分，在AuctionNet上领先0.085分。该研究拓展了AIGB范式，并为在线竞价等领域的未来探索指明了方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the potential of large language models (LLMs) to advance long-sequence decision-making, traditionally tackled by reinforcement learning, given their success in reasoning and their shared Transformer architecture with models like Decision Transformer. To overcome LLMs&#x27; inability to interpret continuous numerical values in trajectory data, the method introduces DecisionLLM, which treats trajectories as a distinct modality and aligns them with natural language task descriptions for autoregressive decision prediction. Experimental results show that DecisionLLM-3B significantly outperforms the traditional Decision Transformer, achieving a 69.4-point improvement on Maze2D umaze-v1 and a 0.085-point gain on AuctionNet, while scaling laws highlight the importance of model scale, data volume, and quality.</div>
<div class="mono" style="margin-top:8px">本文的动机在于利用大型语言模型（LLMs）在推理任务上的成功及其与决策变换器等模型共享的Transformer架构，以改进传统由强化学习处理的长序列决策问题。为解决LLMs无法理解轨迹数据中连续数值的固有缺陷，方法提出DecisionLLM，将轨迹视为一种独立模态，并将其与自然语言任务描述对齐，以实现自回归决策预测。实验结果表明，DecisionLLM-3B显著优于传统决策变换器，在Maze2D umaze-v1上性能提升69.4分，在AuctionNet上提升0.085分，同时缩放定律揭示了模型规模、数据量和数据质量的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis</div>
<div class="meta-line">Authors: Haochong Xia, Yao Long Teng, Regan Tan, Molei Qin, Xinrun Wang, Bo An</div>
<div class="meta-line">First: 2026-01-15T07:38:59+00:00 · Latest: 2026-01-15T07:38:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10143v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra &quot;History Is Not Enough&quot; underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>历史不足为凭：面向金融时间序列合成的自适应数据流系统</div>
<div class="mono" style="margin-top:8px">在量化金融领域，由概念漂移和分布非平稳性驱动的训练与真实表现之间的差距，仍是构建可靠数据驱动系统的关键障碍。基于静态历史数据训练的模型常出现过拟合，导致在动态市场中泛化能力不足。“历史不足为凭”的理念强调需要能够随市场演进的自适应数据生成方法，而非仅依赖过往观测。我们提出一种漂移感知数据流系统，将基于机器学习的自适应控制融入数据治理流程。该系统将参数化数据操作模块（包含单股变换、多股混合与治理操作）与采用基于梯度的双层优化进行系统控制的自适应规划调度器相结合。该设计在统一的可微分框架下整合了数据增强、课程学习与数据工作流管理，支持溯源感知的重放与持续数据质量监控。在预测与强化学习交易任务上的大量实验表明，该框架能提升模型鲁棒性并改善风险调整后收益。该系统为金融数据自适应管理与学习引导的工作流自动化提供了一种可推广的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance gap in quantitative finance due to concept drift and non-stationarity, which causes models trained on static historical data to overfit and generalize poorly in dynamic markets, this paper introduces a drift-aware dataflow system that integrates machine learning-based adaptive control into data curation. The method couples a parameterized data manipulation module with an adaptive planner-scheduler using gradient-based bi-level optimization, unifying data augmentation, curriculum learning, and workflow management in a differentiable framework for provenance-aware replay and quality monitoring. Experimental results on forecasting and reinforcement learning trading tasks show that the framework enhances model robustness and improves risk-adjusted returns, offering a generalizable approach to adaptive data management.</div>
<div class="mono" style="margin-top:8px">本文针对量化金融中因概念漂移和非平稳性导致的性能差距问题，即静态历史数据训练的模型在动态市场中容易过拟合和泛化能力差，提出了一种集成机器学习自适应控制的数据流系统。该方法通过梯度双层优化将参数化数据操作模块与自适应规划调度器结合，在可微分框架下统一了数据增强、课程学习和工作流管理，支持溯源重放和持续质量监控。在预测和强化学习交易任务上的实验表明，该框架提升了模型鲁棒性并改善了风险调整后收益，为金融数据的自适应管理和学习引导的工作流自动化提供了通用方案。</div>
</details>
</div>
<div class="card">
<div class="title">Functional Critics Are Essential in Off-Policy Actor-Critic: Provable Convergence and Efficient Exploration</div>
<div class="meta-line">Authors: Qinxun Bai, Yuxuan Han, Wei Xu, Zhengyuan Zhou</div>
<div class="meta-line">First: 2025-09-26T21:55:26+00:00 · Latest: 2026-01-15T06:25:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22964v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.22964v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-policy reinforcement learning (RL) with function approximation offers an effective way to improve sample efficiency by reusing past experience. Within this setting, the actor-critic (AC) framework has achieved strong empirical success but suffers from the &quot;moving target&quot; problem, where the policy being evaluated changes continually. Functional critics, or policy-conditioned value functions, have been proposed to address this issue by including a representation of the policy as input. While the concept of generalizing value functions across policy space is appealing, previous efforts have struggled to remain competitive against state-of-the-art AC algorithms that do not utilize functional critics. In this work, we revisit functional critics within the off-policy AC framework and identify two aspects that render them a necessity rather than a luxury. First, in off-policy AC, critic learning contends with both the &quot;deadly triad&quot; instability and the &quot;moving target&quot; issue, while actor learning faces the challenge of estimating the exact off-policy policy gradient. This complex interplay makes theoretical convergence extremely difficult for practical algorithms. We demonstrate that a functional critic is essential for addressing this challenge and establish the first convergence proof for an off-policy target-based AC algorithm under linear function approximation. Second, we identify a crucial link between functional critic modeling and efficient exploration. Specifically, we show that approximating posterior sampling for exploration in model-free settings is infeasible without functional critics. Practically, we propose a tailored neural network architecture and a minimal AC algorithm that relies solely on these insights. In experiments on the DeepMind Control Suite, this implementation achieves performance competitive with state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离策略演员-评论家算法中函数型评论家的必要性：可证明的收敛性与高效探索</div>
<div class="mono" style="margin-top:8px">基于函数逼近的离策略强化学习通过重用历史经验有效提升了样本效率。在此框架下，演员-评论家方法虽取得显著实证成果，但始终受困于策略持续更新导致的“移动目标”问题。函数型评论家（或称策略条件价值函数）通过将策略表示作为输入来解决该问题。尽管跨策略空间泛化价值函数的概念颇具吸引力，但先前研究始终难以匹敌未采用函数型评论家的先进演员-评论家算法。本文重新审视离策略演员-评论家框架中的函数型评论家，揭示其不可或缺的双重价值：首先，离策略评论家学习需同时应对“致命三角”不稳定与“移动目标”问题，而演员学习则面临精确估计离策略梯度的挑战。这种复杂交互使实际算法的理论收敛性极难保证。我们证明函数型评论家是解决该挑战的关键，并首次建立了线性函数逼近下离策略目标型演员-评论家算法的收敛性证明。其次，我们发现函数型评论家建模与高效探索存在本质关联：在无模型设定中，若缺乏函数型评论家则无法实现后验采样探索的近似。基于此，我们设计了专用神经网络架构与极简演员-评论家算法。在DeepMind控制套件的实验中，该实现达到了与前沿方法相当的效能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper revisits functional critics, or policy-conditioned value functions, in off-policy actor-critic reinforcement learning, motivated by the need to address the combined instability from the &quot;deadly triad&quot; and the &quot;moving target&quot; problem inherent in this setting. The method involves theoretically demonstrating the necessity of functional critics for convergence and efficient exploration, and practically proposing a tailored neural architecture and a minimal algorithm based on this insight. The main experimental results show that the proposed implementation achieves performance competitive with state-of-the-art methods on the DeepMind Control Suite, validating its practical efficacy.</div>
<div class="mono" style="margin-top:8px">本文重新审视了离线策略演员-评论家强化学习中的功能评论家（即策略条件价值函数），其动机在于解决该设置中固有的“致命三元组”不稳定性和“移动目标”问题的双重挑战。方法上，研究从理论上论证了功能评论家对于算法收敛和高效探索的必要性，并据此提出了一个定制的神经网络架构和简洁算法。主要实验结果表明，在DeepMind控制套件上，所提实现的性能与最先进方法相当，验证了其实际有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts</div>
<div class="meta-line">Authors: Sijia Luo, Xiaokang Zhang, Yuxuan Hu, Bohan Zhang, Ke Wang, Jinbo Su, Mengshu Sun, Lei Liang, Jing Zhang</div>
<div class="meta-line">First: 2026-01-15T05:12:03+00:00 · Latest: 2026-01-15T05:12:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10079v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10079v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏强化学习：通过稳定稀疏推演突破大语言模型强化学习中的内存墙</div>
<div class="mono" style="margin-top:8px">强化学习已成为激发大语言模型复杂推理能力的关键技术。然而，长序列推演过程中存储键值缓存带来的巨大内存开销构成关键瓶颈，常导致有限硬件上无法高效训练。现有键值压缩技术虽能缓解推理压力，但直接应用于强化学习训练会引发严重的策略失配，导致性能崩溃。为此，我们提出稀疏强化学习方法，实现稀疏推演下的稳定训练。研究表明，不稳定性源于稠密旧策略、稀疏采样策略与学习策略之间的根本性失配。为缓解该问题，稀疏强化学习引入稀疏感知拒绝采样与基于重要性的重加权机制，以修正压缩导致信息损失引发的离策略偏差。实验表明，稀疏强化学习在保持性能的同时，显著降低了推演开销。此外，该方法本质具备稀疏感知训练能力，可大幅提升模型在稀疏推理部署中的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the memory bottleneck caused by storing Key-Value caches during long-horizon reinforcement learning for large language models, which hinders efficient training on limited hardware. The proposed method, Sparse-RL, addresses the policy mismatch and instability induced by directly applying existing KV compression techniques to RL training by introducing Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct off-policy bias from compression. Experimental results demonstrate that Sparse-RL successfully reduces rollout memory overhead compared to dense baselines while maintaining performance and also enhances model robustness for sparse inference deployment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型强化学习中因存储键值缓存导致的内存瓶颈问题，该问题阻碍了在有限硬件上的高效训练。所提出的Sparse-RL方法通过引入稀疏感知拒绝采样和基于重要性的重加权，纠正了因压缩导致信息损失而产生的离策略偏差，从而解决了直接将现有KV压缩技术应用于RL训练所引起的策略失配和不稳定性。实验结果表明，与密集基线相比，Sparse-RL在降低训练内存开销的同时保持了性能，并显著增强了模型在稀疏推理部署中的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Learning through Ranking Mean Squared Error</div>
<div class="meta-line">Authors: Chaitanya Kharyal, Calarina Muslimani, Matthew E. Taylor</div>
<div class="meta-line">First: 2026-01-14T07:18:12+00:00 · Latest: 2026-01-15T04:48:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09236v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09236v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., &quot;bad,&quot; &quot;neutral,&quot; &quot;good&quot;). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher&#x27;s ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于排序均方误差的奖励学习</div>
<div class="mono" style="margin-top:8px">奖励设计仍是强化学习应用于现实问题的关键瓶颈。奖励学习作为一种主流替代方案，通过人类反馈而非人工设定来推导奖励函数。近期研究提出从评分形式的人类反馈中学习奖励函数，取代传统的二元偏好标注，以提供更丰富且认知负担更低的监督。基于此范式，我们提出一种新的评分驱动强化学习方法——强化学习排序回报回归。该方法核心采用新颖的排序均方误差损失函数，将教师提供的评分视为序数目标。我们通过轨迹-评分配对数据集进行学习，每条轨迹均标注离散评分。每个训练步骤中，我们采样一组轨迹并预测其回报，通过可微分排序算子生成软排序，随后优化软排序与教师评分间的均方误差损失。相较于现有评分方法，本方法具有形式化保证：在温和假设下可证明其解集既最小又完备。在模拟人类反馈的实验中，该方法在OpenAI Gym与DeepMind Control Suite的机器人运动基准测试中，持续匹配或超越现有评分及偏好驱动的强化学习方法，且所需反馈量显著减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of reward design in reinforcement learning by proposing a method to learn reward functions from human-provided ratings rather than binary preferences, aiming for richer and less demanding supervision. The introduced method, Ranked Return Regression for RL (R4), employs a novel ranking mean squared error loss that treats ratings as ordinal targets, using differentiable sorting to rank predicted returns and optimize against teacher ratings. Experimental results on robotic locomotion benchmarks show that R4 matches or outperforms existing rating and preference-based methods while requiring less feedback, with formal guarantees of minimal and complete solution sets under mild assumptions.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中奖励设计的难题，提出了一种从人类提供的评分而非二元偏好中学习奖励函数的方法，旨在获得更丰富且认知负担更小的监督。所提出的方法R4采用了一种新颖的排序均方误差损失，将评分视为序数目标，通过可微分排序对预测回报进行排名并优化以匹配教师评分。在机器人运动基准测试上的实验结果表明，R4在需要更少反馈的情况下，匹配或超越了现有的评分和偏好基方法，并在温和假设下提供了形式化保证，确保解集的最小性和完备性。</div>
</details>
</div>
<div class="card">
<div class="title">PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization</div>
<div class="meta-line">Authors: Tingyue Pan, Jie Ouyang, Mingyue Cheng, Qingchuan Li, Zirui Liu, Mingfan Pan, Shuo Yu, Qi Liu</div>
<div class="meta-line">First: 2026-01-15T03:21:21+00:00 · Latest: 2026-01-15T03:21:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10029v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PaperScout：一种具有过程感知序列级策略优化的学术论文搜索自主智能体</div>
<div class="mono" style="margin-top:8px">学术论文搜索是科学研究中的基础任务，但现有方法大多依赖僵化的预定义工作流，难以处理复杂的条件式查询。为突破这一局限，我们提出PaperScout——将论文搜索重构为序列决策过程的自主智能体。与静态工作流不同，PaperScout能基于累积检索上下文动态决定是否、何时及如何调用搜索与扩展工具。然而，训练此类智能体面临根本性挑战：标准强化学习方法通常针对单轮任务设计，应用于多轮智能体任务时存在粒度失配问题——令牌级优化与序列级交互的粒度差异导致信用分配噪声。我们提出近端序列策略优化（PSPO），这是一种与智能体-环境交互对齐的过程感知序列级策略优化方法。在合成与真实场景基准上的综合实验表明，PaperScout在召回率与相关性方面显著优于强工作流驱动及强化学习基线，验证了我们自适应智能体框架与优化策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of rigid, predefined workflows in academic paper search, which struggle with complex queries, by developing an autonomous agent that treats search as a sequential decision-making process. The method introduces PaperScout, an agent that dynamically invokes tools based on context, and Proximal Sequence Policy Optimization (PSPO), a novel training approach designed to align optimization with sequence-level agent interactions, addressing the granularity mismatch in standard reinforcement learning. Experimental results on synthetic and real-world benchmarks show that PaperScout significantly outperforms workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of the adaptive framework and optimization strategy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是克服学术论文搜索中僵化的预定义工作流在处理复杂查询时的局限性，通过将搜索重构为序列决策过程，开发了一个自主智能体。方法上提出了PaperScout智能体，它能根据累积的检索上下文动态调用工具，并引入了近端序列策略优化（PSPO），这是一种过程感知的序列级策略优化方法，旨在使优化与智能体-环境交互对齐，解决了标准强化学习中存在的粒度不匹配问题。在合成和真实世界基准上的综合实验表明，PaperScout在召回率和相关性上均显著优于基于工作流和强化学习的基线方法，验证了该自适应智能体框架及优化策略的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Gradient Coupling: The Hidden Barrier to Generalization in Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu</div>
<div class="meta-line">First: 2025-09-28T13:24:38+00:00 · Latest: 2026-01-15T01:18:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23870v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.23870v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a dominant paradigm for training autonomous agents, yet these agents often exhibit poor generalization, failing to adapt to scenarios not seen during training. In this work, we identify a fundamental cause of this brittleness, a phenomenon which we term &quot;gradient coupling.&quot; We hypothesize that in complex agentic tasks, the high similarity between distinct states leads to destructive interference between gradients. Specifically, a gradient update that reinforces an optimal action in one state can inadvertently increase the likelihood of a suboptimal action in a similar, yet different, state. To solve this, we propose a novel objective where the actor is trained to simultaneously function as a classifier that separates good and bad actions. This auxiliary pressure compels the model to learn disentangled embeddings for positive and negative actions, which mitigates negative gradient interference and improve the generalization performance. Extensive experiments demonstrate the effectiveness of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>梯度耦合：智能体强化学习中泛化能力的隐性障碍</div>
<div class="mono" style="margin-top:8px">强化学习是训练自主智能体的主流范式，但这些智能体常表现出较差的泛化能力，难以适应训练中未见的场景。本研究揭示了导致这种脆弱性的根本原因——我们称之为“梯度耦合”的现象。我们假设在复杂的智能体任务中，不同状态间的高度相似性会导致梯度间的破坏性干扰。具体而言，强化某一状态最优动作的梯度更新，可能无意中增加相似但不同状态下次优动作的选择概率。为解决此问题，我们提出一种新颖的目标函数：训练执行器同时作为区分优劣动作的分类器。这种辅助压力迫使模型学习正负动作的解耦嵌入表示，从而减轻负面梯度干扰并提升泛化性能。大量实验验证了本方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the poor generalization of reinforcement learning agents, attributing it to a phenomenon called gradient coupling where similar states cause destructive interference in gradient updates, reinforcing optimal actions in one state while inadvertently promoting suboptimal ones in others. To address this, the authors propose a novel training objective that forces the actor to also act as a classifier separating good and bad actions, thereby learning disentangled embeddings that reduce negative gradient interference. Experimental results show that this method effectively improves generalization performance in agentic tasks.</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习智能体泛化能力差的问题，将其归因于一种称为梯度耦合的现象，即相似状态导致梯度更新中的破坏性干扰，在强化一个状态中最佳动作的同时，无意中增加了其他相似状态中次优动作的可能性。为解决此问题，作者提出了一种新颖的训练目标，强制行动者同时作为分类器区分好坏动作，从而学习解耦的嵌入表示以减轻负面梯度干扰。大量实验结果表明，该方法有效提升了智能体任务的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model</div>
<div class="meta-line">Authors: Enoch H. Kang, Hema Yoganarasimhan, Lalit Jain</div>
<div class="meta-line">First: 2025-02-19T22:22:20+00:00 · Latest: 2026-01-14T22:45:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.14131v6">Abs</a> · <a href="https://arxiv.org/pdf/2502.14131v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于经验风险最小化的离线逆强化学习与动态离散选择模型研究</div>
<div class="mono" style="margin-top:8px">本文研究动态离散选择模型的估计问题，该问题在机器学习中亦称为离线最大熵正则化逆强化学习。目标是从离线行为数据中恢复支配智能体行为的奖励函数或$Q^*$函数。我们提出一种全局收敛的基于梯度的方法，无需依赖线性参数化奖励的严格假设。方法的核心创新在于引入基于经验风险最小化的逆强化学习/动态离散选择框架，避免了贝尔曼方程中对显式状态转移概率的估计。此外，该方法兼容神经网络等非参数估计技术，因此具备扩展到高维无限状态空间的潜力。关键理论依据是贝尔曼残差满足Polyak-Lojasiewicz条件——该性质虽弱于强凸性，但足以保证快速全局收敛。通过系列仿真实验，我们证明该方法持续优于基准方法与现有先进方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of recovering reward or Q* functions from offline behavior data in Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy Inverse Reinforcement Learning (MaxEnt-IRL), where traditional methods often rely on restrictive linear reward parameterizations. The authors propose a novel gradient-based method grounded in an Empirical Risk Minimization (ERM) framework, which eliminates the need to explicitly estimate state transition probabilities in the Bellman equation and supports flexible non-parametric function approximators like neural networks, enabling scalability to high-dimensional state spaces. A key theoretical contribution is proving that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition, ensuring fast global convergence. Synthetic experiments demonstrate that the proposed approach consistently outperforms existing benchmark and state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本文针对从离线行为数据中恢复奖励函数或Q*函数的问题展开研究，该问题对应于动态离散选择（DDC）模型，也称为离线最大熵逆强化学习（MaxEnt-IRL），传统方法通常依赖于限制性的线性奖励参数化假设。作者提出了一种基于经验风险最小化（ERM）框架的新型梯度方法，该方法避免了在贝尔曼方程中显式估计状态转移概率，并支持如神经网络之类的非参数函数逼近器，从而能够扩展到高维状态空间。关键的理论贡献是证明了贝尔曼残差满足Polyak-Lojasiewicz（PL）条件，确保了快速的全局收敛性。通过一系列合成实验，该方法被证明在性能上持续优于现有基准方法和最先进的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Future-as-Label: Scalable Supervision from Real-World Outcomes</div>
<div class="meta-line">Authors: Benjamin Turtel, Paul Wilczewski, Danny Franklin, Kris Skothiem</div>
<div class="meta-line">First: 2026-01-09T22:15:12+00:00 · Latest: 2026-01-14T22:14:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06336v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06336v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time creates free supervision: forecasts about real-world events resolve to verifiable outcomes. The passage of time provides labels that require no annotation. To exploit this structure, we extend reinforcement learning with verifiable rewards to real-world prediction over time. We train language models to make probabilistic forecasts from causally masked information, using proper scoring rules as the reward function once events resolve. Learning is driven entirely by realized outcomes, enabling scalable outcome-based supervision in open-world prediction. On real-world forecasting benchmarks, Qwen3-32B trained using Foresight Learning improves Brier score by 27% and halves calibration error relative to its pretrained baseline, and outperforms Qwen3-235B on both constructed future-event prediction tasks and the Metaculus benchmark despite a 7x parameter disadvantage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未来即标签：基于现实世界结果的可扩展监督</div>
<div class="mono" style="margin-top:8px">时间创造免费监督：对现实事件的预测会转化为可验证的结果。时间的推移提供了无需标注的标签。为利用这一特性，我们将带可验证奖励的强化学习扩展至时序现实世界预测。我们训练语言模型基于因果遮蔽信息生成概率预测，在事件结果揭晓后使用恰当评分规则作为奖励函数。学习完全由已实现的结果驱动，实现了开放世界预测中基于结果的可扩展监督。在现实世界预测基准测试中，采用前瞻学习训练的Qwen3-32B模型相比预训练基线，布里尔分数提升27%，校准误差减半；尽管参数量仅为1/7，其在构建的未来事件预测任务和Metaculus基准测试中均优于Qwen3-235B。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the idea that the passage of time naturally provides verifiable outcomes for predictions, offering a source of free and scalable supervision without manual annotation. The method, termed Future-as-Label or Foresight Learning, extends reinforcement learning by training language models to make probabilistic forecasts from causally masked information, using proper scoring rules as rewards once real-world events resolve. Experimental results show that on real-world forecasting benchmarks, Qwen3-32B trained with this approach improves Brier score by 27% and halves calibration error compared to its pretrained baseline, and even outperforms the much larger Qwen3-235B on tasks like Metaculus despite having 7 times fewer parameters.</div>
<div class="mono" style="margin-top:8px">本文的动机在于时间流逝会自然产生可验证的预测结果，这为监督学习提供了无需人工标注的、可扩展的免费监督信号。该方法称为“未来即标签”或前瞻学习，通过强化学习框架训练语言模型基于因果遮蔽信息进行概率预测，并在事件发生后使用适当评分规则作为奖励函数。实验结果表明，在现实世界预测基准上，采用此方法训练的Qwen3-32B模型相比其预训练基线，Brier分数提升了27%，校准误差减半，并且在构建的未来事件预测任务和Metaculus基准测试中，以7倍少的参数量超越了更大的Qwen3-235B模型。</div>
</details>
</div>
<div class="card">
<div class="title">An intelligent agent-based simulation of human mobility in extreme urban morphologies</div>
<div class="meta-line">Authors: Abderaouf Bahi, Amel Ourici</div>
<div class="meta-line">First: 2025-07-20T22:35:16+00:00 · Latest: 2026-01-14T21:38:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.15143v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.15143v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates the feasibility of human mobility in extreme urban morphologies, characterized by high-density vertical structures and linear city layouts. To assess whether agents can navigate efficiently within such unprecedented topologies, we develop a hybrid simulation framework that integrates agent-based modeling, reinforcement learning (RL), supervised learning, and graph neural networks (GNNs). The simulation captures multi-modal transportation behaviors across multiple vertical levels and varying density scenarios, using both synthetic data and real-world traces from high-density cities. Experiments show that the full AI-integrated architecture enables agents to achieve an average commute time of 7.8--8.4 minutes, a satisfaction rate exceeding 89\%, and a reachability index over 91\%, even during peak congestion periods. Ablation studies indicate that removing intelligent modules such as RL or GNN significantly degrades performance, with commute times increasing by up to 85\% and reachability falling below 70\%. Environmental modeling demonstrates low energy consumption and minimal CO$_2$ emissions when electric modes are prioritized. These results suggest that efficient and sustainable mobility in extreme urban forms is achievable, provided adaptive AI systems, intelligent infrastructure, and real-time feedback mechanisms are implemented.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>极端城市形态下基于智能体的人类移动性仿真研究</div>
<div class="mono" style="margin-top:8px">本文研究了以高密度垂直结构与线性城市布局为特征的极端城市形态中人类移动的可行性。为评估智能体能否在此类新型拓扑结构中高效导航，我们开发了一种融合智能体建模、强化学习、监督学习与图神经网络的混合仿真框架。该仿真通过合成数据与高密度城市真实轨迹，捕捉了跨多垂直层级与不同密度场景的多模式交通行为。实验表明：完整的人工智能集成架构使智能体在高峰拥堵期仍能实现7.8-8.4分钟的平均通勤时间、超过89%的满意度及高于91%的可达性指数。消融实验证明，移除强化学习或图神经网络等智能模块将显著降低性能——通勤时间最多增加85%，可达性跌破70%。环境建模显示，优先采用电动模式时能耗与二氧化碳排放均处于低位。这些结果表明，通过部署自适应人工智能系统、智能基础设施与实时反馈机制，极端城市形态下的高效可持续移动是可实现的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand human mobility in extreme urban morphologies like high-density vertical structures and linear cities, this paper develops a hybrid simulation framework integrating agent-based modeling, reinforcement learning, supervised learning, and graph neural networks to assess navigation efficiency. The method models multi-modal transportation across vertical levels using synthetic and real-world data from dense cities. Experimental results show that the AI-integrated system enables agents to achieve an average commute time of 7.8–8.4 minutes, a satisfaction rate over 89%, and a reachability index above 91% even during peak congestion, while ablation studies confirm the critical role of intelligent modules, and environmental modeling indicates low energy consumption and CO₂ emissions when electric modes are prioritized.</div>
<div class="mono" style="margin-top:8px">本研究旨在探索人类在极端城市形态（如高密度垂直结构和线性城市）中的移动可行性，为此开发了一个结合基于智能体的建模、强化学习、监督学习和图神经网络的混合仿真框架来评估导航效率。该方法利用合成数据和高密度城市的真实轨迹，模拟了多垂直层次的多模式交通行为。实验结果表明，集成人工智能的架构使智能体在高峰拥堵期仍能达到平均7.8–8.4分钟的通勤时间、超过89%的满意率和高于91%的可达性指数；消融研究证实移除智能模块会显著降低性能，而环境建模显示优先使用电动模式可实现低能耗和最小化二氧化碳排放。</div>
</details>
</div>
<div class="card">
<div class="title">CleanSurvival: Automated data preprocessing for time-to-event models using reinforcement learning</div>
<div class="meta-line">Authors: Yousef Koka, David Selby, Gerrit Großmann, Sebastian Vollmer, Kathan Pandya</div>
<div class="meta-line">First: 2025-02-06T10:33:37+00:00 · Latest: 2026-01-14T20:45:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03946v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.03946v2">PDF</a> · <a href="https://github.com/datasciapps/CleanSurvival">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data preprocessing is a critical yet frequently neglected aspect of machine learning, often paid little attention despite its potentially significant impact on model performance. While automated machine learning pipelines are starting to recognize and integrate data preprocessing into their solutions for classification and regression tasks, this integration is lacking for more specialized tasks like survival or time-to-event models. As a result, survival analysis not only faces the general challenges of data preprocessing but also suffers from the lack of tailored, automated solutions in this area. To address this gap, this paper presents &#x27;CleanSurvival&#x27;, a reinforcement-learning-based solution for optimizing preprocessing pipelines, extended specifically for survival analysis. The framework can handle continuous and categorical variables, using Q-learning to select which combination of data imputation, outlier detection and feature extraction techniques achieves optimal performance for a Cox, random forest, neural network or user-supplied time-to-event model. The package is available on GitHub: https://github.com/datasciapps/CleanSurvival Experimental benchmarks on real-world datasets show that the Q-learning-based data preprocessing results in superior predictive performance to standard approaches, finding such a model up to 10 times faster than undirected random grid search. Furthermore, a simulation study demonstrates the effectiveness in different types and levels of missingness and noise in the data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CleanSurvival：基于强化学习的生存时间模型自动化数据预处理</div>
<div class="mono" style="margin-top:8px">数据预处理是机器学习中至关重要却常被忽视的环节，尽管其对模型性能可能产生显著影响，却往往未获足够重视。虽然自动化机器学习流程已开始将数据预处理纳入分类与回归任务的解决方案，但在生存分析或时间-事件模型等更专业的任务中，这一整合仍显不足。因此，生存分析不仅面临数据预处理的普遍挑战，还缺乏针对该领域的定制化自动化解决方案。为填补这一空白，本文提出&#x27;CleanSurvival&#x27;——一种基于强化学习的预处理流程优化方案，专门扩展应用于生存分析。该框架能处理连续型和分类型变量，通过Q学习选择数据填补、异常值检测与特征提取技术的组合，从而为Cox模型、随机森林、神经网络或用户自定义的时间-事件模型实现最优性能。该工具包已在GitHub发布：https://github.com/datasciapps/CleanSurvival 基于真实数据集的实验基准表明，采用Q学习的数据预处理相比标准方法具有更优的预测性能，其模型搜索速度比无导向随机网格搜索快达10倍。此外，模拟研究验证了该方法在不同类型与程度的数据缺失及噪声场景下的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the lack of automated data preprocessing solutions tailored for survival analysis, a critical gap given preprocessing&#x27;s significant impact on model performance. The method introduces &#x27;CleanSurvival&#x27;, a reinforcement learning framework that uses Q-learning to automatically optimize preprocessing pipelines—including imputation, outlier detection, and feature extraction—for time-to-event models such as Cox, random forest, and neural networks. Experimental results on real-world datasets demonstrate that this approach yields superior predictive performance compared to standard methods and finds optimal models up to 10 times faster than random grid search, with simulations confirming its effectiveness across various data missingness and noise levels.</div>
<div class="mono" style="margin-top:8px">本文的动机在于生存分析领域缺乏自动化的数据预处理解决方案，而预处理对模型性能有重要影响。方法上提出了&#x27;CleanSurvival&#x27;，这是一个基于强化学习的框架，利用Q学习自动优化预处理流程（包括数据填补、异常值检测和特征提取），适用于Cox、随机森林、神经网络等时间-事件模型。在真实数据集上的实验结果表明，该方法相比标准方法具有更优的预测性能，且找到最优模型的速度比随机网格搜索快达10倍，模拟研究还验证了其在不同类型和程度的数据缺失与噪声中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing</div>
<div class="meta-line">Authors: Yilin Bao, Ziyao He, Zayden Yang</div>
<div class="meta-line">First: 2026-01-14T20:37:26+00:00 · Latest: 2026-01-14T20:37:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09858v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09858v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OUTLINEFORGE：面向科学写作的显式状态分层强化学习框架</div>
<div class="mono" style="margin-top:8px">科学论文生成需要文档级规划和事实依据，但当前大语言模型尽管局部流畅性较强，在全局结构、输入覆盖和引用一致性方面仍存在不足。我们提出一种强化学习框架，将科学提纲构建视为分层文档结构上的长程规划问题。该方法通过结构化动作对演化提纲进行建模，使系统能逐步构建完整的科学文稿。为支持高效稳定的学习，我们引入两阶段优化流程：（1）通过部分计划的反向提纲重构以强化全局结构一致性；（2）采用前向价值引导的强化学习，其奖励函数显式建模科学正确性、语篇连贯性和引用保真度。此外，我们进一步构建了科学论文生成基准测试，评估文档规划、输入利用、参考文献忠实度、提纲组织和内容层面的事实准确性。实验结果表明，本方法在强神经基线和LLM基线上均取得持续改进，尤其在长程结构连贯性和引用可靠性方面表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of large language models in generating globally coherent and factually grounded scientific papers, this paper introduces OUTLINEFORGE, a hierarchical reinforcement learning framework that treats outline construction as a long-horizon planning problem. The method employs structured actions to model evolving outlines and uses a two-stage optimization procedure combining backward outline reconstruction for structural consistency and forward value-guided reinforcement learning with rewards for scientific correctness, coherence, and citation fidelity. Experimental results on a new benchmark for scientific paper generation demonstrate consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型在生成全局结构连贯、事实依据充分的科学论文方面的不足，本文提出了OUTLINEFORGE，这是一个将大纲构建视为长程规划问题的分层强化学习框架。该方法通过结构化动作对演化中的大纲进行建模，并采用两阶段优化过程，结合了用于确保结构一致性的逆向大纲重建和以科学正确性、篇章连贯性及引用忠实度为奖励的正向价值引导强化学习。在一个新的科学论文生成基准测试上的实验结果表明，该方法相较于强大的神经模型和LLM基线取得了持续改进，尤其在长程结构连贯性和引用可靠性方面表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">Non-Expansive Mappings in Two-Time-Scale Stochastic Approximation: Finite-Time Analysis</div>
<div class="meta-line">Authors: Siddharth Chandak</div>
<div class="meta-line">First: 2025-01-18T16:00:14+00:00 · Latest: 2026-01-14T20:17:50+00:00</div>
<div class="meta-line">Comments: Submitted to SIAM Journal on Control and Optimization</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.10806v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.10806v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two-time-scale stochastic approximation algorithms are iterative methods used in applications such as optimization, reinforcement learning, and control. Finite-time analysis of these algorithms has primarily focused on fixed point iterations where both time-scales have contractive mappings. In this work, we broaden the scope of such analyses by considering settings where the slower time-scale has a non-expansive mapping. For such algorithms, the slower time-scale can be viewed as a stochastic inexact Krasnoselskii-Mann iteration. We also study a variant where the faster time-scale has a projection step which leads to non-expansiveness in the slower time-scale. We show that the last-iterate mean square residual error for such algorithms decays at a rate $O(1/k^{1/4-ε})$, where $ε&gt;0$ is arbitrarily small. We further establish almost sure convergence of iterates to the set of fixed points. We demonstrate the applicability of our framework by applying our results to minimax optimization, linear stochastic approximation, and Lagrangian optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双时间尺度随机逼近中的非扩张映射：有限时间分析</div>
<div class="mono" style="margin-top:8px">双时间尺度随机逼近算法是广泛应用于优化、强化学习和控制等领域的迭代方法。现有有限时间分析主要针对两时间尺度均为压缩映射的不动点迭代。本研究拓展了此类分析框架，考虑慢时间尺度具有非扩张映射的情形。此类算法中，慢时间尺度可视为随机非精确Krasnoselskii-Mann迭代。同时研究了快时间尺度引入投影步骤导致慢时间尺度非扩张性的变体算法。证明此类算法的末次迭代均方残差以$O(1/k^{1/4-ε})$速率衰减（$ε&gt;0$任意小），并建立迭代序列几乎必然收敛至不动点集的理论。通过应用于极小极大优化、线性随机逼近及拉格朗日优化问题，验证了框架的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to extend finite-time analysis of two-time-scale stochastic approximation algorithms beyond the restrictive assumption of contractive mappings, focusing instead on scenarios where the slower time-scale employs a non-expansive mapping, which arises in applications like minimax optimization and Lagrangian optimization. The method involves modeling the slower time-scale as a stochastic inexact Krasnoselskii-Mann iteration and also examines a variant with a projection step in the faster time-scale that induces non-expansiveness. The main experimental results show that the last-iterate mean square residual error decays at a rate of O(1/k^{1/4-ε}) for arbitrarily small ε &gt; 0, and the iterates converge almost surely to the set of fixed points, as demonstrated in applications including minimax optimization, linear stochastic approximation, and Lagrangian optimization.</div>
<div class="mono" style="margin-top:8px">本文的动机在于将双时间尺度随机逼近算法的有限时间分析从压缩映射的限制性假设扩展到更一般的情形，重点关注较慢时间尺度采用非扩张映射的场景，这在极小极大优化和拉格朗日优化等应用中常见。方法上，将较慢时间尺度建模为随机不精确的Krasnoselskii-Mann迭代，并研究了在较快时间尺度引入投影步骤以诱导非扩张性的变体。主要实验结果表明，最后迭代的均方残差误差以O(1/k^{1/4-ε})的速率衰减（其中ε &gt; 0可任意小），且迭代几乎必然收敛到不动点集，这在极小极大优化、线性随机逼近和拉格朗日优化等应用中得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Eluder dimension: localise it!</div>
<div class="meta-line">Authors: Alireza Bakhtiari, Alex Ayoub, Samuel Robertson, David Janz, Csaba Szepesvári</div>
<div class="meta-line">First: 2026-01-14T19:35:46+00:00 · Latest: 2026-01-14T19:35:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09825v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09825v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迷惑者维度：局部化方法</div>
<div class="mono" style="margin-top:8px">我们建立了广义线性模型类迷惑者维度的下界，表明基于标准迷惑者维度的分析无法得到一阶遗憾界。为此，我们引入了迷惑者维度的局部化方法；该分析不仅立即恢复并改进了伯努利多臂老虎机问题的经典结果，还首次为具有有界累积回报的有限时域强化学习任务提供了真正的一阶遗憾界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of standard eluder dimension analysis in achieving first-order regret bounds for generalized linear models, this paper introduces a localized eluder dimension method. The approach refines the analysis by focusing on localized complexity, which enables the recovery and improvement of classic results for Bernoulli bandits and, for the first time, yields genuine first-order regret bounds in finite-horizon reinforcement learning with bounded cumulative returns.</div>
<div class="mono" style="margin-top:8px">针对广义线性模型标准eluder维度分析无法获得一阶遗憾界的局限性，本文提出了一种局部化eluder维度方法。该方法通过聚焦于局部复杂性来改进分析，不仅恢复并改进了伯努利多臂老虎机问题的经典结果，而且首次在有界累积回报的有限时域强化学习任务中实现了真正的一阶遗憾界。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization</div>
<div class="meta-line">Authors: Frank Röder, Jan Benad, Manfred Eppe, Pradeep Kr. Banerjee</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-08-27T22:02:56+00:00 · Latest: 2026-01-14T17:50:26+00:00</div>
<div class="meta-line">Comments: 31 pages, 4 figures, accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20294v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.20294v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI&#x27;s latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向零样本泛化的情境世界模型中动态对齐的潜在想象</div>
<div class="mono" style="margin-top:8px">现实世界的强化学习需要在不进行昂贵重训练的情况下适应未见过的环境条件。情境马尔可夫决策过程（cMDP）对此挑战进行建模，但现有方法通常需要显式情境变量（如摩擦力、重力），当情境变量为潜在或难以测量时，其应用受到限制。我们提出动态对齐潜在想象（DALI），这是一个集成在Dreamer架构中的框架，通过智能体与环境交互推断潜在情境表征。通过训练自监督编码器预测前向动态，DALI生成可操作的表征来调节世界模型与策略，从而桥接感知与控制。我们从理论上证明该编码器对于高效情境推断和鲁棒泛化至关重要。DALI的潜在空间支持反事实一致性：扰动重力编码维度会以物理合理的方式改变想象推演轨迹。在具有挑战性的cMDP基准测试中，DALI相比无情境感知基线取得显著提升，在外推任务中常超越情境感知基线，实现了对未见情境变化的零样本泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of zero-shot generalization in reinforcement learning, where agents must adapt to unseen environmental conditions without retraining. The authors propose Dynamics-Aligned Latent Imagination (DALI), a framework integrated into the Dreamer architecture that infers latent context representations from agent interactions, using a self-supervised encoder trained to predict forward dynamics. This method theoretically ensures efficient context inference and robust generalization, with experiments on cMDP benchmarks showing that DALI outperforms context-unaware baselines and often exceeds context-aware ones in extrapolation tasks, enabling effective zero-shot adaptation to new contextual variations.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的零样本泛化挑战，即智能体需在不重新训练的情况下适应未见的环境条件。作者提出了动态对齐潜在想象（DALI）框架，该框架集成于Dreamer架构中，通过从智能体与环境的交互中推断潜在上下文表示，并利用自监督编码器预测前向动态。该方法在理论上保证了高效的上下文推断和鲁棒泛化能力，在cMDP基准测试中，DALI的表现显著优于无上下文感知的基线方法，并在外推任务中常超越有上下文感知的基线，实现了对新上下文变化的有效零样本适应。</div>
</details>
</div>
<div class="card">
<div class="title">From Prompt to Protocol: Fast Charging Batteries with Large Language Models</div>
<div class="meta-line">Authors: Ge Lei, Ferran Brosa Planella, Sterling G. Baird, Samuel J. Cooper</div>
<div class="meta-line">First: 2026-01-14T16:58:20+00:00 · Latest: 2026-01-14T16:58:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09626v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09626v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which simply writes an explicit function for the current and its scalar parameters. Across our case studies, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search. In a realistic fast charging scenario, both P2O and P2P yield around a 4.2 percent improvement in state of health (capacity retention based health metric under fast charging cycling) over a state-of-the-art multi-step constant current (CC) baseline, with P2P achieving this under matched evaluation budgets (same number of protocol evaluations). These results demonstrate that LLMs can expand the space of protocol functional forms, incorporate language-based constraints, and enable efficient optimization in high cost experimental settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从提示到协议：利用大语言模型实现电池快速充电</div>
<div class="mono" style="margin-top:8px">高效优化电池充电协议具有挑战性，因为每次评估都耗时、昂贵且不可微分。现有方法大多通过严格限制协议搜索空间来应对这一难题，但这限制了可探索协议的多样性，阻碍了更高性能解决方案的发现。我们提出了两种基于大语言模型的无梯度闭环方法：提示到优化器（P2O）和提示到协议（P2P）。P2O利用大语言模型生成小型神经网络协议代码，并通过内部循环进行训练；P2P则直接编写电流及其标量参数的显式函数。案例研究表明，大语言模型引导的P2O在性能上超越了贝叶斯优化、进化算法和随机搜索设计的神经网络。在现实快速充电场景中，P2O和P2P相比先进的多步恒流基线方案，在健康状态（基于快速循环容量保持的健康指标）上均实现了约4.2%的提升，其中P2P在相同评估预算下达成该成果。这些结果表明，大语言模型能够拓展协议函数形式空间，整合基于语言的约束条件，并在高成本实验环境中实现高效优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the slow, costly, and non-differentiable nature of battery charging protocol evaluations, which forces existing methods to constrain search spaces and limit solution diversity, this paper introduces two gradient-free, large language model (LLM)-driven closed-loop methods: Prompt-to-Optimizer (P2O), where an LLM generates code for small neural-network protocols that are then trained, and Prompt-to-Protocol (P2P), where an LLM directly writes an explicit function with scalar parameters. Experimentally, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search; in a realistic fast charging scenario, both P2O and P2P achieve approximately a 4.2% improvement in state of health over a state-of-the-art multi-step constant current baseline, with P2P matching evaluation budgets. These results show that LLMs can expand protocol functional forms, incorporate language constraints, and enable efficient optimization in high-cost experimental settings.</div>
<div class="mono" style="margin-top:8px">本文的动机在于电池充电协议评估缓慢、成本高且不可微分，这迫使现有方法限制搜索空间并降低解决方案的多样性。为此，研究提出了两种无梯度、由大语言模型驱动的闭环方法：提示到优化器，即大语言模型生成小型神经网络协议代码并进行训练；以及提示到协议，即大语言模型直接编写带有标量参数的显式函数。实验结果表明，大语言模型引导的提示到优化器在性能上优于贝叶斯优化、进化算法和随机搜索设计的神经网络；在现实快速充电场景中，两种方法均比先进的多步恒流基线在健康状态上提升了约4.2%，且提示到协议在相同评估预算下实现了这一改进。这些发现证明大语言模型能够扩展协议函数形式，整合语言约束，并在高成本实验环境中实现高效优化。</div>
</details>
</div>
<div class="card">
<div class="title">DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing</div>
<div class="meta-line">Authors: Qian Cao, Yahui Liu, Wei Bi, Yi Zhao, Ruihua Song, Xiting Wang, Ruiming Tang, Guorui Zhou, Han Li</div>
<div class="meta-line">First: 2026-01-14T16:30:20+00:00 · Latest: 2026-01-14T16:30:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09609v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DPWriter：基于多样化规划分支的强化学习创意写作框架</div>
<div class="mono" style="margin-top:8px">基于强化学习（RL）的大型语言模型（LLM）增强方法常导致输出多样性降低，削弱了其在创意写作等开放式任务中的实用性。现有方法缺乏引导多样化探索的显式机制，往往优先考虑优化效率与性能而忽视多样性。本文提出一种围绕半结构化长链思维（CoT）构建的强化学习框架，将生成过程分解为显式规划的中间步骤。我们引入一种多样化规划分支方法，基于多样性变化在规划阶段策略性地引入分岔，并结合群体感知的多样性奖励机制以激励差异化轨迹。创意写作基准测试表明，该方法在保持生成质量的同时显著提升输出多样性，持续优于现有基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that reinforcement learning (RL) applied to large language models (LLMs) often reduces output diversity, which is detrimental for open-ended tasks like creative writing. To address this, the authors propose DPWriter, an RL framework that uses a semi-structured long Chain-of-Thought (CoT) to decompose writing into planned intermediate steps. The method incorporates a Diverse Planning Branching technique to strategically introduce divergence during planning based on diversity variation, along with a group-aware diversity reward to promote distinct generation trajectories. Experiments on creative writing benchmarks show that this approach significantly enhances output diversity while maintaining generation quality, consistently outperforming existing baseline methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到强化学习应用于大语言模型时通常会降低输出多样性，这对创意写作等开放式任务不利。为此，作者提出了DPWriter，一个基于半结构化长思维链的强化学习框架，将写作过程分解为明确规划的中间步骤。该方法引入了多样化规划分支技术，根据多样性变化在规划阶段策略性地引入分歧，并采用群体感知的多样性奖励来鼓励不同的生成轨迹。在创意写作基准测试上的实验结果表明，该方法在保持生成质量的同时显著提高了输出多样性，一致优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Improving CMA-ES Convergence Speed, Efficiency, and Reliability in Noisy Robot Optimization Problems</div>
<div class="meta-line">Authors: Russell M. Martin, Steven H. Collins</div>
<div class="meta-line">First: 2026-01-14T16:12:18+00:00 · Latest: 2026-01-14T16:12:18+00:00</div>
<div class="meta-line">Comments: This is the authors&#x27; final accepted manuscript (post-peer-review, pre-publication). It has been accepted for publication in Evolutionary Computation on 12 Jan 2026. For associated code, see https://github.com/RussellMMartin/AS-CMA-ES</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09594v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09594v1">PDF</a> · <a href="https://github.com/RussellMMartin/AS-CMA-ES">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Experimental robot optimization often requires evaluating each candidate policy for seconds to minutes. The chosen evaluation time influences optimization because of a speed-accuracy tradeoff: shorter evaluations enable faster iteration, but are also more subject to noise. Here, we introduce a supplement to the CMA-ES optimization algorithm, named Adaptive Sampling CMA-ES (AS-CMA), which assigns sampling time to candidates based on predicted sorting difficulty, aiming to achieve consistent precision. We compared AS-CMA to CMA-ES and Bayesian optimization using a range of static sampling times in four simulated cost landscapes. AS-CMA converged on 98% of all runs without adjustment to its tunable parameter, and converged 24-65% faster and with 29-76% lower total cost than each landscape&#x27;s best CMA-ES static sampling time. As compared to Bayesian optimization, AS-CMA converged more efficiently and reliably in complex landscapes, while in simpler landscapes, AS-CMA was less efficient but equally reliable. We deployed AS-CMA in an exoskeleton optimization experiment and found the optimizer&#x27;s behavior was consistent with expectations. These results indicate that AS-CMA can improve optimization efficiency in the presence of noise while minimally affecting optimization setup complexity and tuning requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提升CMA-ES在噪声机器人优化问题中的收敛速度、效率与可靠性</div>
<div class="mono" style="margin-top:8px">机器人实验优化通常需要对每个候选策略进行数秒至数分钟的评估。由于速度-精度权衡的存在，评估时长的选择会影响优化效果：较短评估可加速迭代，但更易受噪声干扰。本文提出一种CMA-ES优化算法的增强方法——自适应采样CMA-ES（AS-CMA），其根据预测的排序难度为候选方案分配采样时间，旨在实现稳定精度。我们在四种模拟代价场景中，将AS-CMA与采用固定采样时长的CMA-ES及贝叶斯优化进行对比。AS-CMA在98%的实验中无需调整可调参数即实现收敛，且收敛速度比各场景最优固定采样CMA-ES快24-65%，总代价降低29-76%。相较于贝叶斯优化，AS-CMA在复杂场景中收敛效率更高、可靠性更强；在简单场景中效率稍低但可靠性相当。在外骨骼优化实验中部署AS-CMA后，其表现符合预期。结果表明，AS-CMA能在噪声环境下提升优化效率，同时几乎不增加优化设置的复杂性和调参需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the speed-accuracy trade-off in noisy robot policy evaluation, where shorter tests are faster but noisier, this paper introduces Adaptive Sampling CMA-ES (AS-CMA) to dynamically allocate evaluation time based on predicted ranking difficulty. The method was tested against CMA-ES with static sampling and Bayesian optimization across four simulated landscapes. Key results show AS-CMA achieved 98% convergence without parameter tuning, converging 24-65% faster and with 29-76% lower total cost than the best static CMA-ES, and outperformed Bayesian optimization in complex landscapes while matching reliability in simpler ones, with consistent behavior validated in a real exoskeleton experiment.</div>
<div class="mono" style="margin-top:8px">针对机器人策略评估中因评估时间短导致噪声大、评估时间长则迭代慢的权衡问题，本文提出了自适应采样CMA-ES（AS-CMA），根据预测的排序难度动态分配评估时间。该方法在四个模拟成本环境中与静态采样CMA-ES和贝叶斯优化进行了比较。主要实验结果表明，AS-CMA在无需调整参数的情况下实现了98%的收敛率，比最佳静态CMA-ES收敛速度快24-65%、总成本低29-76%；在复杂环境中比贝叶斯优化更高效可靠，在简单环境中效率稍低但可靠性相当，并在外骨骼优化实验中验证了其行为符合预期。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-01-14T15:49:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：面向科学复合图表的视觉条件化面板检测与描述生成</div>
<div class="mono" style="margin-top:8px">科学复合图表将多个带标签的面板整合为单张图像，但实际流程中的描述文本常缺失或仅提供图表级摘要，导致面板级理解困难。本文提出FigEx2——一种视觉条件化框架，可直接从复合图表中定位面板并生成面板级描述。为缓解开放式描述中多样化措辞的影响，我们引入噪声感知门控融合模块，自适应过滤词元级特征以稳定检测查询空间。此外，采用结合监督学习与强化学习的阶段式优化策略，利用基于CLIP的对齐奖励和基于BERTScore的语义奖励，确保严格的多模态一致性。为提供高质量监督数据，我们构建了面板级定位基准数据集BioSci-Fig-Cap，以及跨物理与化学学科的测试集。实验表明：FigEx2在检测任务上达到0.726 mAP@0.5:0.95的优异性能，在METEOR和BERTScore指标上分别显著超越Qwen3-VL-8B模型0.51和0.24。值得注意的是，FigEx2在未经微调的情况下，对分布外科学领域展现出卓越的零样本迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the challenge of understanding individual panels within scientific compound figures, as existing captions are often missing or only provide figure-level summaries. The proposed method, FigEx2, is a visual-conditioned framework that localizes panels and generates panel-wise captions directly from the figure, incorporating a noise-aware gated fusion module to stabilize detection queries and a staged optimization strategy combining supervised and reinforcement learning with CLIP and BERTScore rewards for multimodal consistency. Main experimental results show that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms the Qwen3-VL-8B model by 0.51 in METEOR and 0.24 in BERTScore, while also demonstrating strong zero-shot transferability to out-of-distribution scientific domains without fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于科学复合图中各子图的独立理解存在困难，因为现有标注常缺失或仅提供图级摘要。提出的方法FigEx2是一个视觉条件框架，可直接从复合图中定位子图并生成子图级描述，其引入了噪声感知门控融合模块以稳定检测查询空间，并采用结合监督学习与强化学习的阶段优化策略，利用基于CLIP的对齐和基于BERTScore的语义奖励来确保严格的多模态一致性。主要实验结果表明，FigEx2在检测上取得了0.726 mAP@0.5:0.95的优异性能，在METEOR和BERTScore分数上分别显著超越Qwen3-VL-8B模型0.51和0.24，同时在没有微调的情况下展现出对分布外科学领域出色的零样本迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks</div>
<div class="meta-line">Authors: Ningzhe Shi, Yiqing Zhou, Ling Liu, Jinglin Shi, Yihao Wu, Haiwei Shi, Hanxiao Yu</div>
<div class="meta-line">First: 2025-08-16T15:29:59+00:00 · Latest: 2026-01-14T15:28:17+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TMC</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12079v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.12079v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrated sensing and communication (ISAC) can enhance artificial intelligence-generated content (AIGC) networks by providing efficient sensing and transmission. Existing AIGC services usually assume that the accuracy of the generated content can be ensured, given accurate input data and prompt, thus only the content generation quality (CGQ) is concerned. However, it is not applicable in ISAC-based AIGC networks, where content generation is based on inaccurate sensed data. Moreover, the AIGC model itself introduces generation errors, which depend on the number of generating steps (i.e., computing resources). To assess the quality of experience of ISAC-based AIGC services, we propose a content accuracy and quality aware service assessment metric (CAQA). Since allocating more resources to sensing and generating improves content accuracy but may reduce communication quality, and vice versa, this sensing-generating (computing)-communication three-dimensional resource tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution space that grows exponentially with the number of users. To solve the CAQA-AIGC problem with low complexity, a linear programming (LP) guided deep reinforcement learning (DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the LP-guided approach and the action filter, LPDRL-F can transform the original three-dimensional solution space to two dimensions, reducing complexity while improving the learning performance of DRL. Simulations show that compared to existing DRL and generative diffusion model (GDM) algorithms without LP, LPDRL-F converges faster and finds better resource allocation solutions, improving AvgCAQA by more than 10%. With LPDRL-F, CAQA-AIGC can achieve an improvement in AvgCAQA of more than 50% compared to existing schemes focusing solely on CGQ.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于线性规划引导深度强化学习的ISAC驱动AIGC网络内容准确性与质量感知资源分配</div>
<div class="mono" style="margin-top:8px">集成感知与通信（ISAC）可通过提供高效感知与传输增强人工智能生成内容（AIGC）网络。现有AIGC服务通常假设在输入数据与提示准确时能确保生成内容准确性，故仅关注内容生成质量（CGQ）。然而这在基于ISAC的AIGC网络中并不适用，因其内容生成基于不精确的感知数据。此外，AIGC模型本身会引入取决于生成步数（即计算资源）的生成误差。为评估基于ISAC的AIGC服务的体验质量，本文提出内容准确性与质量感知服务评估指标（CAQA）。由于为感知与生成分配更多资源可提升内容准确性但可能降低通信质量，反之亦然，必须优化感知-生成（计算）-通信三维资源权衡，以最大化所有AIGC用户的平均CAQA（AvgCAQA）。该问题属于NP难问题，其解空间随用户数呈指数增长。为低复杂度求解CAQA-AIGC问题，提出一种带动作滤波器的线性规划（LP）引导深度强化学习（DRL）算法（LPDRL-F）。通过LP引导方法与动作滤波器，LPDRL-F可将原始三维解空间降至二维，在降低复杂度的同时提升DRL学习性能。仿真表明，相较于现有无LP的DRL与生成扩散模型（GDM）算法，LPDRL-F收敛更快且能获得更优资源分配方案，将AvgCAQA提升超10%。采用LPDRL-F后，CAQA-AIGC相较于仅关注CGQ的现有方案可实现AvgCAQA超50%的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing resource allocation in integrated sensing and communication (ISAC)-driven artificial intelligence-generated content (AIGC) networks, where content generation relies on inaccurate sensed data and model errors, making traditional quality-only metrics insufficient. To assess user experience, the authors propose a content accuracy and quality aware (CAQA) metric and formulate an NP-hard problem to maximize average CAQA across users by balancing sensing, computing, and communication resources. They introduce a low-complexity linear programming-guided deep reinforcement learning algorithm with an action filter (LPDRL-F), which reduces the solution space dimensionality to improve learning efficiency. Experimental results demonstrate that LPDRL-F converges faster and achieves over 10% higher average CAQA compared to baseline DRL and generative diffusion model methods, and over 50% improvement versus schemes focusing only on content generation quality.</div>
<div class="mono" style="margin-top:8px">本文针对集成感知与通信驱动的AI生成内容网络中的资源分配优化问题，传统方法仅关注生成质量，但在此网络中内容生成依赖于不准确的感知数据和模型误差。为此，作者提出了一种内容准确性与质量感知的服务评估指标，并构建了最大化用户平均CAQA的NP难问题，需权衡感知、计算和通信资源。为解决该问题，他们设计了一种基于线性规划引导的深度强化学习算法，通过动作过滤器降低解空间维度以提升学习性能。仿真结果表明，该算法相比现有无线性规划引导的深度强化学习和生成扩散模型方法收敛更快，平均CAQA提升超过10%，且相较于仅关注生成质量的方案，改进幅度超过50%。</div>
</details>
</div>
<div class="card">
<div class="title">GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents</div>
<div class="meta-line">Authors: Chen Chen, Jiawei Shao, Dakuan Lu, Haoyi Hu, Xiangcheng Liu, Hantao Yao, Wu Liu</div>
<div class="meta-line">First: 2026-01-14T14:27:28+00:00 · Latest: 2026-01-14T14:27:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09770v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09770v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GUI-Eyes：GUI智能体中视觉定位的工具增强感知框架</div>
<div class="mono" style="margin-top:8px">视觉语言模型与强化学习的进展推动了GUI自动化的发展，但现有方法多依赖静态单次视觉输入与被动感知，缺乏自适应判断观察时机、必要性及方式的能力。本文提出GUI-Eyes——面向GUI任务的主动视觉感知强化学习框架。为获取更具信息量的观测，智能体通过两阶段推理学习策略性决策视觉工具（如裁剪、缩放）的调用时机与方式。为此，我们设计了渐进式感知策略，将决策分解为由双层策略协调的粗粒度探索与细粒度定位。同时，针对工具使用特性构建了空间连续奖励函数，融合位置邻近度与区域重叠度以提供密集监督，缓解GUI环境中常见的奖励稀疏问题。在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅用3千标注样本即实现44.8%的定位准确率，显著超越监督学习与强化学习基线方法。结果表明：通过分阶段策略推理与细粒度奖励反馈实现的工具感知型主动感知，对构建鲁棒且数据高效的GUI智能体至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static, one-shot visual perception in existing GUI automation methods, this paper introduces GUI-Eyes, a reinforcement learning framework that enables agents to actively decide when and how to use visual tools like cropping or zooming through a two-stage reasoning process. The method employs a progressive perception strategy with a two-level policy for coarse exploration and fine-grained grounding, alongside a spatially continuous reward function that combines location proximity and region overlap to address reward sparsity. Experimental results on the ScreenSpot-Pro benchmark show that GUI-Eyes-3B achieves 44.8% grounding accuracy with only 3k labeled samples, outperforming both supervised and RL-based baselines, demonstrating the effectiveness of tool-aware active perception for robust and data-efficient GUI agents.</div>
<div class="mono" style="margin-top:8px">针对现有GUI自动化方法中静态、一次性视觉感知的不足，本文提出了GUI-Eyes，这是一个强化学习框架，使智能体能够通过两阶段推理过程主动决定何时以及如何使用裁剪或缩放等视觉工具。该方法采用渐进式感知策略，通过双层策略进行粗略探索和细粒度定位，并设计了结合位置邻近性和区域重叠的空间连续奖励函数以缓解奖励稀疏性问题。在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅使用3k标注样本就实现了44.8%的定位准确率，显著优于监督学习和基于强化学习的基线方法，证明了工具感知的主动感知对于构建鲁棒且数据高效的GUI智能体的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">What Can RL Bring to VLA Generalization? An Empirical Study</div>
<div class="meta-line">Authors: Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-26T10:19:26+00:00 · Latest: 2026-01-14T14:23:30+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19789v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.19789v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rlvla.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习能为视觉语言动作模型泛化带来什么？一项实证研究</div>
<div class="mono" style="margin-top:8px">大型视觉语言动作模型在具身人工智能领域展现出巨大潜力，但其主要依赖监督微调的训练方式因对分布偏移下的误差累积敏感而限制了泛化能力。强化学习通过试错优化任务目标为突破这些限制提供了路径，然而与监督微调相比，其对视觉语言动作模型的具体泛化效益尚缺乏系统认知。为此，本研究建立了评估视觉语言动作模型泛化能力的综合基准，并系统探究了强化学习微调在视觉、语义和执行等多个维度的影响。大量实验表明，强化学习微调（尤其是PPO算法）在语义理解和执行鲁棒性方面显著优于监督微调，同时保持相当的视觉鲁棒性。我们确定PPO是比DPO、GRPO等基于大语言模型的方法更有效的视觉语言动作模型强化学习算法。此外，我们开发了一套适用于视觉语言动作模型的高效PPO训练方案，并验证了其提升模型泛化能力的实用价值。项目页面详见 https://rlvla.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the limited generalization of large Vision-Language-Action (VLA) models under distribution shifts when trained primarily with supervised fine-tuning (SFT), prompting an investigation into whether reinforcement learning (RL) can offer superior generalization benefits. The method involves creating a comprehensive benchmark to evaluate VLA generalization and systematically applying RL fine-tuning, notably with the PPO algorithm, across visual, semantic, and execution dimensions. The main experimental results show that RL fine-tuning, especially with PPO, significantly improves semantic understanding and execution robustness compared to SFT, while maintaining similar visual robustness, and it outperforms other RL methods like DPO and GRPO, with a simple training recipe developed for practical efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型视觉-语言-动作模型在监督微调训练下因分布偏移而泛化能力有限的问题，探索强化学习是否能带来更好的泛化优势。方法上，研究建立了一个全面的基准来评估VLA泛化能力，并系统性地应用强化学习微调，特别是PPO算法，覆盖视觉、语义和执行维度。主要实验结果表明，与监督微调相比，强化学习微调（尤其是PPO）显著提升了语义理解和执行鲁棒性，同时保持了相当的视觉鲁棒性，且PPO优于DPO和GRPO等其他强化学习方法，并开发了一个简单高效的训练方案用于实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering</div>
<div class="meta-line">Authors: Chenliang Zhang, Lin Wang, Yuanyuan Lu, Yusheng Qi, Kexin Wang, Peixu Hou, Wenshi Chen</div>
<div class="meta-line">First: 2025-08-14T04:37:56+00:00 · Latest: 2026-01-14T13:50:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10337v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10337v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper describes the solutions of the Dianping-Trust-Safety team for the META CRAG-MM challenge. The challenge requires building a comprehensive retrieval-augmented generation system capable for multi-modal multi-turn question answering. The competition consists of three tasks: (1) answering questions using structured data retrieved from an image-based mock knowledge graph, (2) synthesizing information from both knowledge graphs and web search results, and (3) handling multi-turn conversations that require context understanding and information aggregation from multiple sources. For Task 1, our solution is based on the vision large language model, enhanced by supervised fine-tuning with knowledge distilled from GPT-4.1. We further applied curriculum learning strategies to guide reinforcement learning, resulting in improved answer accuracy and reduced hallucination. For Task 2 and Task 3, we additionally leveraged web search APIs to incorporate external knowledge, enabling the system to better handle complex queries and multi-turn conversations. Our approach achieved 1st place in Task 1 with a significant lead of 52.38%, and 3rd place in Task 3, demonstrating the effectiveness of the integration of curriculum learning with reinforcement learning in our training pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习的课程学习方法：利用RAG实现多模态问答</div>
<div class="mono" style="margin-top:8px">本文介绍了大众点评-信任与安全团队在META CRAG-MM挑战赛中的解决方案。该挑战要求构建一个能够进行多模态多轮问答的检索增强生成系统。竞赛包含三项任务：(1) 利用基于图像的模拟知识图谱检索结构化数据回答问题，(2) 综合知识图谱与网络搜索结果的信息，(3) 处理需要上下文理解及多源信息聚合的多轮对话。针对任务一，我们基于视觉大语言模型，通过GPT-4.1知识蒸馏的监督微调进行增强，并采用课程学习策略指导强化学习，从而提升答案准确性并减少幻觉。对于任务二和三，我们额外引入网络搜索API整合外部知识，使系统能更好地处理复杂查询和多轮对话。我们的方法在任务一中以52.38%的显著优势获得第一名，在任务三中获得第三名，证明了课程学习与强化学习在训练流程中融合的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the META CRAG-MM challenge to build a robust multimodal question-answering system, this paper proposes a curriculum learning-guided reinforcement learning approach integrated with retrieval-augmented generation (RAG). The method employs a vision large language model fine-tuned with GPT-4.1 distilled knowledge for image-based knowledge graph queries, and augments it with web search APIs for complex and multi-turn tasks. Experimental results show the system achieved first place in Task 1 with a 52.38% performance lead and third place in Task 3, validating reduced hallucination and improved accuracy.</div>
<div class="mono" style="margin-top:8px">本文针对META CRAG-MM挑战中构建多模态问答系统的需求，提出了一种结合课程学习与强化学习的检索增强生成方法。该方法基于视觉大语言模型，利用GPT-4.1蒸馏知识进行监督微调，并通过课程学习策略优化强化训练过程，同时引入网络搜索API处理复杂查询和多轮对话。实验结果表明，该系统在任务一中以52.38%的显著优势获得第一名，在任务三中获得第三名，有效提升了答案准确性并减少了幻觉生成。</div>
</details>
</div>
<div class="card">
<div class="title">SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning</div>
<div class="meta-line">Authors: Caijun Xu, Changyi Xiao, Zhongyuan Peng, Xinrun Wang, Yixin Cao</div>
<div class="meta-line">First: 2026-01-08T10:42:04+00:00 · Latest: 2026-01-14T13:43:32+00:00</div>
<div class="meta-line">Comments: 19 pages,5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04809v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04809v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model&#x27;s capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCALER：用于推理的合成可扩展自适应学习环境</div>
<div class="mono" style="margin-top:8px">强化学习为提升大语言模型的推理能力提供了原则性方法，但其有效性依赖于随模型演化保持信息量的训练信号。实践中，当任务难度与模型能力不匹配，或训练被少数重复问题模式主导时，强化学习进展常会放缓。为协同解决这些问题，我们提出SCALER（用于推理的合成可扩展自适应学习环境），该框架通过自适应环境设计维持有效的学习信号。SCALER引入可扩展的合成流程，将真实编程问题转化为具有可控难度和无限实例生成的可验证推理环境，使强化学习训练能突破有限数据集的限制，同时保持强正确性保证。在此基础上，SCALER进一步采用自适应多环境强化学习策略，动态调整实例难度并筛选活跃环境集合，以追踪模型能力边界并保持分布多样性。这种协同适应机制避免了奖励稀疏性，减轻了对狭窄任务模式的过拟合，并支持训练过程中的持续改进。大量实验表明，SCALER在多样化推理基准测试中始终优于基于数据集的强化学习基线，并展现出更稳定、更长周期的训练动态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge that reinforcement learning (RL) for improving large language model reasoning often suffers from diminishing training signals when task difficulty mismatches model capability or when training data lacks diversity. To address this, the method introduces SCALER, a framework that creates a synthetic, scalable learning environment by converting real-world programming problems into verifiable reasoning tasks with controllable difficulty and unlimited instance generation, coupled with an adaptive multi-environment RL strategy that dynamically adjusts difficulty and curates environments to match the model&#x27;s evolving capabilities. Experimental results demonstrate that SCALER consistently outperforms dataset-based RL baselines across various reasoning benchmarks, showing more stable and sustained training progress.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，用于提升大语言模型推理能力的强化学习常因任务难度与模型能力不匹配或训练数据缺乏多样性而导致训练信号减弱。为解决此问题，方法提出了SCALER框架，它通过将现实编程问题转化为可验证、难度可控且可无限生成实例的推理环境，构建了一个合成的可扩展学习环境，并结合自适应多环境强化学习策略，动态调整难度并筛选环境以匹配模型能力演进。实验结果表明，SCALER在多种推理基准测试中持续优于基于数据集的强化学习基线，展现出更稳定、持久的训练动态。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling for Strategic Multiagent Settings</div>
<div class="meta-line">Authors: Georgios Chalkiadakis, Charilaos Akasiadis, Gerasimos Koresis, Stergios Plataniotis, Leonidas Bakopoulos</div>
<div class="meta-line">First: 2025-11-13T17:06:56+00:00 · Latest: 2026-01-14T13:06:54+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10501v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.10501v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper provides a comprehensive review of mainly GNN, DRL, and PTM methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) ML methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of GNN. Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of RL, and in particular that of multiagent deep reinforcement learning. Single-agent deep RL has been widely used for decision making in demanding game settings. Its application in multiagent settings though is hindered due to, e.g., varying relationships between agents, and non-stationarity of the environment. We describe existing relevant game theoretic solution concepts, and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes probabilistic topic modeling (PTM) in domains other than that of document analysis and classification. Finally, we identify certain open challenges -- specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向战略多智能体场景的图神经网络、深度强化学习与概率主题建模方法综述</div>
<div class="mono" style="margin-top:8px">本文系统综述了图神经网络（GNN）、深度强化学习（DRL）与概率主题建模（PTM）方法在战略多智能体场景中的应用潜力。研究聚焦于：（1）适用于战略对手建模的未知模型结构发现方法；（2）将机器学习方法与博弈论概念结合，避免依赖现实场景中常失效的假设（如共同先验假设与自利假说）。我们分析了方法处理不确定性与异质性（现实应用中的常见特征）的能力及可扩展性。针对多智能体场景中关系与交互的建模难题，我们主张采用GNN方法——这类基于图结构数据的模型在节点分类与链接预测等任务中已展现强大能力。随后，我们综述了强化学习领域，特别是多智能体深度强化学习的进展。单智能体深度强化学习已在复杂博弈决策中广泛应用，但其在多智能体场景的应用受限于智能体间动态关系与环境非平稳性等因素。我们阐述了现有博弈论解概念，并探讨公平性、稳定性等特性。此外，本文还梳理了概率主题建模在文档分析以外领域的应用文献。最后，我们指出若干开放挑战：需（1）适应非平稳环境；（2）平衡稳定性与适应性；（3）应对不确定性与异质性；（4）保障可扩展性与解的可处理性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to model strategic multiagent interactions in complex, real-world environments where traditional game theory assumptions like the Common Prior Assumption and Self-Interest Hypothesis often fail. The method involves a comprehensive review and proposed integration of Graph Neural Networks (GNNs) for capturing agent relationships, Deep Reinforcement Learning (DRL) for decision-making, and Probabilistic Topic Modeling (PTM) for uncovering latent structures, all aimed at handling uncertainty, heterogeneity, and scalability. The main experimental results, derived from literature analysis, highlight GNNs&#x27; effectiveness in tasks like node classification and link prediction, while noting challenges in multiagent DRL due to non-stationarity and varying relationships, with open issues identified in adapting to dynamic environments and balancing stability with adaptation.</div>
<div class="mono" style="margin-top:8px">本文的动机源于需要在复杂现实环境中对战略多智能体交互进行建模，而传统博弈论假设（如共同先验假设和自利假设）在此类场景中常不成立。方法上，论文全面综述并提议整合图神经网络（GNN）以捕捉智能体关系、深度强化学习（DRL）用于决策、以及概率主题模型（PTM）用于揭示潜在结构，旨在处理不确定性、异质性和可扩展性。基于文献分析的主要实验结果表明，GNN在节点分类和链接预测等任务中表现有效，但多智能体DRL因环境非平稳性和关系多变而面临挑战，同时指出了适应动态环境、平衡稳定性与适应性等开放性问题。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Optimization with Preference Exploration using a Monotonic Neural Network Ensemble</div>
<div class="meta-line">Authors: Hanyang Wang, Juergen Branke, Matthias Poloczek</div>
<div class="meta-line">First: 2025-01-30T22:50:34+00:00 · Latest: 2026-01-14T12:51:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.18792v5">Abs</a> · <a href="https://arxiv.org/pdf/2501.18792v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world black-box optimization problems have multiple conflicting objectives. Rather than attempting to approximate the entire set of Pareto-optimal solutions, interactive preference learning allows to focus the search on the most relevant subset. However, few previous studies have exploited the fact that utility functions are usually monotonic. In this paper, we address the Bayesian Optimization with Preference Exploration (BOPE) problem and propose using a neural network ensemble as a utility surrogate model. This approach naturally integrates monotonicity and supports pairwise comparison data. Our experiments demonstrate that the proposed method outperforms state-of-the-art approaches and exhibits robustness to noise in utility evaluations. An ablation study highlights the critical role of monotonicity in enhancing performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单调神经网络集成的贝叶斯优化与偏好探索</div>
<div class="mono" style="margin-top:8px">许多现实世界的黑盒优化问题存在多个相互冲突的目标。相较于试图逼近整个帕累托最优解集，交互式偏好学习能够将搜索聚焦于最相关的子集。然而，先前研究很少利用效用函数通常具有单调性这一事实。本文针对贝叶斯优化与偏好探索问题，提出使用神经网络集成作为效用代理模型。该方法自然地整合了单调性并支持成对比较数据。实验表明，所提方法优于现有先进方法，且在效用评估中表现出对噪声的鲁棒性。消融研究进一步揭示了单调性对提升性能的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses Bayesian Optimization with Preference Exploration (BOPE) for multi-objective black-box optimization, motivated by the need to focus search on user-relevant subsets rather than approximating entire Pareto fronts and by the underutilization of monotonicity in utility functions. The method employs a neural network ensemble as a utility surrogate model that naturally enforces monotonicity constraints and handles pairwise comparison data. Experimental results show that the proposed approach outperforms state-of-the-art methods, demonstrates robustness to noise in utility evaluations, and an ablation study confirms the critical contribution of monotonicity to performance gains.</div>
<div class="mono" style="margin-top:8px">本文针对多目标黑盒优化中的贝叶斯偏好探索问题，其动机在于需要将搜索聚焦于用户相关的子集而非近似整个帕累托前沿，且现有研究较少利用效用函数通常具有单调性这一事实。方法上，采用神经网络集成作为效用代理模型，自然地整合了单调性约束并支持成对比较数据。实验结果表明，所提方法优于现有先进方法，对效用评估中的噪声具有鲁棒性，消融研究进一步凸显了单调性对提升性能的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps</div>
<div class="meta-line">Authors: Siyi Li, Joseph G. Lambourne, Longfei Zhang, Pradeep Kumar Jayaraman, Karl. D. D. Willis</div>
<div class="meta-line">First: 2026-01-14T12:17:34+00:00 · Latest: 2026-01-14T12:17:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09428v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09428v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer&#x27;s input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>如欧几里得般绘制：利用尺规作图步骤教导Transformer模型生成CAD轮廓</div>
<div class="mono" style="margin-top:8px">我们提出一种通过一系列简单几何构造（包括曲线偏移、旋转与求交）生成计算机辅助设计（CAD）轮廓的新方法。这些构造序列以设计师提供的几何图形为起点，逐步构建最终轮廓的点与曲线。我们证明，在设计师输入几何与最终轮廓之间加入构造步骤，能提升生成质量，其效果类似于语言模型中引入思维链。与参数化CAD模型中的约束类似，构造序列将建模形状的自由度缩减为一小组可由设计师调整的参数值，从而实现以浮点精度评估构造几何的参数化编辑。此外，我们表明对构造序列应用强化学习能在包括未显式优化的多项指标上取得进一步改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve the generation quality and parametric editability of Computer Aided Design (CAD) profiles, this paper introduces a method that generates profiles through sequences of geometric construction steps, such as offsetting and intersections, starting from designer-provided geometry. The approach treats these construction sequences analogously to chain-of-thought reasoning in language models, reducing shape degrees of freedom to adjustable parameters for precise parametric editing. Experimental results demonstrate that incorporating construction steps enhances generation quality, and applying reinforcement learning to these sequences yields further improvements across multiple metrics, including those not explicitly optimized.</div>
<div class="mono" style="margin-top:8px">本文旨在提升计算机辅助设计（CAD）轮廓的生成质量与参数化编辑能力，提出了一种通过几何构造步骤序列（如偏移和相交）生成轮廓的方法，从设计师提供的几何图形开始构建。该方法将构造序列类比于语言模型中的思维链，将形状自由度减少为可调整参数，实现高精度的参数化编辑。实验结果表明，引入构造步骤提高了生成质量，并且对这些序列应用强化学习可在多种指标上（包括未明确优化的指标）带来进一步改善。</div>
</details>
</div>
<div class="card">
<div class="title">SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling</div>
<div class="meta-line">Authors: Yixian Zhang, Shu&#x27;ang Yu, Tonghe Zhang, Mo Guang, Haojia Hui, Kaiwen Long, Yu Wang, Chao Yu, Wenbo Ding</div>
<div class="meta-line">First: 2025-09-30T04:21:20+00:00 · Latest: 2026-01-14T11:56:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25756v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25756v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which utilizes a decoded velocity. We then develop a practical SAC-based algorithm, enabled by a noise-augmented rollout, that facilitates direct end-to-end training of these policies. Our approach supports both from-scratch and offline-to-online learning and achieves state-of-the-art performance on continuous control and robotic manipulation benchmarks, eliminating the need for common workarounds like policy distillation or surrogate objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAC Flow：基于速度重参数化序列建模的样本高效流策略强化学习</div>
<div class="mono" style="margin-top:8px">由于多步动作采样过程中的梯度病态问题，利用离策略强化学习训练表达能力强的流策略极不稳定。我们将这种不稳定性归因于一个根本关联：流展开在代数上等价于残差循环计算，使其像RNN一样易受梯度消失和爆炸的影响。为此，我们借鉴现代序列模型原理对速度网络进行重参数化，提出了两种稳定架构：Flow-G（采用门控速度）和Flow-T（采用解码速度）。随后，我们开发了一种基于SAC的实用算法，通过噪声增强的展开过程，实现了对这些策略的直接端到端训练。该方法支持从零开始学习及离线到在线学习，并在连续控制和机器人操作基准测试中取得了最先进的性能，无需依赖策略蒸馏或代理目标等常见变通方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability in training flow-based policies with off-policy reinforcement learning, which stems from gradient issues akin to those in recurrent neural networks during multi-step action sampling. To mitigate this, the authors propose a velocity-reparameterization method, introducing two stable architectures—Flow-G with gated velocity and Flow-T with decoded velocity—and develop a SAC-based algorithm using noise-augmented rollout for end-to-end training. Experimental results demonstrate state-of-the-art performance on continuous control and robotic manipulation benchmarks, enabling effective from-scratch and offline-to-online learning without relying on policy distillation or surrogate objectives.</div>
<div class="mono" style="margin-top:8px">本文针对基于流的策略在离策略强化学习中训练不稳定的问题，该问题源于多步动作采样过程中类似于循环神经网络的梯度消失或爆炸。为解决此问题，作者提出了一种速度重参数化方法，引入了两种稳定架构——Flow-G采用门控速度，Flow-T采用解码速度，并开发了一种基于SAC的算法，通过噪声增强的展开实现端到端训练。实验结果表明，该方法在连续控制和机器人操作基准测试中达到了最先进的性能，支持从零开始和离线到在线的学习，无需依赖策略蒸馏或替代目标等常见变通方案。</div>
</details>
</div>
<div class="card">
<div class="title">GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR</div>
<div class="meta-line">Authors: Jiaying Zhang, Lei Shi, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He</div>
<div class="meta-line">First: 2026-01-14T10:41:34+00:00 · Latest: 2026-01-14T10:41:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09361v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoRA：面向RLVR的几何感知低秩适配方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）对推进大规模推理模型至关重要。然而，现有参数高效方法（如PiSSA和MiLoRA）专为监督微调（SFT）设计，未考虑RLVR特有的优化动态与几何结构。直接应用这些方法会导致谱崩溃和优化不稳定，严重限制模型性能。同时，依赖更新稀疏性的替代方法因非结构化计算在现代硬件上面临显著效率瓶颈。为解决这些问题，我们提出GeoRA（几何感知低秩适配），该方法利用RL更新子空间的各向异性与可压缩特性。GeoRA通过在几何约束子空间内进行奇异值分解（SVD）提取主方向来初始化适配器，同时冻结残差分量。此方法保留了预训练几何结构，并通过稠密算子实现高效GPU计算。在Qwen和Llama上的实验表明，GeoRA缓解了几何失准导致的优化瓶颈，在关键数学基准测试中持续超越主流低秩基线，达到最先进（SOTA）水平。此外，GeoRA在跨域任务中展现出优异的泛化能力与抗灾难性遗忘鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for GeoRA stems from the limitations of existing parameter-efficient fine-tuning methods like PiSSA and MiLoRA, which are designed for supervised fine-tuning and fail to account for the unique optimization dynamics and geometric structures in Reinforcement Learning with Verifiable Rewards (RLVR), leading to spectral collapse and instability. The method introduces Geometry-Aware Low-Rank Adaptation, which exploits the anisotropic and compressible nature of RL update subspaces by initializing adapters via Singular Value Decomposition within a geometrically constrained subspace while freezing residual components, thereby preserving pre-trained geometry and enabling efficient GPU computation. Experimental results on models such as Qwen and Llama show that GeoRA mitigates optimization bottlenecks, consistently outperforms low-rank baselines on mathematical benchmarks with state-of-the-art performance, and demonstrates superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</div>
<div class="mono" style="margin-top:8px">GeoRA的提出动机源于现有参数高效微调方法（如PiSSA和MiLoRA）的局限性，这些方法专为监督微调设计，未能考虑可验证奖励强化学习中独特的优化动态和几何结构，导致谱崩溃和不稳定性。该方法引入了几何感知低秩适应，通过利用RL更新子空间的各向异性和可压缩性，在几何约束子空间内使用奇异值分解初始化适配器并冻结残差分量，从而保留预训练几何结构并实现高效的GPU计算。在Qwen和Llama等模型上的实验结果表明，GeoRA缓解了由几何失配引起的优化瓶颈，在关键数学基准测试中持续优于现有低秩基线，取得了最先进的性能，并在域外任务中表现出优异的泛化能力和对灾难性遗忘的抵抗力。</div>
</details>
</div>
<div class="card">
<div class="title">Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving</div>
<div class="meta-line">Authors: Ioannis Peridis, Dimitrios Troullinos, Georgios Chalkiadakis, Pantelis Giankoulidis, Ioannis Papamichail, Markos Papageorgiou</div>
<div class="meta-line">First: 2026-01-14T10:35:21+00:00 · Latest: 2026-01-14T10:35:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09353v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09353v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles&#x27; policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经网络引导的蒙特卡洛树搜索在无车道自动驾驶中的应用</div>
<div class="mono" style="margin-top:8px">无车道交通环境使车辆能够更好地利用道路横向空间，不受车道保持限制，从而提高交通流量。这为自动驾驶带来了独特且更具挑战性的场景。本研究针对无车道交通中的单智能体自动驾驶，采用蒙特卡洛树搜索规划方法，其关联的马尔可夫决策过程借鉴了现有强化学习框架的方法。此外，MCTS配备了预训练的神经网络来引导选择阶段，该过程结合了神经网络的预测能力，在计算约束下实现更明智的树搜索。实验评估中，我们采用安全性（通过碰撞率）和效率（通过测速）指标，并分析了：（a）无车道环境中车辆各向同性状态信息的影响——车辆因后方快速车辆的存在而产生避让行为；（b）神经网络引导的MCTS变体的性能加速效果；（c）计算资源与求解质量之间的权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of autonomous driving in lane-free traffic environments, which offer higher flow rates but require more complex planning. The method employs a Monte-Carlo Tree Search (MCTS) planner for single-agent decision-making, enhanced with a pre-trained neural network to guide action selection and improve search efficiency under computational limits. Experimental results show that the approach reduces collision rates while maintaining vehicle speed, demonstrates nudging behavior in response to faster trailing vehicles, and achieves a favorable trade-off between computational cost and solution quality.</div>
<div class="mono" style="margin-top:8px">本文针对无车道交通环境中自动驾驶的挑战展开研究，该环境能提升交通流率但规划更复杂。方法采用蒙特卡洛树搜索进行单智能体决策，并引入预训练神经网络指导动作选择，以提高计算受限下的搜索效率。实验结果表明，该方法降低了碰撞率并保持了车速，展示了车辆对后方快速车辆的避让行为，并在计算成本与解质量间取得了良好平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures</div>
<div class="meta-line">Authors: Sofiene Lassoued, Stefan Lier, Andreas Schwung</div>
<div class="meta-line">First: 2026-01-14T08:53:46+00:00 · Latest: 2026-01-14T08:53:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09293v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach follows a model-based paradigm, using Coloured Timed Petri Nets to represent the scheduling environment, and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. To simulate realistic industrial conditions, dynamic job arrivals are modeled using a Gamma distribution, which captures complex temporal patterns such as bursts, clustering, and fluctuating workloads. Machine failures are modeled using a Weibull distribution to represent age-dependent degradation and wear-out dynamics. These stochastic models enable the framework to reflect real-world manufacturing scenarios better. In addition, we study two action-masking strategies: a non-gradient approach that overrides the probabilities of invalid actions, and a gradient-based approach that assigns negative gradients to invalid actions within the policy network. We conduct extensive experiments on dynamic JSSP benchmarks, demonstrating that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization. The results highlight the strength of combining interpretable Petri-net-based models with adaptive reinforcement learning policies, yielding a resilient, scalable, and explainable framework for real-time scheduling in dynamic and uncertain manufacturing environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略强化学习与动作掩码的不确定性动态作业车间调度：处理随机到达与机器故障</div>
<div class="mono" style="margin-top:8px">本文提出一种解决不确定性条件下动态作业车间调度问题的新框架，针对随机工件到达和意外机器故障带来的挑战。该方法采用基于模型的范式，使用着色时间Petri网表示调度环境，并利用可掩码近端策略优化实现动态决策，同时限制智能体在每个决策点仅选择可行动作。为模拟真实工业条件，动态工件到达采用伽马分布建模，以捕捉突发性、聚集性和波动性工作负载等复杂时序特征；机器故障采用威布尔分布建模，以表征与使用时长相关的性能退化与磨损动态。这些随机模型使框架能更准确反映实际制造场景。此外，我们研究了两种动作掩码策略：一种是通过覆盖无效动作概率的非梯度方法，另一种是在策略网络中对无效动作分配负梯度的梯度方法。通过在动态JSSP基准测试上的大量实验，证明本方法在最小化完工时间指标上持续优于传统启发式和基于规则的调度方法。结果表明，将可解释的Petri网模型与自适应强化学习策略相结合，能够为动态不确定制造环境构建具有韧性、可扩展性和可解释性的实时调度框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of Dynamic Job Shop Scheduling under uncertainty, motivated by the need to handle stochastic job arrivals and machine failures in real-world manufacturing. The method employs a model-based framework using Coloured Timed Petri Nets to represent the environment and Maskable Proximal Policy Optimization for decision-making, with action masking to restrict actions to feasible ones; job arrivals are modeled with a Gamma distribution and machine failures with a Weibull distribution to simulate realistic conditions. Experimental results on dynamic benchmarks show that this approach consistently outperforms traditional heuristic and rule-based methods in minimizing makespan, demonstrating the effectiveness of combining interpretable Petri-net models with adaptive reinforcement learning for resilient and explainable scheduling.</div>
<div class="mono" style="margin-top:8px">本文针对动态作业车间调度中的不确定性问题，旨在解决实际制造中随机工件到达和机器故障带来的挑战。方法采用基于模型的框架，使用着色时间Petri网表示调度环境，并利用可掩码近端策略优化进行决策，通过动作掩码限制可行动作；使用伽马分布模拟工件到达和威布尔分布模拟机器故障，以反映真实工业条件。在动态基准测试上的实验结果表明，该方法在最小化完工时间方面持续优于传统启发式和基于规则的方法，凸显了将可解释Petri网模型与自适应强化学习策略相结合的优势，为动态不确定制造环境提供了鲁棒、可扩展且可解释的实时调度框架。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction</div>
<div class="meta-line">Authors: Mianzhi Pan, JianFei Li, Peishuo Liu, Botian Wang, Yawen Ouyang, Yiming Rong, Hao Zhou, Jianbing Zhang</div>
<div class="meta-line">First: 2026-01-14T08:45:07+00:00 · Latest: 2026-01-14T08:45:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09285v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs&#x27; high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强大语言模型空间推理能力用于金属有机框架结构预测</div>
<div class="mono" style="margin-top:8px">金属有机框架（MOFs）是一种多孔晶体材料，在碳捕获和药物递送等领域有广泛应用，但其三维结构的精确预测仍面临重大挑战。尽管大语言模型（LLMs）在晶体生成方面展现出潜力，但MOFs的高原子复杂性限制了其在该领域的应用。受深度生成模型中块级范式成功的启发，我们率先将LLMs引入该领域，提出了首个专用于块级MOF结构预测的LLM框架——MOF-LLM。为有效利用LLMs完成这种模块化组装任务，我们的训练范式整合了空间感知持续预训练（CPT）、结构监督微调（SFT）和匹配驱动强化学习（RL）。通过引入显式空间先验，并利用软自适应策略优化（SAPO）提升结构稳定性，该方法显著增强了Qwen-3 8B模型在MOF结构预测中的空间推理能力。综合实验表明，MOF-LLM在保持优异采样效率的同时，性能超越了当前基于去噪和基于LLM的先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of predicting the complex 3D structures of metal-organic frameworks (MOFs), which are crucial for applications like carbon capture, by enhancing the spatial reasoning of large language models (LLMs). The method introduces MOF-LLM, a novel framework that adapts LLMs for block-level structure prediction through a training paradigm combining spatial-aware continual pre-training, structural supervised fine-tuning, and matching-driven reinforcement learning with Soft Adaptive Policy Optimization to incorporate spatial priors and optimize stability. Experimental results show that MOF-LLM outperforms existing denoising-based and LLM-based methods in accuracy and sampling efficiency, demonstrating improved capability in handling MOFs&#x27; high atomic complexity.</div>
<div class="mono" style="margin-top:8px">本文针对金属有机框架（MOF）这一在碳捕获等领域有重要应用的复杂多孔晶体材料，其三维结构预测的难题，通过增强大语言模型的空间推理能力来提出解决方案。方法上，研究首次引入了MOF-LLM框架，专门用于块级MOF结构预测，其训练范式整合了空间感知的持续预训练、结构监督微调以及采用软自适应策略优化的匹配驱动强化学习，以融入显式空间先验并优化结构稳定性。主要实验结果表明，MOF-LLM在预测准确性和采样效率上均优于当前最先进的去噪基和LLM基方法，有效提升了处理MOF高原子复杂度的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?</div>
<div class="meta-line">Authors: Long Zhang, Yuchen Xia, Bingqing Wei, Zhen Liu, Shiwen Mao, Zhu Han, Mohsen Guizani</div>
<div class="meta-line">First: 2026-01-13T11:05:12+00:00 · Latest: 2026-01-14T08:39:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08434v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08434v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向具身智能驾驶的大型多模态模型：自动驾驶的下一个前沿？</div>
<div class="mono" style="margin-top:8px">大型多模态模型的出现为解决自动驾驶模块化设计在开放场景中因需持续环境理解与逻辑推理而表现不足的局限提供了前景广阔的技术路径。同时，具身人工智能通过闭环交互促进策略优化，以实现持续学习能力，从而推动自动驾驶向具身智能驾驶演进。然而，若仅依赖大型多模态模型增强具身智能驾驶而缺乏联合决策，其能力将受到制约。本文提出一种新颖的语义与策略双驱动混合决策框架以应对此挑战，确保持续学习与联合决策。该框架融合了用于语义理解与认知表征的大型多模态模型，以及用于实时策略优化的深度强化学习。我们首先阐述具身智能驾驶与大型多模态模型的基础原理，进而探讨该框架催生的新兴机遇，包括潜在优势与典型应用场景。通过实验案例研究验证了本框架在完成换道规划任务中的性能优越性。最后，提出了若干赋能具身智能驾驶的未来研究方向以指导后续工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of modular autonomous driving systems in complex open-world scenarios, this paper proposes a hybrid decision framework that integrates Large Multimodal Models (LMMs) for semantic understanding with Deep Reinforcement Learning (DRL) for policy optimization to achieve embodied intelligent driving. The method combines LMMs&#x27; cognitive representation and reasoning with DRL&#x27;s closed-loop, real-time decision-making to enable continuous learning and joint action planning. The main experimental results from a lane-change planning case study demonstrate the performance superiority of this dual-driven framework in completing the driving task.</div>
<div class="mono" style="margin-top:8px">本文针对模块化自动驾驶系统在复杂开放场景中的局限性，提出了一种混合决策框架，将大型多模态模型（LMM）的语义理解与深度强化学习（DRL）的策略优化相结合，以实现具身智能驾驶。该方法融合了LMM的认知表征与推理能力以及DRL的闭环实时决策，以支持持续学习和联合规划。通过车道变换规划的案例实验验证，该双驱动框架在完成驾驶任务方面表现出性能优势。</div>
</details>
</div>
<div class="card">
<div class="title">RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering</div>
<div class="meta-line">Authors: Wencheng Ye, Liang Peng, Xiaoyang Yuan, Yi Bin, Pengpeng Zeng, Hengyu Jin, Heng Tao Shen</div>
<div class="meta-line">First: 2026-01-14T08:04:33+00:00 · Latest: 2026-01-14T08:04:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09269v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09269v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RISER：基于潜在推理能力的自适应激活引导编排框架</div>
<div class="mono" style="margin-top:8px">近期针对大语言模型（LLMs）的领域特定推理研究多依赖需要参数更新的训练密集型方法。虽然激活引导已成为参数高效的替代方案，但现有方法采用静态人工干预，难以适应复杂推理的动态特性。为此，我们提出RISER（基于路由器的可引导推理增强框架），这是一种即插即用的干预框架，可在激活空间自适应引导LLM推理。RISER构建可复用的推理向量库，并采用轻量级路由器为每个输入动态组合这些向量。该路由器通过任务级奖励下的强化学习进行优化，以涌现式组合方式激活潜在认知基元。在七个多样化基准测试中，RISER相比基础模型实现3.4-6.5%的平均零样本准确率提升，同时以2-3倍更高的标记效率超越思维链式推理，并获得稳健的准确率增益。进一步分析表明，RISER能自主组合多个向量形成可解释的精确控制策略，为构建更可控高效的LLM推理系统指明方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of static, manual activation steering methods in large language models (LLMs), which fail to adapt during complex reasoning tasks, and to provide a parameter-efficient alternative to training-intensive approaches. The proposed method, RISER, is a plug-and-play framework that constructs a library of reusable reasoning vectors and uses a lightweight Router optimized via reinforcement learning to dynamically compose these vectors for each input, thereby adaptively steering LLM reasoning in activation space. The main experimental results show that across seven diverse benchmarks, RISER achieves an average zero-shot accuracy improvement of 3.4-6.5% over the base model, surpasses chain-of-thought reasoning with 2-3 times higher token efficiency, and demonstrates robust accuracy gains, while analysis reveals it autonomously combines vectors into interpretable control strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是克服大型语言模型中静态、手动激活引导方法的局限性，这些方法在复杂推理任务中无法自适应调整，同时为训练密集型方法提供一种参数高效的替代方案。所提出的方法RISER是一个即插即用框架，它构建了一个可重用的推理向量库，并通过强化学习优化的轻量级路由器动态组合这些向量以适应每个输入，从而在激活空间中自适应地引导模型推理。主要实验结果表明，在七个多样化基准测试中，RISER相比基础模型实现了3.4-6.5%的平均零样本准确率提升，以2-3倍的标记效率超越思维链推理，并展现出稳健的准确率增益，分析还表明它能自主将向量组合成可解释的控制策略。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability</div>
<div class="meta-line">Authors: Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang</div>
<div class="meta-line">First: 2026-01-14T07:52:14+00:00 · Latest: 2026-01-14T07:52:14+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures. Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09261v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09261v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner&#x27;s own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner&#x27;s internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学会信任经验：不可观测反馈可靠性下的学习监控-信任-调节框架</div>
<div class="mono" style="margin-top:8px">在反馈可靠性不可观测的条件下学习，提出了超越优化鲁棒性的独特挑战：系统必须决定是否从经验中学习，而不仅仅是学习如何稳定学习。我们将此设定研究为不可观测可靠性下的认知可识别性（EIUR），其中每个经验具有潜在可信度，可靠与不可靠反馈在局部可能无法区分，且数据由学习者自身不断演化的信念与行为在闭环中生成。在EIUR中，标准鲁棒学习可能稳定收敛，却形成高置信度但系统错误的信念。
我们提出元认知调节作为实用应对方案：一个内省的二级控制环路，从学习者内部动态的内生证据中推断经验可信度。我们将其形式化为模块化的监控-信任-调节（MTR）分解，并通过自诊断实例化——该方法维护一个缓慢变化的经验信任变量，以软性调节学习更新，无需外生可靠性标签或显式损坏模型。
实证研究表明，在本文探讨的EIUR机制中，自诊断与认知可识别性的提升相关。在强化学习中，它实现了校准后的怀疑态度及在系统损坏奖励下的恢复能力。在监督学习中，它揭示了一个关键分离现象：性能恢复并不等同于认知恢复。准确率可能回升，而内部信念动态仍被早期误导数据锁定，这种失败仅能通过内省诊断检测。MTR与自诊断共同为不可观测可靠性下的自主学习提供了内在可靠性评估的组织抽象与具体设计模板。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of learning when feedback reliability is unobservable, a scenario termed Epistemic Identifiability under Unobservable Reliability (EIUR), where standard robust methods can converge to confidently wrong beliefs. To counter this, the authors propose a metacognitive Monitor-Trust-Regulator (MTR) framework that uses introspective self-diagnosis to infer experience credibility from the learner&#x27;s internal dynamics, softly modulating updates without external labels. Experimental results in reinforcement and supervised learning show that self-diagnosis improves epistemic identifiability, enabling recovery from corrupted rewards and revealing that accuracy recovery can mask persistent, misleading belief lock-in detectable only through introspection.</div>
<div class="mono" style="margin-top:8px">本文研究了在反馈可靠性不可观测情况下的学习挑战，即不可观测可靠性的认知可识别性（EIUR）问题，其中标准鲁棒方法可能稳定地收敛到错误信念。为解决该问题，作者提出了一种元认知的监控-信任-调节器（MTR）框架，通过内省式自我诊断从学习者内部动态推断经验可信度，无需外部标签即可软调节学习更新。在强化学习和监督学习中的实验结果表明，自我诊断提高了认知可识别性，能够从系统腐败的奖励中恢复，并揭示了性能恢复可能掩盖由早期误导数据导致的信念锁定，这种失败只能通过内省诊断检测。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models</div>
<div class="meta-line">Authors: Yan Liu, Feng Zhang, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Han Liu, Yangdong Deng</div>
<div class="meta-line">First: 2026-01-14T07:52:05+00:00 · Latest: 2026-01-14T07:52:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09260v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09260v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效路径与密集奖励：大语言模型的概率流推理</div>
<div class="mono" style="margin-top:8px">高质量思维链已展现出激发大语言模型推理能力的强大潜力。然而，当前范式通常将推理过程视为不可分割的序列，缺乏量化逐步信息增益的内在机制。这种粒度缺失体现在两个局限：因缺乏显式指导而冗余探索导致的推理低效，以及因稀疏结果监督或昂贵外部验证器导致的优化困难。本研究提出CoT-Flow框架，将离散推理步骤重构为连续概率流，量化每个步骤对真实答案的贡献。基于此形式化，CoT-Flow实现两种互补方法：流引导解码——采用基于流的贪心解码策略提取信息高效的推理路径；流强化学习——构建无需验证器的密集奖励函数。在挑战性基准测试中，CoT-Flow在推理效率与推理性能间取得了更优平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of current chain-of-thought paradigms, which treat reasoning as an indivisible sequence and lack mechanisms to quantify step-wise information gain, leading to inference inefficiency and optimization difficulty. The proposed method, CoT-Flow, reconceptualizes discrete reasoning steps as a continuous probabilistic flow to quantify each step&#x27;s contribution toward the ground-truth answer, enabling flow-guided decoding for efficient path extraction and flow-based reinforcement learning with a dense, verifier-free reward function. Main experimental results on challenging benchmarks show that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于当前思维链方法将推理视为不可分割的序列，缺乏量化逐步信息增益的机制，导致推理效率低下和优化困难。所提出的CoT-Flow方法将离散推理步骤重新概念化为连续概率流，以量化每个步骤对正确答案的贡献，从而实现了基于流的解码以提取高效推理路径，以及基于流的强化学习与无需验证器的密集奖励函数。在具有挑战性的基准测试中的主要实验结果表明，CoT-Flow在推理效率和推理性能之间取得了优越的平衡。</div>
</details>
</div>
<div class="card">
<div class="title">GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization</div>
<div class="meta-line">Authors: Zhengyang Zhao, Lu Ma, Yizhen Jiang, Xiaochen Ma, Zimo Meng, Chengyu Shen, Lexiang Tang, Haoze Sun, Peng Pei, Wentao Zhang</div>
<div class="meta-line">First: 2026-01-14T07:13:57+00:00 · Latest: 2026-01-14T07:13:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09233v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09233v1">PDF</a> · <a href="https://github.com/zzy1127/GIFT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GIFT：通过有限温度吉布斯初始化解锁训练后全局最优性</div>
<div class="mono" style="margin-top:8px">当前大型推理模型（LRM）的训练后范式——监督微调（SFT）后接强化学习（RL）——存在内在优化失配问题：SFT固有的刚性监督会导致分布坍缩，从而耗尽后续RL所需的探索空间。本文在统一的训练后框架中重新形式化SFT，并提出有限温度吉布斯初始化（GIFT）。我们将标准SFT描述为抑制基础先验的退化零温极限，而GIFT则将监督作为有限温度能量势引入，建立分布桥梁以确保整个训练后流程的目标一致性。实验表明，GIFT作为RL初始化方法显著优于标准SFT及其他竞争基线，为在训练后实现全局最优性提供了数学原理清晰的路径。代码发布于 https://github.com/zzy1127/GIFT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the optimization mismatch in the standard post-training pipeline for Large Reasoning Models, where Supervised Fine-Tuning (SFT) causes distributional collapse and limits exploration for subsequent Reinforcement Learning (RL). The method introduces Gibbs Initialization with Finite Temperature (GIFT), which reformulates SFT by incorporating supervision as a finite-temperature energy potential to preserve base priors and ensure objective consistency. Experimental results show that GIFT significantly outperforms standard SFT and other baselines when used to initialize RL, offering a principled path toward global optimality in post-training.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大型推理模型标准后训练流程中的优化不匹配问题，即监督微调（SFT）会导致分布坍缩，从而限制后续强化学习（RL）的探索空间。方法上提出了有限温度吉布斯初始化（GIFT），通过将监督作为有限温度能量势重新构建SFT，以保留基础先验并确保整个训练流程的目标一致性。实验结果表明，当用于初始化RL时，GIFT显著优于标准SFT及其他竞争基线，为后训练中实现全局最优性提供了数学原理清晰的路径。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning with Exogenous States and Rewards</div>
<div class="meta-line">Authors: George Trimponias, Thomas G. Dietterich</div>
<div class="meta-line">First: 2023-03-22T23:37:28+00:00 · Latest: 2026-01-14T05:15:05+00:00</div>
<div class="meta-line">Comments: Substantial rewrite to improve rigor and clarity in response to referee reports at JMLR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2303.12957v2">Abs</a> · <a href="https://arxiv.org/pdf/2303.12957v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms can be applied during reinforcement learning to discover the exogenous subspace, remove the exogenous reward, and focus reinforcement learning on the endogenous MDP. Experiments on a variety of challenging synthetic MDPs show that these methods, applied online, discover large exogenous state spaces and produce substantial speedups in reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有外生状态与奖励的强化学习</div>
<div class="mono" style="margin-top:8px">外生状态变量和奖励会通过向奖励信号注入不可控的变异而减缓强化学习进程。本文形式化定义了外生状态变量与奖励，并证明若奖励函数可加性分解为内生与外生成分，则马尔可夫决策过程可分解为基于外生奖励的外生马尔可夫奖励过程，以及优化内生奖励的内生马尔可夫决策过程。内生MDP的任何最优策略同样是原MDP的最优策略，且由于内生奖励通常具有更低的方差，内生MDP更易于求解。本研究探讨了状态空间分解为外生与内生子空间未知而需自主发现的情境。论文提出并证明了在线性混合状态下发现外生与内生子空间算法的正确性。这些算法可在强化学习过程中实时发现外生子空间、移除外生奖励，并将学习聚焦于内生MDP。在多种复杂合成MDP上的实验表明，在线应用这些方法能有效发现大规模外生状态空间，并显著加速强化学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge that exogenous state variables and rewards can impede reinforcement learning by introducing uncontrolled variance into the reward signal. The authors formalize these exogenous components and demonstrate that if the total reward decomposes additively, the original Markov Decision Process (MDP) can be separated into an exogenous Markov Reward Process and an endogenous MDP focusing on the controllable reward. Crucially, an optimal policy for the endogenous MDP remains optimal for the full problem, and learning is accelerated due to the reduced reward variance. The core methodological contribution is algorithms to automatically discover the exogenous and endogenous state subspaces when they are linearly mixed, enabling online identification and removal of exogenous noise during learning. Experimental results on synthetic MDPs confirm that these methods successfully identify large exogenous spaces and achieve significant speedups in reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本文针对外生状态变量和奖励会通过向奖励信号引入不可控方差从而减缓强化学习的问题展开研究。作者形式化了这些外生成分，并证明若总奖励可加性分解，原始马尔可夫决策过程可分解为基于外生奖励的外生马尔可夫奖励过程和一个专注于内生奖励的内生MDP。关键的是，内生MDP的最优策略对完整问题仍是最优的，且由于奖励方差降低，学习得以加速。核心方法贡献在于提出了当状态空间线性混合时自动发现外生与内生子空间的算法，使得能在学习过程中在线识别并移除外生噪声。在合成MDP上的实验结果表明，这些方法能成功识别大规模外生空间，并显著加速强化学习。</div>
</details>
</div>
<div class="card">
<div class="title">ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning</div>
<div class="meta-line">Authors: Ruiyang Zhou, Shuozhe Li, Amy Zhang, Liu Leqi</div>
<div class="meta-line">Venue: NeurIPS 2025 Poster</div>
<div class="meta-line">First: 2025-07-03T17:44:55+00:00 · Latest: 2026-01-14T04:45:32+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025 (Poster). Code available at https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02834v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.02834v2">PDF</a> · <a href="https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-improvement via RL often fails on complex reasoning tasks because GRPO-style post-training methods rely on the model&#x27;s initial ability to generate positive samples. Without guided exploration, these approaches merely reinforce what the model already knows (distribution-sharpening) rather than enabling the model to solve problems where it initially generates no correct solutions. To unlock reasoning ability in such settings, the model must explore new reasoning trajectories beyond its current output distribution. Such exploration requires access to sufficiently good positive samples to guide the learning. While expert demonstrations seem like a natural solution, we find that they are often ineffective in RL post-training. Instead, we identify two key properties of effective positive samples: they should (1) be likely under the current policy, and (2) increase the model&#x27;s likelihood of predicting the correct answer. Based on these insights, we propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and modular framework that generates such samples by conditioning on the ground-truth answer. It can be integrated with popular RL training methods like GRPO and DPO. ExPO enables efficient exploration and guides the model to produce reasoning trajectories more aligned with its policy than expert-written CoTs, while ensuring higher quality than its own (incorrect) samples. Experiments show that ExPO improves both learning efficiency and final performance on reasoning benchmarks, surpassing expert-demonstration-based methods in challenging settings such as MATH level-5, where the model initially struggles the most. Code is available at https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExPO：通过自解释引导的强化学习解锁复杂推理能力</div>
<div class="mono" style="margin-top:8px">基于强化学习的自我改进方法在复杂推理任务上常失效，因为GRPO式后训练方法依赖模型生成正样本的初始能力。若无引导性探索，这类方法仅强化模型已有知识（分布锐化），而无法解决初始无法生成正确解的问题。要在此类场景中解锁推理能力，模型必须探索超出当前输出分布的新推理轨迹。这种探索需要足够优质的正样本引导学习。虽然专家演示看似是自然解决方案，但我们发现其在RL后训练中往往无效。相反，我们识别出有效正样本的两个关键特性：（1）应在当前策略下具有较高可能性；（2）能提升模型预测正确答案的概率。基于此，我们提出$\textbf{自解释策略优化（ExPO）}$——一个通过以真实答案为条件生成此类样本的简洁模块化框架，可与GRPO、DPO等主流RL训练方法集成。ExPO能实现高效探索，引导模型生成比专家撰写思维链更贴合其策略的推理轨迹，同时确保比自身错误样本更高质量。实验表明，ExPO在推理基准上同时提升学习效率和最终性能，在模型初始表现最困难的MATH 5级等挑战性场景中超越基于专家演示的方法。代码发布于https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of self-improvement via reinforcement learning (RL) on complex reasoning tasks, where methods like GRPO often fail because they merely reinforce existing knowledge rather than enabling exploration when the model initially generates no correct solutions. To overcome this, the authors propose Self-Explanation Policy Optimization (ExPO), a modular framework that generates effective positive samples by conditioning on ground-truth answers, ensuring these samples are likely under the current policy and increase the likelihood of correct predictions. Experimental results demonstrate that ExPO improves learning efficiency and final performance on reasoning benchmarks, outperforming expert-demonstration-based methods in challenging settings like MATH level-5, where models initially struggle the most.</div>
<div class="mono" style="margin-top:8px">本文针对复杂推理任务中基于强化学习的自我改进方法的局限性，指出如GRPO等方法因仅强化模型已有知识而无法在初始无正确解时进行有效探索。为解决此问题，作者提出了自我解释策略优化（ExPO），这是一个模块化框架，通过基于真实答案生成有效正样本，确保这些样本在当前策略下具有高可能性并能提高正确预测的似然。实验结果表明，ExPO在推理基准测试中提升了学习效率和最终性能，在如MATH level-5等模型初始表现较差的挑战性场景中超越了基于专家演示的方法。</div>
</details>
</div>
<div class="card">
<div class="title">SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL</div>
<div class="meta-line">Authors: Lijun Liu, Linwei Chen, Zhishou Zhang, Meng Tian, Hengfu Cui, Ruiyang Li, Zhaocheng Liu, Qiang Ju, Qianxi Li, Hong-Yu Zhou</div>
<div class="meta-line">First: 2026-01-14T04:21:07+00:00 · Latest: 2026-01-14T04:21:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to &quot;diffuse attention&quot; - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to &quot;unfold&quot; complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SkinFlow：基于动态视觉编码与分阶段强化学习的开放式皮肤病诊断高效信息传输框架</div>
<div class="mono" style="margin-top:8px">通用大规模视觉语言模型虽参数庞大，但在皮肤病学中常因&#x27;注意力弥散&#x27;问题而表现不佳——即难以从背景噪声中辨识细微病理特征。本文挑战了&#x27;参数扩展是提升医学精度的唯一途径&#x27;这一假设，提出SkinFlow框架，将诊断过程视为视觉信息传输效率的优化问题。该方法采用虚拟宽度动态视觉编码器在不增加物理参数的前提下&#x27;展开&#x27;复杂病理流形，并结合两阶段强化学习策略：第一阶段对齐显性医学描述，第二阶段在受限语义空间中重建隐性诊断纹理。此外，我们提出以临床实践为基础的评估方案，优先考量诊断安全性与层级相关性，而非僵化的标签匹配。实证结果显著：我们的70亿参数模型在Fitzpatrick17k基准测试中刷新最优记录，相比大规模通用模型（如Qwen3VL-235B与GPT-5.2），Top-1准确率提升12.06%，Top-6准确率提升28.57%。这些发现证明，优化几何容量与信息流能比单纯参数扩展产生更卓越的诊断推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper challenges the reliance on scaling model parameters for medical AI precision, motivated by the &#x27;diffuse attention&#x27; problem where general large vision-language models struggle to focus on subtle skin lesions amidst background noise. It introduces SkinFlow, a framework that optimizes visual information transmission efficiency through a Virtual-Width Dynamic Vision Encoder to unfold complex pathological details without adding parameters, combined with a two-stage reinforcement learning strategy that first aligns explicit medical descriptions and then reconstructs implicit diagnostic textures. Experimental results show that the compact 7B model achieves state-of-the-art performance on the Fitzpatrick17k benchmark, with a +12.06% Top-1 accuracy and +28.57% Top-6 accuracy improvement over much larger general-purpose models, demonstrating that enhanced geometric capacity and information flow outperform mere parameter scaling.</div>
<div class="mono" style="margin-top:8px">本文针对通用大规模视觉语言模型在皮肤病诊断中因&#x27;注意力分散&#x27;而难以区分细微病变与背景噪声的问题，挑战了仅靠参数缩放提升医学精度的传统思路。提出了SkinFlow框架，通过虚拟宽度动态视觉编码器在不增加物理参数的情况下&#x27;展开&#x27;复杂病理特征，并结合两阶段强化学习策略，依次对齐显性医学描述并重建隐性诊断纹理。实验结果表明，该紧凑的7B模型在Fitzpatrick17k基准测试中取得了最先进性能，Top-1准确率提升12.06%，Top-6准确率提升28.57%，优于参数量大得多的通用模型，证明优化几何容量和信息流比单纯参数缩放更能提升诊断推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design</div>
<div class="meta-line">Authors: Lianghong Chen, Dongkyu Eugene Kim, Mike Domaratzki, Pingzhao Hu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-24T04:49:23+00:00 · Latest: 2026-01-14T04:12:09+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21153v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21153v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing de novo 3D molecules with desirable properties remains a fundamental challenge in drug discovery and molecular engineering. While diffusion models have demonstrated remarkable capabilities in generating high-quality 3D molecular structures, they often struggle to effectively control complex multi-objective constraints critical for real-world applications. In this study, we propose an uncertainty-aware Reinforcement Learning (RL) framework to guide the optimization of 3D molecular diffusion models toward multiple property objectives while enhancing the overall quality of the generated molecules. Our method leverages surrogate models with predictive uncertainty estimation to dynamically shape reward functions, facilitating balance across multiple optimization objectives. We comprehensively evaluate our framework across three benchmark datasets and multiple diffusion model architectures, consistently outperforming baselines for molecular quality and property optimization. Additionally, Molecular Dynamics (MD) simulations and ADMET profiling of top generated candidates indicate promising drug-like behavior and binding stability, comparable to known Epidermal Growth Factor Receptor (EGFR) inhibitors. Our results demonstrate the strong potential of RL-guided generative diffusion models for advancing automated molecular design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不确定性感知多目标强化学习引导扩散模型用于三维从头分子设计</div>
<div class="mono" style="margin-top:8px">设计具有理想性质的三维从头分子仍是药物发现和分子工程领域的核心挑战。尽管扩散模型在生成高质量三维分子结构方面展现出卓越能力，但其往往难以有效控制实际应用中至关重要的复杂多目标约束。本研究提出一种不确定性感知强化学习框架，用于引导三维分子扩散模型向多属性目标优化，同时提升生成分子的整体质量。该方法利用具有预测不确定性估计的代理模型动态调整奖励函数，促进多优化目标间的平衡。我们在三个基准数据集和多种扩散模型架构上全面评估了该框架，其在分子质量和属性优化方面均持续优于基线方法。此外，对生成候选分子的分子动力学模拟和ADMET分析表明，其具有与已知表皮生长因子受体抑制剂相当的类药行为和结合稳定性。我们的结果证明了强化学习引导的生成扩散模型在推进自动化分子设计方面的巨大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of designing 3D molecules with tailored multi-property constraints, where standard diffusion models often fall short. It introduces an uncertainty-aware reinforcement learning framework that guides diffusion models by using surrogate models with predictive uncertainty to dynamically shape reward functions, balancing multiple objectives. Experimental results on three benchmark datasets show superior performance in molecular quality and property optimization compared to baselines, with Molecular Dynamics simulations and ADMET profiling indicating drug-like behavior and binding stability akin to known EGFR inhibitors.</div>
<div class="mono" style="margin-top:8px">本研究针对设计具有多属性约束的3D分子这一挑战，其中标准扩散模型往往难以有效控制复杂目标。方法上，提出了一种不确定性感知的强化学习框架，通过利用具有预测不确定性的代理模型动态调整奖励函数，以指导扩散模型平衡多个优化目标。在三个基准数据集上的实验结果表明，该方法在分子质量和属性优化方面均优于基线模型，分子动力学模拟和ADMET分析显示，生成的候选分子具有类似已知EGFR抑制剂的药物样行为和结合稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">LSRIF: Logic-Structured Reinforcement Learning for Instruction Following</div>
<div class="meta-line">Authors: Qingyu Ren, Qianyu He, Jingwen Chang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Han Xia, Zeye Sun, Fei Yu</div>
<div class="meta-line">First: 2026-01-10T05:11:38+00:00 · Latest: 2026-01-14T02:51:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06431v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06431v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LSRIF：面向指令跟随的逻辑结构化强化学习</div>
<div class="mono" style="margin-top:8px">指令跟随对大语言模型至关重要，但现实指令常包含顺序依赖和条件分支等逻辑结构。现有方法通常构建带并行约束的数据集并优化平均奖励，忽略了逻辑依赖关系并产生噪声信号。我们提出逻辑结构化训练框架LSRIF，显式建模指令逻辑。首先构建包含并行、顺序、条件等约束结构的LSRInstruct数据集，随后设计结构感知奖励方法LSRIF：对并行结构采用平均聚合，对顺序结构实施失败惩罚传播，对条件分支进行选择性奖励。实验表明LSRIF在指令跟随（域内/域外）和通用推理方面带来显著提升。分析显示，通过显式逻辑结构学习能驱动注意力层的参数更新，并增强对约束条件和逻辑运算符的词元级注意力聚焦。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need to improve instruction-following in large language models, as real-world instructions often contain complex logical structures like sequences and conditions, which existing methods that use parallel constraints and average rewards fail to properly address. The proposed method, LSRIF, introduces a logic-structured training framework that explicitly models instruction logic by first constructing a dataset (LSRInstruct) with various constraint types and then designing a structure-aware rewarding mechanism tailored for parallel, sequential, and conditional structures. Experimental results demonstrate that LSRIF leads to significant improvements in both in-domain and out-of-domain instruction-following performance, as well as in general reasoning tasks, with analysis showing that the approach sharpens attention to constraints and logical operators at the token level.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升大语言模型的指令跟随能力，因为现实指令常包含顺序依赖和条件分支等逻辑结构，而现有方法通常使用并行约束和平均奖励进行优化，忽略了逻辑依赖性并产生噪声信号。所提出的方法LSRIF是一个逻辑结构化的训练框架，它通过首先构建包含并行、顺序和条件等约束类型的数据集（LSRInstruct），然后设计针对不同结构（如并行结构的平均聚合、顺序结构的失败惩罚传播和条件分支的选择性奖励）的结构感知奖励机制，来显式建模指令逻辑。实验结果表明，LSRIF在指令跟随（领域内和领域外）和通用推理方面带来了显著提升，分析显示，学习显式逻辑结构能引起注意力层的参数更新，并增强对约束和逻辑运算符的令牌级关注。</div>
</details>
</div>
<div class="card">
<div class="title">SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache</div>
<div class="meta-line">Authors: Chi-Chih Chang, Siqi Zhu, Zhichen Zeng, Haibin Lin, Jiaxuan You, Mohamed S. Abdelfattah, Ziheng Jiang, Xuehai Qian</div>
<div class="meta-line">First: 2026-01-14T02:34:48+00:00 · Latest: 2026-01-14T02:34:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SRT：基于树结构缓存的推测式轨迹生成加速强化学习</div>
<div class="mono" style="margin-top:8px">本文提出基于树结构缓存的推测式轨迹生成方法（SRT），这是一种无需模型即可加速语言模型同策略强化学习的简易方法，且不牺牲分布准确性。SRT通过将同一提示词在训练过程中生成的延续文本存储于按提示词组织的树结构缓存中，利用跨训练步的轨迹经验相似性。在生成阶段，当前策略将此树作为草稿模型进行推测解码。为保持缓存新鲜度并提升草稿模型质量，SRT通过持续轨迹在线更新树结构，并利用GPU空闲时段主动执行预跑生成。集成至标准RL流程（如PPO、GRPO和DAPO）及多轮对话场景后，SRT持续降低生成与步进延迟，减少单令牌推理成本，在轨迹生成阶段实现最高2.08倍的实际时间加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Speculative Rollout with Tree-Structured Cache (SRT), motivated by the need to accelerate on-policy reinforcement learning for language models without compromising distributional correctness. The method leverages empirical similarities in rollouts across training steps by storing past generated continuations in a per-prompt tree cache, which serves as a draft model for speculative decoding during generation; it maintains cache freshness through online updates from ongoing rollouts and proactive run-ahead generation during GPU idle periods. Experimental results show that SRT, when integrated into RL pipelines like PPO, GRPO, and DAPO, reduces generation latency and per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.</div>
<div class="mono" style="margin-top:8px">本文提出了基于树结构缓存的推测性展开方法（SRT），其动机是在不牺牲分布正确性的前提下，加速语言模型的在线策略强化学习。该方法利用训练步骤中相同提示下展开的经验相似性，将先前生成的续写存储于每个提示的树结构缓存中，作为生成时推测解码的草稿模型；通过在线更新来自持续展开的树以及利用GPU空闲时段进行前瞻生成，保持缓存新鲜度。实验结果表明，SRT集成到PPO、GRPO和DAPO等标准RL流程后，能降低生成延迟和单令牌推理成本，在展开阶段实现最高2.08倍的挂钟时间加速。</div>
</details>
</div>
<div class="card">
<div class="title">Machine Learning-Driven Creep Law Discovery Across Alloy Compositional Space</div>
<div class="meta-line">Authors: Hongshun Chen, Ryan Zhou, Rujing Zha, Zihan Chen, Wenpan Li, Rowan Rolark, John Patrick Reidy, Jian Cao, Ping Guo, David C. Dunand, Horacio D. Espinosa</div>
<div class="meta-line">First: 2026-01-13T20:29:15+00:00 · Latest: 2026-01-13T20:29:15+00:00</div>
<div class="meta-line">Comments: 27 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08970v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08970v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hihg-temperature creep characterization of structural alloys traditionally relies on serial uniaxial tests, which are highly inefficient for exploring the large search space of alloy compositions and for material discovery. Here, we introduce a machine-learning-assisted, high-throughput framework for creep law identification based on a dimple array bulge instrument (DABI) configuration, which enables parallel creep testing of 25 dimples, each fabricated from a different alloy, in a single experiment. Full-field surface displacements of dimples undergoing time-dependent creep-induced bulging under inert gas pressure are measured by 3D digital image correlation. We train a recurrent neural network (RNN) as a surrogate model, mapping creep parameters and loading conditions to the time-dependent deformation response of DABI. Coupling this surrogate with a particle swarm optimization scheme enables rapid and global inverse identification with sparsity regularization of creep parameters from experiment displacement-time histories. In addition, we propose a phenomenological creep law with a time-dependent stress exponent that captures the sigmoidal primary creep observed in wrought INCONEL 625 and extracts its temperature dependence from DABI test at multiple temperatures. Furthermore, we employ a general creep law combining several conventional forms together with regularized inversion to identify the creep laws for 47 additional Fe-, Ni-, and Co-rich alloys and to automatically select the dominant functional form for each alloy. This workflow combined with DABI experiment provides a quantitative, high-throughput creep characterization platform that is compatible with data mining, composition-property modeling, and nonlinear structural optimization with creep behavior across a large alloy design space.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器学习驱动的合金成分空间蠕变规律发现</div>
<div class="mono" style="margin-top:8px">传统结构合金的高温蠕变表征依赖串行单轴测试，在探索广阔合金成分空间和材料发现方面效率低下。本研究提出一种基于凹坑阵列鼓胀仪（DABI）构型的机器学习辅助高通量蠕变规律识别框架，可在单次实验中并行测试25种不同合金制备的凹坑。通过三维数字图像相关技术，测量惰性气体压力下随时间演化的蠕变鼓胀全场表面位移。我们训练循环神经网络（RNN）作为代理模型，将蠕变参数和加载条件映射至DABI的时变变形响应。该代理模型与粒子群优化算法结合，可通过实验位移-时间历程实现带稀疏正则化的蠕变参数快速全局反演识别。此外，我们提出具有时变应力指数的唯象蠕变定律，可捕捉锻造INCONEL 625中观察到的S形初始蠕变，并通过多温度DABI测试提取其温度依赖性。进一步地，我们采用融合多种传统形式的广义蠕变定律与正则化反演方法，识别了47种富铁、镍、钴合金的蠕变规律，并自动筛选各合金的主导函数形式。该工作流结合DABI实验构建了定量化高通量蠕变表征平台，兼容数据挖掘、成分-性能建模及大范围合金设计空间内考虑蠕变行为的非线性结构优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of traditional serial creep testing for exploring vast alloy compositional spaces, this paper introduces a machine-learning-assisted, high-throughput framework using a dimple array bulge instrument (DABI) to enable parallel creep testing of 25 different alloys simultaneously. The method employs 3D digital image correlation to measure full-field surface displacements and trains a recurrent neural network as a surrogate model, which is coupled with particle swarm optimization for inverse identification of creep parameters from experimental data. Key experimental results include the derivation of a phenomenological creep law with a time-dependent stress exponent for INCONEL 625, capturing its sigmoidal primary creep and temperature dependence, and the successful identification of creep laws for 47 additional Fe-, Ni-, and Co-rich alloys, automatically selecting dominant functional forms for each, thereby establishing a quantitative platform for high-throughput creep characterization across a large alloy design space.</div>
<div class="mono" style="margin-top:8px">针对传统串行蠕变测试在探索广阔合金成分空间时效率低下的问题，本文提出了一种机器学习辅助的高通量框架，采用凹坑阵列鼓胀仪器（DABI）配置，可在单次实验中并行测试25种不同合金的蠕变行为。该方法利用三维数字图像相关技术测量全场表面位移，训练循环神经网络作为代理模型，并结合粒子群优化算法，从实验位移-时间历史中逆向识别蠕变参数。主要实验结果包括：为锻造INCONEL 625推导出具有时间依赖性应力指数的唯象蠕变定律，捕捉其S形初始蠕变和温度依赖性；并成功识别了47种富铁、镍和钴合金的蠕变定律，自动为每种合金选择主导函数形式，从而建立了一个适用于大数据挖掘、成分-性能建模和非线性结构优化的高通量蠕变表征平台。</div>
</details>
</div>
<div class="card">
<div class="title">Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation</div>
<div class="meta-line">Authors: Abdolazim Rezaei, Mehdi Sookhak, Ahmad Patooghy, Shahab S. Band, Amir Mosavi</div>
<div class="meta-line">First: 2025-06-18T20:02:24+00:00 · Latest: 2026-01-13T20:26:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.15854v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.15854v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Intelligent Transportation Systems (ITS) rely on a variety of devices that frequently process privacy-sensitive data. Roadside units are important because they use AI-equipped cameras to detect traffic violations in Connected and Autonomous Vehicles (CAV). However, although the interior of a vehicle is generally considered a private space, the privacy risks associated with captured imagery remain a major concern, as such data can be misused for identity theft, profiling, or unauthorized commercial purposes. Methods like face blurring reduce privacy risks, however individuals&#x27; privacy can still be compromised. This paper introduces a novel privacy-preserving framework that leverages feedback-based reinforcement learning (RL) and vision-language models (VLMs) to protect sensitive visual information captured by AIE cameras. The proposed idea transforms images into textual descriptions using an innovative method while the main scene details are preserved and protects privacy. A hierarchical RL strategy is employed to iteratively refine the generated text, enhancing both semantic accuracy and privacy. Unlike prior captioning-based methods, our model incorporates an iterative reinforcement-learning cycle with external knowledge feedback which progressively refines privacy-aware text. In addition to qualitative textual metric evaluations, the privacy-based metrics demonstrate significant improvements in privacy preservation where SSIM, PSNR, MSE, and SRRA values obtained using the proposed method on two different datasets outperform other methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉到文本转换的网联自动驾驶车辆隐私保护</div>
<div class="mono" style="margin-top:8px">智能交通系统依赖各类设备处理隐私敏感数据。路侧单元通过搭载人工智能的摄像头检测网联自动驾驶车辆的交通违规行为，但车辆内部作为私人空间，其图像数据存在身份盗用、用户画像构建或商业滥用的风险。现有面部模糊等方法仍存在隐私泄露隐患。本文提出一种融合反馈式强化学习与视觉语言模型的新型隐私保护框架，通过创新方法将图像转换为保留主要场景细节的文本描述。采用分层强化学习策略迭代优化生成文本，提升语义准确性与隐私保护效果。相较于传统描述生成方法，本模型引入结合外部知识反馈的迭代强化学习循环，逐步生成隐私感知文本。除定性文本指标评估外，基于隐私保护的量化指标（在两种数据集上的SSIM、PSNR、MSE和SRRA值）均显著优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the privacy risks in Intelligent Transportation Systems where AI-equipped cameras capture sensitive visual data from vehicle interiors, this paper proposes a novel privacy-preserving framework that transforms images into textual descriptions. The method employs a feedback-based reinforcement learning strategy combined with vision-language models to iteratively refine the generated text, enhancing both semantic accuracy and privacy protection by preserving scene details while removing sensitive information. Experimental results on two datasets demonstrate significant improvements in privacy metrics, including SSIM, PSNR, MSE, and SRRA, outperforming prior methods and effectively safeguarding against misuse such as identity theft or profiling.</div>
<div class="mono" style="margin-top:8px">针对智能交通系统中配备人工智能的摄像头捕获车辆内部敏感视觉数据所带来的隐私风险，本文提出了一种新颖的隐私保护框架，将图像转换为文本描述。该方法采用基于反馈的强化学习策略结合视觉语言模型，迭代优化生成的文本，在保留场景细节的同时去除敏感信息，从而提升语义准确性和隐私保护。在两个数据集上的实验结果表明，该方法在SSIM、PSNR、MSE和SRRA等隐私指标上取得显著改进，优于现有方法，有效防止了身份盗窃或画像等滥用行为。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
