<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-18 03:23</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260118_0323</div>
    <div class="row"><div class="card">
<div class="title">MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching</div>
<div class="meta-line">Authors: Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin</div>
<div class="meta-line">First: 2026-01-15T18:59:23+00:00 · Latest: 2026-01-15T18:59:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10712v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10712v1">PDF</a> · <a href="https://github.com/quchangle1/MatchTIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MatchTIR：基于二分图匹配的细粒度监督工具集成推理</div>
<div class="mono" style="margin-top:8px">工具集成推理（TIR）通过将推理步骤与外部工具交互交织，使大语言模型（LLM）能够处理复杂任务。然而，现有的强化学习方法通常依赖于结果级或轨迹级奖励，对轨迹中的所有步骤分配统一的优势值。这种粗粒度的信用分配无法区分有效工具调用与冗余或错误调用，尤其在长视野多轮次场景中。为此，我们提出MatchTIR框架，通过基于二分图匹配的轮次级奖励分配和双层优势估计引入细粒度监督。具体而言，我们将信用分配建模为预测轨迹与真实轨迹间的二分图匹配问题，利用两种分配策略生成密集的轮次级奖励。此外，为平衡局部步骤精度与全局任务成功率，我们引入双层优势估计方案，整合轮次级和轨迹级信号，为每个交互轮次分配不同的优势值。在三个基准测试上的大量实验证明了MatchTIR的优越性。值得注意的是，我们的40亿参数模型超越了多数80亿参数竞争对手，尤其在长视野和多轮次任务中。代码发布于https://github.com/quchangle1/MatchTIR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind MatchTIR is to address the coarse-grained credit assignment in existing reinforcement learning methods for Tool-Integrated Reasoning (TIR), which fail to distinguish effective tool calls from redundant or erroneous ones in long-horizon tasks. The method introduces fine-grained supervision via bipartite matching-based turn-level reward assignment, formulating credit assignment as a matching problem between predicted and ground-truth traces, and employs a dual-level advantage estimation scheme to balance local precision with global success. Experimental results on three benchmarks show that MatchTIR outperforms competitors, with a 4B model surpassing most 8B models, especially in long-horizon and multi-turn tasks.</div>
<div class="mono" style="margin-top:8px">MatchTIR的动机是解决现有工具集成推理（TIR）强化学习方法中粗粒度的信用分配问题，这些方法在长视野任务中无法区分有效工具调用与冗余或错误调用。该方法通过基于二分图匹配的轮次级奖励分配引入细粒度监督，将信用分配建模为预测轨迹与真实轨迹之间的匹配问题，并采用双层级优势估计方案以平衡局部精度与全局任务成功。在三个基准测试上的实验结果表明，MatchTIR优于竞争对手，其40亿参数模型超越了大多数80亿参数模型，尤其在长视野和多轮次任务中表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization</div>
<div class="meta-line">Authors: Minh Hieu Ha, Hung Phan, Tung Duy Doan, Tung Dao, Dao Tran, Huynh Thi Thanh Binh</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-07-28T15:26:43+00:00 · Latest: 2026-01-15T18:28:50+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20923v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.20923v3">PDF</a> · <a href="https://github.com/langkhachhoha/MPaGE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于帕累托网格引导的大语言模型实现多目标组合优化的快速高质量启发式设计</div>
<div class="mono" style="margin-top:8px">多目标组合优化问题在实际应用中频繁出现，需要同时优化相互冲突的目标。传统进化算法虽有效，但通常依赖领域知识和重复参数调优，在处理未知问题实例时灵活性受限。近期，大语言模型与进化计算的融合为自动启发式生成开辟了新途径，利用其高级语言理解和代码生成能力。然而，现有方法多聚焦单目标任务，常忽视多目标场景下的运行效率和启发式多样性等关键因素。为填补这一空白，我们提出MPaGE方法——通过帕累托网格引导的LLM进化实现MOCOP多启发式设计。该方法作为简单进化多目标优化框架的创新增强，结合帕累托前沿网格技术，通过将目标空间网格化并保留最优候选方案来引导启发式生成。MPaGE利用LLM在变异过程中优先选择语义逻辑结构相异的启发式，从而提升种群多样性并减少冗余。大量实验表明，MPaGE性能优于现有基于LLM的框架，在显著缩短运行时间的同时，取得了与传统多目标进化算法相竞争的结果。代码已开源：https://github.com/langkhachhoha/MPaGE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of traditional evolutionary algorithms in multi-objective combinatorial optimization (MOCOP), which often require extensive domain expertise and parameter tuning, by integrating Large Language Models (LLMs) to automate heuristic design. The proposed method, MPaGE, enhances the Simple Evolutionary Multiobjective Optimization (SEMO) framework using a Pareto Front Grid (PFG) technique to partition the objective space into grids, retaining top-performing candidates to guide LLMs in generating semantically diverse heuristics, thereby improving population diversity and reducing redundancy. Experimental results show that MPaGE outperforms existing LLM-based frameworks, achieves competitive performance with traditional multi-objective evolutionary algorithms, and operates with significantly faster runtime.</div>
<div class="mono" style="margin-top:8px">本文针对多目标组合优化问题中传统进化算法依赖领域知识和参数调优、灵活性不足的局限，提出通过集成大语言模型来自动设计启发式方法。所提出的MPaGE方法基于简单进化多目标优化框架，利用帕累托前沿网格技术将目标空间划分为网格，保留高性能候选方案以指导大语言模型生成语义多样的启发式规则，从而提升种群多样性并减少冗余。实验结果表明，MPaGE在性能上优于现有基于大语言模型的框架，与传统多目标进化算法竞争相当，且运行速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Uncertainty Guided Policy Learning for Multimodal Reasoning</div>
<div class="meta-line">Authors: Rui Liu, Dian Yu, Tong Zheng, Runpeng Dai, Zongxia Li, Wenhao Yu, Zhenwen Liang, Linfeng Song, Haitao Mi, Pratap Tokekar, Dong Yu</div>
<div class="meta-line">First: 2025-10-01T20:32:08+00:00 · Latest: 2026-01-15T17:51:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01444v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has advanced reasoning capabilities in multimodal large language models. However, existing methods typically treat visual inputs as deterministic, overlooking the perceptual ambiguity inherent to the visual modality. Consequently, they fail to distinguish whether a model&#x27;s uncertainty stems from complex reasoning or ambiguous perception, preventing the targeted allocation of exploration or learning signals. To address this gap, we introduce DUPL, a dual-uncertainty guided policy learning approach for multimodal RLVR that quantifies and leverages both perceptual uncertainty (via symmetric KL divergence) and output uncertainty (via policy entropy) to guide policy updates. By establishing an uncertainty-driven feedback loop and employing a dynamic branch prioritization mechanism, DUPL recalibrates the policy advantage to focus learning on states with high perceptual or decisional ambiguity, enabling effective targeted exploration beyond passive data augmentation. Implemented on top of GRPO and evaluated on six multimodal mathematical and general-domain reasoning benchmarks, DUPL improves Qwen2.5-VL 3B and 7B models, achieving accuracy gains of up to 11.2% on visual math tasks and up to 7.1% on general-domain reasoning tasks, while consistently outperforming GRPO. These results demonstrate that dual-uncertainty guided policy learning is an effective and generalizable approach for multimodal RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双不确定性引导的多模态推理策略学习</div>
<div class="mono" style="margin-top:8px">基于可验证奖励的强化学习（RLVR）提升了多模态大语言模型的推理能力。然而，现有方法通常将视觉输入视为确定性信息，忽略了视觉模态固有的感知模糊性。这导致模型无法区分其不确定性是源于复杂推理还是模糊感知，从而难以针对性地分配探索或学习信号。为解决这一问题，我们提出了DUPL——一种双不确定性引导的多模态RLVR策略学习方法，该方法通过对称KL散度量化感知不确定性，通过策略熵量化输出不确定性，并利用二者共同指导策略更新。通过建立不确定性驱动的反馈循环并采用动态分支优先级机制，DUPL重新校准策略优势，将学习重点聚焦于具有高感知或决策模糊性的状态，实现了超越被动数据增强的有效定向探索。基于GRPO框架实现并在六个多模态数学及通用领域推理基准上评估，DUPL显著提升了Qwen2.5-VL 3B和7B模型的性能：在视觉数学任务上准确率最高提升11.2%，在通用领域推理任务上最高提升7.1%，且持续优于GRPO基线。这些结果表明，双不确定性引导的策略学习是一种有效且可泛化的多模态RLVR方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the oversight of perceptual ambiguity in existing multimodal reinforcement learning with verifiable rewards (RLVR) methods, which treat visual inputs as deterministic and thus conflate reasoning and perceptual uncertainty, this paper introduces DUPL, a dual-uncertainty guided policy learning approach. The method quantifies perceptual uncertainty via symmetric KL divergence and output uncertainty via policy entropy, using them to establish an uncertainty-driven feedback loop and a dynamic branch prioritization mechanism that recalibrates the policy advantage to focus learning on states with high ambiguity. Experimental results on six multimodal reasoning benchmarks show that DUPL, implemented on GRPO, improves Qwen2.5-VL models, achieving accuracy gains of up to 11.2% on visual math tasks and 7.1% on general-domain tasks, consistently outperforming the baseline.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有基于可验证奖励的多模态强化学习方法通常将视觉输入视为确定性信息，忽略了其固有的感知模糊性，从而无法区分模型的不确定性是源于复杂推理还是模糊感知。为此，作者提出了DUPL，一种双重不确定性引导的策略学习方法，该方法通过对称KL散度量化和利用感知不确定性，并通过策略熵量化输出不确定性，进而建立一个不确定性驱动的反馈循环和动态分支优先级机制，以重新校准策略优势，将学习重点集中在高模糊性状态上。在六个多模态数学和通用领域推理基准上的实验结果表明，基于GRPO实现的DUPL提升了Qwen2.5-VL 3B和7B模型的性能，在视觉数学任务上准确率最高提升11.2%，在通用领域推理任务上最高提升7.1%，且持续优于GRPO基线。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yucheng Wang, Yufei He, Jiaying Wu, Yilun Zhao, See-Kiong Ng, Cynthia Breazeal, Anh Tuan Luu, Hae Won Park, Bryan Hooi</div>
<div class="meta-line">First: 2026-01-13T17:48:43+00:00 · Latest: 2026-01-15T17:24:46+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08763v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08763v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励稀缺性：面向大语言模型创造性问题求解的独特性感知强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为大语言模型后训练的核心范式，尤其在复杂推理任务中，但其常面临探索坍缩问题：策略过早集中于少数主导推理模式，虽能提升pass@1指标，却限制了轨迹级多样性及pass@k指标的增益。本文指出该问题源于对局部词元行为的正则化，而非对解集多样性的考量。为此，我们提出独特性感知强化学习——一种在轨迹层级显式奖励采用稀缺高层策略的正确解的优化目标。该方法基于大语言模型的评判器，将同一问题的求解轨迹按高层策略聚类（忽略表面差异），并依据聚类规模对策略优势进行逆向加权，使得正确但新颖的策略获得比冗余策略更高的奖励。在数学、物理和医学推理基准测试中，本方法在大规模采样预算下持续提升pass@$k$指标，在保持pass@1水平的同时显著提高pass@$k$曲线下面积（AUC@$K$），并能维持探索过程，在大规模应用中发掘更多样化的求解策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address exploration collapse in reinforcement learning (RL) for large language models (LLMs), where policies prematurely converge on a narrow set of reasoning patterns, limiting solution diversity and gains in pass@k metrics. The method introduces Uniqueness-Aware Reinforcement Learning, which uses an LLM-based judge to cluster rollouts by high-level solution strategies and reweights policy advantages inversely with cluster size, thereby rewarding correct but rare strategies over redundant ones. Experimental results across mathematics, physics, and medical reasoning benchmarks show that this approach consistently improves pass@k and the area under the pass@k curve without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型强化学习中的探索崩溃问题，即策略过早集中于少数主导推理模式，限制了解决方案多样性和pass@k指标的提升。方法上提出了独特性感知强化学习，利用基于LLM的评判器根据高层解决策略对推演过程进行聚类，并依据聚类大小反向调整策略优势，从而奖励正确但罕见的策略而非冗余策略。在数学、物理和医学推理基准测试中的实验结果表明，该方法在不牺牲pass@1的情况下，持续提升了pass@k和pass@k曲线下面积，同时保持了探索性并大规模发现了更多样化的解决策略。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park</div>
<div class="meta-line">First: 2026-01-14T17:57:43+00:00 · Latest: 2026-01-15T17:20:36+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09667v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09667v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作式多智能体测试时强化学习推理框架</div>
<div class="mono" style="margin-top:8px">多智能体系统已发展为适用于多种应用的实用LLM驱动协作体，其通过多样性与交叉验证获得鲁棒性。然而多智能体强化学习训练存在资源密集且不稳定的问题：智能体间的协同适应会引发非平稳性，奖励信号通常稀疏且方差较高。为此，我们提出\textbf{多智能体测试时强化学习框架}，该框架在推理阶段将结构化文本经验注入多智能体决策过程。MATTRL构建面向多轮讨论的专家团队，检索并整合测试时经验，最终通过共识机制形成决策。我们还研究了信用分配机制以构建轮次级经验池，并将其重新注入对话流程。在医学、数学、教育等领域的挑战性基准测试中，MATTRL相较多智能体基线平均准确率提升3.67%，较可比单智能体基线提升8.67%。消融实验检验了不同信用分配方案，并详细比较了其对训练结果的影响。MATTRL为分布偏移鲁棒的多智能体推理提供了一条稳定、高效且无需调参的实现路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the resource-intensive and unstable nature of multi-agent reinforcement learning (MARL) training, which suffers from non-stationarity and sparse rewards, this work introduces Multi-Agent Test-Time Reinforcement Learning (MATTRL). The method injects structured textual experience into multi-agent deliberation at inference time by forming a team of specialists, retrieving and integrating test-time experiences, and reaching consensus for decisions, while also studying credit assignment for constructing a turn-level experience pool. Experimental results across medicine, math, and education benchmarks show that MATTRL improves accuracy by an average of 3.67% over a multi-agent baseline and 8.67% over single-agent baselines, with ablation studies analyzing different credit-assignment schemes.</div>
<div class="mono" style="margin-top:8px">针对多智能体强化学习训练资源消耗大、不稳定且存在非平稳性和稀疏奖励的问题，本研究提出了多智能体测试时强化学习框架。该方法在推理时通过组建专家团队、检索并整合测试时经验、达成共识进行决策，将结构化文本经验注入多智能体审议过程，并研究了用于构建轮次级经验池的信用分配机制。在医学、数学和教育等领域的基准测试中，该框架相比多智能体基线平均准确率提升3.67%，相比单智能体基线提升8.67%，消融实验还分析了不同信用分配方案对训练结果的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Combinatorial Optimization Augmented Machine Learning</div>
<div class="meta-line">Authors: Maximilian Schiffer, Heiko Hoppe, Yue Su, Louis Bouvier, Axel Parmentier</div>
<div class="meta-line">First: 2026-01-15T16:55:19+00:00 · Latest: 2026-01-15T16:55:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10583v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10583v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>组合优化增强机器学习</div>
<div class="mono" style="margin-top:8px">组合优化增强机器学习（COAML）作为一种将预测模型与组合决策相结合的新兴范式，通过将组合优化求解器嵌入学习流程，构建出既数据驱动又保持可行性的决策策略，从而连接了机器学习、运筹学与随机优化的研究传统。本文系统综述了COAML领域的最新进展：提出统一的COAML流程框架，阐述其方法论构成要素，并形式化其与经验成本最小化的理论关联；基于不确定性与决策结构构建问题分类体系，依此梳理静态与动态问题的算法路径，涵盖调度、车辆路径、随机规划及强化学习等跨领域应用，并从经验成本最小化、模仿学习与强化学习三个维度整合方法论贡献；最后指明关键研究前沿。本综述旨在为领域提供入门导引，并为组合优化与机器学习的交叉研究规划发展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to integrate predictive machine learning models with structured decision-making under constraints, leading to the emerging field of Combinatorial Optimization Augmented Machine Learning (COAML). The method involves embedding combinatorial optimization oracles into learning pipelines to create data-driven, feasibility-preserving policies, and the authors provide a unifying framework, taxonomy, and algorithmic review. Main experimental results are synthesized from surveyed applications across domains like scheduling and vehicle routing, demonstrating COAML&#x27;s effectiveness in empirical cost minimization, imitation learning, and reinforcement learning settings.</div>
<div class="mono" style="margin-top:8px">本文的动机源于将预测性机器学习模型与约束下的结构化决策相结合的需求，从而催生了组合优化增强机器学习这一新兴领域。其方法通过在学习流程中嵌入组合优化预言机，构建数据驱动且保持可行性的策略，作者提出了统一框架、分类体系并回顾了相关算法。主要实验结果综合了调度、车辆路径等领域的应用调研，展示了COAML在经验成本最小化、模仿学习和强化学习场景中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning</div>
<div class="meta-line">Authors: Nilin Abrahamsen</div>
<div class="meta-line">First: 2026-01-15T15:16:15+00:00 · Latest: 2026-01-15T15:16:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10498v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10498v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>投影微批次累积实现强化学习的无参考近端策略更新</div>
<div class="mono" style="margin-top:8px">本文介绍投影微批次累积（PROMA），一种用于大语言模型微调的近端策略更新方法。PROMA通过在微批次聚合前投影掉序列维度的梯度分量，实现跨微批次的策略梯度累积。该投影在反向传播过程中逐层应用，无需额外前向或反向传播即可高效实现。实证表明，PROMA比GRPO能更严格地控制局部KL散度，从而实现更稳定的策略学习。与PPO和GRPO不同，PROMA无需依赖参考策略或似然比裁剪即可实现近端更新，且不会引发熵崩溃。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Projected Microbatch Accumulation (PROMA), a method motivated by the need for stable, reference-free proximal policy updates in reinforcement learning fine-tuning of large language models. PROMA&#x27;s approach involves accumulating policy gradients across microbatches after projecting out sequence-wise gradient components layer-wise during the backward pass, which avoids extra computational overhead. Experimental results show that PROMA provides tighter control of local KL divergence compared to GRPO, leading to more stable learning, and unlike PPO or GRPO, it prevents entropy collapse without relying on a reference policy or likelihood-ratio clipping.</div>
<div class="mono" style="margin-top:8px">本文提出了投影微批次累积（PROMA）方法，其动机在于为大语言模型的强化学习微调提供稳定且无需参考策略的近端策略更新。PROMA通过在反向传播中逐层投影掉序列级梯度分量，然后在微批次间累积策略梯度，实现了高效计算而无需额外前向或反向传播。实验结果表明，与GRPO相比，PROMA能更严格地控制局部KL散度，从而实现更稳定的策略学习，并且不同于PPO和GRPO，它在不引发熵塌缩的情况下实现了近端更新，且不依赖参考策略或似然比裁剪。</div>
</details>
</div>
<div class="card">
<div class="title">Urban Socio-Semantic Segmentation with Vision-Language Reasoning</div>
<div class="meta-line">Authors: Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li</div>
<div class="meta-line">First: 2026-01-15T15:00:36+00:00 · Latest: 2026-01-15T15:00:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10477v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10477v1">PDF</a> · <a href="https://github.com/AMAP-ML/SocioReasoner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach&#x27;s gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言推理的城市社会语义分割</div>
<div class="mono" style="margin-top:8px">作为人类活动的枢纽，城市地表蕴含丰富的语义实体。从卫星图像中分割这些多样实体对一系列下游应用至关重要。当前先进的语义分割模型能可靠分割由物理属性定义的实体（如建筑、水体），但在处理社会定义类别（如学校、公园）时仍面临挑战。本研究通过视觉语言模型推理实现了社会语义分割。为此，我们构建了名为SocioSeg的城市社会语义分割数据集，该资源包含卫星影像、数字地图及按层级结构组织的社会语义实体像素级标注。同时，我们提出名为SocioReasoner的新型视觉语言推理框架，通过跨模态识别与多阶段推理模拟人类识别标注社会语义实体的认知过程。采用强化学习优化这一不可微过程，以激发视觉语言模型的推理能力。实验表明，该方法优于当前最先进模型，并展现出强大的零样本泛化性能。数据集与代码已开源：https://github.com/AMAP-ML/SocioReasoner。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of segmenting socially defined urban entities like schools and parks from satellite imagery, which existing models struggle with, this paper introduces a new dataset, SocioSeg, and a vision-language reasoning framework called SocioReasoner. The method simulates human annotation processes through cross-modal recognition and multi-stage reasoning, optimized via reinforcement learning to handle non-differentiable steps. Experimental results show that this approach outperforms state-of-the-art models and exhibits strong zero-shot generalization capabilities.</div>
<div class="mono" style="margin-top:8px">针对现有模型难以从卫星图像中分割学校、公园等社会语义实体的挑战，本文引入了新数据集SocioSeg和视觉语言推理框架SocioReasoner。该方法通过跨模态识别和多阶段推理模拟人类标注过程，并利用强化学习优化不可微分的推理步骤。实验结果表明，该方法优于现有先进模型，并展现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models</div>
<div class="meta-line">Authors: Ziming Dai, Dabiao Ma, Jinle Tong, Mengyuan Han, Jian Yang, Haojun Fei</div>
<div class="meta-line">First: 2026-01-15T14:48:52+00:00 · Latest: 2026-01-15T14:48:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10457v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10457v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being &quot;non-intrusive&quot;. It treats the legacy model as a frozen model and performs targeted repairs on &quot;hard regions&quot; where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NSR-Boost：面向工业遗留模型的神经符号残差增强框架</div>
<div class="mono" style="margin-top:8px">尽管梯度提升决策树（GBDT）在工业表格应用中占据主导地位，但在高并发生产环境中升级遗留模型仍面临高昂的重新训练成本和系统性风险。为解决这一问题，我们提出了NSR-Boost，一种专为工业场景设计的神经符号残差增强框架。其核心优势在于“非侵入性”：它将遗留模型视为冻结模型，仅对预测失败的“困难区域”进行针对性修复。该框架包含三个关键阶段：首先通过残差定位困难区域，随后利用大语言模型（LLM）生成符号化代码结构，并结合贝叶斯优化微调参数以构建可解释的专家模块，最后通过轻量级聚合器动态整合专家模块与遗留模型的输出。我们报告了NSR-Boost在Qfin Holdings核心金融风控系统中的成功部署案例。该框架不仅在六个公共数据集和一个私有数据集上显著优于现有最优基线，更重要的是在真实在线数据上展现出卓越的性能提升。最终，它有效捕捉了传统模型遗漏的长尾风险，为工业界提供了一种安全、低成本的模型演进范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable safe and low-cost upgrades to legacy Gradient Boosted Decision Tree (GBDT) models in high-stakes industrial settings, where full retraining is often prohibitively expensive and risky. The proposed method, NSR-Boost, is a non-intrusive neuro-symbolic residual boosting framework that freezes the legacy model and targets repairs on its erroneous &quot;hard regions&quot;. It identifies these regions via prediction residuals, generates interpretable symbolic correction experts using a Large Language Model and Bayesian optimization, and then dynamically integrates them via a lightweight aggregator. The main experimental results demonstrate successful deployment in a financial risk control system, with the framework outperforming state-of-the-art baselines on six public and one private dataset, showing significant performance gains on real-world online data and effectively capturing long-tail risks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，为高风险工业环境中遗留的梯度提升决策树模型提供一种安全、低成本的升级方案，以避免全模型重新训练带来的高昂成本和系统风险。所提出的方法NSR-Boost是一个非侵入式的神经符号残差提升框架，它将遗留模型冻结，并专门针对其预测错误的“困难区域”进行修复。该方法通过残差识别困难区域，利用大语言模型和贝叶斯优化生成可解释的符号化专家修正器，最后通过一个轻量级聚合器动态整合专家输出与遗留模型结果。主要实验结果表明，该框架已成功部署于一个核心金融风控系统，在六个公共数据集和一个私有数据集上均优于现有先进基线，在真实在线数据上表现出显著的性能提升，并能有效捕捉传统模型遗漏的长尾风险。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching</div>
<div class="meta-line">Authors: Nadav Merlis</div>
<div class="meta-line">First: 2026-01-15T14:09:49+00:00 · Latest: 2026-01-15T14:09:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10418v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10418v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes (&#x27;fixed batching policies&#x27;), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应批处理的带多步前瞻信息强化学习</div>
<div class="mono" style="margin-top:8px">本研究探讨具有多步前瞻信息的表格型强化学习问题。在决策前，学习者能观测未来ℓ步的状态转移与奖励实现：即在任何可能行动路径下，智能体将抵达的确切状态及将获得的奖励。尽管此类信息能显著提升价值函数，但寻找最优策略是NP难问题，通常采用两种可处理的启发式方法之一：按预定义尺寸分块处理前瞻信息（&#x27;固定批处理策略&#x27;）或模型预测控制。我们首先阐明这两种方法的局限性，并提出采用自适应（状态依赖）批处理方式利用前瞻信息，称此类策略为自适应批处理策略（ABP）。我们推导了这些策略的最优贝尔曼方程，并设计了一种乐观的遗憾最小化算法，可在未知环境中交互时学习最优ABP。我们的遗憾界在阶数上是最优的，仅可能包含前瞻视野ℓ的因子（通常可视为小常数）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of leveraging multi-step lookahead information in tabular reinforcement learning, where observing future transitions and rewards can enhance value but optimal policy computation is NP-hard. The authors critique existing heuristics like fixed batching and model predictive control, proposing instead adaptive batching policies (ABPs) that use state-dependent batch sizes to process lookahead information more flexibly. They derive optimal Bellman equations for ABPs and develop an optimistic regret-minimization algorithm that learns optimal ABPs in unknown environments, with regret bounds shown to be near-optimal up to a factor of the lookahead horizon, which is typically small.</div>
<div class="mono" style="margin-top:8px">本文研究了表格强化学习中利用多步前瞻信息的挑战，其中观察未来转移和奖励能提升价值，但最优策略计算是NP难问题。作者批评了固定批处理和模型预测控制等现有启发式方法，提出了自适应批处理策略（ABPs），通过状态依赖的批大小更灵活地处理前瞻信息。他们推导了ABPs的最优贝尔曼方程，并设计了一种乐观的遗憾最小化算法，用于在未知环境中学习最优ABPs，其遗憾界被证明在通常较小的前瞻视野因子内接近最优。</div>
</details>
</div>
<div class="card">
<div class="title">CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Yuanjie Zhao, Junnan Qiu, Yue Ding, Jie Li</div>
<div class="meta-line">First: 2026-01-15T13:57:52+00:00 · Latest: 2026-01-15T13:57:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10407v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10407v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable Out-of-Distribution (OOD) triggers. In this paper, we propose CS-GBA (Critical Sample-based Gradient-guided Backdoor Attack), a novel framework designed to achieve high stealthiness and destructiveness under a strict budget. Leveraging the theoretical insight that samples with high Temporal Difference (TD) errors are pivotal for value function convergence, we introduce an adaptive Critical Sample Selection strategy that concentrates the attack budget on the most influential transitions. To evade OOD detection, we propose a Correlation-Breaking Trigger mechanism that exploits the physical mutual exclusivity of state features (e.g., 95th percentile boundaries) to remain statistically concealed. Furthermore, we replace the conventional label inversion with a Gradient-Guided Action Generation mechanism, which searches for worst-case actions within the data manifold using the victim Q-network&#x27;s gradient. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms state-of-the-art baselines, achieving high attack success rates against representative safety-constrained algorithms with a minimal 5% poisoning budget, while maintaining the agent&#x27;s performance in clean environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CS-GBA：一种基于关键样本的梯度引导后门攻击方法，针对离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）能够从静态数据集中优化策略，但本质上易受后门攻击。现有攻击策略通常因低效的随机投毒和易于检测的分布外（OOD）触发器，在面对安全约束算法（如CQL）时效果有限。本文提出CS-GBA（基于关键样本的梯度引导后门攻击），这是一种新颖的框架，旨在严格预算下实现高隐蔽性和破坏性。基于高时序差分（TD）误差样本对价值函数收敛至关重要的理论洞见，我们引入自适应关键样本选择策略，将攻击预算集中于最具影响力的状态转移上。为规避OOD检测，提出相关性破坏触发器机制，利用状态特征的物理互斥性（如95%分位数边界）保持统计隐蔽性。此外，我们以梯度引导动作生成机制替代传统的标签反转，利用受害者Q网络的梯度在数据流形中搜索最差动作。在D4RL基准测试上的实验结果表明，该方法显著优于现有基线，仅用5%的投毒预算即可对代表性安全约束算法实现高攻击成功率，同时在干净环境中保持智能体性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of offline reinforcement learning (RL) to stealthy backdoor attacks, motivated by the limitations of existing methods which are inefficient and detectable by safety-constrained algorithms like CQL. The proposed method, CS-GBA, introduces a three-part framework: an adaptive Critical Sample Selection strategy that focuses poisoning on transitions with high Temporal Difference errors for maximum impact, a Correlation-Breaking Trigger mechanism that uses physical feature boundaries to avoid Out-of-Distribution detection, and a Gradient-Guided Action Generation that finds worst-case actions via the victim Q-network&#x27;s gradients instead of simple label inversion. Experimental results on D4RL benchmarks show the method achieves high attack success rates against safety-constrained algorithms with only a 5% poisoning budget, while preserving normal performance in clean environments, outperforming prior baselines.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习对隐蔽后门攻击的脆弱性展开研究，其动机是现有攻击方法效率低下且易被CQL等安全约束算法检测。所提出的CS-GBA方法包含一个三部分框架：采用自适应关键样本选择策略，将攻击预算集中于具有高时序差分误差的转换以最大化影响；设计相关性破坏触发器机制，利用状态特征的物理互斥边界避免分布外检测；以及使用梯度引导动作生成，通过受害者Q网络的梯度寻找最差动作，而非简单标签反转。在D4RL基准上的实验结果表明，该方法仅用5%的中毒预算就能对安全约束算法实现高攻击成功率，同时在干净环境中保持智能体性能，显著优于现有基线。</div>
</details>
</div>
<div class="card">
<div class="title">Bootstrap Off-policy with World Model</div>
<div class="meta-line">Authors: Guojian Zhan, Likun Wang, Xiangteng Zhang, Jiaxin Gao, Masayoshi Tomizuka, Shengbo Eben Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-01T06:33:04+00:00 · Latest: 2026-01-15T13:38:44+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00423v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00423v3">PDF</a> · <a href="https://github.com/molumitu/BOOM_MBRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy&#x27;s actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner&#x27;s non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner&#x27;s action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于世界模型的离线策略自举</div>
<div class="mono" style="margin-top:8px">在线规划在强化学习中已被证明能有效提升样本效率与最终性能。然而，将规划用于环境交互时，收集数据与策略实际行为之间必然产生偏差，从而损害模型学习与策略改进。为此，我们提出BOOM（基于世界模型的离线策略自举）框架，通过自举循环紧密整合规划与离线学习：策略初始化规划器，规划器通过行为对齐优化动作以自举策略。该循环由联合学习的世界模型支持，使规划器能模拟未来轨迹并提供价值目标以促进策略改进。BOOM的核心是无似然对齐损失，它利用规划器的非参数动作分布自举策略，并结合软价值加权机制——该机制优先考虑高回报行为，并缓解回放缓冲区内规划器动作质量的波动性。在DeepMind Control Suite和Humanoid-Bench等高维环境上的实验表明，BOOM在训练稳定性和最终性能方面均达到最先进水平。代码发布于https://github.com/molumitu/BOOM_MBRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue in reinforcement learning where online planning for environment interaction creates a mismatch between collected data and the policy&#x27;s true behavior, harming both model learning and policy improvement. To solve this, the authors propose BOOM, a framework that integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to align and bootstrap the policy, supported by a jointly learned world model for trajectory simulation and value targets. A key innovation is a likelihood-free alignment loss that bootstraps the policy using the planner&#x27;s non-parametric action distribution, along with a soft value-weighted mechanism to prioritize high-return behaviors and reduce variability in planner action quality within the replay buffer. Experimental results on the DeepMind Control Suite and Humanoid-Bench demonstrate that BOOM achieves state-of-the-art performance in both training stability and final outcomes.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中在线规划用于环境交互时，所收集数据与策略实际行为之间存在的偏差问题，该偏差会损害模型学习和策略改进。为解决此问题，作者提出了BOOM框架，通过一个引导循环紧密集成规划与离线学习：策略初始化规划器，规划器则通过行为对齐来优化动作以引导策略，并辅以一个联合学习的世界模型用于轨迹模拟和价值目标。核心创新包括一种无似然对齐损失，利用规划器的非参数动作分布来引导策略，以及一种软价值加权机制，优先考虑高回报行为并减少回放缓冲区中规划器动作质量的变异性。在DeepMind Control Suite和Humanoid-Bench上的实验结果表明，BOOM在训练稳定性和最终性能方面均达到了最先进水平。</div>
</details>
</div>
<div class="card">
<div class="title">Probabilistic Insights for Efficient Exploration Strategies in Reinforcement Learning</div>
<div class="meta-line">Authors: Ernesto Garcia, Paola Bermolen, Matthieu Jonckheere, Seva Shneer</div>
<div class="meta-line">First: 2025-03-05T14:53:32+00:00 · Latest: 2026-01-15T13:24:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03565v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.03565v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate efficient exploration strategies of environments with unknown stochastic dynamics and sparse rewards. Specifically, we analyze first the impact of parallel simulations on the probability of reaching rare states within a finite time budget. Using simplified models based on random walks and Lévy processes, we provide analytical results that demonstrate a phase transition in reaching probabilities as a function of the number of parallel simulations. We identify an optimal number of parallel simulations that balances exploration diversity and time allocation. Additionally, we analyze a restarting mechanism that exponentially enhances the probability of success by redirecting efforts toward more promising regions of the state space. Our findings contribute to a more qualitative and quantitative theory of some exploration schemes in reinforcement learning, offering insights into developing more efficient strategies for environments characterized by rare events.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中高效探索策略的概率性洞见</div>
<div class="mono" style="margin-top:8px">本研究针对具有未知随机动态和稀疏奖励的环境，探究高效探索策略。首先分析了在有限时间预算下，并行仿真对到达稀有状态概率的影响。通过基于随机游走和莱维过程的简化模型，我们提供了解析结果，揭示了到达概率随并行仿真数量变化的相变现象，并确定了平衡探索多样性与时间分配的最优并行仿真数量。此外，分析了一种重启机制，该机制通过将探索资源重新导向状态空间中更具潜力的区域，以指数级提升成功概率。这些发现为强化学习中部分探索方案提供了更定性与定量的理论支持，为开发针对稀有事件环境的更高效策略提供了洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of efficient exploration in reinforcement learning environments with unknown stochastic dynamics and sparse rewards, where reaching rare states is critical. The method employs simplified models based on random walks and Lévy processes to analytically study the impact of parallel simulations and a restarting mechanism on exploration efficiency. The main experimental results reveal a phase transition in the probability of reaching rare states as the number of parallel simulations varies, identifying an optimal number that balances exploration diversity and time allocation, and show that the restarting mechanism exponentially boosts success probability by reallocating effort to more promising state-space regions.</div>
<div class="mono" style="margin-top:8px">本文旨在解决具有未知随机动态和稀疏奖励的强化学习环境中高效探索的难题，其中到达稀有状态至关重要。方法上，通过基于随机游走和Lévy过程的简化模型，对并行模拟和重启机制在探索效率上的影响进行了理论分析。主要实验结果揭示了到达稀有状态的概率随并行模拟数量变化而出现相变，确定了平衡探索多样性和时间分配的最优并行模拟数量，并表明重启机制通过将努力重新分配到更有希望的状态空间区域，能指数级提升成功概率。</div>
</details>
</div>
<div class="card">
<div class="title">SuS: Strategy-aware Surprise for Intrinsic Exploration</div>
<div class="meta-line">Authors: Mark Kashirskiy, Ilya Makarov</div>
<div class="meta-line">First: 2026-01-15T12:48:59+00:00 · Latest: 2026-01-15T12:48:59+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures, 3 tables. Code available at https://github.com/mariklolik/sus</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10349v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10349v1">PDF</a> · <a href="https://github.com/mariklolik/sus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent&#x27;s current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SuS：面向内在探索的策略感知惊喜</div>
<div class="mono" style="margin-top:8px">本文提出策略感知惊喜（SuS），一种新颖的内在激励框架，利用前后预测失配作为强化学习探索的新颖性信号。与传统仅依赖状态预测误差的好奇心驱动方法不同，SuS引入两个互补组件：策略稳定性（SS）与策略惊喜（SuS）。SS衡量时序步骤间行为策略的一致性，SuS则捕捉智能体当前策略表征下的意外结果。我们通过习得的加权系数融合两种信号构建组合奖励函数。在基于大语言模型的数学推理任务上评估SuS，结果显示其在准确性与解多样性方面均有显著提升。消融实验证实移除任一组件会导致至少10%的性能下降，验证了方法的协同性。相比基线方法，SuS在Pass@1指标上提升17.4%，Pass@5指标提升26.4%，同时在训练全程保持更高的策略多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Strategy-aware Surprise (SuS), a novel intrinsic motivation framework designed to enhance exploration in reinforcement learning by addressing limitations of traditional curiosity-driven methods that rely solely on state prediction error. The method incorporates two complementary components: Strategy Stability (SS), which measures consistency in behavioral strategy over time, and Strategy Surprise (SuS), which captures unexpected outcomes relative to the agent&#x27;s current strategy representation, with both signals combined through learned weighting coefficients. Experimental results on mathematical reasoning tasks using large language models show significant improvements, including a 17.4% increase in Pass@1 and a 26.4% increase in Pass@5 over baselines, along with higher solution diversity, while ablation studies confirm that removing either component leads to at least 10% performance degradation, validating the synergistic approach.</div>
<div class="mono" style="margin-top:8px">本文提出了策略感知惊喜（SuS），这是一种新颖的内在动机框架，旨在通过解决传统仅依赖状态预测误差的好奇心驱动方法的局限性，来增强强化学习中的探索能力。该方法包含两个互补组件：策略稳定性（SS），用于衡量行为策略随时间的一致性；以及策略惊喜（SuS），用于捕捉相对于智能体当前策略表示的意外结果，两者通过学习的权重系数结合。在基于大语言模型的数学推理任务上的实验结果显示，该方法取得了显著改进，包括Pass@1指标提升17.4%和Pass@5指标提升26.4%，同时保持了更高的解决方案多样性；消融研究证实，移除任一组件都会导致至少10%的性能下降，从而验证了该协同方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing Safe Mechanical Ventilation Using Offline RL With Hybrid Actions and Clinically Aligned Rewards</div>
<div class="meta-line">Authors: Muhammad Hamza Yousuf, Jason Li, Sahar Vahdati, Raphael Theilen, Jakob Wittenstein, Jens Lehmann</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-06-17T10:17:26+00:00 · Latest: 2026-01-15T12:24:48+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.14375v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.14375v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Invasive mechanical ventilation (MV) is a life-sustaining therapy commonly used in the intensive care unit (ICU) for patients with severe and acute conditions. These patients frequently rely on MV for breathing. Given the high risk of death in such cases, optimal MV settings can reduce mortality, minimize ventilator-induced lung injury, shorten ICU stays, and ease the strain on healthcare resources. However, optimizing MV settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for optimizing MV settings, current methods struggle with the hybrid (continuous and discrete) nature of MV settings. Discretizing continuous settings leads to exponential growth in the action space, which limits the number of optimizable settings. Converting the predictions back to continuous can cause a distribution shift, compromising safety and performance. To address this challenge, in the IntelliLung project, we are developing an AI-based approach where we constrain the action space and employ factored action critics. This approach allows us to scale to six optimizable settings compared to 2-3 in previous studies. We adapt SOTA offline RL algorithms to operate directly on hybrid action spaces, avoiding the pitfalls of discretization. We also introduce a clinically grounded reward function based on ventilator-free days and physiological targets. Using multiobjective optimization for reward selection, we show that this leads to a more equitable consideration of all clinically relevant objectives. Notably, we develop a system in close collaboration with healthcare professionals that is aligned with real-world clinical objectives and designed with future deployment in mind.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于混合动作与临床对齐奖励的离线强化学习推进安全机械通气</div>
<div class="mono" style="margin-top:8px">有创机械通气是重症监护室中维持危重患者生命的重要疗法。优化通气参数可降低死亡率、减少呼吸机相关肺损伤、缩短ICU住院时间并缓解医疗资源压力，但患者个体差异使该过程复杂且易出错。现有离线强化学习方法难以处理通气参数混合连续-离散的特性：离散化会导致动作空间指数级膨胀，而回退连续化可能引发分布偏移。IntelliLung项目提出一种基于人工智能的解决方案，通过约束动作空间与采用分解动作评价器，将可优化参数扩展至6项（既往研究仅2-3项）。我们改进先进离线强化学习算法以直接处理混合动作空间，并构建基于脱机天数与生理指标的临床奖励函数。通过多目标优化进行奖励选择，确保所有临床目标得到均衡考量。该系统与医疗专业人员深度协作开发，契合真实临床需求且具备未来部署可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing invasive mechanical ventilation (MV) settings in intensive care units, where patient-specific variability makes manual optimization complex and error-prone. The authors propose an offline reinforcement learning approach within the IntelliLung project that directly handles hybrid (continuous and discrete) actions using constrained action spaces and factored action critics, avoiding the pitfalls of discretization and scaling to six optimizable settings. Experimental results demonstrate that their method, combined with a clinically grounded reward function based on ventilator-free days and physiological targets, enables more equitable consideration of clinical objectives and outperforms prior studies limited to 2-3 settings.</div>
<div class="mono" style="margin-top:8px">本文针对重症监护病房中有创机械通气（MV）参数优化的挑战，该过程因患者特异性变异而复杂且易出错。作者在IntelliLung项目中提出一种离线强化学习方法，通过约束动作空间和分解动作评价器直接处理混合（连续和离散）动作，避免了离散化的缺陷，并将可优化参数扩展至六个。实验结果表明，该方法结合基于脱机天数和生理目标的临床奖励函数，能更公平地考虑所有临床目标，性能优于先前仅能优化2-3个参数的研究。</div>
</details>
</div>
<div class="card">
<div class="title">Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning</div>
<div class="meta-line">Authors: Xin Guan, Zijian Li, Shen Huang, Pengjun Xie, Jingren Zhou, Jiuxin Cao</div>
<div class="meta-line">First: 2026-01-15T11:40:57+00:00 · Latest: 2026-01-15T11:40:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10306v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded &quot;lucky guesses,&quot; leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于证据增强策略优化与奖励协同进化的长上下文推理方法</div>
<div class="mono" style="margin-top:8px">尽管强化学习（RL）推动了大型语言模型（LLM）的推理能力，但其在长上下文场景中的应用受限于结果奖励的稀疏性。这一局限无法有效惩罚缺乏依据的&#x27;侥幸猜测&#x27;，导致关键的&#x27;大海捞针&#x27;式证据检索过程缺乏监督。为此，我们提出EAPO（证据增强策略优化）框架。首先建立证据增强推理范式，通过树状结构证据抽样验证精确证据提取是长上下文推理的决定性瓶颈。基于此发现，EAPO设计了一种专用强化学习算法：通过奖励模型计算群体相对证据奖励，提供密集的过程监督以显式提升证据质量。为维持训练全程的精准监督，我们进一步引入自适应奖励-策略协同进化机制，该机制利用结果一致的推演轨迹迭代优化奖励模型，增强其判别能力以确保精确的过程指导。在八个基准测试上的综合评估表明，EAPO较现有最优基线模型显著提升了长上下文推理性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of sparse outcome rewards in Reinforcement Learning (RL) for long-context reasoning with large language models, which fails to supervise the critical evidence retrieval process and may reward ungrounded guesses. The proposed method, Evidence-Augmented Policy Optimization (EAPO), first establishes an Evidence-Augmented Reasoning paradigm, identifying precise evidence extraction as the key bottleneck. It then introduces a specialized RL algorithm featuring a Group-Relative Evidence Reward for dense process supervision and an Adaptive Reward-Policy Co-Evolution mechanism to iteratively refine the reward model. Experimental results across eight benchmarks show that EAPO significantly outperforms state-of-the-art baselines in long-context reasoning performance.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型长上下文推理中强化学习结果奖励稀疏的问题，该问题导致关键的证据检索过程缺乏监督，并可能奖励无根据的猜测。提出的方法——证据增强策略优化（EAPO），首先建立了证据增强推理范式，确定精确证据提取是关键瓶颈。随后引入一种专门的强化学习算法，包含用于密集过程监督的组相对证据奖励，以及自适应奖励-策略协同进化机制以迭代优化奖励模型。在八个基准测试上的综合评估表明，EAPO在长上下文推理性能上显著优于现有最先进的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Safe Reinforcement Learning for Stochastic Reach-Avoid Problems with Entropy Regularization</div>
<div class="meta-line">Authors: Abhijit Mazumdar, Rafal Wisniewski, Manuela L. Bujorianu</div>
<div class="meta-line">First: 2026-01-13T15:23:19+00:00 · Latest: 2026-01-15T11:31:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08646v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08646v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>熵正则化随机可达规避问题的可证明安全强化学习</div>
<div class="mono" style="margin-top:8px">我们研究带安全约束的马尔可夫决策过程最优策略学习问题，采用可达规避框架进行建模。目标是设计在线强化学习算法，确保学习阶段以任意高概率满足安全约束。为此，我们首先提出基于不确定性乐观原则的算法，并在此基础上引入熵正则化构建核心算法。通过对两种算法进行有限样本分析，推导其遗憾界。研究表明，熵正则化的引入能有效改善遗憾界，并显著抑制基于不确定性乐观原则的安全强化学习算法固有的回合间波动性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of ensuring safety during online reinforcement learning in constrained Markov decision processes, specifically formulated as a reach-avoid problem. The motivation is to design algorithms that guarantee safety with high probability throughout the learning process. The method introduces two algorithms: an initial one based on the optimism in the face of uncertainty (OFU) principle, and a main algorithm that incorporates entropy regularization to enhance performance. Experimental results from finite-sample analysis show that the entropy-regularized algorithm achieves improved regret bounds and significantly reduces the high episode-to-episode variability typically associated with OFU-based safe RL approaches.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在线强化学习在约束马尔可夫决策过程中的安全性挑战，具体问题被表述为到达-规避问题。其动机是设计能在整个学习阶段以高概率保证安全性的算法。方法上提出了两种算法：一种是基于不确定性面前保持乐观原则的初始算法，另一种是引入了熵正则化的主要算法以提升性能。有限样本分析的实验结果表明，熵正则化算法改善了遗憾界，并显著降低了基于OFU的安全强化学习算法固有的回合间高波动性。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</div>
<div class="meta-line">Authors: Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, Younggyo Seo</div>
<div class="meta-line">First: 2025-05-29T16:41:12+00:00 · Latest: 2026-01-15T11:24:14+00:00</div>
<div class="meta-line">Comments: 29 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00070v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.00070v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate Robot-R1, we also introduce a new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Robot-R1：基于强化学习的机器人具身推理增强框架</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）近期通过结合具身推理与机器人控制，在推动机器人技术发展方面展现出巨大潜力。现有方法通常采用监督微调（SFT）对机器人控制相关的具身推理任务进行训练，但SFT数据集常依赖启发式构建，未针对机器人控制优化，且易导致灾难性遗忘与泛化性能下降。为此，我们提出Robot-R1——一种利用强化学习专门增强机器人控制具身推理能力的新型框架。该框架基于专家示范的当前场景图像与环境元数据，学习预测任务完成所需的下一个关键点状态。受DeepSeek-R1学习方法启发，Robot-R1对基于推理的响应进行采样，并强化那些能产生更准确预测的响应。为系统评估Robot-R1，我们同时构建了需要多样化具身推理能力的新基准测试。实验表明，采用Robot-R1训练的模型在具身推理任务上优于SFT方法。尽管仅拥有70亿参数，Robot-R1在空间与运动推理等底层动作控制相关任务上甚至超越了GPT-4o。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Robot-R1, a framework motivated by the limitations of Supervised Fine-Tuning (SFT) for training Large Vision-Language Models in robotics, such as heuristic dataset construction, catastrophic forgetting, and poor generalization. The method employs reinforcement learning to enhance embodied reasoning for robot control by learning to predict the next keypoint state from current scene images and environment metadata, sampling and reinforcing reasoning-based responses that lead to accurate predictions. Experimental results on a new benchmark show that Robot-R1 outperforms SFT methods in embodied reasoning tasks and, despite having only 7B parameters, surpasses GPT-4o in low-level action control tasks like spatial and movement reasoning.</div>
<div class="mono" style="margin-top:8px">本文提出了Robot-R1框架，其动机在于解决大型视觉语言模型在机器人领域中使用监督微调方法时的局限性，例如启发式数据集构建、灾难性遗忘和泛化性能差。该方法采用强化学习来增强机器人控制中的具身推理能力，通过学习从当前场景图像和环境元数据中预测下一个关键点状态，并对导致准确预测的推理响应进行采样和强化。在新基准上的实验结果表明，Robot-R1在具身推理任务上优于监督微调方法，并且尽管仅有70亿参数，其在空间和运动推理等低层动作控制任务上甚至超越了GPT-4o。</div>
</details>
</div>
<div class="card">
<div class="title">TranslateGemma Technical Report</div>
<div class="meta-line">Authors: Mara Finkelstein, Isaac Caswell, Tobias Domhan, Jan-Thorsten Peter, Juraj Juraska, Parker Riley, Daniel Deutsch, Cole Dilanni, Colin Cherry, Eleftheria Briakou, Elizabeth Nielsen, Jiaming Luo, Kat Black, Ryan Mullins, Sweta Agrawal, Wenda Xu, Erin Kats, Stephane Jaskiewicz, Markus Freitag, David Vilar</div>
<div class="meta-line">First: 2026-01-13T22:23:24+00:00 · Latest: 2026-01-15T10:43:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09012v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09012v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TranslateGemma技术报告</div>
<div class="mono" style="margin-top:8px">本文介绍了TranslateGemma——一套基于Gemma 3基础模型的开源机器翻译模型。为增强Gemma 3在翻译任务中的多语言能力，我们采用两阶段微调策略：首先使用通过前沿模型生成的大规模高质量合成平行数据与人工翻译平行数据混合进行监督微调；随后通过强化学习阶段，采用包含MetricX-QE和AutoMQM的奖励模型集成优化翻译质量。我们在WMT25测试集的10个语言对上进行了人工评估，并在WMT24++基准的55个语言对上进行了自动评估，验证了TranslateGemma的有效性。自动指标显示所有规模的模型均较基线Gemma 3模型取得持续显著提升。值得注意的是，较小规模的TranslateGemma模型常能达到与较大基线模型相当的性能，同时提升效率。实验还表明TranslateGemma模型保留了强大的多模态能力，在Vistra图像翻译基准上表现提升。开源TranslateGemma模型的发布旨在为研究社区提供强大且适应性强的机器翻译工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated to enhance the inherent multilingual capabilities of the Gemma 3 foundation models for machine translation, this work introduces TranslateGemma, a suite of open models. The method employs a two-stage fine-tuning process: first, supervised fine-tuning on a mixture of high-quality synthetic and human-translated parallel data, followed by reinforcement learning optimized with an ensemble of reward models like MetricX-QE and AutoMQM. Experimental results from human evaluation on the WMT25 test set (10 language pairs) and automatic evaluation on WMT24++ (55 pairs) show consistent and substantial gains over baseline Gemma 3 models, with smaller TranslateGemma models often matching the performance of larger baselines for improved efficiency, while also retaining and enhancing multimodal capabilities on benchmarks like Vistra.</div>
<div class="mono" style="margin-top:8px">本研究旨在增强Gemma 3基础模型固有的多语言能力以用于机器翻译任务，由此提出了TranslateGemma这一套开源模型。其方法采用两阶段微调流程：首先使用高质量的大规模合成并行数据与人工翻译并行数据进行混合监督微调，随后通过集成MetricX-QE和AutoMQM等奖励模型进行强化学习以优化翻译质量。在WMT25测试集（10个语言对）上的人工评估和在WMT24++基准（55个语言对）上的自动评估结果表明，相比基线Gemma 3模型，TranslateGemma在所有模型尺寸上均取得了一致且显著的性能提升，较小的模型常能达到与较大基线模型相当的性能，从而提高了效率；同时，模型在Vistra图像翻译基准上展现出保留并增强的多模态能力。</div>
</details>
</div>
<div class="card">
<div class="title">An Ensemble of Evolutionary Algorithms With Both Crisscross Search and Sparrow Search for Processing Inferior Individuals</div>
<div class="meta-line">Authors: Mingxuan Du, Tingzhang Luo, Ziyang Wang, Chengjun Li</div>
<div class="meta-line">First: 2026-01-15T10:36:08+00:00 · Latest: 2026-01-15T10:36:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10263v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10263v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the field of artificial intelligence, real parameter single objective optimization is an important direction. Both the Differential Evolution (DE) and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) demonstrate good performance for real parameter single objective optimization. Nevertheless, there exist other types of evolutionary algorithm for the purpose. In recent years, researchers begin to study long-term search. EA4eig - an ensemble of three DE variants and CMA-ES - performs well for long-term search. In this paper, we introduce two types of evolutionary algorithm proposed recently - crisscross search and sparrow search - into EA4eig as secondary evolutionary algorithms to process inferior individuals. Thus, EA4eigCS is obtained. In our ensemble, the secondary evolutionary algorithms are expected to vary distribution of the population for breaking stagnation. Experimental results show that our EA4eigCS outperforms EA4eig and is competitive when compared with state-of-the-art algorithms. Code and supplementary material are available at:https://anonymous.4open.science/r/EA4eigCS-2A43.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>融合交叉搜索与麻雀搜索的进化算法集成框架及其在劣质个体处理中的应用</div>
<div class="mono" style="margin-top:8px">在人工智能领域，实参数单目标优化是一个重要方向。差分进化算法与协方差矩阵自适应进化策略在该领域均表现出良好性能，但还存在其他类型的进化算法。近年来，研究者开始关注长期搜索问题。EA4eig——集成三种差分进化变体与协方差矩阵自适应进化策略的算法——在长期搜索中表现优异。本文引入近期提出的交叉搜索与麻雀搜索两种进化算法作为EA4eig的辅助进化算法，专门处理劣质个体，从而构建出EA4eigCS算法。在该集成框架中，辅助进化算法通过改变种群分布以突破停滞状态。实验结果表明，EA4eigCS性能优于EA4eig，并与前沿算法具有竞争力。代码及补充材料可通过以下链接获取：https://anonymous.4open.science/r/EA4eigCS-2A43。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance long-term search performance in real parameter single objective optimization, this paper introduces crisscross search and sparrow search as secondary evolutionary algorithms into the existing EA4eig ensemble, which combines three Differential Evolution variants and CMA-ES, to specifically process inferior individuals and vary population distribution to break stagnation. The method, termed EA4eigCS, integrates these algorithms to diversify the search process. Experimental results demonstrate that EA4eigCS outperforms the original EA4eig and is competitive with state-of-the-art algorithms, validating its effectiveness in improving optimization outcomes.</div>
<div class="mono" style="margin-top:8px">本文旨在提升实数参数单目标优化中的长期搜索性能，通过将最近提出的交叉搜索和麻雀搜索作为辅助进化算法，引入到结合了三种差分进化变体和CMA-ES的EA4eig集成中，以专门处理劣质个体并改变种群分布来打破停滞。该方法称为EA4eigCS，通过整合这些算法来多样化搜索过程。实验结果表明，EA4eigCS优于原始EA4eig，并与先进算法具有竞争力，验证了其在改善优化结果方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">PRL: Process Reward Learning Improves LLMs&#x27; Reasoning Ability and Broadens the Reasoning Boundary</div>
<div class="meta-line">Authors: Jiarui Yao, Ruida Wang, Tong Zhang</div>
<div class="meta-line">First: 2026-01-15T09:01:53+00:00 · Latest: 2026-01-15T09:01:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10201v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs&#x27; reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRL：过程奖励学习提升大语言模型推理能力并拓展推理边界</div>
<div class="mono" style="margin-top:8px">提升大语言模型的推理能力是近期持续关注的研究方向。现有工作多基于轨迹层面的结果奖励，缺乏对推理过程的细粒度监督；其他尝试融合过程信号的训练框架则严重依赖蒙特卡洛树搜索、独立奖励模型训练等繁琐步骤，损害训练效率，且过程信号设计缺乏严谨理论支撑，导致优化机制不透明。本文提出过程奖励学习，将熵正则化强化学习目标分解至中间步骤，并分配严格推导的过程奖励。从理论动机出发，推导出与“奖励最大化+策略模型与参考模型间KL散度惩罚项”目标本质等效的PRL公式。PRL能将结果奖励转化为过程监督信号，更好指导强化学习优化中的探索。实验表明，PRL不仅能通过平均@n指标提升模型推理性能，还能通过改进通过率@n指标拓展推理边界。大量实验验证了PRL的有效性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing methods that rely on coarse outcome rewards or inefficient process supervision, this paper introduces Process Reward Learning (PRL) to enhance LLMs&#x27; reasoning by providing fine-grained, theoretically grounded process rewards. The method decomposes an entropy-regularized reinforcement learning objective into intermediate steps, deriving a formulation equivalent to reward maximization with a KL-divergence penalty, which transforms outcome rewards into process signals to guide exploration more effectively. Experimental results demonstrate that PRL improves average reasoning performance and broadens the reasoning boundary, as evidenced by enhancements in both average @ n and pass @ n metrics across extensive evaluations.</div>
<div class="mono" style="margin-top:8px">针对现有方法依赖粗粒度结果奖励或低效过程监督的局限，本文提出过程奖励学习（PRL），通过提供细粒度且理论支撑的过程奖励来增强大语言模型的推理能力。该方法将熵正则化的强化学习目标分解为中间步骤，推导出等价于奖励最大化加上KL散度惩罚项的公式，从而将结果奖励转化为过程信号以更有效地指导探索。实验结果表明，PRL不仅提高了平均推理性能，还拓宽了推理边界，这通过平均@n和通过@n指标的提升在广泛实验中得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning</div>
<div class="meta-line">Authors: Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin</div>
<div class="meta-line">First: 2026-01-15T08:45:54+00:00 · Latest: 2026-01-15T08:45:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10187v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10187v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively &quot;tames&quot; the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HOMURA：通过强化学习驯服沙漏模型实现时间受限的大语言模型翻译</div>
<div class="mono" style="margin-top:8px">大语言模型在多语言翻译领域取得显著进展，但其系统性的跨语言冗长偏差使其难以适用于字幕配音等严格时间受限的任务。现有提示工程方法难以平衡语义保真度与刚性时间可行性之间的矛盾。为填补这一空白，我们首先提出沙漏基准——专门用于评估音节级时长约束下的翻译性能。进一步，我们提出HOMURA强化学习框架，显式优化语义保持与时间合规性的权衡。通过采用KL正则化目标及创新的动态音节比例奖励机制，HOMURA有效实现了输出长度的精准调控。实验结果表明，该方法显著优于主流大语言模型基线，在保持语义充分性的同时，实现了尊重语言密度层级的精确长度控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of cross-lingual verbosity bias in Large Language Models (LLMs) for translation, which makes them unsuitable for time-constrained applications like subtitling where output length must strictly match source duration. To tackle this, the authors first introduce Sand-Glass, a benchmark for evaluating translation under syllable-level duration constraints, and then propose HOMURA, a reinforcement learning framework that optimizes the trade-off between semantic fidelity and temporal compliance using a KL-regularized objective with a dynamic syllable-ratio reward. Experimental results show that HOMURA significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density without compromising semantic quality.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在翻译中存在的跨语言冗长偏差问题展开研究，该偏差使其不适用于字幕翻译等有严格时间约束的任务，因为输出长度必须与源语言时长精确匹配。为解决此问题，作者首先提出了Sand-Glass基准，用于评估音节级时长约束下的翻译性能，进而提出了HOMURA强化学习框架，该框架通过使用带有动态音节比奖励的KL正则化目标，显式优化语义保真度与时间合规性之间的平衡。实验结果表明，该方法显著优于强大的LLM基线，能够在保持语义充分性的同时，实现尊重语言密度层级的精确长度控制。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand</div>
<div class="meta-line">Authors: Kiattikun Chobtham</div>
<div class="meta-line">First: 2026-01-15T08:40:01+00:00 · Latest: 2026-01-15T08:40:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10181v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习发现东北季风指数以预测泰国月降雨量</div>
<div class="mono" style="margin-top:8px">气候预测因地球系统复杂的时空模式而面临挑战。全球气候指数（如厄尔尼诺-南方涛动）是长期降雨预测的标准输入特征。然而，在泰国特定区域能提升预测精度的局地尺度指数仍存在显著空白。本文提出一种基于海表温度计算的新型东北季风气候指数，以反映北半球冬季季风的气候特征。为优化该指数计算区域，深度Q网络强化学习智能体通过探索并选择与季节性降雨相关性最高的矩形区域来实现。降雨站点被划分为12个独立聚类，以区分泰国南部与北部地区的降雨模式。实验结果表明，将优化后的指数融入长短期记忆模型，能显著提升多数聚类区域的长期月降雨预测能力，有效降低12个月前瞻预测的均方根误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of improving long-term monthly rainfall prediction in Thailand by developing a localized climate index, as existing global indices like ENSO are insufficient for regional accuracy. The method introduces a novel NorthEast monsoon index derived from sea surface temperature, optimized using a Deep Q-Network reinforcement learning agent to select the most correlated rectangular sea areas, and integrates this index into Long Short-Term Memory models after clustering rainfall stations into 12 groups to capture regional patterns. Experimental results demonstrate that the optimized index significantly enhances prediction skill across most clusters, effectively reducing the Root Mean Square Error for 12-month-ahead forecasts.</div>
<div class="mono" style="margin-top:8px">本文针对泰国长期月降雨量预测的挑战，提出一种局部气候指数以提升区域预测精度，因为如ENSO等全球指数在本地应用中存在不足。方法通过海表温度计算新型东北季风指数，并利用深度Q网络强化学习智能体优化选择相关性最高的矩形海域区域，结合长短期记忆模型对12个降雨站聚类以区分区域模式。实验结果表明，优化后的指数在大多数聚类区域显著提高了预测能力，有效降低了12个月前瞻预测的均方根误差。</div>
</details>
</div>
<div class="card">
<div class="title">OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning</div>
<div class="meta-line">Authors: Zixun Huang, Jiayi Sheng, Zeyu Zheng</div>
<div class="meta-line">First: 2025-11-28T16:09:28+00:00 · Latest: 2026-01-15T08:06:30+00:00</div>
<div class="meta-line">Comments: 19 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.23310v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.23310v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OBLR-PO：一种稳定的强化学习理论框架</div>
<div class="mono" style="margin-top:8px">现有基于强化学习的大语言模型后训练方法发展迅速，但其设计主要依赖启发式经验而非系统化理论原则。这一局限阻碍了我们对梯度估计器特性及相关优化算法的理解，从而制约了提升训练稳定性与整体性能的潜力。本研究提出一个统一的理论框架，在温和假设下刻画了常用策略梯度估计器的统计特性。通过分析建立无偏性、推导精确方差表达式，并给出优化损失上界，为学习动态提供了理论依据。基于这些结果，我们证明了收敛保证，并推导出由梯度信噪比调控的自适应学习率调度机制。进一步证明方差最优基线是一种梯度加权估计器，为方差缩减提供了新原理，其稳定性提升效果超越现有方法。这些洞见催生了最优基线与学习率策略优化算法，该算法以理论为基础联合调节学习率与基线。在Qwen3-4B-Base和Qwen3-8B-Base上的实验表明，相较于现有策略优化方法取得持续增益，验证了理论贡献在大规模后训练中转化为实际性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the heuristic-driven design of existing reinforcement learning (RL) methods for large language model post-training, which lacks theoretical grounding and limits stability and performance, this work introduces a unified theoretical framework to analyze policy-gradient estimators. The method establishes unbiasedness, derives exact variance expressions, and provides an optimization-loss upper bound, leading to convergence guarantees and an adaptive learning-rate schedule based on gradient signal-to-noise ratio (SNR), along with a variance-optimal baseline as a gradient-weighted estimator for enhanced stability. Experimental results on Qwen3-4B-Base and Qwen3-8B-Base models show consistent performance gains over existing policy optimization methods, validating the practical benefits of the theoretically derived OBLR-PO algorithm.</div>
<div class="mono" style="margin-top:8px">现有基于强化学习的大语言模型后训练方法多依赖启发式设计，缺乏系统理论指导，限制了训练稳定性和性能提升，为此本研究提出了一个统一的理论框架来分析策略梯度估计器。该方法在温和假设下证明了估计器的无偏性，推导了精确方差表达式和优化损失上界，从而获得收敛性保证，并基于梯度信噪比设计了自适应学习率调度，同时将方差最优基线构建为梯度加权估计器以提升稳定性。在Qwen3-4B-Base和Qwen3-8B-Base模型上的实验表明，所提出的OBLR-PO算法相比现有策略优化方法取得了稳定性能提升，验证了理论成果的实际有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DecisionLLM: Large Language Models for Long Sequence Decision Exploration</div>
<div class="meta-line">Authors: Xiaowei Lv, Zhilin Zhang, Yijun Li, Yusen Huo, Siyuan Ju, Xuyan Li, Chunxiang Hong, Tianyu Wang, Yongcai Wang, Peng Sun, Chuan Yu, Jian Xu, Bo Zheng</div>
<div class="meta-line">First: 2026-01-15T07:42:02+00:00 · Latest: 2026-01-15T07:42:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10148v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10148v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs&#x27; inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DecisionLLM：面向长序列决策探索的大语言模型</div>
<div class="mono" style="margin-top:8px">长序列决策通常通过强化学习（RL）解决，是动态环境中优化战略操作（如计算广告中的实时竞价）的关键组成部分。决策变换器（DT）通过将RL构建为自回归序列建模问题，引入了一种强大的范式。与此同时，大语言模型（LLMs）在复杂推理与规划任务中展现出显著成功。这启发我们思考：共享相同Transformer架构但规模更大的LLMs，能否在长周期序列决策问题中实现性能的新突破。本研究探索LLMs在离线决策任务中的应用。该领域的一个根本挑战在于LLMs本质上无法直接理解连续数值，因为当数值以文本字符串表示时，模型缺乏对数值大小与顺序的固有认知。为此，我们提出将轨迹数据视为独立模态。通过学习将轨迹数据与自然语言任务描述对齐，我们的模型能够在我们称为DecisionLLM的统一框架内自回归预测未来决策。我们建立了该范式的扩展规律，证明性能取决于三个因素：模型规模、数据量与数据质量。在离线实验基准与竞价场景中，DecisionLLM均表现出色。具体而言，DecisionLLM-3B在Maze2D umaze-v1上比传统决策变换器（DT）提升69.4分，在AuctionNet上提升0.085分。该工作拓展了AIGB范式，并为在线竞价等领域的未来探索指明了方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the potential of Large Language Models (LLMs), which excel at reasoning and planning, to advance long-sequence decision-making—a domain traditionally tackled by reinforcement learning. The method addresses LLMs&#x27; inability to interpret continuous numerical values by treating decision trajectories as a distinct modality and learning to align them with natural language task descriptions within a unified autoregressive framework called DecisionLLM. The main experimental results show that DecisionLLM achieves strong performance in offline benchmarks, with the 3B-parameter model outperforming the traditional Decision Transformer by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet, establishing scaling laws that link performance to model scale, data volume, and quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大型语言模型在推理和规划任务上的卓越表现，有望推动传统由强化学习处理的长序列决策领域的发展。其方法通过将决策轨迹视为一种独特模态，并学习将其与自然语言任务描述对齐，构建了一个名为DecisionLLM的自回归统一框架，以解决大语言模型无法理解连续数值的问题。主要实验结果表明，DecisionLLM在离线基准测试中表现强劲，其30亿参数模型在Maze2D umaze-v1上比传统决策变换器性能高出69.4，在AuctionNet上高出0.085，同时确立了性能与模型规模、数据量和数据质量相关的缩放定律。</div>
</details>
</div>
<div class="card">
<div class="title">History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis</div>
<div class="meta-line">Authors: Haochong Xia, Yao Long Teng, Regan Tan, Molei Qin, Xinrun Wang, Bo An</div>
<div class="meta-line">First: 2026-01-15T07:38:59+00:00 · Latest: 2026-01-15T07:38:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10143v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra &quot;History Is Not Enough&quot; underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>历史不足为凭：面向金融时间序列合成的自适应数据流系统</div>
<div class="mono" style="margin-top:8px">在量化金融中，由概念漂移和分布非平稳性驱动的训练与真实世界性能之间的差距，仍是构建可靠数据驱动系统的关键障碍。基于静态历史数据训练的模型常出现过拟合，导致在动态市场中泛化能力差。“历史不足为凭”的理念强调需要能够随市场演进的自适应数据生成，而非仅依赖过往观测。我们提出一种漂移感知数据流系统，将基于机器学习的自适应控制集成至数据管理流程中。该系统将参数化数据操作模块（包含单股变换、多股混合及管理操作）与采用基于梯度的双层优化进行系统控制的自适应规划调度器相结合。该设计在统一的可微分框架下整合了数据增强、课程学习及数据工作流管理，支持溯源感知的重放与持续数据质量监控。在预测和强化学习交易任务上的大量实验表明，该框架提升了模型鲁棒性并改善了风险调整后收益。该系统为金融数据提供了一种可推广的自适应数据管理与学习引导的工作流自动化方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance gap in quantitative finance caused by concept drift and non-stationarity, which leads models trained on static historical data to overfit and generalize poorly in dynamic markets, this paper introduces a drift-aware dataflow system for adaptive financial time-series synthesis. The method integrates machine learning-based adaptive control into data curation, coupling a parameterized data manipulation module with an adaptive planner-scheduler that uses gradient-based bi-level optimization to unify data augmentation, curriculum learning, and workflow management in a differentiable framework. Experimental results on forecasting and reinforcement learning trading tasks show that the framework enhances model robustness and improves risk-adjusted returns, offering a generalizable approach to adaptive data management.</div>
<div class="mono" style="margin-top:8px">本文的动机是量化金融中因概念漂移和非平稳性导致的性能差距，这使得基于静态历史数据训练的模型在动态市场中容易过拟合且泛化能力差。为此，论文提出了一种用于自适应金融时间序列合成的漂移感知数据流系统，其方法将基于机器学习的自适应控制集成到数据管理流程中，通过参数化的数据操作模块与采用梯度双层优化的自适应规划调度器相结合，在可微分框架下统一了数据增强、课程学习和工作流管理。在预测和强化学习交易任务上的大量实验表明，该框架增强了模型的鲁棒性并提高了风险调整后收益，为自适应数据管理提供了一种可推广的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Functional Critics Are Essential in Off-Policy Actor-Critic: Provable Convergence and Efficient Exploration</div>
<div class="meta-line">Authors: Qinxun Bai, Yuxuan Han, Wei Xu, Zhengyuan Zhou</div>
<div class="meta-line">First: 2025-09-26T21:55:26+00:00 · Latest: 2026-01-15T06:25:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22964v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.22964v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-policy reinforcement learning (RL) with function approximation offers an effective way to improve sample efficiency by reusing past experience. Within this setting, the actor-critic (AC) framework has achieved strong empirical success but suffers from the &quot;moving target&quot; problem, where the policy being evaluated changes continually. Functional critics, or policy-conditioned value functions, have been proposed to address this issue by including a representation of the policy as input. While the concept of generalizing value functions across policy space is appealing, previous efforts have struggled to remain competitive against state-of-the-art AC algorithms that do not utilize functional critics. In this work, we revisit functional critics within the off-policy AC framework and identify two aspects that render them a necessity rather than a luxury. First, in off-policy AC, critic learning contends with both the &quot;deadly triad&quot; instability and the &quot;moving target&quot; issue, while actor learning faces the challenge of estimating the exact off-policy policy gradient. This complex interplay makes theoretical convergence extremely difficult for practical algorithms. We demonstrate that a functional critic is essential for addressing this challenge and establish the first convergence proof for an off-policy target-based AC algorithm under linear function approximation. Second, we identify a crucial link between functional critic modeling and efficient exploration. Specifically, we show that approximating posterior sampling for exploration in model-free settings is infeasible without functional critics. Practically, we propose a tailored neural network architecture and a minimal AC algorithm that relies solely on these insights. In experiments on the DeepMind Control Suite, this implementation achieves performance competitive with state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离策略演员-评论家算法中函数型评论家的必要性：可证明的收敛性与高效探索</div>
<div class="mono" style="margin-top:8px">基于函数逼近的离策略强化学习通过复用历史经验有效提升了样本效率。在此框架下，演员-评论家方法虽取得显著实证成果，却受困于策略持续变化导致的“移动目标”问题。函数型评论家（或称策略条件价值函数）通过将策略表征作为输入来应对此问题。尽管跨策略空间泛化价值函数的理念颇具吸引力，先前研究始终难以匹敌未采用函数型评论家的先进演员-评论家算法。本研究重新审视离策略演员-评论家框架中的函数型评论家，揭示其不可或缺的双重价值：首先，离策略场景中评论家学习需同时应对“致命三角”不稳定性和“移动目标”问题，而演员学习则面临精确估计离策略梯度的挑战。这种复杂交互使得实际算法的理论收敛性极难保证。我们证明函数型评论家是解决该问题的关键，并首次在线性函数逼近下建立了离策略目标型演员-评论家算法的收敛性证明。其次，我们发现函数型评论家建模与高效探索之间存在本质关联：在无模型设定中，若缺乏函数型评论家则无法实现后验采样探索的近似。基于此，我们设计了专用神经网络架构与精简的演员-评论家算法。在DeepMind控制套件的实验中，该实现达到了与前沿方法相当的效能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper revisits functional critics, or policy-conditioned value functions, in off-policy actor-critic reinforcement learning, motivated by the need to address the combined instability from the &quot;deadly triad&quot; and the &quot;moving target&quot; problem inherent in this setting. The method involves theoretically demonstrating the necessity of functional critics for convergence and efficient exploration, and practically proposing a tailored neural architecture and a minimal algorithm based on this insight. The main experimental results show that the proposed implementation achieves performance competitive with state-of-the-art methods on the DeepMind Control Suite, validating its practical efficacy.</div>
<div class="mono" style="margin-top:8px">本文重新审视了离线策略演员-评论家强化学习中的功能评论家（即策略条件价值函数），其动机在于需要解决该设置中固有的“致命三元组”不稳定性和“移动目标”问题。方法包括从理论上证明功能评论家对于算法收敛和高效探索的必要性，并基于此见解实际提出了一种定制的神经网络架构和最小化算法。主要实验结果表明，所提出的实现在DeepMind控制套件上取得了与最先进方法相竞争的性能，验证了其实用有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts</div>
<div class="meta-line">Authors: Sijia Luo, Xiaokang Zhang, Yuxuan Hu, Bohan Zhang, Ke Wang, Jinbo Su, Mengshu Sun, Lei Liang, Jing Zhang</div>
<div class="meta-line">First: 2026-01-15T05:12:03+00:00 · Latest: 2026-01-15T05:12:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10079v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10079v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏强化学习：通过稳定稀疏推演突破大语言模型强化学习中的内存墙</div>
<div class="mono" style="margin-top:8px">强化学习已成为激发大语言模型复杂推理能力的关键技术。然而，长序列推演过程中存储键值缓存带来的巨大内存开销构成关键瓶颈，常导致有限硬件上无法高效训练。现有键值压缩技术虽能缓解推理压力，但直接应用于强化学习训练会引发严重的策略失配，导致性能崩溃。为此，我们提出稀疏强化学习方法，实现稀疏推演下的稳定训练。研究表明，不稳定性源于稠密旧策略、稀疏采样策略与学习策略之间的根本性失配。为缓解此问题，稀疏强化学习引入稀疏感知拒绝采样与基于重要性的重加权机制，以修正压缩导致信息损失引发的离策略偏差。实验表明，稀疏强化学习在保持性能的同时，显著降低了推演开销。此外，该方法本质实现了稀疏感知训练，大幅提升了模型在稀疏推理部署中的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the memory bottleneck caused by storing Key-Value caches during long-horizon reinforcement learning for large language models, which hinders training on limited hardware. The proposed method, Sparse-RL, addresses the policy mismatch and instability from directly applying existing KV compression techniques to RL by introducing Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct off-policy bias. Experimental results demonstrate that Sparse-RL effectively reduces rollout memory overhead compared to dense baselines while maintaining performance and enhancing model robustness for sparse inference deployment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型强化学习中因存储键值缓存导致的内存瓶颈问题，该问题限制了在有限硬件上的高效训练。所提出的Sparse-RL方法通过引入稀疏感知拒绝采样和基于重要性的重加权，纠正了因压缩导致信息损失而产生的离策略偏差，从而解决了直接应用现有KV压缩技术到强化学习时引发的策略失配和不稳定性。实验结果表明，与密集基线相比，Sparse-RL在保持性能的同时有效降低了训练过程中的内存开销，并增强了模型在稀疏推理部署中的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Learning through Ranking Mean Squared Error</div>
<div class="meta-line">Authors: Chaitanya Kharyal, Calarina Muslimani, Matthew E. Taylor</div>
<div class="meta-line">First: 2026-01-14T07:18:12+00:00 · Latest: 2026-01-15T04:48:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09236v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09236v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., &quot;bad,&quot; &quot;neutral,&quot; &quot;good&quot;). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher&#x27;s ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于排序均方误差的奖励学习</div>
<div class="mono" style="margin-top:8px">奖励设计仍是强化学习应用于现实问题的关键瓶颈。奖励学习作为一种主流替代方案，通过人类反馈而非人工设定来推导奖励函数。近期研究提出从评分形式的人类反馈中学习奖励函数，取代传统的二元偏好，从而提供更丰富且认知负担更低的监督。基于此范式，我们提出一种新的基于评分的强化学习方法——强化学习排序回报回归（R4）。其核心采用新颖的排序均方误差损失函数，将教师提供的评分视为序数目标。该方法通过轨迹-评分配对数据集进行学习，每条轨迹均标注离散评分（如“差”“中”“优”）。在每个训练步骤中，我们采样一组轨迹，预测其回报，并通过可微分排序算子（软排序）进行排名，随后优化软排序结果与教师评分之间的均方误差损失。与现有基于评分的方法不同，R4具备形式化保证：在温和假设下，其解集可证明为最小且完备的。通过模拟人类反馈的实验验证，在OpenAI Gym与DeepMind Control Suite的机器人运动基准测试中，R4在显著减少反馈需求的同时，持续匹配或超越现有基于评分及偏好的强化学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the bottleneck of reward design in reinforcement learning by learning reward functions from human feedback in the form of ratings, which provide richer supervision than binary preferences. The method introduces Ranked Return Regression for RL (R4), which uses a novel ranking mean squared error loss that treats ratings as ordinal targets and employs differentiable soft ranks to align predicted returns with teacher ratings. Experimental results on robotic locomotion benchmarks show that R4 consistently matches or outperforms existing rating and preference-based methods while requiring significantly less feedback, with formal guarantees of minimal and complete solution sets under mild assumptions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过从人类评分形式的反馈中学习奖励函数，以解决强化学习中奖励设计的瓶颈问题，评分比二元偏好提供更丰富的监督。方法引入了排名均方误差损失，将评分视为序数目标，并采用可微分软排名使预测回报与教师评分对齐，提出了R4算法。在机器人运动基准测试上的实验结果表明，R4在显著减少反馈需求的同时，持续匹配或优于现有的评分和偏好方法，并在温和假设下具有形式化保证，确保解集的最小性和完备性。</div>
</details>
</div>
<div class="card">
<div class="title">PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization</div>
<div class="meta-line">Authors: Tingyue Pan, Jie Ouyang, Mingyue Cheng, Qingchuan Li, Zirui Liu, Mingfan Pan, Shuo Yu, Qi Liu</div>
<div class="meta-line">First: 2026-01-15T03:21:21+00:00 · Latest: 2026-01-15T03:21:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10029v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PaperScout：一种采用过程感知序列级策略优化的学术论文搜索自主智能体</div>
<div class="mono" style="margin-top:8px">学术论文搜索是科学研究中的基础任务，但现有方法大多依赖僵化的预定义工作流，难以处理复杂的条件式查询。为突破此局限，我们提出PaperScout——将论文搜索重构为序列决策过程的自主智能体。与静态工作流不同，PaperScout能基于累积检索上下文动态决策是否、何时及如何调用搜索与扩展工具。然而训练此类智能体面临根本性挑战：标准强化学习方法通常针对单轮任务设计，应用于多轮智能体任务时存在粒度失配问题——词元级优化与序列级交互的粒度差异导致信用分配噪声。我们提出近端序列策略优化（PSPO），这是一种与智能体-环境交互对齐的过程感知序列级策略优化方法。在合成与真实基准上的综合实验表明，PaperScout在召回率与相关性上均显著优于强工作流驱动及强化学习基线，验证了自适应智能体框架与优化策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of rigid, predefined workflows in academic paper search, which struggle with complex queries, by developing an autonomous agent that treats search as a sequential decision-making process. The method introduces PaperScout, an agent that dynamically invokes tools based on retrieval context, and Proximal Sequence Policy Optimization (PSPO), a novel sequence-level policy optimization technique designed to address the granularity mismatch in training such multi-turn agents. Experimental results on synthetic and real-world benchmarks show that PaperScout significantly outperforms workflow-driven and standard reinforcement learning baselines in both recall and relevance, validating the effectiveness of the adaptive framework and optimization strategy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是克服学术论文搜索中僵化的预定义工作流在处理复杂查询时的局限性，通过将搜索重构为序列决策过程，开发了一个自主智能体。方法上提出了PaperScout智能体，它能根据累积的检索上下文动态调用工具，并引入了近端序列策略优化（PSPO），这是一种过程感知的序列级策略优化方法，旨在解决训练多轮智能体时的粒度不匹配问题。在合成和真实世界基准上的综合实验表明，PaperScout在召回率和相关性上均显著优于基于工作流和标准强化学习的基线，验证了其自适应智能体框架和优化策略的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Gradient Coupling: The Hidden Barrier to Generalization in Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu</div>
<div class="meta-line">First: 2025-09-28T13:24:38+00:00 · Latest: 2026-01-15T01:18:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23870v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.23870v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a dominant paradigm for training autonomous agents, yet these agents often exhibit poor generalization, failing to adapt to scenarios not seen during training. In this work, we identify a fundamental cause of this brittleness, a phenomenon which we term &quot;gradient coupling.&quot; We hypothesize that in complex agentic tasks, the high similarity between distinct states leads to destructive interference between gradients. Specifically, a gradient update that reinforces an optimal action in one state can inadvertently increase the likelihood of a suboptimal action in a similar, yet different, state. To solve this, we propose a novel objective where the actor is trained to simultaneously function as a classifier that separates good and bad actions. This auxiliary pressure compels the model to learn disentangled embeddings for positive and negative actions, which mitigates negative gradient interference and improve the generalization performance. Extensive experiments demonstrate the effectiveness of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>梯度耦合：智能体强化学习中泛化能力的隐性障碍</div>
<div class="mono" style="margin-top:8px">强化学习是训练自主智能体的主流范式，但这些智能体常表现出较差的泛化能力，难以适应训练中未见的场景。本研究揭示了导致这种脆弱性的根本原因——我们称之为“梯度耦合”的现象。我们假设在复杂的智能体任务中，不同状态间的高度相似性会导致梯度间的破坏性干扰。具体而言，强化某一状态最优动作的梯度更新，可能无意中增加相似但不同状态下次优动作的选择概率。为解决此问题，我们提出一种新颖的目标函数：训练执行器同时作为区分优劣动作的分类器。这种辅助压力迫使模型学习正负动作的解耦嵌入表示，从而减轻负面梯度干扰并提升泛化性能。大量实验验证了本方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the poor generalization of reinforcement learning agents, attributing it to a phenomenon called gradient coupling where similar states cause destructive interference in gradient updates, reinforcing optimal actions in one state while inadvertently promoting suboptimal ones in others. To address this, the authors propose training the actor as a classifier to separate good and bad actions, which encourages disentangled embeddings and reduces negative gradient interference. Experimental results show that this method effectively improves generalization performance in agentic tasks.</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习智能体泛化能力差的问题，将其归因于一种称为梯度耦合的现象，即相似状态导致梯度更新中的破坏性干扰，在强化一个状态中最佳动作的同时，无意中增加了其他相似状态中次优动作的可能性。为解决此问题，作者提出将执行器训练为分类器以区分好坏动作，这促使模型学习正负动作的解耦嵌入，从而减轻负面梯度干扰。大量实验证明该方法能有效提升智能体任务的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model</div>
<div class="meta-line">Authors: Enoch H. Kang, Hema Yoganarasimhan, Lalit Jain</div>
<div class="meta-line">First: 2025-02-19T22:22:20+00:00 · Latest: 2026-01-14T22:45:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.14131v6">Abs</a> · <a href="https://arxiv.org/pdf/2502.14131v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于经验风险最小化的离线逆强化学习与动态离散选择模型研究</div>
<div class="mono" style="margin-top:8px">本文研究动态离散选择模型的估计问题，该问题在机器学习中亦称为离线最大熵正则化逆强化学习。目标是从离线行为数据中恢复支配智能体行为的奖励函数或$Q^*$函数。我们提出一种全局收敛的基于梯度的方法，无需依赖线性参数化奖励的严格假设。方法的核心创新在于引入基于经验风险最小化的逆强化学习/动态离散选择框架，避免了贝尔曼方程中显式状态转移概率估计的需求。此外，该方法兼容神经网络等非参数估计技术，因此具备扩展到高维无限状态空间的潜力。关键理论依据是贝尔曼残差满足Polyak-Lojasiewicz条件——该性质虽弱于强凸性，但足以保证快速全局收敛。通过系列仿真实验，我们证明该方法持续优于基准方法与现有先进方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of recovering reward or Q* functions from offline behavior data in Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy Inverse Reinforcement Learning (MaxEnt-IRL), where traditional methods often rely on restrictive linear reward parameterizations. The authors propose a novel gradient-based method grounded in an Empirical Risk Minimization (ERM) framework, which eliminates the need to explicitly estimate state transition probabilities in the Bellman equation and supports flexible non-parametric models like neural networks, enabling scalability to high-dimensional state spaces. Theoretically, the approach leverages the Polyak-Lojasiewicz (PL) condition of the Bellman residual to ensure fast global convergence. Experimental results on synthetic data demonstrate that the method consistently outperforms existing benchmark and state-of-the-art alternatives.</div>
<div class="mono" style="margin-top:8px">本文针对从离线行为数据中恢复奖励函数或Q*函数的问题展开研究，该问题对应于动态离散选择（DDC）模型，在机器学习中亦称为离线最大熵逆强化学习（MaxEnt-IRL），传统方法通常依赖于限制性的线性奖励参数化假设。作者提出了一种基于经验风险最小化（ERM）框架的新型梯度方法，该方法避免了在贝尔曼方程中显式估计状态转移概率，并支持如神经网络之类的非参数模型，从而能够扩展到高维状态空间。从理论上看，该方法利用贝尔曼残差满足的Polyak-Lojasiewicz（PL）条件来保证快速的全局收敛性。在合成数据上的实验结果表明，该方法持续优于现有的基准方法和最先进的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Future-as-Label: Scalable Supervision from Real-World Outcomes</div>
<div class="meta-line">Authors: Benjamin Turtel, Paul Wilczewski, Danny Franklin, Kris Skothiem</div>
<div class="meta-line">First: 2026-01-09T22:15:12+00:00 · Latest: 2026-01-14T22:14:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06336v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06336v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time creates free supervision: forecasts about real-world events resolve to verifiable outcomes. The passage of time provides labels that require no annotation. To exploit this structure, we extend reinforcement learning with verifiable rewards to real-world prediction over time. We train language models to make probabilistic forecasts from causally masked information, using proper scoring rules as the reward function once events resolve. Learning is driven entirely by realized outcomes, enabling scalable outcome-based supervision in open-world prediction. On real-world forecasting benchmarks, Qwen3-32B trained using Foresight Learning improves Brier score by 27% and halves calibration error relative to its pretrained baseline, and outperforms Qwen3-235B on both constructed future-event prediction tasks and the Metaculus benchmark despite a 7x parameter disadvantage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未来即标签：基于现实结果的规模化监督学习</div>
<div class="mono" style="margin-top:8px">时间创造免费监督：对现实事件的预测会随时间推移转化为可验证的结果。时间的流逝提供了无需人工标注的天然标签。为利用这一特性，我们将带可验证奖励的强化学习扩展至时序现实预测领域。通过因果遮蔽信息训练语言模型生成概率预测，在事件结果揭晓后采用严格评分规则作为奖励函数。学习过程完全由已实现的结果驱动，实现了开放世界预测中基于结果的规模化监督。在现实预测基准测试中，采用前瞻学习训练的Qwen3-32B模型相较预训练基线：布里尔分数提升27%，校准误差降低50%；在构建的未来事件预测任务和Metaculus基准测试中，其表现均优于参数量7倍于自身的Qwen3-235B模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the idea that the passage of time naturally provides free, verifiable supervision for predictions as future events resolve into outcomes, eliminating the need for manual annotation. The method, termed Foresight Learning, extends reinforcement learning by training language models to make probabilistic forecasts from causally masked information, using proper scoring rules as rewards once outcomes are known to enable scalable outcome-based supervision. Experimental results show that on real-world forecasting benchmarks, a Qwen3-32B model trained with this approach improves Brier score by 27% and halves calibration error compared to its pretrained baseline, and even outperforms the much larger Qwen3-235B model on future-event prediction tasks and the Metaculus benchmark despite having 7 times fewer parameters.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，时间的推移能为预测提供免费且可验证的监督，因为未来事件会演变为实际结果，从而无需人工标注。该方法名为“前瞻学习”，通过强化学习框架，训练语言模型基于因果屏蔽信息进行概率预测，并在结果揭晓后使用适当评分规则作为奖励，实现可扩展的基于结果的监督。实验结果表明，在现实世界预测基准上，使用该方法训练的Qwen3-32B模型相比其预训练基线，Brier分数提升了27%，校准误差减半，并且在构建的未来事件预测任务和Metaculus基准测试中，性能甚至超过了参数量大7倍的Qwen3-235B模型。</div>
</details>
</div>
<div class="card">
<div class="title">An intelligent agent-based simulation of human mobility in extreme urban morphologies</div>
<div class="meta-line">Authors: Abderaouf Bahi, Amel Ourici</div>
<div class="meta-line">First: 2025-07-20T22:35:16+00:00 · Latest: 2026-01-14T21:38:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.15143v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.15143v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates the feasibility of human mobility in extreme urban morphologies, characterized by high-density vertical structures and linear city layouts. To assess whether agents can navigate efficiently within such unprecedented topologies, we develop a hybrid simulation framework that integrates agent-based modeling, reinforcement learning (RL), supervised learning, and graph neural networks (GNNs). The simulation captures multi-modal transportation behaviors across multiple vertical levels and varying density scenarios, using both synthetic data and real-world traces from high-density cities. Experiments show that the full AI-integrated architecture enables agents to achieve an average commute time of 7.8--8.4 minutes, a satisfaction rate exceeding 89\%, and a reachability index over 91\%, even during peak congestion periods. Ablation studies indicate that removing intelligent modules such as RL or GNN significantly degrades performance, with commute times increasing by up to 85\% and reachability falling below 70\%. Environmental modeling demonstrates low energy consumption and minimal CO$_2$ emissions when electric modes are prioritized. These results suggest that efficient and sustainable mobility in extreme urban forms is achievable, provided adaptive AI systems, intelligent infrastructure, and real-time feedback mechanisms are implemented.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>极端城市形态下基于智能体的人类移动性仿真研究</div>
<div class="mono" style="margin-top:8px">本文研究了以高密度垂直结构与线性城市布局为特征的极端城市形态中人类移动的可行性。为评估智能体能否在此类新型拓扑结构中高效导航，我们开发了一种融合智能体建模、强化学习、监督学习与图神经网络的混合仿真框架。该仿真通过合成数据与高密度城市真实轨迹，捕捉了跨多垂直层级与不同密度场景的多模式交通行为。实验表明：完整的人工智能集成架构使智能体在高峰拥堵期仍能实现平均7.8-8.4分钟通勤时间、超过89%的满意度及高于91%的可达性指标。消融实验证明，移除强化学习或图神经网络等智能模块将导致性能显著下降——通勤时间最多增加85%，可达性降至70%以下。环境建模显示，优先采用电动模式时能耗与二氧化碳排放量极低。这些结果表明，通过部署自适应人工智能系统、智能基础设施与实时反馈机制，极端城市形态下的高效可持续移动是可实现的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand human mobility in extreme urban morphologies like high-density vertical structures and linear cities, this paper develops a hybrid simulation framework integrating agent-based modeling, reinforcement learning, supervised learning, and graph neural networks to assess navigation efficiency. The method models multi-modal transportation across vertical levels using both synthetic data and real-world traces from dense cities. Experimental results show that the AI-integrated system enables agents to achieve an average commute time of 7.8–8.4 minutes, a satisfaction rate over 89%, and a reachability index above 91% even during peak congestion, while ablation studies confirm the critical role of intelligent modules, and environmental modeling indicates low energy consumption and CO₂ emissions when electric modes are prioritized.</div>
<div class="mono" style="margin-top:8px">本研究旨在探索人类在极端城市形态（如高密度垂直结构和线性城市）中的移动可行性，为此开发了一个融合基于智能体的建模、强化学习、监督学习和图神经网络的混合仿真框架来评估导航效率。该方法利用合成数据和高密度城市的真实轨迹，模拟了跨多个垂直层次的多模式交通行为。实验结果表明，集成人工智能的系统使智能体即使在高峰拥堵期也能实现平均7.8–8.4分钟的通勤时间、超过89%的满意度和高于91%的可达性指数，消融研究证实了智能模块的关键作用，环境建模则显示优先使用电动模式时能耗和二氧化碳排放较低。</div>
</details>
</div>
<div class="card">
<div class="title">CleanSurvival: Automated data preprocessing for time-to-event models using reinforcement learning</div>
<div class="meta-line">Authors: Yousef Koka, David Selby, Gerrit Großmann, Sebastian Vollmer, Kathan Pandya</div>
<div class="meta-line">First: 2025-02-06T10:33:37+00:00 · Latest: 2026-01-14T20:45:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03946v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.03946v2">PDF</a> · <a href="https://github.com/datasciapps/CleanSurvival">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data preprocessing is a critical yet frequently neglected aspect of machine learning, often paid little attention despite its potentially significant impact on model performance. While automated machine learning pipelines are starting to recognize and integrate data preprocessing into their solutions for classification and regression tasks, this integration is lacking for more specialized tasks like survival or time-to-event models. As a result, survival analysis not only faces the general challenges of data preprocessing but also suffers from the lack of tailored, automated solutions in this area. To address this gap, this paper presents &#x27;CleanSurvival&#x27;, a reinforcement-learning-based solution for optimizing preprocessing pipelines, extended specifically for survival analysis. The framework can handle continuous and categorical variables, using Q-learning to select which combination of data imputation, outlier detection and feature extraction techniques achieves optimal performance for a Cox, random forest, neural network or user-supplied time-to-event model. The package is available on GitHub: https://github.com/datasciapps/CleanSurvival Experimental benchmarks on real-world datasets show that the Q-learning-based data preprocessing results in superior predictive performance to standard approaches, finding such a model up to 10 times faster than undirected random grid search. Furthermore, a simulation study demonstrates the effectiveness in different types and levels of missingness and noise in the data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CleanSurvival：基于强化学习的生存时间模型自动化数据预处理</div>
<div class="mono" style="margin-top:8px">数据预处理是机器学习中至关重要却常被忽视的环节，尽管其对模型性能可能产生显著影响，却往往未获足够重视。虽然自动化机器学习流程已开始将数据预处理纳入分类与回归任务的解决方案，但在生存分析或时间-事件模型等更专业的任务中，此类整合仍显不足。因此，生存分析不仅面临数据预处理的普遍挑战，还缺乏针对该领域的定制化自动化解决方案。为填补这一空白，本文提出&#x27;CleanSurvival&#x27;——一种基于强化学习的预处理流程优化方案，专门扩展应用于生存分析。该框架能处理连续与分类变量，通过Q学习算法选择数据填补、异常值检测和特征提取技术的组合，从而为Cox模型、随机森林、神经网络或用户自定义的时间-事件模型实现最优性能。该工具包已在GitHub发布：https://github.com/datasciapps/CleanSurvival 基于真实数据集的实验基准表明，采用Q学习的数据预处理相比标准方法具有更优的预测性能，其模型发现速度比无导向随机网格搜索快达10倍。此外，模拟研究验证了该方法在不同类型与程度的数据缺失及噪声场景中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces CleanSurvival, motivated by the lack of automated data preprocessing solutions tailored for survival analysis, a gap that hinders model performance despite preprocessing&#x27;s critical role. The method employs reinforcement learning, specifically Q-learning, to automatically optimize preprocessing pipelines—including imputation, outlier detection, and feature extraction—for time-to-event models like Cox regression or random forests. Experimental results on real-world datasets demonstrate that this approach yields superior predictive performance compared to standard methods and finds optimal models up to 10 times faster than random grid search, with simulations confirming effectiveness across various data missingness and noise levels.</div>
<div class="mono" style="margin-top:8px">本文提出了CleanSurvival，其动机在于生存分析领域缺乏自动化的数据预处理解决方案，这一缺口影响了模型性能，尽管预处理至关重要。该方法采用强化学习（特别是Q学习）来自动优化预处理流程，包括数据插补、异常值检测和特征提取，适用于Cox回归或随机森林等时间-事件模型。在真实数据集上的实验结果表明，该方法相比标准方法具有更优的预测性能，并能比随机网格搜索快10倍找到最优模型，模拟研究还验证了其在处理不同类型和程度的数据缺失与噪声时的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing</div>
<div class="meta-line">Authors: Yilin Bao, Ziyao He, Zayden Yang</div>
<div class="meta-line">First: 2026-01-14T20:37:26+00:00 · Latest: 2026-01-14T20:37:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09858v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09858v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OUTLINEFORGE：面向科学写作的显式状态分层强化学习框架</div>
<div class="mono" style="margin-top:8px">科学论文生成需要文档级规划与事实依据，但当前大语言模型虽具备局部流畅性，常在全局结构、输入覆盖和引用一致性方面存在不足。本文提出一种强化学习框架，将科学提纲构建视为分层文档结构上的长程规划问题。该方法通过结构化动作对演化提纲进行建模，使系统能渐进构建完整科学文稿。为支持高效稳定的学习，我们引入两阶段优化流程：（i）通过局部计划的反向提纲重构以强化全局结构一致性；（ii）采用前向价值引导的强化学习，其奖励函数显式建模科学正确性、语篇连贯性与引用保真度。此外，我们构建了科学论文生成基准测试，评估文档规划、输入利用、参考文献忠实度、提纲组织及内容层面事实准确性。实验结果表明，本方法在神经基线与大语言模型基线上均取得持续改进，尤其在长程结构连贯性与引用可靠性方面表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of large language models in generating globally coherent and factually grounded scientific papers, this paper introduces OUTLINEFORGE, a hierarchical reinforcement learning framework that treats outline construction as a long-horizon planning problem. The method employs structured actions to model evolving document outlines and uses a two-stage optimization procedure involving backward outline reconstruction for structural consistency and forward value-guided reinforcement learning with rewards for scientific correctness, coherence, and citation fidelity. Experimental results on a new benchmark for scientific paper generation demonstrate consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型在生成全局结构连贯、事实依据充分的科学论文方面的不足，本文提出了OUTLINEFORGE，这是一个将大纲构建视为长程规划问题的分层强化学习框架。该方法通过结构化动作对演化中的文档大纲进行建模，并采用两阶段优化过程，包括用于保证全局结构一致性的逆向大纲重建，以及结合了科学性、连贯性和引用忠实性奖励的正向价值引导强化学习。在一个新的科学论文生成基准测试上的实验结果表明，该方法相较于强大的神经模型和LLM基线取得了持续改进，尤其在长程结构连贯性和引用可靠性方面表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">Non-Expansive Mappings in Two-Time-Scale Stochastic Approximation: Finite-Time Analysis</div>
<div class="meta-line">Authors: Siddharth Chandak</div>
<div class="meta-line">First: 2025-01-18T16:00:14+00:00 · Latest: 2026-01-14T20:17:50+00:00</div>
<div class="meta-line">Comments: Submitted to SIAM Journal on Control and Optimization</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.10806v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.10806v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two-time-scale stochastic approximation algorithms are iterative methods used in applications such as optimization, reinforcement learning, and control. Finite-time analysis of these algorithms has primarily focused on fixed point iterations where both time-scales have contractive mappings. In this work, we broaden the scope of such analyses by considering settings where the slower time-scale has a non-expansive mapping. For such algorithms, the slower time-scale can be viewed as a stochastic inexact Krasnoselskii-Mann iteration. We also study a variant where the faster time-scale has a projection step which leads to non-expansiveness in the slower time-scale. We show that the last-iterate mean square residual error for such algorithms decays at a rate $O(1/k^{1/4-ε})$, where $ε&gt;0$ is arbitrarily small. We further establish almost sure convergence of iterates to the set of fixed points. We demonstrate the applicability of our framework by applying our results to minimax optimization, linear stochastic approximation, and Lagrangian optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双时间尺度随机逼近中的非扩张映射：有限时间分析</div>
<div class="mono" style="margin-top:8px">双时间尺度随机逼近算法是广泛应用于优化、强化学习和控制等领域的迭代方法。现有有限时间分析主要针对两个时间尺度均为压缩映射的不动点迭代。本研究通过考虑慢时间尺度具有非扩张映射的情形，拓展了此类分析的范围。此类算法中，慢时间尺度可视为随机非精确Krasnoselskii-Mann迭代。我们还研究了快时间尺度引入投影步导致慢时间尺度非扩张的变体算法。证明此类算法的末次迭代均方残差以$O(1/k^{1/4-ε})$速率衰减（$ε&gt;0$可任意小），并建立迭代几乎必然收敛至不动点集的结论。通过将结果应用于极小极大优化、线性随机逼近和拉格朗日优化问题，验证了理论框架的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to extend finite-time analysis of two-time-scale stochastic approximation algorithms beyond the restrictive assumption of contractive mappings, focusing instead on scenarios where the slower time-scale employs a non-expansive mapping. The method involves modeling the slower time-scale as a stochastic inexact Krasnoselskii-Mann iteration and also examines a variant with a projection step in the faster time-scale that induces non-expansiveness. Experimentally, the authors prove that the last-iterate mean square residual error decays at a rate of O(1/k^{1/4-ε}) for arbitrarily small ε&gt;0 and establish almost sure convergence to the set of fixed points, demonstrating applicability in minimax optimization, linear stochastic approximation, and Lagrangian optimization.</div>
<div class="mono" style="margin-top:8px">本文的动机在于将双时间尺度随机逼近算法的有限时间分析从压缩映射的限制性假设扩展到更一般的情况，重点关注较慢时间尺度采用非扩张映射的场景。方法上，将较慢时间尺度建模为随机不精确的Krasnoselskii-Mann迭代，并研究了一种在较快时间尺度引入投影步骤从而诱导非扩张性的变体。实验结果表明，最后迭代的均方残差误差以O(1/k^{1/4-ε})的速率衰减（其中ε&gt;0任意小），并证明了迭代几乎必然收敛到不动点集，展示了该方法在极小极大优化、线性随机逼近和拉格朗日优化中的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Eluder dimension: localise it!</div>
<div class="meta-line">Authors: Alireza Bakhtiari, Alex Ayoub, Samuel Robertson, David Janz, Csaba Szepesvári</div>
<div class="meta-line">First: 2026-01-14T19:35:46+00:00 · Latest: 2026-01-14T19:35:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09825v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09825v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迷惑者维度：局部化方法</div>
<div class="mono" style="margin-top:8px">我们建立了广义线性模型类迷惑者维度的下界，表明基于标准迷惑者维度的分析无法得到一阶遗憾界。为此，我们引入了迷惑者维度的局部化方法；该分析不仅立即恢复并改进了伯努利多臂老虎机问题的经典结果，还首次为具有累积回报有界的有限时域强化学习任务提供了真正的一阶遗憾界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of standard eluder dimension analysis in achieving first-order regret bounds for generalized linear models, this paper introduces a localized eluder dimension method. The approach refines the analysis by focusing on localized complexity, which allows it to recover and improve upon classic results for Bernoulli bandits and, for the first time, derive genuine first-order regret bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.</div>
<div class="mono" style="margin-top:8px">针对广义线性模型中标准eluder维度分析无法获得一阶遗憾界的局限性，本文提出了一种局部化eluder维度方法。该方法通过聚焦于局部复杂性来改进分析，从而恢复并改进了伯努利多臂老虎机问题的经典结果，并首次为具有有界累积回报的有限时域强化学习任务推导出真正的一阶遗憾界。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
