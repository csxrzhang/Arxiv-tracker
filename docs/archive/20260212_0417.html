<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-12 04:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260212_0417</div>
    <div class="row"><div class="card">
<div class="title">Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaoyang Wang, Canwen Xu, Boyi Liu, Yite Wang, Siwei Han, Zhewei Yao, Huaxiu Yao, Yuxiong He</div>
<div class="meta-line">First: 2026-02-10T18:55:41+00:00 · Latest: 2026-02-10T18:55:41+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10090v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10090v1">PDF</a> · <a href="https://github.com/Snowflake-Labs/agent-world-model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能体世界模型：面向智能体强化学习的无限合成环境</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的最新进展使自主智能体能够执行需要与工具和环境进行多轮交互的复杂任务。然而，由于缺乏多样且可靠的环境，此类智能体训练的规模化受到限制。本文提出智能体世界模型（AWM），一种完全合成的环境生成流程。利用该流程，我们扩展至覆盖日常场景的1000个环境，智能体可在其中与丰富的工具集（平均每个环境35个工具）交互并获得高质量观测。值得注意的是，这些环境由代码驱动并以数据库为支撑，相比LLM模拟的环境能提供更可靠、一致的状态转换。此外，与从真实环境收集轨迹相比，它们能实现更高效的智能体交互。为验证该资源的有效性，我们对多轮工具使用智能体进行了大规模强化学习。得益于完全可执行的环境和可访问的数据库状态，我们还能设计可靠的奖励函数。在三个基准测试上的实验表明，仅在合成环境中训练（而非特定基准环境）能产生强大的分布外泛化能力。代码发布于https://github.com/Snowflake-Labs/agent-world-model。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation in scaling autonomous agent training due to a lack of diverse and reliable environments by proposing Agent World Model (AWM), a fully synthetic environment generation pipeline. The method creates 1,000 code-driven, database-backed environments covering everyday scenarios with rich toolsets, ensuring reliable state transitions and efficient agent interaction compared to LLM-simulated or real-world settings. Experimental results from large-scale reinforcement learning on multi-turn tool-use agents demonstrate that training exclusively in these synthetic environments enables strong out-of-distribution generalization across three benchmarks, supported by reliable reward functions derived from executable environments and accessible database states.</div>
<div class="mono" style="margin-top:8px">本文针对自主智能体训练因缺乏多样可靠环境而难以扩展的问题，提出了Agent World Model（AWM）这一全合成环境生成流程。该方法创建了1000个代码驱动、数据库支持的日常场景环境，配备丰富工具集，相比基于大语言模型模拟或真实环境，能提供更可靠的状态转换和更高效的智能体交互。通过在多轮工具使用智能体上进行大规模强化学习实验，结果表明，仅在合成环境中训练而非特定基准环境，能在三个基准测试中实现强大的分布外泛化能力，这得益于可执行环境和可访问数据库状态所设计的可靠奖励函数。</div>
</details>
</div>
<div class="card">
<div class="title">CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs</div>
<div class="meta-line">Authors: Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully</div>
<div class="meta-line">First: 2026-02-10T18:51:39+00:00 · Latest: 2026-02-10T18:51:39+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10085v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10085v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/code-sharp/homepage}{here}$">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CODE-SHARP：作为分层奖励程序的持续开放式技能发现与演化</div>
<div class="mono" style="margin-top:8px">开发能够开放式发现和学习新技能的智能体是人工智能领域的重大挑战。尽管强化学习为训练智能体掌握复杂技能提供了强大框架，但它通常依赖于人工设计的奖励函数。这对于开放式技能发现是不可行的，因为有意义技能集合并非先验已知。虽然近期方法在自动化奖励函数设计方面展现出有希望的结果，但它们仍局限于为预定义任务优化奖励。为解决这一限制，我们提出了“作为分层奖励程序的持续开放式技能发现与演化”（CODE-SHARP），这是一个利用基础模型开放式扩展和优化分层技能库的新框架，该库以代码中可执行奖励函数的有向图形式构建。我们证明，仅通过发现的SHARP技能生成的奖励进行训练的目标条件智能体，能够在Craftax环境中学会解决日益长时程的目标。当由基于高层基础模型的规划器组合时，这些发现的技能使单一目标条件智能体能够解决复杂的长时程任务，平均性能超越预训练智能体和任务特定专家策略超过134%。我们将开源代码并提供更多视频$\href{https://sites.google.com/view/code-sharp/homepage}{此处}$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling artificial agents to autonomously discover and learn novel skills without predefined tasks, moving beyond traditional reinforcement learning that relies on hand-designed rewards. It introduces CODE-SHARP, a framework that uses Foundation Models to continuously expand and refine a hierarchical archive of skills, represented as executable reward functions in code within a directed graph. Experimental results in the Craftax environment demonstrate that an agent trained solely on these discovered rewards can solve increasingly long-horizon goals, and when combined with a high-level planner, it outperforms pretrained agents and task-specific expert policies by over 134% on average in complex tasks.</div>
<div class="mono" style="margin-top:8px">本文旨在解决人工智能代理在无需预定义任务的情况下自主发现和学习新技能的挑战，超越了依赖手工设计奖励的传统强化学习方法。它提出了CODE-SHARP框架，利用基础模型持续扩展和优化一个层次化技能库，该库以代码中可执行的奖励函数形式表示，并组织为有向图。在Craftax环境中的实验结果表明，仅基于这些发现奖励训练的代理能够解决日益长视界的目标，当与高层规划器结合时，在复杂任务中平均性能超过预训练代理和任务特定专家策略134%以上。</div>
</details>
</div>
<div class="card">
<div class="title">Anagent For Enhancing Scientific Table &amp; Figure Analysis</div>
<div class="meta-line">Authors: Xuehang Guo, Zhiyong Lu, Tom Hope, Qingyun Wang</div>
<div class="meta-line">First: 2026-02-10T18:46:28+00:00 · Latest: 2026-02-10T18:46:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10081v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10081v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xhguo7.github.io/Anagent/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \&amp; figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \&amp; figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \&amp; figure analysis. Our project page: https://xhguo7.github.io/Anagent/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于增强科学图表分析能力的Anagent框架</div>
<div class="mono" style="margin-top:8px">在科学研究中，分析工作需要准确解读复杂的多模态知识，整合不同来源的证据，并基于领域专业知识进行推理。然而，当前人工智能系统难以稳定展现此类能力。科学图表在复杂性和多样性方面的挑战，加之异构结构和长上下文需求，构成了科学图表分析的根本障碍。为量化这些挑战，我们提出了AnaBench大规模基准测试集，涵盖九个科学领域的63,178个实例，并沿七个复杂度维度进行系统分类。为应对这些挑战，我们提出Anagent多智能体框架，通过四个专业智能体增强科学图表分析能力：规划器将任务分解为可执行的子任务，专家通过定向工具执行检索任务相关信息，求解器综合信息生成连贯分析，评审器通过五维质量评估进行迭代优化。我们进一步开发了模块化训练策略，利用监督微调和专业强化学习优化个体能力，同时保持高效协作。在170个子领域的综合评估表明，Anagent实现了显著提升——无需训练时最高提升13.43%，经微调后最高提升42.12%，同时揭示面向任务的推理和上下文感知的问题解决能力对高质量科学图表分析至关重要。项目页面：https://xhguo7.github.io/Anagent/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of current AI systems in interpreting complex multimodal scientific data, this paper introduces Anagent, a multi-agent framework designed to enhance the analysis of scientific tables and figures. The method employs four specialized agents—Planner, Expert, Solver, and Critic—that work collaboratively to decompose tasks, retrieve domain-specific information, synthesize analyses, and iteratively refine outputs, supported by modular training strategies including supervised finetuning and reinforcement learning. Experimental results on the AnaBench benchmark, comprising 63,178 instances across nine domains, show that Anagent achieves significant performance gains, with improvements of up to 13.43% in training-free settings and 42.12% with finetuning, highlighting the importance of task-oriented reasoning and context-aware problem-solving for high-quality scientific analysis.</div>
<div class="mono" style="margin-top:8px">针对当前人工智能系统在解释复杂多模态科学数据方面的不足，本文提出了Anagent，一个用于增强科学图表分析的多智能体框架。该方法采用四个专门智能体——规划者、专家、求解者和批判者——协同工作，以分解任务、检索领域特定信息、综合分析和迭代优化输出，并辅以包括监督微调和强化学习在内的模块化训练策略。在包含九个领域63,178个实例的AnaBench基准测试上的实验结果表明，Anagent取得了显著的性能提升，在无需训练的设置下改进高达13.43%，经过微调后可达42.12%，这凸显了面向任务的推理和上下文感知问题解决对高质量科学分析的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability</div>
<div class="meta-line">Authors: Aaditya Vikram Prasad, Connor Watts, Jack Merullo, Dhruvil Gala, Owen Lewis, Thomas McGrath, Ekdeep Singh Lubana</div>
<div class="meta-line">First: 2026-02-10T18:33:45+00:00 · Latest: 2026-02-10T18:33:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10067v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>特征作为奖励：通过可解释性实现开放式任务的可扩展监督</div>
<div class="mono" style="margin-top:8px">在大规模数据集上训练的语言模型已被证明能够学习编码抽象概念（如事实性或意图）的特征。此类特征传统上用于测试时监控或引导。我们提出一种替代性功能：将特征作为开放式任务的可扩展监督。我们以幻觉减少这一理想但开放的行为为例，设计了一个强化学习（RL）流程，命名为RLFR（基于特征奖励的强化学习），该流程将特征用作奖励函数。基于一个识别候选幻觉主张的新型探测框架，我们的流程教导模型在不确定其事实性时进行干预并修正其补全结果。此外，该流程通过我们的奖励特征引导，实现了可扩展的测试时计算。在Gemma-3-12B-IT模型上实施的这一端到端流程，产生了一个策略，其产生幻觉的可能性比原始模型降低58%，同时保持标准基准测试的性能。综上所述，通过将监督建立在特征语言的基础上，本文为利用可解释性学习开放式任务引入了一种新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for scalable supervision in open-ended tasks, where traditional reward design is difficult. It introduces RLFR, a reinforcement learning method that uses interpretable features, specifically those encoding factuality, as reward functions to reduce hallucinations in language models. The method involves a probing framework to identify potential hallucinations, then trains the model to intervene and correct uncertain completions, with experimental results showing that the resulting policy reduces hallucination likelihood by 58% on Gemma-3-12B-IT while maintaining benchmark performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是为开放任务提供可扩展的监督方法，因为传统奖励设计在此类任务中较为困难。它提出了RLFR方法，一种利用可解释特征（特别是编码事实性的特征）作为奖励函数的强化学习框架，以减少语言模型的幻觉。该方法通过探测框架识别潜在幻觉，训练模型干预并修正不确定的生成内容，实验结果表明，在Gemma-3-12B-IT模型上，所得策略将幻觉可能性降低了58%，同时保持了标准基准测试的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction</div>
<div class="meta-line">Authors: Abdul Matin, Rupasree Dey, Tanjim Bin Faruk, Shrideep Pallickara, Sangmi Lee Pallickara</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-13T19:59:04+00:00 · Latest: 2026-02-10T18:18:19+00:00</div>
<div class="meta-line">Comments: Accepted to the KGML Bridge at AAAI 2026 (non-archival)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12445v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12445v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识引导的掩码自编码器：融合线性光谱混合与光谱角感知重建</div>
<div class="mono" style="margin-top:8px">将领域知识融入深度学习已成为提升模型可解释性、泛化能力和数据效率的重要方向。本研究提出一种新型知识引导的基于视觉Transformer的掩码自编码器，将科学领域知识嵌入自监督重建过程。该方法不仅依赖数据驱动优化，还引入线性光谱混合模型作为物理约束，并采用基于物理原理的光谱角制图法，确保学习到的表征符合观测信号与潜在成分间的已知结构关系。该框架通过联合优化线性光谱混合模型损失、光谱角制图损失与传统的Huber损失目标，在特征空间中同时提升数值精度与几何一致性。这种知识引导的设计增强了重建保真度，在有限监督下稳定训练过程，并生成基于物理原理的可解释潜在表征。实验结果表明，所提模型显著提升了重建质量与下游任务性能，彰显了在基于Transformer的自监督学习中嵌入物理归纳偏置的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to integrate domain knowledge into deep learning for improved interpretability, generalization, and data efficiency, this paper proposes a knowledge-guided masked autoencoder based on Vision Transformers that incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and uses a Spectral Angle Mapper (SAM) for reconstruction. The method jointly optimizes LSMM and SAM losses with a Huber loss objective to enforce both numerical accuracy and geometric consistency in the feature space. Experimental results demonstrate that the model significantly enhances reconstruction quality and boosts downstream task performance, validating the effectiveness of embedding physics-informed inductive biases in self-supervised learning.</div>
<div class="mono" style="margin-top:8px">本研究旨在将领域知识融入深度学习，以提升模型的可解释性、泛化能力和数据效率，提出了一种基于视觉Transformer的知识引导掩码自编码器，在线性光谱混合模型作为物理约束下，结合基于物理的光谱角制图进行重建。该方法通过联合优化线性光谱混合模型损失、光谱角制图损失和胡伯损失目标，确保特征空间中的数值精度和几何一致性。实验结果表明，该模型显著提高了重建质量并改善了下游任务性能，验证了在基于Transformer的自监督学习中嵌入物理归纳偏置的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization</div>
<div class="meta-line">Authors: Xinchen Han, Hossam Afifi, Michel Marot, Xilu Wang, Lu Yin</div>
<div class="meta-line">Venue: ICASSP</div>
<div class="meta-line">First: 2026-02-10T18:15:58+00:00 · Latest: 2026-02-10T18:15:58+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10048v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10048v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \textbf{F}ine-grained \textbf{G}roup policy \textbf{O}ptimization (\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于细粒度分组策略优化的长链思维压缩</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）常生成不必要的冗长链式思维（CoT）推理，这会增加计算成本和延迟，却未带来相应的性能提升。本文提出细粒度分组策略优化（FGO），这是一种强化学习（RL）算法，通过将分组响应细分为子组，并根据长度和熵分配适当权重，从而实现有效的CoT压缩。同时，作为分组相对策略优化（GRPO）的增强变体，FGO成功解决了GRPO的两个主要局限：数据利用效率低下和熵崩溃。我们在多个推理LLM和基准测试（包括MATH500、AIME24、AMC23和Minerva）上评估FGO。实验结果表明，FGO在不降低性能的情况下实现了高效的CoT压缩，并同时解决了GRPO的关键局限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational inefficiency of overly verbose Chain-of-Thought reasoning in Large Language Models, this paper introduces Fine-grained Group policy Optimization (FGO), a Reinforcement Learning method that compresses reasoning chains by subdividing group responses and weighting them based on length and entropy. The approach improves upon Group Relative Policy Optimization by addressing its data inefficiency and entropy collapse issues. Experiments on reasoning benchmarks like MATH500 and AIME24 demonstrate that FGO achieves effective compression without performance degradation while overcoming GRPO&#x27;s limitations.</div>
<div class="mono" style="margin-top:8px">针对大语言模型中思维链推理冗长导致计算成本高的问题，本文提出了细粒度组策略优化方法，这是一种通过细分组响应并依据长度和熵分配权重的强化学习算法，以实现有效的推理压缩。该方法改进了组相对策略优化，解决了其数据利用低效和熵崩溃两大缺陷。在MATH500、AIME24等多个推理基准上的实验结果表明，FGO能在不降低性能的前提下实现高效压缩，同时成功克服了GRPO的关键限制。</div>
</details>
</div>
<div class="card">
<div class="title">Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Akshay Mete, Shahid Aamir Sheikh, Tzu-Hsiang Lin, Dileep Kalathil, P. R. Kumar</div>
<div class="meta-line">First: 2026-02-10T18:11:00+00:00 · Latest: 2026-02-10T18:11:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10044v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观世界模型：基于模型的深度强化学习中的高效探索</div>
<div class="mono" style="margin-top:8px">高效探索始终是强化学习（RL）的核心挑战，尤其在稀疏奖励环境中。我们提出乐观世界模型（OWMs），这是一个基于原则且可扩展的乐观探索框架，将自适应控制中的经典奖励偏置最大似然估计（RBMLE）引入深度RL。与置信上界（UCB）类探索方法不同，OWMs通过引入乐观动力学损失直接融入模型学习，该损失使想象的状态转移偏向更高奖励结果。这种完全基于梯度的损失既无需不确定性估计，也无需约束优化。我们的方法可与现有世界模型框架即插即用，在保持可扩展性的同时仅需对标准训练流程进行最小改动。我们在两种前沿世界模型架构中实例化OWMs，构建了乐观DreamerV3和乐观STORM模型，相比基线版本在样本效率和累积回报上均展现出显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of efficient exploration in sparse-reward reinforcement learning by introducing Optimistic World Models (OWMs), a framework that integrates optimistic exploration into model-based deep RL through reward-biased maximum likelihood estimation. The method augments standard world model training with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes, eliminating the need for uncertainty estimates or constrained optimization while remaining plug-and-play with existing architectures. Experimental results show that OWMs instantiated as Optimistic DreamerV3 and Optimistic STORM achieve significant improvements in sample efficiency and cumulative return compared to baseline models.</div>
<div class="mono" style="margin-top:8px">本文针对稀疏奖励强化学习中的高效探索问题，提出了乐观世界模型（OWMs），该框架通过奖励偏置最大似然估计将乐观探索融入基于模型的深度强化学习中。该方法通过添加乐观动力学损失来增强标准世界模型训练，使想象的状态转移偏向更高奖励结果，无需不确定性估计或约束优化，并能即插即用地兼容现有架构。实验结果表明，以乐观DreamerV3和乐观STORM实现的OWMs相比基线模型，在样本效率和累积回报上均取得显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection</div>
<div class="meta-line">Authors: Changjiang Jiang, Xinkuan Sha, Fengchang Yu, Jingjing Liu, Jian Liu, Mingqi Fang, Chenfeng Zhang, Wei Lu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-10T18:10:08+00:00 · Latest: 2026-02-10T18:10:08+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10042v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10042v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model&#x27;s ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fake-HR1：面向合成图像检测的视觉语言模型推理机制重构</div>
<div class="mono" style="margin-top:8px">近期研究表明，在检测过程中引入思维链推理能提升模型识别合成图像的能力。但过度冗长的推理会产生显著的资源开销（包括令牌消耗与延迟），在处理明显伪造图像时尤为冗余。为此，我们提出Fake-HR1——据我们所知，这是首个能根据生成式检测任务特性自适应判断是否需要推理的大规模混合推理模型。为实现该目标，我们设计了两阶段训练框架：首先通过混合微调进行冷启动初始化，随后采用混合推理分组策略优化的在线强化学习，隐式学习何时选择适宜的推理模式。实验表明，Fake-HR1能针对不同类型查询自适应执行推理，在推理能力与生成式检测性能上均超越现有大语言模型，同时显著提升响应效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve the efficiency of synthetic image detection by vision-language models, as existing methods using Chain-of-Thought reasoning incur high resource costs and latency, especially for obvious forgeries. The proposed method, Fake-HR1, introduces a hybrid-reasoning approach that adaptively decides when reasoning is needed, employing a two-stage training framework with Hybrid Fine-Tuning for initialization and online reinforcement learning via Hybrid-Reasoning Grouped Policy Optimization to learn optimal reasoning mode selection. Experimental results demonstrate that Fake-HR1 outperforms existing large language models in both reasoning capability and detection performance, while significantly enhancing response efficiency by adaptively applying reasoning across diverse query types.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升视觉语言模型在合成图像检测中的效率，因为现有基于思维链推理的方法存在高资源消耗和延迟问题，尤其对于明显伪造图像冗余。提出的Fake-HR1方法采用混合推理机制，能根据生成检测任务特性自适应决定是否进行推理，通过两阶段训练框架实现：先进行混合微调以冷启动初始化，再采用混合推理分组策略优化的在线强化学习来隐式学习推理模式选择。实验结果表明，Fake-HR1在不同查询类型中自适应执行推理，在推理能力和生成检测性能上均超越现有大语言模型，同时显著提高了响应效率。</div>
</details>
</div>
<div class="card">
<div class="title">Effectiveness of Binary Autoencoders for QUBO-Based Optimization Problems</div>
<div class="meta-line">Authors: Tetsuro Abe, Masashi Yamashita, Shu Tanaka</div>
<div class="meta-line">First: 2026-02-10T17:59:29+00:00 · Latest: 2026-02-10T17:59:29+00:00</div>
<div class="meta-line">Comments: 14 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10037v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In black-box combinatorial optimization, objective evaluations are often expensive, so high quality solutions must be found under a limited budget. Factorization machine with quantum annealing (FMQA) builds a quadratic surrogate model from evaluated samples and optimizes it on an Ising machine. However, FMQA requires binary decision variables, and for nonbinary structures such as integer permutations, the choice of binary encoding strongly affects search efficiency. If the encoding fails to reflect the original neighborhood structure, small Hamming moves may not correspond to meaningful modifications in the original solution space, and constrained problems can yield many infeasible candidates that waste evaluations. Recent work combines FMQA with a binary autoencoder (bAE) that learns a compact binary latent code from feasible solutions, yet the mechanism behind its performance gains is unclear. Using a small traveling salesman problem as an interpretable testbed, we show that the bAE reconstructs feasible tours accurately and, compared with manually designed encodings at similar compression, better aligns tour distances with latent Hamming distances, yields smoother neighborhoods under small bit flips, and produces fewer local optima. These geometric properties explain why bAE+FMQA improves the approximation ratio faster while maintaining feasibility throughout optimization, and they provide guidance for designing latent representations for black-box optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>二元自编码器在基于QUBO的优化问题中的有效性</div>
<div class="mono" style="margin-top:8px">在黑盒组合优化中，目标函数评估通常代价高昂，因此必须在有限预算内找到高质量解。基于量子退火的因子分解机（FMQA）通过已评估样本构建二次代理模型，并在伊辛机上优化该模型。然而，FMQA要求决策变量为二元形式，对于整数排列等非二元结构，二元编码的选择会显著影响搜索效率。若编码未能反映原始邻域结构，微小的汉明移动可能无法对应原始解空间中有意义的修改，且约束问题可能产生大量不可行解，造成评估资源浪费。近期研究将FMQA与二元自编码器（bAE）结合，后者从可行解中学习紧凑的二元潜在编码，但其性能提升的内在机制尚不明确。本文以小型旅行商问题作为可解释测试平台，证明bAE能准确重构可行路径，且在相同压缩率下，相较于人工设计的编码，bAE能更好地对齐路径距离与潜在汉明距离，在小规模比特翻转下产生更平滑的邻域结构，并生成更少的局部最优解。这些几何特性解释了为何bAE+FMQA能在保持优化全程可行性的同时更快提升近似比，并为黑盒优化的潜在表示设计提供了指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the effectiveness of binary autoencoders (bAE) in enhancing quantum annealing-based optimization (FMQA) for black-box combinatorial problems, motivated by the challenge that manual binary encodings for non-binary structures like permutations can misrepresent neighborhood relationships and generate many infeasible solutions, wasting expensive evaluations. The method integrates a bAE with FMQA to learn a compact binary latent representation directly from feasible solutions, aiming to preserve the original solution space&#x27;s structure. Experimental results on a small traveling salesman problem demonstrate that the bAE-based encoding accurately reconstructs feasible tours, better aligns tour distances with latent Hamming distances, creates smoother neighborhoods under bit flips, and reduces local optima, leading to faster improvement in approximation ratio while maintaining feasibility throughout optimization.</div>
<div class="mono" style="margin-top:8px">本文研究了二元自编码器（bAE）在增强基于量子退火的优化（FMQA）处理黑盒组合问题时的有效性，其动机在于，对于排列等非二元结构，手动设计的二元编码可能扭曲邻域关系并产生大量不可行解，从而浪费昂贵的评估资源。该方法将bAE与FMQA结合，直接从可行解中学习紧凑的二元潜在表示，以保持原始解空间的结构。在小型旅行商问题上的实验结果表明，基于bAE的编码能准确重建可行路径，更好地对齐路径距离与潜在汉明距离，在比特翻转下产生更平滑的邻域，并减少局部最优解，从而在优化过程中保持可行性的同时更快地提升近似比。</div>
</details>
</div>
<div class="card">
<div class="title">ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning</div>
<div class="meta-line">Authors: Qingnan Ren, Shiting Huang, Zhen Fang, Zehui Chen, Lin Chen, Lijun Li, Feng Zhao</div>
<div class="meta-line">First: 2026-02-10T17:40:39+00:00 · Latest: 2026-02-10T17:40:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce \textbf{ADORA} (\textbf{A}dvantage \textbf{D}ynamics via \textbf{O}nline \textbf{R}ollout \textbf{A}daptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function&#x27;s weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ADORA：基于动态优势估计的强化学习推理模型训练</div>
<div class="mono" style="margin-top:8px">强化学习已成为在数学解题到想象推理等复杂任务中开发推理模型的核心技术。这些模型的优化通常依赖于策略梯度方法，其效果关键在于优势函数的准确估计。然而，现有方法多采用静态优势估计，这种做法因忽视训练样本随时间变化的动态效用，导致信用分配效率低下。这一局限引发次优的策略更新，进而表现为收敛速度减慢和学习不稳定性增加，因为模型无法有效适应样本效用的动态演变。为解决此问题，我们提出\textbf{ADORA}（基于在线推演自适应的优势动态框架），一种新颖的策略优化框架。ADORA通过在线模型推演过程中，依据训练样本效用演变将其自适应分类为暂时优势与劣势样本，从而动态调整优势函数的权重。这种定制化的数据区分策略使ADORA能无缝集成到现有策略优化算法中，无需显著改动架构，使策略能优先从信息量更大的经验中学习，进而实现更高效的政策更新。跨多种模型家族与不同数据规模的广泛评估表明，ADORA是一个鲁棒且高效的框架。它在几何与数学任务中显著提升长链推理能力，无需敏感的超参数调优即可持续取得显著性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the inefficiency of static advantage estimation in reinforcement learning for training reasoning models, which leads to suboptimal policy updates and slow convergence. The proposed method, ADORA, dynamically adjusts advantage function weighting by adaptively categorizing training data based on evolving sample utility during online rollouts, enabling seamless integration into existing policy optimization algorithms. Experimental results show that ADORA significantly enhances performance in geometric and mathematical reasoning tasks across diverse models and data scales, achieving robust gains without sensitive hyperparameter tuning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于强化学习中用于训练推理模型的静态优势估计效率低下，导致策略更新次优和收敛缓慢。提出的方法ADORA通过在线模型滚动中根据样本效用动态分类训练数据，自适应调整优势函数权重，可无缝集成到现有策略优化算法中。实验结果表明，ADORA在几何和数学推理任务中显著提升了不同模型和数据规模下的性能，无需敏感超参数调优即实现了稳健的性能增益。</div>
</details>
</div>
<div class="card">
<div class="title">A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging</div>
<div class="meta-line">Authors: Bharathkumar Hegde, Melanie Bouroche</div>
<div class="meta-line">First: 2026-02-10T17:30:09+00:00 · Latest: 2026-02-10T17:30:09+00:00</div>
<div class="meta-line">Comments: Accepted in IEEE IV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10007v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10007v1">PDF</a> · <a href="https://github.com/hkbharath/MARL-MASS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向拥堵匝道汇入场景的协同安全屏障：实现CAV安全高效换道</div>
<div class="mono" style="margin-top:8px">密集交通中的换道是网联自动驾驶车辆面临的重要挑战。现有换道控制器多侧重安全保障或协同提升交通效率，却未兼顾二者冲突目标。为此，我们提出基于控制屏障函数设计的多智能体安全屏障，通过简易算法构建交互拓扑图以捕捉CAV间的多智能体交互，实现安全协同换道。进一步，我们将MASS集成至先进的多智能体强化学习换道控制器中，在保障安全的同时通过定制奖励函数优化效率，形成MARL-MASS控制器。在拥堵匝道汇入仿真中验证表明：MASS能严格遵循安全约束实现协同换道；定制奖励函数提升了带安全屏障的MARL策略稳定性。MARL-MASS通过在安全约束下探索协同换道策略，有效平衡了拥堵交通中安全保障与效率提升的权衡。代码已开源：https://github.com/hkbharath/MARL-MASS</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge for Connected and Autonomous Vehicles (CAVs) to perform lane changes in dense traffic by balancing the often conflicting objectives of safety and traffic efficiency, which existing controllers treat separately. The method introduces the Multi-Agent Safety Shield (MASS), built with Control Barrier Functions to guarantee safety through collaborative multi-agent interactions modeled via graph-based topologies, and integrates it with a Multi-Agent Reinforcement Learning (MARL) controller enhanced with a custom reward function to prioritize efficiency. Experimental results from a congested on-ramp merging simulation show that the proposed MARL-MASS controller successfully ensures strict safety constraint adherence while improving policy stability and effectively trading off safety with traffic efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对网联自动驾驶车辆在密集交通中变道时安全与交通效率难以兼顾的挑战，提出了一种协同解决方案。研究方法设计了基于控制屏障函数的多智能体安全屏障，通过图结构建模车辆交互以确保安全协同，并将其与多智能体强化学习控制器结合，利用定制奖励函数优化效率。在拥堵匝道合流模拟中的实验结果表明，所提出的MARL-MASS控制器在严格遵循安全约束的同时，提升了策略稳定性，有效平衡了安全保证与交通效率提升。</div>
</details>
</div>
<div class="card">
<div class="title">Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning</div>
<div class="meta-line">Authors: Shijie Zhang, Xiang Guo, Rujun Guo, Shaoyu Liu, Xiaozhao Wang, Guanjun Jiang, Kevin Zhang</div>
<div class="meta-line">First: 2026-02-10T17:28:12+00:00 · Latest: 2026-02-10T17:28:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10006v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10006v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a &quot;Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)&quot; pipeline to achieve AFRL. However, directly applying existing RL training often leads to \textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards. From an information theory perspective: RL inherently minimizes the \textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to &quot;reward hacking.&quot; On the other hand, SFT minimizes the \textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules. Based on this insight, we propose a \textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance. Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>先回答后推理：通过模式平衡强化学习对齐搜索相关性</div>
<div class="mono" style="margin-top:8px">构建同时满足低延迟与高性能的搜索相关性模型是搜索行业的长期挑战。为满足在线系统的毫秒级响应需求，同时保留大语言模型（LLM）的可解释推理轨迹，我们提出创新的\textbf{先回答后推理（AFRL）}范式。该范式要求模型在首个标记即输出确定性相关性分数，随后生成结构化逻辑解释。受推理模型成功启发，我们采用“监督微调（SFT）+强化学习（RL）”流程实现AFRL。然而，直接应用现有RL训练常导致搜索相关性任务中的\textbf{模式崩溃}——模型为追求高奖励而遗忘复杂长尾规则。从信息论视角看：RL本质最小化\textbf{反向KL散度}，倾向于寻找概率峰值（模式寻求）且易引发“奖励破解”；而SFT最小化\textbf{正向KL散度}，迫使模型覆盖数据分布（模式覆盖）从而有效锚定专家规则。基于此洞见，我们提出\textbf{模式平衡优化}策略，在Stepwise-GRPO训练中融入SFT辅助损失以平衡两种特性。此外，构建自动化指令进化系统与多阶段课程学习机制确保专家级数据质量。大量实验表明，我们的320亿参数教师模型达到最先进性能。AFRL架构还支持高效知识蒸馏，成功将专家级逻辑迁移至6亿参数模型，实现推理深度与部署延迟的协同优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to balance low latency and high performance in search relevance systems, this paper introduces the Answer-First, Reason Later (AFRL) paradigm, which requires models to output a relevance score immediately followed by a structured explanation. The method employs a Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) pipeline, enhanced with a Mode-Balanced Optimization strategy that combines an SFT auxiliary loss to counteract RL-induced mode collapse, alongside an automated instruction evolution system for data quality. Experimental results show that the proposed 32B teacher model achieves state-of-the-art performance, and the AFRL architecture enables effective knowledge distillation to a compact 0.6B model, maintaining reasoning depth while meeting deployment latency constraints.</div>
<div class="mono" style="margin-top:8px">本文旨在解决搜索相关性系统中低延迟与高性能难以兼顾的长期挑战，提出了“先回答，后推理”（AFRL）的新范式，要求模型首先生成相关性分数，随后提供结构化解释。方法上采用监督微调（SFT）与强化学习（RL）结合的流程，并引入模式平衡优化策略，通过加入SFT辅助损失来缓解RL导致的模式崩溃问题，同时构建自动化指令演化系统以确保数据质量。实验结果表明，所提出的320亿参数教师模型达到了最先进的性能，且AFRL架构能有效进行知识蒸馏，成功将专家级逻辑迁移至6亿参数的小模型中，从而在保证推理深度的同时满足部署延迟要求。</div>
</details>
</div>
<div class="card">
<div class="title">ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference</div>
<div class="meta-line">Authors: Junda Wang, Zhichao Yang, Dongxu Zhang, Sanjit Singh Batra, Robert E. Tillman</div>
<div class="meta-line">First: 2026-02-10T17:27:26+00:00 · Latest: 2026-02-10T17:27:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10004v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10004v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated &lt;stop&gt; signals, and (iii) &lt;stop&gt;-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESTAR：面向高效推理的早期停止令牌感知推理方法</div>
<div class="mono" style="margin-top:8px">大型推理模型通过生成长链思维实现最优性能，但常在得出正确答案后继续冗余推理，浪费计算资源。本文提出早期停止令牌感知推理方法，通过检测并减少此类冗余推理，在不牺牲准确性的前提下提升效率。该方法结合：（1）基于轨迹的分类器，用于识别可安全停止推理的时机；（2）监督微调，指导模型生成自发的&lt;停止&gt;信号；（3）具备停止感知的强化学习，在自生成停止点处截断计算路径，并结合计算感知奖励机制。在四个推理数据集上的实验表明，ESTAR将推理长度减少约3.7倍（从4799步降至1290步），同时保持准确性（74.9%对比74.2%），并展现出强大的跨领域泛化能力。这些结果凸显了早期停止作为提升大型推理模型效率的简洁而有效的机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational inefficiency of large reasoning models (LRMs) that often produce unnecessarily long chains-of-thought after reaching correct answers. To address this, the authors propose ESTAR, a method that combines a trajectory-based classifier to detect when reasoning can be safely halted, supervised fine-tuning to teach models to generate self-stop signals, and stop-aware reinforcement learning with compute-aware rewards to truncate reasoning early. Experimental results on four reasoning datasets demonstrate that ESTAR reduces average reasoning length by approximately 3.7 times (from 4,799 to 1,290 tokens) while maintaining comparable accuracy (74.9% vs. 74.2%), and shows strong generalization across domains.</div>
<div class="mono" style="margin-top:8px">本文的动机在于大型推理模型（LRMs）在得出正确答案后仍生成冗长的思维链，导致计算效率低下。为解决此问题，作者提出了ESTAR方法，该方法结合了基于轨迹的分类器来检测何时可安全停止推理、监督微调以教导模型生成自停止信号，以及具有计算感知奖励的停止感知强化学习来提前截断推理过程。在四个推理数据集上的实验结果表明，ESTAR将平均推理长度减少了约3.7倍（从4,799降至1,290个标记），同时保持了相当的准确率（74.9%对比74.2%），并展现出强大的跨领域泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</div>
<div class="meta-line">Authors: Brennen A. Hill</div>
<div class="meta-line">First: 2025-11-04T04:07:16+00:00 · Latest: 2026-02-10T16:34:37+00:00</div>
<div class="meta-line">Comments: National Science Foundation (NSF) workshop on Brain-Inspired Dynamics for Engineering Energy-Efficient Circuits and Artificial Intelligence</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02241v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.02241v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell&#x27;s actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network&#x27;s intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network&#x27;s parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构可塑性作为主动推理：一种受生物学启发的稳态控制架构</div>
<div class="mono" style="margin-top:8px">传统神经网络虽功能强大，却依赖全局反向传播等生物学上难以实现的机制。本文提出结构自适应预测推理网络（SAPIN），该计算模型受主动推理原理及生物神经培养物形态可塑性的启发。SAPIN在二维网格上运行，其处理单元（细胞）通过最小化局部预测误差进行学习。模型具有两种并行的核心学习机制：一是基于细胞实际激活与学习期望间时序差异的局部类赫布突触可塑性规则；二是细胞在网格中物理迁移以优化信息接收场的结构可塑性机制。这种双重方法使网络能同时学习信息处理方式（突触权重）与计算资源布局策略（网络拓扑）。我们在经典Cart Pole强化学习基准上验证了SAPIN模型，结果表明该架构能成功解决CartPole任务并实现稳健性能。网络最小化预测误差与维持稳态的内在驱动力足以发现稳定的平衡策略。研究发现：持续学习会导致不稳定，但在成功后锁定网络参数可获得稳定策略。对100个成功智能体进行锁定后100轮测试显示，锁定网络平均保持82%的成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the biological implausibility of global backpropagation in traditional neural networks, this paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a model inspired by active inference and morphological plasticity. The method employs a 2D grid where cells learn via local prediction error minimization, combining a Hebbian-like synaptic plasticity rule with a structural mechanism for cell migration to optimize both synaptic weights and network topology. Experimental results on the Cart Pole benchmark show that SAPIN can successfully solve the task, achieving robust performance through error minimization and homeostasis, with locked networks maintaining an average 82% success rate over 100 episodes post-training.</div>
<div class="mono" style="margin-top:8px">本文的动机源于传统神经网络中全局反向传播在生物学上的不合理性，因此引入了结构自适应预测推理网络（SAPIN），这是一个受主动推理和形态可塑性启发的模型。该方法采用二维网格，细胞通过局部预测误差最小化进行学习，结合了类赫布突触可塑性规则和细胞迁移的结构机制，以同时优化突触权重和网络拓扑。在Cart Pole基准测试中的实验结果表明，SAPIN能够成功解决该任务，通过误差最小化和稳态维持实现稳健性能，锁定后的网络在100次后续测试中平均保持82%的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Jie Xiao, Meng Chen, Qingnan Ren, Jingwei Song, Jiaqi Huang, Yangshen Deng, Chris Tong, Wanyi Chen, Suli Wang, Ziqian Bi, Shuo Lu, Yiqun Duan, Xu Wang, Rymon Yu, Ween Yang, Lynn Ai, Eric Yang, Bill Shi</div>
<div class="meta-line">First: 2026-02-02T14:57:53+00:00 · Latest: 2026-02-10T15:56:18+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02192v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.02192v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ECHO-2：面向成本高效强化学习的大规模分布式推演框架</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是大语言模型（LLM）后训练的关键阶段，涉及推演生成、奖励评估与集中式学习的反复交互。分布式推演执行为利用更具成本效益的推理资源提供了可能，但引入了广域协调与策略传播的挑战。本文提出ECHO-2——一个面向后训练的分布式RL框架，支持远程推理工作节点且能处理不可忽略的传播延迟。ECHO-2将集中式学习与分布式推演相结合，将有界策略陈旧度作为用户可控参数，实现推演生成、传播与训练的重叠执行。我们提出基于重叠的容量模型，关联训练时间、传播延迟与推演吞吐量，形成维持学习器利用率的实用资源配置规则。为缓解传播瓶颈并降低成本，ECHO-2采用对等辅助流水线广播及异构工作节点的成本感知激活机制。在真实广域带宽环境下对4B与8B模型进行GRPO后训练的实验中，ECHO-2在保持与强基线相当的RL奖励的同时，显著提升了成本效益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for ECHO-2 is to reduce the cost of reinforcement learning (RL) for large language models by distributing rollout generation to remote, cost-efficient inference resources, while addressing the coordination and policy dissemination challenges this introduces. The method combines centralized learning with distributed rollouts, explicitly treating policy staleness as a tunable parameter to overlap rollout, dissemination, and training phases; it also employs an overlap-based capacity model for provisioning and techniques like peer-assisted pipelined broadcast to mitigate dissemination bottlenecks. Main experimental results from GRPO post-training of 4B and 8B models under real wide-area bandwidth conditions show that ECHO-2 significantly improves cost efficiency while maintaining RL reward performance comparable to strong baselines.</div>
<div class="mono" style="margin-top:8px">ECHO-2的动机是通过将强化学习中的策略执行（rollout）分发到远程、成本效益高的推理资源上，以降低大语言模型后训练的成本，同时解决由此带来的广域协调和策略传播挑战。该方法将集中式学习与分布式策略执行相结合，将策略过时性视为可调参数，以实现策略生成、传播和训练的重叠；它还采用基于重叠的容量模型进行资源调配，并使用对等辅助流水线广播等技术来缓解传播瓶颈。在真实广域带宽环境下对4B和8B模型进行GRPO后训练的实验结果表明，ECHO-2在保持与强基线相当的强化学习奖励性能的同时，显著提高了成本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis</div>
<div class="meta-line">Authors: James Rudd-Jones, Mirco Musolesi, María Pérez-Ortiz</div>
<div class="meta-line">First: 2025-04-17T09:18:04+00:00 · Latest: 2026-02-10T15:30:59+00:00</div>
<div class="meta-line">Comments: Published in AAMAS&#x27;25 Blue Sky Ideas Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.12777v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.12777v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate policy development faces significant challenges due to deep uncertainty, complex system dynamics, and competing stakeholder interests. Climate simulation methods, such as Earth System Models, have become valuable tools for policy exploration. However, their typical use is for evaluating potential polices, rather than directly synthesizing them. The problem can be inverted to optimize for policy pathways, but the traditional optimization approaches often struggle with non-linear dynamics, heterogeneous agents, and comprehensive uncertainty quantification. We propose a framework for augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to address these limitations. We identify key challenges at the interface between climate simulations and the application of MARL in the context of policy synthesis, including reward definition, scalability with increasing agents and state spaces, uncertainty propagation across linked systems, and solution validation. Additionally, we discuss challenges in making MARL-derived solutions interpretable and useful for policy-makers. Our framework provides a foundation for more sophisticated climate policy exploration while acknowledging important limitations and areas for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向环境政策合成的多智能体强化学习仿真框架</div>
<div class="mono" style="margin-top:8px">气候政策制定面临深层不确定性、复杂系统动态和多元利益相关者竞争等重大挑战。以地球系统模型为代表的气候仿真方法已成为政策探索的重要工具，但其通常用于评估潜在政策而非直接合成政策。虽然可将问题转化为政策路径优化，但传统优化方法常难以处理非线性动态、异质智能体及综合不确定性量化。本文提出通过多智能体强化学习增强气候仿真的框架以应对这些局限。我们识别了气候仿真与MARL在政策合成应用中的关键挑战，包括奖励定义、智能体与状态空间扩展的可扩展性、关联系统间的不确定性传递以及解决方案验证。同时探讨了使MARL衍生方案对决策者具有可解释性与实用性的挑战。本框架为深化气候政策探索奠定基础，并明确了当前局限与未来研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of deep uncertainty, complex dynamics, and competing interests in climate policy development, this paper proposes augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to directly synthesize policies rather than merely evaluate them. The method addresses limitations of traditional optimization by tackling key interface challenges such as reward definition, scalability with many agents and states, uncertainty propagation, and solution validation. The main experimental results are conceptual, establishing a foundational framework for more sophisticated policy exploration while acknowledging limitations in interpretability and the need for future research to make MARL-derived solutions actionable for policymakers.</div>
<div class="mono" style="margin-top:8px">本文的动机是应对气候政策制定中深度不确定性、复杂系统动态和利益相关者竞争带来的挑战，提出通过多智能体强化学习增强气候模拟，以直接合成政策而非仅进行评估。该方法通过解决奖励定义、智能体与状态空间扩展的可扩展性、关联系统间的不确定性传播以及解决方案验证等关键接口挑战，克服了传统优化方法的局限。主要实验结果是概念性的，建立了一个用于更复杂气候政策探索的基础框架，同时承认了在可解释性方面的局限性，并指出未来研究需使多智能体强化学习得出的解决方案对决策者更具实用性。</div>
</details>
</div>
<div class="card">
<div class="title">SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs</div>
<div class="meta-line">Authors: Niccolo Avogaro, Nayanika Debnath, Li Mi, Thomas Frick, Junling Wang, Zexue He, Hang Hua, Konrad Schindler, Mattia Rigotti</div>
<div class="meta-line">First: 2026-02-06T10:05:25+00:00 · Latest: 2026-02-10T15:30:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06566v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06566v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses &quot;thinking with images&quot; by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARC：分离感知与推理电路以实现视觉语言模型的测试时扩展</div>
<div class="mono" style="margin-top:8px">尽管近期取得进展，视觉语言模型（VLM）的测试时扩展——即在推理过程中按需动态增加计算资源——仍存在脆弱性：针对图像的非结构化思维链会混淆感知与推理，导致生成冗长混乱的上下文，使细微的感知错误可能引发完全错误的答案。此外，现有方法需依赖人工设计奖励的昂贵强化学习才能获得良好性能。本文提出SPARC（分离感知与推理电路），这是一种显式解耦视觉感知与推理的模块化框架。受大脑中序列化感觉-认知处理机制启发，SPARC采用两阶段流程：模型首先执行显式视觉搜索以定位问题相关区域，随后基于这些区域进行条件化推理生成最终答案。这种分离机制支持：1）通过非对称计算分配实现独立测试时扩展（如在分布偏移时优先处理感知阶段）；2）选择性优化（当感知阶段成为端到端性能瓶颈时可单独改进）；3）通过低分辨率全局搜索与高分辨率区域精处理的协同，压缩视觉令牌总量并降低计算开销。在多项视觉推理基准测试中，SPARC均超越整体式基线模型与强视觉定位方法。例如，在V* VQA基准上，SPARC将Qwen3VL-4B的准确率提升6.7个百分点；在具有挑战性的OOD任务中，其以200倍更低的令牌预算超越“图像思维”方法4.6个点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SPARC, a modular framework designed to address the brittleness of test-time scaling in vision-language models (VLMs) by explicitly decoupling visual perception from reasoning, motivated by the need to prevent cascading errors from entangled chains-of-thought and to avoid expensive reinforcement learning. The method implements a two-stage pipeline inspired by brain processing: first performing explicit visual search to localize question-relevant regions, then conditioning reasoning on those regions, enabling independent scaling, asymmetric compute allocation, selective optimization, and compressed contexts via multi-resolution processing. Experimental results show that SPARC outperforms monolithic baselines and visual-grounding approaches on challenging benchmarks, such as improving Qwen3VL-4B accuracy on the V* VQA benchmark by 6.7 percentage points and surpassing &quot;thinking with images&quot; by 4.6 points on an OOD task with a 200× lower token budget.</div>
<div class="mono" style="margin-top:8px">本文提出了SPARC框架，旨在解决视觉语言模型在测试时扩展中的脆弱性问题，其动机是通过解耦视觉感知与推理来避免感知错误在链式思维中传播，并减少对昂贵强化学习的依赖。该方法受大脑处理机制启发，采用两阶段流程：先进行显式视觉搜索以定位问题相关区域，再基于这些区域进行推理，从而支持独立扩展、非对称计算分配、选择性优化以及通过多分辨率处理压缩上下文。实验结果表明，SPARC在多个视觉推理基准测试中优于整体基线和视觉定位方法，例如将Qwen3VL-4B在V* VQA基准上的准确率提升了6.7个百分点，并在一个分布外任务中以200倍更低的令牌预算超越了“thinking with images”方法4.6个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, Wanli Ouyang, Feng Zhao</div>
<div class="meta-line">First: 2025-10-01T17:58:05+00:00 · Latest: 2026-02-10T15:16:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01304v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01304v2">PDF</a> · <a href="https://github.com/yuzeng0-0/AGILE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向视觉语言模型感知与推理能力增强的自主拼图交互学习</div>
<div class="mono" style="margin-top:8px">尽管当前大型视觉语言模型在多模态理解与推理方面取得进展，但其基础感知与推理能力仍存在局限。具体而言，即使在简单拼图任务上，现有模型表现接近随机水平，揭示了核心感知与推理能力的不足。高质量视觉语言数据虽能提升这些能力，但其稀缺性与有限可扩展性构成显著制约。为此，我们提出AGILE（自主拼图交互学习）方法，通过将拼图求解构建为交互过程，使模型能逐步与环境互动：每一步模型根据当前状态生成可执行代码以执行动作，环境则提供细粒度视觉反馈引导任务完成。通过这种观察与交互的迭代循环，模型借助探索与反馈持续提升感知与推理能力。实验表明，AGILE不仅在多种复杂度拼图任务上显著提升性能（如在2×2设定下准确率从9.5%提升至82.8%），还在9项通用视觉任务中展现出强泛化能力（平均提升3.1%）。这些结果证实了感知与推理能力的显著增强。本研究为推进多模态模型的推理与泛化能力开辟了新途径，并为多模态强化学习数据稀缺问题提供了高效可扩展的解决方案。代码与数据集已开源：https://github.com/yuzeng0-0/AGILE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited perceptual and reasoning abilities of current Vision-Language Models (VLMs), which perform poorly even on simple jigsaw tasks, this paper introduces AGILE, an Agentic Jigsaw Interaction Learning method. The approach formulates jigsaw solving as an interactive process where the model generates executable code to act based on the current state and receives fine-grained visual feedback from the environment, enabling iterative improvement through exploration. Experimental results demonstrate that AGILE significantly boosts performance on jigsaw tasks, increasing accuracy from 9.5% to 82.8% in a 2×2 setting, and shows strong generalization across 9 general vision tasks with an average improvement of 3.1%, indicating enhanced perceptual and reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">针对当前视觉语言模型在感知和推理能力上的不足，即使在简单拼图任务上也表现不佳，本文提出了AGILE方法，即一种基于智能体交互的拼图学习框架。该方法将拼图求解构建为一个交互过程，模型根据当前状态生成可执行代码进行操作，并从环境中获得细粒度视觉反馈，通过探索实现迭代改进。实验结果表明，AGILE显著提升了拼图任务性能，在2×2设置下准确率从9.5%提高到82.8%，并在9个通用视觉任务上表现出强泛化能力，平均提升3.1%，证明了其感知和推理能力的有效增强。</div>
</details>
</div>
<div class="card">
<div class="title">From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation</div>
<div class="meta-line">Authors: Zezhou Wang, Ziyun Zhang, Xiaoyi Zhang, Zhuzhong Qian, Yan Lu</div>
<div class="meta-line">First: 2026-01-09T13:26:38+00:00 · Latest: 2026-02-10T15:12:17+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05787v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05787v2">PDF</a> · <a href="https://github.com/LEON-gittech/Verl_GUI.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从离线策略到在线策略：通过双层专家-策略同化增强图形用户界面智能体</div>
<div class="mono" style="margin-top:8px">视觉语言模型正越来越多地被部署为操作桌面和浏览器的计算机使用智能体。性能领先的计算机使用智能体是基于框架的系统，将规划与执行解耦；而端到端的屏幕截图到动作策略虽更易部署，但在OSWorld-Verified等基准测试中表现落后。OSWorld等图形用户界面数据集存在两大瓶颈：仅提供数百个可交互、可验证的任务与环境，且专家轨迹必须通过与这些环境交互收集，导致数据难以扩展。因此，我们研究如何利用强化学习从可验证奖励中，最优地利用少量现有专家轨迹来训练端到端策略。简单地将这些离线策略轨迹混入在线策略强化学习是脆弱的：即使经过格式转换，专家轨迹仍存在与学习器的结构不匹配和分布偏移。我们提出BEPA（双层专家-策略同化），通过基础策略下自生成的可达轨迹将静态专家轨迹转化为策略对齐的指导（第一层），并在强化学习中采用按任务动态更新的缓存（第二层）。在OSWorld-Verified上，BEPA将UITARS1.5-7B的成功率从22.87%提升至32.13%，并将保留测试集从5.74%提升至10.30%，在MMBench-GUI和Online-Mind2Web上也取得一致增益。代码与数据发布于：https://github.com/LEON-gittech/Verl_GUI.git</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of training effective end-to-end GUI agents from limited expert data, motivated by the difficulty of scaling interactive, verifiable GUI datasets like OSWorld. The authors propose BEPA, a bi-level method that assimilates static off-policy expert trajectories into on-policy reinforcement learning by first generating policy-aligned reachable trajectories and then using a dynamically updated cache during training. Experimental results show that BEPA significantly improves success rates on OSWorld-Verified, raising performance from 22.87% to 32.13% for a base model and demonstrating consistent gains on other GUI benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从有限的专家数据中训练有效端到端GUI智能体的挑战，其动机在于像OSWorld这类可交互、可验证的GUI数据集难以扩展。作者提出了BEPA，一种双层方法，通过首先生成与策略对齐的可达轨迹，然后在训练中使用动态更新的缓存，将静态的离策略专家轨迹同化到在策略强化学习中。实验结果表明，BEPA在OSWorld-Verified基准上显著提高了成功率，将基础模型的性能从22.87%提升至32.13%，并在其他GUI基准测试中展现出一致的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Code2World: A GUI World Model via Renderable Code Generation</div>
<div class="meta-line">Authors: Yuhao Zheng, Li&#x27;an Zhong, Yi Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu, Linyuan Lv, Philip Torr, Kevin Qinghong Lin</div>
<div class="meta-line">First: 2026-02-10T14:56:19+00:00 · Latest: 2026-02-10T14:56:19+00:00</div>
<div class="meta-line">Comments: github: https://github.com/AMAP-ML/Code2World project page: https://amap-ml.github.io/Code2World/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09856v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09856v1">PDF</a> · <a href="https://github.com/AMAP-ML/Code2World">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://amap-ml.github.io/Code2World/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Code2World：通过可渲染代码生成的GUI世界模型</div>
<div class="mono" style="margin-top:8px">自主GUI代理通过感知界面并执行操作与环境交互。作为虚拟沙箱，GUI世界模型通过支持条件动作预测，赋予代理类人的预见能力。然而，现有基于文本和像素的方法难以同时实现高视觉保真度和细粒度结构可控性。为此，我们提出Code2World——一种通过可渲染代码生成模拟下一视觉状态的视觉语言编码器。具体而言，为应对数据稀缺问题，我们构建了AndroidCode数据集：将GUI轨迹转换为高保真HTML代码，并通过视觉反馈修正机制优化合成代码，最终获得包含超8万组高质量屏幕-动作对的数据集。为适配现有视觉语言模型进行代码预测，我们首先通过监督微调实现格式布局跟随的冷启动，进而应用渲染感知强化学习——以渲染结果为奖励信号，强制保持视觉语义保真度与动作一致性。大量实验表明，Code2World-8B在下一代UI预测任务中达到最优性能，媲美GPT-5与Gemini-3-Pro-Image。值得注意的是，Code2World以灵活方式显著提升下游导航成功率，在AndroidWorld导航任务中将Gemini-2.5-Flash的性能提升9.5%。代码已开源：https://github.com/AMAP-ML/Code2World。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for GUI world models that combine high visual fidelity with fine-grained structural control for autonomous agents, this paper introduces Code2World, a vision-language model that predicts next visual states by generating renderable HTML code. The method addresses data scarcity by constructing the AndroidCode dataset of over 80K screen-action pairs through translating GUI trajectories into HTML and refining code via visual feedback, then adapts VLMs using supervised fine-tuning followed by render-aware reinforcement learning that rewards visual and action consistency. Experimental results show Code2World-8B achieves top performance in next UI prediction, rivaling models like GPT-5 and Gemini-3-Pro-Image, and boosts downstream navigation success rates, such as improving Gemini-2.5-Flash by +9.5% on AndroidWorld navigation.</div>
<div class="mono" style="margin-top:8px">本文的动机是为自主GUI智能体开发兼具高视觉保真度和细粒度结构控制的世界模型，为此提出了Code2World，这是一种通过生成可渲染HTML代码来预测下一视觉状态的视觉语言模型。方法上，为解决数据稀缺问题，构建了包含超过8万对屏幕-动作的AndroidCode数据集，通过将GUI轨迹转化为HTML并利用视觉反馈机制精炼代码；随后采用监督微调初始化模型，并应用以渲染结果为奖励信号的渲染感知强化学习来确保视觉语义保真和动作一致性。主要实验结果表明，Code2World-8B在下一UI预测任务中达到顶尖性能，可与GPT-5和Gemini-3-Pro-Image相媲美，并能显著提升下游导航成功率，如在AndroidWorld导航任务中将Gemini-2.5-Flash的性能提高了9.5%。</div>
</details>
</div>
<div class="card">
<div class="title">CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization</div>
<div class="meta-line">Authors: Beicheng Xu, Keyao Ding, Wei Liu, Yupeng Lu, Bin Cui</div>
<div class="meta-line">First: 2026-02-10T14:54:17+00:00 · Latest: 2026-02-10T14:54:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09851v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy &quot;FE-then-HPO&quot; workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (ToT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoFEH：基于协作贝叶斯超参数优化的LLM驱动特征工程框架</div>
<div class="mono" style="margin-top:8px">特征工程（FE）在自动化机器学习（AutoML）中至关重要，但传统方法将其视为黑盒搜索，在僵化的预定义搜索空间内操作且缺乏领域感知，成为性能瓶颈。大型语言模型（LLMs）通过语义推理生成无边界算子提供了新途径，但现有方法未能构建自由形式的FE流水线，仍局限于特征生成等孤立子任务。最关键的是，它们很少与机器学习模型的超参数优化（HPO）联合优化，导致贪婪的“先FE后HPO”工作流无法捕捉强FE-HPO交互。本文提出CoFEH——一个将基于LLM的FE与贝叶斯HPO交错结合的协作框架，实现鲁棒的端到端AutoML。CoFEH包含：1）基于思维树（ToT）的LLM驱动FE优化器，探索灵活的FE流水线；2）贝叶斯优化（BO）模块求解HPO；3）动态优化器选择器，通过自适应调度FE与HPO步骤实现交错优化。我们创新性地引入双向条件机制，在LLM与BO间共享上下文以实现协同决策。实验表明，CoFEH不仅优于传统及基于LLM的FE基线，在联合优化下更实现了卓越的端到端性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in automated feature engineering (FE) where traditional methods operate as black-box searches within rigid spaces, and existing LLM-based approaches are confined to isolated subtasks without joint optimization with hyperparameter tuning (HPO). To overcome this, the authors propose CoFEH, a collaborative framework that interleaves LLM-driven FE using Tree of Thought reasoning with Bayesian HPO, facilitated by a dynamic scheduler and a mutual conditioning mechanism for shared context between components. Experimental results demonstrate that CoFEH outperforms both traditional and LLM-based FE baselines, achieving superior end-to-end performance through effective joint optimization of FE and HPO.</div>
<div class="mono" style="margin-top:8px">本文针对自动化特征工程（FE）中的局限性提出改进，传统方法在固定搜索空间中作为黑盒运行，而现有基于大语言模型（LLM）的方法局限于孤立子任务，未能与超参数优化（HPO）联合优化。为此，作者提出了CoFEH框架，该框架通过动态调度器协调LLM驱动的FE（利用思维树推理）与贝叶斯HPO，并引入相互条件机制在组件间共享上下文以实现协同决策。实验结果表明，CoFEH在联合优化FE和HPO时，不仅超越了传统和基于LLM的FE基线，还实现了更优的端到端性能。</div>
</details>
</div>
<div class="card">
<div class="title">AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</div>
<div class="meta-line">Authors: Valter Schütz, Han Wu, Reza Rezvan, Linus Aronsson, Morteza Haghir Chehreghani</div>
<div class="meta-line">First: 2025-08-20T14:29:16+00:00 · Latest: 2026-02-10T14:21:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14734v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.14734v2">PDF</a> · <a href="https://github.com/Linusaronsson/AFA-Benchmark">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world scenarios, acquiring all features of a data instance can be expensive or impractical due to monetary cost, latency, or privacy concerns. Active Feature Acquisition (AFA) addresses this challenge by dynamically selecting a subset of informative features for each data instance, trading predictive performance against acquisition cost. While numerous methods have been proposed for AFA, ranging from myopic information-theoretic strategies to non-myopic reinforcement learning approaches, fair and systematic evaluation of these methods has been hindered by a lack of standardized benchmarks. In this paper, we introduce AFABench, the first benchmark framework for AFA. Our benchmark includes a diverse set of synthetic and real-world datasets, supports a wide range of acquisition policies, and provides a modular design that enables easy integration of new methods and tasks. We implement and evaluate representative algorithms from all major categories, including static, myopic, and reinforcement learning-based approaches. To test the lookahead capabilities of AFA policies, we introduce a novel synthetic dataset, CUBE-NM, designed to expose the limitations of myopic selection. Our results highlight key trade-offs between different AFA strategies and provide actionable insights for future research. The benchmark code is available at: https://github.com/Linusaronsson/AFA-Benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AFABench：一种用于基准测试主动特征获取的通用框架</div>
<div class="mono" style="margin-top:8px">在许多现实场景中，由于成本、延迟或隐私问题，获取数据实例的所有特征可能代价高昂或不切实际。主动特征获取（AFA）通过动态选择每个数据实例的信息特征子集来解决这一挑战，在预测性能与获取成本之间进行权衡。尽管已提出多种AFA方法，从短视的信息论策略到非短视的强化学习方法，但由于缺乏标准化基准，这些方法的公平系统评估一直受阻。本文介绍了首个AFA基准框架AFABench。该基准包含多样化的合成与真实数据集，支持广泛的获取策略，并提供模块化设计，便于新方法与任务的集成。我们实现并评估了所有主要类别的代表性算法，包括静态、短视和基于强化学习的方法。为测试AFA策略的前瞻能力，我们引入了一个新颖的合成数据集CUBE-NM，旨在揭示短视选择的局限性。实验结果凸显了不同AFA策略间的关键权衡，并为未来研究提供了实用见解。基准代码发布于：https://github.com/Linusaronsson/AFA-Benchmark。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for a standardized evaluation framework for Active Feature Acquisition (AFA) methods, which aim to reduce feature acquisition costs while maintaining predictive performance, as existing methods lack fair comparison. The authors introduce AFABench, a generic benchmark framework that includes diverse datasets, supports various acquisition policies, and offers modular design for easy integration of new methods; they also create a synthetic dataset, CUBE-NM, to test non-myopic capabilities. Experimental results from evaluating representative algorithms, including static, myopic, and reinforcement learning approaches, reveal key trade-offs between strategies and provide insights for future research.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要为主动特征获取方法建立一个标准化的评估框架，因为这些方法旨在降低特征获取成本同时保持预测性能，但现有方法缺乏公平比较。作者提出了AFABench，这是一个通用基准框架，包含多样化的数据集，支持多种获取策略，并采用模块化设计以便轻松集成新方法；他们还创建了一个合成数据集CUBE-NM来测试非短视能力。通过评估代表性算法（包括静态、短视和基于强化学习的方法），实验结果揭示了不同策略之间的关键权衡，并为未来研究提供了实用见解。</div>
</details>
</div>
<div class="card">
<div class="title">A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer</div>
<div class="meta-line">Authors: Azka Nasir, Fatima Dossa, Muhammad Ahmed Atif, Mohammad Ahmed Atif</div>
<div class="meta-line">First: 2026-02-10T14:18:03+00:00 · Latest: 2026-02-10T14:18:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09810v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09810v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (DDQN) and Dueling DQN influence transfer behavior across environments. Using CartPole as a source task and LunarLander as a structurally distinct target task, we evaluate a fixed layer-wise representation transfer protocol under identical hyperparameters and training conditions, with baseline agents trained from scratch used to contextualize transfer effects. Empirical results show that DDQN consistently avoids negative transfer under the examined setup and maintains learning dynamics comparable to baseline performance in the target environment. In contrast, Dueling DQN consistently exhibits negative transfer under identical conditions, characterized by degraded rewards and unstable optimization behavior. Statistical analysis across multiple random seeds confirms a significant performance gap under transfer. These findings suggest that architectural inductive bias is strongly associated with robustness to cross-environment transfer in value-based deep reinforcement learning under the examined transfer protocol.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双DQN与竞争DQN在跨环境迁移下的对照研究</div>
<div class="mono" style="margin-top:8px">深度强化学习中的迁移学习通常旨在提升稳定性并降低训练成本，但在显著领域偏移下也可能失效。本文通过对照实验研究双深度Q网络（DDQN）与竞争DQN的架构差异如何影响跨环境迁移行为。以CartPole作为源任务、LunarLander作为结构差异明显的目标任务，在相同超参数与训练条件下采用固定分层表征迁移方案，并以从头训练的基线智能体作为迁移效果的参照基准。实验结果表明：在既定实验设置下，DDQN能持续避免负迁移，且在目标环境中保持与基线相当的学习动态；相比之下，竞争DQN在相同条件下始终呈现负迁移现象，表现为奖励衰减与优化过程不稳定。基于多随机种子的统计分析证实了迁移条件下显著的性能差异。这些发现表明，在所考察的迁移方案中，架构归纳偏置与基于价值的深度强化学习在跨环境迁移的鲁棒性存在强关联。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how architectural differences between Double DQN and Dueling DQN affect transfer learning robustness in deep reinforcement learning, motivated by the need to understand and mitigate negative transfer under domain shifts. The method employs a controlled protocol, transferring learned representations from CartPole to the structurally distinct LunarLander environment while keeping hyperparameters and training conditions identical, with baseline agents trained from scratch for comparison. The main experimental results show that Double DQN consistently avoids negative transfer and maintains performance comparable to baselines, whereas Dueling DQN exhibits significant negative transfer with degraded rewards and unstable optimization, a statistically significant gap confirmed across multiple random seeds, indicating architectural inductive bias strongly influences transfer robustness.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Double DQN与Dueling DQN之间的架构差异如何影响深度强化学习中迁移学习的鲁棒性，其动机在于理解和缓解领域偏移下的负迁移问题。方法采用受控协议，将学习到的表示从CartPole任务迁移到结构不同的LunarLander环境，同时保持超参数和训练条件一致，并使用从头训练的基线智能体进行对比。主要实验结果表明，Double DQN始终避免负迁移并保持与基线相当的性能，而Dueling DQN在相同条件下表现出显著的负迁移，奖励下降且优化不稳定，这一性能差距在多次随机种子实验中得到统计确认，表明架构归纳偏置对所考察迁移协议下的跨环境迁移鲁棒性有重要影响。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning</div>
<div class="meta-line">Authors: Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchun Shi</div>
<div class="meta-line">First: 2025-04-03T16:16:35+00:00 · Latest: 2026-02-10T13:57:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.03784v6">Abs</a> · <a href="https://arxiv.org/pdf/2504.03784v6">PDF</a> · <a href="https://github.com/VRPO/VRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. The code is available at https://github.com/VRPO/VRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型微调的鲁棒人类反馈强化学习</div>
<div class="mono" style="margin-top:8px">人类反馈强化学习已成为将大语言模型输出与人类偏好对齐的关键技术。现有RLHF算法多采用Bradley-Terry模型学习奖励函数，但其基于的人类偏好假设难以反映现实判断的复杂性与多变性。本文提出一种鲁棒算法，以增强现有方法在奖励模型误设情况下的性能。理论分析表明，该算法能降低奖励与策略估计量的方差，从而改进遗憾界。在大语言模型基准数据集上的实证评估显示，所提算法持续优于现有方法，在Anthropic Helpful and Harmless数据集中77-81%的响应优于基线。代码发布于https://github.com/VRPO/VRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of standard RLHF methods, which often rely on the Bradley-Terry model for reward learning—a model that may not capture the full complexity of human preferences, leading to potential misspecifications. To address this, the authors propose a robust algorithm designed to enhance existing RLHF approaches by reducing the variance of reward and policy estimators, thereby improving theoretical regret bounds. Experimentally, the method is validated on large language model benchmarks, where it consistently outperforms baseline techniques; notably, on the Anthropic Helpful and Harmless dataset, 77-81% of the generated responses are preferred over those from existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，标准的基于人类反馈的强化学习方法通常依赖Bradley-Terry模型来学习奖励函数，但该模型的假设可能无法反映人类偏好的复杂性和多变性，从而导致奖励模型设定错误。为解决此问题，作者提出了一种鲁棒算法，旨在通过降低奖励和策略估计量的方差来增强现有方法，从而改进理论遗憾界。在大型语言模型基准数据集上的实验评估表明，该算法始终优于现有方法；特别是在Anthropic Helpful and Harmless数据集上，77-81%的生成响应比基线方法更受青睐。</div>
</details>
</div>
<div class="card">
<div class="title">Flexible Entropy Control in RLVR with Gradient-Preserving Perspective</div>
<div class="meta-line">Authors: Kun Chen, Peng Shi, Fanfan Liu, Haibo Qiu, Zhixiong Zeng, Siqi Yang, Wenji Mao</div>
<div class="meta-line">First: 2026-02-10T13:42:12+00:00 · Latest: 2026-02-10T13:42:12+00:00</div>
<div class="meta-line">Comments: https://github.com/Kwen-Chen/Flexible-Entropy-Control</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09782v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09782v1">PDF</a> · <a href="https://github.com/Kwen-Chen/Flexible-Entropy-Control">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于梯度保持视角的RLVR灵活熵控制</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为增强大语言模型推理能力的关键方法。然而，持续训练常导致策略熵崩溃，表现为熵值快速衰减引发过早过度自信、输出多样性降低，以及抑制学习的梯度范数消失。梯度保持裁剪是影响该动态的主要因素，但现有缓解策略多为静态方案，缺乏将裁剪机制与精确熵控制相连接的框架。本文提出从梯度保持裁剪视角重塑强化学习的熵控制。我们首先通过理论与实验验证了特定重要性采样比率区域对熵增长与削减的贡献。基于这些发现，我们引入采用动态裁剪阈值的新型调控机制以实现精确熵管理。此外，我们设计并评估了动态熵控制策略，包括“先增后减”“减-增-减”及振荡衰减模式。实验结果表明，这些策略能有效缓解熵崩溃，并在多个基准测试中取得更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of policy entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models, where continuous training leads to premature overconfidence, reduced output diversity, and vanishing gradients that hinder learning. The authors propose reshaping entropy control from the perspective of Gradient-Preserving Clipping, first analyzing the contributions of specific importance sampling ratio regions to entropy dynamics and then introducing a novel regulation mechanism using dynamic clipping thresholds to manage entropy precisely. Experimental results show that dynamic strategies like increase-then-decrease and oscillatory decay effectively mitigate entropy collapse and achieve superior performance across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型的可验证奖励强化学习中策略熵崩溃的问题展开研究，该问题会导致模型过早过度自信、输出多样性降低以及梯度消失从而阻碍学习。作者从梯度保持裁剪的角度重塑熵控制方法，首先理论并实证分析了重要性采样比率特定区域对熵增减的贡献，进而提出一种使用动态裁剪阈值的新型调节机制以实现精确的熵管理。实验结果表明，诸如先增后减、振荡衰减等动态策略能有效缓解熵崩溃，并在多个基准测试中取得了更优的性能。</div>
</details>
</div>
<div class="card">
<div class="title">PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning</div>
<div class="meta-line">Authors: Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei</div>
<div class="meta-line">First: 2026-01-13T16:02:35+00:00 · Latest: 2026-02-10T13:41:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08679v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08679v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PersonaDual：通过自适应推理平衡个性化与客观性</div>
<div class="mono" style="margin-top:8px">随着用户日益期望大语言模型（LLM）与其偏好保持一致，个性化信息变得愈发重要。然而，个性化信息是一把双刃剑：它虽能提升交互体验，却可能损害客观性与事实准确性，尤其当其与问题不匹配时。为缓解此问题，我们提出PersonaDual框架，该框架在单一模型中同时支持通用客观推理与个性化推理，并能根据上下文自适应切换模式。PersonaDual首先通过监督微调（SFT）学习两种推理模式，随后借助我们提出的DualGRPO强化学习方法进一步优化模式选择。在客观性与个性化基准测试上的实验表明，PersonaDual在保留个性化优势的同时减少了干扰，实现了近乎无干扰的性能表现，并能更有效地利用有益的个性化信号来提升客观问题解决能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of balancing personalization with objectivity in LLMs, as user-aligned responses can enhance interaction but risk factual compromise when preferences conflict with queries. To mitigate this, the authors propose PersonaDual, a framework that integrates both general objective and personalized reasoning in one model, using adaptive mode switching based on context; it is trained via supervised fine-tuning to learn dual reasoning patterns and further optimized with a novel reinforcement learning method called DualGRPO to refine mode selection. Experimental results on objective and personalized benchmarks demonstrate that PersonaDual nearly eliminates interference from unhelpful personalization while effectively leveraging beneficial personalized signals to enhance problem-solving, thus maintaining personalization benefits without sacrificing objectivity.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型中个性化与客观性之间的平衡问题展开研究，因为适应用户偏好的回答虽能提升交互体验，但当偏好与查询冲突时可能损害事实准确性。为缓解此问题，作者提出了PersonaDual框架，该框架在单一模型中整合了通用客观推理和个性化推理，并根据上下文自适应切换模式；其训练首先通过监督微调学习两种推理模式，随后采用新颖的强化学习方法DualGRPO进行优化以改进模式选择。在客观性和个性化基准测试上的实验结果表明，PersonaDual几乎消除了有害个性化信息的干扰，同时有效利用有益的个性化信号来提升问题解决能力，从而在保留个性化优势的同时确保了客观性。</div>
</details>
</div>
<div class="card">
<div class="title">Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO</div>
<div class="meta-line">Authors: Kun Peng, Conghui Tan, Yu Liu, Guohua Tang, Zhongqian Sun, Wei Yang, Zining Zhu, Lei Jiang, Yanbing Liu, Hao Peng</div>
<div class="meta-line">First: 2026-02-09T11:32:02+00:00 · Latest: 2026-02-10T13:34:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08533v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08533v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users&#x27; traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework&#x27;s superior performance, sample efficiency, and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于智能体博弈与自适应树状GRPO的对话模型优化</div>
<div class="mono" style="margin-top:8px">开放域对话智能体旨在通过适应用户特质实现个性化交互，但现有方法存在关键局限：过度依赖预收集用户数据，以及强化学习中忽视长期对话价值的短视偏差。为此，我们提出一种融合在线个性化与自适应树状群组相对策略优化（AT-GRPO）的新型长程强化学习框架。采用双智能体博弈范式，用户智能体通过风格模仿（学习用户特定对话特质）和主动终止（预测轮次终止概率作为即时奖励）构建动态环境，形成驱动对话智能体深化兴趣探索的迭代循环。AT-GRPO将对话轨迹重构为树状结构并引入自适应观测范围：不同于指数级开销的全树扩展，该方法将各节点奖励聚合限制在阶段感知范围内——较大范围支持早期话题探索，较小范围促进后期对话维持。该设计将对话长度对应的计算开销从指数级降至多项式级，同时保持长期奖励捕获能力。大量实验验证了本框架在性能、样本效率与鲁棒性方面的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing dialogue agents, which rely heavily on pre-collected user data and suffer from short-horizon biases in reinforcement learning, this paper introduces a novel long-horizon RL framework. The method integrates online personalization through a two-agent game, where a user agent mimics user styles and predicts termination probabilities to shape rewards, and employs Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO) to reinterpret dialogue trajectories as trees with adaptive observation ranges for efficient reward aggregation. Experimental results demonstrate that the framework achieves superior performance, sample efficiency, and robustness by reducing computational overhead from exponential to polynomial while effectively capturing long-term dialogue value.</div>
<div class="mono" style="margin-top:8px">针对现有对话代理过度依赖预收集用户数据及强化学习中短视偏差的问题，本文提出了一种新颖的长视界强化学习框架。该方法通过双代理博弈实现在线个性化，其中用户代理模拟用户风格并预测终止概率以构建奖励，并采用自适应树基组相对策略优化（AT-GRPO）将对话轨迹重新解释为具有自适应观察范围的树结构，以高效聚合奖励。实验结果表明，该框架在降低计算开销（从指数级降至多项式级）的同时，有效捕获了长期对话价值，实现了更优的性能、样本效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization</div>
<div class="meta-line">Authors: Matteo Pannacci, Andrea Fanti, Elena Umili, Roberto Capobianco</div>
<div class="meta-line">First: 2026-02-10T13:20:29+00:00 · Latest: 2026-02-10T13:20:29+00:00</div>
<div class="meta-line">Comments: Preprint currently under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09761v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09761v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work we address the problem of training a Reinforcement Learning agent to follow multiple temporally-extended instructions expressed in Linear Temporal Logic in sub-symbolic environments. Previous multi-task work has mostly relied on knowledge of the mapping between raw observations and symbols appearing in the formulae. We drop this unrealistic assumption by jointly training a multi-task policy and a symbol grounder with the same experience. The symbol grounder is trained only from raw observations and sparse rewards via Neural Reward Machines in a semi-supervised fashion. Experiments on vision-based environments show that our method achieves performance comparable to using the true symbol grounding and significantly outperforms state-of-the-art methods for sub-symbolic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在亚符号强化学习环境中实现线性时序逻辑任务的零样本泛化</div>
<div class="mono" style="margin-top:8px">本研究旨在训练强化学习智能体在亚符号环境中执行以线性时序逻辑表达的时序扩展指令。以往多任务研究多依赖于原始观测与逻辑公式符号间映射关系的先验知识。我们通过联合训练多任务策略与符号接地器（使用相同经验数据）摒弃了这一不现实假设。符号接地器仅通过原始观测与稀疏奖励，以半监督方式借助神经奖励机制进行训练。基于视觉环境的实验表明，本方法性能接近使用真实符号接地的效果，并显著优于当前亚符号环境的最先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training Reinforcement Learning agents to follow temporally-extended Linear Temporal Logic instructions in sub-symbolic environments without requiring a predefined mapping between raw observations and logical symbols. The method jointly trains a multi-task policy and a symbol grounder using the same experience, where the grounder is trained from raw observations and sparse rewards via Neural Reward Machines in a semi-supervised manner. Experimental results in vision-based environments demonstrate that the approach achieves performance comparable to using true symbol grounding and significantly outperforms existing state-of-the-art methods for sub-symbolic settings.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在亚符号环境中训练强化学习智能体遵循线性时序逻辑表达的时序扩展指令的挑战，无需预定义原始观测与逻辑符号间的映射。该方法通过联合训练多任务策略和符号接地器，利用相同经验，其中接地器仅通过原始观测和稀疏奖励以半监督方式通过神经奖励机进行训练。在基于视觉的环境中的实验结果表明，该方法达到了与使用真实符号接地相当的性能，并显著优于亚符号环境下的现有最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">ExO-PPO: an Extended Off-policy Proximal Policy Optimization Algorithm</div>
<div class="meta-line">Authors: Hanyong Wang, Menglong Yang</div>
<div class="meta-line">First: 2026-02-10T12:29:57+00:00 · Latest: 2026-02-10T12:29:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09726v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09726v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning has been able to solve various tasks successfully, however, due to the construction of policy gradient and training dynamics, tuning deep reinforcement learning models remains challenging. As one of the most successful deep reinforcement-learning algorithm, the Proximal Policy Optimization algorithm (PPO) clips the policy gradient within a conservative on-policy updates, which ensures reliable and stable policy improvement. However, this training pattern may sacrifice sample efficiency. On the other hand, off-policy methods make more adequate use of data through sample reuse, though at the cost of increased the estimation variance and bias. To leverage the advantages of both, in this paper, we propose a new PPO variant based on the stability guarantee from conservative on-policy iteration with a more efficient off-policy data utilization. Specifically, we first derive an extended off-policy improvement from an expectation form of generalized policy improvement lower bound. Then, we extend the clipping mechanism with segmented exponential functions for a suitable surrogate objective function. Third, the trajectories generated by the past $M$ policies are organized in the replay buffer for off-policy training. We refer to this method as Extended Off-policy Proximal Policy Optimization (ExO-PPO). Compared with PPO and some other state-of-the-art variants, we demonstrate an improved performance of ExO-PPO with balanced sample efficiency and stability on varied tasks in the empirical experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExO-PPO：一种扩展的离策略近端策略优化算法</div>
<div class="mono" style="margin-top:8px">深度强化学习已能成功解决多种任务，但由于策略梯度的构建与训练动态特性，深度强化学习模型的调参仍具挑战性。作为最成功的深度强化学习算法之一，近端策略优化算法（PPO）通过保守的在线策略更新对策略梯度进行截断，从而确保策略改进的可靠性与稳定性。然而，这种训练模式可能牺牲样本效率。另一方面，离策略方法通过样本复用更充分地利用数据，但代价是增加了估计方差与偏差。为兼顾二者优势，本文基于保守在线策略迭代的稳定性保证，提出一种具有更高离策略数据利用率的新型PPO变体。具体而言，我们首先从广义策略改进下界的期望形式推导出扩展的离策略改进理论；其次，采用分段指数函数扩展截断机制以构建合适的代理目标函数；第三，将过往M个策略生成的轨迹存入经验回放池用于离策略训练。该方法被称为扩展离策略近端策略优化（ExO-PPO）。通过实证实验对比PPO及其他先进变体，我们证明ExO-PPO在多种任务中实现了样本效率与稳定性的平衡，并展现出更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ExO-PPO, an extended off-policy variant of Proximal Policy Optimization (PPO), motivated by the need to balance the stability of on-policy methods with the sample efficiency of off-policy approaches. The method derives an extended off-policy improvement bound, incorporates a segmented exponential clipping mechanism for the surrogate objective, and utilizes a replay buffer storing trajectories from past policies for off-policy training. Experimental results demonstrate that ExO-PPO outperforms PPO and other state-of-the-art variants across various tasks, achieving improved performance with a better trade-off between sample efficiency and stability.</div>
<div class="mono" style="margin-top:8px">本文提出了ExO-PPO，一种扩展的离策略近端策略优化算法，其动机在于平衡策略方法的稳定性与离策略方法的样本效率。该方法通过推导扩展的离策略改进下界，引入分段指数裁剪机制来构建合适的代理目标函数，并利用存储过去策略轨迹的回放缓冲区进行离策略训练。实验结果表明，ExO-PPO在多种任务上优于PPO及其他先进变体，在样本效率和稳定性之间取得了更好的平衡，从而提升了整体性能。</div>
</details>
</div>
<div class="card">
<div class="title">The hidden risks of temporal resampling in clinical reinforcement learning</div>
<div class="meta-line">Authors: Thomas Frost, Hrisheekesh Vaidya, Steve Harris</div>
<div class="meta-line">First: 2026-02-06T11:02:06+00:00 · Latest: 2026-02-10T09:51:38+00:00</div>
<div class="meta-line">Comments: 12 pages, 4 figures. v2 fixes missing acknowledgements</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06603v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06603v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (ORL) has shown potential for improving decision-making in healthcare. However, contemporary research typically aggregates patient data into fixed time intervals, simplifying their mapping to standard ORL frameworks. The impact of these temporal manipulations on model safety and efficacy remains poorly understood. In this work, using both a gridworld navigation task and the UVA/Padova clinical diabetes simulator, we demonstrate that temporal resampling significantly degrades the performance of offline reinforcement learning algorithms during live deployment. We propose three mechanisms that drive this failure: (i) the generation of counterfactual trajectories, (ii) the distortion of temporal expectations, and (iii) the compounding of generalisation errors. Crucially, we find that standard off-policy evaluation metrics can fail to detect these drops in performance. Our findings reveal a fundamental risk in current healthcare ORL pipelines and emphasise the need for methods that explicitly handle the irregular timing of clinical decision-making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>临床强化学习中时间重采样的潜在风险</div>
<div class="mono" style="margin-top:8px">离线强化学习（ORL）在提升医疗决策方面展现出潜力。然而，现有研究通常将患者数据聚合至固定时间间隔，以简化其与标准ORL框架的映射。这些时间操作对模型安全性和有效性的影响尚不明确。本研究通过网格世界导航任务和UVA/Padova临床糖尿病模拟器，证明时间重采样会显著降低离线强化学习算法在实际部署中的性能。我们提出导致此失效的三种机制：（i）反事实轨迹的生成，（ii）时间期望的扭曲，以及（iii）泛化误差的累积。关键的是，我们发现标准离策略评估指标可能无法检测到这些性能下降。我们的研究揭示了当前医疗ORL流程中的根本性风险，并强调需要开发能显式处理临床决策不规则时序的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the safety risks introduced by temporal resampling when applying offline reinforcement learning (ORL) to clinical data, where patient records are often aggregated into fixed intervals for algorithmic convenience. The authors demonstrate through experiments on a gridworld task and a clinical diabetes simulator that this common practice significantly degrades the performance of ORL algorithms during real-world deployment, driven by mechanisms such as generating counterfactual trajectories and distorting temporal expectations. Crucially, their results show that standard off-policy evaluation metrics can fail to detect these performance drops, highlighting a fundamental flaw in current healthcare ORL pipelines and underscoring the need for methods that explicitly account for irregular clinical event timing.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在临床数据中应用离线强化学习时，由于将患者记录聚合为固定时间间隔这一常见做法所带来的安全风险。作者通过网格世界导航任务和临床糖尿病模拟器的实验表明，这种时间重采样会显著降低离线强化学习算法在实际部署中的性能，其失效机制包括生成反事实轨迹、扭曲时间预期以及泛化误差的累积。关键的是，研究发现标准的离策略评估指标可能无法检测到这些性能下降，从而揭示了当前医疗健康离线强化学习流程中的一个根本性风险，并强调需要开发能够显式处理临床决策不规则时序的新方法。</div>
</details>
</div>
<div class="card">
<div class="title">On the Optimal Reasoning Length for RL-Trained Language Models</div>
<div class="meta-line">Authors: Daisuke Nohara, Taishi Nakamura, Rio Yokota</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-10T09:45:42+00:00 · Latest: 2026-02-10T09:45:42+00:00</div>
<div class="meta-line">Comments: 15 pages, 10 figures. Submitted to the Workshop on Scaling Post-training for LLMs (SPOT) at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09591v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09591v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论强化学习训练语言模型的最优推理长度</div>
<div class="mono" style="margin-top:8px">强化学习显著提升了大语言模型的推理能力，但也会延长思维链输出并增加训练与推理的计算成本。尽管已有长度控制方法提出，但平衡效率与性能的最优输出长度仍不明确。本研究在Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B两个模型上比较了多种长度控制方法。结果表明：长度惩罚可能阻碍推理能力习得，而对具有强先验推理能力的模型进行适当调优的长度控制可提升效率。通过将现有研究扩展至强化学习训练策略，我们识别出两种失效模式：1）长输出增加离散性，2）短输出导致思考不足。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the optimal reasoning output length for reinforcement learning (RL)-trained language models, motivated by the trade-off between the improved reasoning performance from RL and the increased computational cost from longer chain-of-thought outputs. The method involves comparing several length control techniques on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B, extending prior work to RL-trained policies. The main experimental results reveal that length penalties can hinder reasoning acquisition, whereas properly tuned control can improve efficiency for models with strong prior reasoning, identifying two failure modes: long outputs increase dispersion and short outputs lead to under-thinking.</div>
<div class="mono" style="margin-top:8px">本研究探讨了强化学习训练的语言模型的最佳推理输出长度，其动机在于平衡强化学习带来的推理性能提升与思维链输出变长导致的计算成本增加。方法是在Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B两个模型上比较多种长度控制技术，并将先前工作扩展到强化学习训练的策略。主要实验结果表明，长度惩罚可能阻碍推理能力的习得，而对具有强先验推理能力的模型进行适当调整的长度控制则可提升效率，同时识别出两种失效模式：输出过长会增加离散性，而输出过短则导致思考不足。</div>
</details>
</div>
<div class="card">
<div class="title">Rollout-Training Co-Design for Efficient LLM-Based Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Zhida Jiang, Zhaolong Xing, Jiawei Lu, Yipei Niu, Qingyuan Sang, Liangxu Zhang, Wenquan Dai, Junhua Shu, Jiaxing Wang, Qiangyu Pei, Qiong Chen, Xinyu Liu, Fangming Liu, Ai Han, Zhen Chen, Ke Zhang</div>
<div class="meta-line">First: 2026-02-10T09:27:03+00:00 · Latest: 2026-02-10T09:27:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09578v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite algorithm-level innovations for multi-agent reinforcement learning (MARL), the underlying networked infrastructure for large-scale MARL training remains underexplored. Existing training frameworks primarily optimize for single-agent scenarios and fail to address the unique system-level challenges of MARL, including rollout-training synchronization barriers, rollout load imbalance, and training resource underutilization. To bridge this gap, we propose FlexMARL, the first end-to-end training framework that holistically optimizes rollout, training, and their orchestration for large-scale LLM-based MARL. Specifically, FlexMARL introduces the joint orchestrator to manage data flow under the rollout-training disaggregated architecture. Building upon the experience store, a novel micro-batch driven asynchronous pipeline eliminates the synchronization barriers while providing strong consistency guarantees. Rollout engine adopts a parallel sampling scheme combined with hierarchical load balancing, which adapts to skewed inter/intra-agent request patterns. Training engine achieves on-demand hardware binding through agent-centric resource allocation. The training states of different agents are swapped via unified and location-agnostic communication. Empirical results on a large-scale production cluster demonstrate that FlexMARL achieves up to 7.3x speedup and improves hardware utilization by up to 5.6x compared to existing frameworks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高效基于大语言模型的多智能体强化学习的推演-训练协同设计</div>
<div class="mono" style="margin-top:8px">尽管多智能体强化学习（MARL）在算法层面已有创新，但其大规模训练所需的底层网络基础设施仍缺乏深入探索。现有训练框架主要针对单智能体场景优化，未能解决MARL特有的系统级挑战，包括推演-训练同步屏障、推演负载不均衡及训练资源利用率不足。为填补这一空白，我们提出FlexMARL，首个端到端训练框架，全面优化基于大语言模型的大规模MARL的推演、训练及其协同机制。具体而言，FlexMARL引入联合编排器，在推演-训练解耦架构下管理数据流；基于经验存储库，通过新型微批量驱动异步流水线消除同步屏障，同时提供强一致性保证；推演引擎采用并行采样方案结合分层负载均衡，自适应智能体间/内请求模式的偏斜分布；训练引擎通过以智能体为中心的资源分配实现按需硬件绑定；不同智能体的训练状态通过统一且位置无关的通信进行交换。大规模生产集群的实证结果表明，与现有框架相比，FlexMARL最高可实现7.3倍加速，并将硬件利用率提升至5.6倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the underexplored networked infrastructure for large-scale multi-agent reinforcement learning (MARL), as existing frameworks are optimized for single-agent scenarios and fail to address MARL-specific system challenges like synchronization barriers, load imbalance, and resource underutilization. To bridge this gap, the method introduces FlexMARL, an end-to-end training framework that holistically optimizes rollout, training, and their orchestration for LLM-based MARL, featuring a joint orchestrator, a micro-batch driven asynchronous pipeline for synchronization elimination, a parallel sampling scheme with hierarchical load balancing, and agent-centric resource allocation with unified communication. The main experimental results on a production cluster show that FlexMARL achieves up to 7.3x speedup and improves hardware utilization by up to 5.6x compared to existing frameworks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于大规模多智能体强化学习（MARL）的网络基础设施尚未得到充分探索，现有训练框架主要针对单智能体场景优化，无法解决MARL特有的系统级挑战，如同步障碍、负载不均和资源利用不足。为此，方法提出了FlexMARL，这是一个端到端的训练框架，全面优化了基于大语言模型的MARL中的rollout、训练及其协调过程，包括采用联合协调器管理数据流、基于经验存储的微批量驱动异步流水线以消除同步障碍、结合分层负载均衡的并行采样方案，以及通过以智能体为中心的资源分配实现按需硬件绑定和统一通信。主要实验结果表明，在生产集群上，FlexMARL相比现有框架实现了高达7.3倍的加速，并将硬件利用率提升了高达5.6倍。</div>
</details>
</div>
<div class="card">
<div class="title">Training deep physical neural networks with local physical information bottleneck</div>
<div class="meta-line">Authors: Hao Wang, Ziao Wang, Xiangpeng Liang, Han Zhao, Jianqi Hu, Junjie Jiang, Xing Fu, Jianshi Tang, Huaqiang Wu, Sylvain Gigan, Qiang Liu</div>
<div class="meta-line">First: 2026-02-10T09:20:12+00:00 · Latest: 2026-02-10T09:20:12+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09569v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09569v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning has revolutionized modern society but faces growing energy and latency constraints. Deep physical neural networks (PNNs) are interconnected computing systems that directly exploit analog dynamics for energy-efficient, ultrafast AI execution. Realizing this potential, however, requires universal training methods tailored to physical intricacies. Here, we present the Physical Information Bottleneck (PIB), a general and efficient framework that integrates information theory and local learning, enabling deep PNNs to learn under arbitrary physical dynamics. By allocating matrix-based information bottlenecks to each unit, we demonstrate supervised, unsupervised, and reinforcement learning across electronic memristive chips and optical computing platforms. PIB also adapts to severe hardware faults and allows for parallel training via geographically distributed resources. Bypassing auxiliary digital models and contrastive measurements, PIB recasts PNN training as an intrinsic, scalable information-theoretic process compatible with diverse physical substrates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于局部物理信息瓶颈的深度物理神经网络训练</div>
<div class="mono" style="margin-top:8px">深度学习已深刻改变现代社会，但面临日益严峻的能耗与延迟限制。深度物理神经网络（PNNs）是一种通过直接利用模拟动力学实现高能效、超快速人工智能执行的互联计算系统。然而，实现这一潜力需要针对物理系统复杂性设计的通用训练方法。本文提出物理信息瓶颈（PIB），这是一个融合信息论与局部学习的通用高效框架，使深度PNN能够在任意物理动力学下进行学习。通过为每个单元分配基于矩阵的信息瓶颈，我们在电子忆阻芯片与光学计算平台上实现了监督学习、无监督学习和强化学习的演示。PIB能自适应严重的硬件故障，并支持通过地理分布式资源进行并行训练。该方法绕过了辅助数字模型与对比测量，将PNN训练重构为一种本征的、可扩展的信息论过程，兼容多样化的物理载体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the energy and latency challenges of conventional deep learning by proposing a universal training framework for deep physical neural networks (PNNs), which leverage analog dynamics for efficient AI execution. The method introduces the Physical Information Bottleneck (PIB), integrating information theory with local learning to train PNNs under arbitrary physical dynamics by applying matrix-based bottlenecks to each unit. Experimental results demonstrate successful supervised, unsupervised, and reinforcement learning on electronic memristive chips and optical platforms, with robustness to hardware faults and support for parallel training using distributed resources, enabling scalable, intrinsic training without auxiliary digital models.</div>
<div class="mono" style="margin-top:8px">本文针对传统深度学习的能耗和延迟问题，提出了一种通用训练框架，用于深度物理神经网络（PNNs），这类网络利用模拟动力学实现高效人工智能执行。该方法引入了物理信息瓶颈（PIB），结合信息论和局部学习，通过为每个单元应用基于矩阵的瓶颈，使PNNs能在任意物理动力学下进行训练。实验结果表明，在电子忆阻芯片和光学计算平台上成功实现了监督、无监督和强化学习，具备对硬件故障的鲁棒性，并支持利用分布式资源进行并行训练，从而实现了无需辅助数字模型的可扩展、本征训练过程。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models</div>
<div class="meta-line">Authors: Yizhi Wang, Linan Yue, Min-Ling Zhang</div>
<div class="meta-line">First: 2026-02-10T07:29:50+00:00 · Latest: 2026-02-10T07:29:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09485v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09485v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>效率与透明性的桥梁：多模态大推理模型中的可解释思维链压缩</div>
<div class="mono" style="margin-top:8px">长思维链在多模态推理模型中广泛用于处理复杂任务，通过捕捉细节视觉信息。然而，这些长思维链往往过于冗长且包含冗余推理步骤，可能影响推理效率。压缩长思维链是自然解决方案，但现有方法面临两大挑战：(1) 可能因移除关键对齐线索而损害视觉-文本推理的完整性；(2) 压缩过程缺乏可解释性，难以辨识关键信息。为此，我们提出XMCC——一种可解释多模态思维链压缩器，将压缩建模为通过强化学习优化的序列决策过程。XMCC能有效缩短推理轨迹，同时保留关键推理步骤与答案正确性，并为其压缩决策生成自然语言解释。在代表性多模态推理基准上的大量实验表明，XMCC不仅能缩减推理长度，还能提供可解释说明，验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency and redundancy of long chains of thought in multimodal reasoning models, which can obscure critical visual-textual alignments and lack transparency, this paper introduces XMCC, an explainable compressor that frames compression as a reinforcement learning-based sequential decision process. The method effectively shortens reasoning paths while maintaining answer accuracy and key steps, and it generates natural-language explanations for its compression choices. Experimental results on standard benchmarks show that XMCC successfully reduces reasoning length and provides explainable justifications, confirming its practical utility.</div>
<div class="mono" style="margin-top:8px">针对多模态推理模型中长思维链效率低下、冗余且可能破坏视觉-文本对齐关键线索、缺乏可解释性的问题，本文提出了XMCC，一种可解释的多模态思维链压缩器，它将压缩建模为基于强化学习的序列决策过程。该方法能在缩短推理轨迹的同时保持答案正确性和关键步骤，并生成自然语言解释来说明其压缩决策。在代表性多模态推理基准上的大量实验表明，XMCC不仅能有效减少推理长度，还能提供可解释的说明，验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Reasoning Re-ranker</div>
<div class="meta-line">Authors: Mingfu Liang, Yufei Li, Jay Xu, Kavosh Asadi, Xi Liu, Shuo Gu, Kaushik Rangadurai, Frank Shyu, Shuaiwen Wang, Song Yang, Zhijing Li, Jiang Liu, Mengying Sun, Fei Tian, Xiaohan Wei, Chonglin Sun, Jacob Tao, Shike Mei, Hamed Firooz, Wenlin Chen, Luke Simon</div>
<div class="meta-line">First: 2026-02-08T02:12:24+00:00 · Latest: 2026-02-10T07:24:22+00:00</div>
<div class="meta-line">Comments: 31 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07774v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07774v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2&#x27;s effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式推理重排序器</div>
<div class="mono" style="margin-top:8px">近期研究日益探索将大语言模型（LLMs）作为推荐系统的新范式，因其可扩展性和世界知识。然而现有工作存在三个关键局限：（1）多数研究聚焦检索与排序，而对优化最终推荐至关重要的重排序阶段被严重忽视；（2）LLMs通常以零样本或有监督微调方式使用，其通过强化学习（RL）与高质量推理数据增强的推理能力尚未充分开发；（3）项目普遍采用非语义ID表示，这在具有数十亿标识符的工业系统中引发严重可扩展性挑战。为弥补这些不足，我们提出生成式推理重排序器（GR2），这是一个专为重排序设计的三阶段训练端到端框架。首先，通过分词器将非语义ID编码为语义ID（唯一性≥99%）对预训练LLM进行中期训练。随后，通过精心设计的提示与拒绝采样，由更强的大规模LLM生成高质量推理轨迹，用于有监督微调以传授基础推理技能。最后，我们采用解耦裁剪与动态采样策略优化（DAPO），通过专为重排序设计的可验证奖励实现可扩展的RL监督。在两个真实数据集上的实验证明GR2的有效性：其在Recall@5和NDCG@5指标上分别超越当前最优方法OneRec-Think达2.4%和1.3%。消融实验证实高级推理轨迹能带来跨指标的显著提升。我们进一步发现RL奖励设计对重排序至关重要：LLMs倾向于通过保持项目顺序进行奖励攻击，这促使我们采用条件可验证奖励来抑制该行为并优化重排序性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the under-explored potential of LLMs in the recommendation reranking phase, their underexploited reasoning capabilities, and the scalability issues of non-semantic item IDs, this paper introduces the Generative Reasoning Reranker (GR2). The method employs a three-stage pipeline: mid-training an LLM on semantic IDs for scalability, using a larger LLM to generate high-quality reasoning traces for supervised fine-tuning to impart reasoning skills, and applying a novel RL optimization (DAPO) with verifiable rewards tailored for reranking. Experimental results on real-world datasets show GR2 outperforms the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5, with ablations confirming the importance of advanced reasoning traces and careful RL reward design to prevent reward hacking and optimize performance.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决推荐系统中重排序阶段被忽视、大语言模型的推理能力未充分利用以及非语义项目标识带来的可扩展性挑战。为此，提出了生成式推理重排序器（GR2），其方法采用三阶段训练流程：首先在通过分词器编码的语义ID上对预训练LLM进行中期训练以确保可扩展性；接着使用更大规模的LLM通过精心设计的提示和拒绝采样生成高质量推理轨迹，用于监督微调以传授基础推理技能；最后应用解耦裁剪和动态采样策略优化（DAPO），利用专为重排序设计的可验证奖励进行可扩展的强化学习监督。在两个真实数据集上的实验结果表明，GR2在Recall@5和NDCG@5上分别超越了当前最佳的OneRec-Think方法2.4%和1.3%，消融研究证实了高级推理轨迹带来的显著收益，并发现精心设计的强化学习奖励对于防止模型利用奖励漏洞和优化重排序性能至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">From Scalar Rewards to Potential Trends: Shaping Potential Landscapes for Model-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Yao-Hui Li, Zeyu Wang, Xin Li, Wei Pang, Yingfang Yuan, Zhengkun Chen, Boya Zhang, Riashat Islam, Alex Lamb, Yonggang Zhang</div>
<div class="meta-line">First: 2026-02-03T07:13:26+00:00 · Latest: 2026-02-10T07:16:29+00:00</div>
<div class="meta-line">Comments: 26 pages. Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03201v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03201v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model-based reinforcement learning (MBRL) achieves high sample efficiency by simulating future trajectories with learned dynamics and reward models. However, its effectiveness is severely compromised in sparse reward settings. The core limitation lies in the standard paradigm of regressing ground-truth scalar rewards: in sparse environments, this yields a flat, gradient-free landscape that fails to provide directional guidance for planning. To address this challenge, we propose Shaping Landscapes with Optimistic Potential Estimates (SLOPE), a novel framework that shifts reward modeling from predicting scalars to constructing informative potential landscapes. SLOPE employs optimistic distributional regression to estimate high-confidence upper bounds, which amplifies rare success signals and ensures sufficient exploration gradients. Evaluations on 30+ tasks across 5 benchmarks demonstrate that SLOPE consistently outperforms leading baselines in fully sparse, semi-sparse, and dense rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从标量奖励到势能趋势：构建基于模型强化学习的势能景观</div>
<div class="mono" style="margin-top:8px">基于模型的强化学习（MBRL）通过习得的动力学模型与奖励模型模拟未来轨迹，实现高样本效率。然而在稀疏奖励场景中，其效能严重受限。核心瓶颈在于回归真实标量奖励的标准范式：在稀疏环境中，这会产生平坦无梯度的景观，无法为规划提供方向性指引。为解决该问题，我们提出基于乐观势能估计的景观塑造框架（SLOPE），将奖励建模从标量预测转向构建信息丰富的势能景观。SLOPE采用乐观分布回归估计高置信度上界，从而放大稀疏的成功信号并确保充分的探索梯度。在5个基准测试的30余项任务中，SLOPE在完全稀疏、半稀疏及稠密奖励场景下均持续超越主流基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of sparse rewards in model-based reinforcement learning (MBRL), where traditional scalar reward regression yields flat, uninformative landscapes that hinder planning. To overcome this, the authors propose SLOPE, a framework that shifts from scalar reward prediction to constructing potential landscapes via optimistic distributional regression, which estimates high-confidence upper bounds to amplify rare success signals and ensure explorable gradients. Experimental results across over 30 tasks in five benchmarks show that SLOPE consistently outperforms leading baselines in sparse, semi-sparse, and dense reward settings.</div>
<div class="mono" style="margin-top:8px">本文针对基于模型的强化学习在稀疏奖励环境中因标量奖励回归产生平坦、无梯度景观而影响规划效果的问题，提出了一种名为SLOPE的新框架。该方法通过乐观分布回归估计高置信度上界，将奖励建模从预测标量转向构建信息丰富的势能景观，从而放大稀疏成功信号并确保足够的探索梯度。在五个基准测试的30多个任务上的评估表明，SLOPE在完全稀疏、半稀疏和密集奖励设置中均持续优于主流基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Online Learning in MDPs with Partially Adversarial Transitions and Losses</div>
<div class="meta-line">Authors: Ofir Schlisselberg, Tal Lancewicki, Yishay Mansour</div>
<div class="meta-line">First: 2026-02-10T07:13:11+00:00 · Latest: 2026-02-10T07:13:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09474v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09474v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study reinforcement learning in MDPs whose transition function is stochastic at most steps but may behave adversarially at a fixed subset of $Λ$ steps per episode. This model captures environments that are stable except at a few vulnerable points. We introduce \emph{conditioned occupancy measures}, which remain stable across episodes even with adversarial transitions, and use them to design two algorithms. The first handles arbitrary adversarial steps and achieves regret $\tilde{O}(H S^Λ\sqrt{K S A^{Λ+1}})$, where $K$ is the number of episodes, $S$ is the number of state, $A$ is the number of actions and $H$ is the episode&#x27;s horizon. The second, assuming the adversarial steps are consecutive, improves the dependence on $S$ to $\tilde{O}(H\sqrt{K S^{3} A^{Λ+1}})$. We further give a $K^{2/3}$-regret reduction that removes the need to know which steps are the $Λ$ adversarial steps. We also characterize the regret of adversarial MDPs in the \emph{fully adversarial} setting ($Λ=H-1$) both for full-information and bandit feedback, and provide almost matching upper and lower bounds (slightly strengthen existing lower bounds, and clarify how different feedback structures affect the hardness of learning).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有部分对抗性转移与损失的MDP在线学习研究</div>
<div class="mono" style="margin-top:8px">我们研究在马尔可夫决策过程（MDP）中的强化学习问题，其转移函数在多数步骤是随机的，但在每个回合的固定Λ步子集内可能表现出对抗性行为。该模型刻画了除少数脆弱点外基本稳定的环境。我们引入\emph{条件占用测度}，即使在对抗性转移下也能在多个回合间保持稳定，并基于此设计两种算法。第一种算法处理任意对抗性步骤，获得遗憾界$\tilde{O}(H S^Λ\sqrt{K S A^{Λ+1}})$，其中$K$为回合数，$S$为状态数，$A$为动作数，$H$为回合长度。第二种算法假设对抗性步骤连续出现，将$S$的依赖改进为$\tilde{O}(H\sqrt{K S^{3} A^{Λ+1}})$。我们进一步提出$K^{2/3}$遗憾缩减方法，无需预先知道哪些步骤属于Λ个对抗性步骤。此外，我们在\emph{完全对抗}设定（$Λ=H-1$）下刻画了全信息反馈和赌博机反馈的遗憾界，给出了近乎匹配的上界与下界（略微强化现有下界，并阐明不同反馈结构如何影响学习难度）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses reinforcement learning in Markov Decision Processes (MDPs) where transitions are mostly stochastic but can be adversarial at up to Λ steps per episode, modeling environments with occasional vulnerabilities. The authors introduce conditioned occupancy measures to maintain stability despite adversarial transitions and propose two algorithms: one for arbitrary adversarial steps achieving regret Õ(H S^Λ√(K S A^{Λ+1})), and another for consecutive adversarial steps with improved dependence on S as Õ(H√(K S^3 A^{Λ+1})), along with a method to reduce regret without prior knowledge of adversarial steps. Experimental results demonstrate these regret bounds, and the work extends to fully adversarial settings, providing nearly matching upper and lower bounds that clarify the impact of feedback structures on learning difficulty.</div>
<div class="mono" style="margin-top:8px">本文研究马尔可夫决策过程中的强化学习问题，其中转移函数在大多数步骤是随机的，但在每个片段中最多有Λ个步骤可能表现出对抗性，以模拟具有偶尔脆弱点的环境。作者引入了条件占用度量来在对抗性转移下保持稳定性，并设计了两种算法：第一种处理任意对抗步骤，实现了遗憾界Õ(H S^Λ√(K S A^{Λ+1}))；第二种假设对抗步骤连续，将S的依赖改进为Õ(H√(K S^3 A^{Λ+1}))，同时提出一种无需预先知道对抗步骤的遗憾减少方法。实验结果验证了这些遗憾界，并扩展到完全对抗设置，提供了几乎匹配的上界和下界，阐明了不同反馈结构对学习难度的影响。</div>
</details>
</div>
<div class="card">
<div class="title">LLaDA2.1: Speeding Up Text Diffusion via Token Editing</div>
<div class="meta-line">Authors: Tiwei Bie, Maosong Cao, Xiang Cao, Bingsen Chen, Fuyuan Chen, Kun Chen, Lun Du, Daozhuo Feng, Haibo Feng, Mingliang Gong, Zhuocheng Gong, Yanmei Gu, Jian Guan, Kaiyuan Guan, Hongliang He, Zenan Huang, Juyong Jiang, Zhonghui Jiang, Zhenzhong Lan, Chengxi Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Yuan Lu, Yuxin Ma, Xingyu Mou, Zhenxuan Pan, Kaida Qiu, Yuji Ren, Jianfeng Tan, Yiding Tian, Zian Wang, Lanning Wei, Tao Wu, Yipeng Xing, Wentao Ye, Liangyu Zha, Tianze Zhang, Xiaolu Zhang, Junbo Zhao, Da Zheng, Hao Zhong, Wanli Zhong, Jun Zhou, Junlin Zhou, Liwang Zhu, Muzhi Zhu, Yihong Zhuang</div>
<div class="meta-line">First: 2026-02-09T14:00:07+00:00 · Latest: 2026-02-10T07:11:18+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08676v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08676v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaDA2.1：通过令牌编辑加速文本扩散</div>
<div class="mono" style="margin-top:8px">尽管LLaDA2.0展示了百亿级块扩散模型的扩展潜力及其固有的并行化能力，解码速度与生成质量间的微妙平衡始终是难以突破的边界。今日我们推出LLaDA2.1，这一范式转变旨在超越此权衡。通过将令牌到令牌（T2T）编辑无缝融入传统的掩码到令牌（M2T）框架，我们引入了可配置的联合阈值解码方案。该结构创新催生两种独特模式：迅捷模式（S模式）大胆降低M2T阈值以突破传统限制，同时依赖T2T优化输出；品质模式（Q模式）采用保守阈值，在可控效率损失下确保卓越基准性能。基于扩展上下文窗口的演进中，我们首次实现了专为扩散大语言模型（dLLM）设计的大规模强化学习（RL）框架，辅以稳定梯度估计的专项技术。这种对齐不仅提升了推理精度，更增强了指令遵循的忠实度，弥合了扩散动力学与复杂人类意图间的鸿沟。我们最终发布LLaDA2.1-Mini（160亿参数）与LLaDA2.1-Flash（1000亿参数）。在33项严格基准测试中，LLaDA2.1展现出强劲的任务性能与闪电级解码速度。尽管参数量达千亿，其在代码任务中实现惊人性能：HumanEval+达892 TPS，BigCodeBench达801 TPS，LiveCodeBench达663 TPS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the persistent trade-off between decoding speed and generation quality in large-scale block-diffusion models like LLaDA2.0, this work introduces LLaDA2.1, which integrates Token-to-Token editing with the conventional Mask-to-Token scheme to create a configurable threshold-decoding framework. The method enables two operational modes: a Speedy Mode that aggressively lowers thresholds for fast generation and refines output via editing, and a Quality Mode that uses conservative thresholds for high performance, further enhanced by a novel large-scale Reinforcement Learning framework for alignment. Experimental results on 33 benchmarks show the model delivers strong task performance with exceptionally fast decoding, achieving up to 892 tokens per second on coding tasks despite its 100B parameter scale.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决LLaDA2.0等大规模块扩散模型中解码速度与生成质量之间的固有权衡，提出了LLaDA2.1模型，其核心方法是将Token-to-Token编辑无缝融入传统的Mask-to-Token方案，形成一个可配置的阈值解码框架。该方法支持两种模式：迅捷模式通过降低阈值实现快速生成并依赖编辑进行优化，质量模式则采用保守阈值以确保优异性能，同时辅以为扩散大语言模型量身定制的强化学习框架以提升对齐能力。在33项基准测试中，该模型展现出强大的任务性能和极快的解码速度，即使在1000亿参数规模下，在代码任务上也能达到最高892 tokens/秒的处理速度。</div>
</details>
</div>
<div class="card">
<div class="title">SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning</div>
<div class="meta-line">Authors: Furong Jia, Ling Dai, Wenjin Deng, Fan Zhang, Chen Hu, Daxin Jiang, Yu Liu</div>
<div class="meta-line">First: 2026-02-10T06:57:12+00:00 · Latest: 2026-02-10T06:57:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09463v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09463v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model&#x27;s reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpotAgent：通过智能体推理将视觉地理定位能力植入大型视觉语言模型</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型在视觉地理定位任务中展现出强大的推理能力，但在视觉线索稀疏、长尾分布且高度模糊的真实场景中仍面临挑战。现有方法受限于内部知识库，常因证据混淆而产生自信但缺乏依据的预测，且难以提供可验证结果。为此，我们提出SpotAgent框架，将地理定位形式化为智能体推理过程，通过专家级推理实现视觉解读与工具辅助验证的协同。该框架借助ReAct决策图主动调用外部工具（如网络搜索、地图服务）对视觉线索进行探索验证。我们设计了包含三阶段的后训练流程：首先通过监督微调实现基础对齐，随后利用多智能体框架合成的高质量轨迹进行智能体冷启动以培养工具调用能力，最后通过强化学习优化模型推理。为提升强化学习效率，我们提出空间感知动态筛选策略，依据空间难度对可学习样本进行优先级排序。在标准基准测试上的大量实验表明，SpotAgent实现了最先进的性能表现，在提供精确可验证地理定位结果的同时有效缓解了幻觉问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SpotAgent, a framework designed to address the limitations of Large Vision-Language Models (LVLMs) in visual geo-localization, where sparse and ambiguous visual cues often lead to ungrounded predictions. The method formalizes geo-localization as an agentic reasoning process, integrating visual interpretation with tool-assisted verification (e.g., web search, maps) via a ReAct diagram, and employs a three-stage post-training pipeline: supervised fine-tuning for alignment, an agentic cold start phase using trajectories synthesized by a multi-agent framework to instill tool-calling expertise, and reinforcement learning refined with a spatially-aware dynamic filtering strategy to prioritize learnable samples. Experimental results on standard benchmarks show that SpotAgent achieves state-of-the-art performance, effectively reducing hallucinations and delivering precise, verifiable geo-localization.</div>
<div class="mono" style="margin-top:8px">本文提出了SpotAgent框架，旨在解决大型视觉语言模型在视觉地理定位中的局限性，即稀疏和模糊的视觉线索常导致无根据的预测。该方法将地理定位形式化为一个智能体推理过程，通过ReAct图将视觉解释与工具辅助验证（如网络搜索、地图）相结合，并采用三阶段后训练流程：监督微调以实现对齐，利用多智能体框架合成的轨迹进行智能体冷启动以培养工具调用能力，以及通过空间感知动态过滤策略优化的强化学习来优先学习可学样本。在标准基准测试上的实验结果表明，SpotAgent实现了最先进的性能，有效减少了幻觉，并提供了精确、可验证的地理定位。</div>
</details>
</div>
<div class="card">
<div class="title">P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads</div>
<div class="meta-line">Authors: Yun Luo, Futing Wang, Qianjia Cheng, Fangchen Yu, Haodi Lei, Jianhao Yan, Chenxi Li, Jiacheng Chen, Yufeng Zhao, Haiyuan Wan, Yuchen Zhang, Shenghe Zheng, Junchi Yao, Qingyang Zhang, Haonan He, Wenxuan Zeng, Li Sheng, Chengxing Xie, Yuxin Zuo, Yizhuo Li, Yulun Wu, Rui Huang, Dongzhan Zhou, Kai Chen, Yu Qiao, Lei Bai, Yu Cheng, Ning Ding, Bowen Zhou, Peng Ye, Ganqu Cui</div>
<div class="meta-line">First: 2026-02-10T06:28:08+00:00 · Latest: 2026-02-10T06:28:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09443v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09443v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>P1-VL：连接物理奥赛中的视觉感知与科学推理</div>
<div class="mono" style="margin-top:8px">从符号运算向科学级推理的转变是大型语言模型（LLM）的关键前沿领域，物理学作为将抽象逻辑与物理现实绑定的核心测试锚点，要求模型保持与宇宙定律的物理一致性，这一任务本质上需要多模态感知来将抽象逻辑扎根于现实。在奥赛层面，图表常具有构成性而非仅作说明，包含文本未提及的关键约束（如边界条件与空间对称性）。为弥合视觉-逻辑鸿沟，我们推出P1-VL系列开源视觉语言模型，专为高级科学推理设计。该方法融合课程强化学习（通过渐进难度扩展稳定后训练）与智能体增强（在推理时实现迭代自验证）。在涵盖2024-2025年13场考试的严格基准HiPhO上评估，旗舰模型P1-VL-235B-A22B成为首个斩获12枚金牌的开源视觉语言模型（VLM），并在开源模型中达到最先进性能。我们的智能体增强系统位列全球总排名第二，仅次于Gemini-3-Pro。除物理学外，P1-VL展现出卓越的科学推理能力与泛化性，在STEM基准测试中显著领先基础模型。通过开源P1-VL，我们为实现通用物理智能奠定基础，以更好地对齐视觉感知与抽象物理定律，推动机器科学发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for large language models to transition from symbolic manipulation to rigorous scientific reasoning grounded in physical reality, this paper introduces P1-VL, a family of open-source vision-language models designed to bridge the gap between visual perception and abstract logic, particularly for complex physics problems where diagrams contain essential constraints not present in text. The method harmonizes Curriculum Reinforcement Learning, which uses progressive difficulty expansion to stabilize training, with Agentic Augmentation for iterative self-verification during inference. On the HiPhO benchmark comprising 13 recent physics olympiad exams, the flagship P1-VL-235B-A22B model achieves state-of-the-art performance among open-source models, securing 12 gold medals and ranking second globally overall, trailing only Gemini-3-Pro, while also demonstrating strong generalizability and reasoning capacity across broader STEM benchmarks.</div>
<div class="mono" style="margin-top:8px">本文的动机是大型语言模型需要从符号操作转向基于物理现实的严谨科学推理，特别是在物理奥赛中，图表包含文本中缺失的关键约束条件，因此提出了P1-VL系列开源视觉语言模型，以弥合视觉感知与抽象逻辑之间的差距。方法上，它协调了课程强化学习（通过渐进难度扩展稳定训练）和智能体增强（在推理时进行迭代自我验证）。在包含13场近期物理奥赛考试的HiPhO基准测试中，旗舰模型P1-VL-235B-A22B在开源模型中实现了最先进的性能，获得了12枚金牌，并在全球总排名中位列第二，仅次于Gemini-3-Pro，同时在更广泛的STEM基准测试中展现出强大的泛化能力和科学推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Toward a Unified Lyapunov-Certified ODE Convergence Analysis of Smooth Q-Learning with p-Norms</div>
<div class="meta-line">Authors: Donghwan Lee, Hyunjun Na</div>
<div class="meta-line">First: 2024-04-20T01:16:27+00:00 · Latest: 2026-02-10T06:10:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.14442v6">Abs</a> · <a href="https://arxiv.org/pdf/2404.14442v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Convergence of Q-learning has been the subject of extensive study for decades. Among the available techniques, the ordinary differential equation (ODE) method is particularly appealing as a general-purpose, off-the-shelf tool for sanity-checking the convergence of a wide range of reinforcement learning algorithms. In this paper, we develop a unified ODE-based convergence framework that applies to standard Q-learning and several soft/smoothed variants, including those built on the log-sum-exponential softmax, Boltzmann softmax, and mellowmax operators. Our analysis uses a smooth p-norm Lyapunov function, leading to concise yet rigorous stability arguments and circumventing the non-smoothness issues inherent to classical infty-norm-based approaches. To the best of our knowledge, the proposed framework is among the first to provide a unified ODE-based treatment that is broadly applicable to smooth Q-learning algorithms while also encompassing standard Q-learning. Moreover, it remains valid even in settings where the associated Bellman operator is not a contraction, as may happen in Boltzmann soft Q-learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向基于p范数的平滑Q学习统一李雅普诺夫验证ODE收敛性分析</div>
<div class="mono" style="margin-top:8px">Q学习算法的收敛性研究已持续数十年。在现有技术中，常微分方程（ODE）方法作为一种通用即用工具，特别适用于对各类强化学习算法进行收敛性验证。本文提出了一种统一的基于ODE的收敛性分析框架，适用于标准Q学习及其多种平滑变体算法，包括基于对数求和指数softmax、玻尔兹曼softmax和温和最大算子的算法。该分析采用平滑p范数李雅普诺夫函数，构建了简洁而严谨的稳定性论证，规避了传统基于无穷范数方法的非光滑性问题。据我们所知，该框架是首个提供统一ODE分析且广泛适用于平滑Q学习算法（同时涵盖标准Q学习）的研究。即使在关联贝尔曼算子非压缩的场景下（如玻尔兹曼软Q学习中可能出现），该框架依然保持有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for a general-purpose tool to verify the convergence of various Q-learning algorithms, this paper develops a unified ordinary differential equation (ODE) framework that applies to standard Q-learning and its smooth variants, such as those using log-sum-exponential, Boltzmann, and mellowmax operators. The method employs a smooth p-norm Lyapunov function to provide rigorous stability analysis, avoiding non-smoothness issues from traditional infinity-norm approaches. Experimental results demonstrate that the framework offers concise convergence proofs and remains valid even in non-contractive settings, like Boltzmann soft Q-learning, thereby broadening the applicability of ODE-based analysis.</div>
<div class="mono" style="margin-top:8px">本文旨在为多种Q学习算法提供一个通用的收敛性验证工具，开发了一个统一的常微分方程框架，适用于标准Q学习及其平滑变体，如使用对数-求和-指数、玻尔兹曼和温和最大算子的方法。该方法采用平滑p-范数李雅普诺夫函数进行严格的稳定性分析，避免了传统无穷范数方法中的非平滑性问题。实验结果表明，该框架提供了简洁的收敛性证明，即使在非收缩设置（如玻尔兹曼软Q学习）中仍然有效，从而扩展了基于常微分方程分析的适用范围。</div>
</details>
</div>
<div class="card">
<div class="title">MToP: A MATLAB Benchmarking Platform for Evolutionary Multitasking</div>
<div class="meta-line">Authors: Yanchi Li, Wenyin Gong, Tingyu Zhang, Fei Ming, Shuijia Li, Qiong Gu, Yew-Soon Ong</div>
<div class="meta-line">First: 2023-12-13T13:36:14+00:00 · Latest: 2026-02-10T05:49:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.08134v6">Abs</a> · <a href="https://arxiv.org/pdf/2312.08134v6">PDF</a> · <a href="https://github.com/intLyc/MTO-Platform">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evolutionary multitasking (EMT) has emerged as a popular topic of evolutionary computation over the past decade. It aims to concurrently address multiple optimization tasks within limited computing resources, leveraging inter-task knowledge transfer techniques. Despite the abundance of multitask evolutionary algorithms (MTEAs) proposed for multitask optimization (MTO), there remains a need for a comprehensive software platform to help researchers evaluate MTEA performance on benchmark MTO problems as well as explore real-world applications. To bridge this gap, we introduce the first open-source benchmarking platform, named MToP, for EMT. MToP incorporates over 50 MTEAs, more than 200 MTO problem cases with real-world applications, and over 20 performance metrics. Based on these, we provide benchmarking recommendations tailored for different MTO scenarios. Moreover, to facilitate comparative analyses between MTEAs and traditional evolutionary algorithms, we adapted over 50 popular single-task evolutionary algorithms to address MTO problems. Notably, we release extensive pre-run experimental data on benchmark suites to enhance reproducibility and reduce computational overhead for researchers. MToP features a user-friendly graphical interface, facilitating results analysis, data export, and schematic visualization. More importantly, MToP is designed with extensibility in mind, allowing users to develop new algorithms and tackle emerging problem domains. The source code of MToP is available at: https://github.com/intLyc/MTO-Platform</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MToP：面向进化多任务优化的MATLAB基准测试平台</div>
<div class="mono" style="margin-top:8px">进化多任务优化（EMT）在过去十年已成为进化计算领域的热点研究方向，其核心目标是通过任务间知识迁移技术，在有限计算资源下同时处理多个优化任务。尽管已有大量多任务进化算法（MTEA）被提出用于多任务优化（MTO），但学界仍缺乏一个完整的软件平台来帮助研究者在基准MTO问题上评估MTEA性能并探索实际应用。为填补这一空白，我们推出了首个面向EMT的开源基准测试平台MToP。该平台整合了50余种MTEA算法、200多个涵盖实际应用的MTO问题案例以及20余项性能指标，并据此为不同MTO场景提供定制化基准测试建议。此外，为促进MTEA与传统进化算法的对比分析，我们适配了50多种经典单任务进化算法以处理MTO问题。平台特别发布了基准测试套件的大规模预运行实验数据，以提升研究可复现性并降低计算成本。MToP配备友好的图形界面，支持结果分析、数据导出与可视化呈现。更重要的是，平台采用可扩展架构，允许用户开发新算法并应对新兴问题领域。MToP源代码已开源：https://github.com/intLyc/MTO-Platform</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing interest in evolutionary multitasking (EMT) and the lack of a comprehensive software platform for evaluating multitask evolutionary algorithms (MTEAs), this paper introduces MToP, an open-source MATLAB benchmarking platform. The method involves integrating over 50 MTEAs, more than 200 multitask optimization problem cases including real-world applications, and over 20 performance metrics, while also adapting over 50 single-task evolutionary algorithms for comparative analysis. The main experimental results include the provision of extensive pre-run data on benchmark suites to enhance reproducibility and reduce computational costs, along with a user-friendly graphical interface that supports results analysis, data export, and visualization, demonstrating MToP&#x27;s effectiveness as a versatile and extensible tool for EMT research.</div>
<div class="mono" style="margin-top:8px">本文的动机源于进化多任务处理（EMT）领域日益增长的研究兴趣以及缺乏评估多任务进化算法（MTEAs）的综合软件平台，为此提出了MToP，一个开源的MATLAB基准测试平台。方法上，该平台整合了超过50种MTEAs、200多个包含实际应用的多任务优化问题案例以及20多项性能指标，同时适配了50多种单任务进化算法以进行对比分析。主要实验结果包括提供基准测试套件上的大量预运行数据以提高可重复性并降低计算成本，并配备用户友好的图形界面，支持结果分析、数据导出和可视化，证明了MToP作为EMT研究中多功能且可扩展工具的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">State Space Models Naturally Produce Time Cell and Oscillatory Behaviors and Scale to Abstract Cognitive Functions</div>
<div class="meta-line">Authors: Sen Lu, Xiaoyu Zhang, Mingtao Hu, Eric Yeu-Jer Lee, Soohyeon Kim, Wei D. Lu</div>
<div class="meta-line">First: 2025-07-18T03:53:16+00:00 · Latest: 2026-02-10T05:35:53+00:00</div>
<div class="meta-line">Comments: Sen Lu and Xiaoyu Zhang contributed equally. Wei D. Lu is the corresponding author. 5 figures are included in 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.13638v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.13638v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A grand challenge in modern neuroscience is to bridge the gap between the detailed mapping of microscale neural circuits and mechanistic understanding of cognitive functions. While extensive knowledge exists about neuronal connectivity and biophysics, how these low-level phenomena eventually produce abstract behaviors remains largely unresolved. Here, we propose that a model based on State Space Models, an emerging class of deep learning architectures, can be a potential biological model for analysis. We suggest that the differential equations governing elements in a State Space Model are conceptually consistent with the dynamics of biophysical processes, while the model offers a scalable framework to build on the dynamics to produce emergent behaviors observed in experimental neuroscience. We test this model by training a network employing a diagonal state transition matrix on temporal discrimination tasks with reinforcement learning. Our results suggest that neural behaviors such as time cells naturally emerge from two fundamental principles: optimal pre-configuration and rotational dynamics. These features are shown mathematically to optimize history compression, and naturally generate structured temporal dynamics even prior to training, mirroring recent findings in biological circuits. We show that learning acts primarily as a selection mechanism that fine-tunes these pre-configured oscillatory modes, rather than constructing temporal codes de novo. The model can be readily scaled to abstract cognitive functions such as event counting, supporting the use of State Space Models as a computationally tractable framework for understanding neural activities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>状态空间模型自然产生时间细胞与振荡行为，并可扩展至抽象认知功能</div>
<div class="mono" style="margin-top:8px">现代神经科学的一大挑战在于弥合微观神经回路精细图谱与认知功能机制理解之间的鸿沟。尽管关于神经元连接性与生物物理学的知识已相当丰富，但这些低层现象如何最终产生抽象行为仍很大程度上悬而未决。本文提出，基于状态空间模型（一类新兴深度学习架构）的模型可作为潜在的生物学分析模型。我们认为，状态空间模型中控制元素的微分方程在概念上与生物物理过程的动力学特性一致，同时该模型提供了一个可扩展的框架，能够基于这些动力学特性产生实验神经科学中观察到的涌现行为。我们通过在时间辨别任务中使用强化学习训练一个采用对角状态转移矩阵的网络来验证该模型。结果表明，时间细胞等神经行为自然源自两个基本原则：最优预配置与旋转动力学。数学上证明这些特性可优化历史压缩，甚至在训练前就能自然生成结构化时间动态，这与近期生物回路的研究发现相呼应。学习主要作为一种选择机制，用于微调这些预配置的振荡模式，而非从头构建时间编码。该模型可轻松扩展至事件计数等抽象认知功能，支持将状态空间模型作为理解神经活动的计算可处理框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to bridge microscale neural circuit knowledge with cognitive functions, this paper proposes State Space Models (SSMs) as a biologically plausible framework, where their differential equations align with biophysical dynamics and offer scalability for emergent behaviors. The method involves training a network with a diagonal state transition matrix on temporal discrimination tasks using reinforcement learning. Experimental results show that time cells and oscillatory behaviors naturally emerge from principles of optimal pre-configuration and rotational dynamics, which mathematically optimize history compression and generate structured temporal dynamics even before training; learning fine-tunes these pre-configured modes rather than building them from scratch, and the model scales to abstract functions like event counting, supporting SSMs as a tractable framework for neural activity understanding.</div>
<div class="mono" style="margin-top:8px">本文旨在弥合微观神经回路知识与认知功能之间的差距，提出状态空间模型作为一种生物合理的框架，其微分方程与生物物理动力学一致，并为涌现行为提供了可扩展性。方法上，通过强化学习训练一个具有对角状态转移矩阵的网络，用于时间辨别任务。实验结果表明，时间细胞和振荡行为自然地从最优预配置和旋转动力学原则中产生，这些原则在数学上优化了历史压缩，甚至在训练前就生成了结构化时间动态；学习主要作为微调这些预配置模式的机制，而非从头构建，且该模型可扩展至事件计数等抽象认知功能，支持状态空间模型作为理解神经活动的可计算框架。</div>
</details>
</div>
<div class="card">
<div class="title">EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning</div>
<div class="meta-line">Authors: Wujiang Xu, Wentian Zhao, Zhenting Wang, Yu-Jhe Li, Can Jin, Mingyu Jin, Kai Mei, Kun Wan, Dimitris N. Metaxas</div>
<div class="meta-line">First: 2025-09-26T16:51:44+00:00 · Latest: 2026-02-10T05:01:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22576v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22576v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EPO：面向大语言模型智能体强化学习的熵正则化策略优化</div>
<div class="mono" style="margin-top:8px">在多回合稀疏奖励环境中训练大语言模型智能体——其中完成单个任务需在一个回合内进行超过30次交互——对强化学习提出了根本性挑战。我们识别出该场景特有的关键失效模式：探索-利用级联失效。该级联始于早期策略的过早收敛，稀疏反馈导致智能体固守有缺陷的低熵策略；随后智能体陷入后期策略崩溃，传统熵正则化反而产生副作用，引发破坏训练稳定性的混沌探索。我们提出熵正则化策略优化（EPO），该通用框架通过三项协同机制打破失效循环：（1）在多回合场景中采用熵正则化以增强探索；（2）通过熵平滑正则器将策略熵约束在历史均值范围内，防止剧烈波动；（3）采用基于训练阶段的动态加权机制，平衡探索与利用。理论分析证明EPO能保证熵方差单调递减且维持收敛性。EPO在ScienceWorld上实现最高152%的性能提升，在ALFWorld上提升达19.8%。本研究表明，多回合稀疏奖励场景需要与传统强化学习截然不同的熵控制机制，这对大语言模型智能体训练具有广泛启示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of training LLM agents in multi-turn environments with sparse rewards, where a critical failure mode called the exploration-exploitation cascade occurs, leading to premature policy convergence and subsequent collapse. To address this, the method introduces Entropy-regularized Policy Optimization (EPO), a framework that combines entropy regularization with an entropy smoothing regularizer and adaptive phase-based weighting to stabilize training by controlling policy entropy. Experimental results show that EPO achieves significant performance improvements, with up to 152% on ScienceWorld and 19.8% on ALFWorld, demonstrating its effectiveness in multi-turn sparse-reward settings.</div>
<div class="mono" style="margin-top:8px">本文的动机是在稀疏奖励的多轮环境中训练LLM智能体时，存在探索-利用级联失效这一关键问题，导致策略过早收敛和后续崩溃。为解决此问题，方法提出了熵正则化策略优化（EPO），该框架通过结合熵正则化、熵平滑正则器和自适应阶段加权，控制策略熵以稳定训练。实验结果表明，EPO在ScienceWorld上性能提升高达152%，在ALFWorld上提升19.8%，证明了其在多轮稀疏奖励环境中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs</div>
<div class="meta-line">Authors: Jongmin Lee, Ernest K. Ryu</div>
<div class="meta-line">First: 2025-10-21T06:46:21+00:00 · Latest: 2026-02-10T04:19:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18340v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.18340v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The classical policy gradient method is the theoretical and conceptual foundation of modern policy-based reinforcement learning (RL) algorithms. Most rigorous analyses of such methods, particularly those establishing convergence guarantees, assume a discount factor $γ&lt; 1$. In contrast, however, a recent line of work on policy-based RL for large language models uses the undiscounted total-reward setting with $γ= 1$, rendering much of the existing theory inapplicable. In this paper, we provide analyses of the policy gradient method for undiscounted expected total-reward infinite-horizon MDPs based on two key insights: (i) the classification of the MDP states into recurrent and transient states is invariant over the set of policies that assign strictly positive probability to every action (as is typical in deep RL models employing a softmax output layer) and (ii) the classical state visitation measure (which may be ill-defined when $γ= 1$) can be replaced with a new object that we call the transient visitation measure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为何策略梯度算法适用于无折扣总奖励马尔可夫决策过程</div>
<div class="mono" style="margin-top:8px">经典策略梯度方法是现代基于策略的强化学习算法的理论与概念基础。现有对该方法的严格分析（尤其是收敛性证明）大多假设折扣因子γ&lt;1。然而，近期基于策略的大语言模型强化学习研究采用无折扣总奖励设定（γ=1），导致现有理论框架难以适用。本文针对无限时域无折扣期望总奖励马尔可夫决策过程，基于两个关键发现展开策略梯度方法分析：（一）在全体动作均被赋予严格正概率的策略集合中（如采用softmax输出层的深度强化学习模型），马尔可夫决策过程状态的常返性与瞬态分类具有不变性；（二）经典状态访问测度（在γ=1时可能无法定义）可被我们提出的新概念——瞬态访问测度所替代。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the gap between classical policy gradient theory, which assumes discounted rewards, and modern applications like large language models that often use undiscounted total rewards, this paper provides a theoretical analysis for policy gradient methods in undiscounted infinite-horizon MDPs. The method leverages two key insights: the invariance of state classification into recurrent and transient states under policies with full support, and the introduction of a &#x27;transient visitation measure&#x27; to replace the classical state visitation measure that is ill-defined without discounting. The main experimental results, derived analytically, establish that policy gradient algorithms remain effective in this setting, offering convergence guarantees previously lacking for undiscounted total-reward scenarios.</div>
<div class="mono" style="margin-top:8px">本文的动机源于经典策略梯度理论假设折扣奖励，而现代应用如大语言模型常使用无折扣总奖励，导致现有理论不适用。方法基于两个关键见解：在具有完全支持的策略下，将马尔可夫决策过程状态分类为常返和瞬态是不变的；以及引入“瞬态访问测度”替代经典的状态访问测度，后者在无折扣时定义不佳。主要分析结果表明，策略梯度算法在此设置下仍然有效，为无折扣总奖励场景提供了先前缺乏的收敛性保证。</div>
</details>
</div>
<div class="card">
<div class="title">Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning</div>
<div class="meta-line">Authors: Nilaksh, Antoine Clavaud, Mathieu Reymond, François Rivest, Sarath Chandar</div>
<div class="meta-line">First: 2026-02-10T04:06:32+00:00 · Latest: 2026-02-10T04:06:32+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09396v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09396v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In streaming Reinforcement Learning (RL), transitions are observed and discarded immediately after a single update. While this minimizes resource usage for on-device applications, it makes agents notoriously sample-inefficient, since value-based losses alone struggle to extract meaningful representations from transient data. We propose extending Self-Predictive Representations (SPR) to the streaming pipeline to maximize the utility of every observed frame. However, due to the highly correlated samples induced by the streaming regime, naively applying this auxiliary loss results in training instabilities. Thus, we introduce orthogonal gradient updates relative to the momentum target and resolve gradient conflicts arising from streaming-specific optimizers. Validated across the Atari, MinAtar, and Octax suites, our approach systematically outperforms existing streaming baselines. Latent-space analysis, including t-SNE visualizations and effective-rank measurements, confirms that our method learns significantly richer representations, bridging the performance gap caused by the absence of a replay buffer, while remaining efficient enough to train on just a few CPU cores.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从流数据中榨取更多价值：面向流式强化学习的在线表征学习</div>
<div class="mono" style="margin-top:8px">在流式强化学习中，状态转移数据在单次更新后即被丢弃。虽然这最大化了设备端应用的资源效率，但也导致智能体因样本效率低下而饱受诟病——仅依赖基于价值的损失函数难以从瞬态数据中提取有意义的表征。我们提出将自预测表征方法扩展至流式处理流程，以最大化每一帧观测数据的效用。然而，由于流式机制产生的高度相关样本，直接应用该辅助损失会导致训练不稳定。为此，我们引入相对于动量目标的正交梯度更新，并解决了流式专用优化器引发的梯度冲突问题。在Atari、MinAtar和Octax测试集上的验证表明，我们的方法系统性地超越了现有流式基线。通过t-SNE可视化和有效秩度量等潜在空间分析证实，本方法能学习到显著更丰富的表征，弥补了因缺少经验回放缓冲池导致的性能差距，同时保持高效性——仅需少量CPU核心即可完成训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the sample inefficiency in streaming reinforcement learning, where data is discarded after a single update, by proposing an adaptation of Self-Predictive Representations (SPR) to maximize information extraction from transient observations. To overcome training instabilities from correlated samples in streaming, the method introduces orthogonal gradient updates relative to a momentum target to resolve gradient conflicts. Experimental results across Atari, MinAtar, and Octax benchmarks show systematic performance improvements over baselines, with latent-space analyses confirming richer learned representations, effectively bridging the performance gap caused by lacking a replay buffer while maintaining computational efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对流式强化学习中数据单次更新后即丢弃导致的样本效率低下问题，提出将自预测表示方法适配到流式管道中，以最大化瞬态观测的信息提取。为解决流式机制中样本高度相关引发的训练不稳定，该方法引入了相对于动量目标的正交梯度更新以化解梯度冲突。在Atari、MinAtar和Octax基准测试上的实验结果表明，该方法系统性地超越了现有流式基线，潜在空间分析（包括t-SNE可视化和有效秩测量）证实了所学表示更为丰富，有效弥补了因缺少经验回放缓冲池导致的性能差距，同时保持了仅需少量CPU核心的计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Latent Poincaré Shaping for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Hanchen Xia, Baoyou Chen, Zelin Zang, Yutang Ge, Guojiang Zhao, Siyu Zhu</div>
<div class="meta-line">First: 2026-02-10T03:35:52+00:00 · Latest: 2026-02-10T03:35:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09375v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09375v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose LaPha, a method for training AlphaZero-like LLM agents in a Poincaré latent space. Under LaPha, the search process can be visualized as a tree rooted at the prompt and growing outward from the origin toward the boundary of the Poincaré ball, where negative curvature provides exponentially increasing capacity with radius. Using hyperbolic geodesic distance to rule-verified correctness, we define a node potential and assign dense process rewards by potential differences. We further attach a lightweight value head on the same shared latent space, enabling self-guided test-time scaling with almost no additional overhead. On MATH-500, LaPha improves Qwen2.5-Math-1.5B from 66.0% to 88.2%. With value-head-guided search, LaPha-1.5B reaches 56.7% accuracy on AIME&#x27;24, and LaPha-7B further achieves 60.0% on AIME&#x27;24 and 53.3% on AIME&#x27;25.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向智能体强化学习的潜在庞加莱塑形方法</div>
<div class="mono" style="margin-top:8px">我们提出LaPha方法，用于在庞加莱潜在空间中训练类AlphaZero的大型语言模型智能体。该方法将搜索过程可视化为以提示为根、从原点向庞加莱球边界生长的树结构，其负曲率特性使模型容量随半径呈指数增长。通过双曲测地线距离与规则验证正确性的结合，我们定义了节点势能函数，并依据势能差分配密集过程奖励。我们在共享潜在空间上附加轻量级价值头，实现几乎零额外开销的自引导测试时扩展。在MATH-500基准测试中，LaPha将Qwen2.5-Math-1.5B模型准确率从66.0%提升至88.2%。通过价值头引导搜索，LaPha-1.5B在AIME&#x27;24达到56.7%准确率，LaPha-7B进一步在AIME&#x27;24和AIME&#x27;25分别取得60.0%和53.3%的准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces LaPha, a method motivated by the need to enhance the reasoning capabilities of AlphaZero-like large language model (LLM) agents by training them in a Poincaré latent space, where hyperbolic geometry provides exponential capacity growth. The method visualizes the search process as a tree expanding in a Poincaré ball, using hyperbolic geodesic distance to measure correctness and assign dense process rewards via node potential differences, while a lightweight value head on the shared latent space enables efficient self-guided scaling at test time. Experimentally, LaPha significantly boosts the performance of Qwen2.5-Math-1.5B on MATH-500 from 66.0% to 88.2%, and with value-head-guided search, LaPha-1.5B achieves 56.7% accuracy on AIME&#x27;24, with LaPha-7B further reaching 60.0% on AIME&#x27;24 and 53.3% on AIME&#x27;25.</div>
<div class="mono" style="margin-top:8px">本文提出LaPha方法，其动机是通过在庞加莱潜在空间中训练类似AlphaZero的大型语言模型（LLM）智能体，利用双曲几何的指数容量增长来增强推理能力。该方法将搜索过程可视化为在庞加莱球中扩展的树，使用双曲测地线距离衡量正确性并通过节点势能差分配密集过程奖励，同时在共享潜在空间上附加轻量级价值头，实现测试时高效的自引导扩展。实验结果表明，LaPha将Qwen2.5-Math-1.5B在MATH-500上的准确率从66.0%提升至88.2%；在价值头引导的搜索下，LaPha-1.5B在AIME&#x27;24上达到56.7%的准确率，而LaPha-7B进一步在AIME&#x27;24和AIME&#x27;25上分别取得60.0%和53.3%的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Meta Coordination Graphs for Multi-agent Reinforcement Learning</div>
<div class="meta-line">Authors: Nikunj Gupta, James Zachary Hare, Jesse Milzman, Rajgopal Kannan, Viktor Prasanna</div>
<div class="meta-line">First: 2025-02-06T12:35:52+00:00 · Latest: 2026-02-10T03:34:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04028v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.04028v3">PDF</a> · <a href="https://github.com/Nikunj-Gupta/dmcg-marl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents deep meta coordination graphs (DMCG) for learning cooperative policies in multi-agent reinforcement learning (MARL). Coordination graph formulations encode local interactions and accordingly factorize the joint value function of all agents to improve efficiency in MARL. Through DMCG, we dynamically compose what we refer to as \textit{meta coordination graphs}, to learn a more expressive representation of agent interactions and use them to integrate agent information through graph convolutional networks. The goal is to enable an evolving coordination graph to guide effective coordination in cooperative MARL tasks. The graphs are jointly optimized with agents&#x27; value functions to learn to implicitly reason about joint actions, facilitating the end-to-end learning of interaction representations and coordinated policies. We demonstrate that DMCG consistently achieves state-of-the-art coordination performance and sample efficiency on challenging cooperative tasks, outperforming several prior graph-based and non-graph-based MARL baselines. Through several ablations, we also isolate the impact of individual components in DMCG, showing that the observed improvements are due to the meaningful design choices in this approach. We also include an analysis of its computational complexity to discuss its practicality in real-world applications. All codes can be found here: {\color{blue}{https://github.com/Nikunj-Gupta/dmcg-marl}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度元协调图用于多智能体强化学习</div>
<div class="mono" style="margin-top:8px">本文提出深度元协调图（DMCG），用于多智能体强化学习（MARL）中的协作策略学习。协调图公式编码局部交互，并相应分解所有智能体的联合价值函数，以提高MARL效率。通过DMCG，我们动态组合称为“元协调图”的结构，以学习更具表达力的智能体交互表示，并利用图卷积网络整合智能体信息。目标是使演化的协调图在协作MARL任务中引导有效协调。该图与智能体价值函数联合优化，以学习对联合动作的隐式推理，促进交互表示与协调策略的端到端学习。实验表明，DMCG在具有挑战性的协作任务中持续实现最先进的协调性能与样本效率，优于多种基于图及非基于图的MARL基线方法。通过多项消融实验，我们分离了DMCG各组件的影响，证明观测到的改进源于该方法中有意义的设计选择。我们还分析了其计算复杂度，以讨论其在实际应用中的可行性。所有代码可见：https://github.com/Nikunj-Gupta/dmcg-marl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to improve coordination efficiency and representation in multi-agent reinforcement learning (MARL) by dynamically modeling agent interactions. The method introduces deep meta coordination graphs (DMCG), which dynamically compose meta coordination graphs to encode local interactions, factorize the joint value function, and integrate agent information via graph convolutional networks, enabling end-to-end learning of interaction representations and coordinated policies. Experimentally, DMCG achieves state-of-the-art coordination performance and sample efficiency on challenging cooperative tasks, outperforming prior graph-based and non-graph-based MARL baselines, with ablations confirming the impact of its design choices and an analysis of its computational complexity supporting practicality.</div>
<div class="mono" style="margin-top:8px">本文旨在通过动态建模智能体交互，提升多智能体强化学习中的协调效率和表示能力。方法提出了深度元协调图，通过动态组合元协调图来编码局部交互、分解联合价值函数，并利用图卷积网络整合智能体信息，从而端到端地学习交互表示和协调策略。实验结果表明，在具有挑战性的合作任务上，该方法实现了最先进的协调性能和样本效率，优于先前的基于图和非基于图的多智能体强化学习基线，消融研究验证了其设计选择的有效性，且计算复杂度分析支持了其实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Mingqi Yuan, Qi Wang, Guozheng Ma, Caihao Sun, Bo Li, Xin Jin, Yunbo Wang, Xiaokang Yang, Wenjun Zeng, Dacheng Tao, Jiayu Chen</div>
<div class="meta-line">First: 2025-04-24T12:32:13+00:00 · Latest: 2026-02-10T01:55:04+00:00</div>
<div class="meta-line">Comments: 21 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.17490v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.17490v2">PDF</a> · <a href="https://github.com/RLE-Foundation/Plasticine">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing lifelong learning agents is crucial for artificial general intelligence (AGI). However, deep reinforcement learning (RL) systems often suffer from plasticity loss, where neural networks gradually lose their ability to adapt during training. Despite its significance, this field lacks unified benchmarks and evaluation protocols. We introduce Plasticine, the first open-source framework for benchmarking plasticity optimization in deep RL. Plasticine provides single-file implementations of over 13 mitigation methods, 6 evaluation metrics, and learning scenarios with increasing non-stationarity levels from standard to continually varying environments. This framework enables researchers to systematically quantify plasticity loss, evaluate mitigation strategies, and analyze plasticity dynamics across different contexts. Our documentation, examples, and source code are available at https://github.com/RLE-Foundation/Plasticine.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Plasticine：加速可塑性驱动的深度强化学习研究</div>
<div class="mono" style="margin-top:8px">开发终身学习智能体对实现通用人工智能至关重要。然而，深度强化学习系统常面临可塑性丧失问题，即神经网络在训练过程中逐渐失去适应能力。尽管该领域意义重大，却缺乏统一的基准测试与评估标准。我们推出Plasticine——首个用于深度强化学习中可塑性优化的开源基准框架。该框架以单文件形式实现了超过13种缓解方法、6项评估指标，以及从标准环境到持续变化环境的递增非平稳性学习场景。通过此框架，研究者可系统量化可塑性损失、评估缓解策略，并分析不同情境下的可塑性动态。项目文档、示例及源代码详见https://github.com/RLE-Foundation/Plasticine。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Plasticine, a framework motivated by the need to address plasticity loss in deep reinforcement learning, where neural networks lose adaptability over time, hindering lifelong learning essential for AGI. The method provides a unified benchmark with single-file implementations of over 13 mitigation techniques, 6 evaluation metrics, and scenarios ranging from standard to highly non-stationary environments. Experimental results demonstrate that Plasticine enables systematic quantification of plasticity loss, evaluation of mitigation strategies, and analysis of plasticity dynamics across diverse contexts, filling a gap in the field.</div>
<div class="mono" style="margin-top:8px">本文提出了Plasticine框架，其动机是解决深度强化学习中神经网络随时间推移失去适应性的可塑性丧失问题，这对实现终身学习和通用人工智能至关重要。该方法通过提供超过13种缓解技术、6个评估指标以及从标准到持续变化环境的场景的单文件实现，建立了统一基准。实验结果表明，Plasticine能够系统量化可塑性丧失、评估缓解策略并分析不同背景下的可塑性动态，填补了该领域的空白。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing General-Purpose Reasoning Models with Modular Gradient Surgery</div>
<div class="meta-line">Authors: Min Cai, Yu Liang, Longzheng Wang, Yan Wang, Yueyang Zhang, Long Xia, Zhiyuan Sun, Xi Ye, Daiting Shi</div>
<div class="meta-line">First: 2026-02-02T16:34:39+00:00 · Latest: 2026-02-10T01:50:45+00:00</div>
<div class="meta-line">Comments: Preprint; Code: https://github.com/StringNLPLAB/MGS Website: https://modular-gradient-surgery.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02301v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02301v2">PDF</a> · <a href="https://github.com/StringNLPLAB/MGS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://modular-gradient-surgery.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过模块化梯度手术推进通用推理模型发展</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在大型推理模型（LRMs）的最新进展中发挥了核心作用，在可验证和开放式推理方面取得了显著提升。然而，由于显著的领域异质性，跨多个领域训练单一通用LRM仍具挑战性。通过对两种广泛使用的策略——顺序RL和混合RL——进行系统研究，我们发现两者在行为和梯度层面均产生显著的跨领域干扰，导致整体增益有限。为解决这些问题，我们引入了**模块化梯度手术（MGS）**，该方法在Transformer内部模块层面解决梯度冲突。应用于Llama和Qwen模型时，MGS在三个代表性领域（数学、通用对话和指令遵循）上，相比标准多任务RL分别实现了平均4.3（16.6%）和4.5（11.1%）个百分点的提升。进一步分析表明，MGS在长时间训练下仍保持有效。总体而言，我们的研究阐明了多领域RL中干扰的来源，并为训练通用LRMs提供了有效解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training a single general-purpose large reasoning model (LRM) across diverse domains, where existing reinforcement learning (RL) strategies like Sequential RL and Mixed RL suffer from substantial cross-domain interference, limiting overall performance gains. The authors propose Modular Gradient Surgery (MGS), a method that resolves gradient conflicts at the module level within transformer architectures to mitigate this interference. Experimental results on Llama and Qwen models show that MGS achieves average improvements of 4.3 and 4.5 points, respectively, over standard multi-task RL across math, general chat, and instruction-following domains, with further analysis confirming its effectiveness under prolonged training.</div>
<div class="mono" style="margin-top:8px">本文针对在多样化领域训练单一通用大型推理模型（LRM）的挑战，指出现有强化学习策略（如顺序RL和混合RL）存在显著的跨领域干扰，限制了整体性能提升。作者提出了模块化梯度手术（MGS）方法，通过在Transformer架构中模块层面解决梯度冲突来减轻这种干扰。在Llama和Qwen模型上的实验结果表明，MGS在数学、通用聊天和指令遵循三个代表性领域上，相比标准多任务强化学习分别平均提升了4.3和4.5个点，进一步分析证实了其在长期训练下的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Modeling for Reinforcement Learning-Based LLM Reasoning: Design, Challenges, and Evaluation</div>
<div class="meta-line">Authors: Pei-Chi Pan, Yingbin Liang, Sen Lin</div>
<div class="meta-line">First: 2026-02-10T00:45:24+00:00 · Latest: 2026-02-10T00:45:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09305v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09305v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) demonstrate transformative potential, yet their reasoning remains inconsistent and unreliable. Reinforcement learning (RL)-based fine-tuning is a key mechanism for improvement, but its effectiveness is fundamentally governed by reward design. Despite its importance, the relationship between reward modeling and core LLM challenges--such as evaluation bias, hallucination, distribution shift, and efficient learning--remains poorly understood. This work argues that reward modeling is not merely an implementation detail but a central architect of reasoning alignment, shaping what models learn, how they generalize, and whether their outputs can be trusted. We introduce Reasoning-Aligned Reinforcement Learning (RARL), a unifying framework that systematizes diverse reward paradigms for multi-step reasoning. Within this framework, we present a taxonomy of reward mechanisms, analyze reward hacking as a pervasive failure mode, and examine how reward signals unify challenges ranging from inference-time scaling to hallucination mitigation. We further critically evaluate existing benchmarks, highlighting vulnerabilities such as data contamination and reward misalignment, and outline directions for more robust evaluation. By integrating fragmented research threads and clarifying the interplay between reward design and fundamental reasoning capabilities, this work provides a foundational roadmap for building reasoning models that are robust, verifiable, and trustworthy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的大语言模型推理奖励建模：设计、挑战与评估</div>
<div class="mono" style="margin-top:8px">大语言模型展现出变革性潜力，但其推理能力仍存在不一致与不可靠的问题。基于强化学习的微调是关键改进机制，而其效果根本上取决于奖励设计。尽管奖励建模至关重要，其与核心大语言模型挑战（如评估偏差、幻觉、分布偏移和高效学习）之间的关系仍缺乏深入理解。本研究主张奖励建模不仅是实现细节，更是推理对齐的核心架构，它决定了模型学习内容、泛化方式以及输出可信度。我们提出推理对齐强化学习框架，系统化整合多步推理的多样化奖励范式。在此框架内，我们建立了奖励机制分类体系，分析了奖励黑客攻击作为普遍失效模式的现象，并探讨了奖励信号如何统一从推理时缩放至幻觉缓解等系列挑战。我们进一步批判性评估现有基准测试，揭示数据污染与奖励失准等脆弱性，并规划更稳健评估的发展方向。通过整合碎片化研究脉络并阐明奖励设计与基础推理能力间的相互作用，本研究为构建鲁棒、可验证且可信赖的推理模型提供了基础路线图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the inconsistent reasoning of Large Language Models (LLMs) and the critical yet poorly understood role of reward design in reinforcement learning (RL)-based fine-tuning. The authors propose Reasoning-Aligned Reinforcement Learning (RARL), a framework to systematize reward paradigms for multi-step reasoning, and present a taxonomy of reward mechanisms while analyzing pervasive issues like reward hacking. Their main experimental analysis critically evaluates existing benchmarks, revealing vulnerabilities such as data contamination and reward misalignment, and argues that proper reward modeling is central to addressing core challenges like hallucination and distribution shift for building trustworthy reasoning models.</div>
<div class="mono" style="margin-top:8px">本文的动机在于大型语言模型（LLM）的推理能力存在不一致性，且强化学习微调中奖励设计的关键作用尚未被充分理解。作者提出了推理对齐的强化学习（RARL）框架，以系统化多步推理的奖励范式，并给出了奖励机制的分类法，同时分析了奖励黑客攻击等普遍失效模式。其主要实验分析批判性评估了现有基准，揭示了数据污染和奖励错位等漏洞，论证了正确的奖励建模对于解决幻觉和分布偏移等核心挑战、构建可靠可信的推理模型至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Risk-sensitive reinforcement learning using expectiles, shortfall risk and optimized certainty equivalent risk</div>
<div class="meta-line">Authors: Sumedh Gupte, Shrey Rakeshkumar Patel, Soumen Pachal, Prashanth L. A., Sanjay P. Bhat</div>
<div class="meta-line">First: 2026-02-10T00:38:21+00:00 · Latest: 2026-02-10T00:38:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09300v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09300v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose risk-sensitive reinforcement learning algorithms catering to three families of risk measures, namely expectiles, utility-based shortfall risk and optimized certainty equivalent risk. For each risk measure, in the context of a finite horizon Markov decision process, we first derive a policy gradient theorem. Second, we propose estimators of the risk-sensitive policy gradient for each of the aforementioned risk measures, and establish $\mathcal{O}\left(1/m\right)$ mean-squared error bounds for our estimators, where $m$ is the number of trajectories. Further, under standard assumptions for policy gradient-type algorithms, we establish smoothness of the risk-sensitive objective, in turn leading to stationary convergence rate bounds for the overall risk-sensitive policy gradient algorithm that we propose. Finally, we conduct numerical experiments to validate the theoretical findings on popular RL benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于期望分位数、短缺风险与优化确定性等价风险的风险敏感强化学习</div>
<div class="mono" style="margin-top:8px">本文针对三类风险度量（期望分位数、基于效用的短缺风险及优化确定性等价风险）提出了风险敏感强化学习算法。在有限时域马尔可夫决策过程框架下，我们首先推导了各类风险度量的策略梯度定理；其次为前述风险度量分别构建了风险敏感策略梯度估计器，并证明了估计器具有$\mathcal{O}\left(1/m\right)$的均方误差界（其中$m$为轨迹数量）。进一步地，在策略梯度类算法的标准假设下，我们证明了风险敏感目标函数的平滑性，从而为所提出的整体算法建立了平稳收敛速率界。最后通过经典强化学习基准任务的数值实验验证了理论结论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to incorporate risk sensitivity into reinforcement learning (RL) through formal risk measures. The method involves deriving policy gradient theorems for three risk measure families—expectiles, utility-based shortfall risk, and optimized certainty equivalent risk—within finite-horizon Markov decision processes, and proposing corresponding gradient estimators with established mean-squared error bounds. The main experimental results, validated on standard RL benchmarks, confirm the theoretical convergence rates and demonstrate the practical efficacy of the proposed risk-sensitive policy gradient algorithms.</div>
<div class="mono" style="margin-top:8px">本文的动机是通过形式化的风险度量将风险敏感性引入强化学习。方法上，针对期望分位数、基于效用的不足风险和优化确定性等价风险这三类风险度量，在有限时域马尔可夫决策过程中推导了策略梯度定理，并提出了相应的梯度估计器，同时建立了均方误差界。主要实验结果在标准强化学习基准上得到验证，证实了理论收敛速度，并证明了所提出的风险敏感策略梯度算法的实际有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ReflexGrad: A Dual-Process Architecture for Gradient-Free Inference-Time Learning</div>
<div class="meta-line">Authors: Ankush Kadu, Ashwanth Krishnan</div>
<div class="meta-line">First: 2025-11-18T15:25:05+00:00 · Latest: 2026-02-10T00:38:10+00:00</div>
<div class="meta-line">Comments: 10 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14584v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.14584v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling inference-time compute has emerged as a powerful paradigm--yet deliberating longer is not the same as learning. Current approaches to extended reasoning in large language models allocate more computation to thinking but remain fundamentally static: they cannot adapt from mistakes encountered during execution. Online reinforcement learning offers adaptation but requires gradient updates at runtime--expensive, prone to catastrophic forgetting, and unstable in deployment. We introduce ReflexGrad, a gradient-free framework for genuine inference-time learning: adaptation without retraining, without weight updates, without demonstrations. Our key insight is that effective runtime learning requires two complementary mechanisms--rapid policy refinement during forward progress, and deliberate causal diagnosis when stuck--with intelligent routing between them. ReflexGrad implements this by optimizing a natural language &quot;policy&quot; through textual feedback while keeping model weights frozen. When failures occur, the system analyzes recent action-outcome sequences to identify root causes and immediately applies corrections within the same execution--eliminating the need for multiple trials. Evaluated zero-shot across diverse interactive tasks without task-specific engineering, ReflexGrad achieves strong single-execution performance, demonstrating that gradient-free inference-time learning is not just theoretically appealing but practically viable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReflexGrad：一种用于无梯度推理时学习的双过程架构</div>
<div class="mono" style="margin-top:8px">扩展推理时计算已成为一种强大范式——但延长思考时间并不等同于学习。当前大型语言模型的扩展推理方法虽分配更多计算资源用于思考，本质上仍是静态的：无法根据执行过程中遇到的错误进行自适应调整。在线强化学习虽能实现自适应，但需要在运行时进行梯度更新——成本高昂、易导致灾难性遗忘且部署不稳定。我们提出ReflexGrad，一种实现真正推理时学习的无梯度框架：无需重新训练、无需权重更新、无需演示即可完成自适应。我们的核心洞见在于：有效的运行时学习需要两种互补机制——前进过程中的快速策略优化，以及陷入困境时的系统性因果诊断——并通过智能路由实现两者协同。ReflexGrad通过文本反馈优化自然语言“策略”同时保持模型权重冻结来实现这一目标。当出现失败时，系统分析最近的动作-结果序列以定位根本原因，并在同一执行过程中立即实施修正——无需多次尝试。在无需任务特定工程设计的多样化交互任务上进行零样本评估，ReflexGrad实现了优异的单次执行性能，证明无梯度推理时学习不仅具有理论吸引力，更具备实践可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ReflexGrad, a gradient-free framework designed to enable genuine inference-time learning in large language models, addressing the limitation that current extended reasoning approaches remain static and cannot adapt from mistakes during execution. The method employs a dual-process architecture that optimizes a natural language policy through textual feedback while keeping model weights frozen, combining rapid policy refinement during forward progress with deliberate causal diagnosis when failures occur, and intelligently routing between these mechanisms. Experimental results show that ReflexGrad achieves strong single-execution performance zero-shot across diverse interactive tasks without task-specific engineering, demonstrating the practical viability of gradient-free inference-time learning.</div>
<div class="mono" style="margin-top:8px">本文介绍了ReflexGrad，一种无梯度框架，旨在实现大型语言模型在推理时的真正学习，以解决当前扩展推理方法静态且无法在执行过程中从错误中适应的局限性。该方法采用双过程架构，通过文本反馈优化自然语言策略，同时保持模型权重不变，结合了前向进展中的快速策略改进和失败时的因果诊断，并在这些机制间智能路由。实验结果表明，ReflexGrad在无需任务特定工程的情况下，零样本跨多种交互任务实现了强大的单次执行性能，证明了无梯度推理时学习的实际可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents</div>
<div class="meta-line">Authors: Elizaveta Tennant, Stephen Hailes, Mirco Musolesi</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2024-03-07T04:12:24+00:00 · Latest: 2026-02-09T23:48:24+00:00</div>
<div class="meta-line">Comments: Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and Society - San Jose, CA, USA) - see https://ojs.aaai.org/index.php/AIES/article/view/31736</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.04202v8">Abs</a> · <a href="https://arxiv.org/pdf/2403.04202v8">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents: a promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents; however, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., focused on maximizing outcomes over time), norm-based (i.e., conforming to specific norms), or virtue-based (i.e., considering a combination of different virtues). The extent to which agents&#x27; co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using an Iterated Prisoner&#x27;s Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents&#x27; learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain types of moral agents are able to steer selfish agents towards more cooperative behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>异构学习智能体群体中的道德行为动态研究</div>
<div class="mono" style="margin-top:8px">对人工智能系统安全性与对齐性日益增长的关注，凸显了在人工智能体中嵌入道德能力的重要性：一种前景广阔的解决方案是利用经验学习，即强化学习。在多智能体（社会）环境中，个体学习智能体间的互动可能催生复杂的群体层面现象。现有研究多依赖模拟社会困境环境来考察独立学习智能体的交互，但往往忽视了实践中智能体社会可能存在的道德异质性。例如，单个学习智能体在不同时间点可能面对结果主义（即注重长期结果最大化）、规范遵循型（即遵从特定规范）或美德导向型（即综合考虑多种美德）的对手。群体中的道德异质性对智能体协同发展的影响程度尚未得到充分理解。本文通过社会困境场景中道德异质群体的互动，研究其学习动态。借助配备伙伴选择机制的迭代囚徒困境环境，我们探究了群体中多样化道德智能体的普遍性如何影响个体学习行为及涌现的群体层面结果。研究观察到亲社会与反社会智能体间多种非平凡交互模式，并发现特定类型的道德智能体能够引导自私智能体转向更合作的行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand how moral diversity affects AI safety and alignment in multi-agent systems, this study investigates learning dynamics in populations with heterogeneous moral agents, such as consequentialist, norm-based, and virtue-based types, interacting in a social dilemma setting. The method employs an Iterated Prisoner&#x27;s Dilemma environment with partner selection to simulate interactions, analyzing how varying prevalences of moral types influence individual learning and emergent population outcomes. Experimental results reveal non-trivial interactions between pro-social and anti-social agents, showing that certain moral agents can steer selfish agents toward more cooperative behavior, highlighting the impact of moral heterogeneity on collective dynamics.</div>
<div class="mono" style="margin-top:8px">本研究旨在探讨道德多样性如何影响多智能体系统中的人工智能安全与对齐问题，关注具有不同道德类型（如结果主义、规范型和美德型）的智能体在社交困境中的学习动态。方法上，采用带有伙伴选择机制的迭代囚徒困境环境，模拟智能体间的交互，分析不同道德类型比例对个体学习行为和群体层面涌现结果的影响。实验结果表明，亲社会与反社会智能体之间存在复杂的非平凡交互，某些道德类型能够引导自私智能体转向更合作的行为，揭示了道德异质性对群体动态的重要作用。</div>
</details>
</div>
<div class="card">
<div class="title">Nudging the Boundaries of LLM Reasoning</div>
<div class="meta-line">Authors: Justin Chih-Yao Chen, Becky Xiangyu Peng, Prafulla Kumar Choubey, Kung-Hsiang Huang, Jiaxin Zhang, Mohit Bansal, Chien-Sheng Wu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-30T02:01:40+00:00 · Latest: 2026-02-09T22:43:34+00:00</div>
<div class="meta-line">Comments: ICLR 2026 (Camera-Ready)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25666v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25666v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current online reinforcement learning (RL) algorithms like GRPO share a key limitation in LLM reasoning: they cannot learn from problems that are &quot;unsolvable&quot; to the model. In other words, they can only improve performance on problems where the model is capable of exploring the correct answer. Consequently, the model&#x27;s &quot;upper limit&quot; remains unchanged after RL training, even though the likelihood of solving easier, solvable problems may increase. These hard samples cannot contribute to training, as no rollouts yield rewards and thus no gradients are produced. To unlock learning from these hard samples, we propose NuRL, a &quot;nudging&quot; method that aims to push the upper bound of LLM reasoning using self-generated hints, i.e., abstract cues that help reduce the problem difficulty for the model. Given a question and its gold answer, the model generates a CoT and then produces a hint containing the core knowledge needed to solve the problem. During training, we generate G rollouts from the base policy and use the pass rate to decide whether the hint should be injected. For hard samples with a 0% pass rate, we inject the hint and regenerate a new batch of trajectories. This yields two benefits: (1) the hint boosts pass rates (from 0% to non-zero), thereby introducing training signals for previously unsolvable samples, and (2) the hints are self-generated, avoiding distributional shift and do not rely on external models. NuRL achieves consistent improvements across 6 benchmarks and 3 models, while remaining complementary to test-time scaling. Notably, NuRL can raise the model&#x27;s upper limit, whereas GRPO leaves pass@1024 unchanged from the base model. Furthermore, we present a systematic study of what makes an effective hint and when hints are most useful. Interestingly, the best hints are abstract and high-level, and are most beneficial when applied necessarily and after GRPO has converged.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推动大语言模型推理的边界</div>
<div class="mono" style="margin-top:8px">当前在线强化学习算法如GRPO在大语言模型推理中存在一个关键局限：无法从模型“无法解决”的问题中学习。换言之，它们只能提升模型在有能力探索正确答案的问题上的表现。因此，即使解决较简单问题的可能性增加，RL训练后模型的“上限”仍保持不变。这些困难样本无法贡献于训练，因为没有任何轨迹能产生奖励，从而无法生成梯度。为解锁从这些困难样本中学习的能力，我们提出NuRL——一种“助推”方法，旨在通过自生成提示（即帮助降低问题难度的抽象线索）来推动大语言模型推理的上限。给定问题及其标准答案，模型生成思维链，随后产生包含解题所需核心知识的提示。训练时，我们从基础策略生成G条轨迹，并根据通过率决定是否注入提示。对于通过率为0%的困难样本，我们注入提示并重新生成一批轨迹。这带来两个优势：（1）提示提升通过率（从0%至非零），从而为先前无法解决的样本引入训练信号；（2）提示为自生成，避免了分布偏移且不依赖外部模型。NuRL在6个基准测试和3个模型上实现了一致的改进，同时与测试时缩放技术保持互补。值得注意的是，NuRL能提升模型上限，而GRPO的pass@1024指标与基础模型相比无变化。此外，我们系统研究了有效提示的构成要素及其最佳应用时机。有趣的是，最佳提示具有抽象性和高层次特性，且在必要时应用、并于GRPO收敛后使用效果最为显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a key limitation in online reinforcement learning for LLM reasoning, where models cannot learn from unsolvable problems that yield no reward signals, thus failing to improve their upper performance bound. The proposed method, NuRL, introduces a nudging technique that uses self-generated abstract hints to reduce problem difficulty; for hard samples with zero pass rates, these hints are injected to regenerate trajectories, enabling non-zero pass rates and providing training gradients. Experimental results across six benchmarks and three models show that NuRL consistently improves performance and raises the model&#x27;s upper limit, unlike baseline methods like GRPO, while also demonstrating that effective hints are abstract and most beneficial when applied after initial convergence.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型推理中在线强化学习的一个关键限制：模型无法从无奖励信号的不可解问题中学习，导致性能上限无法提升。提出的NuRL方法采用一种“推动”技术，利用自生成的抽象提示来降低问题难度；对于通过率为零的困难样本，注入提示以重新生成轨迹，从而产生非零通过率并提供训练梯度。在六个基准测试和三个模型上的实验结果表明，NuRL能持续提升性能并提高模型上限，而基线方法如GRPO则无法做到，同时研究发现有效的提示应是抽象的，且在初始收敛后应用最为有益。</div>
</details>
</div>
<div class="card">
<div class="title">STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</div>
<div class="meta-line">Authors: Chen Li, Han Zhang, Zhantao Yang, Fangyi Chen, Zihan Wang, Anudeepsekhar Bolimera, Marios Savvides</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-12T07:27:50+00:00 · Latest: 2026-02-09T22:31:37+00:00</div>
<div class="meta-line">Comments: This paper has been accepted at AAAI 2026. This is the author&#x27;s extended version. The final version will appear in the official proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08688v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.08688v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STELAR-VISION：面向视觉对齐推理的自拓扑感知高效学习框架</div>
<div class="mono" style="margin-top:8px">视觉语言模型在推理方面取得显著进展，但在处理复杂多模态任务时仍面临困难，且常产生冗余输出。其关键局限在于依赖链式思维推理，而许多任务本可从树状或图状等拓扑结构中获益。为此，我们提出STELAR-Vision——一种拓扑感知推理训练框架。其核心是TopoAug合成数据管道，通过多样化拓扑结构增强训练。采用监督微调与强化学习，我们在保持准确率的同时对Qwen2VL模型进行高效后训练。此外，我们提出节俭学习法，能以极小的精度损失缩减输出长度。在MATH-V和VLM-S2H数据集上，STELAR-Vision较基础模型准确率提升9.7%，并超越更大的Qwen2VL-72B-Instruct模型7.3%。在五个分布外基准测试中，其性能最高超越Phi-4-Multimodal-Instruct达28.4%，超越LLaMA-3.2-11B-Vision-Instruct达13.2%，展现出强大泛化能力。与纯链式训练相比，本方法在分布内数据集上整体准确率提升4.3%，且在全部分布外基准测试中均表现更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of vision-language models (VLMs) in handling complex multimodal tasks and generating verbose outputs due to their reliance on chain-of-thought reasoning, this paper introduces STELAR-Vision, a training framework for topology-aware reasoning. The method employs TopoAug, a synthetic data pipeline to enrich training with diverse topological structures like trees or graphs, and uses supervised fine-tuning and reinforcement learning to post-train Qwen2VL models, incorporating Frugal Learning to reduce output length. Experimental results show that STELAR-Vision improves accuracy by 9.7% over its base model and surpasses larger models like Qwen2VL-72B-Instruct by 7.3% on benchmarks such as MATH-V and VLM-S2H, while also demonstrating strong generalization on out-of-distribution tasks, outperforming models like Phi-4-Multimodal-Instruct by up to 28.4%.</div>
<div class="mono" style="margin-top:8px">本文的动机是视觉语言模型在处理复杂多模态任务时，因依赖链式思维推理而存在输出冗长和性能不足的问题。为此，提出了STELAR-Vision训练框架，其方法核心是TopoAug合成数据管道，通过引入树或图等多样化拓扑结构来丰富训练数据，并结合监督微调和强化学习对Qwen2VL模型进行后训练，同时采用Frugal Learning以减少输出长度。主要实验结果表明，在MATH-V和VLM-S2H基准测试中，STELAR-Vision相比基础模型准确率提升9.7%，并超过更大的Qwen2VL-72B-Instruct模型7.3%；在五个分布外基准测试中，其性能优于Phi-4-Multimodal-Instruct模型达28.4%，展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling</div>
<div class="meta-line">Authors: Nicholas E. Corrado, Josiah P. Hanna</div>
<div class="meta-line">First: 2023-11-14T16:37:28+00:00 · Latest: 2026-02-09T21:19:19+00:00</div>
<div class="meta-line">Comments: TMLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2311.08290v3">Abs</a> · <a href="https://arxiv.org/pdf/2311.08290v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On-policy reinforcement learning (RL) algorithms are typically characterized as algorithms that perform policy updates using i.i.d. trajectories collected by the agent&#x27;s current policy. However, after observing only a finite number of trajectories, such on-policy sampling may produce data that fails to match the expected on-policy data distribution. This sampling error leads to high-variance gradient estimates that yield data-inefficient on-policy learning. Recent work in the policy evaluation setting has shown that non-i.i.d., off-policy sampling can produce data with lower sampling error w.r.t. the expected on-policy distribution than on-policy sampling can produce (Zhong et. al, 2022). Motivated by this observation, we introduce an adaptive, off-policy sampling method to reduce sampling error during on-policy policy gradient RL training. Our method, Proximal Robust On-Policy Sampling (PROPS), reduces sampling error by collecting data with a behavior policy that increases the probability of sampling actions that are under-sampled w.r.t. the current policy. We empirically evaluate PROPS on continuous-action MuJoCo benchmark tasks as well as discrete-action tasks and demonstrate that (1) PROPS decreases sampling error throughout training and (2) increases the data efficiency of on-policy policy gradient algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需在线采样的在线策略梯度强化学习</div>
<div class="mono" style="margin-top:8px">在线强化学习算法通常被定义为使用智能体当前策略收集的独立同分布轨迹进行策略更新的算法。然而，在仅观测有限数量轨迹后，此类在线采样可能产生与预期在线数据分布不匹配的数据。这种采样误差会导致高方差的梯度估计，从而降低在线学习的数据效率。近期策略评估研究表明，非独立同分布的离线采样能比在线采样产生更接近预期在线分布的采样误差（Zhong等，2022）。受此启发，我们提出一种自适应离线采样方法以减少在线策略梯度训练中的采样误差。该方法——近端鲁棒在线采样（PROPS）——通过行为策略收集数据来增加当前策略下欠采样动作的概率，从而降低采样误差。我们在连续动作MuJoCo基准任务和离散动作任务上实证评估PROPS，证明：（1）PROPS能持续降低训练过程中的采样误差；（2）提升在线策略梯度算法的数据效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that on-policy reinforcement learning suffers from high sampling error when using a finite number of i.i.d. trajectories, leading to inefficient learning. To address this, the authors propose Proximal Robust On-Policy Sampling (PROPS), an adaptive off-policy sampling method that uses a behavior policy to increase the probability of sampling actions underrepresented in the current policy&#x27;s distribution. Experimental results on MuJoCo continuous-action and discrete-action tasks demonstrate that PROPS effectively reduces sampling error throughout training and improves the data efficiency of on-policy policy gradient algorithms.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到在策略强化学习在使用有限数量的独立同分布轨迹时存在高采样误差，导致学习效率低下。为解决此问题，作者提出了近端鲁棒在策略采样（PROPS），这是一种自适应的离策略采样方法，它使用一个行为策略来增加对当前策略分布中采样不足动作的采样概率。在MuJoCo连续动作和离散动作任务上的实验结果表明，PROPS在整个训练过程中有效降低了采样误差，并提高了在策略策略梯度算法的数据效率。</div>
</details>
</div>
<div class="card">
<div class="title">CausalGDP: Causality-Guided Diffusion Policies for Reinforcement Learning</div>
<div class="meta-line">Authors: Xiaofeng Xiao, Xiao Hu, Yang Ye, Xubo Yue</div>
<div class="meta-line">First: 2026-02-09T21:18:32+00:00 · Latest: 2026-02-09T21:18:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09207v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has achieved remarkable success in a wide range of sequential decision-making problems. Recent diffusion-based policies further improve RL by modeling complex, high-dimensional action distributions. However, existing diffusion policies primarily rely on statistical associations and fail to explicitly account for causal relationships among states, actions, and rewards, limiting their ability to identify which action components truly cause high returns. In this paper, we propose Causality-guided Diffusion Policy (CausalGDP), a unified framework that integrates causal reasoning into diffusion-based RL. CausalGDP first learns a base diffusion policy and an initial causal dynamical model from offline data, capturing causal dependencies among states, actions, and rewards. During real-time interaction, the causal information is continuously updated and incorporated as a guidance signal to steer the diffusion process toward actions that causally influence future states and rewards. By explicitly considering causality beyond association, CausalGDP focuses policy optimization on action components that genuinely drive performance improvements. Experimental results demonstrate that CausalGDP consistently achieves competitive or superior performance over state-of-the-art diffusion-based and offline RL methods, especially in complex, high-dimensional control tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CausalGDP：强化学习中基于因果引导的扩散策略</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在广泛的序列决策问题中取得了显著成功。近期基于扩散的策略通过建模复杂的高维动作分布进一步提升了RL性能。然而，现有扩散策略主要依赖统计关联，未能显式考虑状态、动作与奖励间的因果关系，限制了其识别哪些动作成分真正导致高回报的能力。本文提出因果引导扩散策略（CausalGDP），这是一个将因果推理融入基于扩散的RL的统一框架。CausalGDP首先从离线数据中学习基础扩散策略和初始因果动态模型，捕捉状态、动作与奖励间的因果依赖关系。在实时交互过程中，因果信息持续更新并作为引导信号融入扩散过程，使动作生成朝向能因果影响未来状态与奖励的方向。通过显式考虑超越关联的因果关系，CausalGDP将策略优化聚焦于真正驱动性能提升的动作成分。实验结果表明，CausalGDP在复杂高维控制任务中持续取得优于或媲美最先进的基于扩散及离线RL方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of existing diffusion-based reinforcement learning policies, which rely on statistical associations and overlook causal relationships, thereby hindering their ability to identify action components that truly cause high returns. To overcome this, the authors propose CausalGDP, a framework that integrates causal reasoning by first learning a base diffusion policy and a causal dynamical model from offline data, then continuously updating causal information during interaction to guide the diffusion process toward actions with causal influence on future states and rewards. Experimental results show that CausalGDP achieves competitive or superior performance compared to state-of-the-art diffusion-based and offline RL methods, particularly in complex, high-dimensional control tasks.</div>
<div class="mono" style="margin-top:8px">该论文针对现有基于扩散的强化学习策略主要依赖统计关联而忽视因果关系的问题，提出CausalGDP框架以整合因果推理。该方法首先从离线数据中学习基础扩散策略和初始因果动态模型，捕捉状态、动作和奖励间的因果依赖；在实时交互中，持续更新因果信息作为引导信号，指导扩散过程生成对未来状态和奖励有因果影响的动作。实验结果表明，CausalGDP在复杂高维控制任务中，性能优于或与最先进的基于扩散和离线强化学习方法相当。</div>
</details>
</div>
<div class="card">
<div class="title">EExApp: GNN-Based Reinforcement Learning for Radio Unit Energy Optimization in 5G O-RAN</div>
<div class="meta-line">Authors: Jie Lu, Peihao Yan, Huacheng Zeng</div>
<div class="meta-line">First: 2026-02-09T21:17:23+00:00 · Latest: 2026-02-09T21:17:23+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE INFOCOM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09206v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09206v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With over 3.5 million 5G base stations deployed globally, their collective energy consumption (projected to exceed 131 TWh annually) raises significant concerns over both operational costs and environmental impacts. In this paper, we present EExAPP, a deep reinforcement learning (DRL)-based xApp for 5G Open Radio Access Network (O-RAN) that jointly optimizes radio unit (RU) sleep scheduling and distributed unit (DU) resource slicing. EExAPP uses a dual-actor-dual-critic Proximal Policy Optimization (PPO) architecture, with dedicated actor-critic pairs targeting energy efficiency and quality-of-service (QoS) compliance. A transformer-based encoder enables scalable handling of variable user equipment (UE) populations by encoding all-UE observations into fixed-dimensional representations. To coordinate the two optimization objectives, a bipartite Graph Attention Network (GAT) is used to modulate actor updates based on both critic outputs, enabling adaptive tradeoffs between power savings and QoS. We have implemented EExAPP and deployed it on a real-world 5G O-RAN testbed with live traffic, commercial RU and smartphones. Extensive over-the-air experiments and ablation studies confirm that EExAPP significantly outperforms existing methods in reducing the energy consumption of RU while maintaining QoS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EExApp：基于图神经网络的强化学习在5G O-RAN无线单元能耗优化中的应用</div>
<div class="mono" style="margin-top:8px">随着全球部署超过350万个5G基站，其总体能耗（预计每年超过131太瓦时）引发了运营成本和环境影响的重大关切。本文提出EExAPP——一种基于深度强化学习的5G开放式无线接入网络xApp，可联合优化无线单元休眠调度与分布式单元资源切片。该框架采用双执行器-双评价器的近端策略优化架构，配备分别针对能效和服务质量合规性的专用执行器-评价器对。基于Transformer的编码器通过将所有用户设备观测编码为固定维度表示，实现了对可变用户设备规模的可扩展处理。为协调两个优化目标，系统采用二分图注意力网络，依据双评价器输出调节执行器更新，从而在节能与服务质量间实现自适应权衡。我们已在实际承载流量的5G O-RAN测试平台上部署EExAPP，使用商用无线单元与智能手机进行验证。大量空中实测与消融实验证实，EExAPP在保障服务质量的同时，能显著超越现有方法降低无线单元能耗。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the substantial energy consumption and environmental impact of the millions of deployed 5G base stations, this paper introduces EExAPP, a deep reinforcement learning xApp for O-RAN that jointly optimizes radio unit sleep scheduling and distributed unit resource slicing. The method employs a dual-actor-dual-critic PPO architecture with dedicated components for energy efficiency and QoS, enhanced by a transformer encoder for scalable user equipment handling and a bipartite Graph Attention Network to coordinate the two objectives. Experimental deployment on a real-world 5G O-RAN testbed with live traffic demonstrates that EExAPP significantly outperforms existing methods in reducing radio unit energy consumption while maintaining quality of service.</div>
<div class="mono" style="margin-top:8px">本文的动机是应对全球数百万已部署5G基站带来的巨大能耗与环境影响，提出了EExAPP这一用于O-RAN的深度强化学习xApp，以联合优化射频单元休眠调度与分布式单元资源切片。该方法采用双行动者-双评论者PPO架构，设有专门处理能效和服务质量的组件，并通过变压器编码器实现可扩展的用户设备处理，以及使用二分图注意力网络来协调两个优化目标。在具有实时流量的真实5G O-RAN测试平台上进行的实验部署表明，EExAPP在显著降低射频单元能耗的同时保持了服务质量，性能明显优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Weighted Wasserstein Barycenter of Gaussian Processes for exotic Bayesian Optimization tasks</div>
<div class="meta-line">Authors: Antonio Candelieri, Francesco Archetti</div>
<div class="meta-line">First: 2026-02-09T20:40:33+00:00 · Latest: 2026-02-09T20:40:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09181v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exploiting the analogy between Gaussian Distributions and Gaussian Processes&#x27; posterior, we present how the weighted Wasserstein Barycenter of Gaussian Processes (W2BGP) can be used to unify, under a common framework, different exotic Bayesian Optimization (BO) tasks. Specifically, collaborative/federated BO, (synchronous) batch BO, and multi-fidelity BO are considered in this paper. Our empirical analysis proves that each one of these tasks requires just an appropriate weighting schema for the W2BGP, while the entire framework remains untouched. Moreover, we demonstrate that the most well-known BO acquisition functions can be easily re-interpreted under the proposed framework and also enable a more computationally efficient way to deal with the computation of the Wasserstein Barycenter, compared with state-of-the-art methods from the Machine Learning literature. Finally, research perspectives branching from the proposed approach are presented.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高斯过程加权Wasserstein质心在非常规贝叶斯优化任务中的应用</div>
<div class="mono" style="margin-top:8px">通过利用高斯分布与高斯过程后验之间的类比关系，本文提出加权Wasserstein高斯过程质心（W2BGP）方法，可在统一框架下处理多种非常规贝叶斯优化任务。具体涵盖协作/联邦贝叶斯优化、同步批量贝叶斯优化以及多保真度贝叶斯优化。实证分析表明，仅需为W2BGP配置相应权重方案即可适应各类任务，而无需改变整体框架。此外，研究证明主流贝叶斯优化采集函数均可在该框架下重新阐释，且相比机器学习领域现有方法，能更高效地计算Wasserstein质心。最后，本文提出了基于该方法的后续研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to unify various exotic Bayesian Optimization (BO) tasks under a single framework. The method introduces the Weighted Wasserstein Barycenter of Gaussian Processes (W2BGP), which leverages analogies between Gaussian distributions and Gaussian processes&#x27; posteriors to handle collaborative/federated, batch, and multi-fidelity BO by simply adjusting weighting schemes. Experimental results show that this approach not only unifies these tasks effectively but also allows for a reinterpretation of standard BO acquisition functions and offers computational efficiency improvements over existing methods for computing Wasserstein barycenters.</div>
<div class="mono" style="margin-top:8px">本文旨在将各种复杂的贝叶斯优化任务统一到一个框架下。方法上提出了高斯过程的加权Wasserstein重心，利用高斯分布与高斯过程后验之间的类比，通过调整权重方案来处理协作/联邦、批量和多保真度贝叶斯优化。实验结果表明，该方法不仅有效统一了这些任务，还能重新解释标准贝叶斯优化采集函数，并在计算Wasserstein重心方面相比现有方法提高了计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Privileged Information Distillation for Language Models</div>
<div class="meta-line">Authors: Emiliano Penaloza, Dheeraj Vattikonda, Nicolas Gontier, Alexandre Lacoste, Laurent Charlin, Massimo Caccia</div>
<div class="meta-line">First: 2026-02-04T18:46:17+00:00 · Latest: 2026-02-09T20:36:14+00:00</div>
<div class="meta-line">Comments: Abstract border should have been purple</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04942v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04942v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, which typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable, but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically, we find that π-Distill and, in some cases, OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型的特权信息蒸馏</div>
<div class="mono" style="margin-top:8px">训练阶段的特权信息（PI）能使语言模型在原本会失败的任务上取得成功，这使其成为困难、长周期场景下强化学习的强大工具。然而，将借助PI习得的能力迁移至推理时无法使用PI的策略，仍是一个根本性挑战。我们在为多轮智能体环境蒸馏前沿模型的背景下研究该问题——这类环境通常隐藏内部推理过程，仅暴露行动轨迹。这使得标准蒸馏流程失效，因为成功行为可观测，但推理过程不可见。为此，我们提出π-Distill：一种联合师生目标函数，使用同一模型同步训练PI条件化教师模型与无条件化学生模型。同时，我们还提出策略上自蒸馏（OPSD）——通过强化学习训练学生模型，并施加其与PI条件化教师模型间反向KL惩罚的替代方案。实验表明，这两种算法均能有效利用仅含行动信息的PI蒸馏前沿智能体。具体而言，π-Distill（及某些场景下的OPSD）在多个智能体基准测试、模型架构和PI形式中，均优于假设能获取完整思维链监督的行业标准流程（监督微调后接强化学习）。我们通过深入分析补充实验结果，重点解析π-Distill实现PI有效学习的关键因素，并阐明OPSD具备竞争力的适用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of distilling capabilities from language models trained with privileged information (PI), which is unavailable at inference time, particularly in multi-turn agentic environments where only action trajectories are observable. To overcome this, the authors introduce two methods: π-Distill, a joint teacher-student objective that trains both a PI-conditioned teacher and an unconditioned student simultaneously, and On-Policy Self-Distillation (OPSD), which uses reinforcement learning with a reverse KL-penalty between the student and teacher. Experimental results demonstrate that both π-Distill and, in some cases, OPSD outperform standard practices like supervised fine-tuning followed by RL, which assume full chain-of-thought supervision, across various agentic benchmarks, models, and forms of PI, with extensive analysis highlighting the factors enabling effective learning.</div>
<div class="mono" style="margin-top:8px">本文研究了在推理时无法获取特权信息（PI）的情况下，如何从训练时使用PI的语言模型中蒸馏能力，特别是在多轮智能体环境中仅能观察到行动轨迹的场景。为解决这一问题，作者提出了两种方法：π-Distill，一种联合教师-学生目标，同时训练PI条件教师和无条件学生；以及On-Policy Self-Distillation（OPSD），该方法使用强化学习并引入学生与教师之间的反向KL惩罚。实验结果表明，在多种智能体基准测试、模型和PI形式中，π-Distill以及在某些情况下的OPSD均优于标准实践（如监督微调后接强化学习），后者假设能获取完整的思维链监督，并通过深入分析揭示了实现有效学习的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Symmetry-Guided Memory Augmentation for Efficient Locomotion Learning</div>
<div class="meta-line">Authors: Kaixi Bao, Chenhao Li, Yarden As, Andreas Krause, Marco Hutter</div>
<div class="meta-line">First: 2025-02-03T17:00:19+00:00 · Latest: 2026-02-09T20:33:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.01521v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.01521v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training reinforcement learning (RL) policies for legged locomotion often requires extensive environment interactions, which are costly and time-consuming. We propose Symmetry-Guided Memory Augmentation (SGMA), a framework that improves training efficiency by combining structured experience augmentation with memory-based context inference. Our method leverages robot and task symmetries to generate additional, physically consistent training experiences without requiring extra interactions. To avoid the pitfalls of naive augmentation, we extend these transformations to the policy&#x27;s memory states, enabling the agent to retain task-relevant context and adapt its behavior accordingly. We evaluate the approach on quadruped and humanoid robots in simulation, as well as on a real quadruped platform. Across diverse locomotion tasks involving joint failures and payload variations, our method achieves efficient policy training while maintaining robust performance, demonstrating a practical route toward data-efficient RL for legged robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对称性引导的记忆增强用于高效运动学习</div>
<div class="mono" style="margin-top:8px">为足式运动训练强化学习策略通常需要大量环境交互，成本高昂且耗时。我们提出对称性引导记忆增强框架，通过结合结构化经验增强与基于记忆的上下文推断来提升训练效率。该方法利用机器人与任务的对称性生成额外物理一致的训练经验，无需额外交互。为避免简单增强的缺陷，我们将这些变换扩展至策略的记忆状态，使智能体能够保留任务相关上下文并相应调整行为。我们在仿真四足与仿人机器人以及真实四足平台上评估该方法。在涉及关节故障与负载变化的多样化运动任务中，本方法在保持鲁棒性能的同时实现了高效策略训练，为足式机器人数据高效强化学习提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high sample inefficiency in training reinforcement learning policies for legged locomotion by introducing Symmetry-Guided Memory Augmentation (SGMA), a framework that leverages robot and task symmetries to generate physically consistent training experiences without additional environment interactions. The method extends these symmetry transformations to the policy&#x27;s memory states, allowing the agent to maintain task-relevant context and adapt behavior effectively, thereby avoiding the limitations of naive data augmentation. Experimental results in simulation with quadruped and humanoid robots, and on a real quadruped platform, demonstrate that SGMA achieves efficient policy training across diverse locomotion tasks involving joint failures and payload variations while maintaining robust performance, offering a practical approach to data-efficient reinforcement learning for legged robots.</div>
<div class="mono" style="margin-top:8px">本文针对足式机器人强化学习策略训练中样本效率低的问题，提出了对称性引导的记忆增强框架，通过利用机器人和任务的对称性生成物理一致的训练经验，无需额外环境交互。该方法将对称变换扩展到策略的记忆状态，使智能体能够保持任务相关上下文并有效调整行为，从而避免了简单数据增强的缺陷。在四足和双足机器人仿真以及真实四足平台上的实验结果表明，该框架在涉及关节故障和负载变化的多种运动任务中实现了高效策略训练，同时保持了鲁棒性能，为足式机器人的数据高效强化学习提供了一条实用路径。</div>
</details>
</div>
<div class="card">
<div class="title">$n$-Musketeers: Reinforcement Learning Shapes Collaboration Among Language Models</div>
<div class="meta-line">Authors: Ryozo Masukawa, Sanggeon Yun, Hyunwoo Oh, SuhgHeon Jeong, Raheeb Hassa, Hanning Chen, Wenjun Huang, Mahdi Imani, Pietro Mercati, Nathaniel D. Bastian, Mohsen Imani</div>
<div class="meta-line">First: 2026-02-09T20:27:52+00:00 · Latest: 2026-02-09T20:27:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09173v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09173v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in reinforcement learning with verifiable rewards (RLVR) shows that small, specialized language models (SLMs) can exhibit structured reasoning without relying on large monolithic LLMs. We introduce soft hidden-state collaboration, where multiple heterogeneous frozen SLM experts are integrated through their internal representations via a trainable attention interface. Experiments on Reasoning Gym and GSM8K show that this latent integration is competitive with strong single-model RLVR baselines. Ablations further reveal a dual mechanism of expert utilization: for simpler arithmetic domains, performance gains can largely be explained by static expert preferences, whereas more challenging settings induce increasingly concentrated and structured expert attention over training, indicating emergent specialization in how the router connects to relevant experts. Overall, hidden-state collaboration provides a compact mechanism for leveraging frozen experts, while offering an observational window into expert utilization patterns and their evolution under RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$n$-火枪手：强化学习塑造语言模型间的协作机制</div>
<div class="mono" style="margin-top:8px">近期基于可验证奖励的强化学习（RLVR）进展表明，小型专业化语言模型（SLM）无需依赖大型单体LLM即可展现结构化推理能力。我们提出软隐藏状态协作机制，通过可训练的注意力接口将多个异构冻结SLM专家的内部表征进行集成。在Reasoning Gym和GSM8K数据集上的实验表明，这种潜在集成能力与强大的单模型RLVR基线具有竞争力。消融研究进一步揭示了专家利用的双重机制：在简单算术领域，性能提升主要源于静态专家偏好；而在更具挑战性的场景中，训练过程中会逐渐形成集中化、结构化的专家注意力分布，表明路由机制在与相关专家连接时出现了新兴专业化特征。总体而言，隐藏状态协作为利用冻结专家提供了紧凑的机制，同时为观察专家利用模式及其在RLVR下的演化过程提供了观测窗口。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the potential of reinforcement learning with verifiable rewards (RLVR) to enable structured reasoning in small, specialized language models (SLMs) without large monolithic LLMs. The method introduces soft hidden-state collaboration, where multiple frozen, heterogeneous SLM experts are integrated through their internal representations using a trainable attention interface. Experimental results on Reasoning Gym and GSM8K show this latent integration is competitive with strong single-model RLVR baselines, with ablations revealing a dual mechanism: simpler arithmetic tasks show static expert preferences, while more challenging settings induce increasingly concentrated and structured expert attention, indicating emergent specialization in how the router connects to relevant experts.</div>
<div class="mono" style="margin-top:8px">本文的动机在于利用可验证奖励的强化学习（RLVR），使小型专业化语言模型（SLM）无需依赖大型单体LLM即可进行结构化推理。方法上提出了软隐藏状态协作，通过一个可训练的注意力接口，将多个冻结的异构SLM专家模型在其内部表示层面进行整合。在Reasoning Gym和GSM8K上的实验结果表明，这种潜在整合与强大的单模型RLVR基线性能相当；消融实验进一步揭示了双重利用机制：对于较简单的算术任务，性能提升主要源于静态的专家偏好，而在更具挑战性的场景中，训练过程会诱导出越来越集中和结构化的专家注意力，这表明路由机制在连接相关专家时出现了新兴的专业化分工。</div>
</details>
</div>
<div class="card">
<div class="title">Boltzmann Reinforcement Learning for Noise resilience in Analog Ising Machines</div>
<div class="meta-line">Authors: Aditya Choudhary, Saaketh Desai, Prasad Iyer</div>
<div class="meta-line">First: 2026-02-09T20:07:42+00:00 · Latest: 2026-02-09T20:07:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09162v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09162v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Analog Ising machines (AIMs) have emerged as a promising paradigm for combinatorial optimization, utilizing physical dynamics to solve Ising problems with high energy efficiency. However, the performance of traditional optimization and sampling algorithms on these platforms is often limited by inherent measurement noise. We introduce BRAIN (Boltzmann Reinforcement for Analog Ising Networks), a distribution learning framework that utilizes variational reinforcement learning to approximate the Boltzmann distribution. By shifting from state-by-state sampling to aggregating information across multiple noisy measurements, BRAIN is resilient to Gaussian noise characteristic of AIMs. We evaluate BRAIN across diverse combinatorial topologies, including the Curie-Weiss and 2D nearest-neighbor Ising systems. We find that under realistic 3\% Gaussian measurement noise, BRAIN maintains 98\% ground state fidelity, whereas Markov Chain Monte Carlo (MCMC) methods degrade to 51\% fidelity. Furthermore, BRAIN reaches the MCMC-equivalent solution up to 192x faster under these conditions. BRAIN exhibits $\mathcal{O}(N^{1.55})$ scaling up to 65,536 spins and maintains robustness against severe measurement uncertainty up to 40\%. Beyond ground state optimization, BRAIN accurately captures thermodynamic phase transitions and metastable states, providing a scalable and noise-resilient method for utilizing analog computing architectures in complex optimizations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>玻尔兹曼强化学习在模拟伊辛机噪声鲁棒性中的应用</div>
<div class="mono" style="margin-top:8px">模拟伊辛机（AIMs）作为一种新兴的组合优化范式，利用物理动力学高效求解伊辛问题。然而，传统优化与采样算法在该平台上的性能常受固有测量噪声限制。本文提出BRAIN（模拟伊辛网络的玻尔兹曼强化学习框架），该分布学习框架采用变分强化学习逼近玻尔兹曼分布。通过将逐状态采样转为聚合多组噪声测量信息，BRAIN对AIMs典型的高斯噪声具有鲁棒性。我们在多种组合拓扑（包括居里-外斯模型与二维最近邻伊辛系统）中验证BRAIN的性能。实验表明，在3%高斯测量噪声下，BRAIN保持98%基态保真度，而马尔可夫链蒙特卡洛方法降至51%。此外，在此条件下BRAIN获得等效解的速度提升达192倍。BRAIN在65,536个自旋规模下呈现$\mathcal{O}(N^{1.55})$的复杂度扩展，并在高达40%的严重测量不确定度下保持稳定性。除基态优化外，BRAIN能精确捕捉热力学相变与亚稳态，为复杂优化中模拟计算架构的应用提供了可扩展且抗噪声的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the performance degradation of analog Ising machines (AIMs) due to inherent measurement noise during combinatorial optimization. The authors propose BRAIN, a distribution learning framework that uses variational reinforcement learning to approximate the Boltzmann distribution, aggregating information from multiple noisy measurements to achieve noise resilience. Experimental results on systems like Curie-Weiss and 2D Ising models show that under 3% Gaussian noise, BRAIN maintains 98% ground state fidelity and reaches solutions up to 192 times faster than Markov Chain Monte Carlo methods, while also scaling efficiently to large systems and accurately capturing thermodynamic properties.</div>
<div class="mono" style="margin-top:8px">本文针对模拟伊辛机在组合优化中因固有测量噪声导致的性能下降问题，提出了BRAIN这一分布学习框架。该方法利用变分强化学习来近似玻尔兹曼分布，通过聚合多次噪声测量的信息实现噪声鲁棒性。在Curie-Weiss和二维伊辛等模型上的实验表明，在3%高斯噪声下，BRAIN能保持98%的基态保真度，且求解速度比马尔可夫链蒙特卡洛方法快达192倍，同时能高效扩展到大规模自旋系统并准确捕捉热力学相变等特性。</div>
</details>
</div>
<div class="card">
<div class="title">What Do You Need for Compositional Generalization in Diffusion Planning?</div>
<div class="meta-line">Authors: Quentin Clark, Florian Shkurti</div>
<div class="meta-line">First: 2025-05-23T16:41:08+00:00 · Latest: 2026-02-09T19:49:59+00:00</div>
<div class="meta-line">Comments: 8 Pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18083v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18083v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In policy learning, stitching and compositional generalization refer to the extent to which the policy is able to piece together sub-trajectories of data it is trained on to generate new and diverse behaviours. While stitching has been identified as a significant strength of offline reinforcement learning, recent generative behavioural cloning (BC) methods have also shown proficiency at stitching. However, the main factors behind this are poorly understood, hindering the development of new algorithms that can reliably stitch by design. Focusing on diffusion planners trained via generative behavioural cloning, and without resorting to dynamic programming or TD-learning, we find three properties are key enablers for composition: shift equivariance, local receptive fields, and inference choices. We use these properties to explain architecture, data, and inference choices in existing generative BC methods based on diffusion planning including replanning frequency, data augmentation, and data scaling. Our experiments show that while local receptive fields are more important than shift equivariance in creating a diffusion planner capable of composition, both are crucial. Using findings from our experiments, we develop a new architecture for diffusion planners called Eq-Net, that is simple, produces diverse trajectories competitive with more computationally expensive methods such as replanning or scaling data, and can be guided to enable generalization in goal-conditioned settings. We show that Eq-Net exhibits significant compositional generalization in a variety of navigation and manipulation tasks designed to test planning diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散规划中实现组合泛化需要什么？</div>
<div class="mono" style="margin-top:8px">在策略学习中，拼接与组合泛化指策略将训练数据的子轨迹组合以生成新颖多样行为的能力。尽管拼接已被视为离线强化学习的重要优势，近期生成式行为克隆方法也展现出拼接能力，但其背后的关键因素尚不明确，阻碍了设计可靠拼接算法的发展。聚焦于通过生成式行为克隆训练的扩散规划器，在不依赖动态规划或时序差分学习的情况下，我们发现三个属性是实现组合的关键：平移等变性、局部感受野和推理选择。利用这些属性，我们解释了现有基于扩散规划的生成式行为克隆方法在架构、数据和推理上的选择，包括重规划频率、数据增强和数据缩放。实验表明，在构建具备组合能力的扩散规划器时，局部感受野比平移等变性更重要，但两者均不可或缺。基于实验结果，我们提出一种名为Eq-Net的新型扩散规划器架构，其结构简单，能生成与计算成本更高的方法（如重规划或数据缩放）相竞争的多样化轨迹，并可通过引导实现目标条件设定下的泛化。实验证明，Eq-Net在多种为测试规划多样性设计的导航与操作任务中展现出显著的组合泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the factors enabling compositional generalization in diffusion planners trained via generative behavioral cloning, aiming to understand how such models can stitch together sub-trajectories to produce novel behaviors. The authors identify three key enablers: shift equivariance, local receptive fields, and inference choices, which they use to explain architectural and training decisions like replanning frequency and data augmentation. Experimental results reveal that while local receptive fields are more critical than shift equivariance for composition, both are essential; based on these findings, the authors propose Eq-Net, a new diffusion planner architecture that generates diverse trajectories competitively without extensive computation and demonstrates strong compositional generalization in navigation and manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文研究了通过生成式行为克隆训练的扩散规划器中实现组合泛化的因素，旨在理解此类模型如何拼接子轨迹以产生新行为。作者确定了三个关键促成因素：平移等变性、局部感受野和推理选择，并用它们解释了如重规划频率和数据增强等架构与训练决策。实验结果表明，虽然局部感受野比平移等变性对组合能力更重要，但两者都必不可少；基于这些发现，作者提出了Eq-Net这一新的扩散规划器架构，它无需大量计算即可生成具有竞争力的多样化轨迹，并在导航和操作任务中展现出显著的组合泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">iGRPO: Self-Feedback-Driven LLM Reasoning</div>
<div class="meta-line">Authors: Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping, Yejin Choi, Jan Kautz</div>
<div class="meta-line">First: 2026-02-09T18:45:11+00:00 · Latest: 2026-02-09T18:45:11+00:00</div>
<div class="meta-line">Comments: Tech report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>iGRPO：基于自我反馈驱动的大语言模型推理方法</div>
<div class="mono" style="margin-top:8px">大语言模型在解决复杂数学问题上展现出潜力，但其生成结果的准确性与一致性仍有不足。强化学习通过任务特定奖励对齐模型，可提升整体质量与可靠性。组相对策略优化是一种无需价值函数的高效方法，替代近端策略优化并利用组相对奖励归一化。本文提出迭代组相对策略优化，作为GRPO的两阶段扩展，通过模型生成草稿实现动态自我条件化。第一阶段，iGRPO采样多个探索性草稿，并使用与优化相同的标量奖励信号选取最高奖励草稿。第二阶段，将最优草稿附加至原始提示，并对草稿条件化改进执行GRPO式更新，训练策略超越其先前最佳尝试。在相同计算预算下，iGRPO在多个基础模型上均稳定优于GRPO。此外，将iGRPO应用于经AceReason-Math训练的OpenReasoning-Nemotron-7B模型，在AIME24和AIME25基准上分别取得85.62%和79.64%的最新最优结果。消融实验进一步表明：改进框架可泛化至GRPO变体之外，受益于生成式评判器，并通过延迟熵崩溃改变学习动态。这些结果凸显了基于迭代自我反馈的强化学习在推进可验证数学推理方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces iGRPO, an iterative reinforcement learning method aimed at improving the accuracy and consistency of large language models (LLMs) in complex mathematical reasoning tasks, addressing their current limitations. The method extends Group Relative Policy Optimization (GRPO) by adding a two-stage process: first, it samples multiple exploratory drafts and selects the highest-reward one using a scalar reward signal; second, it appends this best draft to the original prompt and applies GRPO-style updates on draft-conditioned refinements, enabling the model to self-improve beyond its prior attempts. Experimental results show that iGRPO consistently outperforms GRPO across various base models under matched rollout budgets, and when applied to OpenReasoning-Nemotron-7B, it achieves state-of-the-art scores of 85.62% and 79.64% on the AIME24 and AIME25 benchmarks, respectively, with ablations confirming its generalization benefits and impact on learning dynamics.</div>
<div class="mono" style="margin-top:8px">本文提出了iGRPO，一种迭代强化学习方法，旨在提升大语言模型在复杂数学推理任务中的准确性和一致性，以解决其现有不足。该方法扩展了组相对策略优化（GRPO），采用两阶段流程：首先，采样多个探索性草稿并使用标量奖励信号选择奖励最高的草稿；其次，将此最佳草稿附加到原始提示后，在草稿条件化的改进上应用GRPO式更新，使模型能够基于先前尝试自我提升。实验结果表明，在匹配的采样预算下，iGRPO在不同基础模型上均优于GRPO，且应用于OpenReasoning-Nemotron-7B模型时，在AIME24和AIME25基准测试中分别达到了85.62%和79.64%的最新最优成绩，消融实验进一步验证了其泛化能力和对学习动态的影响。</div>
</details>
</div>
<div class="card">
<div class="title">UI-Venus-1.5 Technical Report</div>
<div class="meta-line">Authors: Veuns-Team, :, Changlong Gao, Zhangxuan Gu, Yulin Liu, Xinyu Qiu, Shuheng Shen, Yue Wen, Tianyu Xia, Zhenyu Xu, Zhengwen Zeng, Beitong Zhou, Xingran Zhou, Weizhi Chen, Sunhao Dai, Jingya Dou, Yichen Gong, Yuan Guo, Zhenlin Guo, Feng Li, Qian Li, Jinzhen Lin, Yuqi Zhou, Linchao Zhu, Liang Chen, Zhenyu Guo, Changhua Meng, Weiqiang Wang</div>
<div class="meta-line">First: 2026-02-09T18:43:40+00:00 · Latest: 2026-02-09T18:43:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09082v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09082v1">PDF</a> · <a href="https://github.com/inclusionAI/UI-Venus">Code1</a> · <a href="https://huggingface.co/collections/inclusionAI/ui-venus">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UI-Venus-1.5技术报告</div>
<div class="mono" style="margin-top:8px">GUI代理已成为自动化数字环境交互的强大范式，但实现广泛通用性与持续强劲的任务性能仍具挑战。本报告介绍了UI-Venus-1.5，一个为稳健现实世界应用设计的统一端到端GUI代理。该模型系列包含两个密集变体（2B和8B）及一个专家混合变体（30B-A3B），以适应多样下游应用场景。相较于前代版本，UI-Venus-1.5引入三项关键技术进展：（1）利用30余个数据集的100亿token进行综合中期训练，建立基础GUI语义；（2）采用全轨迹推演的在线强化学习，使训练目标与大规模环境中的长时程动态导航对齐；（3）通过模型融合构建单一统一GUI代理，将领域专用模型（基础定位、网页与移动端）合成为连贯的检查点。大量评估表明，UI-Venus-1.5在ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和AndroidWorld（77.6%）等基准测试中创下性能新纪录，显著超越先前强基线。此外，UI-Venus-1.5在多种中文移动应用中展现出稳健的导航能力，能在真实场景中有效执行用户指令。代码：https://github.com/inclusionAI/UI-Venus；模型：https://huggingface.co/collections/inclusionAI/ui-venus</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind UI-Venus-1.5 is to address the challenge of creating GUI agents that are both broadly generalizable and highly effective at real-world tasks. The method involves developing a unified, end-to-end agent family with dense and mixture-of-experts variants, incorporating three key advances: a mid-training stage on extensive data to build GUI semantics, online reinforcement learning for long-horizon navigation, and model merging to combine domain-specific expertise into a single checkpoint. Experimental results show state-of-the-art performance on benchmarks like ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), along with robust navigation in Chinese mobile apps.</div>
<div class="mono" style="margin-top:8px">UI-Venus-1.5的研发动机是解决图形用户界面（GUI）智能体在广泛通用性和强大任务性能之间难以兼顾的挑战。该方法提出了一个统一的端到端智能体系列，包括密集型和混合专家型变体，并引入了三项关键技术：利用大规模数据进行中期训练以建立GUI语义基础、采用在线强化学习进行长程动态导航、以及通过模型合并将领域特定模型融合为单一检查点。主要实验结果表明，该模型在ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和AndroidWorld（77.6%）等基准测试中取得了领先性能，并在多种中国移动应用中展现出强大的导航能力。</div>
</details>
</div>
<div class="card">
<div class="title">f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment</div>
<div class="meta-line">Authors: Rajdeep Haldar, Lantao Mei, Guang Lin, Yue Xing, Qifan Song</div>
<div class="meta-line">First: 2026-02-05T18:01:52+00:00 · Latest: 2026-02-09T18:34:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05946v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05946v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>f-GRPO及其拓展：基于散度的通用大语言模型对齐强化学习算法</div>
<div class="mono" style="margin-top:8px">近期研究表明，偏好对齐目标可视为对齐（选中）与未对齐（拒绝）响应分布间的散度估计器。本研究将这一基于散度的视角拓展至通用对齐场景，例如仅依赖环境奖励的可验证奖励强化学习。在此统一框架下，我们提出了基于f-散度变分表示的通用大语言模型对齐方法：f-群体相对策略优化（一类在线策略强化学习算法）与f-混合对齐损失（融合在线/离线策略的混合目标）。理论分析证明这些目标能提升对齐后的平均奖励。实验部分在可验证奖励强化学习（数学推理）和偏好对齐任务（安全对齐）上验证了框架的有效性，其性能与灵活性均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the insight that preference alignment objectives can be viewed as divergence estimators, this work extends a divergence-based framework to general language model alignment, including scenarios with only environmental rewards. The method introduces f-GRPO, an on-policy reinforcement learning class, and f-HAL, a hybrid objective, both derived from variational representations of f-divergences to optimize alignment. Experimental results on math reasoning and safety alignment tasks demonstrate that the proposed algorithms achieve superior performance and flexibility compared to existing methods, with theoretical guarantees of reward improvement.</div>
<div class="mono" style="margin-top:8px">本研究基于偏好对齐目标可作为分布散度估计器的见解，将散度框架扩展至通用大语言模型对齐，包括仅有环境奖励的场景。方法上提出了基于f散度变分表示的f-GRPO在线策略强化学习算法和f-HAL混合目标，用于优化模型对齐。在数学推理和安全对齐任务上的实验结果表明，所提算法相比现有方法具有更优的性能和灵活性，并具备理论上的奖励提升保证。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: John Gardiner, Orlando Romero, Brendan Tivnan, Nicolò Dal Fabbro, George J. Pappas</div>
<div class="meta-line">First: 2026-02-09T18:01:40+00:00 · Latest: 2026-02-09T18:01:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中基于量子纠缠的协同学习</div>
<div class="mono" style="margin-top:8px">通信缺失是多智能体强化学习（MARL）协同面临的主要挑战。先前研究探索通过共享随机性（有时以关联设备形式）关联局部策略，作为辅助分散决策的机制。与此不同，本研究首次提出训练MARL智能体利用共享量子纠缠作为协同资源的框架，该框架允许比单纯共享随机性更广泛的免通信关联策略类别。这一思路源于量子物理学中的经典结论：对于某些无通信的单轮协作博弈，共享量子纠缠能实现优于仅使用共享随机性的策略，此类情况称为量子优势。我们的框架基于新型可微分策略参数化方法（支持对量子测量的优化）与创新的策略架构（将联合策略分解为量子协调器与分散式局部执行器）。为验证方法的有效性，我们首先证明能够纯从经验中学习在单轮博弈（视为黑盒预言机）中实现量子优势的策略；进而展示该机制如何在以分散部分可观测马尔可夫决策过程（Dec-POMDP）建模的示例性多智能体序贯决策问题中学习具有量子优势的策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of coordination in multi-agent reinforcement learning (MARL) without communication, where prior methods rely on shared randomness. The authors introduce a novel framework that enables MARL agents to exploit shared quantum entanglement as a coordination resource, proposing a differentiable policy parameterization for optimizing quantum measurements and a policy architecture combining a quantum coordinator with decentralized local actors. Experimentally, the method successfully learns strategies that achieve quantum advantage in single-round cooperative games treated as black boxes and also demonstrates this advantage in a sequential Dec-POMDP setting.</div>
<div class="mono" style="margin-top:8px">本文的动机是在无需通信的多智能体强化学习中解决协调难题，而现有方法主要依赖共享随机性。作者提出了一种新颖框架，使智能体能利用共享量子纠缠作为协调资源，引入了可优化量子测量的可微分策略参数化方法，以及将联合策略分解为量子协调器和分散局部执行器的策略架构。实验结果表明，该方法能在被视为黑盒的单轮合作游戏中学习到实现量子优势的策略，并在一个序列化的分散部分可观测马尔可夫决策过程示例中同样展示了量子优势。</div>
</details>
</div>
<div class="card">
<div class="title">Delay-Aware Reinforcement Learning for Highway On-Ramp Merging under Stochastic Communication Latency</div>
<div class="meta-line">Authors: Amin Tabrizian, Zhitong Huang, Arsyi Aziz, Peng Wei</div>
<div class="meta-line">First: 2024-03-18T15:02:46+00:00 · Latest: 2026-02-09T17:51:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.11852v4">Abs</a> · <a href="https://arxiv.org/pdf/2403.11852v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Delayed and partially observable state information poses significant challenges for reinforcement learning (RL)-based control in real-world autonomous driving. In highway on-ramp merging, a roadside unit (RSU) can sense nearby traffic, perform edge perception, and transmit state estimates to the ego vehicle over vehicle-to-infrastructure (V2I) links. With recent advancements in intelligent transportation infrastructure and edge computing, such RSU-assisted perception is increasingly realistic and already deployed in modern connected roadway systems. However, edge processing time and wireless transmission can introduce stochastic V2I communication delays, violating the Markov assumption and substantially degrading control performance. In this work, we propose DAROM, a Delay-Aware Reinforcement Learning framework for On-ramp Merging that is robust to stochastic delays. We model the problem as a random delay Markov decision process (RDMDP) and develop a unified RL agent for joint longitudinal and lateral control. To recover a Markovian representation under delayed observations, we introduce a Delay-Aware Encoder that conditions on delayed observations, masked action histories, and observed delay magnitude to infer the current latent state. We further integrate a physics-based safety controller to reduce collision risk during merging. Experiments in the Simulation of Urban MObility (SUMO) simulator using real-world traffic data from the Next Generation Simulation (NGSIM) dataset demonstrate that DAROM consistently outperforms standard RL baselines across traffic densities. In particular, the gated recurrent unit (GRU)-based encoder achieves over 99% success in high-density traffic with random V2I delays of up to 2.0 seconds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机通信延迟下高速公路匝道汇入的延迟感知强化学习</div>
<div class="mono" style="margin-top:8px">延迟且部分可观测的状态信息对基于强化学习（RL）的现实世界自动驾驶控制提出了重大挑战。在高速公路匝道汇入场景中，路侧单元（RSU）能够感知附近交通、执行边缘感知，并通过车路协同（V2I）链路将状态估计传输给主车。随着智能交通基础设施和边缘计算的最新进展，此类RSU辅助感知日益可行，并已部署于现代网联道路系统中。然而，边缘处理时间和无线传输可能引入随机V2I通信延迟，破坏马尔可夫假设并显著降低控制性能。本研究提出DAROM——一种针对随机延迟具有鲁棒性的匝道汇入延迟感知强化学习框架。我们将问题建模为随机延迟马尔可夫决策过程（RDMDP），并开发了用于纵向与横向联合控制的统一RL智能体。为在延迟观测下恢复马尔可夫表示，我们引入延迟感知编码器，该编码器以延迟观测、掩码动作历史及观测延迟量为条件来推断当前潜在状态。我们进一步集成基于物理的安全控制器以降低汇入过程中的碰撞风险。基于下一代仿真（NGSIM）数据集真实交通数据，在SUMO仿真平台进行的实验表明，DAROM在不同交通密度下均持续优于标准RL基线方法。特别地，基于门控循环单元（GRU）的编码器在高达2.0秒随机V2I延迟的高密度交通场景中实现了超过99%的成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of stochastic communication delays in vehicle-to-infrastructure (V2I) links, which degrade reinforcement learning (RL) control for autonomous highway on-ramp merging by breaking the Markov assumption. The authors propose DAROM, a delay-aware RL framework that models the problem as a random delay Markov decision process and employs a delay-aware encoder to infer the current latent state from delayed observations, action histories, and observed delay magnitudes, alongside a safety controller for collision risk reduction. Experimental validation in the SUMO simulator using NGSIM traffic data shows that DAROM, particularly with a GRU-based encoder, outperforms standard RL baselines across traffic densities, achieving over 99% success in high-density traffic even with random delays up to 2.0 seconds.</div>
<div class="mono" style="margin-top:8px">本文针对车辆与基础设施（V2I）通信中随机延迟导致强化学习在高速公路匝道汇入控制中性能下降的问题，提出了一种延迟感知的强化学习框架DAROM。该方法将问题建模为随机延迟马尔可夫决策过程，通过延迟感知编码器结合延迟观测、动作历史和观测延迟幅度来推断当前潜在状态，并集成基于物理的安全控制器以降低碰撞风险。在SUMO模拟器中使用NGSIM真实交通数据进行的实验表明，DAROM在不同交通密度下均优于标准强化学习基线，其中基于门控循环单元（GRU）的编码器在高达2.0秒的随机V2I延迟下，于高密度交通中实现了超过99%的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning</div>
<div class="meta-line">Authors: Ethan Rathbun, Wo Wei Lin, Alina Oprea, Christopher Amato</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-04T22:17:23+00:00 · Latest: 2026-02-09T17:46:50+00:00</div>
<div class="meta-line">Comments: 10 pages main body, ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05089v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05089v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger&#x27;&#x27;, leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent&#x27;s training pipeline, enabling them to both alter and observe agent&#x27;s rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze&#x27;&#x27; which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze&#x27;s effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>警惕不可信模拟器——强化学习中的无奖励后门攻击</div>
<div class="mono" style="margin-top:8px">模拟环境是强化学习（RL）成功的关键，使从业者和研究人员无需在真实硬件上进行昂贵实验即可训练决策智能体。然而，模拟器仍是一个安全盲区，恶意开发者可通过修改其发布的模拟器动态实现有害目的。为此，本文揭示了一种新型威胁，展示了如何利用模拟器动态在RL智能体中隐秘植入动作级后门。该后门使攻击者能在观察到预定义“触发器”时，可靠地激活智能体中的目标动作，可能导致危险后果。传统后门攻击受限于其强威胁模型，假设攻击者几乎完全控制智能体训练流程，既能修改也能观察其奖励。由于这些假设在模拟器中难以实现，我们提出新型攻击方法“Daze”，该方法能在不修改甚至不观察奖励的情况下，可靠且隐蔽地向面向真实世界任务的RL智能体植入后门。我们通过形式化证明验证了Daze在通用RL任务中保证攻击成功的有效性，并在离散与连续动作空间领域进行了广泛实证评估。此外，我们首次展示了RL后门攻击可迁移至真实机器人硬件。这些进展推动了对RL训练流程全组件安全防护的进一步研究，以防范恶意攻击。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses a security vulnerability in reinforcement learning (RL) where untrusted simulators can be maliciously altered to implant stealthy backdoors into trained agents, posing risks for real-world deployment. The proposed method, &#x27;Daze&#x27;, is a reward-free backdoor attack that exploits simulator dynamics to embed triggers without needing to observe or modify the agent&#x27;s reward signals, overcoming limitations of prior attacks that required full control over the training pipeline. Experimental results demonstrate the attack&#x27;s effectiveness across both discrete and continuous action spaces, supported by formal proofs, and include the first documented case of an RL backdoor successfully transferring to real robotic hardware, highlighting the need for securing the entire RL training pipeline.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的安全漏洞展开研究，指出不可信的模拟器可能被恶意篡改，从而在训练好的智能体中植入隐蔽的后门，对实际部署构成风险。所提出的方法&#x27;Daze&#x27;是一种无需奖励的后门攻击，通过利用模拟器动态嵌入触发机制，无需观察或修改智能体的奖励信号，克服了以往攻击需完全控制训练流程的限制。实验结果表明，该攻击在离散和连续动作空间均有效，并有形式化证明支持，且首次记录了强化学习后门成功迁移到真实机器人硬件的案例，强调了保障整个强化学习训练流程安全的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors</div>
<div class="meta-line">Authors: Suraj Ranganath, Atharv Ramesh</div>
<div class="meta-line">First: 2026-02-09T17:33:46+00:00 · Latest: 2026-02-09T17:33:46+00:00</div>
<div class="meta-line">Comments: Expanded version of a workshop submission. Code available</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08934v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08934v1">PDF</a> · <a href="https://github.com/suraj-ranganath/StealthRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StealthRL：面向多检测器规避的强化学习改写攻击对抗AI文本检测器</div>
<div class="mono" style="margin-top:8px">AI文本检测器面临严峻的鲁棒性挑战：对抗性改写攻击能在保持语义的同时规避检测。本文提出StealthRL强化学习框架，在真实对抗条件下对检测器鲁棒性进行压力测试。该方法基于Qwen3-4B模型，采用带LoRA适配器的组相对策略优化（GRPO）训练针对多检测器集成系统的改写策略，通过平衡检测规避与语义保持的复合奖励函数进行优化。我们在安全相关的1%误报率工作点上评估了六种攻击配置（M0-M5）对抗三类检测器家族（RoBERTa、FastDetectGPT和Binoculars）。StealthRL实现了接近零的检测率（TPR@1%FPR均值0.001），将平均AUROC从0.74降至0.27，攻击成功率高达99.9%。关键的是，攻击能迁移到训练中未见的检测器家族，揭示了共享架构漏洞而非检测器特定脆弱性。我们还通过Likert评分进行基于LLM的质量评估，分析检测器分数分布以解释规避成功原因，并提供带自助法置信区间的各检测器AUROC。研究结果暴露了当前AI文本检测的重大鲁棒性缺陷，并将StealthRL确立为原则性的对抗评估协议。代码与评估流程已开源：https://github.com/suraj-ranganath/StealthRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the robustness challenge of AI-text detectors against adversarial paraphrasing attacks, this paper introduces StealthRL, a reinforcement learning framework that trains a paraphrase policy using Group Relative Policy Optimization with LoRA adapters on Qwen3-4B to evade a multi-detector ensemble while preserving semantics. The method optimizes a composite reward for evasion and semantic fidelity, and experimental results across six attack settings against three detector families show it reduces mean detection to near-zero (0.001 TPR@1%FPR), lowers mean AUROC from 0.74 to 0.27, and achieves a 99.9% attack success rate, with attacks successfully transferring to an unseen detector family, revealing shared architectural vulnerabilities.</div>
<div class="mono" style="margin-top:8px">本文针对AI文本检测器在对抗性改述攻击下的鲁棒性挑战，提出了StealthRL框架，该方法采用强化学习，通过基于Qwen3-4B的LoRA适配器和组相对策略优化训练改述策略，以在保持语义的同时规避多检测器集成。该方法优化了兼顾规避和语义保真的复合奖励，实验在六种攻击设置下针对三个检测器家族进行，结果显示其将平均检测率降至接近零（0.001 TPR@1%FPR），将平均AUROC从0.74降至0.27，攻击成功率达99.9%，且攻击能迁移到训练中未见的检测器家族，揭示了共享的架构脆弱性。</div>
</details>
</div>
<div class="card">
<div class="title">Fast and robust parametric and functional learning with Hybrid Genetic Optimisation (HyGO)</div>
<div class="meta-line">Authors: Isaac Robledo, Yiqing Li, Guy Y. Cornejo Maceda, Rodrigo Castellanos</div>
<div class="meta-line">First: 2025-10-10T13:45:32+00:00 · Latest: 2026-02-09T17:22:16+00:00</div>
<div class="meta-line">Comments: 37 pages, 16 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09391v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09391v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Hybrid Genetic Optimisation framework (HYGO) is introduced to meet the pressing need for efficient and unified optimisation frameworks that support both parametric and functional learning in complex engineering problems. Evolutionary algorithms are widely employed as derivative-free global optimisation methods but often suffer from slow convergence rates, especially during late-stage learning. HYGO integrates the global exploration capabilities of evolutionary algorithms with accelerated local search for robust solution refinement. The key enabler is a two-stage strategy that balances exploration and exploitation. For parametric problems, HYGO alternates between genetic algorithm and targeted improvement through a degeneracy-proof Dowhill Simplex Method (DSM). For function optimisation tasks, HYGO rotates between genetic programming and DSM. Validation is performed on (a) parametric optimisation benchmarks, where HYGO demonstrates faster and more robust convergence than standard genetic algorithms, and (b) function optimisation tasks, including control of a damped Landau oscillator. Practical relevance is showcased through aerodynamic drag reduction of an Ahmed body via Reynolds-Averaged Navier-Stokes simulations, achieving consistently interpretable results and reductions exceeding 20% by controlled jet injection in the back of the body for flow reattachment and separation bubble reduction. Overall, HYGO emerges as a versatile hybrid optimisation framework suitable for a broad spectrum of engineering and scientific problems involving parametric and functional learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于混合遗传优化（HyGO）的快速鲁棒参数与函数学习</div>
<div class="mono" style="margin-top:8px">为满足复杂工程问题中对支持参数与函数学习的高效统一优化框架的迫切需求，本文提出混合遗传优化框架（HYGO）。进化算法作为无导数全局优化方法被广泛采用，但常存在收敛速度慢的问题，尤其在后期学习阶段。HYGO将进化算法的全局探索能力与加速局部搜索相结合，实现鲁棒的解优化。其关键机制是平衡探索与利用的两阶段策略：针对参数优化问题，HYGO在遗传算法与抗退化下山单纯形法（DSM）之间交替进行；针对函数优化任务，则在遗传编程与DSM之间轮转。验证工作包括：（a）参数优化基准测试，显示HYGO比标准遗传算法收敛更快更鲁棒；（b）函数优化任务，包括阻尼朗道振荡器控制。通过Ahmed车身气动减阻的雷诺平均Navier-Stokes模拟展示了实际应用价值，通过在车尾受控喷射实现流动再附着与分离泡减小，获得可解释性结果且减阻率超过20%。总体而言，HYGO成为一种适用于涉及参数与函数学习的广泛工程与科学问题的通用混合优化框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient and unified optimization frameworks for complex engineering problems involving both parametric and functional learning, this paper introduces the Hybrid Genetic Optimization (HyGO) framework. The method integrates evolutionary algorithms for global exploration with accelerated local search, specifically employing a two-stage strategy that alternates between a genetic algorithm (or genetic programming for functional tasks) and a degeneracy-proof Downhill Simplex Method for refinement. Experimental results demonstrate that HyGO achieves faster and more robust convergence than standard genetic algorithms on parametric benchmarks and function optimization tasks, including controlling a damped Landau oscillator; a practical aerodynamic drag reduction application on an Ahmed body via simulations shows consistent interpretability and drag reductions exceeding 20% through controlled jet injection.</div>
<div class="mono" style="margin-top:8px">本文针对复杂工程问题中参数与函数学习对高效统一优化框架的迫切需求，提出了混合遗传优化（HyGO）框架。该方法将进化算法的全局探索能力与加速局部搜索相结合，采用两阶段策略，在参数优化中交替使用遗传算法和抗退化下山单纯形法进行改进，在函数优化中则轮换遗传规划和单纯形法。实验结果表明，在参数优化基准测试和函数优化任务（如阻尼朗道振荡器控制）中，HyGO比标准遗传算法收敛更快、更稳健；在一个实际空气动力学应用中，通过对Ahmed车身进行模拟和受控射流注入，实现了结果可解释且阻力降低超过20%的减阻效果。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Stable Reinforcement Learning for Diffusion Language Models</div>
<div class="meta-line">Authors: Jiawei Liu, Xiting Wang, Yuanyuan Zhong, Defu Lian, Yu Yang</div>
<div class="meta-line">First: 2026-02-09T17:04:23+00:00 · Latest: 2026-02-09T17:04:23+00:00</div>
<div class="meta-line">Comments: 13 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08905v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08905v1">PDF</a> · <a href="https://github.com/Lolo1222/STP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散语言模型的高效稳定强化学习方法</div>
<div class="mono" style="margin-top:8px">强化学习（RL）对于释放基于扩散的大语言模型（dLLMs）的复杂推理能力至关重要。然而，将RL应用于dLLMs在效率和稳定性方面面临独特挑战。为解决这些挑战，我们提出了时空剪枝（STP）框架，旨在同时提升dLLMs强化学习的效率与稳定性。STP通过以下方式压缩生成过程中的冗余：（1）空间剪枝：利用静态先验约束探索空间；（2）时间剪枝：跳过冗余的后期优化步骤。理论分析表明，STP能严格降低对数似然估计的方差，从而确保更稳定的策略更新。大量实验证明，STP在效率和准确性上均超越现有先进基线。代码发布于https://github.com/Lolo1222/STP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the efficiency and stability challenges of applying reinforcement learning (RL) to diffusion-based large language models (dLLMs), which are crucial for enhancing their reasoning capabilities. The authors propose the Spatio-Temporal Pruning (STP) framework, which improves RL by compressing redundancy in the generative process through two mechanisms: spatial pruning, which uses static priors to constrain the exploration space, and temporal pruning, which bypasses unnecessary late-stage refinement steps. Theoretically, STP reduces the variance of log-likelihood estimation for more stable policy updates. Experimental results show that STP outperforms state-of-the-art baselines in both efficiency and accuracy.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在扩散大语言模型应用中存在的效率与稳定性挑战展开研究，旨在提升模型的复杂推理能力。作者提出了时空剪枝框架，通过两种机制压缩生成过程中的冗余：空间剪枝利用静态先验约束探索空间，时间剪枝则跳过后期冗余的优化步骤。理论分析表明，该框架能严格降低对数似然估计的方差，从而确保更稳定的策略更新。大量实验证明，该方法在效率和准确性上均超越了现有先进基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Yuchen Yan, Liang Jiang, Jin Jiang, Shuaicheng Li, Zujie Wen, Zhiqiang Zhang, Jun Zhou, Jian Shao, Yueting Zhuang, Yongliang Shen</div>
<div class="meta-line">First: 2026-02-06T18:59:27+00:00 · Latest: 2026-02-09T17:01:31+00:00</div>
<div class="meta-line">Comments: Project Page: https://zju-real.github.io/InftyThink-Plus Code: https://github.com/ZJU-REAL/InftyThink-Plus</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06960v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06960v2">PDF</a> · <a href="https://github.com/ZJU-REAL/InftyThink-Plus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zju-real.github.io/InftyThink-Plus">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InftyThink+：基于强化学习的有效高效无限视野推理方法</div>
<div class="mono" style="margin-top:8px">大型推理模型通过扩展推理时思维链获得优异性能，但该范式存在二次成本、上下文长度限制及因&#x27;中间迷失效应&#x27;导致的推理质量下降等问题。迭代推理通过定期总结中间思路缓解了这些问题，然而现有方法依赖监督学习或固定启发式规则，未能优化何时总结、保留哪些信息以及如何恢复推理。我们提出InftyThink+——一个端到端的强化学习框架，基于模型控制的迭代边界和显式总结机制，优化整个迭代推理轨迹。InftyThink+采用两阶段训练方案：监督式冷启动后接轨迹级强化学习，使模型能学习策略性总结与续推决策。在DeepSeek-R1-Distill-Qwen-1.5B上的实验表明，InftyThink+在AIME24上准确率提升21%，显著优于传统长思维链强化学习方法，同时在分布外基准测试中展现更强泛化能力。此外，InftyThink+大幅降低推理延迟并加速强化学习训练，在提升性能的同时显著提高推理效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiencies and limitations of scaling inference-time chain-of-thought reasoning, such as high computational cost and degraded performance due to lost-in-the-middle effects, this paper introduces InftyThink+, a reinforcement learning framework designed to optimize iterative reasoning over infinite horizons. The method employs a two-stage training approach, starting with supervised cold-start followed by trajectory-level reinforcement learning, to enable strategic decisions on when to summarize intermediate thoughts and how to resume reasoning. Experimental results on DeepSeek-R1-Distill-Qwen-1.5B demonstrate that InftyThink+ improves accuracy by 21% on AIME24, outperforms conventional long chain-of-thought reinforcement learning, generalizes better to out-of-distribution tasks, and significantly reduces inference latency while accelerating training.</div>
<div class="mono" style="margin-top:8px">本文针对推理时思维链扩展存在的计算成本高、上下文长度限制及因中间信息丢失导致的性能下降等问题，提出了InftyThink+，一个通过强化学习优化无限视野迭代推理的框架。该方法采用两阶段训练策略，先进行监督式冷启动，再进行轨迹级强化学习，以学习何时总结中间思维、保留何种信息以及如何恢复推理的策略性决策。在DeepSeek-R1-Distill-Qwen-1.5B上的实验表明，InftyThink+在AIME24上准确率提升21%，优于传统的长思维链强化学习方法，在分布外基准上泛化能力更强，同时显著降低了推理延迟并加速了训练过程。</div>
</details>
</div>
<div class="card">
<div class="title">AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection</div>
<div class="meta-line">Authors: Junru Zhang, Lang Feng, Haoran Shi, Xu Guo, Han Yu, Yabo Dong, Duanqing Xu</div>
<div class="meta-line">First: 2026-02-09T16:30:13+00:00 · Latest: 2026-02-09T16:30:13+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnomSeer：强化多模态大语言模型进行时序异常检测推理</div>
<div class="mono" style="margin-top:8px">基于多模态大语言模型（MLLMs）的时序异常检测（TSAD）是一个新兴领域，但存在持续挑战：MLLMs依赖粗粒度时序启发式方法，却难以进行对理解复杂时序数据至关重要的多维度精细推理。我们提出AnomSeer，通过强化模型使其推理基于时序的精确结构细节，统一异常分类、定位与解释。其核心是生成专家思维链轨迹，从经典分析（如统计度量、频域变换）提供可验证的细粒度推理。在此基础上，我们提出一种新颖的时序基础策略优化方法（TimerPO），在标准强化学习之外引入两个组件：基于最优传输的时序基础优势函数，以及确保该辅助细粒度信号不干扰主要检测目标的正交投影。在多种异常场景下，采用Qwen2.5-VL-3B/7B-Instruct的AnomSeer在分类与定位准确率上超越更大规模的商业基线模型（如GPT-4o），尤其在点异常和频域驱动异常方面表现突出。此外，它能生成支持其结论的合理时序推理轨迹。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitation of multimodal large language models (MLLMs) in performing detailed, multi-dimensional reasoning for time-series anomaly detection (TSAD), which is crucial for handling complex data. The method introduces AnomSeer, which reinforces MLLMs by grounding reasoning in precise structural details through an expert chain-of-thought trace derived from classical analyses, and proposes a novel time-series grounded policy optimization (TimerPO) that incorporates an optimal transport-based advantage and orthogonal projection to avoid interference with detection objectives. Experimental results show that AnomSeer, implemented with Qwen2.5-VL-3B/7B-Instruct models, outperforms larger commercial baselines like GPT-4o in classification and localization accuracy, especially for point- and frequency-driven anomalies, while generating verifiable reasoning traces to support its conclusions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态大语言模型（MLLMs）在时间序列异常检测（TSAD）中难以进行多维度详细推理的问题，这对于理解复杂时间序列数据至关重要。方法上提出了AnomSeer，通过基于经典分析（如统计度量、频率变换）生成专家思维链轨迹，将推理过程锚定于时间序列的精确结构细节，并设计了一种新颖的时间序列锚定策略优化（TimerPO），结合基于最优传输的优势计算和正交投影，以确保辅助的细粒度信号不干扰主要检测目标。实验结果表明，使用Qwen2.5-VL-3B/7B-Instruct模型的AnomSeer在分类和定位准确性上优于GPT-4o等更大规模的商业基线模型，尤其在点和频率驱动的异常检测中表现突出，同时能生成合理的推理轨迹来支撑其结论。</div>
</details>
</div>
<div class="card">
<div class="title">Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems</div>
<div class="meta-line">Authors: Lang Feng, Longtao Zheng, Shuo He, Fuxiang Zhang, Bo An</div>
<div class="meta-line">First: 2026-02-09T16:13:39+00:00 · Latest: 2026-02-09T16:13:39+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08847v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08847v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents&#x27; reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent&#x27;s own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dr. MAS：面向多智能体大语言模型系统的稳定强化学习</div>
<div class="mono" style="margin-top:8px">多智能体大语言模型系统通过角色分工实现高级推理与工具调用，但针对此类系统的可靠强化学习后训练仍具挑战。本研究从理论上指出，将基于群体的强化学习扩展至多智能体大语言模型系统时，训练不稳定的关键原因在于：在GRPO式优化中，全局归一化基线可能偏离异构智能体的奖励分布，最终导致梯度范数失稳。基于此发现，我们提出Dr. MAS——一种简洁稳定的多智能体大语言模型系统强化学习训练方案。该方法采用智能体级修正策略：利用各智能体自身的奖励统计量对优势函数进行独立归一化，从而校准梯度尺度，在理论与实证层面显著提升训练稳定性。除算法外，Dr. MAS构建了端到端的多智能体大语言模型强化学习训练框架，支持可扩展的编排调度、灵活的智能体级大语言模型服务与优化配置，以及大语言模型执行器后端的共享资源调度。基于Qwen2.5与Qwen3系列模型，我们在多智能体数学推理与多轮搜索基准测试中评估Dr. MAS。相比原始GRPO，该方法在数学任务上实现平均指标提升5.6%、通过率提升4.6%，在搜索任务上实现平均指标提升15.2%、通过率提升13.1%，同时基本消除梯度尖峰现象。此外，在异构智能体-模型分配场景下，该方法仍保持高效性能并提升系统效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the training instability in multi-agent large language model (LLM) systems when using reinforcement learning (RL) post-training, which stems from a global reward normalization baseline misaligning with diverse agents&#x27; reward distributions. The method proposed, Dr. MAS, introduces an agent-wise remedy that normalizes advantages per agent using each agent&#x27;s own reward statistics to calibrate gradient scales and stabilize training, alongside providing an end-to-end RL training framework for scalable orchestration and flexible agent configurations. Experimental results on multi-agent math reasoning and multi-turn search benchmarks with Qwen models show that Dr. MAS achieves significant performance gains over vanilla GRPO, such as +5.6% average and +4.6% pass rate on math tasks, while largely eliminating gradient spikes and remaining effective under heterogeneous agent-model assignments.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多智能体大语言模型系统在使用强化学习进行后训练时的不稳定性问题，该问题源于全局奖励归一化基线无法适应不同智能体的奖励分布。提出的方法Dr. MAS采用了一种按智能体归一化的补救措施，即利用每个智能体自身的奖励统计量来归一化优势值，以校准梯度尺度并稳定训练，同时提供了一个端到端的强化学习训练框架，支持可扩展的编排和灵活的智能体配置。在基于Qwen模型的多智能体数学推理和多轮搜索基准测试中的实验结果表明，Dr. MAS相比原始GRPO取得了显著性能提升，例如在数学任务上平均提升5.6%且通过率提升4.6%，同时基本消除了梯度尖峰，并在异构智能体模型分配下保持高效。</div>
</details>
</div>
<div class="card">
<div class="title">Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning</div>
<div class="meta-line">Authors: Andrés Holgado-Sánchez, Peter Vamplew, Richard Dazeley, Sascha Ossowski, Holger Billhardt</div>
<div class="meta-line">First: 2026-02-09T16:06:36+00:00 · Latest: 2026-02-09T16:06:36+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures. To be published in proceedings of the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). This is a full version that includes the supplementary material</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08835v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏好多目标强化学习的社会价值体系学习</div>
<div class="mono" style="margin-top:8px">价值感知人工智能应能识别人类价值观并适应不同用户的价值体系（基于价值的偏好）。这需要对价值观进行操作化处理，但可能存在设定偏差。价值观的社会性要求其表征需兼顾多用户需求，而价值体系虽具多样性，却在群体间呈现规律性模式。在序列决策中，已有研究通过多样化智能体的示范行为，针对不同目标或价值观进行个性化适配。然而，这些方法或需人工设计特征，或缺乏基于价值的可解释性及对多元用户偏好的适应性。
我们提出基于聚类与偏好多目标强化学习的算法，用于在马尔可夫决策过程中学习智能体社会的价值对齐模型与价值体系。该方法联合学习社会衍生的价值对齐模型（基础表征）以及能简洁表征社会内不同用户群体（聚类）的价值体系集合。每个聚类包含代表其成员价值偏好的价值体系，以及反映符合该价值体系行为的近似帕累托最优策略。我们在两个包含人类价值观的马尔可夫决策过程环境中，将本方法与前沿的偏好多目标强化学习算法及基线模型进行对比评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling AI to recognize and adapt to diverse human value systems, which is crucial for value-aware AI but prone to misspecification. The authors propose a method that combines clustering and preference-based multi-objective reinforcement learning (PbMORL) to jointly learn value alignment models and a set of value systems representing different societal groups in Markov Decision Processes. Experimental results on two MDPs with human values demonstrate that their approach outperforms a state-of-the-art PbMORL algorithm and baselines, effectively capturing group-specific value-based preferences and generating corresponding Pareto-optimal policies.</div>
<div class="mono" style="margin-top:8px">本文旨在解决人工智能识别和适应多样化人类价值体系的挑战，这对于价值感知AI至关重要但容易产生误判。作者提出了一种结合聚类和基于偏好的多目标强化学习（PbMORL）的方法，在马尔可夫决策过程中联合学习价值对齐模型以及代表不同社会群体的价值体系。在两个人性化价值的MDP上的实验结果表明，该方法优于现有的先进PbMORL算法和基线，能够有效捕捉群体特定的价值偏好并生成相应的帕累托最优策略。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Preference Learning for Test-Time Steerable Reward Models</div>
<div class="meta-line">Authors: Jiwoo Hong, Shao Tang, Zhipeng Wang</div>
<div class="meta-line">First: 2026-02-09T15:55:56+00:00 · Latest: 2026-02-09T15:55:56+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08819v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯偏好学习用于测试时可引导的奖励模型</div>
<div class="mono" style="margin-top:8px">奖励模型通过强化学习（RL）在将语言模型与人类偏好对齐中起核心作用。随着RL在可验证奖励和多目标对齐等场景中的应用日益增多，奖励模型需编码更复杂、多维的偏好分布。然而，分类器奖励模型一旦训练完成即保持静态，限制了其在测试时的适应性。我们提出变分上下文奖励建模（ICRM），这是一种新颖的贝叶斯奖励建模目标，通过上下文偏好演示实现测试时可引导性。ICRM在Bradley-Terry模型下，使用共轭Beta先验，将奖励建模转化为对潜在偏好概率的摊销变分推断。我们证明ICRM能在测试时适应单目标和多目标场景中未见过的偏好分布。在单目标设置中，随着上下文演示增加，ICRM在SafeRLHF上准确率提升34%，在RM-Bench上提升9%；同时在助益性与拒绝性基准测试中，帕累托前沿得到扩展，超体积指标提升4%。我们进一步研究了ICRM在RL训练中的实际适用性，显示其通过数学推理任务超越传统奖励模型，能有效编码可验证奖励。最后，我们提供了理论保证：该变分目标存在具有有限置信度的全局内部最优解，并分析了KL正则化如何缓解奖励过度优化问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for reward models that can adapt to complex and multifaceted human preferences at test time, as static classifier-based reward models limit flexibility in settings like verifiable rewards and multi-objective alignment. The method introduces Variational In-Context Reward Modeling (ICRM), a Bayesian approach that frames reward modeling as amortized variational inference over latent preferences using a Beta prior under the Bradley-Terry model, enabling steerability via in-context demonstrations. Experimental results show that ICRM adapts to unseen preference distributions, achieving a 34% accuracy gain on SafeRLHF and 9% on RM-Bench in single-objective tasks, while improving multi-objective performance with a 4% hypervolume gain on helpfulness and refusal benchmarks; it also outperforms conventional reward models in math reasoning for RL training, with theoretical guarantees on optimality and mitigation of reward over-optimization.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要奖励模型能够在测试时适应复杂多面的人类偏好，因为基于分类器的静态奖励模型在可验证奖励和多目标对齐等场景中限制了灵活性。方法上提出了变分上下文奖励建模（ICRM），这是一种贝叶斯方法，将奖励建模构建为基于Beta先验和Bradley-Terry模型的潜在偏好摊销变分推断，通过上下文演示实现可操控性。实验结果表明，ICRM能适应未见过的偏好分布，在单目标任务中于SafeRLHF上准确率提升34%，在RM-Bench上提升9%，同时在多目标场景中于帮助性和拒绝基准上帕累托前沿扩展，超体积增益达4%；在数学推理的强化学习训练中优于传统奖励模型，并具有理论保证的最优性和对奖励过度优化的缓解作用。</div>
</details>
</div>
<div class="card">
<div class="title">Twice Sequential Monte Carlo for Tree Search</div>
<div class="meta-line">Authors: Yaniv Oren, Joery A. de Vries, Pascal R. van der Vaart, Matthijs T. J. Spaan, Wendelin Böhmer</div>
<div class="meta-line">First: 2025-11-18T07:54:29+00:00 · Latest: 2026-02-09T15:18:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14220v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.14220v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS as a policy improvement operator, scales favorably with sequential compute, reduces estimator variance and mitigates the effects of path degeneracy while retaining the properties that make SMC natural to parallelize.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双重序贯蒙特卡洛树搜索</div>
<div class="mono" style="margin-top:8px">基于模型的强化学习方法通过搜索机制实现了该领域的多项里程碑突破。序贯蒙特卡洛方法近期作为推动这些突破的蒙特卡洛树搜索算法的替代方案出现，其更易于并行化且更适合GPU加速。然而，该方法存在方差过大和路径退化问题，难以随搜索深度（即序列计算量）增加而有效扩展。为解决这些问题，我们提出双重序贯蒙特卡洛树搜索算法。在离散与连续环境中，该算法作为策略改进算子，其性能优于序贯蒙特卡洛基线及主流现代蒙特卡洛树搜索变体，能够随序列计算量增加实现良好扩展，有效降低估计方差并缓解路径退化效应，同时保持序贯蒙特卡洛固有的并行化优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve upon Sequential Monte Carlo (SMC) methods for tree search in model-based reinforcement learning, which suffer from high variance and path degeneracy when scaling to deeper searches, this paper introduces Twice Sequential Monte Carlo Tree Search (TSMCTS). The method aims to retain SMC&#x27;s parallelization advantages while addressing its limitations. Experimental results across discrete and continuous environments demonstrate that TSMCTS outperforms both SMC baselines and a modern MCTS variant, showing better scalability with increased search depth, reduced estimator variance, and mitigated path degeneracy.</div>
<div class="mono" style="margin-top:8px">本文的动机是改进基于模型的强化学习中用于树搜索的序列蒙特卡洛方法，该方法在搜索深度增加时存在高方差和路径退化问题。为此，作者提出了双重序列蒙特卡洛树搜索方法，旨在保持SMC易于并行化的优势的同时克服其缺陷。实验结果表明，在离散和连续环境中，TSMCTS均优于SMC基线和现代MCTS变体，能够更好地随搜索深度扩展，降低估计方差，并缓解路径退化现象。</div>
</details>
</div>
<div class="card">
<div class="title">Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization</div>
<div class="meta-line">Authors: Nicolás Villagrán Prieto, Eduardo C. Garrido-Merchán</div>
<div class="meta-line">First: 2026-02-09T15:15:52+00:00 · Latest: 2026-02-09T15:15:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08774v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08774v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>默认机器学习超参数无法为贝叶斯优化提供信息化的初始化</div>
<div class="mono" style="margin-top:8px">贝叶斯优化因其在昂贵黑盒函数上的样本效率，成为超参数调优的标准工具。尽管多数贝叶斯优化流程采用均匀随机初始化，但主流机器学习库（如scikit-learn）内置的默认超参数值蕴含了隐性的专家知识，理论上可作为加速收敛的信息化起点。这一直观假设却长期缺乏实证检验。本研究通过以库默认值为中心构建截断高斯分布进行初始化，并与均匀随机基线对比，系统评估该方法的有效性。实验涵盖三种贝叶斯优化后端（BoTorch、Optuna、Scikit-Optimize）、三类模型（随机森林、支持向量机、多层感知机）以及五个涵盖分类与回归任务的基准数据集。通过收敛速度与最终预测质量评估性能，并采用单侧二项检验确定统计显著性。在所有实验条件下，基于默认值的初始化均未展现出显著优势（p值范围0.141-0.908）。先验方差敏感性分析表明：虽然更紧致的默认值分布能提升早期评估效果，但随着优化进程推进，这种暂时优势逐渐消失，最终性能无显著差异。本研究未发现默认超参数能为优化提供有效方向性证据。因此建议实践者将超参数调优视为模型开发的核心环节，优先采用基于数据的系统搜索策略，而非依赖启发式的库默认值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the hypothesis that default hyperparameters from machine learning libraries might serve as informative starting points to accelerate Bayesian Optimization (BO), this study formalizes the idea by initializing BO with points drawn from truncated Gaussian distributions centered at these defaults and compares them against uniform random initialization. The method involves an extensive empirical evaluation across three BO back-ends, three model families, and five benchmark datasets, assessing convergence speed and final predictive quality with statistical significance determined via one-sided binomial tests. The main experimental results show that default-informed initialization provides no statistically significant advantage over random sampling, with p-values ranging from 0.141 to 0.908, and sensitivity analysis confirms that any early benefits vanish as optimization progresses, leading to the conclusion that default hyperparameters do not encode useful directional information for BO.</div>
<div class="mono" style="margin-top:8px">本研究基于机器学习库默认超参数可能作为信息性起点以加速贝叶斯优化（BO）的假设，通过从以这些默认值为中心截断高斯分布中采样点来初始化BO，并与均匀随机初始化进行比较。方法包括对三个BO后端、三个模型系列和五个基准数据集进行广泛实证评估，使用单侧二项检验评估收敛速度和最终预测质量的统计显著性。主要实验结果表明，默认信息初始化相比随机采样没有统计显著优势，p值范围在0.141至0.908之间，敏感性分析证实任何早期优势在优化过程中消失，因此默认超参数并未为BO提供有用的方向性信息。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data</div>
<div class="meta-line">Authors: Zhihao Zhang, Keith Redmill, Chengyang Peng, Bowen Weng</div>
<div class="meta-line">First: 2026-01-29T19:09:28+00:00 · Latest: 2026-02-09T15:05:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22242v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22242v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A driving algorithm that aligns with good human driving practices, or at the very least collaborates effectively with human drivers, is crucial for developing safe and efficient autonomous vehicles. In practice, two main approaches are commonly adopted: (i) supervised or imitation learning, which requires comprehensive naturalistic driving data capturing all states that influence a vehicle&#x27;s decisions and corresponding actions, and (ii) reinforcement learning (RL), where the simulated driving environment either matches or is intentionally more challenging than real-world conditions. Both methods depend on high-quality observations of real-world driving behavior, which are often difficult and costly to obtain. State-of-the-art sensors on individual vehicles can gather microscopic data, but they lack context about the surrounding conditions. Conversely, roadside sensors can capture traffic flow and other macroscopic characteristics, but they cannot associate this information with individual vehicles on a microscopic level. Motivated by this complementarity, we propose a framework that reconstructs unobserved microscopic states from macroscopic observations, using microscopic data to anchor observed vehicle behaviors, and learns a shared policy whose behavior is microscopically consistent with the partially observed trajectories and actions and macroscopically aligned with target traffic statistics when deployed population-wide. Such constrained and regularized policies promote realistic flow patterns and safe coordination with human drivers at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐微观车辆与宏观交通统计：基于部分数据重建驾驶行为</div>
<div class="mono" style="margin-top:8px">开发与优秀人类驾驶行为对齐、或至少能与人类驾驶员有效协作的驾驶算法，对构建安全高效的自动驾驶车辆至关重要。实践中主要采用两种方法：(i) 监督学习或模仿学习，需要能捕捉影响车辆决策的所有状态及对应动作的完整自然驾驶数据；(ii) 强化学习，其模拟驾驶环境需与现实条件匹配或刻意更具挑战性。两种方法均依赖高质量的真实驾驶行为观测数据，而这类数据往往难以获取且成本高昂。单车搭载的先进传感器可采集微观数据，但缺乏对周边环境的上下文感知；反之，路侧传感器能捕捉车流等宏观特征，却无法在微观层面关联具体车辆。基于这种互补性，我们提出一个框架：利用宏观观测重建未观测的微观状态，以微观数据锚定观测到的车辆行为，并学习一个共享策略——该策略在微观层面与部分观测到的轨迹动作保持一致，在宏观层面与全域部署时的目标交通统计特征对齐。这种经过约束和正则化的策略，能够促进大规模实现符合现实的交通流模式以及与人类驾驶员的安全协同。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complementary strengths of vehicle-mounted sensors, which capture detailed individual driving data but lack broader traffic context, and roadside sensors, which provide aggregate traffic flow statistics but not individual vehicle behaviors, this paper proposes a framework to reconstruct unobserved microscopic driving states from macroscopic observations. The method uses available microscopic data to anchor individual vehicle behaviors and learns a shared driving policy that is both consistent with partially observed trajectories and actions at the microscopic level and, when deployed across a population, aligns with target macroscopic traffic statistics. Experimental results demonstrate that this constrained and regularized approach promotes realistic traffic flow patterns and safe coordination with human drivers at scale, effectively bridging the gap between individual vehicle behavior and aggregate traffic dynamics.</div>
<div class="mono" style="margin-top:8px">本研究基于车载传感器能获取详细微观驾驶数据但缺乏宏观交通背景，而路侧传感器能提供宏观交通流统计却无法关联个体车辆行为的互补性，提出一个从宏观观测重建未观测微观状态的框架。该方法利用可得的微观数据锚定个体车辆行为，并学习一个共享驾驶策略，该策略在微观层面与部分观测到的轨迹和动作保持一致，在宏观层面当群体部署时与目标交通统计对齐。实验结果表明，这种约束和正则化的方法能有效促进现实的交通流模式和大规模下与人类驾驶员的安全协调，从而弥合了微观车辆行为与宏观交通动态之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Spatiotemporal Attention-Augmented Inverse Reinforcement Learning for Multi-Agent Task Allocation</div>
<div class="meta-line">Authors: Huilin Yin, Zhikun Yang, Linchuan Zhang, Daniel Watzenig</div>
<div class="meta-line">First: 2025-04-07T13:14:45+00:00 · Latest: 2026-02-09T15:01:05+00:00</div>
<div class="meta-line">Comments: Revised version with substantial new experimental results, improved analysis, and a restructured layout for better clarity</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.05045v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.05045v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial inverse reinforcement learning (IRL) for multi-agent task allocation (MATA) is challenged by non-stationary interactions and high-dimensional coordination. Unconstrained reward inference in these settings often leads to high variance and poor generalization. We propose an attention-structured adversarial IRL framework that constrains reward inference via spatiotemporal representation learning. Our method employs multi-head self-attention (MHSA) for long-range temporal dependencies and graph attention networks (GAT) for agent-task relational structures. We formulate reward inference as a low-capacity, adaptive linear transformation of the environment reward, ensuring stable and interpretable guidance. This framework decouples reward inference from policy learning and optimizes the reward model adversarially. Experiments on benchmark MATA scenarios show that our approach outperforms representative MARL baselines in convergence speed, cumulative rewards, and spatial efficiency. Results demonstrate that attention-guided, capacity-constrained reward inference is a scalable and effective mechanism for stabilizing adversarial IRL in complex multi-agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多智能体任务分配的时空注意力增强逆强化学习</div>
<div class="mono" style="margin-top:8px">多智能体任务分配中的对抗式逆强化学习面临非平稳交互与高维协调的挑战。无约束的奖励推断常导致高方差与泛化能力不足。本文提出一种注意力结构化的对抗式逆强化学习框架，通过时空表征学习约束奖励推断。该方法采用多头自注意力机制捕捉长程时序依赖，利用图注意力网络建模智能体-任务关系结构。我们将奖励推断形式化为环境奖励的低容量自适应线性变换，确保稳定可解释的引导。该框架实现奖励推断与策略学习的解耦，并以对抗方式优化奖励模型。在标准多智能体任务分配场景上的实验表明，本方法在收敛速度、累积奖励与空间效率方面优于主流多智能体强化学习基线。结果证明，注意力引导的容量约束奖励推断是稳定复杂多智能体系统中对抗式逆强化学习的可扩展有效机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of non-stationary interactions and high-dimensional coordination in multi-agent task allocation (MATA) under adversarial inverse reinforcement learning (IRL), which often leads to high variance and poor generalization, this paper proposes an attention-structured adversarial IRL framework. The method employs multi-head self-attention (MHSA) to capture long-range temporal dependencies and graph attention networks (GAT) to model agent-task relational structures, formulating reward inference as a low-capacity, adaptive linear transformation of the environment reward to ensure stable and interpretable guidance while decoupling it from policy learning. Experimental results on benchmark MATA scenarios demonstrate that this approach outperforms representative multi-agent reinforcement learning baselines in convergence speed, cumulative rewards, and spatial efficiency, showing that attention-guided, capacity-constrained reward inference effectively stabilizes adversarial IRL in complex multi-agent systems.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体任务分配中对抗性逆强化学习面临的非平稳交互和高维协调挑战，以及由此导致的高方差和泛化性差的问题，提出了一种注意力结构的对抗性逆强化学习框架。该方法采用多头自注意力机制捕获长程时间依赖性，并利用图注意力网络建模智能体-任务关系结构，将奖励推断构建为环境奖励的低容量自适应线性变换，以确保稳定且可解释的指导，同时将其与策略学习解耦。在基准多智能体任务分配场景上的实验结果表明，该方法在收敛速度、累积奖励和空间效率方面优于代表性的多智能体强化学习基线，证明了注意力引导、容量受限的奖励推断是稳定复杂多智能体系统中对抗性逆强化学习的可扩展且有效的机制。</div>
</details>
</div>
<div class="card">
<div class="title">Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse</div>
<div class="meta-line">Authors: Shaojie Wang, Jinghui Wang, Yinghan Cui, Xuxing Chen, Chao Wang, Liang Huang, Xiaojiang Zhang, Junyi Peng, Li Wan, Haotian Zhang, Bin Chen</div>
<div class="meta-line">First: 2025-11-01T05:56:49+00:00 · Latest: 2026-02-09T14:55:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00413v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00413v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic large language model (LLM) training often involves multi-turn interaction trajectories that branch into multiple execution paths due to concurrent tool use, think-mode, sub-agent, context management and other runtime designs. As a result, the token produced by a single task naturally forms a tree-structured token trajectory with shared prefixes, rather than a linear sequence. Existing training pipelines linearize such trajectories and treat each branch independently, leading to substantial redundant computation in both forward and backward passes. To eliminate such redundancy, we introduce Tree Training, an efficient training framework for tree-structured trajectories. Its core component, Gradient Restoration, enables correct gradient aggregation across shared prefixes, allowing each prefix to be computed exactly once while remaining mathematically equivalent to independent training on all branches. To support large trajectory trees in practice, we redesign the training engine to natively ingest tree-structured data and propose Tree Packing, a memory-efficient partitioning strategy that preserves high prefix reuse. Experiments conducted on dense and MOE models of real-world agentic trajectories show 6.2x training speedup for both supervised fine-tuning and the model update phase in reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>树形训练：通过共享前缀复用加速智能体大语言模型训练</div>
<div class="mono" style="margin-top:8px">智能体大语言模型（LLM）训练常涉及多轮交互轨迹，由于并发工具调用、思考模式、子智能体、上下文管理等运行时设计，这些轨迹会分叉为多条执行路径。因此，单个任务生成的令牌自然形成具有共享前缀的树形令牌轨迹，而非线性序列。现有训练流程将此类轨迹线性化并独立处理每个分支，导致前向与反向传播中存在大量冗余计算。为消除冗余，我们提出树形训练——针对树形轨迹的高效训练框架。其核心组件梯度复原技术，能实现跨共享前缀的正确梯度聚合，使每个前缀仅需计算一次，同时在数学上等效于所有分支的独立训练。为支持实际应用中的大型轨迹树，我们重新设计训练引擎以原生处理树形数据，并提出树形打包——一种保持高前缀复用率的内存高效分区策略。在真实世界智能体轨迹的稠密模型与混合专家模型上的实验表明，该方法在监督微调和强化学习的模型更新阶段均实现6.2倍训练加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the inefficiency of existing training pipelines for agentic large language models, which linearize the tree-structured token trajectories arising from multi-turn interactions and concurrent tool use, leading to redundant computations. The method introduces Tree Training, a framework featuring Gradient Restoration to correctly aggregate gradients across shared prefixes and Tree Packing for memory-efficient partitioning, enabling native handling of tree-structured data. Experimental results on dense and mixture-of-experts models with real-world agentic trajectories demonstrate a 6.2x training speedup for both supervised fine-tuning and reinforcement learning model updates.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有训练流程在处理智能体大语言模型时因线性化树状令牌轨迹而导致的冗余计算问题，这些轨迹源于多轮交互和并发工具使用。方法上提出了树训练框架，其核心包括梯度恢复技术以实现共享前缀的正确梯度聚合，以及树打包策略进行内存高效分区，从而原生支持树状结构数据。实验结果表明，在真实世界智能体轨迹的密集和混合专家模型上，监督微调和强化学习模型更新阶段均实现了6.2倍的训练加速。</div>
</details>
</div>
<div class="card">
<div class="title">A Review of Online Diffusion Policy RL Algorithms for Scalable Robotic Control</div>
<div class="meta-line">Authors: Wonhyeok Choi, Shutong Ding, Minwoo Choi, Jungwan Woo, Kyumin Hwang, Jaeyeul Kim, Ye Shi, Sunghoon Im</div>
<div class="meta-line">First: 2026-01-05T05:19:23+00:00 · Latest: 2026-02-09T14:52:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06133v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06133v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion policies have emerged as a powerful approach for robotic control, demonstrating superior expressiveness in modeling multimodal action distributions compared to conventional policy networks. However, their integration with online reinforcement learning remains challenging due to fundamental incompatibilities between diffusion model training objectives and standard RL policy improvement mechanisms. This paper presents the first comprehensive review and empirical analysis of current Online Diffusion Policy Reinforcement Learning (Online DPRL) algorithms for scalable robotic control systems. We propose a novel taxonomy that categorizes existing approaches into four distinct families--Action-Gradient, Q-Weighting, Proximity-Based, and Backpropagation Through Time (BPTT) methods--based on their policy improvement mechanisms. Through extensive experiments on a unified NVIDIA Isaac Lab benchmark encompassing 12 diverse robotic tasks, we systematically evaluate representative algorithms across five critical dimensions: task diversity, parallelization capability, diffusion step scalability, cross-embodiment generalization, and environmental robustness. Our analysis identifies key findings regarding the fundamental trade-offs inherent in each algorithmic family, particularly concerning sample efficiency and scalability. Furthermore, we reveal critical computational and algorithmic bottlenecks that currently limit the practical deployment of online DPRL. Based on these findings, we provide concrete guidelines for algorithm selection tailored to specific operational constraints and outline promising future research directions to advance the field toward more general and scalable robotic learning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向可扩展机器人控制的在线扩散策略强化学习算法综述</div>
<div class="mono" style="margin-top:8px">扩散策略已成为机器人控制的一种强大方法，相比传统策略网络，其在建模多模态动作分布方面展现出卓越的表达能力。然而，由于扩散模型训练目标与标准强化学习策略改进机制之间存在根本性不兼容，将其与在线强化学习结合仍具挑战性。本文首次对当前面向可扩展机器人控制系统的在线扩散策略强化学习算法进行了全面综述与实证分析，提出了一种新颖的分类法，依据策略改进机制将现有方法归纳为四大类：动作梯度法、Q值加权法、邻近性优化法以及时序反向传播法。通过在涵盖12种多样化机器人任务的统一NVIDIA Isaac Lab基准测试中进行广泛实验，我们从任务多样性、并行化能力、扩散步长可扩展性、跨实体泛化能力和环境鲁棒性五个关键维度，系统评估了代表性算法。分析揭示了各类算法家族内在的基本权衡关系，特别是在样本效率与可扩展性方面。此外，我们指出了当前限制在线扩散策略强化学习实际部署的关键计算与算法瓶颈。基于这些发现，我们针对具体操作约束提供了算法选择的实用指南，并展望了推动该领域向更通用、可扩展的机器人学习系统发展的未来研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of integrating expressive diffusion policies with online reinforcement learning for robotic control, due to incompatibilities in their training objectives. The method involves proposing a novel taxonomy that categorizes existing Online Diffusion Policy RL algorithms into four families based on their policy improvement mechanisms: Action-Gradient, Q-Weighting, Proximity-Based, and Backpropagation Through Time methods. The main experimental results, derived from extensive benchmarking on 12 diverse robotic tasks, systematically evaluate these algorithms across key dimensions like task diversity and scalability, revealing fundamental trade-offs in sample efficiency and identifying computational bottlenecks that limit practical deployment.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决扩散策略与在线强化学习在机器人控制中整合的挑战，这源于两者训练目标的不兼容性。研究方法提出了一种新颖的分类法，根据策略改进机制将现有的在线扩散策略强化学习算法分为四类：动作梯度法、Q值加权法、邻近性方法和通过时间的反向传播法。主要实验结果基于对12项多样化机器人任务的广泛基准测试，系统评估了这些算法在任务多样性和可扩展性等关键维度的表现，揭示了样本效率方面的根本性权衡，并指出了限制实际部署的计算瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning</div>
<div class="meta-line">Authors: David Hudák, Maris F. L. Galesloot, Martin Tappler, Martin Kurečka, Nils Jansen, Milan Češka</div>
<div class="meta-line">First: 2026-02-09T14:39:16+00:00 · Latest: 2026-02-09T14:39:16+00:00</div>
<div class="meta-line">Comments: 17 pages (8 main paper, 2 references, 7 appendix). 3 figures in the main paper, 3 figures in the appendix. Accepted AAMAS&#x27;26 submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08734v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08734v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的（隐模型）部分可观测马尔可夫决策过程有限状态控制器</div>
<div class="mono" style="margin-top:8px">求解部分可观测马尔可夫决策过程需要在状态信息不完整的情况下计算策略。尽管近期研究取得进展，现有POMDP求解器的可扩展性仍受限。此外，许多场景要求策略在多个POMDP中保持鲁棒性，进一步加剧了可扩展性问题。我们提出Lexpop框架用于POMDP求解，该框架（1）采用深度强化学习训练由循环神经网络表示的神经策略，（2）通过高效提取方法构建模拟神经策略的有限状态控制器。关键的是，与神经策略不同，此类控制器可进行形式化验证并提供性能保证。我们将Lexpop扩展至隐模型POMDP的鲁棒策略计算，该方法通过关联每个提取控制器与其最差情况POMDP，利用此类POMDP集合迭代训练鲁棒神经策略并最终提取鲁棒控制器。实验表明，在大状态空间问题上，Lexpop在POMDP及HM-POMDP求解性能上均优于当前最先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited scalability of existing solvers for partially observable Markov decision processes (POMDPs) and the need for policies robust across multiple models, this paper introduces the Lexpop framework. The method first trains a neural policy using deep reinforcement learning with a recurrent neural network, then extracts a finite-state controller that mimics this policy, enabling formal evaluation and performance guarantees. The approach is extended to hidden-model POMDPs (HM-POMDPs) by iteratively training a robust neural policy against worst-case POMDPs and extracting a corresponding robust controller. Experimental results demonstrate that Lexpop outperforms state-of-the-art solvers on problems with large state spaces for both POMDPs and HM-POMDPs.</div>
<div class="mono" style="margin-top:8px">针对现有部分可观测马尔可夫决策过程（POMDP）求解器可扩展性有限，且需要跨多个模型具有鲁棒性的策略这一问题，本文提出了Lexpop框架。该方法首先使用深度强化学习和循环神经网络训练神经策略，然后通过高效提取方法构建模仿该策略的有限状态控制器，从而支持形式化验证和性能保证。该框架被扩展到隐模型POMDP（HM-POMDP），通过针对最坏情况POMDP迭代训练鲁棒神经策略并提取相应的鲁棒控制器。实验结果表明，在大状态空间问题上，Lexpop在POMDP和HM-POMDP求解上均优于现有最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Language Bottleneck Models for Qualitative Knowledge State Modeling</div>
<div class="meta-line">Authors: Antonin Berthon, Mihaela van der Schaar</div>
<div class="meta-line">First: 2025-06-20T13:21:14+00:00 · Latest: 2026-02-09T14:23:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.16982v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.16982v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately assessing student knowledge is central to education. Cognitive Diagnosis (CD) models estimate student proficiency at a fixed point in time, while Knowledge Tracing (KT) methods model evolving knowledge states to predict future performance. However, existing approaches either provide quantitative concept mastery estimates with limited expressivity (CD, probabilistic KT) or prioritize predictive accuracy at the cost of interpretability (deep learning KT). We propose Language Bottleneck Models (LBMs), where an encoder LLM produces textual knowledge state summaries, which a decoder LLM uses to predict future performance. This produces interpretable summaries that can express nuanced insights--such as misconceptions--that CD and KT models cannot capture. Extensive validation across synthetic and real-world datasets shows LBMs reveal qualitative insights beyond what CD and KT models can capture, while achieving competitive accuracy with improved sample efficiency. We demonstrate that the encoder and decoder can be fine-tuned with reinforcement learning and supervised fine-tuning respectively to improve both summary quality and predictive performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言瓶颈模型用于定性知识状态建模</div>
<div class="mono" style="margin-top:8px">准确评估学生知识是教育的核心。认知诊断模型估计学生在固定时间点的能力水平，而知识追踪方法则建模动态变化的知识状态以预测未来表现。然而，现有方法要么提供表达能力有限的定量概念掌握度估计，要么以牺牲可解释性为代价优先考虑预测准确性。我们提出语言瓶颈模型，其中编码器大语言模型生成文本化知识状态摘要，解码器大语言模型利用该摘要预测未来表现。这种方法产生可解释的摘要，能够表达认知诊断和知识追踪模型无法捕捉的细微见解。在合成和真实数据集上的广泛验证表明，语言瓶颈模型能揭示超越传统模型的定性洞察，同时以更优样本效率实现具有竞争力的预测精度。我们证明编码器和解码器可分别通过强化学习和监督微调进行优化，从而同步提升摘要质量与预测性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for more expressive and interpretable models of student knowledge, as existing Cognitive Diagnosis and Knowledge Tracing methods are either limited to quantitative estimates or sacrifice interpretability for predictive accuracy. The proposed method, Language Bottleneck Models (LBMs), uses an encoder LLM to generate textual summaries of student knowledge states, which a decoder LLM then leverages to predict future performance, enabling the capture of nuanced qualitative insights like misconceptions. Experimental results on synthetic and real-world datasets show that LBMs achieve competitive predictive accuracy with improved sample efficiency while providing interpretable summaries that reveal insights beyond traditional models, with further enhancements possible through reinforcement learning and supervised fine-tuning of the encoder and decoder.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有认知诊断和知识追踪方法要么局限于定量评估，要么为预测准确性牺牲可解释性，因此需要更具表达力和可解释性的学生知识状态模型。所提出的方法——语言瓶颈模型（LBMs）——利用编码器大语言模型生成学生知识状态的文本摘要，解码器大语言模型则基于这些摘要预测未来表现，从而能够捕捉如误解等细微的定性洞察。在合成和真实数据集上的实验结果表明，LBMs在实现具有竞争力的预测准确性和更高样本效率的同时，提供了超越传统模型的可解释摘要，揭示了更多定性洞察，且通过强化学习和监督微调可进一步提升编码器和解码器的摘要质量与预测性能。</div>
</details>
</div>
<div class="card">
<div class="title">SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity</div>
<div class="meta-line">Authors: Shae McFadden, Myles Foley, Elizabeth Bates, Ilias Tsingenopoulos, Sanyam Vyas, Vasilios Mavroudis, Chris Hicks, Fabio Pierazzi</div>
<div class="meta-line">First: 2026-02-09T14:12:41+00:00 · Latest: 2026-02-09T14:12:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08690v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SoK：深度强化学习在网络安全应用中的常见陷阱</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）在需要序列决策的领域取得了显著成功，这推动了其在网络安全问题中的应用。然而，将DRL从实验室模拟迁移到定制化的网络环境中可能引发诸多问题。大多数网络安全任务通常具有对抗性、非平稳性和部分可观测性，进一步加剧了这些挑战。本文系统性地识别并归纳了在网络安全领域的DRL（DRL4Sec）文献中，于环境建模、智能体训练、性能评估和系统部署阶段频繁出现的11个方法论陷阱。通过分析66篇重要的DRL4Sec论文（2018-2025年），我们量化了每个陷阱的普遍性，发现平均每篇论文存在超过五个陷阱。我们通过（i）自主网络防御、（ii）对抗性恶意软件生成和（iii）Web安全测试环境中的受控实验，展示了这些陷阱的实际影响。最后，针对每个陷阱提供了可操作的建议，以支持开发更严谨且可部署的基于DRL的安全系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing but problematic application of deep reinforcement learning (DRL) to complex cybersecurity tasks, this paper systematically identifies 11 common methodological pitfalls across environment modeling, agent training, evaluation, and deployment. The method involves analyzing 66 key DRL for cybersecurity papers from 2018-2025, quantifying the prevalence of these issues, and demonstrating their practical impact through controlled experiments in domains like autonomous defense and malware creation. The main experimental results reveal that the surveyed literature contains an average of over five pitfalls per paper, highlighting significant gaps in rigor and deployability that the authors&#x27; actionable recommendations aim to address.</div>
<div class="mono" style="margin-top:8px">本文的动机源于深度强化学习在网络安全领域应用日益增多但问题频现的现状，旨在系统化地找出在环境建模、智能体训练、性能评估和系统部署阶段常见的11个方法性缺陷。研究方法包括分析2018至2025年间66篇重要的网络安全深度强化学习论文，量化每个缺陷的普遍性，并通过在自主网络防御、对抗性恶意软件生成等环境中的受控实验来验证其实际影响。主要实验结果表明，所调研的论文平均每篇存在超过五个缺陷，这凸显了现有研究在严谨性和可部署性上的显著不足，作者为此针对每个缺陷提供了可操作的建议。</div>
</details>
</div>
<div class="card">
<div class="title">Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning</div>
<div class="meta-line">Authors: Constant Bourdrez, Alexandre Vérine, Olivier Cappé</div>
<div class="meta-line">First: 2026-02-09T14:10:44+00:00 · Latest: 2026-02-09T14:10:44+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08689v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08689v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于逆强化学习的扩散模型采样方法学习</div>
<div class="mono" style="margin-top:8px">扩散模型通过神经网络引导的迭代去噪过程生成样本。虽然基于真实数据训练去噪器的计算成本较高，但采样过程本身更具灵活性。这种适应性在实践中成为关键调节手段，既能提升生成样本的质量，又能提高采样效率。本研究提出一种逆强化学习框架，用于学习采样策略而无需重新训练去噪器。我们将扩散采样过程建模为离散时间有限时域马尔可夫决策过程，其中动作对应采样动力学的可选修改。为优化动作调度，我们避免定义显式奖励函数，转而直接使用策略梯度技术匹配采样器预期目标行为。实验证明，该方法能提升预训练扩散模型的生成样本质量，并自动优化采样超参数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the computational cost of training diffusion model denoisers and the practical flexibility of the sampling process, aiming to enhance sample quality and sampling efficiency without retraining the denoiser. The method introduces an inverse reinforcement learning framework that formulates diffusion sampling as a Markov Decision Process, where actions modify sampling dynamics, and optimizes action scheduling via policy gradient techniques to match target behavior without an explicit reward function. Experimental results demonstrate that this approach improves sample quality from pretrained diffusion models and effectively tunes sampling hyperparameters.</div>
<div class="mono" style="margin-top:8px">本文的动机在于扩散模型去噪器训练的计算成本高昂，而采样过程更具灵活性，旨在不重新训练去噪器的情况下提升生成样本的质量和采样效率。方法引入逆强化学习框架，将扩散采样建模为马尔可夫决策过程，其中动作对应采样动态的修改，并通过策略梯度技术优化动作调度以匹配目标行为，无需显式奖励函数。实验结果表明，该方法能改进预训练扩散模型的样本质量，并有效自动调整采样超参数。</div>
</details>
</div>
<div class="card">
<div class="title">From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism</div>
<div class="meta-line">Authors: Sarthak Wanjari</div>
<div class="meta-line">First: 2026-02-09T13:48:25+00:00 · Latest: 2026-02-09T13:48:25+00:00</div>
<div class="meta-line">Comments: 10 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08655v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08655v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL&#x27;s 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从机器人学到脓毒症治疗：基于几何悲观主义的离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）旨在从静态数据集中恢复最优策略，但其仍易高估分布外（OOD）动作，尤其在断裂稀疏的数据流形中。现有方法需在计算效率与性能间权衡：CQL等方法提供严格保守性但需巨大算力，而基于期望分位数的高效方法（如IQL）常无法修正病态数据集上的OOD误差，退化为行为克隆。本研究提出几何悲观主义——一种模块化、计算高效的框架，通过基于状态-动作嵌入空间中k近邻距离的密度惩罚增强标准IQL。通过预计算应用于各状态-动作对的惩罚项，本方法以O(1)训练开销通过奖励塑形注入OOD保守性。在D4RL MuJoCo基准测试中，我们的Geo-IQL方法在敏感不稳定的medium-replay任务上超越标准IQL超过18分，同时将种子间方差降低4倍，且在稳定流形上性能无衰减。关键地，我们在MIMIC-III脓毒症重症监护数据集上验证算法：当标准IQL退化为行为克隆时，Geo-IQL展现出主动策略改进，在保持安全约束下达成86.4%的临床终局决策符合率（IQL为75%）。结果表明几何悲观主义为关键现实决策系统安全克服局部最优提供了必要正则化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge in offline reinforcement learning where existing methods either demand high computational resources or fail to correct out-of-distribution errors, often collapsing to behavioral cloning. The authors propose Geometric Pessimism, a modular framework that enhances the efficient IQL method by incorporating a density-based penalty derived from k-nearest-neighbor distances in the state-action embedding space, applied via reward shaping with minimal training overhead. Experimental results on the D4RL MuJoCo benchmark show that Geo-IQL outperforms standard IQL by over 18 points on sensitive tasks and reduces variance, while on the MIMIC-III Sepsis dataset, it achieves active policy improvement with 86.4% terminal agreement with clinicians, compared to IQL&#x27;s 75%, demonstrating safer and more effective decision-making in critical real-world applications.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中现有方法要么计算成本高昂、要么无法纠正分布外误差并常退化为行为克隆的问题，提出了一种名为几何悲观主义的模块化框架。该方法通过基于状态-动作嵌入空间中k近邻距离的密度惩罚来增强高效的IQL算法，并以最小训练开销通过奖励塑形实现。在D4RL MuJoCo基准测试中，Geo-IQL在敏感任务上比标准IQL性能提升超过18分并降低了方差；在MIMIC-III脓毒症数据集上，它实现了积极的策略改进，与临床医生决策的终端一致性达到86.4%，优于IQL的75%，表明其在关键现实决策系统中能更安全有效地克服局部最优。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces</div>
<div class="meta-line">Authors: Heiko Hoppe, Fabian Akkerman, Wouter van Heeswijk, Maximilian Schiffer</div>
<div class="meta-line">First: 2026-02-09T13:05:07+00:00 · Latest: 2026-02-09T13:05:07+00:00</div>
<div class="meta-line">Comments: 26 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08616v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08616v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破网格限制：大规模离散与混合动作空间中的距离引导强化学习</div>
<div class="mono" style="margin-top:8px">强化学习在物流、调度和推荐系统中的应用日益广泛，但标准算法在应对大规模离散动作空间时面临维度灾难。现有算法通常依赖受限的网格结构或计算成本高昂的最近邻搜索，限制了其在高维或不规则结构领域的效果。我们提出距离引导强化学习（DGRL），结合采样动态邻域（SDN）和基于距离的更新（DBU），可在动作规模高达10^20的空间中实现高效强化学习。与先前方法不同，SDN利用语义嵌入空间进行随机体积探索，可证明在局部信任区域提供完全支持。DBU将策略优化转化为稳定的回归任务，使梯度方差与动作空间基数解耦，并保证策略的单调改进。DGRL能自然推广到混合连续-离散动作空间，无需层级依赖。我们在规则与不规则结构环境中验证了该方法，相比最先进基准性能提升最高达66%，同时提升了收敛速度并降低了计算复杂度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying reinforcement learning to domains with extremely large discrete or hybrid action spaces, such as logistics and recommender systems, where traditional methods suffer from the curse of dimensionality due to reliance on grid structures or costly nearest-neighbor searches. The authors propose Distance-Guided Reinforcement Learning (DGRL), which integrates Sampled Dynamic Neighborhoods (SDN) for stochastic volumetric exploration in a semantic embedding space and Distance-Based Updates (DBU) to reformulate policy optimization as a stable regression task, thereby decoupling gradient variance from action count and ensuring monotonic improvement. Experimental results show that DGRL achieves up to 66% performance gains over state-of-the-art benchmarks in both regularly and irregularly structured environments, while also enhancing convergence speed and computational efficiency, and it naturally extends to hybrid action spaces without hierarchical constraints.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在物流、推荐系统等具有极大离散或混合动作空间领域中的应用挑战，传统方法因依赖网格结构或昂贵的最近邻搜索而受维度灾难限制。作者提出了距离引导强化学习（DGRL），该方法结合了采样动态邻域（SDN）在语义嵌入空间中进行随机体积探索，以及距离基更新（DBU）将策略优化转化为稳定回归任务，从而解耦梯度方差与动作数量并保证策略单调改进。实验结果表明，DGRL在规则和非规则结构环境中相比最先进基准实现了高达66%的性能提升，同时提高了收敛速度和计算效率，并能自然扩展到混合动作空间而无需层次依赖。</div>
</details>
</div>
<div class="card">
<div class="title">Playing 20 Question Game with Policy-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu, Zhan Chen</div>
<div class="meta-line">First: 2018-08-23T06:34:32+00:00 · Latest: 2026-02-09T13:00:05+00:00</div>
<div class="meta-line">Comments: Withdrawal from the conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/1808.07645v4">Abs</a> · <a href="https://arxiv.org/pdf/1808.07645v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisy-free simulation environment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略的强化学习在二十问游戏中的应用</div>
<div class="mono" style="margin-top:8px">二十问游戏是一种鼓励演绎推理与创造力的经典游戏。游戏中，回答者首先选定一个对象（如名人或动物类别），提问者则通过最多二十个问题尝试猜中该对象。在二十问游戏系统中，用户扮演回答者，系统作为提问者需采用高效的问题选择策略以推断正确对象并获胜。然而，由于游戏环境的复杂性与多变性，最优提问策略难以直接推导。本文提出一种新颖的基于策略的强化学习方法，使提问智能体能够通过与用户的持续交互学习最优提问策略。为提升训练效率，我们同时提出使用奖励网络来估算信息量更高的奖励值。相较于既有方法，本强化学习方法对噪声答案具有鲁棒性，且无需依赖对象知识库。实验结果表明，该方法显著优于基于信息熵的工程系统，并在无噪声模拟环境中展现出竞争优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of developing an optimal question selection strategy for the 20 Questions game, where traditional methods struggle due to environmental complexity and noisy user responses. The authors propose a policy-based reinforcement learning approach that learns through continuous interaction with users, supplemented by a reward network to provide more informative feedback during training. Experimental results demonstrate that this method outperforms an entropy-based baseline system and achieves competitive performance in noise-free simulations, while being robust to answer noise and independent of a predefined knowledge base.</div>
<div class="mono" style="margin-top:8px">本文针对二十问游戏中因环境复杂性和用户回答噪声导致传统方法难以获得最优提问策略的问题，提出了一种基于策略的强化学习方法，通过持续与用户交互进行学习，并引入奖励网络来提供更有效的训练反馈。实验结果表明，该方法在性能上明显优于基于熵的工程系统，并在无噪声模拟环境中表现出竞争力，同时具备对噪声的鲁棒性且不依赖于预设的知识库。</div>
</details>
</div>
<div class="card">
<div class="title">GPTOpt: Teaching LLMs to do Interpretable Black-Box Optimization</div>
<div class="meta-line">Authors: Jamison Meindl, Yunsheng Tian, Tony Cui, Veronika Thost, Zhang-Wei Hong, Jie Chen, Wojciech Matusik, Mina Konaković Luković</div>
<div class="meta-line">First: 2025-10-29T11:21:55+00:00 · Latest: 2026-02-09T12:46:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25404v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25404v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency and decision interpretability. While Large Language Models (LLMs) have shown broad capabilities, even state-of-the-art models remain limited in solving continuous black-box optimization tasks and struggle to maintain exploration-exploitation balance. We introduce GPTOpt, an optimization method that equips LLMs with continuous black-box optimization capabilities by fine-tuning Llama 3.1 8B on structured Bayesian optimization (BO) data, including surrogate model information. This provides an explainable framework calibrated to produce surrogate model outputs comparable to a Gaussian process, while keeping the advantages of flexible LLM-based optimization. On a variety of black-box optimization benchmarks, our model shows favorable performance compared to traditional optimizers and transformer-based alternatives, while providing important context and insight into the model&#x27;s decisions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GPTOpt：指导大语言模型进行可解释黑盒优化</div>
<div class="mono" style="margin-top:8px">昂贵、无导数黑盒函数的全局优化需要极高的样本效率和决策可解释性。尽管大语言模型展现出广泛能力，但即使是先进模型在解决连续黑盒优化任务时仍存在局限，难以保持探索与利用的平衡。我们提出GPTOpt方法，通过在结构化贝叶斯优化数据（包含代理模型信息）上微调Llama 3.1 8B模型，使大语言模型获得连续黑盒优化能力。该方法构建了可解释的校准框架，能产生与高斯过程相当的代理模型输出，同时保持基于大语言模型的灵活优化优势。在多种黑盒优化基准测试中，相较于传统优化器和基于Transformer的替代方案，本模型展现出优越性能，并为模型决策提供了重要的上下文与洞察。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the need for sample-efficient and interpretable global optimization of expensive, black-box functions, where current Large Language Models (LLMs) are limited in handling continuous tasks and balancing exploration with exploitation. The method, named GPTOpt, fine-tunes the Llama 3.1 8B model on structured Bayesian optimization data that includes surrogate model information, thereby equipping the LLM with continuous black-box optimization capabilities within an explainable framework calibrated to produce outputs comparable to a Gaussian process. The main experimental results demonstrate that GPTOpt achieves favorable performance on various black-box optimization benchmarks compared to traditional optimizers and transformer-based alternatives, while also providing contextual insight into its decision-making process.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决对昂贵黑盒函数进行样本高效且可解释的全局优化的需求，而当前的大型语言模型在处理连续优化任务及平衡探索与利用方面存在局限。该方法名为GPTOpt，通过在包含代理模型信息的结构化贝叶斯优化数据上对Llama 3.1 8B模型进行微调，使大型语言模型具备了连续黑盒优化能力，并构建了一个可解释的框架，其输出校准到与高斯过程相当的水平。主要实验结果表明，在各种黑盒优化基准测试中，GPTOpt相较于传统优化器和基于Transformer的替代方法表现出更优的性能，同时能为模型的决策提供重要的上下文和洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Conditional Sequence Modeling for Safe Reinforcement Learning</div>
<div class="meta-line">Authors: Wensong Bai, Chao Zhang, Qihang Xu, Chufan Chen, Chenhao Zhou, Hui Qian</div>
<div class="meta-line">First: 2026-02-09T12:22:57+00:00 · Latest: 2026-02-09T12:22:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08584v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08584v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>条件序列建模在安全强化学习中的应用</div>
<div class="mono" style="margin-top:8px">离线安全强化学习旨在从固定数据集中学习策略，同时在累积成本约束下最大化性能。实际部署中，不同场景的需求常存在差异，需要单一策略能够零样本适应不同的成本阈值。然而，现有离线安全强化学习方法大多基于预设阈值训练，导致策略在跨成本阈值时的泛化能力和部署灵活性受限。受条件序列建模近期进展的启发——该方法通过指定目标回报实现灵活的目标条件控制，我们提出RCDT，一种基于条件序列建模的方法，支持在单一训练策略内跨多个成本阈值的零样本部署。RCDT是首个基于条件序列建模的离线安全强化学习算法，融合了拉格朗日式成本惩罚与自适应惩罚系数。为避免过度保守行为并实现更优的回报-成本权衡，该方法进一步引入了奖励-成本感知轨迹重加权机制与Q值正则化。在DSRL基准上的大量实验表明，RCDT在回报-成本权衡方面持续优于代表性基线方法，推动了离线安全强化学习的技术前沿。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for offline safe reinforcement learning policies that can adapt zero-shot to varying deployment cost thresholds, this paper introduces RCDT, a conditional sequence modeling method that integrates a Lagrangian-style cost penalty with an auto-adaptive coefficient. The approach incorporates a reward-cost-aware trajectory reweighting mechanism and Q-value regularization to avoid overly conservative behavior and optimize the return-cost trade-off. Experimental results on the DSRL benchmark show that RCDT consistently outperforms existing baselines in achieving superior return-cost trade-offs, advancing the state-of-the-art in offline safe RL.</div>
<div class="mono" style="margin-top:8px">本文针对离线安全强化学习中策略需能零样本适应不同成本阈值部署需求的问题，提出了RCDT方法，这是一种基于条件序列建模的方法，融合了拉格朗日式成本惩罚与自适应惩罚系数。该方法通过奖励-成本感知的轨迹重加权机制和Q值正则化，避免了过于保守的行为，优化了回报与成本的权衡。在DSRL基准上的大量实验表明，RCDT在回报-成本权衡方面持续优于代表性基线方法，推动了离线安全强化学习的技术前沿。</div>
</details>
</div>
<div class="card">
<div class="title">A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search</div>
<div class="meta-line">Authors: Yu Xue, Pengcheng Jiang, Chenchen Zhu, Yong Zhang, Ran Cheng, Kaizhou Gao, Dunwei Gong</div>
<div class="meta-line">First: 2026-02-09T11:04:07+00:00 · Latest: 2026-02-09T11:04:07+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Transactions on Neural Networks and Learning Systems. Published on this https URL: https://doi.org/10.1109/TNNLS.2026.3659508</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08513v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08513v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural architecture search (NAS) automates neural network design, improving efficiency over manual approaches. However, efficiently discovering high-performance neural network architectures that simultaneously optimize multiple objectives remains a significant challenge in NAS. Existing methods often suffer from limited population diversity and inadequate exploration of the search space, particularly in regions with extreme complexity values. To address these challenges, we propose MOEA-BUS, an innovative multi-objective evolutionary algorithm based on bi-population with uniform sampling for neural architecture search, aimed at simultaneously optimizing both accuracy and network complexity. In MOEA-BUS, a novel uniform sampling method is proposed to initialize the population, ensuring that architectures are distributed uniformly across the objective space. Furthermore, to enhance exploration, we deploy a bi-population framework where two populations evolve synergistically, facilitating comprehensive search space coverage. Experiments on CIFAR-10 and ImageNet demonstrate MOEA-BUS&#x27;s superiority, achieving top-1 accuracies of 98.39% on CIFAR-10, and 80.03% on ImageNet. Notably, it achieves 78.28% accuracy on ImageNet with only 446M MAdds. Ablation studies confirm that both uniform sampling and bi-population mechanisms enhance population diversity and performance. Additionally, in terms of the Kendall&#x27;s tau coefficient, the SVM achieves an improvement of at least 0.035 compared to the other three commonly used machine learning models, and uniform sampling provided an enhancement of approximately 0.07.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于双种群均匀采样的多目标进化算法用于神经架构搜索</div>
<div class="mono" style="margin-top:8px">神经架构搜索（NAS）通过自动化神经网络设计，相比人工方法提升了效率。然而，在NAS中，如何高效发现能同时优化多个目标的高性能神经网络架构仍是一个重大挑战。现有方法常受限于种群多样性不足及搜索空间探索不充分，尤其在具有极端复杂度值的区域。为应对这些挑战，我们提出MOEA-BUS——一种基于双种群均匀采样的创新多目标进化算法，旨在同时优化精度与网络复杂度。MOEA-BUS采用新颖的均匀采样方法初始化种群，确保架构在目标空间中均匀分布。此外，为增强探索能力，我们部署了双种群协同进化框架，促进对搜索空间的全面覆盖。在CIFAR-10和ImageNet上的实验表明，MOEA-BUS在CIFAR-10上达到98.39%的top-1准确率，在ImageNet上达到80.03%。值得注意的是，其仅用446M MAdds即在ImageNet上实现78.28%的准确率。消融研究证实均匀采样与双种群机制均能提升种群多样性和性能。此外，在肯德尔τ系数方面，SVM相比其他三种常用机器学习模型至少提升0.035，而均匀采样带来约0.07的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge in neural architecture search (NAS) of efficiently discovering high-performance architectures that optimize multiple objectives, such as accuracy and network complexity, where existing methods often suffer from limited population diversity and inadequate exploration. The authors propose MOEA-BUS, a multi-objective evolutionary algorithm based on a bi-population framework with uniform sampling, which initializes the population uniformly across the objective space and uses two synergistic populations to enhance exploration. Experimental results on CIFAR-10 and ImageNet show superior performance, achieving top-1 accuracies of 98.39% on CIFAR-10 and 80.03% on ImageNet, with a notable 78.28% accuracy on ImageNet at only 446M MAdds, while ablation studies confirm that both uniform sampling and the bi-population mechanism improve diversity and performance.</div>
<div class="mono" style="margin-top:8px">本文针对神经架构搜索（NAS）中高效发现同时优化多个目标（如精度和网络复杂度）的高性能架构的挑战，现有方法常存在种群多样性有限和搜索空间探索不足的问题。作者提出了MOEA-BUS，一种基于双种群框架和均匀采样的多目标进化算法，通过均匀采样初始化种群以确保目标空间均匀分布，并利用两个协同进化的种群来增强探索。在CIFAR-10和ImageNet上的实验结果表明了其优越性，在CIFAR-10上达到98.39%的top-1精度，在ImageNet上达到80.03%，其中在仅446M MAdds下在ImageNet上实现了78.28%的精度，消融研究证实均匀采样和双种群机制均提升了多样性和性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Self-Correction in Vision-Language Models via Rollout Augmentation</div>
<div class="meta-line">Authors: Yi Ding, Ziliang Qiu, Bolian Li, Ruqi Zhang</div>
<div class="meta-line">First: 2026-02-09T10:55:13+00:00 · Latest: 2026-02-09T10:55:13+00:00</div>
<div class="meta-line">Comments: 17 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08503v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08503v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过轨迹增强在视觉语言模型中学习自我校正</div>
<div class="mono" style="margin-top:8px">自我校正在视觉语言模型中解决复杂推理问题至关重要。然而，现有强化学习方法难以习得该能力，因为有效的自我校正行为极少出现，导致学习信号极度稀疏。为应对这一挑战，我们提出校正专用轨迹框架，通过重组现有轨迹合成密集的自我校正样本。该增强方法借助轨迹复用提升样本效率，同时通过均衡监督稳定强化学习优化。此外，我们引入响应掩码策略，将自我校正与直接推理解耦，避免信号冲突并使两种行为都能有效学习。基于此，我们推出具备可控自我校正能力的推理模型。在7个基准测试中，该模型在开源视觉语言模型中取得最先进性能，以仅需单步训练时间0.72倍的代价，超越最佳基线模型1.0分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of learning self-correction in vision-language models, where existing reinforcement learning methods suffer from sparse learning signals due to the rarity of effective self-correction behaviors. To overcome this, the authors propose Octopus, a framework that augments RL rollouts by recombining existing ones to synthesize dense self-correction examples, improving sample efficiency and stabilizing optimization. They also introduce a response-masking strategy to decouple self-correction from direct reasoning, preventing signal conflicts. The resulting model, Octopus-8B, achieves state-of-the-art performance across seven benchmarks among open-source VLMs, outperforming the best baseline by 1.0 score while requiring only 0.72 times the training time per step.</div>
<div class="mono" style="margin-top:8px">该论文针对视觉语言模型中自我纠正学习面临的挑战，即现有强化学习方法因有效自我纠正行为罕见而导致学习信号稀疏。为解决此问题，作者提出了Octopus框架，通过重组现有轨迹来合成密集的自我纠正示例，从而增强强化学习轨迹，提高样本效率并稳定优化过程。他们还引入了一种响应掩码策略，将自我纠正与直接推理解耦，避免信号冲突。由此开发的模型Octopus-8B在七个基准测试中实现了开源视觉语言模型的最先进性能，以每步仅需0.72倍训练时间的代价，超越最佳基线1.0分。</div>
</details>
</div>
<div class="card">
<div class="title">Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Xiaodong Lu, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin, Zhijun Chen, Yu Luo, Fuzhen Zhuang, Yikun Ban, Deqing Wang</div>
<div class="meta-line">First: 2026-02-09T10:51:58+00:00 · Latest: 2026-02-09T10:51:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08499v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08499v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于情境化滚动策略的强化学习可验证奖励机制</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是提升大语言模型推理能力的有效范式。然而现有RLVR方法在滚动策略使用上存在盲目性与短视性：对同一提示下质量参差的响应进行均质化处理，且历史滚动数据仅单次使用即被丢弃。这导致监督信号噪声大、样本效率低下及策略更新次优等问题。本研究通过将RLVR中的滚动调度构建为情境化多臂赌博机问题，提出统一的神经调度框架，能在训练过程中自适应筛选高价值滚动数据。每个滚动数据被视作一个赌博臂，其奖励由连续优化步骤间的性能增益定义。该调度器在统一理论框架内同时支持噪声感知的组内筛选与历史滚动数据的自适应全局复用。我们通过推导次线性遗憾界证明理论合理性，并表明扩大滚动缓冲区可提升性能上限。在六个数学推理基准测试上的实验表明，该框架在多种RLVR优化方法中均能持续提升性能与训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiencies in existing Reinforcement Learning with Verifiable Rewards (RLVR) methods, which treat all rollouts uniformly and discard them after single use, leading to noisy supervision and poor sample efficiency, this paper proposes a contextual bandit formulation for adaptive rollout scheduling. The method introduces a neural scheduling framework that treats each rollout as an arm, with rewards defined by performance gains between optimization steps, enabling noise-aware selection and adaptive reuse of historical rollouts. Experimental results on six mathematical reasoning benchmarks show consistent improvements in both performance and training efficiency across multiple RLVR optimization methods, supported by theoretical analysis demonstrating sublinear regret bounds and benefits from enlarging the rollout buffer.</div>
<div class="mono" style="margin-top:8px">针对现有可验证奖励强化学习（RLVR）方法中因均匀处理所有模拟轨迹并单次使用后丢弃所导致的监督噪声大、样本效率低的问题，本文提出将模拟轨迹调度建模为上下文赌博机问题。该方法设计了一个统一的神经调度框架，将每个模拟轨迹视为一个臂，其奖励由连续优化步骤间的性能增益定义，从而支持噪声感知的组内选择和历史轨迹的自适应全局重用。在六个数学推理基准上的实验结果表明，该方法在多种RLVR优化方法中均能一致提升性能和训练效率，理论分析也证明了次线性遗憾界及扩大轨迹缓冲区的益处。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Correctness: Learning Robust Reasoning via Transfer</div>
<div class="meta-line">Authors: Hyunseok Lee, Soheil Abbasloo, Jihoon Tack, Jinwoo Shin</div>
<div class="meta-line">First: 2026-02-09T10:41:44+00:00 · Latest: 2026-02-09T10:41:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08489v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08489v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR&#x27;s average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越正确性：通过迁移学习实现鲁棒推理</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）近期增强了大型语言模型的推理能力，但其对最终答案正确性的关注存在关键缺陷：无法确保推理过程本身的鲁棒性。我们采用一种简洁的哲学观点——鲁棒推理应在其产生者之外保持效用，并将推理视为一种必须经受截断、重释与延续的意义迁移形式。基于此原则，我们提出可迁移奖励强化学习（RLTR），通过迁移奖励将鲁棒性操作化：测试来自某模型的局部推理前缀能否引导另一模型得出正确答案。该方法促使大型语言模型产生稳定、可解释且真正可泛化的推理过程。我们的方法在提升采样一致性的同时提高了最终答案准确率，且能以显著更少的训练步骤达到可比性能。例如在MATH500数据集上，RLTR相比RLVR在Maj@64指标上获得+3.6%的提升，并以约2.5倍更少的训练步骤达到RLVR的平均准确率，既提供了更可靠的推理，也实现了显著的样本效率提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation that existing Reinforcement Learning with Verifiable Rewards (RLVR) focuses only on final answer correctness without ensuring the robustness of the reasoning process, this paper introduces Reinforcement Learning with Transferable Reward (RLTR) to promote stable and generalizable reasoning. The method operationalizes robustness by testing whether a partial reasoning prefix from one model can effectively guide a separate model to the correct answer, thereby encouraging reasoning that survives truncation and reinterpretation. Experimental results show that RLTR improves sampling consistency and final answer accuracy, achieving a +3.6%p gain in Maj@64 on MATH500 compared to RLVR and matching RLVR&#x27;s average accuracy with roughly 2.5x fewer training steps, demonstrating both more reliable reasoning and greater sample efficiency.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有基于可验证奖励的强化学习（RLVR）仅关注最终答案的正确性，而未能确保推理过程本身的鲁棒性，因此提出了基于可转移奖励的强化学习（RLTR）来促进稳定且可泛化的推理。该方法通过测试一个模型的部分推理前缀是否能有效指导另一个模型得出正确答案，从而将鲁棒性操作化，鼓励推理能够经受截断和重新解释。实验结果表明，RLTR提高了采样一致性和最终答案准确率，在MATH500数据集上相比RLVR在Maj@64指标上提升了3.6个百分点，并以约2.5倍更少的训练步数达到RLVR的平均准确率，证明了其推理更可靠且样本效率更高。</div>
</details>
</div>
<div class="card">
<div class="title">No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping</div>
<div class="meta-line">Authors: Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-26T05:03:54+00:00 · Latest: 2026-02-09T10:15:07+00:00</div>
<div class="meta-line">Comments: ICLR 2026 camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21880v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.21880v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bltnynk.github.io/publications/rl-zvp/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward -- so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce Reinforcement Learning with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR. The project page is available at https://bltnynk.github.io/publications/rl-zvp/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不遗漏任何提示：通过熵引导优势塑造利用大语言模型强化学习中的零方差提示</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是提升大语言模型（LLMs）推理能力的强大框架。然而，当前方法（如GRPO）仅依赖模型对同一输入产生不同正确性响应的任务，而忽略了所有响应获得相同奖励的所谓“零方差提示”。本研究主张此类提示并非无用，反而能为策略优化提供有意义的反馈。为此，我们提出零方差提示强化学习（RL-ZVP）算法，该算法从零方差提示中提取学习信号。RL-ZVP无需对比响应即可直接奖励正确行为并惩罚错误，同时通过词元级特征调节反馈以保留信息丰富、细致入微的信号。在六项数学推理基准测试中，RL-ZVP相比GRPO在准确率上最高提升8.61个百分点，通过率提升7.77个百分点，且持续优于其他过滤零方差提示的基线方法。这些结果揭示了RLVR中零方差提示学习尚未开发的潜力。项目页面详见：https://bltnynk.github.io/publications/rl-zvp/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that existing reinforcement learning methods for large language models discard prompts where all responses receive identical rewards, this paper introduces RL-ZVP, a novel algorithm that extracts learning signals from these zero-variance prompts by directly rewarding correctness and penalizing errors, modulated by token-level characteristics. The method demonstrates significant improvements over prior approaches like GRPO, achieving accuracy gains of up to 8.61 points and pass rate improvements of up to 7.77 points across six math reasoning benchmarks, thereby validating the utility of learning from previously ignored data.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到现有大语言模型强化学习方法会丢弃所有响应获得相同奖励的提示，因此提出了RL-ZVP这一新算法，通过直接奖励正确性和惩罚错误，并利用词元级特征调制反馈，从这些零方差提示中提取学习信号。实验结果表明，在六个数学推理基准上，该方法相比GRPO等基线取得了显著提升，准确率最高提高8.61个百分点，通过率最高提高7.77个百分点，从而验证了从先前被忽略的数据中学习的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Intelligent support for Human Oversight: Integrating Reinforcement Learning with Gaze Simulation to Personalize Highlighting</div>
<div class="meta-line">Authors: Thorsten Klößner, João Belo, Zekun Wu, Jörg Hoffmann, Anna Maria Feit</div>
<div class="meta-line">First: 2026-02-09T09:04:48+00:00 · Latest: 2026-02-09T09:04:48+00:00</div>
<div class="meta-line">Comments: AI CHAOS &#x27;26: Workshop Series on the Challenges for Human Oversight of AI Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08403v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08403v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interfaces for human oversight must effectively support users&#x27; situation awareness under time-critical conditions. We explore reinforcement learning (RL)-based UI adaptation to personalize alerting strategies that balance the benefits of highlighting critical events against the cognitive costs of interruptions. To enable learning without real-world deployment, we integrate models of users&#x27; gaze behavior to simulate attentional dynamics during monitoring. Using a delivery-drone oversight scenario, we present initial results suggesting that RL-based highlighting can outperform static, rule-based approaches and discuss challenges of intelligent oversight support.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类监督的智能支持：集成强化学习与视线模拟实现个性化高亮</div>
<div class="mono" style="margin-top:8px">人类监督界面必须在时间紧迫条件下有效支持用户的情境感知。本研究探索基于强化学习（RL）的界面自适应方法，通过个性化预警策略来平衡关键事件高亮的益处与认知中断的代价。为实现在无需实际部署的情况下进行学习，我们整合用户视线行为模型以模拟监控过程中的注意力动态。以配送无人机监督场景为例，初步结果表明基于强化学习的高亮策略优于静态的基于规则方法，并讨论了智能监督支持面临的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to enhance human oversight interfaces by improving situation awareness in time-critical scenarios, specifically through personalized alerting that balances highlighting benefits against interruption costs. The method employs reinforcement learning (RL) to adapt UI highlighting strategies, integrating gaze behavior models to simulate user attention dynamics without real-world deployment. Main experimental results, from a delivery-drone oversight scenario, indicate that RL-based highlighting outperforms static rule-based approaches, though challenges in intelligent oversight support are noted.</div>
<div class="mono" style="margin-top:8px">该研究的动机是通过个性化警报来增强时间关键场景下的人类监督界面，以平衡高亮关键事件的益处与中断的认知成本。方法上，它采用强化学习来调整用户界面高亮策略，并整合注视行为模型以模拟监控中的注意力动态，无需实际部署。主要实验结果基于配送无人机监督场景，表明基于强化学习的高亮方法优于静态的基于规则的方法，同时讨论了智能监督支持面临的挑战。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
