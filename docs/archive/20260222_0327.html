<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-22 03:27</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260222_0327</div>
    <div class="row"><div class="card">
<div class="title">SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer</div>
<div class="meta-line">Authors: Nathan S. de Lara, Florian Shkurti</div>
<div class="meta-line">First: 2026-02-19T18:47:31+00:00 · Latest: 2026-02-19T18:47:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17632v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17632v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMAC：基于分数匹配的演员-评论家算法实现稳健的离线至在线迁移</div>
<div class="mono" style="margin-top:8px">现代离线强化学习方法虽能训练出高性能的演员-评论家模型，但使用基于值的在线强化学习算法对其进行微调通常会导致性能骤降。我们提供的证据支持以下假设：在损失函数景观中，现有算法的离线最优解与在线最优解之间存在低性能谷地，而基于梯度的微调过程会穿越这些谷地。基于此，我们提出分数匹配演员-评论家算法（SMAC），这是一种离线强化学习方法，旨在训练出能够无缝迁移至在线值基强化学习算法且保持性能不降的演员-评论家模型。SMAC通过在离线阶段对Q函数施加正则化约束，使其满足策略分数与Q函数动作梯度之间的一阶导数等式，从而规避离线与在线最优解之间的性能谷地。实验表明，SMAC收敛至的离线最优解可通过一阶优化找到的单调递增奖励路径与更优的在线最优解相连。在全部6项D4RL任务中，SMAC均能平滑迁移至Soft Actor-Critic和TD3算法；在4/6的环境中，其遗憾值较最佳基线降低34-58%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that fine-tuning offline RL actor-critics online with value-based methods often leads to immediate performance drops, which the authors hypothesize is due to offline and online maxima being separated by low-performance valleys in the loss landscape. To address this, they propose Score Matched Actor-Critic (SMAC), a method that regularizes the Q-function during offline training to enforce a derivative equality between the policy score and the Q-function&#x27;s action-gradient, aiming to connect offline and online maxima. Experimental results show that SMAC enables smooth transfer to online algorithms like Soft Actor-Critic and TD3 across all six D4RL tasks, reducing regret by 34-58% in four environments compared to baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到离线强化学习中的行动者-评论家模型在使用基于值的方法进行在线微调时通常会导致性能立即下降，作者假设这是由于损失景观中离线最大值和在线最大值被低性能谷隔开所致。为解决此问题，他们提出了评分匹配行动者-评论家（SMAC）方法，该方法在离线训练期间通过正则化Q函数，强制策略评分与Q函数动作梯度之间的导数相等，以连接离线和在线最大值。实验结果表明，SMAC在全部六个D4RL任务中能平滑转移到Soft Actor-Critic和TD3等在线算法，在四个环境中相比基线将遗憾降低了34-58%。</div>
</details>
</div>
<div class="card">
<div class="title">Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs</div>
<div class="meta-line">Authors: Luke Huang, Zhuoyang Zhang, Qinghao Hu, Shang Yang, Song Han</div>
<div class="meta-line">First: 2026-02-19T18:40:51+00:00 · Latest: 2026-02-19T18:40:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17616v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17616v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稳定异步：面向大语言模型的方差控制离策略强化学习</div>
<div class="mono" style="margin-top:8px">强化学习被广泛用于提升大语言模型在推理任务上的表现，异步强化学习训练因其能提高端到端吞吐量而备受关注。然而，对于广泛采用的无评论家策略梯度方法（如REINFORCE和GRPO），高度异步性会使策略梯度估计器产生显著的方差增大：基于陈旧轨迹样本的训练会产生重尾重要性权重，导致少量样本主导参数更新。这种放大效应使梯度噪声增大，学习稳定性低于匹配的同策略训练。在数学和通用推理基准测试中，我们发现训练崩溃可通过有效样本量和梯度范数失稳可靠预测。基于此诊断，我们提出方差控制策略优化——一种针对REINFORCE/GRPO类算法的通用稳定方法，其通过（1）依据有效样本量动态缩放学习率以抑制不可靠更新，（2）为离策略场景应用闭式最小方差基线，避免引入辅助价值模型且仅增加极小开销。实验表明，VCPO在数学推理、通用推理和工具使用等任务的异步训练中显著提升鲁棒性，优于包括掩码/截断稳定器和算法变体在内的多种基线方法。该方法在保持同步训练性能的同时，将长上下文多轮训练时间缩短2.5倍，证明显式控制策略梯度方差是实现大规模可靠异步强化学习的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability in asynchronous reinforcement learning (RL) for large language models, where high asynchrony leads to high variance in policy-gradient estimators like REINFORCE and GRPO due to stale rollouts and heavy-tailed importance ratios, causing noisy gradients and learning collapse. To mitigate this, the authors propose Variance Controlled Policy Optimization (VCPO), a stabilization method that scales the learning rate based on effective sample size to dampen unreliable updates and employs a closed-form minimum-variance baseline for off-policy settings without needing an auxiliary value model. Experimental results across math, general reasoning, and tool-use benchmarks show that VCPO substantially improves robustness and reduces training time by 2.5× while matching synchronous performance, demonstrating its effectiveness in controlling policy-gradient variance for scalable asynchronous RL.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型异步强化学习中的不稳定性问题展开研究，指出高异步性会导致REINFORCE和GRPO等策略梯度估计器方差显著增大，原因是陈旧的样本轨迹和重尾重要性比率使得少量样本主导更新，引发梯度噪声和学习崩溃。为解决此问题，作者提出了方差控制策略优化（VCPO）方法，该方法基于有效样本量调整学习率以抑制不可靠更新，并采用闭式最小方差基线处理离策略场景，无需额外价值模型且开销极小。在数学、通用推理和工具使用等任务上的实验表明，VCPO显著提升了异步训练的鲁棒性，将长上下文多轮训练时间减少2.5倍的同时达到同步训练性能，验证了显式控制策略梯度方差对大规模可靠异步强化学习的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</div>
<div class="meta-line">Authors: Jowaria Khan, Anindya Sarkar, Yevgeniy Vorobeychik, Elizabeth Bondi-Kelly</div>
<div class="meta-line">First: 2026-02-19T18:30:18+00:00 · Latest: 2026-02-19T18:30:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17605v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17605v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method&#x27;s reliability at uncovering targets with limited data and a varying environment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态主动适应：面向地理空间发现的概念引导在线元学习框架</div>
<div class="mono" style="margin-top:8px">在环境监测、灾害响应或公共卫生等现实场景中，数据采集成本高昂且环境动态变化，如何在资源受限条件下通过未观测区域的策略性采样高效发现隐藏目标至关重要。然而，稀疏且存在偏差的地理空间真值数据限制了强化学习等现有基于学习的方法的适用性。为此，我们提出一个统一的地理空间发现框架，整合主动学习、在线元学习和概念引导推理。该框架基于*概念相关性*这一共享理念（用于捕捉领域特定因素对目标存在性的影响）构建了两项关键创新：一是*概念加权不确定性采样策略*，通过基于易获取的领域特定概念（如土地覆盖、污染源邻近度）学习到的相关性来调节不确定性；二是*相关性感知元批次构建策略*，在在线元更新过程中促进语义多样性，从而提升动态环境下的泛化能力。我们在真实世界致癌性PFAS（全氟及多氟烷基化合物）污染数据集上进行实验，验证了该方法在数据有限且环境多变的条件下发现目标的可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient geospatial discovery in dynamic, data-scarce real-world settings like environmental monitoring, this paper introduces a unified framework that integrates active learning, online meta-learning, and concept-guided reasoning. The method centers on a shared notion of concept relevance, employing a concept-weighted uncertainty sampling strategy to modulate sampling based on domain-specific factors and a relevance-aware meta-batch formation strategy to enhance generalization through semantic diversity during online updates. Experimental results on a real-world PFAS contamination dataset demonstrate the method&#x27;s reliability in uncovering hidden targets with limited data under varying environmental conditions.</div>
<div class="mono" style="margin-top:8px">本文针对环境监测等动态、数据稀缺的真实场景中高效地理空间发现的需求，提出了一个融合主动学习、在线元学习和概念引导推理的统一框架。该方法以概念相关性为核心，采用基于领域特定因素调节采样的概念加权不确定性采样策略，以及通过在线更新中语义多样性增强泛化能力的相关性感知元批次形成策略。在真实世界PFAS污染数据集上的实验结果表明，该方法能在数据有限和环境变化的条件下可靠地发现隐藏目标。</div>
</details>
</div>
<div class="card">
<div class="title">MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning</div>
<div class="meta-line">Authors: Xiaoliang Fu, Jiaye Lin, Yangyi Fang, Binbin Zheng, Chaowen Hu, Zekai Shao, Cong Qin, Lu Pan, Ke Zeng, Xunliang Cai</div>
<div class="meta-line">First: 2026-02-19T17:05:20+00:00 · Latest: 2026-02-19T17:05:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17550v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17550v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MASPO：统一梯度利用、概率质量与信号可靠性以实现鲁棒且样本高效的LLM推理</div>
<div class="mono" style="margin-top:8px">现有基于可验证奖励的强化学习（RLVR）算法（如GRPO）依赖僵化、均匀且对称的信任域机制，这与大型语言模型（LLM）复杂的优化动态存在根本性错配。本文指出此类方法存在三个关键挑战：（1）硬截断的二元截止导致梯度利用效率低下；（2）均匀比率约束忽视词元分布，引发概率质量不敏感；（3）正负样本间信用分配模糊性差异导致信号可靠性不对称。为弥合这些缺陷，我们提出质量自适应软策略优化（MASPO），这是一个统一框架，旨在协调上述三个维度。MASPO集成了可微分软高斯门控以最大化梯度效用，质量自适应限幅器以平衡概率空间探索，以及非对称风险控制器使更新幅度与信号置信度对齐。大量实验表明，MASPO作为一种鲁棒的全集成RLVR解决方案，显著优于现有基线方法。代码已开源：https://anonymous.4open.science/r/ma1/README.md。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of existing Reinforcement Learning with Verifiable Rewards (RLVR) methods for Large Language Models, which use rigid trust regions that lead to inefficient gradient use, insensitive handling of token probability mass, and unreliable asymmetric credit assignment. To address these issues, the method introduces Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework incorporating soft Gaussian gating for better gradient utilization, a mass-adaptive limiter for balanced probability exploration, and an asymmetric risk controller for reliable signal-based updates. Experimental results show that MASPO robustly outperforms strong baselines, offering a sample-efficient and effective RLVR solution for LLM reasoning.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有基于可验证奖励的强化学习方法在大型语言模型中的局限性，这些方法使用僵化的信任区域，导致梯度利用低效、对词元概率质量处理不敏感以及信用分配不对称不可靠。为解决这些问题，该方法提出了质量自适应软策略优化，这是一个统一框架，集成了软高斯门控以提升梯度效用、质量自适应限制器以平衡概率探索，以及非对称风险控制器以实现基于信号可靠性的更新。实验结果表明，MASPO鲁棒地超越了强基线方法，为LLM推理提供了一个样本高效且有效的RLVR解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Capturing Individual Human Preferences with Reward Features</div>
<div class="meta-line">Authors: André Barreto, Vincent Dumoulin, Yiran Mao, Mark Rowland, Nicolas Perez-Nieves, Bobak Shahriari, Yann Dauphin, Doina Precup, Hugo Larochelle</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-03-21T17:39:33+00:00 · Latest: 2026-02-19T16:23:22+00:00</div>
<div class="meta-line">Comments: Published at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.17338v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.17338v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback usually models preferences using a reward function that does not distinguish between people. We argue that this is unlikely to be a good design choice in contexts with high potential for disagreement, like in the training of large language models. We formalise and analyse the problem of learning a reward model that can be specialised to a user. Using the principle of empirical risk minimisation, we derive a probably approximately correct (PAC) bound showing the dependency of the approximation error on the number of training examples, as usual, and also on the number of human raters who provided feedback on them. Based on our theoretical findings, we discuss how to best collect pairwise preference data and argue that adaptive reward models should be beneficial when there is considerable disagreement among users. We also propose a concrete architecture for an adaptive reward model. Our approach leverages the observation that individual preferences can be captured as a linear combination of a set of general reward features. We show how to learn such features and subsequently use them to quickly adapt the reward model to a specific individual, even if their preferences are not reflected in the training data. We present experiments with large language models illustrating our theoretical results and comparing the proposed architecture with a non-adaptive baseline. Consistent with our analysis, the benefits provided by our model increase with the number of raters and the heterogeneity of their preferences. We also show that our model compares favourably to adaptive counterparts, including those performing in-context personalisation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用奖励特征捕捉个体人类偏好</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习通常使用不区分个体的奖励函数来建模偏好。我们认为，在存在高度分歧可能性的场景（如大语言模型训练）中，这并非理想的设计选择。我们形式化并分析了学习可适配特定用户的奖励模型问题。基于经验风险最小化原则，我们推导出近似正确概率（PAC）边界，揭示了近似误差对训练样本数量的依赖关系（如常规情况），以及对提供反馈的人类评分者数量的依赖关系。基于理论发现，我们探讨了如何最优收集成对偏好数据，并论证当用户间存在显著分歧时，自适应奖励模型应具有优势。我们还提出了一种自适应奖励模型的具体架构。该方法基于以下观察：个体偏好可表示为通用奖励特征集的线性组合。我们展示了如何学习此类特征，并利用它们快速将奖励模型适配至特定个体——即使其偏好未体现在训练数据中。我们通过大语言模型实验验证理论结果，并将所提架构与非自适应基线进行比较。与理论分析一致，我们模型的优势随评分者数量及其偏好异质性的增加而提升。实验还表明，我们的模型优于包括上下文个性化方法在内的其他自适应模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to model diverse human preferences in contexts like large language model training, this paper proposes learning adaptive reward models that capture individual differences. The method formalizes the problem using empirical risk minimization, deriving a PAC bound linking approximation error to both data quantity and rater count, and introduces an architecture where individual preferences are represented as linear combinations of learned general reward features, enabling quick personalization. Experimental results with large language models show that the adaptive model&#x27;s benefits increase with more raters and greater preference heterogeneity, outperforming non-adaptive baselines and other adaptive approaches, including in-context personalization methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是在大型语言模型训练等场景中，需要建模多样化的人类偏好，因此提出了学习能够捕捉个体差异的自适应奖励模型。方法上，通过经验风险最小化形式化该问题，推导出近似误差与数据量及评分者数量相关的PAC界，并提出一种架构，将个体偏好表示为已学习通用奖励特征的线性组合，从而实现快速个性化。在大型语言模型上的实验结果表明，自适应模型的优势随评分者数量和偏好异质性的增加而提升，优于非自适应基线及其他自适应方法，包括上下文个性化方法。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration</div>
<div class="meta-line">Authors: Yan Sun, Jia Guo, Stanley Kok, Zihao Wang, Zujie Wen, Zhiqiang Zhang</div>
<div class="meta-line">First: 2025-11-02T04:16:47+00:00 · Latest: 2026-02-19T16:18:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00794v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00794v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has improved the reasoning ability of large language models, yet training remains costly because many rollouts contribute little to optimization, considering the amount of computation required. This study investigates how simply leveraging intrinsic data properties, almost free benefit during training, can improve data efficiency for RLVR. We propose PREPO with two complementary components. First, we adopt prompt perplexity as an indicator of model adaptability in learning, enabling the model to progress from well-understood contexts to more challenging ones. Second, we amplify the discrepancy among the rollouts by differentiating their relative entropy, and prioritize sequences that exhibit a higher degree of exploration. Together, these mechanisms reduce rollout demand while preserving competitive performance. On the Qwen and Llama models, PREPO achieves effective results on mathematical reasoning benchmarks with up to 3 times fewer rollouts than the baselines. Beyond empirical gains, we provide theoretical and in-depth analyses explaining the underlying rationale of our method to improve the data efficiency of RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于内在探索的大语言模型高效强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）提升了大语言模型的推理能力，但由于许多计算轨迹对优化贡献有限，训练成本依然高昂。本研究探讨如何利用训练中几乎零成本的内在数据特性，提升RLVR的数据效率。我们提出PREPO方法，包含两个互补组件：首先，采用提示困惑度作为模型学习适应性的指标，使模型从已理解语境逐步过渡至更具挑战性的语境；其次，通过区分轨迹间的相对熵来放大其差异，并优先选择探索程度更高的序列。这些机制共同在保持竞争力的同时降低了轨迹需求。在Qwen和Llama模型上，PREPO在数学推理基准测试中以比基线少3倍的轨迹取得显著效果。除实证成果外，我们还通过理论与深度分析阐释了该方法提升RLVR数据效率的内在原理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high computational cost of reinforcement learning with verifiable rewards (RLVR) for large language models, where many rollouts are inefficient for optimization. The authors propose PREPO, a method that improves data efficiency by leveraging intrinsic data properties through two components: using prompt perplexity to guide learning from easier to harder contexts, and amplifying rollout discrepancy via relative entropy to prioritize exploratory sequences. Experiments on Qwen and Llama models show PREPO achieves competitive performance on mathematical reasoning benchmarks while reducing rollout demand by up to three times compared to baselines, supported by theoretical analysis of its efficiency gains.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型的可验证奖励强化学习训练成本高、许多计算轨迹优化效率低的问题，提出了一种提升数据效率的方法。研究者设计了PREPO框架，包含两个互补机制：利用提示困惑度作为模型适应性的指标，使学习从易到难推进；并通过区分相对熵来放大轨迹间的差异，优先选择探索性更强的序列。在Qwen和Llama模型上的实验表明，该方法在数学推理基准测试中仅需最多减少三倍的轨迹数量即可保持竞争力，同时提供了理论分析以解释其提升数据效率的原理。</div>
</details>
</div>
<div class="card">
<div class="title">LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy</div>
<div class="meta-line">Authors: Hsin-Jung Yang, Zhanhong Jiang, Prajwal Koirala, Qisai Liu, Cody Fleming, Soumik Sarkar</div>
<div class="meta-line">First: 2026-02-19T12:22:50+00:00 · Latest: 2026-02-19T12:22:50+00:00</div>
<div class="meta-line">Comments: 17th ACM/IEEE International Conference on Cyber-Physical Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17312v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17312v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task performance compared to constrained offline baselines. By unifying lexicographic prioritization with structural bias, LexiSafe offers a practical and theoretically grounded approach for safety-critical CPS decision-making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LexiSafe：基于词典序安全-奖励层次结构的离线安全强化学习</div>
<div class="mono" style="margin-top:8px">离线安全强化学习在信息物理系统中日益重要，因为训练期间的安全违规不可接受，且仅能使用预收集数据。现有离线安全强化学习方法通常通过约束松弛或联合优化来平衡奖励与安全，但缺乏防止安全漂移的结构化机制。本文提出LexiSafe，一种词典序离线强化学习框架，旨在保持安全对齐行为。我们首先开发LexiSafe-SC，针对标准离线安全强化学习的单成本公式，推导出安全违规与性能次优性边界，共同提供样本复杂度保证。随后扩展框架至层次化安全需求，提出LexiSafe-MC，支持多安全成本并具备独立的样本复杂度分析。实验表明，与约束离线基线相比，LexiSafe显著降低安全违规并提升任务性能。通过将词典序优先级与结构化偏置相统一，LexiSafe为安全关键信息物理系统决策提供了实用且理论完备的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of offline safe reinforcement learning for cyber-physical systems, where safety violations during training are unacceptable and only pre-collected data is available. Existing methods often lack structural mechanisms to prevent safety drift, so the authors propose LexiSafe, a lexicographic framework that prioritizes safety through a hierarchical approach. The method includes LexiSafe-SC for single-cost formulations with theoretical sample-complexity guarantees and LexiSafe-MC for multiple safety costs, both designed to preserve safety-aligned behavior. Experimental results show that LexiSafe reduces safety violations and improves task performance compared to constrained offline baselines, offering a practical, theoretically grounded solution for safety-critical decision-making.</div>
<div class="mono" style="margin-top:8px">本文针对信息物理系统的离线安全强化学习问题展开研究，其中训练期间的安全违规不可接受且仅能使用预收集数据。现有方法常缺乏防止安全漂移的结构性机制，因此作者提出了LexiSafe，一种基于词典序的框架，通过分层方法优先保障安全。该方法包括针对单成本公式的LexiSafe-SC（具有理论样本复杂度保证）和支持多安全成本的LexiSafe-MC，两者均旨在保持安全对齐行为。实验结果表明，与受限离线基线方法相比，LexiSafe减少了安全违规并提升了任务性能，为安全关键决策提供了实用且理论扎实的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">RLGT: A reinforcement learning framework for extremal graph theory</div>
<div class="meta-line">Authors: Ivan Damnjanović, Uroš Milivojević, Irena Đorđević, Dragan Stevanović</div>
<div class="meta-line">First: 2026-02-19T11:25:22+00:00 · Latest: 2026-02-19T11:25:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17276v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a subfield of machine learning that focuses on developing models that can autonomously learn optimal decision-making strategies over time. In a recent pioneering paper, Wagner demonstrated how the Deep Cross-Entropy RL method can be applied to tackle various problems from extremal graph theory by reformulating them as combinatorial optimization problems. Subsequently, many researchers became interested in refining and extending the framework introduced by Wagner, thereby creating various RL environments specialized for graph theory. Moreover, a number of problems from extremal graph theory were solved through the use of RL. In particular, several inequalities concerning the Laplacian spectral radius of graphs were refuted, new lower bounds were obtained for certain Ramsey numbers, and contributions were made to the Turán-type extremal problem in which the forbidden structures are cycles of length three and four. Here, we present Reinforcement Learning for Graph Theory (RLGT), a novel RL framework that systematizes the previous work and provides support for both undirected and directed graphs, with or without loops, and with an arbitrary number of edge colors. The framework efficiently represents graphs and aims to facilitate future RL-based research in extremal graph theory through optimized computational performance and a clean and modular design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLGT：极值图论中的强化学习框架</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是机器学习的一个子领域，专注于开发能够随时间自主习得最优决策策略的模型。在近期一篇开创性论文中，Wagner展示了如何通过将极值图论中的各类问题重构为组合优化问题，应用深度交叉熵强化学习方法加以解决。随后，许多研究者致力于改进和扩展Wagner提出的框架，由此创建了多种专用于图论的强化学习环境。此外，已有若干极值图论问题借助强化学习得以解决：具体包括证伪了关于图谱拉普拉斯半径的若干不等式、获得了特定拉姆齐数的新下界，并对禁止结构为三阶与四阶环的图兰型极值问题作出了贡献。本文提出“图论强化学习（RLGT）”这一新型强化学习框架，该系统整合了先前工作，支持含或不含自环的无向图与有向图，并可处理任意边着色数量。该框架实现了高效的图表示，并通过优化的计算性能与清晰模块化设计，旨在推动未来基于强化学习的极值图论研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to systematize and extend prior applications of reinforcement learning (RL) to extremal graph theory, this paper introduces RLGT, a novel RL framework designed to unify and support research in this area. The method involves a modular framework that efficiently represents both undirected and directed graphs, with or without loops and multiple edge colors, optimizing computational performance for combinatorial optimization problems. Key experimental results from applying this and related RL approaches include refuting several inequalities concerning the Laplacian spectral radius, obtaining new lower bounds for certain Ramsey numbers, and contributing to Turán-type extremal problems involving forbidden cycles.</div>
<div class="mono" style="margin-top:8px">本文的动机是系统化和扩展强化学习在极值图论中的应用，为此提出了RLGT这一新颖的强化学习框架。该方法采用模块化设计，高效表示无向图和有向图，支持自环和多边着色，并通过优化计算性能来处理组合优化问题。主要的实验成果包括：利用相关强化学习方法否定了若干关于拉普拉斯谱半径的不等式，获得了某些拉姆齐数的新下界，并对涉及禁止三阶和四阶环的Turán型极值问题做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies</div>
<div class="meta-line">Authors: Ximan Sun, Xiang Cheng</div>
<div class="meta-line">First: 2025-10-28T21:26:18+00:00 · Latest: 2026-02-19T11:08:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24983v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24983v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion policies are competitive for offline reinforcement learning (RL) but are typically guided at sampling time by heuristics that lack a statistical notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that treats each denoising step as a sequential hypothesis test between the unconditional prior and the state-conditional policy head. Concretely, we accumulate a log-likelihood ratio and gate the conditional mean with a logistic controller whose threshold tau is calibrated once under H0 to meet a user-specified Type-I level alpha. This turns guidance from a fixed push into an evidence-driven adjustment with a user-interpretable risk budget. Importantly, we deliberately leave training vanilla (two heads with standard epsilon-prediction) under the structure of DDPM. LRT guidance composes naturally with Q-gradients: critic-gradient updates can be taken at the unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum from exploitation to conservatism. We standardize states and actions consistently at train and test time and report a state-conditional out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks, LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines in our implementation while honoring the desired alpha. Theoretically, we establish level-alpha calibration, concise stability bounds, and a return comparison showing when LRT surpasses Q-guidance-especially when off-support errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method that adds principled, calibrated risk control to diffusion policies for offline RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LRT-Diffusion：扩散策略的校准风险感知引导方法</div>
<div class="mono" style="margin-top:8px">扩散策略在离线强化学习中具有竞争力，但采样时通常依赖缺乏风险统计定义的启发式引导。我们提出LRT-Diffusion，这是一种风险感知采样规则，将每个去噪步骤视为无条件先验与状态条件策略头之间的序贯假设检验。具体而言，我们累积对数似然比，并通过逻辑控制器对条件均值进行门控——该控制器的阈值τ在H0假设下一次性校准，以满足用户指定的I类错误水平α。这使得引导从固定推动转变为具有可解释风险预算的证据驱动调整。重要的是，我们在DDPM结构下刻意保持训练过程原始性（采用标准ε预测的双头结构）。LRT引导可与Q梯度自然组合：批评者梯度更新可在无条件均值、LRT门控均值或二者混合处进行，形成从利用到保守的连续谱。我们在训练和测试时对状态与动作进行标准化处理，并同时报告状态条件分布外指标与回报。在D4RL MuJoCo任务中，LRT-Diffusion在实现预期α水平的同时，通过我们的实现改进了回报-分布外权衡关系。理论上，我们建立了α水平校准、简洁稳定性边界及回报比较分析，证明LRT在分布外误差主导时尤其优于Q引导。总体而言，LRT-Diffusion是一种即插即用的推理时方法，为离线RL的扩散策略增添了原则性、可校准的风险控制机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces LRT-Diffusion, a method to address the lack of statistical risk control in diffusion policies for offline reinforcement learning, which typically rely on heuristic guidance. The approach treats each denoising step as a sequential hypothesis test, using a log-likelihood ratio and a logistic controller calibrated to a user-specified Type-I error level, thereby providing evidence-driven, risk-aware sampling without modifying the vanilla training process. Experimental results on D4RL MuJoCo tasks demonstrate that LRT-Diffusion improves the trade-off between return and out-of-distribution detection compared to Q-guided baselines while maintaining the desired risk level, with theoretical guarantees on calibration and stability.</div>
<div class="mono" style="margin-top:8px">本文提出了LRT-Diffusion方法，旨在解决离线强化学习中扩散策略因依赖启发式引导而缺乏统计风险控制的问题。该方法将每个去噪步骤视为序贯假设检验，利用对数似然比和经用户指定第一类错误水平校准的逻辑控制器，在不改变原始训练过程的情况下实现证据驱动的风险感知采样。在D4RL MuJoCo任务上的实验结果表明，与基于Q值引导的基线方法相比，LRT-Diffusion在保持期望风险水平的同时，改善了回报与分布外检测之间的权衡，并提供了校准和稳定性的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">Continual uncertainty learning</div>
<div class="meta-line">Authors: Heisei Yonezawa, Ansei Yonezawa, Itsuro Kajiwara</div>
<div class="meta-line">First: 2026-02-19T08:39:42+00:00 · Latest: 2026-02-19T08:39:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>持续不确定性学习</div>
<div class="mono" style="margin-top:8px">具有多重不确定性的机械系统鲁棒控制仍是一个根本性挑战，尤其在非线性动力学与工况变化错综交织时。尽管深度强化学习结合领域随机化在缓解仿真到现实的差距方面展现出潜力，但同步处理所有不确定性源常导致策略次优与学习效率低下。本研究针对涉及多重不确定性同时叠加的非线性动力学系统鲁棒控制问题，提出一种基于课程学习的持续学习框架。其核心思想是将含多重不确定性的复杂控制问题分解为一系列持续学习任务，依次获取处理各类不确定性的策略。通过将原系统扩展为有限个动态不确定性随学习进程逐步扩展与多样化的被控对象集合，策略能在不同不确定性配置定义的任务关联的全体对象集上稳定更新，避免灾难性遗忘。为提升学习效率，我们将基于模型的控制器联合融入学习过程，该控制器可确保跨对象集的共享基线性能，从而加速收敛。此残差学习机制促进了深度强化学习智能体针对各不确定性的任务专用优化，进而提高样本效率。作为实际工业应用，本研究将所提方法用于设计汽车动力总成主动振动控制器，验证了所得控制器对结构非线性和动态变化具有鲁棒性，成功实现了仿真到现实的迁移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robust control for nonlinear mechanical systems with multiple, intertwined uncertainties, where standard deep reinforcement learning with domain randomization often yields inefficient learning and suboptimal policies. The authors propose a continual learning framework that decomposes the complex control problem into a sequence of tasks, gradually expanding and diversifying dynamic uncertainties across a set of plants to acquire strategies sequentially without catastrophic forgetting. A model-based controller is incorporated to ensure baseline performance and accelerate convergence via residual learning. Experimental validation on an automotive powertrain vibration control application demonstrates that the resulting controller achieves robustness against structural nonlinearities and dynamic variations, enabling successful sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">本文针对具有多种交织不确定性的非线性机械系统的鲁棒控制挑战，其中标准的深度强化学习与领域随机化方法常导致学习效率低下和策略次优。作者提出一种持续学习框架，将复杂的控制问题分解为一系列任务，在学习过程中逐步扩展和多样化一组被控对象的动态不确定性，从而顺序获取应对策略且避免灾难性遗忘。该方法结合了一个模型控制器来保证基线性能，并通过残差学习加速收敛。在汽车动力总成振动控制的实际工业应用中验证表明，所得控制器对结构非线性和动态变化具有鲁棒性，成功实现了从仿真到现实的迁移。</div>
</details>
</div>
<div class="card">
<div class="title">CaveAgent: Transforming LLMs into Stateful Runtime Operators</div>
<div class="meta-line">Authors: Maohao Ran, Zhenglin Wan, Cooper Lin, Yanting Zhang, Hongyu Xin, Hongwei Fan, Yibo Xu, Beier Luo, Yaxin Zhou, Wangbo Zhao, Lijie Yang, Lang Feng, Fuchao Yang, Jingxuan Wu, Yiqiao Huang, Chendong Ma, Dailing Jiang, Jianbo Deng, Sirui Han, Yang You, Bo An, Yike Guo, Jun Song</div>
<div class="meta-line">First: 2026-01-04T15:32:47+00:00 · Latest: 2026-02-19T07:07:15+00:00</div>
<div class="meta-line">Comments: ver.2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01569v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01569v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms that struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. We present CaveAgent, a framework that shifts tool use from ``LLM-as-Text-Generator&#x27;&#x27; to ``LLM-as-Runtime-Operator.&#x27;&#x27; CaveAgent introduces a dual-stream architecture that inverts the conventional paradigm: rather than treating the LLM&#x27;s text context as the primary workspace with tools as auxiliary, CaveAgent elevates the persistent Python runtime as the central locus of state, with a lightweight semantic stream serving as its orchestrator. Beyond leveraging code generation to resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, CaveAgent introduces \textit{Stateful Runtime Management}: it injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns, unlike existing code-based approaches that remain text-bound. CaveAgent further provides a runtime-integrated skill management system that extends the Agent Skills open standard, enabling ecosystem interoperability through executable skill injections. This persistence mechanism serves as a high-fidelity external memory that reduces context drift in multi-turn interactions and preserves processed data for downstream applications without information loss. Evaluations show consistent improvement across challenging benchmarks, enabling CaveAgent to handle data scales that cause context overflow in both JSON-based and code-based agents. The accessible runtime state further provides programmatically verifiable feedback, enabling automated evaluation and reward signal generation without human annotation and establishing a structural foundation for future research in Reinforcement Learning with Verifiable Rewards (RLVR).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CaveAgent：将大语言模型转化为有状态运行时算子</div>
<div class="mono" style="margin-top:8px">基于大语言模型的智能体执行复杂任务的能力日益增强，但现有智能体系统仍受限于以文本为中心的范式，因其脆弱的多轮依赖关系和语境漂移问题，难以处理长周期任务。本文提出CaveAgent框架，将工具使用范式从&#x27;LLM作为文本生成器&#x27;转变为&#x27;LLM作为运行时算子&#x27;。该框架采用双流架构颠覆传统范式：不再将大语言模型的文本语境作为主要工作空间、工具作为辅助，而是将持久化Python运行时提升为核心状态载体，由轻量级语义流充当协调器。除通过代码生成单步解决相互依赖的子任务（如循环、条件判断）外，CaveAgent引入&#x27;有状态运行时管理&#x27;机制：注入、操作和检索跨轮次持久化的复杂Python对象（如DataFrame、数据库连接），突破现有基于代码方法仍受文本束缚的限制。框架进一步提供运行时集成的技能管理系统，扩展了Agent Skills开放标准，通过可执行技能注入实现生态互操作性。这种持久化机制作为高保真外部记忆，减少多轮交互中的语境漂移，完整保存已处理数据供下游应用使用。实验表明，在多项挑战性基准测试中均取得稳定提升，使CaveAgent能够处理导致JSON基和代码基智能体语境溢出的数据规模。可访问的运行时状态进一步提供可编程验证的反馈，无需人工标注即可实现自动评估与奖励信号生成，为&#x27;可验证奖励强化学习&#x27;的未来研究奠定结构基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind CaveAgent is to overcome the limitations of text-centric LLM agents, which struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. The method introduces a dual-stream architecture that shifts from treating the LLM as a text generator to using it as a runtime operator, elevating a persistent Python runtime as the central state locus with a lightweight semantic stream as orchestrator, enabling stateful runtime management of complex Python objects across turns and a runtime-integrated skill system. Main experimental results show consistent improvements on challenging benchmarks, allowing CaveAgent to handle data scales that cause context overflow in existing agents while providing programmatically verifiable feedback for automated evaluation.</div>
<div class="mono" style="margin-top:8px">CaveAgent的动机是克服以文本为中心的大型语言模型代理在长视野任务中因脆弱的多轮依赖和上下文漂移而受限的问题。该方法采用双流架构，将LLM从文本生成器转变为运行时操作符，将持久化的Python运行时提升为核心状态载体，以轻量级语义流作为编排器，实现了跨轮次的复杂Python对象的状态化运行时管理和运行时集成的技能系统。主要实验结果表明，在具有挑战性的基准测试中持续改进，使CaveAgent能够处理导致现有代理上下文溢出的数据规模，同时提供可编程验证的反馈以支持自动评估。</div>
</details>
</div>
<div class="card">
<div class="title">Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning</div>
<div class="meta-line">Authors: Jaebak Hwang, Sanghyeon Lee, Jeongmo Kim, Seungyul Han</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-26T06:35:42+00:00 · Latest: 2026-02-19T06:46:42+00:00</div>
<div class="meta-line">Comments: 10 pages for main, 26 pages for total, Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21039v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.21039v2">PDF</a> · <a href="https://github.com/Jaebak1996/SSE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon goal-conditioned tasks pose fundamental challenges for reinforcement learning (RL), particularly when goals are distant and rewards are sparse. While hierarchical and graph-based methods offer partial solutions, their reliance on conventional hindsight relabeling often fails to correct subgoal infeasibility, leading to inefficient high-level planning. To address this, we propose Strict Subgoal Execution (SSE), a graph-based hierarchical RL framework that integrates Frontier Experience Replay (FER) to separate unreachable from admissible subgoals and streamline high-level decision making. FER delineates the reachability frontier using failure and partial-success transitions, which identifies unreliable subgoals, increases subgoal reliability, and reduces unnecessary high-level decisions. Additionally, SSE employs a decoupled exploration policy to cover underexplored regions of the goal space and a path refinement that adjusts edge costs using observed low-level failures. Experimental results across diverse long-horizon benchmarks show that SSE consistently outperforms existing goal-conditioned and hierarchical RL methods in both efficiency and success rate. Our code is available at https://github.com/Jaebak1996/SSE</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>严格子目标执行：分层强化学习中可靠的长时程规划</div>
<div class="mono" style="margin-top:8px">长时程目标导向任务对强化学习提出了根本性挑战，尤其在目标遥远且奖励稀疏的情况下。尽管分层和图基方法提供了部分解决方案，但它们对传统后见之明重标记的依赖往往无法修正子目标不可行性，导致高层规划效率低下。为此，我们提出严格子目标执行——一种基于图的分层强化学习框架，集成前沿经验回放以区分不可达与可采纳的子目标，并优化高层决策。该回放机制利用失败与部分成功的状态转移划定可达性边界，识别不可靠子目标，提升子目标可靠性，并减少不必要的高层决策。此外，本框架采用解耦探索策略覆盖目标空间中未充分探索的区域，并通过路径细化机制利用观测到的底层失败调整边权成本。在多种长时程基准测试中，本方法在效率与成功率上均持续优于现有目标导向及分层强化学习方法。代码已开源：https://github.com/Jaebak1996/SSE</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of long-horizon goal-conditioned reinforcement learning, where sparse rewards and distant goals hinder performance, and existing hierarchical methods suffer from subgoal infeasibility due to conventional hindsight relabeling. The proposed method, Strict Subgoal Execution (SSE), introduces a graph-based hierarchical framework that integrates Frontier Experience Replay (FER) to distinguish reachable from unreachable subgoals, along with a decoupled exploration policy and path refinement using low-level failures to adjust edge costs. Experimental results across various long-horizon benchmarks demonstrate that SSE consistently surpasses existing goal-conditioned and hierarchical RL approaches in both efficiency and success rate.</div>
<div class="mono" style="margin-top:8px">本文针对长视野目标条件强化学习中的挑战，即稀疏奖励和遥远目标导致性能不佳，而现有分层方法因传统事后重标记常产生子目标不可行问题。所提出的方法——严格子目标执行（SSE），引入了一个基于图的分层框架，集成了前沿经验回放（FER）以区分可达与不可达子目标，同时采用解耦探索策略和基于低级失败调整边成本的路径细化。在多种长视野基准测试中的实验结果表明，SSE在效率和成功率上均一致优于现有的目标条件与分层强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning for Optimal Portfolio Allocation: A Comparative Study with Mean-Variance Optimization</div>
<div class="meta-line">Authors: Srijan Sood, Kassiani Papasotiriou, Marius Vaiciulis, Tucker Balch</div>
<div class="meta-line">First: 2026-02-19T05:47:23+00:00 · Latest: 2026-02-19T05:47:23+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures. Published at the FinPlan&#x27;23 Workshop, the 33rd International Conference on Automated Planning and Scheduling (ICAPS 2023)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17098v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Portfolio Management is the process of overseeing a group of investments, referred to as a portfolio, with the objective of achieving predetermined investment goals. Portfolio optimization is a key component that involves allocating the portfolio assets so as to maximize returns while minimizing risk taken. It is typically carried out by financial professionals who use a combination of quantitative techniques and investment expertise to make decisions about the portfolio allocation.
  Recent applications of Deep Reinforcement Learning (DRL) have shown promising results when used to optimize portfolio allocation by training model-free agents on historical market data. Many of these methods compare their results against basic benchmarks or other state-of-the-art DRL agents but often fail to compare their performance against traditional methods used by financial professionals in practical settings. One of the most commonly used methods for this task is Mean-Variance Portfolio Optimization (MVO), which uses historical time series information to estimate expected asset returns and covariances, which are then used to optimize for an investment objective.
  Our work is a thorough comparison between model-free DRL and MVO for optimal portfolio allocation. We detail the specifics of how to make DRL for portfolio optimization work in practice, also noting the adjustments needed for MVO. Backtest results demonstrate strong performance of the DRL agent across many metrics, including Sharpe ratio, maximum drawdowns, and absolute returns.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习在最优投资组合配置中的应用：与均值-方差优化方法的比较研究</div>
<div class="mono" style="margin-top:8px">投资组合管理是监督一组投资（即投资组合）以实现预定投资目标的过程。投资组合优化是其核心环节，旨在通过资产配置实现收益最大化与风险最小化。该任务通常由金融专业人士运用量化技术与投资经验协同完成。
  深度强化学习（DRL）的最新应用表明，通过基于历史市场数据训练无模型智能体来优化投资组合配置，已取得显著成效。现有研究多将DRL方法与基础基准或其他前沿DRL智能体进行比较，却常忽略其与实际金融从业者采用的传统方法的对比。均值-方差投资组合优化（MVO）是该领域最常用的传统方法之一，其利用历史时间序列数据估算资产预期收益与协方差，进而优化投资目标。
  本研究对无模型DRL与MVO在最优投资组合配置中的表现进行了系统比较。我们详细阐述了DRL应用于投资组合优化的实践方案，并说明了MVO所需的调整。回测结果表明，DRL智能体在夏普比率、最大回撤和绝对收益等多个指标上均表现优异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to compare modern deep reinforcement learning (DRL) approaches for portfolio optimization against the traditional and widely-used mean-variance optimization (MVO) method, as prior DRL studies often lacked such practical benchmarking. The method involves a detailed, practical implementation of a model-free DRL agent trained on historical market data, alongside necessary adjustments for a standard MVO baseline, to ensure a fair comparison. The main experimental results from backtesting show that the DRL agent achieves strong performance, outperforming MVO across key financial metrics including Sharpe ratio, maximum drawdown, and absolute returns.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，需要将现代深度强化学习（DRL）投资组合优化方法与传统且广泛使用的均值-方差优化（MVO）方法进行对比，因为先前的DRL研究常缺乏此类实际基准测试。方法包括详细、实用的模型无关DRL智能体实现（基于历史市场数据训练），以及对标准MVO基线进行必要调整，以确保公平比较。主要回溯测试实验结果表明，DRL智能体表现强劲，在夏普比率、最大回撤和绝对收益等多个关键金融指标上均优于MVO方法。</div>
</details>
</div>
<div class="card">
<div class="title">Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Sijia li, Xinran Li, Shibo Chen, Jun Zhang</div>
<div class="meta-line">First: 2026-01-12T12:17:11+00:00 · Latest: 2026-02-19T05:27:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07463v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07463v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拼图求解：面向离线多智能体强化学习的局部到全局世界模型</div>
<div class="mono" style="margin-top:8px">离线多智能体强化学习（MARL）旨在利用预收集数据集解决多智能体系统中的协作决策问题。现有离线MARL方法主要将训练约束在数据集分布内，导致策略过于保守，难以泛化至数据支持范围之外。基于模型的方法通过学习的世界模型生成合成数据以扩展原始数据集，为问题提供了可行方案，但多智能体系统的高维性、非平稳性和复杂性使得准确估计离线MARL中的状态转移与奖励函数极具挑战。针对直接建模联合动力学的困难，本文提出局部到全局（LOGO）世界模型，该创新框架利用更易估计的局部预测来推断全局状态动力学，在隐式捕获智能体间依赖关系的同时提升预测精度。借助训练后的世界模型生成合成数据以增强原始数据集，从而扩展有效状态-动作空间。为确保策略学习的可靠性，进一步引入不确定性感知采样机制，通过预测不确定性自适应加权合成数据，减少近似误差向策略的传播。相较于传统的集成方法，本方法仅需额外编码器进行不确定性估计，在保持精度的同时显著降低计算开销。在8种场景下与8个基线方法的对比实验表明，本方法在标准离线MARL基准测试中超越现有最优基线，为可泛化的离线多智能体学习建立了新的基于模型基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of overly conservative policies in offline multi-agent reinforcement learning (MARL) by proposing a model-based approach to expand the dataset distribution and improve generalization. The method introduces a local-to-global (LOGO) world model that infers global state dynamics from easier-to-estimate local predictions, capturing agent dependencies, and augments data with synthetic samples while using an uncertainty-aware sampling mechanism to weight them based on prediction error, reducing computational cost compared to ensemble methods. Experimental results across eight scenarios show that this approach outperforms state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable learning.</div>
<div class="mono" style="margin-top:8px">本文针对离线多智能体强化学习中策略过于保守、难以泛化的问题，提出了一种基于模型的方法来扩展数据集分布并提升泛化能力。该方法采用局部到全局的世界模型，通过易于估计的局部预测来推断全局状态动态，从而捕捉智能体间的依赖关系，并利用合成数据增强原始数据集；同时引入不确定性感知的采样机制，根据预测误差自适应加权合成数据，降低了计算开销。在八个场景上的广泛实验表明，该方法在标准离线多智能体强化学习基准测试中超越了现有先进基线，为可泛化的离线多智能体学习建立了新的基于模型的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Decision-Making under Model Misspecification: A Stochastic Stability Approach</div>
<div class="meta-line">Authors: Xinyu Dai, Daniel Chen, Yian Qian</div>
<div class="meta-line">First: 2026-02-19T05:14:09+00:00 · Latest: 2026-02-19T05:14:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17086v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17086v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic decision-making under model uncertainty is central to many economic environments, yet existing bandit and reinforcement learning algorithms rely on the assumption of correct model specification. This paper studies the behavior and performance of one of the most commonly used Bayesian reinforcement learning algorithms, Thompson Sampling (TS), when the model class is misspecified. We first provide a complete dynamic classification of posterior evolution in a misspecified two-armed Gaussian bandit, identifying distinct regimes: correct model concentration, incorrect model concentration, and persistent belief mixing, characterized by the direction of statistical evidence and the model-action mapping. These regimes yield sharp predictions for limiting beliefs, action frequencies, and asymptotic regret. We then extend the analysis to a general finite model class and develop a unified stochastic stability framework that represents posterior evolution as a Markov process on the belief simplex. This approach characterizes two sufficient conditions to classify the ergodic and transient behaviors and provides inductive dimensional reductions of the posterior dynamics. Our results offer the first qualitative and geometric classification of TS under misspecification, bridging Bayesian learning with evolutionary dynamics, and also build the foundations of robust decision-making in structured bandits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模型误设下的动态决策：一种随机稳定性方法</div>
<div class="mono" style="margin-top:8px">模型不确定性下的动态决策是许多经济环境的核心问题，但现有的多臂赌博机与强化学习算法均依赖于模型正确设定的假设。本文研究了最常用的贝叶斯强化学习算法之一——汤普森采样在模型类别误设时的行为与性能表现。我们首先对误设双臂高斯赌博机中的后验演化进行了完整的动态分类，识别出由统计证据方向与模型-行动映射关系所界定的三种不同机制：正确模型集中、错误模型集中以及持续信念混合。这些机制对极限信念、行动频率与渐近遗憾给出了精确预测。随后我们将分析扩展至一般有限模型类别，并建立了一个统一的随机稳定性框架，将后验演化表示为信念单纯形上的马尔可夫过程。该方法通过两个充分条件对遍历性与暂态行为进行分类，并提供了后验动力学的归纳降维分析。我们的研究首次对误设条件下的汤普森采样进行了定性与几何分类，搭建了贝叶斯学习与演化动力学之间的桥梁，同时为结构化赌博机中的鲁棒决策奠定了理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the performance of Thompson Sampling, a widely used Bayesian reinforcement learning algorithm, when the underlying model class is misspecified, a common but understudied challenge in dynamic decision-making. The authors first analyze a two-armed Gaussian bandit, categorizing posterior evolution into distinct regimes—correct concentration, incorrect concentration, and persistent mixing—which predict limiting beliefs, action frequencies, and asymptotic regret. They then generalize this analysis to finite model classes using a novel stochastic stability framework that treats posterior dynamics as a Markov process, enabling the classification of ergodic and transient behaviors through inductive dimensional reductions. The results provide the first geometric classification of Thompson Sampling under model misspecification, linking Bayesian learning with evolutionary dynamics and establishing foundations for robust decision-making in structured bandits.</div>
<div class="mono" style="margin-top:8px">本文研究了在模型类别设定错误的情况下，广泛使用的贝叶斯强化学习算法汤普森采样的性能，这是动态决策中常见但研究不足的挑战。作者首先分析了一个双臂高斯赌博机，将后验演化分类为不同的机制——正确集中、错误集中和持续混合——这些机制预测了极限信念、行动频率和渐近遗憾。随后，他们使用一种新颖的随机稳定性框架将分析推广到有限模型类别，该框架将后验动态视为马尔可夫过程，通过归纳降维实现了遍历和瞬态行为的分类。这些成果首次提供了模型设定错误下汤普森采样的几何分类，将贝叶斯学习与演化动力学联系起来，并为结构化赌博机中的稳健决策奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments</div>
<div class="meta-line">Authors: Sushant Mehta, Logan Ritchie, Suhaas Garre, Nick Heiner, Edwin Chen</div>
<div class="meta-line">First: 2026-02-18T04:35:46+00:00 · Latest: 2026-02-19T05:10:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16179v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16179v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce CoreCraft, the first environment in EnterpriseBench, Surge AI&#x27;s suite of agentic RL environments. CoreCraft is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM 4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37% to 36.76% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5% on BFCL Parallel, +7.4% on Tau2-Bench Retail, and +6.8% on Tool Decathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EnterpriseBench Corecraft：在高保真强化学习环境中训练可泛化智能体</div>
<div class="mono" style="margin-top:8px">研究表明，在高保真强化学习环境中训练AI智能体能够产生超越训练分布范围的泛化能力。我们推出CoreCraft——这是Surge AI智能体强化学习环境套件EnterpriseBench中的首个环境。CoreCraft是一个完全可运行的客户支持组织企业级仿真系统，包含14种实体类型超过2500个实体及23种独特工具，旨在评估AI智能体能否执行现实工作中所需的多步骤、领域特定任务。当需要满足所有专家制定的评估标准时，GPT-5.2和Claude Opus 4.6等前沿模型的任务通过率不足30%。通过该环境，我们采用组相对策略优化（GRPO）与自适应剪裁技术对GLM 4.6进行训练。仅单轮训练后，模型在保留评估任务上的通过率就从25.37%提升至36.76%。更重要的是，这些提升能迁移至分布外基准测试：BFCL Parallel提升4.5%、Tau2-Bench Retail提升7.4%、Tool Decathlon（Pass@1）提升6.8%。我们认为三个环境特性与观察到的迁移效果相符：以任务为中心的世界构建确保任务多样性与挑战性；专家制定的评估标准实现可靠奖励计算；反映真实专业模式的企业工作流。结果表明，环境质量、多样性和真实性是形成可泛化智能体能力的关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study is motivated by the need to develop AI agents that can generalize beyond their training distribution, particularly for complex enterprise tasks. The method involves training agents on CoreCraft, a high-fidelity reinforcement learning environment that simulates a customer support organization with thousands of entities and tools, using Group Relative Policy Optimization (GRPO) with adaptive clipping on the GLM 4.6 model. Experimental results show that after one training epoch, the model&#x27;s task pass rate improved from 25.37% to 36.76% on held-out tasks, with significant transfer gains of up to 7.4% on out-of-distribution benchmarks, indicating that environment quality and realism are crucial for fostering generalizable agent capabilities.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发能够超越训练分布进行泛化的AI智能体，以应对复杂的企业任务。方法上，通过在CoreCraft这一高保真强化学习环境中训练智能体，该环境模拟了包含数千个实体和工具的客户支持组织，并对GLM 4.6模型采用了带自适应裁剪的组相对策略优化（GRPO）。实验结果表明，经过一轮训练后，模型在保留任务上的通过率从25.37%提升至36.76%，并在分布外基准测试中实现了高达7.4%的迁移增益，这表明环境的质量、多样性和真实性是培养智能体泛化能力的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Improving Skill Learning for Robust Skill-based Meta-Reinforcement Learning</div>
<div class="meta-line">Authors: Sanghyeon Lee, Sangjun Bae, Yisak Park, Seungyul Han</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-02-06T03:28:45+00:00 · Latest: 2026-02-19T05:01:11+00:00</div>
<div class="meta-line">Comments: 10 pages main, 27 pages appendix with reference. Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03752v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.03752v4">PDF</a> · <a href="https://github.com/epsilog/SISL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, leading to unstable skill learning and degraded performance. To address this, we propose Self-Improving Skill Learning (SISL), which performs self-guided skill refinement using decoupled high-level and skill improvement policies, while applying skill prioritization via maximum return relabeling to focus updates on task-relevant trajectories, resulting in robust and stable adaptation even under noisy and suboptimal data. By mitigating the effect of noise, SISL achieves reliable skill learning and consistently outperforms other skill-based meta-RL methods on diverse long-horizon tasks. Our code is available at https://github.com/epsilog/SISL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向鲁棒技能元强化学习的自改进技能学习方法</div>
<div class="mono" style="margin-top:8px">元强化学习（Meta-RL）虽能快速适应未知任务，但在长时程环境中面临挑战。基于技能的方法通过将状态-动作序列分解为可复用技能并采用分层决策来解决此问题，但这些方法极易受噪声离线演示影响，导致技能学习不稳定与性能下降。为此，我们提出自改进技能学习（SISL），该方法通过解耦的高层策略与技能改进策略进行自引导式技能优化，同时利用基于最大回报重标注的技能优先级机制，将更新聚焦于任务相关轨迹，从而即使在噪声和次优数据下也能实现鲁棒稳定的适应。通过抑制噪声影响，SISL实现了可靠的技能学习，并在多种长时程任务中持续优于其他基于技能的元强化学习方法。代码发布于 https://github.com/epsilog/SISL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge that skill-based meta-reinforcement learning is highly sensitive to noisy offline demonstrations, which destabilizes skill acquisition and hampers adaptation in long-horizon tasks, this paper introduces Self-Improving Skill Learning (SISL). The method employs a decoupled policy framework for self-guided skill refinement and uses maximum return relabeling to prioritize updates on task-relevant trajectories, thereby enhancing robustness. Experimental results demonstrate that SISL effectively mitigates noise impact, achieves reliable skill learning, and consistently outperforms existing skill-based meta-RL methods across various long-horizon environments.</div>
<div class="mono" style="margin-top:8px">本文的动机是基于技能的元强化学习方法对噪声离线演示高度敏感，导致技能学习不稳定，在长时程任务中适应性能下降。为此，论文提出了自改进技能学习（SISL）方法，该方法采用解耦的高层策略和技能改进策略进行自引导技能优化，并通过最大回报重标注实现技能优先级排序，以聚焦于任务相关轨迹的更新，从而提升鲁棒性。主要实验结果表明，SISL能有效减轻噪声影响，实现可靠的技能学习，并在多种长时程任务上持续优于其他基于技能的元强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Xuefeng Wang, Lei Zhang, Henglin Pu, Ahmed H. Qureshi, Husheng Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-11T04:12:50+00:00 · Latest: 2026-02-19T04:56:13+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026. 21 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09135v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.09135v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习的连续时间价值迭代方法</div>
<div class="mono" style="margin-top:8px">现有强化学习方法在处理需要高频或非规则时间交互的复杂动态系统时面临困难。连续时间强化学习通过用汉密尔顿-雅可比-贝尔曼方程粘性解定义的微分价值函数替代离散时间贝尔曼递归，成为具有前景的替代方案。然而该方法目前主要局限于单智能体领域，原因在于：(i)传统HJB方程求解方法受维度诅咒限制，难以处理高维系统；(ii)即使采用基于HJB的学习方法，在多智能体设置中精确逼近集中式价值函数仍具挑战性，进而导致策略训练不稳定。本文提出CT-MARL框架，采用物理信息神经网络大规模逼近基于HJB的价值函数。为确保价值函数与其微分结构的一致性，我们通过引入价值梯度迭代模块实现价值学习与价值梯度学习的对齐，该模块沿轨迹迭代优化价值梯度。这提升了梯度保真度，进而产生更精确的价值估计和更强的策略学习能力。我们在连续时间化标准基准测试（包括多智能体粒子环境和多智能体MuJoCo）上评估本方法，结果表明我们的方法持续优于现有连续时间强化学习基线，并能扩展到复杂多智能体动态系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing reinforcement learning methods in handling complex dynamical systems requiring high-frequency or irregular interactions, particularly in multi-agent settings where continuous-time RL faces challenges like the curse of dimensionality and instability in approximating centralized value functions. The proposed CT-MARL framework employs physics-informed neural networks to approximate Hamilton-Jacobi-Bellman-based value functions at scale and introduces a Value Gradient Iteration module to align value learning with value-gradient learning, thereby refining gradients along trajectories for improved accuracy and policy stability. Experimental results on continuous-time variants of multi-agent benchmarks, such as multi-agent particle environment and MuJoCo, show that this approach consistently outperforms existing continuous-time RL baselines and scales effectively to complex multi-agent dynamics.</div>
<div class="mono" style="margin-top:8px">本文针对现有强化学习方法在处理需要高频或不规则交互的复杂动态系统时的局限性，特别是在多智能体设置中，连续时间强化学习面临维度诅咒和集中式价值函数近似不稳定等挑战。提出的CT-MARL框架采用物理信息神经网络来大规模近似基于Hamilton-Jacobi-Bellman方程的价值函数，并引入价值梯度迭代模块，将价值学习与价值梯度学习对齐，从而沿轨迹细化梯度以提高准确性和策略稳定性。在多智能体粒子环境和MuJoCo等连续时间基准变体上的实验结果表明，该方法一致优于现有连续时间强化学习基线，并能有效扩展到复杂的多智能体动态中。</div>
</details>
</div>
<div class="card">
<div class="title">Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control</div>
<div class="meta-line">Authors: Xiaocai Zhang, Neema Nassir, Milad Haghani</div>
<div class="meta-line">First: 2026-02-19T04:18:50+00:00 · Latest: 2026-02-19T04:18:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17068v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17068v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures spatio-temporal dependencies through a novel dual-stage hypergraph attention mechanism that models interactions across both spatial and temporal hyperedges. In addition, a hybrid discrete action space is introduced to jointly determine the next signal phase configuration and its corresponding green duration, enabling more adaptive signal timing decisions. Experiments conducted on a corridor network under five traffic scenarios demonstrate that STDSH-MARL consistently improves multimodal performance and provides clear benefits for public transportation priority. Compared with state-of-the-art baseline methods, the proposed approach achieves superior overall performance. Further ablation studies confirm the contribution of each component of STDSH-MARL, with temporal hyperedges identified as the most influential factor driving the observed performance gains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向人本化多模式走廊交通信号控制的时空双阶段超图多智能体强化学习方法</div>
<div class="mono" style="margin-top:8px">走廊网络中的人本化交通信号控制需日益关注多模式出行者（尤其是高载客量的公共交通），而非仅聚焦车辆导向的通行效率。本文提出STDSH-MARL（基于时空双阶段超图的多智能体强化学习框架），该可扩展的深度强化学习框架遵循集中训练与分散执行的范式。该方法通过创新的双阶段超图注意力机制捕获时空依赖关系，该机制可同时建模空间超边与时间超边的交互作用。此外，引入混合离散动作空间以联合确定下一信号相位配置及其对应绿灯时长，从而实现更具适应性的信号配时决策。在五种交通场景下对走廊网络的实验表明，STDSH-MARL能持续提升多模式交通性能，并为公共交通优先提供显著效益。与前沿基线方法相比，本方法实现了更优的综合性能。进一步的消融研究证实了STDSH-MARL各组成部分的贡献，其中时间超边被识别为驱动性能提升的最关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to shift from vehicle-centric to human-centric traffic signal control that prioritizes multimodal travelers, especially public transportation, this paper introduces STDSH-MARL, a multi-agent reinforcement learning framework. The method employs a centralized training and decentralized execution paradigm, capturing spatio-temporal dependencies via a dual-stage hypergraph attention mechanism and using a hybrid discrete action space to jointly determine signal phases and green durations. Experimental results on a corridor network across five traffic scenarios show that STDSH-MARL consistently enhances multimodal performance, particularly benefiting public transportation priority, and outperforms state-of-the-art baselines, with ablation studies confirming the key role of temporal hyperedges in these gains.</div>
<div class="mono" style="margin-top:8px">本文的动机是从以车辆为中心转向以人为中心的交通信号控制，优先考虑多模式出行者，特别是公共交通。该方法提出了STDSH-MARL，一个采用集中训练与分散执行范式的多智能体强化学习框架，通过双阶段超图注意力机制捕捉时空依赖，并利用混合离散动作空间联合决定信号相位和绿灯时长。在走廊网络五种交通场景下的实验结果表明，STDSH-MARL持续提升了多模式性能，明显有利于公共交通优先，且优于现有先进基线方法，消融研究进一步证实了时间超边是驱动性能提升的最关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Yonghyeon Jo, Sunwoo Lee, Seungyul Han</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-19T04:07:55+00:00 · Latest: 2026-02-19T04:07:55+00:00</div>
<div class="meta-line">Comments: 10 technical page followed by references and appendix. Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17062v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17062v1">PDF</a> · <a href="https://github.com/hyeon1996/S2Q">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中保留次优动作以追踪动态最优解</div>
<div class="mono" style="margin-top:8px">值分解是协作式多智能体强化学习的核心方法，但现有方法仍依赖单一最优动作，在训练中基础值函数发生偏移时难以适应，常收敛至次优策略。为突破此局限，我们提出连续次值Q学习，通过学得多个次值函数以保留替代性高价值动作。将这些次值函数整合至基于Softmax的行为策略中，该方法能促进持续探索，并使总Q值快速适应动态最优解。在复杂多智能体强化学习基准测试中的实验表明，该方法持续优于多种现有算法，展现出更强的适应性与综合性能。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation in cooperative multi-agent reinforcement learning where existing value decomposition methods rely on a single optimal action and struggle to adapt to shifts in the underlying value function during training, often converging to suboptimal policies. The proposed method, Successive Sub-value Q-learning (S2Q), learns multiple sub-value functions to retain alternative high-value actions and incorporates them into a Softmax-based behavior policy to encourage persistent exploration and enable rapid adjustment to changing optima. Experimental results on challenging MARL benchmarks confirm that S2Q consistently outperforms various baseline MARL algorithms, demonstrating improved adaptability and overall performance.</div>
<div class="mono" style="margin-top:8px">本文针对协作多智能体强化学习中现有价值分解方法依赖单一最优动作、难以适应训练期间底层价值函数变化并常收敛于次优策略的局限性。提出的方法称为连续次优价值Q学习（S2Q），它通过学习多个次价值函数来保留替代的高价值动作，并将其整合到一个基于Softmax的行为策略中，以鼓励持续探索并实现快速适应变化的最优解。在具有挑战性的多智能体强化学习基准测试上的实验结果表明，S2Q一致优于多种基线算法，展现出更强的适应性和整体性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Phase-Aware Mixture of Experts for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Shengtian Yang, Yu Li, Shuo He, Yewen Li, Qingpeng Cai, Peng Jiang, Lei Feng</div>
<div class="meta-line">First: 2026-02-19T03:18:30+00:00 · Latest: 2026-02-19T03:18:30+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向智能体强化学习的相位感知专家混合模型</div>
<div class="mono" style="margin-top:8px">强化学习（RL）为LLM智能体赋予了解决复杂任务的强大能力。然而，现有RL方法通常使用单一策略网络，导致简单任务占据大部分参数并主导梯度更新的“简单性偏差”，使复杂任务缺乏足够建模容量。可行的改进方案是在策略网络中采用专家混合（MoE）架构，因为MoE允许不同参数（专家）专精于不同任务，防止简单任务垄断所有参数。但传统MoE的关键局限在于其基于令牌的路由机制——路由器将每个令牌分配给特定专家，这会将相位一致的模式碎片化为分散的专家分配，从而削弱专家专精性。本文提出相位感知专家混合模型（PA-MoE），其核心是轻量级相位路由器，可直接从RL目标学习潜在相位边界而无需预定义相位类别。该路由器为时序一致的任务分配相同专家，使专家能保持相位特定专长。实验结果验证了PA-MoE的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the simplicity bias in reinforcement learning (RL) for LLM agents, where a single policy network leads to simple tasks dominating parameter updates and hindering performance on complex tasks. To mitigate this, the authors propose Phase-Aware Mixture of Experts (PA-MoE), which replaces token-level routing with a lightweight phase router that learns latent phase boundaries from the RL objective, enabling temporally consistent expert assignments to preserve phase-specific expertise. Experimental results validate the effectiveness of PA-MoE in improving agent performance by allowing experts to specialize in different task phases without fragmentation.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中LLM智能体因使用单一策略网络而导致的简单任务主导参数更新的“简单性偏差”问题，提出了一种相位感知专家混合模型（PA-MoE）。该方法通过轻量级相位路由器从强化学习目标中学习潜在相位边界，替代传统的令牌级路由，从而为同一专家分配时间上一致的输入，以保持相位特定专业知识。实验结果表明，PA-MoE能有效提升智能体性能，使专家能够在不同任务阶段实现专业化，避免模式碎片化。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning to Discover a North-East Monsoon Index for Rainfall Prediction in Thailand</div>
<div class="meta-line">Authors: Kiattikun Chobtham</div>
<div class="meta-line">First: 2026-01-15T08:40:01+00:00 · Latest: 2026-02-19T03:18:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10181v5">Abs</a> · <a href="https://arxiv.org/pdf/2601.10181v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately predicting long-term rainfall is challenging. Global climate indices, such as the El Niño-Southern Oscillation, are standard input features for machine learning. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel North-East monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习发现东北季风指数以预测泰国降雨</div>
<div class="mono" style="margin-top:8px">准确预测长期降雨具有挑战性。全球气候指数（如厄尔尼诺-南方涛动）是机器学习的标准输入特征，但针对泰国特定区域、能提升预测精度的局地尺度指数仍存在显著空白。本文提出一种基于海表温度计算的新型东北季风气候指数，以反映北半球冬季季风的气候特征。为优化该指数的计算区域，采用深度Q网络强化学习智能体探索并选择与季节性降雨相关性最高的矩形区域。降雨站点被划分为12个独立聚类，以区分泰国南部与北部地区的降雨模式。实验结果表明，将优化后的指数融入长短期记忆模型，能显著提升多数聚类区域的长期月降雨预测能力，有效降低12个月前瞻预测的均方根误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of accurate long-term rainfall prediction and the lack of effective local-scale climate indices for Thailand, this paper proposes a novel North-East monsoon index derived from sea surface temperature. The method employs a Deep Q-Network reinforcement learning agent to optimize the geographic areas used for calculating this index by selecting sea surface temperature rectangles that best correlate with seasonal rainfall, which is analyzed across 12 distinct regional clusters. The main experimental results demonstrate that integrating this optimized index into Long Short-Term Memory models significantly enhances the skill of 12-month-ahead monthly rainfall predictions for most clusters, effectively reducing the Root Mean Square Error.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决长期降雨预测的准确性挑战，并弥补泰国地区缺乏有效本地气候指标的不足，提出了一种基于海表温度计算的新型东北季风指数。方法上，采用深度Q网络强化学习智能体来优化用于计算该指数的地理区域，通过选择与季节性降雨相关性最佳的海温矩形区域进行分析，并将降雨站点划分为12个不同的区域集群以区分降雨模式。主要实验结果表明，将这一优化后的指数融入长短期记忆模型，能显著提升对大多数集群区域未来12个月的月度降雨预测能力，有效降低了均方根误差。</div>
</details>
</div>
<div class="card">
<div class="title">Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Nikunj Gupta, James Zachary Hare, Jesse Milzman, Rajgopal Kannan, Viktor Prasanna</div>
<div class="meta-line">First: 2026-02-19T02:13:29+00:00 · Latest: 2026-02-19T02:13:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17009v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17009v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents&#x27; available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动作图策略：多智能体强化学习中动作协同依赖关系的学习</div>
<div class="mono" style="margin-top:8px">在多智能体强化学习（MARL）中，动作协调是最基础的合作形式。成功的分散式决策不仅依赖于良好的个体动作，更需要跨智能体选择兼容动作以实现行为同步、避免冲突并满足全局约束。本文提出动作图策略（AGP），该模型能刻画智能体可用动作选择间的依赖关系。它构建了被称为“协调情境”的框架，使智能体能够根据全局动作依赖关系调整决策。理论上，我们证明相较于完全独立策略，AGP能生成表达能力严格更强的联合策略，并能实现经证明比集中式价值分解方法的贪婪执行更优的协调联合动作。实证研究表明，在具有部分可观测性和反协调惩罚的典型协调任务中，AGP达到80-95%的成功率，而其他MARL方法仅达到10-25%。我们进一步证明，AGP在多样化多智能体环境中持续优于这些基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for effective coordination in multi-agent reinforcement learning, where successful decentralized decision-making depends not just on individual actions but on selecting compatible actions across agents to synchronize behavior and avoid conflicts. The method introduces Action Graph Policies (AGP), which model dependencies among agents&#x27; available action choices by constructing coordination contexts that allow agents to condition their decisions on global action dependencies. Experimentally, AGP achieves 80-95% success on canonical coordination tasks with partial observability and anti-coordination penalties, significantly outperforming other MARL methods that reach only 10-25%, and it consistently surpasses baselines in diverse multi-agent environments.</div>
<div class="mono" style="margin-top:8px">本文的动机源于多智能体强化学习中有效协调的需求，其中成功的分散决策不仅依赖于个体行动，还依赖于跨智能体选择兼容行动以同步行为并避免冲突。该方法提出了行动图策略（AGP），通过构建协调上下文来建模智能体可用行动选择之间的依赖关系，使智能体能基于全局行动依赖调整决策。实验结果表明，在具有部分可观测性和反协调惩罚的典型协调任务中，AGP实现了80-95%的成功率，显著优于其他仅达到10-25%的多智能体强化学习方法，并在多样化的多智能体环境中持续超越基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs</div>
<div class="meta-line">Authors: Zhiliang Chen, Alfred Wei Lun Leong, Shao Yong Ong, Apivich Hemachandra, Gregory Kang Ruey Lau, Chuan-Sheng Foo, Zhengyuan Liu, Nancy F. Chen, Bryan Kian Hsiang Low</div>
<div class="meta-line">First: 2026-02-09T07:33:40+00:00 · Latest: 2026-02-19T01:32:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08351v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08351v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS&#x27;s average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鸡与蛋的困境：大语言模型数据与模型配置的协同优化</div>
<div class="mono" style="margin-top:8px">为大语言模型训练协同优化数据与模型配置存在经典的鸡与蛋困境：下游任务的最佳训练数据配置（如数据混合比例）取决于所选模型配置（如模型架构），反之亦然。然而，联合优化二者常被视为难以处理，现有方法仅侧重数据或模型优化而忽略其相互作用。我们提出JoBS方法，利用基于缩放律的性能预测器辅助贝叶斯优化，高效联合优化大语言模型训练的数据与模型配置。JoBS分配部分优化预算学习性能预测器，该预测器可通过少量训练步骤评估训练配置的潜力；剩余预算则完全通过预测器执行贝叶斯优化，从而分摊完整训练成本。我们分析了JoBS的平均遗憾值并设计最优预算分配方案以最小化遗憾。在相同优化预算下，JoBS在多样化大语言模型任务中均优于现有多保真度贝叶斯优化基线及数据/模型优化方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the interdependent challenge of co-optimizing data and model configurations for LLMs, a chicken-and-egg problem where each depends on the other, making joint optimization traditionally intractable. The authors propose JoBS, a method that uses a scaling-law-inspired performance predictor to guide Bayesian optimization, efficiently allocating part of the budget to learn the predictor from limited training steps and the rest to optimize configurations via the predictor, thereby amortizing full-training costs. Experimental results show that JoBS reduces average regret, outperforming existing multi-fidelity Bayesian optimization and separate data or model optimization methods across various LLM tasks under fixed budgets.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型中数据与模型配置相互依赖、难以联合优化的“鸡与蛋”困境，提出了一种名为JoBS的方法。该方法利用基于缩放定律的性能预测器来引导贝叶斯优化，通过分配部分预算从少量训练步骤中学习预测器，其余预算则完全依靠预测器进行配置优化，从而分摊完整训练的成本。实验结果表明，在相同优化预算下，JoBS在多种大语言模型任务中均优于现有的多保真度贝叶斯优化基线以及单独优化数据或模型的方法，有效降低了平均遗憾值。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Framework for Locality in Scalable MARL</div>
<div class="meta-line">Authors: Sourav Chakraborty, Amit Kiran Rege, Claire Monteleoni, Lijun Chen</div>
<div class="meta-line">First: 2026-02-19T00:02:02+00:00 · Latest: 2026-02-19T00:02:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16966v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16966v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^π$, which decouples the environment&#x27;s sensitivity to state ($E^{\mathrm{s}}$) and action ($E^{\mathrm{a}}$) from the policy&#x27;s sensitivity to state ($Π(π)$). This decomposition reveals that locality can be induced by a smooth policy (small $Π(π)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) &lt; 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展多智能体强化学习中局部性的统一框架</div>
<div class="mono" style="margin-top:8px">可扩展多智能体强化学习（MARL）面临维度灾难的根本性挑战。利用局部性是常见解决方案，其关键在于价值函数具备指数衰减特性（EDP）。然而，现有保证EDP的条件通常较为保守，因为它们基于最坏情况下的环境边界（如动作上的上确界），未能捕捉策略本身的正则化效应。本文论证了局部性同样可以是一种策略依赖现象。我们的核心贡献是提出了一种新颖的策略诱导互依矩阵$H^π$分解方法，将环境对状态（$E^{\mathrm{s}}$）和动作（$E^{\mathrm{a}}$）的敏感度与策略对状态（$Π(π)$）的敏感度解耦。该分解表明：即使环境存在强动作耦合，平滑策略（较小的$Π(π)$）仍可诱导局部性，这揭示了局部性与最优性之间的根本权衡。基于此框架，我们推导出指数衰减的普适谱条件$ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) &lt; 1$，该条件严格优于先前的基于范数的条件。最后，我们运用该理论分析了一个可证明的局部块坐标策略改进框架，其保证直接与此谱半径相关联。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scalability in Multi-Agent Reinforcement Learning (MARL) by revisiting the concept of locality, which is often assumed to rely on an Exponential Decay Property (EDP) of the value function. The motivation stems from the observation that existing conditions for EDP are overly conservative, as they ignore the policy&#x27;s regularizing effect and rely solely on worst-case environment bounds. The method introduces a novel decomposition of the policy-induced interdependence matrix, separating environment sensitivity to state and action from policy sensitivity to state, thereby showing that locality can be policy-dependent. Experimental results demonstrate that this leads to a tighter spectral condition for exponential decay, which is used to develop a localized block-coordinate policy improvement framework with guarantees linked to the spectral radius, offering a more refined approach to scalable MARL.</div>
<div class="mono" style="margin-top:8px">本文针对可扩展多智能体强化学习（MARL）中的维度诅咒问题，重新探讨了局部性概念，该概念通常依赖于价值函数的指数衰减特性（EDP）。研究动机源于现有EDP条件过于保守，仅基于环境的最坏情况界限而忽略了策略本身的调节作用。方法上，提出了一种新的策略诱导相互依赖矩阵分解，将环境对状态和动作的敏感性与策略对状态的敏感性分离开来，从而表明局部性可以是策略依赖的。主要实验结果证明，这导出了一个更严格的指数衰减谱条件，并用于开发一个局部块坐标策略改进框架，其保证直接与谱半径相关，为可扩展MARL提供了更精细的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Automating Agent Hijacking via Structural Template Injection</div>
<div class="meta-line">Authors: Xinhao Deng, Jiaqing Wu, Miao Chen, Yue Xiao, Ke Xu, Qi Li</div>
<div class="meta-line">First: 2026-02-18T23:52:14+00:00 · Latest: 2026-02-18T23:52:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16958v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16958v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过结构化模板注入实现智能体劫持自动化</div>
<div class="mono" style="margin-top:8px">智能体劫持被OWASP列为大语言模型生态系统的关键威胁，攻击者可通过在检索内容中注入恶意指令来操控执行流程。现有攻击多依赖人工构建的语义驱动提示词操纵，成功率低且对闭源商业模型的迁移性有限。本文提出Phantom——基于结构化模板注入的自动化智能体劫持框架，其核心洞见在于：智能体依赖特定对话模板标记来区分系统、用户、助手及工具指令。通过向检索上下文注入优化的结构化模板，可诱发角色混淆，使智能体将注入内容误判为合法用户指令或先前的工具输出。为提升对黑盒智能体的攻击迁移性，Phantom设计了新型攻击模板搜索框架：先通过多级模板增强提升结构多样性，再训练模板自编码器将离散模板嵌入连续可搜索的隐空间，最后运用贝叶斯优化高效解码出高效对抗性结构化模板。在Qwen、GPT和Gemini上的实验表明，该框架在攻击成功率和查询效率上均显著优于现有基线。研究还发现70余个商业产品的实际漏洞（已获厂商确认），揭示了结构化模板劫持的现实危害性，为保障下一代智能体系统安全提供了实证基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of manual, semantics-based agent hijacking attacks which suffer from low success rates and poor transferability to closed-source models, this paper introduces Phantom, an automated framework leveraging Structural Template Injection to exploit the architectural reliance of LLM agents on specific chat template tokens. The method involves injecting optimized structured templates into retrieved context to induce role confusion, and employs a novel attack template search framework that uses multi-level template augmentation, a Template Autoencoder for continuous latent space embedding, and Bayesian optimization to efficiently find high-potency adversarial templates. Experimental results on models like Qwen, GPT, and Gemini show that Phantom significantly outperforms existing baselines in Attack Success Rate and query efficiency, and the discovery of over 70 vulnerabilities in real-world commercial products confirms the practical severity of this approach.</div>
<div class="mono" style="margin-top:8px">针对现有基于手动语义操作的智能体劫持攻击成功率低、对闭源模型可迁移性差的问题，本文提出了Phantom框架，通过结构化模板注入自动化地利用大语言模型智能体对特定对话模板令牌的架构依赖。该方法将优化的结构化模板注入检索上下文以引发角色混淆，并采用一种新颖的攻击模板搜索框架，通过多级模板增强、模板自编码器构建连续潜在空间嵌入，以及贝叶斯优化来高效寻找高效对抗性模板。在Qwen、GPT和Gemini等模型上的实验结果表明，Phantom在攻击成功率和查询效率上显著优于现有基线，且在现实商业产品中发现的70多个已获厂商确认的漏洞，证实了这种基于结构化模板劫持的实际严重性。</div>
</details>
</div>
<div class="card">
<div class="title">LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation</div>
<div class="meta-line">Authors: Hejia Zhang, Zhongming Yu, Chia-Tung Ho, Haoxing Ren, Brucek Khailany, Jishen Zhao</div>
<div class="meta-line">First: 2026-02-18T23:36:46+00:00 · Latest: 2026-02-18T23:36:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16953v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM4Cov：面向高覆盖率测试平台生成的执行感知智能体学习框架</div>
<div class="mono" style="margin-top:8px">执行感知大语言模型智能体为从工具反馈中学习提供了前景广阔的范式，但此类反馈通常获取成本高昂且速度缓慢，使得在线强化学习难以实际应用。高覆盖率硬件验证正是这一挑战的典型体现，因其依赖工业级仿真器且执行信号不可微分。我们提出LLM4Cov——一种离线智能体学习框架，将验证过程建模为由确定性评估器引导的无记忆状态转移。基于此形式化框架，我们引入执行验证数据筛选、策略感知智能体数据合成及最劣状态优先采样机制，以在执行约束下实现可扩展学习。我们进一步通过修订的评估协议，从现有验证套件中构建了贴合实际场景的基准测试集。采用该流程后，一个紧凑的40亿参数模型在智能体评估中达到69.2%的覆盖率通过率，较其教师模型提升5.3%，并与规模大一个数量级的模型展现出相当的性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of using execution feedback from expensive, slow tools like hardware simulators for online reinforcement learning by proposing LLM4Cov, an offline agent-learning framework for high-coverage testbench generation in hardware verification. The method models verification as memoryless state transitions guided by deterministic evaluators, incorporating execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. Experimental results show that a compact 4B-parameter model trained with this pipeline achieves a 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and competing with models an order of magnitude larger.</div>
<div class="mono" style="margin-top:8px">本文针对硬件验证中依赖昂贵、缓慢的工业模拟器执行反馈导致在线强化学习不切实际的问题，提出了LLM4Cov离线智能体学习框架，用于生成高覆盖率的测试平台。该方法将验证建模为由确定性评估器引导的无记忆状态转换，通过执行验证的数据整理、策略感知的智能体数据合成和最差状态优先采样，实现在执行约束下的可扩展学习。实验结果表明，采用该流程训练的紧凑4B参数模型在智能体评估下达到了69.2%的覆盖率通过率，比其教师模型高出5.3%，并与规模大一个数量级的模型性能相当。</div>
</details>
</div>
<div class="card">
<div class="title">Discovering Multiagent Learning Algorithms with Large Language Models</div>
<div class="meta-line">Authors: Zun Li, John Schultz, Daniel Hennes, Marc Lanctot</div>
<div class="meta-line">First: 2026-02-18T22:41:00+00:00 · Latest: 2026-02-18T22:41:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16928v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16928v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型发现多智能体学习算法</div>
<div class="mono" style="margin-top:8px">不完全信息博弈中的多智能体强化学习（MARL）进展长期依赖对基线模型的人工迭代优化。尽管反事实遗憾最小化（CFR）和策略空间响应预言（PSRO）等基础理论框架具有坚实的理论基础，但其最有效变体的设计往往需要依靠人类直觉在庞大的算法设计空间中探索。本研究提出使用由大语言模型驱动的进化编码智能体AlphaEvolve，以自动发现新的多智能体学习算法。我们通过为两种不同的博弈论学习范式演化新变体，证明了该框架的通用性。首先，在迭代遗憾最小化领域，我们演化出控制遗憾累积与策略推导的逻辑，发现了新算法——波动自适应折现（VAD-）CFR。该算法采用包括波动敏感折现、一致性强制乐观策略及硬性热启动策略累积机制在内的非直观创新机制，在性能上超越了折现预测CFR+等先进基线模型。其次，在基于种群的训练算法领域，我们为PSRO演化出训练时与评估时的元策略求解器，发现了新变体——平滑混合乐观遗憾（SHOR-）PSRO。该变体引入了一种混合元求解器，将乐观遗憾匹配与基于温度控制的平滑纯策略最优分布进行线性融合。通过在训练中动态退火混合因子及多样性奖励，该算法实现了从种群多样性到严格均衡寻找的自动过渡，相比标准静态元求解器展现出更优的经验收敛性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the reliance on manual refinement in multi-agent reinforcement learning (MARL) for imperfect-information games, this work introduces AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover novel learning algorithms. The method applies this framework to two paradigms: for iterative regret minimization, it evolves regret accumulation and policy logic to produce Volatility-Adaptive Discounted CFR (VAD-CFR), which incorporates mechanisms like volatility-sensitive discounting and consistency-enforced optimism; for population-based training, it evolves meta-solvers for Policy Space Response Oracles to create Smoothed Hybrid Optimistic Regret PSRO (SHOR-PSRO), featuring a hybrid meta-solver with dynamic annealing. Experimental results show that VAD-CFR outperforms state-of-the-art baselines like Discounted Predictive CFR+, while SHOR-PSRO achieves superior empirical convergence compared to standard static meta-solvers.</div>
<div class="mono" style="margin-top:8px">针对不完全信息博弈中多智能体强化学习依赖人工迭代改进的问题，本研究提出AlphaEvolve，一种基于大语言模型的进化编码智能体，以自动发现新的学习算法。该方法将该框架应用于两个范式：在迭代后悔最小化中，进化后悔累积和策略逻辑，产生了波动自适应折扣CFR（VAD-CFR），其包含波动敏感折扣和一致性强制乐观等机制；在基于种群的训练中，进化策略空间响应预言机的元求解器，创建了平滑混合乐观后悔PSRO（SHOR-PSRO），其采用具有动态退火的混合元求解器。实验结果表明，VAD-CFR在性能上超越了如折扣预测CFR+等先进基线，而SHOR-PSRO相比标准静态元求解器实现了更优的经验收敛性。</div>
</details>
</div>
<div class="card">
<div class="title">Online Robust Reinforcement Learning with General Function Approximation</div>
<div class="meta-line">Authors: Debamita Ghosh, George K. Atia, Yue Wang</div>
<div class="meta-line">Venue: ICML 2026</div>
<div class="meta-line">First: 2025-12-22T02:12:04+00:00 · Latest: 2026-02-18T21:18:07+00:00</div>
<div class="meta-line">Comments: This version corresponds to the ICML 2026 submission. Major updates include: extension from TV to general phi-divergence uncertainty sets such as TV, chi-square, and KL; introduction of the robust Bellman-Eluder dimension; refined regret and sample complexity bounds; improved proofs; and new empirical evaluations</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18957v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.18957v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world settings, reinforcement learning systems suffer performance degradation when the environment encountered at deployment differs from that observed during training. Distributionally robust reinforcement learning (DR-RL) mitigates this issue by seeking policies that maximize performance under the most adverse transition dynamics within a prescribed uncertainty set. Most existing DR-RL approaches, however, rely on strong data availability assumptions, such as access to a generative model or large offline datasets, and are largely restricted to tabular settings.
  In this work, we propose a fully online DR-RL algorithm with general function approximation that learns robust policies solely through interaction, without requiring prior knowledge or pre-collected data. Our approach is based on a dual-driven fitted robust Bellman procedure that simultaneously estimates the value function and the corresponding worst-case backup operator. We establish regret guarantees for online DR-RL characterized by an intrinsic complexity notion, the robust Bellman-Eluder dimension, covering a broad class of phi-divergence uncertainty sets. The resulting regret bounds are sublinear, do not scale with the size of the state or action spaces, and specialize to tight rates in structured problem classes, demonstrating the practicality and scalability of our framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于通用函数逼近的在线鲁棒强化学习</div>
<div class="mono" style="margin-top:8px">在现实场景中，当部署环境与训练环境存在差异时，强化学习系统常出现性能下降。分布鲁棒强化学习通过寻找在预设不确定性集合内最差转移动态下仍能最大化性能的策略来缓解此问题。然而现有方法大多依赖强数据假设（如生成模型或大规模离线数据集），且主要局限于表格化设置。本研究提出一种完全在线的通用函数逼近DR-RL算法，仅通过交互学习鲁棒策略，无需先验知识或预收集数据。该方法基于对偶驱动的拟合鲁棒贝尔曼过程，同步估计值函数及对应最差情况回溯算子。我们建立了以鲁棒贝尔曼-埃尔uder维度这一内在复杂性概念表征的在线DR-RL遗憾保证，覆盖广泛的phi散度不确定性集合。所得遗憾界具有次线性特征，不随状态/动作空间规模增长，在结构化问题类中可收敛至紧致速率，证明了框架的实用性与可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance degradation of reinforcement learning agents when deployed in environments that differ from their training conditions, this paper introduces an online distributionally robust reinforcement learning algorithm that operates with general function approximation, eliminating the need for strong data assumptions like generative models or large offline datasets. The method employs a dual-driven fitted robust Bellman procedure to simultaneously estimate the value function and the worst-case backup operator through direct interaction. Experimental results, supported by theoretical analysis, demonstrate sublinear regret guarantees characterized by the robust Bellman-Eluder dimension, which does not scale with state or action space sizes and achieves tight rates in structured problem classes, confirming the framework&#x27;s practicality and scalability.</div>
<div class="mono" style="margin-top:8px">针对强化学习智能体在部署环境与训练环境不同时性能下降的问题，本文提出了一种基于通用函数逼近的在线分布鲁棒强化学习算法，无需依赖生成模型或大规模离线数据等强数据假设。该方法采用双驱动拟合鲁棒贝尔曼过程，通过直接交互同时估计值函数和最坏情况备份算子。理论分析与实验结果表明，该算法在由鲁棒贝尔曼-埃尔德维度刻画的广泛φ-散度不确定性集上实现了次线性遗憾保证，其遗憾界限不随状态或动作空间大小增长，并在结构化问题类中达到紧致收敛速率，证明了该框架的实用性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts</div>
<div class="meta-line">Authors: Mert Cemri, Nived Rajaraman, Rishabh Tiwari, Xiaoxuan Liu, Kurt Keutzer, Ion Stoica, Kannan Ramchandran, Ahmad Beirami, Ziteng Sun</div>
<div class="meta-line">First: 2025-06-15T05:50:05+00:00 · Latest: 2026-02-18T21:13:11+00:00</div>
<div class="meta-line">Comments: 28 pages, 6 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.15733v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.15733v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPECS：通过推测性草稿实现更快的测试时扩展</div>
<div class="mono" style="margin-top:8px">测试时计算扩展推动了大型语言模型（LLMs）推理能力的近期进展，通常通过分配额外计算资源进行更深入探索。然而，增加计算往往以更高的用户端延迟为代价，直接影响用户体验。当前测试时扩展方法主要基于总计算资源（FLOPS）优化准确率，常忽略延迟约束。为填补这一空白，我们提出SPECS——一种受推测解码启发的延迟感知测试时扩展方法。SPECS使用更小、更快的模型高效生成候选序列，并利用大型目标模型和专用奖励模型的双重信号评估这些候选。我们引入了新的集成策略，包括奖励引导的软验证和基于奖励的延迟机制。在MATH500、AMC23和OlympiadBench数据集上的实验结果表明，SPECS在匹配或超越束搜索准确率的同时，将延迟降低高达约19.1%。理论分析表明，随着束宽增加，我们的算法会收敛到KL正则化强化学习目标的解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the trade-off between increased test-time compute for better reasoning and the resulting higher latency that harms user experience, this paper proposes SPECS, a latency-aware test-time scaling method inspired by speculative decoding. The method employs a smaller, faster model to efficiently generate candidate sequences, which are then evaluated using signals from both a larger target model and a dedicated reward model, incorporating novel strategies like reward-guided soft verification and a reward-based deferral mechanism. Experimental results on MATH500, AMC23, and OlympiadBench datasets demonstrate that SPECS achieves accuracy comparable to or better than beam search while reducing latency by up to approximately 19.1%, with theoretical analysis linking the algorithm to a KL-regularized reinforcement learning objective.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决测试时计算增加以提升推理能力与由此导致的高延迟损害用户体验之间的矛盾，提出了SPECS这一受推测解码启发的延迟感知测试时扩展方法。该方法使用更小、更快的模型高效生成候选序列，然后通过更大目标模型和专用奖励模型的信号进行评估，并引入了奖励引导的软验证和基于奖励的延迟机制等新策略。在MATH500、AMC23和OlympiadBench数据集上的实验结果表明，SPECS在达到或超越束搜索精度的同时，将延迟降低了高达约19.1%，理论分析表明该算法随束宽增加收敛于一个KL正则化的强化学习目标解。</div>
</details>
</div>
<div class="card">
<div class="title">SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation</div>
<div class="meta-line">Authors: Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-18T20:42:39+00:00 · Latest: 2026-02-18T20:42:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16863v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimToolReal：一种面向零样本灵巧工具操作的物体中心策略</div>
<div class="mono" style="margin-top:8px">操作工具的能力显著扩展了机器人可执行的任务范围。然而，工具操作代表了一类具有挑战性的灵巧性任务，需要抓握细薄物体、进行手内物体旋转以及强力的交互。由于收集这些行为的遥操作数据较为困难，仿真到现实的强化学习成为一种有前景的替代方案。但以往方法通常需要大量工程努力来建模物体并为每个任务调整奖励函数。本文提出SimToolReal，旨在推动仿真到现实强化学习策略在工具操作任务中的泛化。该方法不再局限于单一物体和任务，而是在仿真中程序化生成大量多样化的类工具物体基元，并训练一个统一的强化学习策略，其通用目标是将每个物体操控至随机目标姿态。这一方法使SimToolReal在测试时无需任何物体或任务特定训练即可执行通用的灵巧工具操作。实验表明，SimToolReal在性能上超越先前的重定向和固定抓取方法37%，同时达到针对特定目标物体和任务训练的专用强化学习策略的水平。最后，我们证明SimToolReal能够泛化至多种日常工具，在涵盖24个任务、12个物体实例和6个工具类别的120次真实世界测试中均展现出强大的零样本性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the difficulty of collecting real-world teleoperation data for dexterous tool manipulation, this work introduces SimToolReal, a sim-to-real reinforcement learning approach that aims for generalization. The method involves procedurally generating a diverse set of tool-like object primitives in simulation and training a single policy to manipulate these objects to random goal poses, avoiding task-specific engineering. Experimental results show that this policy outperforms prior retargeting and fixed-grasp methods by 37%, matches the performance of specialist policies trained on specific objects, and demonstrates strong zero-shot generalization across 24 real-world tasks, 12 object instances, and 6 tool categories.</div>
<div class="mono" style="margin-top:8px">本研究针对灵巧工具操作中真实遥操作数据收集困难的问题，提出了SimToolReal这一旨在实现泛化的仿真到现实强化学习方法。该方法通过在仿真中程序化生成多样化的工具状物体基元，并训练一个单一策略来将这些物体操控至随机目标姿态，从而避免了针对特定任务的工程调优。实验结果表明，该策略的性能优于先前的重定向和固定抓取方法37%，与在特定目标物体和任务上训练的专家策略表现相当，并在24个真实世界任务、12个物体实例和6个工具类别上展现了强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Training Large Reasoning Models Efficiently via Progressive Thought Encoding</div>
<div class="meta-line">Authors: Zeliang Zhang, Xiaodong Liu, Hao Cheng, Hao Sun, Chenliang Xu, Jianfeng Gao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-18T20:03:38+00:00 · Latest: 2026-02-18T20:03:38+00:00</div>
<div class="meta-line">Comments: ICLR 2026, 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过渐进式思维编码高效训练大型推理模型</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）在复杂问题上表现出色，但其效率面临关键瓶颈：基于结果的强化学习训练需要长序列展开，其中自回归解码占据主要时间和内存开销。滑动窗口缓存策略虽能限制内存，但会破坏长上下文推理并降低性能。我们提出渐进式思维编码，一种参数高效的微调方法，使LRMs能在固定大小缓存下有效推理。该方法通过将中间推理过程逐步编码为固定大小的向量表示，无需在完整缓存序列上进行反向传播，从而降低内存使用，并在推理期间保持恒定内存。在三个模型（包括Qwen2.5-3B-Instruct、Qwen2.5-7B-Instruct和DeepSeek-R1-Distill-Llama-8B）和六个广泛使用的数学基准测试上的实验表明：本方法相比基于LoRA的微调平均提升19.3%，相比未微调的LRMs平均提升29.9%，在相同严格缓存限制下AIME2024/2025准确率最高提升23.4%。这些结果证明渐进式思维编码不仅能提升推理精度，还能在实际内存限制下显著提高LRMs强化学习训练的效率和可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the inefficiency of training large reasoning models (LRMs) using reinforcement learning, where long autoregressive rollouts for outcome-based rewards consume excessive time and memory, and existing sliding-window cache strategies degrade long-context reasoning performance. The method introduced, Progressive Thought Encoding, is a parameter-efficient fine-tuning technique that progressively encodes intermediate reasoning steps into fixed-size vector representations, eliminating the need for backpropagation through full-cache rollouts and maintaining constant memory during inference. Experimental results on three models across six challenging mathematical benchmarks show that this approach consistently outperforms baselines, achieving an average improvement of +19.3% over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning, with up to +23.4 accuracy gain on AIME2024/2025 under tight cache budgets, demonstrating enhanced reasoning accuracy and training efficiency under memory constraints.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型推理模型（LRMs）在强化学习训练中的效率问题，即基于结果奖励的长序列自回归解码消耗大量时间和内存，而现有的滑动窗口缓存策略会破坏长上下文推理并降低性能。提出的方法称为渐进思维编码，这是一种参数高效的微调技术，通过将中间推理步骤逐步编码为固定大小的向量表示，避免了在完整缓存序列上进行反向传播，并在推理时保持恒定内存。在三个模型和六个具有挑战性的数学基准测试上的实验结果表明，该方法一致优于基线，平均比基于LoRA的微调提升19.3%，比未微调的LRMs提升29.9%，在严格缓存限制下于AIME2024/2025上最高获得23.4的准确率提升，证明了其在内存约束下提高了推理准确性和训练效率。</div>
</details>
</div>
<div class="card">
<div class="title">VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study</div>
<div class="meta-line">Authors: Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang</div>
<div class="meta-line">First: 2026-02-18T19:56:43+00:00 · Latest: 2026-02-18T19:56:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16833v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16833v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VAM：强化学习后训练中的言语化动作掩码可控探索——以国际象棋为例</div>
<div class="mono" style="margin-top:8px">探索仍是大型语言模型（LLM）强化学习（RL）后训练的关键瓶颈，稀疏反馈与庞大动作空间易导致模型过早陷入重复行为。本文提出言语化动作掩码（VAM），通过在提示中言语化动作掩码，强制模型从掩码集合输出动作。基于此接口，我们引入迭代动作空间剪枝：若未采样到目标动作，则从掩码中移除已采样的有效动作，在缩减候选集中重新采样，直至采样到目标或耗尽固定预算。我们在国际象棋中验证VAM，并在两种训练模式下评估：一是通过与引擎对弈生成状态的引擎对局模式，二是基于带验证器评分的固定棋局数据集训练模式。在保留的象棋谜题和以平均百分兵损失（ACPL）衡量的完整对局中，VAM较基线方法显著提升了学习效率与最终性能，证明言语化掩码是LLM RL后训练中实现可控探索的有效机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of exploration in reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces often cause models to collapse into repetitive behaviors. The authors propose Verbalized Action Masking (VAM), a method that verbalizes an action mask in the prompt to restrict the model&#x27;s output to a masked set of actions, and they enhance it with iterative action-space pruning to refine the candidate actions if the target is not initially sampled. Experimental results in chess, using both engine-play and fixed-dataset training regimes, show that VAM improves learning efficiency and final performance on held-out puzzles and full-game play, as measured by average centipawn loss, demonstrating its effectiveness for controllable exploration in LLM RL post-training.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLM）强化学习后训练中的探索难题，即稀疏反馈和大动作空间常导致模型陷入重复行为的问题，提出了语言化动作掩码（VAM）方法。该方法通过在提示中语言化动作掩码，将模型输出限制在掩码动作集内，并结合迭代动作空间剪枝策略，在目标动作未被采样时逐步缩减候选集以重新采样。在国际象棋案例中，通过引擎对弈和固定数据集两种训练机制进行实验，结果表明VAM在保留谜题测试和完整对弈中，以平均厘兵损失衡量，提升了学习效率和最终性能，凸显了语言化掩码在LLM强化学习后训练中可控探索的实用性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260221_0347.html">20260221_0347</a>
<a href="archive/20260220_0349.html">20260220_0349</a>
<a href="archive/20260219_0406.html">20260219_0406</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
