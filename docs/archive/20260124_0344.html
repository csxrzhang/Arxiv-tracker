<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-24 03:44</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260124_0344</div>
    <div class="row"><div class="card">
<div class="title">LLM-in-Sandbox Elicits General Agentic Intelligence</div>
<div class="meta-line">Authors: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</div>
<div class="meta-line">First: 2026-01-22T18:57:09+00:00 · Latest: 2026-01-22T18:57:09+00:00</div>
<div class="meta-line">Comments: Project Page: https://llm-in-sandbox.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16206v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16206v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://llm-in-sandbox.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#x27;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM-in-Sandbox激发通用智能体智能</div>
<div class="mono" style="margin-top:8px">我们提出LLM-in-Sandbox方法，使大语言模型能在代码沙盒（即虚拟计算机）内进行探索，从而激发非代码领域的通用智能。我们首先证明，未经额外训练的强大大语言模型展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能自发访问外部资源获取新知识、利用文件系统处理长上下文、执行脚本满足格式要求。我们进一步表明，通过LLM-in-Sandbox强化学习（仅使用非智能体数据训练模型进行沙盒探索），可增强这些智能体能力。实验表明，LLM-in-Sandbox在免训练和训练后两种设定下，均实现了跨越数学、物理、化学、生物医学、长上下文理解及指令遵循的稳健泛化。最后，我们从计算与系统视角分析其效率，并将其开源为Python包以促进实际部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to elicit general agentic intelligence from large language models (LLMs) for non-code domains, this paper introduces LLM-in-Sandbox, a method that enables LLMs to explore within a code sandbox (a virtual computer) without additional training. The method leverages the sandbox for tasks like accessing external resources, managing long contexts via the file system, and executing scripts, with capabilities further enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL) trained on non-agentic data. Experimental results show robust generalization across mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following in both training-free and post-trained settings, with efficiency analyzed from computational and system perspectives and the tool open-sourced as a Python package.</div>
<div class="mono" style="margin-top:8px">本文旨在激发大型语言模型（LLM）在非代码领域的通用智能，提出了LLM-in-Sandbox方法，允许LLM在代码沙箱（虚拟计算机）中探索而无需额外训练。该方法利用沙箱执行访问外部资源、通过文件系统处理长上下文和运行脚本等任务，并通过基于非智能数据训练的LLM-in-Sandbox强化学习（LLM-in-Sandbox-RL）进一步增强能力。实验结果表明，该方法在训练无关和训练后设置下，在数学、物理、化学、生物医学、长上下文理解和指令遵循等方面实现了稳健的泛化，并从计算和系统角度分析了效率，最终作为Python包开源以促进实际部署。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-01-22T18:24:00+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v1">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题发现新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时进行强化学习，使大型语言模型能够持续训练，但此时训练经验专门针对测试问题。这种持续学习形式非常特殊，其目标是产生一个卓越解决方案而非多个平均良好的方案，并专注于解决当前问题而非泛化到其他问题。因此，我们的学习目标和搜索子程序被设计为优先考虑最有潜力的解决方案。我们将此方法称为“测试时训练发现法”。遵循先前研究，我们聚焦于具有连续奖励的问题。我们报告了在数学、GPU内核工程、算法设计和生物学领域尝试的所有问题的结果。测试时训练发现法在几乎所有问题上都创造了新的最优解：（i）埃尔德什最小重叠问题与自相关不等式；（ii）GPUMode内核竞赛（比先前最优方案快达2倍）；（iii）往届AtCoder算法竞赛；（iv）单细胞分析中的去噪问题。我们的解决方案均经过专家或组织者评审。所有结果均使用开源模型OpenAI gpt-oss-120b实现，并可通过我们公开的代码复现，而先前最优结果需依赖封闭前沿模型。测试时训练运行通过Thinking Machines的Tinker API执行，每个问题成本仅数百美元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the goal of using AI to achieve new state-of-the-art solutions for scientific problems, this paper introduces Test-Time Training to Discover (TTT-Discover), a method that performs reinforcement learning at test time to allow a large language model (LLM) to continually train on experience specific to a given problem, prioritizing the most promising solutions. The approach focuses on problems with continuous rewards and is designed to produce one great solution rather than average performance across many. Experimental results demonstrate that TTT-Discover sets new state-of-the-art benchmarks across multiple domains: it advances solutions in Erdős&#x27; minimum overlap problem and an autocorrelation inequality in mathematics, achieves up to 2x speedup in a GPU kernel engineering competition, excels in past AtCoder algorithm contests, and improves denoising in single-cell biology analysis, with all solutions verified by experts. Notably, these results are achieved using an open model (OpenAI gpt-oss-120b) and publicly available code, contrasting with prior work that relied on closed frontier models, with test-time training costs kept to a few hundred dollars per problem.</div>
<div class="mono" style="margin-top:8px">本文旨在利用人工智能为科学问题发现新的最优解，提出了测试时训练发现方法（TTT-Discover），该方法在测试时通过强化学习使大型语言模型（LLM）能够针对特定问题持续训练，并优先探索最有希望的解决方案。该方法专注于具有连续奖励的问题，旨在产生单个优秀解而非平均性能。实验结果表明，TTT-Discover在多个领域设立了新的最先进基准：在数学中改进了Erdős最小重叠问题和自相关不等式，在GPU内核工程竞赛中实现了比先前技术快2倍的速度，在过去的AtCoder算法竞赛中表现出色，并在单细胞生物学分析中提升了去噪效果，所有解均经专家验证。值得注意的是，这些成果使用开源模型（OpenAI gpt-oss-120b）和公开代码实现，与依赖封闭前沿模型的先前工作形成对比，且每个问题的测试时训练成本仅需数百美元。</div>
</details>
</div>
<div class="card">
<div class="title">Structured Hints for Sample-Efficient Lean Theorem Proving</div>
<div class="meta-line">Authors: Zachary Burton</div>
<div class="meta-line">First: 2026-01-22T18:16:46+00:00 · Latest: 2026-01-22T18:16:46+00:00</div>
<div class="meta-line">Comments: 9 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构化提示在样本高效Lean定理证明中的应用</div>
<div class="mono" style="margin-top:8px">当前最先进的神经定理证明器（如DeepSeek-Prover-V1.5）将大语言模型与强化学习相结合，通过复杂训练取得了显著成果。我们提出：这些经过高度训练的模型在推理阶段是否仍能从简单的结构引导中获益？我们在miniF2F基准测试中评估了一种轻量级干预方法——基于15种常见策略骨架的固定提示调度方案。这种简单方法在相同样本数（k=16）和相同最大生成长度（1024个标记）下，实现了21.7%的pass@16，而标准采样方法仅为15.2%，相对提升达43%。结果表明，即使经过RL训练的证明器也未能充分利用策略语言中的结构化先验，而简单的推理阶段引导仍能提供低成本、互补性的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates whether advanced neural theorem provers, which are already trained with reinforcement learning, can still benefit from simple structural guidance during inference. The method introduces a lightweight intervention using a fixed prompt schedule over 15 common tactic skeletons to provide hints. Experimental results on the miniF2F benchmark show that this approach achieves a 21.7% pass@16 rate, compared to 15.2% with standard sampling, representing a 43% relative improvement with the same sample count and generation length, indicating that inference-time guidance offers a cheap complementary boost.</div>
<div class="mono" style="margin-top:8px">本研究探讨了经过强化学习训练的高级神经定理证明器在推理时是否仍能从简单的结构指导中受益。方法采用轻量级干预，通过固定提示计划覆盖15种常见策略骨架来提供提示。在miniF2F基准测试中的实验结果显示，该方法实现了21.7%的pass@16率，而标准采样仅为15.2%，在相同样本数和生成长度下相对提升了43%，表明推理时指导是一种低成本且有效的补充增强手段。</div>
</details>
</div>
<div class="card">
<div class="title">ViSymRe: Vision-guided Multimodal Symbolic Regression</div>
<div class="meta-line">Authors: Da Li, Junping Yin, Jin Xu, Xinxin Li, Juan Zhang</div>
<div class="meta-line">First: 2024-12-15T10:05:31+00:00 · Latest: 2026-01-22T17:29:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.11139v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.11139v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extracting simple mathematical expression from an observational dataset to describe complex natural phenomena is one of the core objectives of artificial intelligence (AI). This field is known as symbolic regression (SR). Traditional SR models are based on genetic programming (GP) or reinforcement learning (RL), facing well-known challenges, such as low efficiency and overfitting. Recent studies have integrated SR with large language models (LLMs), enabling fast zero-shot inference by learning mappings from millions of dataset-expression pairs. However, since the input and output are inherently different modalities, such models often struggle to converge effectively. In this paper, we introduce ViSymRe, a vision-guided multimodal SR model that incorporates the third resource, expression graph, to bridge the modality gap. Different from traditional multimodal models, ViSymRe is trained to extract vision, termed virtual vision, from datasets, without relying on the global availability of expression graphs, which addresses the essential challenge of visual SR, i.e., expression graphs are not available during inference. Evaluation results on multiple mainstream benchmarks show that ViSymRe achieves more competitive performance than the state-of-the-art dataset-only baselines. The expressions predicted by ViSymRe not only fit the dataset well but are also simple and structurally accurate, goals that SR models strive to achieve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSymRe：视觉引导的多模态符号回归</div>
<div class="mono" style="margin-top:8px">从观测数据集中提取简单数学表达式以描述复杂自然现象，是人工智能（AI）的核心目标之一，该领域称为符号回归（SR）。传统SR模型基于遗传编程（GP）或强化学习（RL），面临效率低下和过拟合等常见挑战。近期研究将SR与大型语言模型（LLMs）结合，通过从数百万数据集-表达式对中学习映射，实现快速零样本推理。然而，由于输入与输出本质属于不同模态，此类模型常难以有效收敛。本文提出ViSymRe，一种视觉引导的多模态SR模型，通过引入第三种资源——表达式图——来弥合模态鸿沟。与传统多模态模型不同，ViSymRe训练从数据集中提取视觉信息（称为虚拟视觉），无需依赖表达式图的全局可用性，从而解决了视觉SR的关键挑战：推理时表达式图不可用。在多个主流基准测试上的评估结果表明，ViSymRe比当前最先进的仅使用数据集的基线模型具有更强竞争力。ViSymRe预测的表达式不仅与数据集高度拟合，而且结构简洁准确，这正是SR模型追求的目标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ViSymRe, a vision-guided multimodal symbolic regression model motivated by the need to overcome the modality gap and inefficiencies in traditional genetic programming or reinforcement learning-based approaches, as well as the convergence struggles of recent large language model integrations. The method uniquely incorporates a third resource, expression graphs, to bridge the modality gap by training the model to extract &quot;virtual vision&quot; from datasets without relying on globally available expression graphs during inference, thus addressing the core challenge of visual symbolic regression. Experimental results on multiple mainstream benchmarks demonstrate that ViSymRe outperforms state-of-the-art dataset-only baselines, producing expressions that are not only well-fitting but also simple and structurally accurate, aligning with key goals of symbolic regression.</div>
<div class="mono" style="margin-top:8px">本文提出了ViSymRe，一种视觉引导的多模态符号回归模型，其动机在于克服传统基于遗传编程或强化学习方法效率低下和过拟合的问题，以及近期大语言模型集成中因输入输出模态不同而难以有效收敛的挑战。该方法创新性地引入第三种资源——表达式图，通过训练模型从数据集中提取“虚拟视觉”来弥合模态差距，且无需在推理时依赖全局可用的表达式图，从而解决了视觉符号回归的核心难题。在多个主流基准测试上的评估结果表明，ViSymRe的性能优于当前最先进的仅基于数据集的基线方法，其预测的表达式不仅与数据拟合良好，而且结构简单准确，实现了符号回归模型追求的关键目标。</div>
</details>
</div>
<div class="card">
<div class="title">GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning</div>
<div class="meta-line">Authors: Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya Wang, Jun Wang, Ye Shi</div>
<div class="meta-line">First: 2025-05-24T15:57:07+00:00 · Latest: 2026-01-22T17:10:05+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18763v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.18763v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO&#x27;s superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenPO：生成扩散模型与在线策略强化学习的融合</div>
<div class="mono" style="margin-top:8px">强化学习领域的最新进展揭示了基于生成扩散的策略在探索能力与多模态表达方面的优势。尽管离线强化学习和离线策略强化学习已取得显著进步，但将扩散策略整合至PPO等在线策略框架的研究仍显不足。这一空白在IsaacLab等专为在线策略算法优化的大规模并行GPU加速模拟器广泛应用的背景下尤为突出，此类模拟器能高效训练复杂机器人任务。核心挑战在于计算扩散策略下的状态-动作对数似然：高斯策略可直接计算，而基于流的模型因不可逆的前向-反向过程及离散化误差（如欧拉-丸山近似）难以处理。为填补这一空白，我们提出GenPO——一种利用精确扩散反演构建可逆动作映射的生成式策略优化框架。GenPO通过创新的双重虚拟动作机制，借助交替更新实现可逆性，突破了对数似然计算瓶颈。此外，我们利用动作对数似然进行无偏熵与KL散度估计，实现在线策略更新中的KL自适应学习率与熵正则化。在IsaacLab八项基准测试（涵盖足式运动、灵巧操作、空中控制与机械臂任务）上的大量实验表明，GenPO显著优于现有强化学习基线。值得注意的是，GenPO是首个成功将扩散策略融入在线策略强化学习的方法，为大规模并行化训练与现实机器人部署开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the underexplored integration of generative diffusion models into on-policy reinforcement learning (RL), motivated by the widespread use of GPU-accelerated simulators like IsaacLab that favor on-policy algorithms for training complex robotic tasks. The proposed method, GenPO, introduces a generative policy optimization framework that uses exact diffusion inversion to create invertible action mappings via a doubled dummy action mechanism, enabling tractable computation of state-action log-likelihoods—a key challenge with diffusion policies—and allowing for unbiased entropy and KL divergence estimation in on-policy updates. Experimental results across eight IsaacLab benchmarks, including legged locomotion, dexterous manipulation, aerial control, and robotic arm tasks, demonstrate GenPO&#x27;s superiority over existing RL baselines, marking the first successful integration of diffusion policies into on-policy RL for scalable robotic training and deployment.</div>
<div class="mono" style="margin-top:8px">本文针对生成扩散模型在在线策略强化学习中应用不足的问题展开研究，其动机源于像IsaacLab这样的大规模并行GPU加速模拟器广泛采用在线策略算法来训练复杂机器人任务。所提出的方法GenPO是一个生成策略优化框架，它通过精确扩散反演和一种双虚拟动作机制构建可逆的动作映射，从而解决了扩散策略中状态-动作对数似然计算的关键难题，并实现了在线策略更新中无偏的熵和KL散度估计。在八个IsaacLab基准测试（包括足式运动、灵巧操作、空中控制和机械臂任务）上的实验结果表明，GenPO优于现有强化学习基线，首次成功将扩散策略集成到在线策略强化学习中，为大规模机器人训练和部署开辟了潜力。</div>
</details>
</div>
<div class="card">
<div class="title">GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Zhengqing Yan, Xinyang Liu, Yi Zhang, Fan Guo, ChengXun Jia, Junchen Wan, Yao Liu, Qi Liu, Jihao Huang, Kang Song</div>
<div class="meta-line">First: 2026-01-11T07:34:41+00:00 · Latest: 2026-01-22T14:58:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06795v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.06795v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDEPO：面向样本受限强化学习的增强训练数据利用率的群组双动态等权优势策略优化方法</div>
<div class="mono" style="margin-top:8px">自动定理证明（ATP）是人工智能（AI）领域的一项基础性挑战，其目标是在Lean等形式化语言中构建机器可验证的证明，以评估AI的推理能力。强化学习（RL），尤其是高性能的群组相对策略优化（GRPO）算法，已成为该任务的主流方法。然而，在ATP场景中，GRPO面临两个关键问题：当使用复合奖励时，其相对优势估计可能与形式化验证器的二元反馈产生冲突；同时，其静态采样策略若未找到有效证明，会丢弃整批数据，导致模型更新无贡献并造成显著数据浪费。为解决这些局限，我们提出了群组双动态等权优势策略优化（GDEPO），该方法包含三个核心机制：1）动态补充采样，对无效批次重新采样直至发现有效证明；2）等权优势，将优势函数的符号（基于正确性）与其幅度（由辅助奖励调节）解耦，以确保策略更新的稳定性和正确性；3）动态补充迭代，对初始失败但最终成功的样本应用额外梯度步骤，以加速对困难案例的学习。在三个不同难度数据集（MinF2F-test、MathOlympiadBench、PutnamBench）上的实验验证了GDEPO的有效性，消融研究则证实了其协同组件的必要性。该方法提升了数据利用率和优化效率，为ATP提供了一种新颖的训练范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing reinforcement learning methods like GRPO in Automated Theorem Proving (ATP), where static sampling wastes data and composite rewards conflict with verifier feedback, this paper introduces GDEPO. The method employs dynamic additional sampling to reuse failed batches, an equal-right advantage mechanism to decouple reward signals for stable policy updates, and dynamic additional iterations to focus on hard cases. Experimental results on datasets including MinF2F-test, MathOlympiadBench, and PutnamBench demonstrate GDEPO&#x27;s effectiveness in improving data utilization and optimization efficiency, with ablation studies confirming the contribution of each component.</div>
<div class="mono" style="margin-top:8px">针对自动定理证明中现有强化学习方法如GRPO存在的静态采样导致数据浪费、复合奖励与验证器反馈冲突等问题，本文提出了GDEPO方法。该方法采用动态补充采样重用失败批次，通过等权优势机制解耦奖励信号以实现稳定策略更新，并利用动态额外迭代专注于困难样本。在MinF2F-test、MathOlympiadBench和PutnamBench等不同难度数据集上的实验结果表明，GDEPO有效提升了数据利用率和优化效率，消融研究验证了各协同组件的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Representation-Driven Reinforcement Learning</div>
<div class="meta-line">Authors: Ofir Nabati, Guy Tennenholtz, Shie Mannor</div>
<div class="meta-line">Venue: ICML 2023</div>
<div class="meta-line">First: 2023-05-31T14:59:12+00:00 · Latest: 2026-01-22T14:39:55+00:00</div>
<div class="meta-line">Comments: Accepted to ICML 2023</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2305.19922v3">Abs</a> · <a href="https://arxiv.org/pdf/2305.19922v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>表征驱动的强化学习</div>
<div class="mono" style="margin-top:8px">我们提出了一种表征驱动的强化学习框架。通过将策略表示为其期望值的估计，我们利用上下文赌博机技术来指导探索与利用。特别地，将策略网络嵌入线性特征空间，使我们能够将探索-利用问题重新定义为表征-利用问题，其中良好的策略表征能够实现最优探索。我们通过将该框架应用于进化和基于策略梯度的方法，证明了其有效性，与传统方法相比性能显著提升。该框架为强化学习提供了新视角，强调了策略表征在确定最优探索-利用策略中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better balance exploration and exploitation in reinforcement learning, this paper introduces a representation-driven framework that reinterprets policies as estimates of their expected values, applying contextual bandit techniques to guide decision-making. The method embeds policy networks into a linear feature space, reformulating the classic exploration-exploitation dilemma as a representation-exploitation problem where effective policy representations facilitate optimal exploration. Experimental results applying this framework to both evolutionary and policy gradient-based methods show significant performance improvements over traditional approaches, underscoring the critical role of policy representation in determining effective strategies.</div>
<div class="mono" style="margin-top:8px">本文的动机是改进强化学习中探索与利用的平衡，提出了一个表征驱动的框架，将策略重新定义为对其期望值的估计，并利用上下文赌博机技术来指导决策。该方法将策略网络嵌入线性特征空间，从而将经典的探索-利用问题重构为表征-利用问题，其中良好的策略表征能够实现最优探索。实验结果表明，该框架应用于进化和基于策略梯度的方法时，相比传统方法性能显著提升，突显了策略表征在决定有效策略中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour</div>
<div class="meta-line">Authors: Liang Wang, Kanzhong Yao, Yang Liu, Weikai Qin, Jun Wu, Zhe Sun, Qiuguo Zhu</div>
<div class="meta-line">First: 2026-01-22T14:16:12+00:00 · Latest: 2026-01-22T14:16:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15995v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15995v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot&#x27;s real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA&#x27;s exceptional agility and robustness in challenging scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PUMA：面向移动增强型四足跑酷的感知驱动统一落脚点先验</div>
<div class="mono" style="margin-top:8px">四足机器人的跑酷任务已成为敏捷运动能力的重要基准。人类运动员能有效感知环境特征以选择合适落脚点穿越障碍，但赋予腿式机器人类似的感知推理能力仍是重大挑战。现有方法多依赖遵循预计算落脚点的分层控制器，限制了机器人的实时适应性与强化学习的探索潜力。为克服这些挑战，我们提出PUMA——一种将视觉感知与落脚点先验整合至单阶段训练过程的端到端学习框架。该方法利用地形特征估计以自我为中心的极坐标落脚点先验（包含相对距离与航向），引导机器人在跑酷任务中进行主动姿态调整。在仿真与真实环境中对多种离散复杂地形开展的广泛实验表明，PUMA在挑战性场景中具有卓越的敏捷性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge of enabling legged robots to perform agile parkour by autonomously selecting footholds based on environmental perception, moving beyond hierarchical controllers that rely on pre-computed footholds and limit adaptability. The method introduces PUMA, an end-to-end learning framework that integrates visual perception with egocentric polar foothold priors—encoding relative distance and heading—into a single-stage training process to guide active posture adaptation. Experimental results from both simulation and real-world tests across various complex terrains demonstrate that PUMA achieves exceptional agility and robustness in challenging parkour scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决让四足机器人基于环境感知自主选择落脚点以执行敏捷跑酷的挑战，超越依赖预计算落脚点、限制适应性的分层控制器。方法上提出了PUMA，这是一个端到端学习框架，将视觉感知与以自我为中心的极坐标落脚点先验（编码相对距离和朝向）集成到单阶段训练过程中，以引导主动姿态调整。在多种复杂离散地形的仿真和真实环境实验结果表明，PUMA在具有挑战性的跑酷场景中展现出卓越的敏捷性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Return-to-Go for Efficient Decision Transformer</div>
<div class="meta-line">Authors: Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li, Qirui Zheng, Xionghui Yang, Wenxin Li</div>
<div class="meta-line">First: 2026-01-22T13:42:08+00:00 · Latest: 2026-01-22T13:42:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15953v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT&#x27;s performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦回报到目标以实现高效决策Transformer</div>
<div class="mono" style="margin-top:8px">决策Transformer（DT）为离线强化学习建立了一种强大的序列建模方法，其动作预测以回报到目标（RTG）为条件，既用于训练中区分轨迹质量，也用于推理时指导动作生成。本研究揭示了该设计中的一个关键冗余：将整个RTG序列输入Transformer在理论上是不必要的，因为只有最近的RTG会影响动作预测。我们通过实验证明这种冗余可能损害DT的性能。为此，我们提出解耦DT（DDT）。DDT简化了架构，仅通过Transformer处理观测和动作序列，并使用最新的RTG指导动作预测。这种简化方法不仅提升了性能，还降低了计算成本。实验表明，DDT在多个离线RL任务中显著优于DT，并与最先进的DT变体相比展现出有竞争力的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper identifies a redundancy in the Decision Transformer (DT), where feeding the entire sequence of Return-to-Go (RTG) values into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction, and shows this can impair performance. To resolve this, the authors propose the Decoupled DT (DDT), which simplifies the architecture by processing only observation and action sequences through the Transformer while using the latest RTG to guide action prediction. Experimental results demonstrate that DDT not only improves performance over the original DT but also reduces computational cost and achieves competitive results against state-of-the-art DT variants across multiple offline reinforcement learning tasks.</div>
<div class="mono" style="margin-top:8px">本文指出决策变换器（DT）中存在冗余设计，即将整个回报目标（RTG）序列输入变换器在理论上是不必要的，因为只有最近的RTG影响动作预测，且实验表明这会损害性能。为解决此问题，作者提出了解耦决策变换器（DDT），它简化了架构，仅通过变换器处理观测和动作序列，同时使用最新的RTG来指导动作预测。实验结果表明，DDT不仅性能优于原始DT、降低了计算成本，而且在多个离线强化学习任务中与先进的DT变体相比取得了有竞争力的结果。</div>
</details>
</div>
<div class="card">
<div class="title">VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing</div>
<div class="meta-line">Authors: Guanyuan Pan, Shuai Wang, Yugui Lin, Tiansheng Zhou, Pietro Liò, Yaqi Wang, Zhenxin Zhao</div>
<div class="meta-line">First: 2026-01-12T08:37:32+00:00 · Latest: 2026-01-22T11:46:08+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures, submitted to the 10th International Conference on Control, Automation and Diagnosis (ICCAD&#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07315v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07315v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches rely solely on netlists, ignoring the circuit schematic, which hinders the cognitive link between the schematic and its performance. Furthermore, the black-box nature of machine learning methods and hallucination risks in large language models fail to provide the necessary ground-truth explainability required for industrial sign-off. To address these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-start from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance while maintaining physics-based explainability. VLM-CAD meets all specification requirements while maintaining low power consumption in optimizing an amplifier with a complementary input and a class-AB output stage, with a total runtime under 66 minutes across all experiments on two amplifiers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLM-CAD：面向模拟电路尺寸优化的视觉语言模型协同智能体设计流程</div>
<div class="mono" style="margin-top:8px">模拟混合信号电路尺寸优化涉及高维设计空间中的复杂权衡。现有自动优化方法仅依赖网表而忽略电路原理图，阻碍了原理图与性能间的认知关联。机器学习方法的黑箱特性及大语言模型的幻觉风险，均无法满足工业签核所需的可解释性要求。为此，我们提出视觉语言模型优化的协同智能体设计流程（VLM-CAD），通过电路分析、直流工作点优化、基于推理的尺寸确定及外部尺寸优化四个阶段，集成Image2Net工具标注原理图并生成结构化JSON描述供视觉语言模型精准解析。进一步提出可解释信任域贝叶斯优化方法（ExTuRBO），采用智能体生成种子的协同热启动策略，提供双粒度灵敏度分析支持外部尺寸优化，并生成完整设计报告。基于180nm、90nm和45nm预测技术模型的放大器尺寸优化实验表明，VLM-CAD在保持物理可解释性的同时有效平衡功耗与性能。在优化具有互补输入级和AB类输出级的放大器时，VLM-CAD满足全部指标要求且保持低功耗，两个放大器的全实验总运行时间低于66分钟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing automatic analog circuit sizing methods, which rely solely on netlists and lack explainability, by introducing VLM-CAD, a Vision Language Model-optimized collaborative agent workflow. The method integrates Image2Net to annotate circuit schematics into structured JSON for VLM interpretation and employs ExTuRBO, an explainable trust region Bayesian optimization approach with collaborative warm-start and sensitivity analysis, to optimize sizing. Experimental results on amplifier sizing tasks across 180nm, 90nm, and 45nm technology nodes show that VLM-CAD effectively balances power and performance while maintaining physics-based explainability, meeting all specifications with low power consumption and a total runtime under 66 minutes for two amplifiers.</div>
<div class="mono" style="margin-top:8px">本文针对现有自动模拟电路尺寸设计方法仅依赖网表且缺乏可解释性的问题，提出了VLM-CAD，一种视觉语言模型优化的协同智能体工作流程。该方法集成Image2Net将电路原理图标注为结构化JSON供VLM解析，并采用ExTuRBO，一种具有协同热启动和灵敏度分析的可解释信任域贝叶斯优化方法，进行尺寸优化。在180nm、90nm和45nm工艺节点的放大器尺寸设计实验中，VLM-CAD有效平衡了功耗与性能，同时保持基于物理的可解释性，满足所有规格要求且功耗较低，两个放大器的总运行时间在66分钟以内。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling for Strategic Multiagent Settings</div>
<div class="meta-line">Authors: Georgios Chalkiadakis, Charilaos Akasiadis, Gerasimos Koresis, Stergios Plataniotis, Leonidas Bakopoulos</div>
<div class="meta-line">First: 2025-11-13T17:06:56+00:00 · Latest: 2026-01-22T10:36:51+00:00</div>
<div class="meta-line">Comments: 28 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10501v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.10501v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper provides a comprehensive review of mainly GNN, DRL, and PTM methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) ML methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of GNN. Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of RL, and in particular that of multiagent deep reinforcement learning. Single-agent deep RL has been widely used for decision making in demanding game settings. Its application in multiagent settings though is hindered due to, e.g., varying relationships between agents, and non-stationarity of the environment. We describe existing relevant game theoretic solution concepts, and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes probabilistic topic modeling (PTM) in domains other than that of document analysis and classification. Finally, we identify certain open challenges -- specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向战略多智能体场景的图神经网络、深度强化学习与概率主题建模</div>
<div class="mono" style="margin-top:8px">本文系统综述了图神经网络（GNN）、深度强化学习（DRL）与概率主题建模（PTM）方法，重点关注其在战略多智能体场景中的潜在整合应用。研究聚焦于：（1）当前用于揭示可适应战略对手建模任务的未知模型结构的机器学习方法；（2）将这些方法与博弈论概念相结合，以规避现实场景中常不成立的假设（如共同先验假设与自利假说）。文章分析了方法处理不确定性与异质性（现实应用中的常见特征）的能力及可扩展性。针对多智能体场景中关系与交互的有效建模，我们倡导使用GNN——该方法专为图结构数据设计，已被证明在节点分类、链接预测等任务中具有强大效能。其次，综述涵盖了强化学习领域，特别是多智能体深度强化学习。单智能体深度强化学习已在复杂博弈决策中广泛应用，但其在多智能体场景中的应用受限于智能体间动态关系与环境非平稳性等因素。我们阐述了现有相关博弈论解概念，并考量公平性、稳定性等特性。综述还延伸探讨了概率主题建模在文档分析分类以外领域的应用文献。最后，我们指出若干开放挑战：需（1）适配非平稳环境；（2）平衡稳定性与适应性；（3）应对不确定性与异质性；（4）保障可扩展性与解的可处理性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This review paper is motivated by the need to model strategic multiagent interactions in complex, real-world settings where traditional game theory assumptions like the Common Prior Assumption and Self-Interest Hypothesis often fail. The method involves a comprehensive analysis of three machine learning approaches—Graph Neural Networks (GNNs) for capturing relational structures, Deep Reinforcement Learning (DRL) for decision-making, and Probabilistic Topic Modeling (PTM) for extracting latent patterns—and their potential integration with game-theoretic concepts to handle uncertainty, heterogeneity, and scalability. The main experimental insights highlight GNNs as a powerful tool for relationship modeling in graphs, while noting that multiagent DRL faces challenges like non-stationarity, and PTM shows promise beyond traditional text applications; the review concludes by identifying open challenges including adapting to non-stationary environments and balancing stability with adaptation.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要对复杂现实世界中的战略多智能体交互进行建模，而传统博弈论中的共同先验假设和自利假设等前提在此类场景中常常失效。方法上，该综述系统分析了三种机器学习方法——用于捕捉关系结构的图神经网络、用于决策的深度强化学习，以及用于提取潜在模式的概率主题建模——并探讨它们与博弈论概念结合的可能性，以处理不确定性、异质性和可扩展性。主要实验结果表明，图神经网络是处理图结构关系建模的强大工具，而多智能体深度强化学习面临环境非平稳性等挑战，概率主题建模在文本以外的领域也显示出潜力；综述最后指出了开放挑战，包括适应非平稳环境以及平衡稳定性与适应性。</div>
</details>
</div>
<div class="card">
<div class="title">PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling</div>
<div class="meta-line">Authors: Ai Jian, Jingqing Ruan, Xing Ma, Dailin Li, Weipeng Zhang, Ke Zeng, Xunliang Cai</div>
<div class="meta-line">First: 2025-10-28T09:43:47+00:00 · Latest: 2026-01-22T10:20:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24235v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24235v2">PDF</a> · <a href="https://github.com/JaneEyre0530/PaTaRM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are central to reinforcement learning from human feedback (RLHF), providing the critical supervision signals that align large language models (LLMs) with human preferences. Generative reward models (GRMs) provide greater interpretability than traditional scalar RMs, but they come with a critical trade-off: pairwise methods are hindered by a training-inference mismatch, while pointwise methods require expensive absolute annotations. To bridge this gap, we propose the Preference-aware Task-adaptive Reward Model (PaTaRM). Unlike prior approaches, PaTaRM enables robust pointwise training using readily available pairwise data via a novel Preference-Aware Reward (PAR) mechanism, eliminating the need for explicit rating labels. Furthermore, it incorporates a Task-Adaptive Rubric system that dynamically generates instance-specific criteria for precise evaluation. Extensive experiments demonstrate that PATRM achieves a 8.7% average improvement on RewardBench and RMBench across Qwen3-8B/14B models. Crucially, it boosts downstream RLHF performance by an average relative improvement of 13.6% across IFEval and InFoBench, validating its effectiveness for policy alignment. Our code is available at https://github.com/JaneEyre0530/PaTaRM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PaTaRM：通过偏好感知任务自适应奖励建模桥接成对与逐点信号</div>
<div class="mono" style="margin-top:8px">奖励模型（RMs）是基于人类反馈的强化学习（RLHF）的核心，为对齐大语言模型（LLMs）与人类偏好提供关键监督信号。生成式奖励模型（GRMs）比传统标量RMs具有更强的可解释性，但面临关键权衡：成对方法受限于训练-推理不匹配，而逐点方法需要昂贵的绝对标注。为弥合这一差距，我们提出偏好感知任务自适应奖励模型（PaTaRM）。与先前方法不同，PaTaRM通过新颖的偏好感知奖励（PAR）机制，利用现成的成对数据实现稳健的逐点训练，无需显式评分标签。此外，它引入任务自适应准则系统，动态生成实例特定标准以进行精确评估。大量实验表明，PaTaRM在Qwen3-8B/14B模型上的RewardBench和RMBench平均提升8.7%。关键的是，其在IFEval和InFoBench的下游RLHF性能平均相对提升13.6%，验证了其对策略对齐的有效性。代码发布于https://github.com/JaneEyre0530/PaTaRM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a key trade-off in generative reward models (GRMs) for RLHF: pairwise methods suffer from training-inference mismatch, while pointwise ones need costly absolute annotations. To bridge this gap, it proposes PaTaRM, a method that enables robust pointwise training using only pairwise data via a novel Preference-Aware Reward mechanism and incorporates a Task-Adaptive Rubric for instance-specific evaluation. Experiments show PaTaRM achieves an 8.7% average improvement on RewardBench and RMBench with Qwen3 models and boosts downstream RLHF performance by 13.6% on average across IFEval and InFoBench, validating its alignment effectiveness.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习人类反馈中生成式奖励模型的关键权衡问题：成对方法存在训练-推断不匹配，而逐点方法需要昂贵的绝对标注。为弥补这一差距，提出了PaTaRM方法，通过新颖的偏好感知奖励机制，仅使用成对数据实现稳健的逐点训练，并引入任务自适应准则系统进行实例特定评估。实验表明，PaTaRM在Qwen3模型上于RewardBench和RMBench平均提升8.7%，并在IFEval和InFoBench的下游RLHF性能中平均相对提升13.6%，验证了其对齐有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking</div>
<div class="meta-line">Authors: Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang, Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang, Ailin Ren, Xin Li, Pengjun Xie, Jiawei Liu, Ning Guo, Jingren Zhou, Zheng-Jun Zha</div>
<div class="meta-line">First: 2026-01-10T08:43:07+00:00 · Latest: 2026-01-22T09:16:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06487v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06487v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ArenaRL：基于锦标赛相对排序的开放式智能体强化学习扩展框架</div>
<div class="mono" style="margin-top:8px">强化学习显著提升了LLM智能体在可验证结果任务上的表现，但在开放式智能体任务（如复杂旅行规划）的广阔解空间中仍面临挑战。由于此类任务缺乏客观真值，现有RL算法主要依赖为单条响应分配标量分数的奖励模型。我们认为这种逐点评分存在固有的判别力坍缩问题：奖励模型难以区分不同轨迹间的细微优势，导致组内分数被压缩至狭窄区间，有效奖励信号被模型噪声主导，引发优化停滞。为此，我们提出ArenaRL强化学习范式，将逐点标量评分转为组内相对排序。该框架引入过程感知的成对评估机制，采用多级量规为轨迹分配细粒度相对分数；同时构建组内对抗竞技场，设计基于锦标赛的排序方案以获取稳定的优势信号。实验证实：构建的种子单败淘汰方案在仅O(N)复杂度下，其优势估计精度与O(N²)复杂度的全成对比较近乎等价，实现了效率与精度的最优平衡。此外，针对开放式智能体缺乏全周期基准的问题，我们构建了Open-Travel与Open-DeepResearch两个高质量基准，涵盖SFT、RL训练及多维评估的完整流程。大量实验表明，ArenaRL显著优于标准RL基线，使LLM智能体能针对复杂现实任务生成更鲁棒的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying reinforcement learning to open-ended agent tasks, such as complex travel planning, where vast solution spaces and the absence of objective ground-truth cause conventional pointwise reward models to suffer from discrimination collapse, compressing scores and stalling optimization. To overcome this, the authors propose ArenaRL, a paradigm that shifts from scalar scoring to intra-group relative ranking via a process-aware pairwise evaluation mechanism with multi-level rubrics and a tournament-based ranking scheme in an adversarial arena, achieving O(N) complexity with accuracy comparable to O(N^2) full comparisons. Experimental results on newly built benchmarks, Open-Travel and Open-DeepResearch, demonstrate that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to produce more robust solutions for complex real-world tasks.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在开放式智能体任务（如复杂旅行规划）中的应用挑战，其中庞大的解空间和缺乏客观真值导致传统点式奖励模型出现判别崩溃，压缩分数并阻碍优化。为解决此问题，作者提出ArenaRL范式，通过基于过程感知的成对评估机制和多级评分标准，结合锦标赛式排名方案在对抗性竞技场中实现从标量评分到组内相对排名的转变，以O(N)复杂度达到与O(N^2)全比较相当的精度。在新建基准Open-Travel和Open-DeepResearch上的实验结果表明，ArenaRL显著优于标准强化学习基线，使LLM智能体能为复杂现实任务生成更稳健的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-22T09:09:47+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但其仍易受对抗性越狱攻击影响——攻击者通过精心构造的细微扰动绕过安全机制，触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等问题，难以应用于现实黑盒场景。为此，我们提出基于同步扰动随机逼近零阶优化（ZO-SPSA）的黑盒越狱攻击方法，具备三大优势：（1）通过输入输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低GPU内存占用的资源需求。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，且扰动不可感知性与白盒方法相当。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱的现实可行性，并暴露了当前LVLMs安全机制的关键缺陷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods that require full model access, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free, model-agnostic approach approximates gradients through input-output interactions without needing internal model knowledge, thereby reducing computational and memory costs. Experimental results on models like InstructBLIP, LLaVA, and MiniGPT-4 demonstrate high jailbreak success rates, up to 83.0% on InstructBLIP, with imperceptible perturbations, and show strong transferability of adversarial examples across models, with attack success rates reaching 64.18%, highlighting significant safety weaknesses in LVLMs.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型（LVLMs）易受对抗性越狱攻击的脆弱性，以及现有白盒方法需要完整模型访问权限而不切实际的问题，本文提出了一种使用基于同时扰动随机逼近的零阶优化（ZO-SPSA）的黑盒攻击方法。这种无需梯度、与模型无关的方法通过输入-输出交互来近似梯度，无需了解模型内部知识，从而降低了计算和内存成本。在InstructBLIP、LLaVA和MiniGPT-4等模型上的实验结果表明，该方法实现了较高的越狱成功率（在InstructBLIP上最高达83.0%），且扰动难以察觉，同时生成的对抗样本在模型间表现出强迁移性，攻击成功率可达64.18%，揭示了LVLMs安全机制的重大缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</div>
<div class="meta-line">Authors: Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T17:59:44+00:00 · Latest: 2026-01-22T08:52:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025 Track on Datasets and Benchmarks. Project page: https://faceong.github.io/VIKI-R/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09049v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09049v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://faceong.github.io/VIKI-R/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIKI-R：基于强化学习的具身多智能体协同协调框架</div>
<div class="mono" style="margin-top:8px">在动态环境中协调多个具身智能体仍是人工智能的核心挑战，需要感知驱动的推理与可扩展的协作策略。尽管近期研究利用大语言模型进行多智能体规划，少数工作开始探索视觉语言模型在视觉推理中的应用，但这些基于VLM的方法对多样化具身形态的支持仍显不足。本研究提出首个面向具身多智能体协作的层次化基准VIKI-Bench，包含智能体激活、任务规划与轨迹感知三层结构，涵盖多类机器人形态、多视角视觉观测及结构化监督信号，以评估基于视觉输入的推理能力。为验证其效用，我们提出两阶段框架VIKI-R：先通过思维链标注样本微调预训练视觉语言模型，再基于多层次奖励信号进行强化学习。实验表明VIKI-R在所有任务层级均显著优于基线方法，且强化学习能促使异构智能体涌现组合式协作模式。VIKI-Bench与VIKI-R共同为具身AI系统的多智能体视觉驱动协作提供了统一测试平台与方法体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of coordinating multiple embodied agents in dynamic environments, where existing vision-language model (VLM) approaches lack support for diverse robot embodiments. The authors introduce VIKI-Bench, a hierarchical benchmark for embodied multi-agent cooperation with three levels—agent activation, task planning, and trajectory perception—and propose VIKI-R, a two-stage method that fine-tunes a pretrained VLM using Chain-of-Thought demonstrations followed by reinforcement learning with multi-level rewards. Experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all task levels and enables the emergence of compositional cooperation patterns among heterogeneous agents.</div>
<div class="mono" style="margin-top:8px">本文针对动态环境中多具身智能体协调的挑战，现有视觉语言模型方法对多样化机器人具身形态支持不足。作者提出了VIKI-Bench，这是一个面向具身多智能体协作的分层基准，包含智能体激活、任务规划和轨迹感知三个层级，并提出了VIKI-R方法，该方法通过思维链标注演示微调预训练视觉语言模型，再结合多级奖励进行强化学习。实验结果表明，VIKI-R在所有任务层级上均显著优于基线方法，并能促进异构智能体间涌现组合式协作模式。</div>
</details>
</div>
<div class="card">
<div class="title">Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning</div>
<div class="meta-line">Authors: Xiefeng Wu, Mingyu Hu, Shu Zhang</div>
<div class="meta-line">First: 2026-01-22T08:51:16+00:00 · Latest: 2026-01-22T08:51:16+00:00</div>
<div class="meta-line">Comments: 7 pages main text 2 page reference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15761v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15761v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Sigmoid有界熵的离策略演员-评论家方法在现实世界机器人学习中的应用</div>
<div class="mono" style="margin-top:8px">由于样本效率低、奖励稀疏和视觉观测噪声等问题，在现实世界中部署强化学习仍具挑战。先前研究利用演示和人类反馈提升学习效率与鲁棒性，但离线转在线方法需要大规模数据集且可能不稳定，而视觉语言模型辅助的强化学习依赖大规模预训练与微调。因此，尚需一种数据需求极低的低成本现实世界强化学习方法。本文提出\textbf{SigEnt-SAC}——一种仅需单条专家轨迹即可从零学习的离策略演员-评论家方法。其核心设计是sigmoid有界熵项，可防止负熵驱动优化导致分布外动作，并减少Q函数振荡。我们在D4RL基准任务中对比代表性基线方法，实验表明SigEnt-SAC显著缓解Q函数振荡，并以更快速度达到100%成功率。最后，我们在四种跨不同实体形态的现实机器人任务中验证该方法，智能体仅通过原始图像和稀疏奖励进行学习。结果表明SigEnt-SAC仅需少量现实交互即可习得有效策略，为现实世界强化学习部署提供了低成本实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of deploying reinforcement learning in real-world robotics, where issues like sample inefficiency, sparse rewards, and noisy visual observations hinder practical application, and existing methods often require large datasets or extensive pretraining. To overcome this, the authors propose SigEnt-SAC, an off-policy actor-critic method that learns from scratch using only a single expert trajectory, incorporating a sigmoid-bounded entropy term to prevent optimization toward out-of-distribution actions and reduce Q-function oscillations. Experimental results on D4RL benchmarks show that SigEnt-SAC significantly mitigates Q-function oscillations and achieves a 100% success rate more quickly than prior methods, and validation on four real-world robotic tasks with raw images and sparse rewards demonstrates that it can learn successful policies with minimal real-world interactions, offering a low-cost pathway for real-world RL deployment.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在现实世界机器人部署中的挑战，如样本效率低、奖励稀疏和视觉观测噪声等问题，而现有方法通常需要大量数据或预训练。为解决这一问题，作者提出了SigEnt-SAC，一种离策略演员-评论家方法，仅使用单条专家轨迹从头开始学习，通过引入sigmoid有界熵项来防止优化偏离分布动作并减少Q函数振荡。在D4RL基准测试中的实验结果表明，SigEnt-SAC显著缓解了Q函数振荡，并比先前方法更快达到100%成功率；在四个现实世界机器人任务中，使用原始图像和稀疏奖励进行验证，证明该方法能以极少的现实交互学习到成功策略，为现实世界强化学习部署提供了一条低成本实用途径。</div>
</details>
</div>
<div class="card">
<div class="title">PhysProver: Advancing Automatic Theorem Proving for Physics</div>
<div class="meta-line">Authors: Hanning Zhang, Ruida Wang, Rui Pan, Wenyuan Wang, Bingxu Meng, Tong Zhang</div>
<div class="meta-line">First: 2026-01-22T08:05:32+00:00 · Latest: 2026-01-22T08:05:32+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15737v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15737v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysProver：推进物理学领域的自动定理证明</div>
<div class="mono" style="margin-top:8px">可验证语言与大型语言模型的结合为定理证明提供了严谨基础，深刻影响了数学与计算机科学领域。该领域的最新进展通过基础模型与智能体系统，正将形式化数学推理能力推向接近大型语言模型的自然语言处理水平。然而，形式化物理推理领域却鲜有关注，其同样高度依赖类似的问题求解与定理证明框架。为此，本文提出了首个增强物理学领域形式化定理证明的方法。我们构建了专用数据集PhysLeanData，包含从PhysLean采样的定理以及基于猜想的形式化数据生成流程产生的数据。在训练流程中，我们基于开源数学定理证明器DeepSeek-Prover-V2-7B，采用可验证奖励强化学习技术训练出PhysProver模型。综合实验表明，仅使用约5K训练样本，PhysProver在多个子领域实现了2.4%的整体性能提升。经形式化物理训练后，模型在MiniF2F-Test基准测试中取得1.3%的性能增益，这显示出其不仅具备跨物理学领域的泛化能力，还增强了形式化数学推理能力。研究成果验证了本方法在扩展形式化证明器至数学领域之外的有效性与高效性。为促进后续研究，我们将公开数据集与模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of attention to formal physics reasoning despite advances in mathematical theorem proving, this paper introduces PhysProver, the first approach to enhance automatic theorem proving specifically for physics. The method involves creating a dedicated dataset, PhysLeanData, from sampled theorems and a conjecture-based generation pipeline, and then training the DeepSeek-Prover-V2-7B model using Reinforcement Learning with Verifiable Rewards (RLVR). Experimental results show that PhysProver achieves an overall 2.4% improvement across multiple physics sub-domains with only about 5,000 training samples, and it also demonstrates non-trivial generalization with a 1.3% gain on the MiniF2F-Test benchmark, indicating enhanced formal math capability beyond physics.</div>
<div class="mono" style="margin-top:8px">本文的动机是，尽管数学定理证明领域已取得进展，但形式化物理推理却鲜有关注，因此提出了首个专门针对物理领域的自动定理证明增强方法PhysProver。该方法通过从采样定理和基于猜想的生成流程构建专用数据集PhysLeanData，并利用可验证奖励的强化学习（RLVR）训练DeepSeek-Prover-V2-7B模型。主要实验结果表明，PhysProver仅使用约5,000个训练样本就在多个物理子领域实现了整体2.4%的性能提升，并在MiniF2F-Test基准上获得了1.3%的增益，显示出超越物理领域的非平凡泛化能力，同时增强了形式化数学能力。</div>
</details>
</div>
<div class="card">
<div class="title">Online Operator Design in Evolutionary Optimization for Flexible Job Shop Scheduling via Large Language Models</div>
<div class="meta-line">Authors: Rongjie Liao, Junhao Qiu, Xin Chen, Xiaoping Li</div>
<div class="meta-line">First: 2025-11-20T15:56:09+00:00 · Latest: 2026-01-22T07:54:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16485v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.16485v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Customized static operator design has enabled widespread application of Evolutionary Algorithms (EAs), but their search effectiveness often deteriorates as evolutionary progresses. Dynamic operator configuration approaches attempt to alleviate this issue, but they typically rely on predefined operator structures and localized parameter control, lacking sustained adaptive optimization throughout evolution. To overcome these limitations, this work leverages Large Language Models (LLMs) to perceive evolutionary dynamics and enable operator-level meta-evolution. The proposed framework, LLMs for online operator design in Evolutionary Optimization, named LLM4EO, comprises three components: knowledge-transfer-based operator design, evolution perception and analysis, and adaptive operator evolution. Firstly, operators are initialized by leveraging LLMs to distill and transfer knowledge from well-established operators. Then, search behaviors and potential limitations of operators are analyzed by integrating fitness performance with evolutionary features, accompanied by suggestions for improvement. Upon stagnation of population evolution, an LLM-driven meta-operator dynamically optimizes gene selection of operators by prompt-guided improvement strategies. This approach achieves co-evolution of solutions and operators within a unified optimization framework, introducing a novel paradigm for enhancing the efficiency and adaptability of EAs. Finally, extensive experiments on multiple benchmarks of flexible job shop scheduling problem demonstrate that LLM4EO accelerates population evolution and outperforms tailored EAs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的柔性作业车间调度进化优化在线算子设计</div>
<div class="mono" style="margin-top:8px">定制化静态算子设计促进了进化算法的广泛应用，但其搜索效能常随进化进程而衰减。动态算子配置方法试图缓解此问题，但通常依赖预定义算子结构和局部参数控制，缺乏贯穿进化过程的持续自适应优化。为突破这些局限，本研究利用大语言模型感知进化动态，实现算子层面的元进化。所提出的框架——进化优化在线算子设计大语言模型（LLM4EO）包含三个组件：基于知识迁移的算子设计、进化感知与分析、自适应算子进化。首先，通过大语言模型从成熟算子中提炼并迁移知识以初始化算子。随后，结合适应度表现与进化特征分析算子的搜索行为及潜在局限，并提出改进建议。当种群进化停滞时，由大语言模型驱动的元算子通过提示引导的改进策略动态优化算子的基因选择。该方法在统一优化框架内实现解与算子的协同进化，为提升进化算法效率与适应性引入新范式。最终，在柔性作业车间调度问题的多基准测试中，LLM4EO被证实能加速种群进化并优于定制化进化算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static and dynamic operator designs in Evolutionary Algorithms (EAs), which often degrade in effectiveness over time or rely on predefined structures, this paper introduces LLM4EO, a framework that leverages Large Language Models (LLMs) for online operator design. The method involves initializing operators via LLM-based knowledge transfer from established operators, analyzing evolutionary dynamics through fitness and features to identify limitations, and using an LLM-driven meta-operator to adaptively optimize gene selection during stagnation. Experimental results on flexible job shop scheduling benchmarks show that LLM4EO accelerates population evolution and outperforms tailored EAs, demonstrating enhanced efficiency and adaptability.</div>
<div class="mono" style="margin-top:8px">针对进化算法中静态和动态算子设计效果随进化进程下降或依赖预定义结构的局限性，本文提出了LLM4EO框架，利用大语言模型实现在线算子设计。该方法通过大语言模型从成熟算子中提炼知识来初始化算子，结合适应度与进化特征分析搜索行为与缺陷，并在种群进化停滞时使用大语言模型驱动的元算子根据提示策略动态优化基因选择。在柔性作业车间调度问题的多个基准测试中，实验结果表明LLM4EO能加速种群进化并优于定制进化算法，提升了效率与适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind</div>
<div class="meta-line">Authors: Zhitao He, Zongwei Lyu, Yi R Fung</div>
<div class="meta-line">First: 2026-01-22T07:36:48+00:00 · Latest: 2026-01-22T07:36:48+00:00</div>
<div class="meta-line">Comments: Preprint, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15715v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15715v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author&#x27;s own critical analysis and response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>枷锁之舞：基于心智理论的学术反驳策略性说服研究</div>
<div class="mono" style="margin-top:8px">尽管人工智能已深度融入科研工作流并取得显著进展，学术反驳仍是一个重要且探索不足的挑战。这是因为反驳是在严重信息不对称下进行的策略性沟通过程，而非简单的技术辩论。现有方法因主要模仿表层语言特征，缺乏有效说服所需的关键视角采择能力而表现不佳。本文提出首个基于心智理论的学术反驳框架RebuttalAgent，通过心智理论-策略-响应（TSR）流程实现：建模审稿人心理状态、制定说服策略、生成策略驱动的回应。为训练智能体，我们通过创新的批判-精炼方法构建了大规模数据集RebuttalBench。训练过程分为两阶段：首先通过监督微调使智能体掌握基于心智理论的分析与策略规划能力，随后利用自奖励机制进行强化学习以实现可扩展的自我改进。为进行可靠高效的自动化评估，我们进一步开发了专用评估器Rebuttal-RM，该模型基于超10万条多源反驳数据训练，其评分与人类偏好的一致性超越了GPT-4.1。大量实验表明，RebuttalAgent在自动化指标上平均超越基线模型18.3%，同时在自动化与人工评估中均优于先进的专有模型。免责声明：生成的反驳内容仅供作者启发思路与辅助草拟，不替代作者自身的批判性分析与回应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complex strategic communication and information asymmetry inherent in academic rebuttal, which existing AI approaches fail to address by merely imitating surface-level language, this paper introduces RebuttalAgent, a framework grounded in Theory of Mind (ToM). The method employs a ToM-Strategy-Response pipeline to model reviewer mental states and formulate persuasive strategies, trained on a novel synthesized dataset, RebuttalBench, via supervised fine-tuning and reinforcement learning with a self-reward mechanism. Main experimental results, evaluated by a specialized automated evaluator, Rebuttal-RM, show that RebuttalAgent outperforms the base model by an average of 18.3% on automated metrics and surpasses advanced proprietary models in both automated and human evaluations.</div>
<div class="mono" style="margin-top:8px">本文的动机在于学术反驳是一个存在严重信息不对称的复杂策略性沟通过程，现有AI方法仅模仿表层语言而难以应对。为此，论文提出了首个基于心理理论（ToM）的框架RebuttalAgent，其方法通过ToM-策略-响应流程建模审稿人心理状态并制定说服策略，并利用新颖合成数据集RebuttalBench，通过监督微调和基于自奖励机制的强化学习进行训练。主要实验结果表明，经专门评估器Rebuttal-RM评测，RebuttalAgent在自动化指标上平均超越基础模型18.3%，并在自动和人工评估中均优于先进的专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</div>
<div class="meta-line">Authors: Shaocong Ma, Heng Huang</div>
<div class="meta-line">First: 2025-10-22T18:22:25+00:00 · Latest: 2026-01-22T06:31:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19950v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19950v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>金融领域的鲁棒强化学习：基于椭圆不确定性集合的市场冲击建模</div>
<div class="mono" style="margin-top:8px">在金融应用中，强化学习智能体通常基于历史数据进行训练，此时其行为不会影响市场价格。然而在实际部署时，这些智能体在实时市场中进行交易，其自身交易行为可能导致资产价格变动，这种现象称为市场冲击。训练环境与部署环境之间的这种不匹配会显著降低模型性能。传统鲁棒强化学习方法通过在一组不确定性上优化最坏情况性能来解决模型误设问题，但通常依赖对称结构，无法捕捉市场冲击的方向性特征。为解决这一问题，我们开发了一类新型椭圆不确定性集合。针对这些集合下的最坏情况不确定性，我们建立了隐式和显式闭式解，实现了高效且可处理的鲁棒策略评估。在单资产和多资产交易任务上的实验表明，该方法能获得更优的夏普比率，并在交易量增加时保持鲁棒性，为金融市场中的强化学习提供了更可靠且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the performance degradation of reinforcement learning agents in finance due to market impact, where actions during live trading affect prices, unlike in historical training data. The authors propose a novel robust RL method using elliptic uncertainty sets to better model the directional nature of market impact, deriving implicit and explicit closed-form solutions for worst-case uncertainty to enable efficient policy evaluation. Experimental results on trading tasks show that this approach achieves a higher Sharpe ratio and maintains robustness with increasing trade volumes, providing a more accurate and scalable solution for financial applications.</div>
<div class="mono" style="margin-top:8px">本文针对金融领域中强化学习智能体因市场冲击（即实时交易行为影响价格，与历史训练数据不符）导致的性能下降问题，提出了一种使用椭圆不确定性集的新型鲁棒强化学习方法，以更好地捕捉市场冲击的方向性特征，并推导了最坏情况不确定性的隐式和显式闭式解，从而实现高效策略评估。在单资产和多资产交易任务上的实验结果表明，该方法获得了更高的夏普比率，并在交易量增加时保持鲁棒性，为金融市场提供了更准确且可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models</div>
<div class="meta-line">Authors: Jiaxin Zhang, Wendi Cui, Zhuohang Li, Lifu Huang, Bradley Malin, Caiming Xiong, Chien-Sheng Wu</div>
<div class="meta-line">First: 2026-01-22T06:21:31+00:00 · Latest: 2026-01-22T06:21:31+00:00</div>
<div class="meta-line">Comments: 20 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从被动度量到主动信号：不确定性量化在大语言模型中的角色演变</div>
<div class="mono" style="margin-top:8px">尽管大语言模型展现出卓越能力，但其不可靠性仍是高风险领域部署的关键障碍。本综述描绘了应对这一挑战的功能演进：不确定性从被动的诊断度量演变为指导实时模型行为的主动控制信号。我们展示了不确定性如何作为主动控制信号应用于三大前沿领域：在**高级推理**中优化计算并触发自我修正；在**自主智能体**中管理关于工具使用和信息寻求的元认知决策；在**强化学习**中缓解奖励攻击并通过内在奖励实现自我改进。通过将这些进展根植于贝叶斯方法和保形预测等新兴理论框架，我们为这一变革性趋势提供了统一视角。本综述提供了全面概述、批判性分析和实用设计模式，论证掌握不确定性的新趋势对于构建下一代可扩展、可靠且可信赖的人工智能至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey is motivated by the critical need to enhance the reliability of Large Language Models (LLMs) for high-stakes applications, addressing their inherent unreliability. The method involves analyzing the functional evolution of uncertainty quantification, shifting from a passive diagnostic metric to an active control signal that guides real-time model behavior across three key frontiers: advanced reasoning, autonomous agents, and reinforcement learning, grounded in theoretical frameworks like Bayesian methods and Conformal Prediction. The main experimental results, as synthesized from the literature, demonstrate that leveraging uncertainty as an active signal optimizes computation and self-correction in reasoning, governs metacognitive decisions in agents for tool use and information seeking, and mitigates reward hacking while enabling self-improvement via intrinsic rewards in reinforcement learning, offering a unified perspective for building more reliable AI systems.</div>
<div class="mono" style="margin-top:8px">本综述的动机是，大型语言模型（LLMs）在关键领域部署时存在不可靠性，亟需提升其可靠性以应对高风险应用。方法上，它分析了不确定性量化的功能演变，即从被动的诊断指标转变为主动的控制信号，实时指导模型行为，涵盖三个前沿领域：高级推理、自主智能体和强化学习，并以贝叶斯方法和共形预测等理论框架为基础。主要实验结果综合文献表明，将不确定性作为主动信号利用，可在推理中优化计算并触发自我纠正，在智能体中管理关于工具使用和信息寻求的元认知决策，并在强化学习中减轻奖励黑客攻击并通过内在奖励实现自我改进，为构建更可靠的人工智能系统提供了统一视角。</div>
</details>
</div>
<div class="card">
<div class="title">Performance-guided Reinforced Active Learning for Object Detection</div>
<div class="meta-line">Authors: Zhixuan Liang, Xingyu Zeng, Rui Zhao, Ping Luo</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-22T06:17:08+00:00 · Latest: 2026-01-22T06:17:08+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026. Camera-ready Version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15688v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15688v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data&#x27;s distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL&#x27;s active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向目标检测的性能引导强化主动学习</div>
<div class="mono" style="margin-top:8px">主动学习（AL）策略旨在以最少的标注工作量训练高性能模型，仅选择信息量最大的样本进行标注。当前评估数据信息量的方法主要关注数据分布或内在信息内容，而未与下游任务性能（如目标检测中的平均精度均值mAP）直接关联。为此，我们提出面向目标检测的性能引导（即mAP引导）强化主动学习方法（MGRAL），该方法创新性地将模型预期输出变化作为信息量度量标准。针对批量样本选择的组合爆炸问题，以及模型性能与所选批次间不可微分的关联性，MGRAL巧妙采用基于强化学习的采样代理，以mAP提升作为奖励，通过策略梯度优化选择过程。此外，为降低未标注样本的mAP估计计算开销，MGRAL采用基于快速查找表的无监督方式，确保实际部署可行性。我们在PASCAL VOC和COCO基准检测任务上评估MGRAL的主动学习性能，该方法展现出最优的AL曲线与可信的可视化结果，为强化学习驱动的主动目标检测建立了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of current active learning methods for object detection, which often select data based on distribution or intrinsic information without directly optimizing for task performance like mean average precision (mAP). To overcome this, the authors propose MGRAL, a reinforced active learning approach that uses expected model output changes as informativeness and employs a reinforcement learning agent with policy gradient optimization, where mAP improvement serves as the reward, while leveraging unsupervised fast look-up tables to reduce computational overhead from mAP estimation. Experimental results on PASCAL VOC and COCO benchmarks show that MGRAL achieves the highest active learning curve with convincing visualizations, establishing a new reinforcement learning-driven paradigm for active object detection.</div>
<div class="mono" style="margin-top:8px">本文针对当前目标检测主动学习方法效率不足的问题，这些方法通常基于数据分布或内在信息选择样本，而未直接优化如平均精度均值（mAP）等任务性能。为此，作者提出MGRAL，一种强化主动学习方法，以期望模型输出变化作为信息量度量，采用强化学习代理通过策略梯度优化进行样本选择，其中mAP提升作为奖励，并利用无监督快速查找表降低mAP估计的计算开销。在PASCAL VOC和COCO基准上的实验结果表明，MGRAL实现了最高的主动学习曲线并提供了可信的可视化效果，为强化学习驱动的主动目标检测建立了新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving in Tasks: Empowering the Multi-modality Large Language Model as the Computer Use Agent</div>
<div class="meta-line">Authors: Yuhao Cheng, Liang Tang, Shuxian Li, Yukang Huo, Tiaonan Duan, Kaer Huang, Yanzhe Jing, Yiqiang Yan</div>
<div class="meta-line">First: 2025-08-06T02:57:22+00:00 · Latest: 2026-01-22T05:22:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.04037v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.04037v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computer use agents represent an emerging area in artificial intelligence, aiming to operate computers autonomously to fulfill user tasks, attracting significant attention from both industry and academia. However, the performance of existing agents remains insufficient for practical deployment. In this paper, we propose the Self-Evolution Agent (SEA) for computer operation, alongside three core innovations in data generation, reinforcement learning, and model enhancement to develop this agent. Specifically, we first design an automatic pipeline to generate verifiable task trajectories for training. Second, we propose Efficient Step-wise Reinforcement Learning to reduce the substantial computational overhead of long-horizon training. Finally, we introduce a model enhancement method that integrates grounding and planning capabilities into a single model without additional training. Leveraging these innovations, our SEA (with only 7B parameters) outperforms existing models of the same parameter scale and achieves performance comparable to larger models (e.g., 32B/72B parameters) on computer use tasks. We plan to release the model weights and related code as open-source resources in the future.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>任务演进：赋能多模态大语言模型成为计算机使用智能体</div>
<div class="mono" style="margin-top:8px">计算机使用智能体是人工智能领域的新兴方向，旨在通过自主操作计算机完成用户任务，已引起工业界与学术界的广泛关注。然而现有智能体的性能尚不足以支撑实际部署。本文提出用于计算机操作的自我演进智能体（SEA），并通过数据生成、强化学习和模型增强三个核心创新来构建该智能体。具体而言，我们首先设计了自动生成可验证任务轨迹的训练流水线；其次提出高效分步强化学习方法以降低长周期训练的巨大计算开销；最后引入一种无需额外训练即可将基础操作与规划能力集成至单一模型的增强方法。依托这些创新，我们的SEA（仅70亿参数）在计算机使用任务上超越了同参数规模的现有模型，并达到与更大参数模型（如320亿/720亿参数）相当的性能。我们计划未来将模型权重及相关代码作为开源资源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the insufficient performance of existing computer use agents for practical deployment, this paper introduces the Self-Evolution Agent (SEA) to enhance autonomous computer operation. The method involves three core innovations: an automatic pipeline for generating verifiable task trajectories, Efficient Step-wise Reinforcement Learning to reduce computational overhead in long-horizon training, and a model enhancement technique that integrates grounding and planning capabilities without extra training. Experimentally, the 7B-parameter SEA outperforms same-scale models and achieves performance comparable to much larger models (e.g., 32B/72B parameters) on computer use tasks, with plans to release the model and code as open-source.</div>
<div class="mono" style="margin-top:8px">针对现有计算机使用智能体性能不足、难以实际部署的问题，本文提出了自我进化智能体（SEA）以提升自主操作计算机的能力。方法包含三项核心创新：设计自动流水线生成可验证的任务轨迹用于训练，提出高效逐步强化学习以减少长序列训练的计算开销，以及引入一种无需额外训练即可将基础定位与规划能力集成到单一模型中的增强技术。实验结果表明，仅70亿参数的SEA在计算机使用任务上超越了同参数规模的现有模型，并达到了与更大参数模型（如320亿/720亿参数）相当的性能，未来计划开源模型权重及相关代码。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors</div>
<div class="meta-line">Authors: Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang Wang, Yao Hu, Shaosheng Cao, Kam-Fai Wong</div>
<div class="meta-line">First: 2026-01-22T03:57:35+00:00 · Latest: 2026-01-22T03:57:35+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15625v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#x27;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Fission-GRPO的鲁棒工具使用：学习从执行错误中恢复</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）能有效调用工具，但在多轮执行中仍显脆弱：工具调用出错后，较小模型常退化为重复无效调用，无法解读错误反馈并自我修正。这种脆弱性阻碍了实际可靠部署，因为工具交互过程中执行错误本就不可避免。我们发现现有方法的关键局限：标准强化学习（RL）将错误视为稀疏负奖励，未提供恢复指导；而预收集的合成纠错数据集与模型在策略错误模式存在分布失配。为弥合此差距，我们提出Fission-GRPO框架，将执行错误转化为RL训练循环内的纠正监督。核心机制通过微调的错误模拟器为每个失败轨迹添加诊断反馈，将其裂解为新训练实例，再在策略上重采样恢复轨迹。这使得模型能从探索期间产生的具体错误中学习，而非依赖静态预收集错误案例。在BFCL v4多轮测试中，Fission-GRPO将Qwen3-8B的错误恢复率绝对提升5.7%，关键的是，整体准确率较GRPO提升4%（42.75%至46.75%），并超越专用工具使用智能体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the brittleness of large language models (LLMs) in multi-turn tool use, where they often fail to recover from execution errors, hindering reliable real-world deployment. To address this, the authors propose Fission-GRPO, a method that integrates corrective supervision into reinforcement learning by converting failed trajectories into training instances augmented with diagnostic feedback from a finetuned Error Simulator, enabling on-policy learning from self-generated errors. Experimental results on the BFCL v4 Multi-Turn benchmark show that Fission-GRPO improves the error recovery rate of the Qwen3-8B model by 5.7% and boosts overall accuracy by 4% over baseline methods, outperforming specialized tool-use agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于大型语言模型在多轮工具调用中表现脆弱，难以从执行错误中恢复，这阻碍了其在实际场景中的可靠部署。为此，作者提出了Fission-GRPO方法，该方法通过将失败轨迹转化为训练实例，并利用微调的错误模拟器提供诊断反馈，将纠正性监督整合到强化学习训练中，使模型能够从自身探索产生的错误中学习。在BFCL v4多轮基准测试上的实验结果表明，Fission-GRPO将Qwen3-8B模型的错误恢复率提升了5.7%，整体准确率较基线方法提高了4%，性能优于专门的工具使用智能体。</div>
</details>
</div>
<div class="card">
<div class="title">Your Group-Relative Advantage Is Biased</div>
<div class="meta-line">Authors: Fengkai Yang, Zherui Chen, Xiaohan Wang, Xiaodong Lu, Jiajun Chai, Guojun Yin, Wei Lin, Shuai Ma, Fuzhen Zhuang, Deqing Wang, Yaodong Yang, Jianxin Li, Yikun Ban</div>
<div class="meta-line">First: 2026-01-13T13:03:15+00:00 · Latest: 2026-01-22T03:42:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08521v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08521v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>群体相对优势估计存在偏差</div>
<div class="mono" style="margin-top:8px">基于验证器奖励的强化学习已成为推理任务上对大语言模型进行后训练的常用方法，其中GRPO及其变体等基于群体的方法获得广泛应用。这些方法依赖群体相对优势估计以避免学习判别器，但其理论性质仍缺乏深入理解。本研究揭示了基于群体强化学习的一个根本问题：群体相对优势估计量相对于真实期望优势存在固有偏差。我们首次通过理论分析证明，该方法会系统性地低估困难提示的优势、高估简单提示的优势，导致探索与利用失衡。为解决此问题，我们提出历史感知自适应难度加权方案，该自适应重加权机制基于动态演化的难度锚点与训练状态调整优势估计。在五个数学推理基准上的理论分析与实验均表明，HA-DW在融入GRPO及其变体后能持续提升性能。我们的结果表明，纠正有偏差的优势估计对于实现稳健高效的RLVR训练至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the widespread use of group-relative advantage estimation in methods like GRPO for Reinforcement Learning from Verifier Rewards (RLVR), despite a lack of theoretical understanding of its properties. The authors identify a fundamental bias in this estimator, which systematically underestimates advantages for hard prompts and overestimates them for easy ones, leading to imbalanced exploration and exploitation. To address this, they propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on a dynamic difficulty anchor and training history. Experimental results on five mathematical reasoning benchmarks show that HA-DW consistently improves performance when integrated into GRPO and its variants, highlighting the importance of correcting biased advantage estimation for robust RLVR training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管基于群体的相对优势估计方法（如GRPO）在基于验证器奖励的强化学习中广泛应用，但其理论性质尚不明确。作者发现该估计器存在固有偏差，会系统性地低估困难提示的优势而高估简单提示的优势，导致探索与利用失衡。为解决此问题，他们提出了历史感知自适应难度加权方法，这是一种根据动态难度锚点和训练历史调整优势估计的自适应重加权方案。在五个数学推理基准上的实验结果表明，该方法集成到GRPO及其变体后能持续提升性能，表明纠正有偏的优势估计对于实现稳健高效的强化学习训练至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards</div>
<div class="meta-line">Authors: Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun Zhou</div>
<div class="meta-line">First: 2026-01-22T03:15:57+00:00 · Latest: 2026-01-22T03:15:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15609v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当锐化演变为坍缩：可验证奖励强化学习中的采样偏差与语义耦合</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是将大语言模型转化为可靠问题求解器的核心范式，尤其在逻辑密集型领域表现突出。尽管实证研究已证实其有效性，但RLVR究竟是激发了新能力，还是仅强化了现有知识的分布，仍不明确。本研究通过形式化“过度锐化”现象——即策略坍缩至有限模式并压制有效替代方案的过程——来探讨此问题。研究发现，有限批次更新本质上会使学习偏向已采样模式，进而通过语义耦合引发全局性坍缩。为缓解此问题，我们提出逆成功优势校准（优先处理困难查询）与分布级校准（通过记忆网络实现采样多样化）两种方法。实证评估表明，所提策略能有效提升模型的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether Reinforcement Learning with Verifiable Rewards (RLVR) genuinely enhances large language models&#x27; capabilities or merely refines existing knowledge, identifying a collapse phenomenon termed &#x27;over-sharpening&#x27; where policies converge on limited modes due to sampling bias. The method attributes this to finite-batch updates favoring sampled modes and semantic coupling that spreads collapse globally, proposing inverse-success advantage calibration to prioritize hard queries and distribution-level calibration with a memory network for diversified sampling. Experimental results confirm that these strategies effectively mitigate collapse and improve model generalization.</div>
<div class="mono" style="margin-top:8px">本文研究了基于可验证奖励的强化学习（RLVR）是真正提升大语言模型能力还是仅优化已有知识，发现了一种称为“过度锐化”的崩溃现象，即策略因采样偏差而收敛于有限模式。方法上，该研究将原因归结为有限批次更新偏向采样模式以及语义耦合全局传播崩溃，并提出了逆成功优势校准以优先处理困难查询，以及通过记忆网络进行分布级校准以实现多样化采样。实验结果表明，这些策略能有效缓解崩溃并提升模型的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Report for NSF Workshop on AI for Electronic Design Automation</div>
<div class="meta-line">Authors: Deming Chen, Vijay Ganesh, Weikai Li, Yingyan Celine Lin, Yong Liu, Subhasish Mitra, David Z. Pan, Ruchir Puri, Jason Cong, Yizhou Sun</div>
<div class="meta-line">First: 2026-01-20T23:45:40+00:00 · Latest: 2026-01-22T02:12:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14541v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14541v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ai4eda-workshop.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>美国国家科学基金会人工智能赋能电子设计自动化研讨会报告</div>
<div class="mono" style="margin-top:8px">本报告提炼了2024年12月10日于温哥华NeurIPS 2024同期举办的美国国家科学基金会人工智能赋能电子设计自动化（EDA）研讨会的讨论与建议。研讨会汇聚机器学习与EDA领域的专家，探讨了人工智能技术——涵盖大语言模型（LLMs）、图神经网络（GNNs）、强化学习（RL）、神经符号方法等——如何助力EDA流程并缩短设计周期。会议围绕四大主题展开：（1）面向物理综合与制造设计（DFM）的人工智能，探讨物理制造工艺中的挑战及潜在AI应用；（2）面向高层次与逻辑级综合（HLS/LLS）的人工智能，涉及编译指导插入、程序转换、RTL代码生成等；（3）用于优化与设计的人工智能工具箱，讨论可能应用于EDA任务的前沿AI进展；（4）面向测试与验证的人工智能，包括LLM辅助验证工具、机器学习增强的SAT求解、安全性与可靠性挑战等。报告建议美国国家科学基金会应促进AI/EDA跨领域合作，投资EDA基础性AI研究，建设稳健的数据基础设施，推广可扩展的计算资源，并加强人才培养以普及硬件设计能力、赋能下一代硬件系统。研讨会详情可通过网站 https://ai4eda-workshop.github.io/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This report summarizes the NSF Workshop on AI for Electronic Design Automation (EDA), motivated by the need to leverage AI technologies like large language models, graph neural networks, and reinforcement learning to accelerate electronic design processes and reduce turnaround time. The method involved expert discussions organized around four key themes: AI for physical synthesis and manufacturing, AI for high-level and logic synthesis, AI toolboxes for optimization, and AI for test and verification. The main experimental results are not applicable as this is a workshop report, but the outcomes include concrete recommendations for NSF to foster AI/EDA collaboration, invest in foundational AI research, develop data and compute infrastructures, and support workforce development to democratize hardware design and enable next-generation systems.</div>
<div class="mono" style="margin-top:8px">本报告总结了美国国家科学基金会（NSF）关于人工智能用于电子设计自动化（EDA）的研讨会，其动机在于利用大语言模型、图神经网络和强化学习等人工智能技术来加速电子设计流程并缩短周转时间。方法基于专家讨论，围绕四个主题展开：用于物理合成和制造的人工智能、用于高层次和逻辑合成的人工智能、用于优化的AI工具箱，以及用于测试和验证的人工智能。由于这是一份研讨会报告，未涉及具体实验成果，但主要成果包括向NSF提出的具体建议，如促进AI/EDA合作、投资EDA基础AI研究、开发稳健的数据和可扩展的计算基础设施，并投资人才培养，以普及硬件设计并推动下一代硬件系统的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation</div>
<div class="meta-line">Authors: Yushen Liu, Yanfu Zhang, Xugui Zhou</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2025-11-16T02:11:33+00:00 · Latest: 2026-01-21T23:25:37+00:00</div>
<div class="meta-line">Comments: 5 pages, 3 figures, ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12417v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12417v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>融合神经微分预测与安全强化学习的血糖调控方法</div>
<div class="mono" style="margin-top:8px">针对1型糖尿病的自动胰岛素输送系统需在不确定的饮食和生理波动下平衡血糖控制与安全性。虽然强化学习（RL）能实现自适应个性化调控，但现有方法难以同时保障安全性，导致在实现个性化与风险感知的血糖控制（如餐前过量注射或叠加校正）方面存在不足。为填补这一空白，我们提出TSODE——一种集成汤普森采样强化学习（Thompson Sampling RL）与神经常微分方程（NeuralODE）预测器的安全感知控制器。具体而言，NeuralODE根据拟定的胰岛素剂量预测短期血糖轨迹，而保形校准层通过量化预测不确定性来拒绝或调整风险动作。在FDA认证的UVa/Padova模拟器（成人队列）中，TSODE实现了87.9%的血糖在目标范围内时间占比，且低于70 mg/dL的时间占比小于10%，性能优于相关基线方法。结果表明，将自适应强化学习经过校准的NeuralODE预测相结合，可实现可解释、安全且稳健的血糖调控。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for automated insulin delivery systems that balance personalized glucose control with safety guarantees under meal and physiological uncertainties, this paper proposes TSODE, a safety-aware controller integrating Thompson Sampling reinforcement learning with a NeuralODE forecaster. The method uses the NeuralODE to predict short-term glucose trajectories based on proposed insulin doses and employs a conformal calibration layer to quantify predictive uncertainty, allowing it to reject or scale risky actions to prevent issues like overdosing. Experimental results on the FDA-approved UVa/Padova simulator with an adult cohort show that TSODE achieved 87.9% time-in-range and less than 10% time below 70 mg/dL, outperforming baselines and demonstrating robust, interpretable, and safe glucose regulation.</div>
<div class="mono" style="margin-top:8px">针对1型糖尿病自动胰岛素输送系统需在不确定的饮食和生理变化下平衡血糖控制与安全性的问题，本文提出了TSODE这一安全感知控制器，它结合了汤普森采样强化学习和神经常微分方程预测器。该方法利用神经常微分方程根据提议的胰岛素剂量预测短期血糖轨迹，并通过共形校准层量化预测不确定性，以拒绝或调整风险动作，避免餐前过量或叠加校正等问题。在FDA批准的UVa/Padova模拟器（成人队列）上的实验结果表明，TSODE实现了87.9%的时间在目标范围内且低于70 mg/dL的时间少于10%，优于相关基线，证明了这种自适应强化学习结合校准预测的方法能实现可解释、安全且稳健的血糖调节。</div>
</details>
</div>
<div class="card">
<div class="title">Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter</div>
<div class="meta-line">Authors: Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han</div>
<div class="meta-line">First: 2025-11-20T18:59:25+00:00 · Latest: 2026-01-21T23:19:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16665v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16665v2">PDF</a> · <a href="https://github.com/mit-han-lab/fastrl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>驯服长尾：基于自适应草稿器的高效推理强化学习训练</div>
<div class="mono" style="margin-top:8px">具备强大推理能力的大语言模型（LLMs）的出现标志着重要里程碑，为复杂问题求解开辟了新前沿。然而，通常使用强化学习（RL）训练这些推理模型时，面临关键效率瓶颈：RL训练中的响应生成呈现持续的长尾分布，少数极长响应主导执行时间，造成资源浪费与成本激增。为此，我们提出TLT系统，通过集成自适应推测解码无损加速推理RL训练。在RL中应用推测解码面临动态工作负载、持续演进的目标模型以及草稿模型训练开销等挑战。TLT通过两个协同组件克服这些障碍：（1）自适应草稿器——在长尾生成期间利用空闲GPU持续训练的轻量级草稿模型，无需额外成本即可保持与目标模型对齐；（2）自适应执行引擎——维护内存高效预捕获CUDAGraph池，并为每个输入批次自适应选择合适推测解码策略。评估表明，TLT相比前沿系统实现超过1.7倍端到端RL训练加速，保持模型精度，并免费获得适用于高效部署的高质量草稿模型副产品。代码发布于https://github.com/mit-han-lab/fastrl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the efficiency bottleneck in training large language models for reasoning tasks via reinforcement learning, where the long-tail distribution of response lengths leads to wasted computational resources and high costs. The proposed method, TLT, introduces an adaptive speculative decoding system comprising two components: an Adaptive Drafter that trains a lightweight draft model on idle GPUs during long-tail generation to stay aligned with the target model, and an Adaptive Rollout Engine that manages a pool of CUDAGraphs and selects speculative decoding strategies per batch. Experimental results show that TLT achieves over 1.7x end-to-end training speedup compared to state-of-the-art systems while maintaining model accuracy and producing a high-quality draft model as a deployable byproduct.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决通过强化学习训练大型语言模型进行推理任务时的效率瓶颈，其中响应长度的长尾分布会导致计算资源浪费和成本高昂。所提出的方法TLT引入了一种自适应推测解码系统，包含两个组件：自适应草稿模型在长尾生成期间利用空闲GPU持续训练轻量级草稿模型以保持与目标模型对齐，以及自适应执行引擎管理CUDAGraphs池并为每个批次选择推测解码策略。实验结果表明，TLT相比最先进系统实现了超过1.7倍的端到端训练加速，同时保持了模型准确性，并产生了一个可部署的高质量草稿模型作为副产品。</div>
</details>
</div>
<div class="card">
<div class="title">On the Exponential Convergence for Offline RLHF with Pairwise Comparisons</div>
<div class="meta-line">Authors: Zhirui Chen, Vincent Y. F. Tan</div>
<div class="meta-line">Venue: AAAI 2026 oral presentation</div>
<div class="meta-line">First: 2024-06-18T02:03:12+00:00 · Latest: 2026-01-21T21:33:45+00:00</div>
<div class="meta-line">Comments: Accepted as an oral presentation at AAAI 2026 (AI Alignment Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.12205v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.12205v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of offline reinforcement learning from human feedback (RLHF) with pairwise comparisons proposed by Zhu et al. (2023), where the implicit reward is a linear function of an unknown parameter. Given an offline dataset, our objective consists in ascertaining the optimal action for each state, with the ultimate goal of minimizing the {\em simple regret}. We propose an algorithm, \underline{RL} with \underline{L}ocally \underline{O}ptimal \underline{W}eights or {\sc RL-LOW}, which yields an exponential form of simple regret of $\exp ( - Ω(n/H) )$ where $n$ is the number of data samples and $H$ denotes an instance-dependent hardness quantity that depends explicitly on the suboptimality gap of each action. Furthermore, we derive a first-of-its-kind instance-dependent lower bound in offline RLHF with pairwise comparisons. Interestingly, we observe that the lower and upper bounds on the simple regret match order-wise in the exponent, demonstrating order-wise optimality of our {\sc RL-LOW}. In view of privacy considerations in practical applications, we also extend {\sc RL-LOW} to the setting of $(\varepsilon,δ)$-differential privacy and show, somewhat surprisingly, that the hardness parameter $H$ is unchanged in the asymptotic regime as $n$ tends to infinity; this underscores the inherent efficiency of {\sc RL-LOW} in terms of preserving the privacy of the observed rewards. Given our focus on establishing instance-dependent bounds of exponential convergence, our research fills the research gap in existing studies that concentrate on establishing worst-case regrets of {\em inverse polynomial convergence} (e.g., $\widetilde{O}(\frac{1}{\sqrt{n}})$) for offline RLHF with pairwise comparisons.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于成对比较的离线RLHF指数收敛性研究</div>
<div class="mono" style="margin-top:8px">本文研究Zhu等人（2023）提出的基于成对比较的离线人类反馈强化学习（RLHF）问题，其中隐式奖励是未知参数的线性函数。给定离线数据集，我们的目标是为每个状态确定最优动作，最终最小化简单遗憾。我们提出一种算法——局部最优权重强化学习（RL-LOW），该算法能产生指数形式的简单遗憾$\exp ( - Ω(n/H) )$，其中$n$为数据样本数，$H$是依赖于实例的硬度量，显式取决于各动作的次优间隙。此外，我们首次推导出成对比较离线RLHF的实例相关下界。有趣的是，我们观察到简单遗憾的上下界在指数阶上匹配，证明了RL-LOW的阶数最优性。考虑到实际应用中的隐私问题，我们还将RL-LOW扩展至$(\varepsilon,δ)$-差分隐私设置，并出人意料地发现当$n$趋于无穷时，硬度参数$H$在渐近状态下保持不变；这凸显了RL-LOW在保护观测奖励隐私方面的内在效率。由于我们专注于建立指数收敛的实例相关界，本研究填补了现有文献的空白——现有研究主要关注建立成对比较离线RLHF的逆多项式收敛最坏情况遗憾（例如$\widetilde{O}(\frac{1}{\sqrt{n}})$）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses offline reinforcement learning from human feedback (RLHF) with pairwise comparisons, motivated by the need to move beyond worst-case inverse polynomial regret bounds and achieve faster, instance-dependent convergence. The authors propose an algorithm called RL-LOW, which leverages locally optimal weights to estimate an implicit linear reward function from offline data, aiming to minimize simple regret by identifying the optimal action per state. The main experimental results demonstrate that RL-LOW achieves an exponential simple regret bound of exp(-Ω(n/H)), where n is the sample size and H is an instance-dependent hardness measure, and they prove a matching lower bound to establish order-wise optimality; additionally, the algorithm is extended to differentially private settings with preserved efficiency.</div>
<div class="mono" style="margin-top:8px">本文研究基于成对比较的离线人类反馈强化学习（RLHF），其动机是超越现有的最坏情况逆多项式遗憾界，实现更快的、依赖于实例的收敛。作者提出了一种名为RL-LOW的算法，该方法利用局部最优权重从离线数据中估计隐式线性奖励函数，旨在通过为每个状态确定最优动作来最小化简单遗憾。主要实验结果表明，RL-LOW实现了指数级简单遗憾界exp(-Ω(n/H))，其中n为样本数量，H是依赖于实例的难度度量，并通过证明匹配的下界确立了算法的阶次最优性；此外，该算法被扩展到差分隐私设置中，且保持了效率不变。</div>
</details>
</div>
<div class="card">
<div class="title">Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias</div>
<div class="meta-line">Authors: Xia Chen</div>
<div class="meta-line">First: 2025-12-30T00:34:24+00:00 · Latest: 2026-01-21T20:37:33+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23916v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23916v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional deep learning prioritizes unconstrained optimization, yet biological systems operate under strict metabolic constraints. We propose that these physical constraints shape dynamics to function not as limitations, but as a temporal inductive bias that breeds generalization. Through a phase-space analysis of signal propagation, we reveal a fundamental asymmetry: expansive dynamics amplify noise, whereas proper dissipative dynamics compress phase space that aligns with the network&#x27;s spectral bias, compelling the abstraction of invariant features. This condition can be imposed externally via input encoding, or intrinsically through the network&#x27;s own temporal dynamics. Both pathways require architectures capable of temporal integration and proper constraints to decode induced invariants, whereas static architectures fail to capitalize on temporal structure. Through comprehensive evaluations across supervised classification, unsupervised reconstruction, and zero-shot reinforcement learning, we demonstrate that a critical &quot;transition&quot; regime maximizes generalization capability. These findings establish dynamical constraints as a distinct class of inductive bias, suggesting that robust AI development requires not only scaling and removing limitations, but computationally mastering the temporal characteristics that naturally promote generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束催生泛化：作为归纳偏置的时序动态机制</div>
<div class="mono" style="margin-top:8px">传统深度学习优先考虑无约束优化，而生物系统却在严格的代谢约束下运行。我们提出，这些物理约束塑造的动态机制并非限制，而是作为一种时序归纳偏置促进泛化能力。通过对信号传播的相空间分析，我们揭示了一个基本不对称性：扩张性动态会放大噪声，而恰当的耗散性动态则会压缩相空间，使其与网络的光谱偏置对齐，从而强制提取不变特征。这种条件可通过外部输入编码施加，或由网络自身的时序动态内在实现。两种途径都需要具备时序整合能力及适当约束的架构来解码诱导不变性，而静态架构则无法利用时序结构。通过在监督分类、无监督重建和零样本强化学习等任务上的综合评估，我们证明了一个关键的“过渡”机制能最大化泛化能力。这些发现确立了动态约束作为一类独特的归纳偏置，表明稳健的AI发展不仅需要扩大规模和消除限制，更需在计算层面掌握那些天然促进泛化的时序特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that biological systems operate under strict metabolic constraints unlike unconstrained deep learning optimization, this paper proposes that such physical constraints act as a temporal inductive bias to enhance generalization. The method involves a phase-space analysis of signal propagation, revealing that proper dissipative dynamics compress phase space and align with spectral bias to abstract invariant features, achievable through external input encoding or intrinsic network dynamics requiring temporal integration. Experimental results across supervised classification, unsupervised reconstruction, and zero-shot reinforcement learning show that a critical transition regime maximizes generalization, establishing dynamical constraints as a distinct inductive bias for robust AI development.</div>
<div class="mono" style="margin-top:8px">本文的动机源于生物系统在严格代谢约束下运行，而传统深度学习优先考虑无约束优化，提出物理约束可作为时间归纳偏置以提升泛化能力。方法上通过信号传播的相空间分析，揭示适当的耗散动力学能压缩相空间并与频谱偏置对齐以抽象不变特征，可通过外部输入编码或需要时间积分的内部网络动态实现。在监督分类、无监督重建和零样本强化学习的综合评估中，实验结果表明临界“过渡”机制能最大化泛化能力，从而确立动力学约束为一类独特的归纳偏置，暗示稳健AI开发需掌握促进泛化的时间特性。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC</div>
<div class="meta-line">Authors: Ashna Nawar Ahmed, Banooqa Banday, Terry Jones, Tanzima Z. Islam</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-21T19:11:12+00:00 · Latest: 2026-01-21T19:11:12+00:00</div>
<div class="meta-line">Comments: 13 pages, 6 figures Published in MLForSys workshop in NeurIPS 2025 Link: https://openreview.net/forum?id=R0Vc9lnDd5</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15399v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15399v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力感知的代理模型用于导航高性能计算中的功耗-性能权衡</div>
<div class="mono" style="margin-top:8px">高性能计算调度器必须在用户性能与设施级资源约束间取得平衡，其核心任务是为给定作业选择最优节点数量。本研究提出一种代理辅助的多目标贝叶斯优化框架，以自动化这一复杂决策。核心假设是：基于作业遥测注意力嵌入构建的代理模型，能比标准回归技术更有效地捕捉性能动态特征。结合智能样本采集策略，该方法实现了数据高效性。在两个生产级HPC数据集上的实验表明，相较于基线方法，本嵌入感知方法始终能生成更优质的运行时-功耗帕累托前沿。智能数据采样策略在提升结果稳定性的同时，大幅降低了训练成本。据我们所知，这是首次成功将嵌入感知代理模型应用于MOBO框架以解决HPC调度问题，实现了生产负载下性能与功耗的联合优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for HPC schedulers to balance job performance with facility-wide power constraints, this paper introduces a surrogate-assisted multi-objective Bayesian optimization framework to automate the selection of optimal node counts for jobs. The method employs attention-based embeddings of job telemetry to inform surrogate models, hypothesizing they capture performance dynamics more effectively than standard regression, and pairs this with an intelligent data acquisition strategy for efficiency. Experimental results on two production HPC datasets show that the embedding-informed method consistently identifies higher-quality Pareto fronts for runtime-power trade-offs compared to baselines, while the sampling strategy reduces training costs and improves result stability.</div>
<div class="mono" style="margin-top:8px">本文的动机是高性能计算调度器需要在作业性能与设施整体功耗约束之间取得平衡，为此提出了一个基于代理模型的多目标贝叶斯优化框架，以自动化地为作业选择最优节点数量。该方法利用基于注意力的作业遥测嵌入来构建代理模型，假设其比标准回归技术更能有效捕捉性能动态，并结合智能数据采样策略以提高效率。在两个生产级HPC数据集上的实验结果表明，与基线方法相比，该嵌入感知方法能持续识别出更优的运行时-功耗权衡帕累托前沿，同时其采样策略显著降低了训练成本并提升了结果稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">QueStER: Query Specification for Generative keyword-based Retrieval</div>
<div class="meta-line">Authors: Arthur Satouf, Yuxuan Zong, Habiboulaye Amadou-Boubacar, Pablo Piantanida, Benjamin Piwowarski</div>
<div class="meta-line">Venue: eACL 2026</div>
<div class="meta-line">First: 2025-11-07T15:01:38+00:00 · Latest: 2026-01-21T17:37:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05301v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05301v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and generating retrieval cues directly from the query, but it can be brittle out of domain and expensive to scale. We introduce QueStER (QUEry SpecificaTion for gEnerative Keyword-Based Retrieval), which bridges GR and query reformulation by learning to generate explicit keyword-based search specifications. Given a user query, a lightweight LLM produces a keyword query that is executed by a standard retriever (BM25), combining the generalization benefits of generative query rewriting with the efficiency and scalability of lexical indexing. We train the rewriting policy with reinforcement learning techniques. Across in- and out-of-domain evaluations, QueStER consistently improves over BM25 and is competitive with neural IR baselines, while maintaining strong efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QueStER：基于生成式关键词检索的查询规范方法</div>
<div class="mono" style="margin-top:8px">生成式检索（GR）通过将相关性存储在模型参数中并直接从查询生成检索线索，与传统“先索引后检索”流程不同，但其存在跨领域脆弱性和扩展成本高的问题。本文提出QueStER（基于生成式关键词检索的查询规范方法），通过学习生成显式的基于关键词的搜索规范，桥接生成式检索与查询重构。给定用户查询时，轻量级大语言模型生成可由标准检索器（BM25）执行的关键词查询，从而融合生成式查询重写的泛化优势与词法索引的效率和可扩展性。我们采用强化学习技术训练重写策略。在领域内外的评估中，QueStER持续优于BM25，并与神经信息检索基线保持竞争力，同时维持高效性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the brittleness and scalability challenges of generative retrieval by introducing QueStER, a method that bridges generative retrieval and query reformulation. The approach uses a lightweight LLM to generate explicit keyword-based search specifications from user queries, which are then executed by a standard BM25 retriever, combining the generalization of generative query rewriting with the efficiency of lexical indexing. Experimental results across in- and out-of-domain evaluations show that QueStER consistently improves over BM25 and is competitive with neural IR baselines while maintaining strong efficiency.</div>
<div class="mono" style="margin-top:8px">该论文针对生成式检索的脆弱性和可扩展性挑战，提出了QueStER方法，以桥接生成式检索与查询重写。该方法使用轻量级大语言模型从用户查询生成显式的基于关键词的搜索规范，然后通过标准BM25检索器执行，结合了生成式查询重写的泛化能力和词法索引的效率。在领域内和领域外的评估实验中，QueStER一致性地超越了BM25，并与神经信息检索基线方法竞争，同时保持了高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Finding Kissing Numbers with Game-theoretic Reinforcement Learning</div>
<div class="meta-line">Authors: Chengdong Ma, Théo Tao Zhaowei, Pengyu Li, Minghao Liu, Haojun Chen, Zihao Mao, Yuan Cheng, Yuan Qi, Yaodong Yang</div>
<div class="meta-line">First: 2025-11-17T14:02:00+00:00 · Latest: 2026-01-21T16:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13391v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13391v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert&#x27;s 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game that can be fully parallelized at large scale, and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions, discovers over 6000 new structures in 14 and other dimensions, and establishes new records for generalized kissing configurations under various angular constraints. These results demonstrate AI&#x27;s power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于博弈论强化学习的接吻数求解</div>
<div class="mono" style="margin-top:8px">自艾萨克·牛顿于1694年首次研究接吻数问题以来，确定中心球体周围非重叠球体的最大数量始终是基础性难题。该问题是希尔伯特第18个球堆积问题的局部类比，连接了几何、数论与信息论。尽管通过格与编码已取得重要进展，但8维以上高维几何的不规则性与指数级增长的组合复杂度（超过围棋复杂度），限制了现有方法的可扩展性。本研究将该问题建模为可大规模并行化的双玩家矩阵补全博弈，并训练博弈论强化学习系统PackingStar以高效探索高维空间。矩阵元素表示球心向量的成对余弦值；一方填充元素，另一方修正次优项，共同最大化矩阵尺寸以对应接吻数。这种协作机制显著提升样本质量，使极大空间可处理。PackingStar复现了已有构型，并在25至31维超越所有人已知记录——其中25维构型几何对应Leech格并暗示潜在最优性。系统首次在13维突破1971年以来的有理结构局限，在14维及其他维度发现超6000种新结构，并在多种角度约束下建立了广义接吻构型的新记录。这些成果证明人工智能具备超越人类直觉探索高维空间的能力，为接吻数问题及更广泛几何问题开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the long-standing challenge of determining the maximum number of non-overlapping spheres that can touch a central sphere, a problem with deep connections to geometry and information theory, this paper introduces a novel method that frames the kissing number problem as a two-player matrix completion game. The approach, named PackingStar, employs game-theoretic reinforcement learning where one player fills matrix entries representing pairwise cosines of sphere center vectors and another corrects suboptimal ones, enabling efficient, parallel exploration of high-dimensional spaces. Experimentally, PackingStar not only reproduces known configurations but also surpasses all previous human records in dimensions 25 to 31, achieves the first breakthrough beyond 1971 results in dimension 13, discovers thousands of new structures in dimension 14 and others, and sets new records for generalized kissing configurations, demonstrating AI&#x27;s capability to navigate complex high-dimensional geometries beyond human intuition.</div>
<div class="mono" style="margin-top:8px">本文的研究动机源于确定一个中心球体周围可容纳非重叠球体的最大数量这一长期挑战，该问题与几何和信息理论有深刻联系。方法上，作者将亲吻数问题建模为一个双玩家矩阵补全游戏，开发了名为PackingStar的游戏理论强化学习系统，其中一个玩家填充表示球心向量对之间余弦的矩阵条目，另一玩家修正次优条目，从而实现对高维空间的高效并行探索。主要实验结果表明，PackingStar不仅复现了已知构型，还在25至31维中超越了所有已知人类记录，在13维取得了自1971年以来的首次突破，在14维等维度发现了超过6000个新结构，并为广义亲吻构型建立了新记录，证明了人工智能在探索超越人类直觉的高维复杂几何空间方面的强大能力。</div>
</details>
</div>
<div class="card">
<div class="title">The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</div>
<div class="meta-line">Authors: Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</div>
<div class="meta-line">First: 2026-01-21T16:41:58+00:00 · Latest: 2026-01-21T16:41:58+00:00</div>
<div class="meta-line">Comments: Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15165v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15165v1">PDF</a> · <a href="https://github.com/LeapLabTHU/JustGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://nzl-thu.github.io/the-flexibility-trap">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵活性陷阱：为何任意顺序生成会限制扩散语言模型的推理潜力</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）突破了传统LLMs严格的从左到右约束，实现了按任意顺序生成词元。直观上，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上为数学和编程等通用任务解锁了更优的推理潜力。因此，许多研究采用强化学习（RL）来激发dLLMs的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但没有扩展dLLMs的推理边界，反而使其收窄。我们发现dLLMs倾向于利用这种顺序灵活性来规避对探索至关重要的高不确定性词元，导致解空间过早坍缩。这一观察挑战了现有dLLMs强化学习方法的前提——这些方法往往投入大量复杂度（如处理组合轨迹和难解似然）来维持这种灵活性。我们证明，通过主动放弃任意顺序生成并改用标准组相对策略优化（GRPO），能更有效地激发推理能力。我们的方法JustGRPO极简却出奇有效（例如在GSM8K上达到89.1%准确率），同时完全保留了dLLMs的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the assumption that arbitrary token generation order in diffusion language models inherently enhances reasoning, showing it instead leads to a &#x27;flexibility trap&#x27; where models avoid high-uncertainty tokens, prematurely collapsing the solution space. The authors propose JustGRPO, a minimalist method that intentionally forgoes arbitrary order and applies standard Group Relative Policy Optimization, which effectively elicits reasoning while retaining parallel decoding. Key experimental results include achieving 89.1% accuracy on the GSM8K benchmark, demonstrating superior performance without the complexities of existing reinforcement learning approaches that preserve order flexibility.</div>
<div class="mono" style="margin-top:8px">本文挑战了扩散语言模型中任意令牌生成顺序能增强推理能力的假设，揭示其反而导致“灵活性陷阱”，即模型回避高不确定性令牌，使解空间过早坍缩。作者提出JustGRPO方法，有意放弃任意顺序，采用标准组相对策略优化，在保持并行解码的同时有效激发推理能力。主要实验结果包括在GSM8K基准上达到89.1%的准确率，表明无需保留顺序灵活性的复杂强化学习方法即可实现更优性能。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning</div>
<div class="meta-line">Authors: Yuval Kansal, Niraj K. Jha</div>
<div class="meta-line">First: 2026-01-21T16:38:59+00:00 · Latest: 2026-01-21T16:38:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &quot;compositional bridge&quot;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识图谱作为隐式奖励模型：路径衍生信号赋能组合推理</div>
<div class="mono" style="margin-top:8px">大型语言模型在数学和编程等结构化推理领域已接近专家水平，但在专业科学领域进行组合式多跳推理的能力仍有限。我们提出一种自底向上的学习范式，使模型基于公理化领域事实进行组合以解决复杂未知任务。为此，我们提出一种结合监督微调与强化学习的后训练流程，其中知识图谱充当隐式奖励模型。通过从知识图谱路径中衍生新颖的奖励信号，我们提供可验证、可扩展且基于事实的监督，促使模型在强化学习中组合中间公理而非仅优化最终答案。我们在医学领域验证该方法，使用短跳推理路径（1-3跳）训练140亿参数模型，并评估其对复杂多跳查询（4-5跳）的零样本泛化能力。实验表明，路径衍生奖励作为“组合桥梁”，使我们的模型在最困难推理任务上显著超越GPT-5.2和Gemini 3 Pro等更大规模前沿系统。此外，我们验证了该方法在选项重排压力测试中对对抗性干扰的鲁棒性。这项工作表明，将推理过程锚定于结构化知识是实现智能推理的可扩展高效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitation of large language models in performing compositional multi-hop reasoning within specialized scientific fields, despite their strong performance in structured domains like mathematics. The method introduces a bottom-up learning paradigm where models are grounded in axiomatic domain facts and composed to solve complex tasks, using a post-training pipeline that combines supervised fine-tuning and reinforcement learning with knowledge graphs serving as implicit reward models. The main experimental results, validated in the medical domain with a 14B model trained on short-hop reasoning paths, show that path-derived rewards enable significant zero-shot generalization to complex multi-hop queries, outperforming larger models and frontier systems like GPT-5.2 and Gemini 3 Pro on difficult reasoning tasks, while also demonstrating robustness to adversarial perturbations in option-shuffling stress tests.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大语言模型在专业科学领域中进行组合式多跳推理的局限性，尽管它们在数学等结构化领域表现优异。方法上提出了一种自底向上的学习范式，将模型基于公理化的领域事实进行组合以解决复杂任务，采用结合监督微调和强化学习的后训练流程，其中知识图谱作为隐式奖励模型。主要实验结果在医学领域验证，使用一个140亿参数的模型在短跳推理路径上训练，表明路径衍生的奖励信号能显著提升模型对复杂多跳查询的零样本泛化能力，在困难推理任务上超越了更大模型及前沿系统如GPT-5.2和Gemini 3 Pro，同时在对选项重排压力测试的对抗性扰动中展现出鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</div>
<div class="meta-line">Authors: Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-05-29T04:09:19+00:00 · Latest: 2026-01-21T16:37:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23062v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.23062v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating pre-collected offline data can substantially improve the sample efficiency of reinforcement learning (RL), but its benefits can break down when the transition dynamics in the offline dataset differ from those encountered online. Existing approaches typically mitigate this issue by penalizing or filtering offline transitions in regions with large dynamics gap. However, their dynamics-gap estimators often rely on KL divergence or mutual information, which can be ill-defined when offline and online dynamics have mismatched support. To address this challenge, we propose CompFlow, a principled framework built on the theoretical connection between flow matching and optimal transport. Specifically, we model the online dynamics as a conditional flow built upon the output distribution of a pretrained offline flow, rather than learning it directly from a Gaussian prior. This composite structure provides two advantages: (1) improved generalization when learning online dynamics under limited interaction data, and (2) a well-defined and stable estimate of the dynamics gap via the Wasserstein distance between offline and online transitions. Building on this dynamics-gap estimator, we further develop an optimistic active data collection strategy that prioritizes exploration in high-gap regions, and show theoretically that it reduces the performance gap to the optimal policy. Empirically, CompFlow consistently outperforms strong baselines across a range of RL benchmarks with shifted-dynamics data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向动态偏移数据强化学习的复合流匹配方法</div>
<div class="mono" style="margin-top:8px">利用预收集的离线数据可显著提升强化学习的样本效率，但当离线数据集中的状态转移动态与在线环境存在差异时，其优势可能失效。现有方法通常通过对动态差异较大区域的离线转移施加惩罚或过滤来缓解此问题，但其动态差异估计器常依赖KL散度或互信息，在离线与在线动态支持集不匹配时可能失效。为此，我们提出CompFlow——一个基于流匹配与最优传输理论联系构建的原则性框架。具体而言，我们将在线动态建模为基于预训练离线流输出分布的条件流，而非直接从高斯先验学习。这种复合结构具有双重优势：(1) 在有限交互数据下学习在线动态时提升泛化能力；(2) 通过离线与在线转移的Wasserstein距离获得定义良好且稳定的动态差异估计。基于此估计器，我们进一步开发了乐观主动数据收集策略，优先探索高差异区域，并从理论上证明其能缩小与最优策略的性能差距。实验表明，在多个含动态偏移数据的强化学习基准测试中，CompFlow均稳定优于现有强基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve reinforcement learning sample efficiency using offline data, which is challenging when the offline and online transition dynamics differ, especially with mismatched support where existing dynamics-gap estimators like KL divergence fail. The method, CompFlow, introduces a composite flow matching framework that models online dynamics as a conditional flow built upon a pretrained offline flow, leveraging optimal transport theory to provide a stable Wasserstein distance estimate of the dynamics gap. Experimental results across RL benchmarks with shifted dynamics show that CompFlow consistently outperforms strong baselines, supported by an optimistic active data collection strategy that theoretically reduces the performance gap to the optimal policy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是利用离线数据提升强化学习的样本效率，但当离线与在线转移动态不同时，尤其存在支持不匹配的情况下，现有基于KL散度等动态差距估计方法可能失效。所提出的CompFlow方法构建了一个复合流匹配框架，将在线动态建模为基于预训练离线流的条件流，利用最优传输理论提供稳定的Wasserstein距离来估计动态差距。在动态偏移的强化学习基准测试中，实验结果表明CompFlow持续优于强基线方法，其采用的乐观主动数据收集策略理论上能缩小与最优策略的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-01-21T16:36:19+00:00</div>
<div class="meta-line">Comments: 80 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动梯度下降发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的梯度流动力学来探讨此问题——该任务必须依赖思维链才能解决，但存在简单的迭代解法。我们证明：尽管仅通过最终答案正确性进行训练，梯度流仍会驱动模型收敛至一种结构化、可解释的逐顶点迭代遍历算法。我们刻画了该现象涌现所需的分布特性，指出“简单示例”（需要较少推理步骤的实例）的关键作用：当训练分布中此类简单实例具有足够权重时，模型将学会可泛化至更长链的遍历策略；若此类权重消失，基于梯度的学习将无法实现。我们通过合成数据实验及真实世界语言模型的数学推理任务验证了理论结论在实际场景中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how transformers trained with outcome-based reinforcement learning can develop chain-of-thought reasoning, motivated by the need to understand the mechanisms behind this emergent capability. The method involves a theoretical analysis of gradient flow dynamics in single-layer transformers on a synthetic graph traversal task that necessitates intermediate reasoning steps. The main experimental results prove that, given sufficient training mass on &#x27;simple examples&#x27; requiring fewer steps, gradient descent converges to an interpretable, iterative algorithm that generalizes to longer chains, a finding corroborated by experiments on both synthetic data and real-world language models.</div>
<div class="mono" style="margin-top:8px">本文研究了基于结果的强化学习训练出的Transformer如何发展出思维链推理能力，其动机在于理解这种涌现能力背后的机制。方法包括对单层Transformer在需要中间推理步骤的合成图遍历任务上的梯度流动态进行理论分析。主要实验结果证明，当训练数据中足够集中于需要较少步骤的&#x27;简单示例&#x27;时，梯度下降会收敛到一个可解释的、可泛化至更长链的迭代算法，这一发现在合成数据和真实世界语言模型的实验中都得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Tianshi Xu, Yuteng Chen, Meng Li</div>
<div class="meta-line">First: 2026-01-21T16:14:30+00:00 · Latest: 2026-01-21T16:14:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15141v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model&#x27;s intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLEANER：自净化轨迹提升智能体强化学习</div>
<div class="mono" style="margin-top:8px">智能体强化学习（RL）使大语言模型（LLM）能够利用Python解释器等工具解决复杂问题。然而，对于参数受限模型（如4B-7B规模），探索阶段常因频繁执行失败而产生噪声轨迹，阻碍策略优化。在标准结果奖励机制下，这种噪声会导致严重的信用分配问题——错误动作与成功结果被同时强化。现有改进方法面临两难：密集奖励易引发奖励破解，而过采样则带来高昂计算成本。为此，我们提出CLEANER框架。区别于外部过滤方法，CLEANER利用模型内在自校正能力，直接在数据收集阶段消除错误污染。其核心相似性感知自适应回滚（SAAR）机制通过追溯性替换失败步骤为成功自校正，自主构建纯净轨迹。SAAR基于语义相似度自适应调节替换粒度，从浅层执行修复到深层推理替换。通过在这些自净化路径上训练，模型内化了正确推理模式而非错误恢复循环。在AIME24/25、GPQA和LiveCodeBench的实验中，平均准确率较基线分别提升6%、3%和5%。值得注意的是，CLEANER仅用三分之一训练步数即达到最优性能，证明轨迹净化是可扩展的高效智能体RL解决方案。模型与代码已在GitHub开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of noisy exploration trajectories in agentic reinforcement learning for parameter-constrained LLMs, where execution failures during tool use create credit assignment issues that hinder policy optimization. The proposed method, CLEANER, introduces a Similarity-Aware Adaptive Rollback mechanism that leverages the model&#x27;s intrinsic self-correction capability to autonomously purify trajectories by retrospectively replacing failed actions with successful self-corrections based on semantic similarity, thereby enabling training on clean reasoning patterns. Experimental results on benchmarks including AIME24/25, GPQA, and LiveCodeBench demonstrate average accuracy improvements of 6%, 3%, and 5% over baselines, with CLEANER achieving state-of-the-art performance using only one-third of the training steps, highlighting its efficiency and scalability.</div>
<div class="mono" style="margin-top:8px">本文针对参数受限大语言模型在代理强化学习中探索轨迹噪声的问题，即工具使用时的执行失败会导致信用分配困难，从而阻碍策略优化。提出的CLEANER方法引入了相似性感知自适应回滚机制，利用模型内在的自我纠正能力，基于语义相似性自主净化轨迹，通过回顾性地用成功的自我纠正替换失败动作，使模型能基于清洁推理模式进行训练。在AIME24/25、GPQA和LiveCodeBench等基准测试上的实验结果显示，相比基线方法平均准确率分别提升6%、3%和5%，且CLEANER仅用三分之一的训练步数就达到了最先进性能，体现了其高效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding</div>
<div class="meta-line">Authors: Ayan Maity, Sudeshna Sarkar</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-01-21T16:05:04+00:00 · Latest: 2026-01-21T16:05:04+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26 Workshop on AI for Urban Planning</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15131v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于改进网络嵌入的深度强化学习求解有限时间范围车辆路径问题</div>
<div class="mono" style="margin-top:8px">本文研究有限时间范围下的车辆路径问题，目标是在有限时间内最大化服务的客户请求数量。我们提出一种新颖的路由网络嵌入模块，可生成局部节点嵌入向量和上下文感知的全局图表示。针对该路径问题构建的马尔可夫决策过程，将节点特征、网络邻接矩阵和边特征纳入状态空间。我们将剩余有限时间范围整合到网络嵌入模块中，为嵌入模块提供准确的路由上下文。通过将该嵌入模块与基于策略梯度的深度强化学习框架相结合，求解有限时间范围的车辆路径问题。我们在真实世界路由网络及合成生成的欧几里得网络上对所提方法进行了训练与验证。实验结果表明，本方法相比现有路由方法实现了更高的客户服务率，且求解时间显著低于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vehicle routing problem with a finite time horizon, aiming to maximize served customer requests within a limited period. The method introduces a novel network embedding module that generates local node embeddings and a global graph representation, incorporating the remaining time horizon for context, and integrates this with a policy gradient deep reinforcement learning framework. Experimental results on real-world and synthetic networks demonstrate that the proposed approach achieves a higher customer service rate and significantly reduces solution time compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文研究有限时间范围内的车辆路径问题，目标是在限定时间内最大化服务的客户请求数量。方法提出了一种新颖的网络嵌入模块，生成局部节点嵌入和全局图表示，并结合剩余时间范围提供上下文，将其与基于策略梯度的深度强化学习框架集成。在真实世界和合成网络上的实验结果表明，该方法相比现有方法实现了更高的客户服务率，并显著降低了求解时间。</div>
</details>
</div>
<div class="card">
<div class="title">DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search</div>
<div class="meta-line">Authors: Bostan Khan, Masoud Daneshtalab</div>
<div class="meta-line">First: 2026-01-21T16:03:25+00:00 · Latest: 2026-01-21T16:03:25+00:00</div>
<div class="meta-line">Comments: This paper significantly extends the preliminary work accepted at ESANN 2026. Source Code: https://github.com/bostankhan6/DeepFedNAS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15127v1">PDF</a> · <a href="https://github.com/bostankhan6/DeepFedNAS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepFedNAS：一种面向原则性、硬件感知且无需预测器的联邦神经架构搜索统一框架</div>
<div class="mono" style="margin-top:8px">联邦神经架构搜索旨在为隐私保护的联邦学习自动化模型设计，但当前面临两大瓶颈：无引导的超网络训练导致次优模型，以及训练后子网发现需耗费数小时的高成本流程。本文提出DeepFedNAS——一种基于原则性多目标适应度函数的新型两阶段框架，该函数融合了数学化网络设计与架构启发式规则。通过重构的超网络，DeepFedNAS引入联邦帕累托最优超网络训练，利用预计算的高适应度架构帕累托最优缓存作为智能课程来优化共享超网络权重。随后，其无预测器搜索方法通过将该适应度函数作为零成本的精度直接代理，无需昂贵精度替代模型，实现数秒内按需子网发现。DeepFedNAS在CIFAR-100上取得最高1.21%的绝对精度提升，具备更优的参数与通信效率，并将训练后搜索流程总时间加速约61倍。通过将流程从超过20小时缩减至约20分钟（含初始缓存生成），并实现20秒级单次子网搜索，本框架使硬件感知的联邦学习部署具备即时性与实用性。完整源代码与实验脚本已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces DeepFedNAS, a framework addressing inefficiencies in Federated Neural Architecture Search (FedNAS), where unguided supernet training and slow post-training search hinder practical deployment. The method employs a principled multi-objective fitness function and a re-engineered supernet to enable Federated Pareto Optimal Supernet Training, using a pre-computed cache of high-fitness architectures as a curriculum, followed by a predictor-free search that uses the fitness function as a zero-cost accuracy proxy for rapid subnet discovery. Experimental results show state-of-the-art accuracy improvements (e.g., up to 1.21% on CIFAR-100), better parameter and communication efficiency, and a ~61x speedup in the search pipeline, reducing total time from over 20 hours to about 20 minutes and enabling instant 20-second subnet searches for hardware-aware federated learning.</div>
<div class="mono" style="margin-top:8px">本文提出了DeepFedNAS框架，旨在解决联邦神经架构搜索（FedNAS）中因无指导的超网训练和缓慢的训练后搜索导致的效率低下问题。该方法采用一种原则性的多目标适应度函数和重新设计的超网，实现了联邦帕累托最优超网训练，利用预计算的高适应度架构缓存作为课程，随后通过无预测器搜索将适应度函数作为零成本精度代理进行快速子网发现。实验结果表明，该框架实现了最先进的精度提升（如在CIFAR-100上最高提升1.21%），具有更优的参数和通信效率，并将搜索流程加速约61倍，总时间从超过20小时缩短至约20分钟，支持20秒内的即时子网搜索，使硬件感知的联邦学习部署变得即时且实用。</div>
</details>
</div>
<div class="card">
<div class="title">Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning</div>
<div class="meta-line">Authors: Oleg Shchendrigin, Egor Cherepanov, Alexey K. Kovalev, Aleksandr I. Panov</div>
<div class="meta-line">First: 2026-01-21T15:27:23+00:00 · Latest: 2026-01-21T15:27:23+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15086v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15086v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://quartz-admirer.github.io/Memory-Rewriting/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中仅凭记忆保持不足以掌握记忆任务</div>
<div class="mono" style="margin-top:8px">现实世界中的有效决策依赖于兼具稳定性与适应性的记忆机制：环境随时间变化，智能体需长期保留相关信息，同时在情境变化时更新或覆盖过时内容。现有强化学习基准测试与记忆增强型智能体主要关注记忆保持，而对同等重要的记忆重写能力探索不足。为填补这一空白，我们提出一个在部分可观测条件下（即智能体必须依赖记忆而非当前观测的自然场景）显式测试持续记忆更新的基准，并借此比较循环网络、基于Transformer的结构化记忆架构。实验表明，经典循环模型尽管结构简单，却在记忆重写任务中展现出比现代结构化记忆（仅在狭窄条件下成功）和基于Transformer的智能体（常无法超越简单保持任务）更强的灵活性与鲁棒性。这些发现揭示了当前方法的根本局限，强调需要平衡稳定保持与适应性更新的记忆机制。本研究聚焦这一被忽视的挑战，提出评估基准，并为设计具有显式可训练遗忘机制的未来强化学习智能体提供洞见。代码：https://quartz-admirer.github.io/Memory-Rewriting/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of current reinforcement learning (RL) benchmarks and memory-augmented agents, which primarily test memory retention but overlook the critical ability to adaptively update or rewrite memory as environments change. The authors introduce a new benchmark designed to evaluate continual memory updating under partial observability, where agents must rely on memory rather than immediate observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Experimental results show that classic recurrent models exhibit greater flexibility and robustness in memory rewriting tasks, while structured memories succeed only under narrow conditions and transformer-based agents often fail beyond simple retention, highlighting a fundamental gap in current approaches and underscoring the need for mechanisms that balance stable retention with adaptive updating.</div>
<div class="mono" style="margin-top:8px">本文针对当前强化学习基准测试和记忆增强智能体的局限性，即主要测试记忆保留能力而忽视了在环境变化时适应性更新或重写记忆的关键能力。作者引入了一个新的基准，旨在评估部分可观测性下的持续记忆更新，其中智能体必须依赖记忆而非即时观察，并以此比较循环、基于Transformer和结构化记忆架构。实验结果表明，经典循环模型在记忆重写任务中表现出更大的灵活性和鲁棒性，而结构化记忆仅在狭窄条件下成功，基于Transformer的智能体在简单保留之外常常失败，这揭示了当前方法的基本缺陷，并强调了平衡稳定保留与适应性更新的机制的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF</div>
<div class="meta-line">Authors: Wang Zixian</div>
<div class="meta-line">First: 2026-01-18T13:57:44+00:00 · Latest: 2026-01-21T14:54:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12415v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12415v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model alignment objectives are often presented as a collection of distinct algorithms, such as PPO, DPO, IPO, and their variants, each motivated by different derivations. In this work, we argue that this diversity obscures a simpler underlying structure. At a fundamental level, alignment objectives involve two independent design choices: (i) how training signals are sampled and weighted, and (ii) how deviations from a reference policy are geometrically penalized. Existing methods typically entangle these choices through a single divergence, most commonly the Kullback-Leibler divergence.
  We show that this entanglement is not merely a modeling convenience but a source of systematic instability. When the same divergence simultaneously determines sample weighting and optimization curvature, adjusting one aspect, such as exploration strength, inevitably alters the other, such as gradient geometry. This coupling is particularly problematic in preference-based reinforcement learning, where advantage signals are unbounded and high-confidence regimes are common.
  We propose a simple but structural remedy by formulating alignment as an orthogonal mirror descent problem, in which sampling geometry enters only as a linear driving force, while optimization geometry is determined independently by a mirror map. This perspective leads to a new alignment objective called Orthogonalized Policy Optimization (OPO), obtained by choosing a Euclidean mirror map in likelihood ratio space. The resulting objective admits a closed-form solution, linear and non-saturating gradient dynamics, and a well-conditioned trust region, while remaining fully compatible with standard large language model training pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正交化策略优化：在RLHF中解耦采样几何与优化几何</div>
<div class="mono" style="margin-top:8px">大语言模型对齐目标常以PPO、DPO、IPO等不同算法形式呈现，各自基于不同推导。本文指出这种多样性掩盖了更简洁的底层结构：对齐目标本质上包含两个独立设计选择：(i)训练信号的采样与加权方式，(ii)对参考策略偏离的几何惩罚机制。现有方法通常通过单一散度（多为KL散度）耦合这两个选择。我们证明这种耦合不仅是建模便利，更是系统不稳定的根源——当同一散度同时决定样本加权和优化曲率时，调整探索强度等参数必然改变梯度几何特性，这在优势信号无界、高置信度场景常见的基于偏好的强化学习中尤为严重。我们提出结构性解决方案：将对齐问题构建为正交镜像下降问题，使采样几何仅作为线性驱动力，而优化几何由镜像映射独立决定。该视角催生了正交化策略优化（OPO）目标，通过在似然比空间选择欧几里得镜像映射实现。所得目标具有闭式解、线性非饱和梯度动态、良态信任域，同时完全兼容标准大语言模型训练流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that existing alignment methods for large language models, such as PPO and DPO, entangle two design choices—how training signals are sampled and how deviations from a reference policy are penalized—through a single divergence like KL, leading to systematic instability, especially with unbounded advantage signals in preference-based RL. To address this, the method proposes Orthogonalized Policy Optimization (OPO), which formulates alignment as an orthogonal mirror descent problem, decoupling sampling geometry as a linear driving force from optimization geometry defined independently by a mirror map, specifically using a Euclidean mirror map in likelihood ratio space. Experimental results indicate that OPO yields a closed-form solution, linear and non-saturating gradient dynamics, a well-conditioned trust region, and maintains compatibility with standard LLM training pipelines, offering improved stability.</div>
<div class="mono" style="margin-top:8px">本文的动机在于观察到现有大语言模型对齐方法（如PPO和DPO）通过单一散度（如KL散度）将训练信号采样方式与偏离参考策略的惩罚方式这两个设计选择纠缠在一起，导致系统性不稳定，在偏好强化学习中无界优势信号下尤为突出。为解决此问题，该方法提出了正交化策略优化（OPO），将对齐问题表述为正交镜像下降问题，将采样几何作为线性驱动力与由镜像映射独立定义的优化几何解耦，具体在似然比空间中使用欧几里得镜像映射。实验结果表明，OPO能够获得闭式解、线性且非饱和的梯度动态、条件良好的信任区域，并保持与标准大语言模型训练流程的兼容性，从而提升了稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem</div>
<div class="meta-line">Authors: Mertcan Daysalilar, Fuat Uyguroglu, Gabriel Nicolosi, Adam Meyers</div>
<div class="meta-line">First: 2026-01-21T14:42:33+00:00 · Latest: 2026-01-21T14:42:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15038v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于课程学习的深度强化学习框架在电动汽车路径规划问题中的应用</div>
<div class="mono" style="margin-top:8px">带时间窗的电动汽车路径规划问题（EVRPTW）是可持续物流领域的一个复杂优化问题，需要在满足严格客户时间约束的同时，最小化总行驶距离、车队规模和电池消耗。尽管深度强化学习（DRL）作为传统启发式算法和精确求解器的替代方案展现出巨大潜力，但现有DRL模型常因约束密集而难以保持训练稳定性——无法收敛或泛化。本研究提出一种基于课程学习的深度强化学习（CB-DRL）框架以解决该稳定性问题。该框架采用结构化三阶段课程，逐步提升问题复杂度：智能体先学习距离与车队优化（阶段A），再学习电池管理（阶段B），最后学习完整EVRPTW问题（阶段C）。为确保跨阶段稳定学习，框架采用改进的近端策略优化算法，配备阶段特定超参数、价值优势裁剪和自适应学习率调度。策略网络基于异构图注意力编码器构建，并通过全局-局部注意力机制与特征线性调制增强。该专用架构能显式捕捉配送中心、客户和充电站的差异化特性。模型仅使用N=10的小规模实例训练，在N=5至N=100的未见实例上展现出强泛化能力，在中规模问题上显著优于标准基线。实验结果表明，在标准DRL基线失效的分布外实例上，这种课程引导方法能实现高可行率与有竞争力的解质量，有效弥合了神经网络的求解速度与运营可靠性之间的鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the training instability of deep reinforcement learning (DRL) models when solving the complex electric vehicle routing problem with time windows (EVRPTW). The proposed solution is a curriculum-based DRL framework that structures learning into three phases of increasing complexity, starting with distance and fleet optimization, then adding battery management, and finally tackling the full problem. The method employs a modified proximal policy optimization algorithm with phase-specific adjustments and a specialized graph attention network to model different node types. Experiments show that the model, trained only on small instances with 10 customers, generalizes robustly to unseen problems of up to 100 customers, outperforming standard baselines in feasibility and solution quality on medium-scale, out-of-distribution instances.</div>
<div class="mono" style="margin-top:8px">本研究针对深度强化学习模型在求解带时间窗的电动汽车路径规划这一复杂优化问题时存在的训练不稳定问题，提出了一种课程式深度强化学习框架。该方法将学习过程构建为三个逐步增加复杂度的阶段：先学习距离和车队优化，再融入电池管理，最后解决完整问题；其采用改进的近端策略优化算法与阶段特定的超参数，并利用一个基于异构图注意力机制的策略网络来显式建模仓库、客户和充电站的不同特性。实验结果表明，该模型仅使用10个客户的小规模实例进行训练，便能良好泛化到5至100个客户的未见实例上，在中等规模、分布外的问题上，其可行性与解的质量显著优于基线方法，有效弥补了神经求解器速度与可靠性之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Non-Stationary Functional Bilevel Optimization</div>
<div class="meta-line">Authors: Jason Bohne, Ieva Petrulionyte, Michael Arbel, Julien Mairal, Paweł Polak</div>
<div class="meta-line">First: 2026-01-21T14:35:23+00:00 · Latest: 2026-01-21T14:35:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15363v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Functional bilevel optimization (FBO) provides a powerful framework for hierarchical learning in function spaces, yet current methods are limited to static offline settings and perform suboptimally in online, non-stationary scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO with both theoretical guarantees and practical scalability. SmoothFBO introduces a time-smoothed stochastic hypergradient estimator that reduces variance through a window parameter, enabling stable outer-loop updates with sublinear regret. Importantly, the classical parametric bilevel case is a special reduction of our framework, making SmoothFBO a natural extension to online, non-stationary settings. Empirically, SmoothFBO consistently outperforms existing FBO methods in non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness. Together, these results establish SmoothFBO as a general, theoretically grounded, and practically viable foundation for bilevel optimization in online, non-stationary scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非平稳函数双层优化</div>
<div class="mono" style="margin-top:8px">函数双层优化为函数空间中的分层学习提供了强大框架，但现有方法局限于静态离线设置，在在线非平稳场景中表现欠佳。我们提出首个兼具理论保证与实践可扩展性的非平稳FBO算法SmoothFBO。该算法通过引入时间平滑的随机超梯度估计器，利用窗口参数降低方差，实现具有次线性遗憾的稳定外层更新。值得注意的是，经典参数化双层问题可视为本框架的特例，使SmoothFBO自然扩展至在线非平稳场景。实验表明，在非平稳超参数优化与基于模型的强化学习中，SmoothFBO持续优于现有FBO方法，验证了其实用效能。这些成果共同确立了SmoothFBO作为在线非平稳场景中双层优化的通用、理论坚实且实践可行的基础框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing functional bilevel optimization methods in static offline settings, which perform poorly in online, non-stationary environments, this paper introduces SmoothFBO, the first algorithm designed for non-stationary functional bilevel optimization with theoretical guarantees and practical scalability. The method employs a time-smoothed stochastic hypergradient estimator that reduces variance via a window parameter, enabling stable outer-loop updates with sublinear regret, and notably generalizes classical parametric bilevel optimization as a special case. Experimentally, SmoothFBO consistently outperforms existing functional bilevel optimization methods in tasks like non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness and establishing it as a theoretically grounded foundation for online, non-stationary bilevel optimization.</div>
<div class="mono" style="margin-top:8px">针对现有函数双层优化方法局限于静态离线设置、在在线非平稳场景中表现不佳的问题，本文提出了SmoothFBO，这是首个具有理论保证和实践可扩展性的非平稳函数双层优化算法。该方法采用了一种时间平滑的随机超梯度估计器，通过窗口参数降低方差，从而实现具有次线性遗憾的稳定外环更新，并显著地将经典参数化双层优化作为特例纳入框架。实验结果表明，在非平稳超参数优化和基于模型的强化学习等任务中，SmoothFBO consistently outperforms existing functional bilevel optimization methods in tasks like non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness and establishing it as a theoretically grounded foundation for online, non-stationary bilevel optimization。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Black-Box Attacks Against Cooperative Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Amine Andam, Jamal Bentahar, Mustapha Hedabou</div>
<div class="meta-line">First: 2025-08-12T18:31:15+00:00 · Latest: 2026-01-21T14:25:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09275v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.09275v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Collaborative multi-agent reinforcement learning has rapidly evolved, offering state-of-the-art algorithms for real-world applications, including sensitive domains. However, a key challenge to its widespread adoption is the lack of a thorough investigation into its vulnerabilities to adversarial attacks. Existing work predominantly focuses on training-time attacks or unrealistic scenarios, such as access to policy weights or the ability to train surrogate policies. In this paper, we investigate new vulnerabilities under more challenging and constrained conditions, assuming an adversary can only collect and perturb the observations of deployed agents. We also consider scenarios where the adversary has no access at all (no observations, actions, or weights). Our main approach is to generate perturbations that intentionally misalign how victim agents see their environment. Our approach is empirically validated on three benchmarks and 22 environments, demonstrating its effectiveness across diverse algorithms and environments. Furthermore, we show that our algorithm is sample-efficient, requiring only 1,000 samples compared to the millions needed by previous methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束条件下针对协作多智能体强化学习的黑盒攻击</div>
<div class="mono" style="margin-top:8px">协作多智能体强化学习快速发展，为包括敏感领域在内的现实应用提供了先进算法。然而，其广泛采用面临的关键挑战在于缺乏对其对抗攻击脆弱性的深入研究。现有工作主要集中于训练时攻击或不切实际的场景，如获取策略权重或训练替代策略。本文在更具挑战性和约束性的条件下研究新的脆弱性，假设攻击者仅能收集并扰动已部署智能体的观测数据。我们还考虑了攻击者完全无法访问（无观测、动作或权重）的场景。我们的核心方法是生成故意使受害智能体对环境感知失准的扰动。该方法在三个基准测试和22个环境中进行了实证验证，证明了其在多种算法和环境中的有效性。此外，我们的算法具有样本高效性，仅需1,000个样本，而先前方法需要数百万样本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the security vulnerabilities of cooperative multi-agent reinforcement learning (MARL) systems under constrained black-box attack scenarios, motivated by the lack of thorough studies on adversarial threats in sensitive real-world applications. The method involves generating perturbations to misalign the observations of deployed agents, assuming an adversary can only collect and perturb observations or, in more extreme cases, has no access to the system&#x27;s internals. Experimental results across three benchmarks and 22 environments demonstrate the approach&#x27;s effectiveness against diverse MARL algorithms, with notable sample efficiency requiring only 1,000 samples compared to millions in prior methods.</div>
<div class="mono" style="margin-top:8px">本文研究了协作式多智能体强化学习系统在受限黑盒攻击场景下的安全漏洞，其动机在于现有研究缺乏对敏感现实应用中对抗性威胁的深入探讨。该方法通过生成扰动来误导已部署智能体的观测，假设攻击者仅能收集并扰动观测数据，或在更极端情况下完全无法访问系统内部。在三个基准测试和22个环境中的实验结果表明，该攻击方法对多种多智能体强化学习算法均有效，且样本效率显著，仅需1,000个样本，而先前方法需要数百万。</div>
</details>
</div>
<div class="card">
<div class="title">Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control</div>
<div class="meta-line">Authors: Jannis Becktepe, Aleksandra Franz, Nils Thuerey, Sebastian Peitz</div>
<div class="meta-line">First: 2026-01-21T14:13:44+00:00 · Latest: 2026-01-21T14:13:44+00:00</div>
<div class="meta-line">Comments: Code available at https://github.com/safe-autonomous-systems/fluidgym</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15015v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15015v1">PDF</a> · <a href="https://github.com/safe-autonomous-systems/fluidgym">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模流动控制强化学习算法的即插即用基准测试</div>
<div class="mono" style="margin-top:8px">强化学习在主动流动控制领域已展现出良好前景，但由于现有研究依赖异构的观测与执行方案、数值设置及评估协议，该领域进展仍难以评估。现有AFC基准虽尝试解决这些问题，但严重依赖外部计算流体动力学求解器，不具备完全可微性，且对三维与多智能体场景支持有限。为突破这些限制，我们推出了首个独立、完全可微的AFC强化学习基准套件FluidGym。该套件完全基于GPU加速的PICT求解器在PyTorch上构建，运行于单一Python栈，无需外部CFD软件，并提供标准化评估协议。我们展示了PPO与SAC的基线结果，并将所有环境、数据集及训练模型作为公共资源发布。FluidGym实现了控制方法的系统化比较，为基于学习的流动控制研究建立了可扩展基础，项目地址：https://github.com/safe-autonomous-systems/fluidgym。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the lack of standardized evaluation in reinforcement learning for active flow control, where existing benchmarks rely on heterogeneous setups and external CFD solvers with limited differentiability and 3D support. The method introduces FluidGym, a fully differentiable benchmark suite built entirely in PyTorch using the GPU-accelerated PICT solver, eliminating the need for external CFD software and providing a standalone, standardized environment. Main experimental results include baseline performance using PPO and SAC algorithms, with all environments, datasets, and trained models released as public resources to enable systematic comparison and scalable future research in learning-based flow control.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决强化学习在主动流动控制领域缺乏标准化评估的问题，现有基准依赖异构设置和外部计算流体动力学求解器，且可微性及三维支持有限。方法上提出了FluidGym，这是一个完全可微的基准套件，完全基于PyTorch并利用GPU加速的PICT求解器构建，无需外部CFD软件，提供了独立且标准化的环境。主要实验结果包括使用PPO和SAC算法的基线性能，所有环境、数据集和训练模型均作为公共资源发布，以支持系统化比较并为基于学习的流动控制研究奠定可扩展的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Shaping to Mitigate Reward Hacking in RLHF</div>
<div class="meta-line">Authors: Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</div>
<div class="meta-line">First: 2025-02-26T02:57:59+00:00 · Latest: 2026-01-21T13:46:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18770v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.18770v5">PDF</a> · <a href="https://github.com/PorUna-byte/PAR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR&#x27;s superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过奖励塑形缓解RLHF中的奖励黑客问题</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）对于使大语言模型（LLMs）与人类价值观对齐至关重要。然而，RLHF容易受到奖励黑客攻击，即智能体利用奖励函数的缺陷而非学习预期行为，从而降低对齐效果。尽管奖励塑形有助于稳定RLHF并部分缓解奖励黑客问题，但对塑形技术及其基本原理的系统性研究仍显不足。为填补这一空白，我们对主流奖励塑形方法进行了全面研究。分析提出两个关键设计原则：（1）RL奖励应有界；（2）RL奖励受益于快速初始增长后逐渐收敛。基于这些见解，我们提出偏好即奖励（PAR）方法，利用奖励模型中隐含的偏好作为强化学习信号。PAR具有两个关键的方差缩减特性，有助于稳定RLHF训练过程并有效扩展早停容忍窗口。我们在Gemma2-2B基础模型上使用Ultrafeedback-Binarized和HH-RLHF数据集评估PAR，实验结果表明PAR优于其他奖励塑形方法。在AlpacaEval 2.0基准测试中，PAR的胜率至少比其他方法高5个百分点。此外，PAR展现出卓越的数据效率，仅需单个参考奖励即可实现最优性能，并在完整训练两轮后仍能保持对奖励黑客攻击的鲁棒性。代码发布于https://github.com/PorUna-byte/PAR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward hacking in Reinforcement Learning from Human Feedback (RLHF), where agents exploit flaws in reward functions instead of learning desired behaviors, thereby undermining alignment. The authors conduct a systematic study of existing reward shaping methods, identifying two key design principles: bounded RL rewards and a reward structure that grows quickly initially before converging gradually. Based on these insights, they propose Preference As Reward (PAR), a novel method that uses latent preferences from the reward model as the RL signal, offering variance-reduction properties to stabilize training and extend the early-stopping tolerance window. Experiments on Gemma2-2B with Ultrafeedback-Binarized and HH-RLHF datasets show that PAR outperforms other shaping methods, achieving at least a 5 percentage point higher win rate on AlpacaEval 2.0, while demonstrating strong data efficiency and robustness against reward hacking even after extended training.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习（RLHF）中的奖励黑客问题展开研究，即智能体利用奖励函数的缺陷而非学习预期行为，从而损害对齐效果。作者系统分析了现有奖励塑形方法，提出了两个关键设计原则：强化学习奖励应有界，且奖励结构应快速初始增长后逐渐收敛。基于这些发现，他们提出了偏好即奖励（PAR）新方法，利用奖励模型中的潜在偏好作为强化学习信号，具备方差减少特性以稳定训练并延长早停容忍窗口。在Gemma2-2B模型上使用Ultrafeedback-Binarized和HH-RLHF数据集的实验表明，PAR优于其他塑形方法，在AlpacaEval 2.0基准上胜率至少高出5个百分点，同时展现出卓越的数据效率和鲁棒性，即使在完整训练两轮后仍能有效抵抗奖励黑客。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Regret Approximation for Unsupervised Dynamic Environment Generation</div>
<div class="meta-line">Authors: Harry Mead, Bruno Lacerda, Jakob Foerster, Nick Hawes</div>
<div class="meta-line">First: 2026-01-21T12:58:40+00:00 · Latest: 2026-01-21T12:58:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14957v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14957v1">PDF</a> · <a href="https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进无监督动态环境生成中的遗憾近似方法</div>
<div class="mono" style="margin-top:8px">无监督环境设计旨在为强化学习智能体自动生成训练课程，以提升泛化能力和零样本性能。然而，设计有效课程仍具挑战性，尤其在特定环境参数子集会显著增加策略复杂度的场景中。现有方法面临困难的信用分配问题，且依赖的遗憾近似方法难以识别高难度环境层级，这些问题随环境规模扩大而加剧。本文提出动态环境生成方法，通过增强层级生成器的奖励信号密度来缓解信用分配难题，使无监督环境设计能扩展至更大规模环境。同时引入新型遗憾近似指标——最大化负优势，该指标能更有效识别高难度环境层级。实验表明，最大化负优势指标优于现有遗憾近似方法，与动态环境生成结合后，尤其在环境规模扩大时，持续超越现有方法。相关代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of Unsupervised Environment Design (UED) in generating effective training curricula for reinforcement learning agents, particularly when scaling to larger environments where current methods struggle with credit assignment and identifying challenging levels. The method introduces Dynamic Environment Generation for UED (DEGen) to provide a denser reward signal for level generators, easing credit assignment, and proposes a new regret approximation called Maximised Negative Advantage (MNA) to better pinpoint difficult environment parameterizations. Experimental results demonstrate that MNA outperforms existing regret approximations and, when combined with DEGen, consistently surpasses prior methods, especially as environment size increases, improving generalization and zero-shot performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决无监督环境设计（UED）在为强化学习智能体生成有效训练课程时的局限性，尤其是在扩展到更大环境规模时，现有方法难以处理信用分配和识别挑战性关卡的问题。方法上，提出了动态环境生成（DEGen）为关卡生成器提供更密集的奖励信号以简化信用分配，并引入了一种新的遗憾近似度量——最大化负优势（MNA），以更准确地识别困难的环境参数化。实验结果表明，MNA优于现有的遗憾近似方法，且与DEGen结合后，尤其在环境规模增大时，持续超越现有方法，提升了泛化能力和零样本性能。</div>
</details>
</div>
<div class="card">
<div class="title">What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study</div>
<div class="meta-line">Authors: Keyu Lv, Manyi Zhang, Xiaobo Xia, Jingchen Ni, Shannan Yan, Xianzhi Yu, Lu Hou, Chun Yuan, Haoli Bai</div>
<div class="meta-line">First: 2026-01-21T11:22:29+00:00 · Latest: 2026-01-21T11:22:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14888v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14888v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低比特量化感知训练为何适用于推理大语言模型？一项系统性研究</div>
<div class="mono" style="margin-top:8px">推理模型在编程和数学等复杂任务上表现出色，但其推理过程通常速度较慢且令牌效率低下。为提高推理效率，训练后量化（PTQ）常伴随较大的精度损失，尤其在低比特设置下的推理任务中。本研究对推理模型的量化感知训练（QAT）进行了系统性实证研究。主要发现包括：（1）知识蒸馏是通过监督微调或强化学习训练的推理模型的稳健目标；（2）PTQ为QAT提供了强初始化，在降低训练成本的同时提升精度；（3）在可行的冷启动条件下，强化学习对量化模型仍然适用，并能带来额外增益；（4）将PTQ校准域与QAT训练域对齐可加速收敛，并常能提高最终精度。最后，我们将这些发现整合为优化工作流（Reasoning-QAT），并证明其在多个大语言模型骨干和推理数据集上持续优于最先进的PTQ方法。例如在Qwen3-0.6B模型上，该方法在MATH-500数据集上超越GPTQ达44.53%，并在2比特量化场景中持续恢复性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study systematically investigates quantization-aware training (QAT) for reasoning large language models (LLMs) to address the inference inefficiency and accuracy drops common with low-bit post-training quantization (PTQ). The method identifies key factors for effective QAT, including using knowledge distillation as a robust objective, leveraging PTQ for initialization, applying reinforcement learning to quantized models, and aligning calibration and training domains. Experimental results demonstrate that the proposed Reasoning-QAT workflow consistently outperforms state-of-the-art PTQ methods across various LLM backbones and reasoning datasets, achieving significant gains such as a 44.53% improvement over GPTQ on MATH-500 for Qwen3-0.6B and recovering performance even at 2-bit precision.</div>
<div class="mono" style="margin-top:8px">本研究系统探讨了面向推理大语言模型的量化感知训练（QAT），旨在解决低比特后训练量化（PTQ）常见的推理效率低下和精度下降问题。方法上，研究确定了有效QAT的关键因素，包括使用知识蒸馏作为鲁棒目标、利用PTQ进行初始化、对量化模型应用强化学习，以及对齐校准与训练领域。实验结果表明，所提出的Reasoning-QAT工作流程在多种LLM骨干和推理数据集上持续优于最先进的PTQ方法，取得了显著提升，例如在Qwen3-0.6B模型上，MATH-500数据集上的性能比GPTQ高出44.53%，并在2比特精度下恢复了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Listwise Direct Preference Optimization with Multi-Dimensional Preference Mixing</div>
<div class="meta-line">Authors: Yuhui Sun, Xiyao Wang, Zixi Li, YiTian Ding, Tianyang Ling, Jialuo Chen, Tianyi Yu, Zhenlong Yuan, Jinman Zhao</div>
<div class="meta-line">First: 2025-06-24T16:47:17+00:00 · Latest: 2026-01-21T10:13:03+00:00</div>
<div class="meta-line">Comments: 13 pages, 1 figures, appendix included</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.19780v6">Abs</a> · <a href="https://arxiv.org/pdf/2506.19780v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent alignment methods based on Direct Preference Optimization (DPO) reformulate preference learning as supervised optimization over pairwise comparisons, offering improved efficiency and stability over reinforcement learning from human feedback (RLHF). However, existing DPO-style methods implicitly assume a single fixed preference objective, which limits their ability to model the structured and sometimes conflicting nature of real-world human judgments that span multiple preference dimensions. In this work, we propose Listwise Direct Preference Optimization ($λ$-DPO), a unified framework that simultaneously improves supervision granularity and preference flexibility. Instead of collapsing multi-dimensional preference signals into a single ranking, $λ$-DPO constructs a mixture of listwise preference distributions weighted by a preference vector $λ$ on the probability simplex, enabling a single model to internalize a continuous spectrum of preference trade-offs. To further improve robustness, we introduce a performance-driven stochastic $λ$ scheduler that adaptively samples preference weights based on empirical downstream performance, explicitly mitigating the risks of misspecification inherent to static weighting schemes. We evaluate our method across multiple model families and scales on six widely used benchmarks. Experimental results show the consistent improvement against baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多维偏好混合的列表式直接偏好优化</div>
<div class="mono" style="margin-top:8px">基于直接偏好优化（DPO）的近期对齐方法将偏好学习重构为基于成对比较的监督优化，相比基于人类反馈的强化学习（RLHF）提升了效率与稳定性。然而现有DPO类方法隐含假设单一固定偏好目标，难以建模现实人类评判中跨越多维度的结构化且可能相互冲突的特性。本研究提出列表式直接偏好优化（$λ$-DPO），该统一框架同步提升监督粒度与偏好灵活性：通过概率单纯形上的偏好向量$λ$加权构建列表式偏好分布的混合模型，使单一模型能够内化连续谱系的偏好权衡，而非将多维偏好信号压缩为单一排序。为增强鲁棒性，我们引入性能驱动的随机$λ$调度器，根据下游实证表现自适应采样偏好权重，显式缓解静态加权方案固有的误设风险。我们在六个广泛使用的基准测试中跨越多模型家族与规模进行评估，实验结果表明该方法相对基线模型取得持续改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing Direct Preference Optimization (DPO) methods, which assume a single fixed preference objective and thus struggle to model the multi-dimensional and potentially conflicting nature of real human judgments, this paper proposes Listwise Direct Preference Optimization (λ-DPO). The method introduces a unified framework that constructs a mixture of listwise preference distributions weighted by a preference vector λ on the probability simplex, allowing a single model to internalize a continuous spectrum of preference trade-offs; it further enhances robustness with a performance-driven stochastic λ scheduler that adaptively samples preference weights based on downstream performance to mitigate risks from static weighting. Experimental results across multiple model families and scales on six benchmarks demonstrate consistent improvements over baseline methods.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有直接偏好优化（DPO）方法假设单一的固定偏好目标，难以建模现实世界中多维度且可能冲突的人类判断。为此，论文提出了列表式直接偏好优化（λ-DPO），该方法引入了一个统一框架，通过在概率单纯形上使用偏好向量λ加权混合列表式偏好分布，使单一模型能够内化连续的偏好权衡谱；为进一步提升鲁棒性，还引入了性能驱动的随机λ调度器，基于下游性能自适应采样偏好权重，以缓解静态加权方案固有的误设风险。在多个模型系列和规模上对六个广泛使用的基准进行的实验结果表明，该方法相较于基线取得了持续一致的改进。</div>
</details>
</div>
<div class="card">
<div class="title">CI4A: Semantic Component Interfaces for Agents Empowering Web Automation</div>
<div class="meta-line">Authors: Zhi Qiu, Jiazheng Sun, Chenxiao Xia, Jun Zheng, Xin Peng</div>
<div class="meta-line">First: 2026-01-21T09:14:04+00:00 · Latest: 2026-01-21T09:14:04+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14790v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14790v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CI4A：面向智能体的语义化组件接口赋能网页自动化</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在高层语义规划方面展现出卓越能力，但在处理细粒度的底层网页组件操作时仍存在局限。为突破此限制，现有研究多通过强化学习等技术增强模型的基础操作能力。然而，与其迫使智能体适应以人为中心的交互界面，我们提出构建专为智能体优化的交互接口。本文提出面向智能体的组件接口（CI4A），这是一种将UI组件的复杂交互逻辑抽象为智能体可访问的统一工具原语的语义封装机制。我们在工业级前端框架Ant Design中实现了CI4A，覆盖23类常用UI组件。此外，我们开发了具备动态更新动作空间的混合智能体，可根据页面状态灵活调用可用的CI4A工具。基于集成CI4A的Ant Design，我们重构并升级了WebArena基准测试以评估现有前沿方法。实验结果表明，基于CI4A的智能体显著优于现有方法，以86.3%的任务成功率创下新纪录，同时执行效率也获得大幅提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of Large Language Models in performing fine-grained web component manipulations by proposing a shift from adapting agents to human-centric interfaces to creating agent-optimized interfaces. It introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts complex UI interaction logic into unified tool primitives for agents, implemented in the Ant Design framework covering 23 common UI component categories. A hybrid agent with a dynamically updating action space was developed to flexibly invoke CI4A tools, and evaluation on a refactored WebArena benchmark showed that the CI4A-based agent significantly outperforms existing methods, achieving a state-of-the-art task success rate of 86.3% with improved execution efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在细粒度网页组件操作上的局限性，提出从让智能体适应以人为中心的界面转向创建为智能体优化的界面。它引入了组件接口（CI4A），这是一种语义封装机制，将复杂的用户界面交互逻辑抽象为智能体可访问的统一工具原语，并在Ant Design前端框架中实现，覆盖了23类常用UI组件。研究开发了一个具有动态更新动作空间的混合智能体，以灵活调用CI4A工具，并在重构的WebArena基准测试上进行评估，结果表明基于CI4A的智能体显著优于现有方法，实现了86.3%的最先进任务成功率，同时执行效率也得到大幅提升。</div>
</details>
</div>
<div class="card">
<div class="title">Impartial Games: A Challenge for Reinforcement Learning</div>
<div class="meta-line">Authors: Bei Zhou, Søren Riis</div>
<div class="meta-line">First: 2022-05-25T14:02:02+00:00 · Latest: 2026-01-21T08:31:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2205.12787v6">Abs</a> · <a href="https://arxiv.org/pdf/2205.12787v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AlphaZero-style reinforcement learning (RL) algorithms have achieved superhuman performance in many complex board games such as Chess, Shogi, and Go. However, we showcase that these algorithms encounter significant and fundamental challenges when applied to impartial games, a class where players share game pieces and optimal strategy often relies on abstract mathematical principles. Specifically, we utilise the game of Nim as a concrete and illustrative case study to reveal critical limitations of AlphaZero-style and similar self-play RL algorithms. We introduce a novel conceptual framework distinguishing between champion and expert mastery to evaluate RL agent performance. Our findings reveal that while AlphaZero-style agents can achieve champion-level play on very small Nim boards, their learning progression severely degrades as the board size increases. This difficulty stems not merely from complex data distributions or noisy labels, but from a deeper representational bottleneck: the inherent struggle of generic neural networks to implicitly learn abstract, non-associative functions like parity, which are crucial for optimal play in impartial games. This limitation causes a critical breakdown in the positive feedback loop essential for self-play RL, preventing effective learning beyond rote memorisation of frequently observed states. These results align with broader concerns regarding AlphaZero-style algorithms&#x27; vulnerability to adversarial attacks, highlighting their inability to truly master all legal game states. Our work underscores that simple hyperparameter adjustments are insufficient to overcome these challenges, establishing a crucial foundation for the development of fundamentally novel algorithmic approaches, potentially involving neuro-symbolic or meta-learning paradigms, to bridge the gap towards true expert-level AI in combinatorial games.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>公平博弈：强化学习面临的新挑战</div>
<div class="mono" style="margin-top:8px">AlphaZero式强化学习算法已在国际象棋、将棋、围棋等复杂棋盘游戏中实现超越人类的表现。然而，本研究揭示该类算法在应用于公平博弈时遭遇根本性挑战——这类博弈中玩家共享棋子，最优策略常依赖抽象数学原理。我们以尼姆游戏为具体案例，系统揭示AlphaZero式自博弈强化学习算法的关键局限：提出区分冠军级与专家级能力的新评估框架。实验表明，AlphaZero式智能体仅能在极小规模尼姆棋盘实现冠军级表现，其学习能力随棋盘尺寸增加急剧退化。该困境不仅源于数据分布复杂性，更根植于表征瓶颈：通用神经网络难以隐式学习奇偶性等抽象非结合函数，而这正是公平博弈最优策略的核心。此缺陷导致自博弈强化学习依赖的正反馈循环崩溃，使学习机制退化为高频状态的机械记忆。该发现与AlphaZero式算法易受对抗攻击的普遍担忧形成呼应，揭示其无法真正掌握所有合法游戏状态。本研究证实简单超参数调整无法解决此类问题，为发展神经符号计算、元学习等新型算法奠定理论基础，以推动组合博弈领域实现真正专家级人工智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the limitations of AlphaZero-style reinforcement learning algorithms when applied to impartial games like Nim, motivated by their superhuman success in games like Chess and Go but their fundamental struggle with games requiring abstract mathematical reasoning. The method involves using Nim as a case study and introducing a framework distinguishing champion-level (exploiting opponent weaknesses) from expert-level (mastering all states) play to evaluate agents. The main experimental results show that while these agents can achieve champion-level play on small Nim boards, their performance severely degrades with larger boards due to a representational bottleneck: neural networks fail to implicitly learn crucial non-associative functions like parity, breaking the self-play feedback loop and preventing true expert mastery.</div>
<div class="mono" style="margin-top:8px">本文研究了AlphaZero式强化学习算法在应用于尼姆等公平组合游戏时的局限性，其动机在于这类算法在国际象棋和围棋等游戏中取得了超人类表现，但在需要抽象数学推理的游戏中却存在根本性困难。研究方法以尼姆游戏为具体案例，并引入了一个区分冠军级（利用对手弱点）和专家级（掌握所有状态）表现的概念框架来评估智能体。主要实验结果表明，虽然这些智能体能在小型尼姆棋盘上达到冠军级水平，但随着棋盘增大，其性能严重下降，这源于一个表征瓶颈：神经网络无法隐式学习奇偶性等关键的非结合函数，导致自我对弈的正反馈循环崩溃，从而无法实现真正的专家级掌握。</div>
</details>
</div>
<div class="card">
<div class="title">Q-Probe: Scaling Image Quality Assessment to High Resolution via Context-Aware Agentic Probing</div>
<div class="meta-line">Authors: Xiang Li, XueHeng Li, Yu Wang, XuanHua He, ZhangChi Hu, WeiWei Yu, ChengJun Xie</div>
<div class="meta-line">First: 2026-01-21T08:02:32+00:00 · Latest: 2026-01-21T08:02:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15356v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has empowered Multimodal Large Language Models (MLLMs) to achieve superior human preference alignment in Image Quality Assessment (IQA). However, existing RL-based IQA models typically rely on coarse-grained global views, failing to capture subtle local degradations in high-resolution scenarios. While emerging &quot;Thinking with Images&quot; paradigms enable multi-scale visual perception via zoom-in mechanisms, their direct adaptation to IQA induces spurious &quot;cropping-implies-degradation&quot; biases and misinterprets natural depth-of-field as artifacts. To address these challenges, we propose Q-Probe, the first agentic IQA framework designed to scale IQA to high resolution via context-aware probing. First, we construct Vista-Bench, a pioneering benchmark tailored for fine-grained local degradation analysis in high-resolution IQA settings. Furthermore, we propose a three-stage training paradigm that progressively aligns the model with human preferences, while simultaneously eliminating causal bias through a novel context-aware cropping strategy. Extensive experiments demonstrate that Q-Probe achieves state-of-the-art performance in high-resolution settings while maintaining superior efficacy across resolution scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Q-Probe：通过上下文感知智能探测将图像质量评估扩展至高分辨率</div>
<div class="mono" style="margin-top:8px">强化学习（RL）赋能多模态大语言模型（MLLM）在图像质量评估（IQA）中实现了更优的人类偏好对齐。然而，现有基于RL的IQA模型通常依赖粗粒度的全局视图，难以捕捉高分辨率场景中细微的局部退化。尽管新兴的“图像思维”范式通过放大机制实现了多尺度视觉感知，但其直接应用于IQA会引发虚假的“裁剪即退化”偏差，并将自然景深误解为伪影。为解决这些挑战，我们提出Q-Probe——首个通过上下文感知探测将IQA扩展至高分辨率的智能代理框架。首先，我们构建了Vista-Bench，这是专为高分辨率IQA场景中细粒度局部退化分析设计的开创性基准。此外，我们提出一种三阶段训练范式，在逐步对齐模型与人类偏好的同时，通过新颖的上下文感知裁剪策略消除因果偏差。大量实验表明，Q-Probe在高分辨率场景中实现了最先进的性能，同时在多分辨率尺度上保持卓越效能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing Reinforcement Learning (RL)-based Image Quality Assessment (IQA) models, which rely on coarse global views and fail to capture subtle local degradations in high-resolution images, while also avoiding the biases introduced by direct adaptation of multi-scale &#x27;Thinking with Images&#x27; paradigms. The method proposed, Q-Probe, is a novel agentic IQA framework that scales to high resolution via context-aware probing; it introduces a new benchmark called Vista-Bench for fine-grained local degradation analysis and employs a three-stage training paradigm with a context-aware cropping strategy to align the model with human preferences and eliminate causal bias. The main experimental results show that Q-Probe achieves state-of-the-art performance in high-resolution IQA settings while maintaining superior efficacy across different resolution scales.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有基于强化学习的图像质量评估模型在高分辨率场景下的局限性，这些模型依赖粗粒度的全局视图，无法捕捉细微的局部退化，同时避免直接采用多尺度“图像思维”范式所带来的偏差。所提出的方法Q-Probe是一种新型的智能体图像质量评估框架，通过上下文感知的探测来适应高分辨率；它引入了一个名为Vista-Bench的新基准，用于细粒度局部退化分析，并采用三阶段训练范式及上下文感知裁剪策略，以对齐模型与人类偏好并消除因果偏差。主要实验结果表明，Q-Probe在高分辨率图像质量评估中实现了最先进的性能，同时在多种分辨率尺度上保持了卓越的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach</div>
<div class="meta-line">Authors: Wenyun Li, Wenjie Huang, Chen Sun</div>
<div class="meta-line">First: 2025-01-31T13:35:19+00:00 · Latest: 2026-01-21T07:57:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.19128v4">Abs</a> · <a href="https://arxiv.org/pdf/2501.19128v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world scenarios, reward signal for agents are exceedingly sparse, making it challenging to learn an effective reward function for reward shaping. To address this issue, the proposed approach in this paper performs reward shaping not only by utilizing non-zero-reward transitions but also by employing the \emph{Semi-Supervised Learning} (SSL) technique combined with a novel data augmentation to learn trajectory space representations from the majority of transitions, {i.e}., zero-reward transitions, thereby improving the efficacy of reward shaping. Experimental results in Atari and robotic manipulation demonstrate that our method outperforms supervised-based approaches in reward inference, leading to higher agent scores. Notably, in more sparse-reward environments, our method achieves up to twice the peak scores compared to supervised baselines. The proposed double entropy data augmentation enhances performance, showcasing a 15.8\% increase in best score over other augmentation methods</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中稀疏奖励的塑造：一种半监督方法</div>
<div class="mono" style="margin-top:8px">在许多现实场景中，智能体的奖励信号极为稀疏，这为学习有效的奖励函数以进行奖励塑造带来了挑战。为解决此问题，本文提出的方法不仅利用非零奖励转移进行奖励塑造，还结合半监督学习（SSL）技术与一种新颖的数据增强方法，从大多数转移（即零奖励转移）中学习轨迹空间表示，从而提升奖励塑造的效果。在Atari游戏和机器人操作任务中的实验结果表明，本方法在奖励推断方面优于基于监督学习的方法，使智能体获得更高得分。值得注意的是，在奖励更稀疏的环境中，本方法的峰值得分可达监督基线的两倍。所提出的双熵数据增强技术进一步提升了性能，最佳得分相比其他增强方法提高了15.8%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of sparse rewards in reinforcement learning, which hinders effective reward shaping. It proposes a semi-supervised learning approach that leverages both non-zero and zero-reward transitions, using data augmentation to learn trajectory representations and improve reward inference. Experiments in Atari and robotic manipulation show the method outperforms supervised baselines, achieving up to double the peak scores in sparse-reward environments, with a 15.8% best-score improvement from the novel double entropy augmentation.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中奖励稀疏导致奖励塑形困难的问题，提出了一种半监督学习方法，利用非零奖励和零奖励转移，通过数据增强学习轨迹表示以改进奖励推断。在Atari和机器人操控实验中的结果表明，该方法优于监督基线，在稀疏奖励环境下峰值分数可达基准的两倍，其中双重熵数据增强使最佳分数提升了15.8%。</div>
</details>
</div>
<div class="card">
<div class="title">PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Yao Lu, Dengdong Fan, Jianzheng Nie, Fan Xu, Jie Chen, Bin Zhou, Yonghong Tian</div>
<div class="meta-line">First: 2026-01-21T07:11:40+00:00 · Latest: 2026-01-21T07:11:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14716v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14716v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PCL-Reasoner-V1.5：基于离线强化学习的数学推理模型进展</div>
<div class="mono" style="margin-top:8px">我们推出PCL-Reasoner-V1.5，这是一个拥有320亿参数、专用于数学推理的大语言模型（LLM）。该模型基于Qwen2.5-32B构建，通过监督微调（SFT）和强化学习（RL）进行优化。核心创新在于我们提出的离线RL方法，相比GRPO等标准在线RL方法，具有更优的训练稳定性和效率。在基于Qwen2.5-32B的后训练模型中，我们的模型取得了最先进的性能，在AIME 2024和AIME 2025上的平均准确率分别达到90.9%和85.6%。本研究表明，离线RL是提升LLM推理能力的稳定高效范式。所有实验均在华为昇腾910C NPU上完成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces PCL-Reasoner-V1.5, a 32-billion-parameter large language model designed to enhance mathematical reasoning capabilities. The motivation is to improve training stability and efficiency over conventional online reinforcement learning approaches like GRPO. The method involves building upon the Qwen2.5-32B base model through supervised fine-tuning followed by a novel offline reinforcement learning technique. Experimental results show state-of-the-art performance among models derived from Qwen2.5-32B, achieving average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025, demonstrating the effectiveness of offline RL for advancing reasoning in LLMs.</div>
<div class="mono" style="margin-top:8px">本文介绍了PCL-Reasoner-V1.5，这是一个拥有320亿参数的大型语言模型，旨在提升数学推理能力。其动机是通过改进传统在线强化学习方法（如GRPO）来增强训练稳定性和效率。方法基于Qwen2.5-32B模型，先进行监督微调，再采用一种新颖的离线强化学习技术进行优化。实验结果表明，该模型在基于Qwen2.5-32B的模型中取得了最先进的性能，在AIME 2024和AIME 2025上平均准确率分别达到90.9%和85.6%，验证了离线强化学习在推进大语言模型推理能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs</div>
<div class="meta-line">Authors: Mingxuan Song, Yusen Huo, Bohan Zhou, Shenglin Yin, Zhen Xiao, Jieyi Long, Zhilin Zhang, Chuan Yu</div>
<div class="meta-line">Venue: WWW</div>
<div class="meta-line">First: 2026-01-21T06:58:44+00:00 · Latest: 2026-01-21T06:58:44+00:00</div>
<div class="meta-line">Comments: Accepted at The ACM Web Conference (WWW) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing the advertiser&#x27;s cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs&#x27; in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DARA：基于强化学习微调大语言模型的上下文决策实现在线广告中的少样本预算分配</div>
<div class="mono" style="margin-top:8px">在AI生成竞价（AIGB）范式下，如何在预算约束下优化广告主赢取展示的累积价值是在线广告中的复杂挑战。广告主常有个性化目标但历史交互数据有限，形成传统强化学习方法难以有效应对的少样本场景。大语言模型凭借其上下文学习能力，为AIGB提供了从有限数据泛化的新途径，但缺乏细粒度优化所需的数值精度。为此，我们提出GRPO-Adaptive——一种通过动态更新训练参考策略来同步增强推理与数值精度的高效大语言模型后训练策略。在此基础上，我们进一步提出DARA这一新颖的双阶段框架：第一阶段通过少样本推理器基于上下文提示生成初始方案，第二阶段由细粒度优化器通过反馈驱动推理进行方案精调。这种解耦设计使DARA能融合大语言模型的上下文学习优势与AIGB任务所需的精确适应性。在真实场景与合成数据环境中的大量实验表明，本方法在预算约束下的广告主累积价值指标上持续优于现有基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of optimizing advertiser value under budget constraints in few-shot online advertising scenarios, where traditional reinforcement learning methods are ineffective due to limited data. To overcome this, the authors propose DARA, a dual-phase framework that leverages large language models (LLMs) enhanced with GRPO-Adaptive, a post-training strategy improving reasoning and numerical precision. This framework first uses in-context prompting to generate initial plans and then refines them through feedback-driven optimization, effectively combining LLMs&#x27; generalization abilities with precise task adaptation. Experimental results on real-world and synthetic data show that DARA consistently outperforms existing baselines in cumulative advertiser value while adhering to budget limits.</div>
<div class="mono" style="margin-top:8px">本文针对在线广告中预算约束下广告主价值优化的挑战，在数据有限的少样本场景下，传统强化学习方法因数据不足而效果不佳。为此，作者提出了DARA框架，该框架利用经过GRPO-Adaptive后训练策略增强的大型语言模型，提升推理和数值精度。DARA将决策过程分解为两个阶段：首先通过上下文提示生成初始计划，然后基于反馈驱动优化进行细化，从而结合了大型语言模型的泛化能力和任务所需的精确适应性。在真实世界和合成数据环境上的大量实验表明，该方法在预算约束下，在累积广告主价值方面持续优于现有基线。</div>
</details>
</div>
<div class="card">
<div class="title">Case-Guided Sequential Assay Planning in Drug Discovery</div>
<div class="meta-line">Authors: Tianchi Chen, Jan Bima, Sean L. Wu, Otto Ritter, Bingjia Yang, Xiang Yu</div>
<div class="meta-line">First: 2026-01-21T06:58:01+00:00 · Latest: 2026-01-21T06:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14710v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s&#x27;)$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>药物发现中的案例引导序贯实验规划</div>
<div class="mono" style="margin-top:8px">在药物发现中，如何在高度不确定性和资源限制下优化排序实验检测是一个关键规划难题。标准强化学习的主要障碍在于缺乏明确的环境模拟器或状态转移数据$(s, a, s&#x27;)$，规划只能依赖静态历史结果数据库。我们提出隐式贝叶斯马尔可夫决策过程，这是一个专为无模拟器场景设计的基于模型的强化学习框架。该框架通过相似历史结果构建非参数置信分布，形成案例引导的隐式状态转移模型。该机制支持基于证据积累的贝叶斯置信更新，并采用集成蒙特卡洛树搜索规划来生成稳定策略，以平衡信息获取与资源效率。通过综合实验验证，在真实世界中枢神经系统药物发现任务中，相比现有启发式方法，该框架在保持决策置信度的同时将资源消耗降低达92%。为严格评估决策质量，我们还在可计算最优策略的合成环境中进行了基准测试。与使用相同基于相似性模型的确定性值迭代方法相比，该框架与最优策略的一致性显著更高，证明了集成规划器的优越性。该框架为数据丰富但模拟器稀缺领域的序贯实验设计提供了实用解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of optimally sequencing experimental assays in drug discovery, a high-stakes planning problem under uncertainty and resource constraints, where standard reinforcement learning is hindered by the lack of an explicit environment simulator or transition data. To overcome this, the authors propose the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework that constructs a case-guided implicit model of transition dynamics using nonparametric belief distributions from similar historical outcomes, enabling Bayesian belief updating and ensemble MCTS planning to balance information gain and resource efficiency. Experimental validation on a real-world CNS drug discovery task shows IBMDP reduces resource consumption by up to 92% compared to established heuristics while maintaining decision confidence, and in a synthetic environment with a computable optimal policy, it achieves significantly higher alignment with this optimal policy than a deterministic alternative, demonstrating its superiority for sequential experimental design in data-rich but simulator-poor domains.</div>
<div class="mono" style="margin-top:8px">本文针对药物发现中实验检测顺序优化这一高风险、不确定性和资源约束下的规划问题，其中标准强化学习因缺乏明确的环境模拟器或转移数据而受限。为解决此问题，作者提出了隐式贝叶斯马尔可夫决策过程（IBMDP），这是一种基于模型的强化学习框架，通过使用相似历史结果的非参数信念分布构建案例引导的隐式转移动态模型，支持贝叶斯信念更新和集成蒙特卡洛树搜索规划，以平衡信息获取与资源效率。在真实世界中枢神经系统药物发现任务上的实验验证表明，IBMDP相比现有启发式方法将资源消耗降低高达92%，同时保持决策置信度；在具有可计算最优策略的合成环境中，IBMDP比使用相同相似性模型的确定性替代方法显著更接近最优策略，证明了其在数据丰富但模拟器匮乏领域中顺序实验设计的优越性。</div>
</details>
</div>
<div class="card">
<div class="title">Proximal Policy Optimization with Evolutionary Mutations</div>
<div class="meta-line">Authors: Casimir Czworkowski, Stephen Hornish, Alhassan S. Yasin</div>
<div class="meta-line">First: 2026-01-21T06:34:53+00:00 · Latest: 2026-01-21T06:34:53+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 2 tables, 1 algorithm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14705v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm known for its stability and sample efficiency, but it often suffers from premature convergence due to limited exploration. In this paper, we propose POEM (Proximal Policy Optimization with Evolutionary Mutations), a novel modification to PPO that introduces an adaptive exploration mechanism inspired by evolutionary algorithms. POEM enhances policy diversity by monitoring the Kullback-Leibler (KL) divergence between the current policy and a moving average of previous policies. When policy changes become minimal, indicating stagnation, POEM triggers an adaptive mutation of policy parameters to promote exploration. We evaluate POEM on four OpenAI Gym environments: CarRacing, MountainCar, BipedalWalker, and LunarLander. Through extensive fine-tuning using Bayesian optimization techniques and statistical testing using Welch&#x27;s t-test, we find that POEM significantly outperforms PPO on three of the four tasks (BipedalWalker: t=-2.0642, p=0.0495; CarRacing: t=-6.3987, p=0.0002; MountainCar: t=-6.2431, p&lt;0.0001), while performance on LunarLander is not statistically significant (t=-1.8707, p=0.0778). Our results highlight the potential of integrating evolutionary principles into policy gradient methods to overcome exploration-exploitation tradeoffs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于进化突变的近端策略优化算法</div>
<div class="mono" style="margin-top:8px">近端策略优化（PPO）是一种广泛使用的强化学习算法，以其稳定性和样本效率著称，但常因探索能力有限而陷入早熟收敛。本文提出POEM（基于进化突变的近端策略优化算法），这是一种受进化算法启发的自适应探索机制改进方案。POEM通过监控当前策略与历史策略移动平均之间的KL散度来增强策略多样性。当策略变化趋于停滞时，POEM会触发策略参数的自适应突变以促进探索。我们在四个OpenAI Gym环境（CarRacing、MountainCar、BipedalWalker、LunarLander）上评估POEM，通过贝叶斯优化技术进行精细调参，并采用韦尔奇t检验进行统计验证。实验表明POEM在四项任务中的三项显著优于PPO（BipedalWalker：t=-2.0642，p=0.0495；CarRacing：t=-6.3987，p=0.0002；MountainCar：t=-6.2431，p&lt;0.0001），而LunarLander任务未呈现统计显著性差异（t=-1.8707，p=0.0778）。本研究揭示了将进化原理融入策略梯度方法以平衡探索-利用权衡的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of premature convergence in Proximal Policy Optimization (PPO) due to limited exploration by proposing POEM, a novel algorithm that integrates evolutionary mutation principles. The method enhances policy diversity by monitoring KL divergence between the current policy and a moving average of past policies, triggering adaptive parameter mutations when stagnation is detected to encourage exploration. Experimental results on four OpenAI Gym environments, validated through Bayesian optimization and Welch&#x27;s t-test, show that POEM significantly outperforms standard PPO in three tasks (BipedalWalker, CarRacing, and MountainCar), while performance on LunarLander is not statistically significant, demonstrating the value of evolutionary mechanisms for improving exploration-exploitation tradeoffs in policy gradient methods.</div>
<div class="mono" style="margin-top:8px">本文针对近端策略优化（PPO）因探索有限而导致的早熟收敛问题，提出了POEM算法，该算法融合了进化突变思想以增强探索能力。该方法通过监控当前策略与历史策略移动平均之间的KL散度来评估策略多样性，在检测到停滞时触发策略参数的自适应突变以促进探索。在四个OpenAI Gym环境上的实验结果表明，经过贝叶斯优化调参和韦尔奇t检验统计验证，POEM在三个任务（BipedalWalker、CarRacing和MountainCar）上显著优于标准PPO，而在LunarLander任务上未呈现统计显著性，这揭示了将进化原理融入策略梯度方法对于平衡探索与利用的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization</div>
<div class="meta-line">Authors: Hanyu Li, Jiangshan Duo, Bofei Gao, Hailin Zhang, Sujian Li, Xiaotie Deng, Liang Zhao</div>
<div class="meta-line">First: 2025-12-19T06:30:54+00:00 · Latest: 2026-01-21T06:34:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06052v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06052v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-thought reasoning in large language models can trigger an &quot;overthinking trap&quot;: longer rollouts raise cost and latency yet often yield unreliable accuracy gains. Existing methods use global, static controls that may suppress needed reasoning. We propose mastery-gated, sample-level, soft reinforcement learning compression that penalizes long rollouts only when the model already solves the problem and has produced a shorter rollout. Across benchmarks, it cuts response length by 20-40% with comparable or higher accuracy and generalizes across domains: a model trained on math spontaneously shortens unseen tasks (code, instruction following, general-knowledge QA) without hurting accuracy. We further show two-way transfer between non-agent CoT and tool-use agents: non-agent training reduces SWE-Bench Verified rounds by 13%, while compressing a thinking agent cuts SWE trajectories by 67% tokens and 52% rounds and shortens non-agent outputs by up to 44%. Compression is thus not cosmetic brevity, but an inherent computation policy -- what to keep, and what to forget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的思维链压缩与单领域到全领域泛化方法</div>
<div class="mono" style="margin-top:8px">大语言模型中的思维链推理可能引发&quot;过度思考陷阱&quot;：更长的推理过程会增加计算成本和延迟，却往往无法带来可靠的准确率提升。现有方法采用全局静态控制机制，可能抑制必要的推理步骤。我们提出一种基于掌握度门控的样本级软强化学习压缩方法，仅在模型已解决问题且生成更短推理路径时才对长推理过程进行惩罚。在多个基准测试中，该方法将响应长度缩减20-40%，同时保持相当或更高的准确率，并实现跨领域泛化：在数学领域训练的模型能自发缩短未见任务（代码生成、指令跟随、常识问答）的推理长度而不损害准确率。研究进一步展示了非智能体思维链与工具使用智能体之间的双向迁移：非智能体训练使SWE-Bench验证轮次减少13%，而压缩思考型智能体可使SWE轨迹减少67%的标记量和52%的轮次，并将非智能体输出缩短达44%。这表明压缩并非表面简洁，而是一种内在的计算策略——决定保留什么与遗忘什么。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the &#x27;overthinking trap&#x27; in chain-of-thought reasoning, where longer reasoning sequences increase computational cost without reliably improving accuracy. The authors propose a mastery-gated, sample-level soft reinforcement learning method that compresses reasoning by penalizing long rollouts only when the model has already solved the problem with a shorter alternative. Experimental results show this approach reduces response length by 20-40% while maintaining or improving accuracy across benchmarks, and it demonstrates strong cross-domain generalization, such as training on math tasks leading to spontaneous compression in unseen domains like code and QA. Additionally, the method enables two-way transfer between non-agent and tool-use agents, significantly reducing token counts and rounds in tasks like SWE-Bench, indicating that compression acts as an inherent computation policy rather than mere brevity.</div>
<div class="mono" style="margin-top:8px">本文针对思维链推理中的&#x27;过度思考陷阱&#x27;问题，即较长的推理序列会增加计算成本却未必提升准确性。作者提出了一种基于掌握度门控、样本级软强化学习方法，仅在模型已用更短序列解决问题时才惩罚长推理路径，从而实现压缩。实验结果表明，该方法在多个基准测试中将响应长度减少了20-40%，同时保持或提高了准确性，并展现出强大的跨领域泛化能力，例如在数学任务上训练的模型能自发压缩未见过的代码和问答任务。此外，该方法实现了非智能体与工具使用智能体间的双向迁移，显著降低了SWE-Bench等任务中的令牌数量和轮次，表明压缩本质上是一种计算策略而非简单的简洁化处理。</div>
</details>
</div>
<div class="card">
<div class="title">GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes</div>
<div class="meta-line">Authors: Mohammad Pivezhandi, Mahdi Banisharif, Saeed Bakhshan, Abusayeed Saifullah, Ali Jannesari</div>
<div class="meta-line">First: 2025-12-12T23:46:05+00:00 · Latest: 2026-01-21T06:28:07+00:00</div>
<div class="meta-line">Comments: 49 pages, 4 figures, 19 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12091v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.12091v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous AI agents on embedded platforms require real-time, risk-aware scheduling under resource and thermal constraints. Classical heuristics struggle with workload irregularity, tabular regressors discard structural information, and model-free reinforcement learning (RL) risks overheating. We introduce GraphPerf-RT, a graph neural network surrogate achieving deep learning accuracy at heuristic speeds (2-7ms). GraphPerf-RT is, to our knowledge, the first to unify task DAG topology, CFG-derived code semantics, and runtime context (per-core DVFS, thermal state, utilization) in a heterogeneous graph with typed edges encoding precedence, placement, and contention. Evidential regression with Normal-Inverse-Gamma priors provides calibrated uncertainty; we validate on makespan prediction for risk-aware scheduling. Experiments on three ARM platforms (Jetson TX2, Orin NX, RUBIK Pi) achieve R^2 = 0.81 on log-transformed makespan with Spearman rho = 0.95 and conservative uncertainty calibration (PICP = 99.9% at 95% confidence). Integration with four RL methods demonstrates that multi-agent model-based RL with GraphPerf-RT as the world model achieves 66% makespan reduction and 82% energy reduction versus model-free baselines, with zero thermal violations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphPerf-RT：一种面向OpenMP代码硬件感知调度的图驱动性能模型</div>
<div class="mono" style="margin-top:8px">嵌入式平台上的自主AI智能体需要在资源与热约束下进行实时、风险感知的调度。传统启发式方法难以应对工作负载的不规则性，表格回归器会丢弃结构信息，而无模型强化学习（RL）存在过热风险。我们提出了GraphPerf-RT，这是一种图神经网络代理模型，能在启发式算法速度（2-7毫秒）下达到深度学习精度。据我们所知，GraphPerf-RT首次将任务DAG拓扑、CFG衍生的代码语义及运行时上下文（每核DVFS、热状态、利用率）统一于异构图中，其带类型边编码了执行顺序、任务放置和资源争用。采用正态-逆伽马先验的证据回归提供了校准的不确定性；我们通过完工时间预测验证了其风险感知调度能力。在三种ARM平台（Jetson TX2、Orin NX、RUBIK Pi）上的实验显示，对数转换后完工时间的R²=0.81，斯皮尔曼相关系数ρ=0.95，且不确定性校准保守（95%置信水平下PICP=99.9%）。与四种RL方法的集成表明，以GraphPerf-RT作为世界模型的多智能体模型化RL相比无模型基线实现了66%的完工时间缩减和82%的能耗降低，且零热违规。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for real-time, risk-aware scheduling of AI agents on resource-constrained embedded platforms, where classical heuristics and tabular models are inadequate, this paper introduces GraphPerf-RT, a graph neural network surrogate model. The method unifies task DAG topology, code semantics from control flow graphs, and runtime context into a heterogeneous graph with typed edges, and employs evidential regression for calibrated uncertainty estimation. Experimental results on three ARM platforms show high predictive accuracy (R²=0.81, Spearman ρ=0.95) and well-calibrated uncertainty; when integrated with reinforcement learning for scheduling, it achieves significant reductions in makespan (66%) and energy (82%) compared to model-free baselines, with no thermal violations.</div>
<div class="mono" style="margin-top:8px">本文的动机是为资源受限的嵌入式平台上AI代理的实时风险感知调度提供解决方案，以克服传统启发式方法和表格模型的不足。方法上提出了GraphPerf-RT，这是一种图神经网络代理模型，它将任务DAG拓扑、控制流图衍生的代码语义和运行时上下文统一到一个具有类型化边的异质图中，并采用证据回归进行校准的不确定性估计。在三个ARM平台上的实验结果表明，该模型具有较高的预测准确性（R²=0.81，Spearman ρ=0.95）和良好的不确定性校准；当与强化学习结合用于调度时，相比无模型基线，它能显著降低完工时间（66%）和能耗（82%），且无热违规。</div>
</details>
</div>
<div class="card">
<div class="title">CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation</div>
<div class="meta-line">Authors: Yutong Chen, Jiandong Gao, Ji Wu</div>
<div class="meta-line">First: 2026-01-21T06:17:52+00:00 · Latest: 2026-01-21T06:17:52+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14695v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14695v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&#x27;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&#x27;s reasoning ability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoScale-RL：通过协同扩展数据与计算实现高效后训练</div>
<div class="mono" style="margin-top:8px">训练大型推理模型通常不稳定且难以预测，尤其在处理难题或基础模型较弱时。我们发现当前的后训练扩展策略在这些情况下仍有改进空间。为此，我们提出CoScale-RL——一种具有更优数据与计算效率的新型扩展策略。该方法首先通过扩展解决方案使问题可解，其核心在于为每个问题收集多种解法，而非简单扩大数据集。随后，通过扩展推演计算以稳定强化学习过程。我们还采用名为“再蒸馏”的模型融合技术，在扩展时维持甚至提升计算效率。实验表明，该方法显著提升了数据与计算效率，在四个基准测试中平均准确率提升3.76倍。CoScale-RL能够在无需大量监督微调数据集的情况下拓展大型推理模型的能力边界，为提升模型推理能力提供了新的扩展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability and unpredictability in training Large Reasoning Models (LRMs) on hard problems or with weak foundation models, motivating the development of a more efficient post-training scaling strategy. The proposed CoScale-RL method enhances data efficiency by collecting multiple solutions per problem instead of merely enlarging the dataset, and improves computational efficiency by scaling up rollout computation to stabilize Reinforcement Learning, supplemented by a Re-distillation model merge technique. Experimental results demonstrate significant gains, with an average 3.76× accuracy improvement across four benchmarks, enabling LRMs to advance their reasoning ability boundaries without extensive supervised fine-tuning datasets.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型在困难问题或弱基础模型上训练不稳定和不可预测的问题，提出了一种更高效的训练后扩展策略。CoScale-RL方法通过为每个问题收集多个解决方案而非简单扩大数据集来提高数据效率，并通过扩展推演计算以稳定强化学习来提升计算效率，同时利用再蒸馏模型融合技术维持效率。实验结果表明，该方法在四个基准测试上平均实现了3.76倍的准确率提升，使大型推理模型能够在无需大量监督微调数据集的情况下突破其能力边界。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning</div>
<div class="meta-line">Authors: Jianwen Sun, Xinrui Li, Fuqing Li, Xiaoxuan Shen</div>
<div class="meta-line">First: 2026-01-21T06:08:37+00:00 · Latest: 2026-01-21T06:08:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14693v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14693v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越误差驱动优化：基于目标条件强化学习的经验驱动符号回归</div>
<div class="mono" style="margin-top:8px">符号回归旨在自动识别紧凑且可解释的数学表达式，以建模输入与输出变量之间的函数关系。现有大多数基于搜索的符号回归方法通常依赖拟合误差来指导搜索过程。然而，在庞大的表达式空间中，大量候选表达式可能具有相似的误差值，但结构差异显著，导致搜索方向模糊，阻碍收敛到潜在的真实函数。为解决这一挑战，我们提出名为EGRL-SR（面向符号回归的经验驱动目标条件强化学习）的新框架。与传统误差驱动方法不同，EGRL-SR引入新视角：利用精确的历史轨迹并优化动作价值网络，主动引导搜索过程，从而实现更稳健的表达式搜索。具体而言，我们将符号回归构建为目标条件强化学习问题，并引入后见经验回放机制，使动作价值网络能够从多样化的输入-输出对中泛化通用映射模式。此外，我们设计了全点满足二元奖励函数，促使动作价值网络关注结构模式而非低误差表达式，同时提出结构引导启发式探索策略以增强搜索多样性与空间覆盖。在公开基准测试上的实验表明，EGRL-SR在恢复率与鲁棒性方面持续优于现有先进方法，且能在相同搜索预算下恢复更复杂的表达式。消融实验结果验证了动作价值网络对搜索的有效引导，其中奖励函数与探索策略均发挥关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional error-based symbolic regression methods, which often face ambiguous search directions due to many structurally different expressions having similar fitting errors, this paper proposes EGRL-SR, a novel framework using goal-conditioned reinforcement learning with hindsight experience replay. The method formulates symbolic regression as a goal-conditioned RL problem, employing an all-point satisfaction binary reward function to focus on structural patterns and a structure-guided heuristic exploration strategy to improve search diversity. Experimental results on public benchmarks demonstrate that EGRL-SR outperforms state-of-the-art methods in recovery rate and robustness, successfully recovering more complex expressions under the same search budget, with ablation studies confirming the critical roles of the reward function and exploration strategy.</div>
<div class="mono" style="margin-top:8px">针对传统基于误差的符号回归方法因许多结构不同的表达式具有相似拟合误差而导致搜索方向模糊的问题，本文提出了EGRL-SR框架，采用目标条件强化学习与事后经验回放的新方法。该方法将符号回归构建为目标条件强化学习问题，设计了全点满足二元奖励函数以关注结构模式，并提出了结构引导的启发式探索策略以增强搜索多样性。在公开基准测试上的实验结果表明，EGRL-SR在恢复率和鲁棒性上均优于现有先进方法，能在相同搜索预算下恢复更复杂的表达式，消融研究验证了奖励函数和探索策略的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Reinforcement Learning in the Real World: A Survey of Challenges and Future Directions</div>
<div class="meta-line">Authors: Asim H. Gazi, Yongyi Guo, Daiqi Gao, Ziping Xu, Kelly W. Zhang, Susan A. Murphy</div>
<div class="meta-line">First: 2026-01-21T04:58:49+00:00 · Latest: 2026-01-21T04:58:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15353v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15353v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has achieved remarkable success in real-world decision-making across diverse domains, including gaming, robotics, online advertising, public health, and natural language processing. Despite these advances, a substantial gap remains between RL research and its deployment in many practical settings. Two recurring challenges often underlie this gap. First, many settings offer limited opportunity for the agent to interact extensively with the target environment due to practical constraints. Second, many target environments often undergo substantial changes, requiring redesign and redeployment of RL systems (e.g., advancements in science and technology that change the landscape of healthcare delivery). Addressing these challenges and bridging the gap between basic research and application requires theory and methodology that directly inform the design, implementation, and continual improvement of RL systems in real-world settings.
  In this paper, we frame the application of RL in practice as a three-component process: (i) online learning and optimization during deployment, (ii) post- or between-deployment offline analyses, and (iii) repeated cycles of deployment and redeployment to continually improve the RL system. We provide a narrative review of recent advances in statistical RL that address these components, including methods for maximizing data utility for between-deployment inference, enhancing sample efficiency for online learning within-deployment, and designing sequences of deployments for continual improvement. We also outline future research directions in statistical RL that are use-inspired -- aiming for impactful application of RL in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现实世界中的统计强化学习：挑战与未来方向综述</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在游戏、机器人、在线广告、公共卫生和自然语言处理等多个领域的现实世界决策中取得了显著成功。尽管取得了这些进展，但RL研究与其在许多实际场景中的部署之间仍存在巨大差距。这一差距通常源于两个反复出现的挑战：首先，由于实际限制，许多场景中智能体与目标环境进行广泛交互的机会有限；其次，许多目标环境经常发生重大变化，需要重新设计和重新部署RL系统（例如，科技进步改变了医疗服务的格局）。要应对这些挑战并弥合基础研究与应用之间的差距，需要能够直接指导RL系统在现实环境中设计、实施和持续改进的理论与方法。
本文提出将RL在实际中的应用构建为一个三部分过程：（i）部署期间的在线学习与优化，（ii）部署后或部署间的离线分析，以及（iii）通过重复的部署与再部署周期持续改进RL系统。我们对统计RL中针对这些部分的最新进展进行了叙述性综述，包括最大化数据效用以支持部署间推断的方法、提升部署内在线学习样本效率的方法，以及设计持续改进的部署序列的方法。我们还概述了统计RL中未来以应用为导向的研究方向，旨在推动RL在实践中产生深远影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey addresses the gap between reinforcement learning (RL) research and real-world deployment, motivated by challenges such as limited environmental interaction and the need for systems to adapt to changing conditions. The authors frame practical RL application as a three-component process involving online learning during deployment, offline analysis between deployments, and repeated cycles for continual improvement, reviewing statistical RL methods that enhance data utility, sample efficiency, and deployment sequencing. The main experimental results highlight advances in maximizing data utility for inference, improving online learning efficiency, and designing deployment sequences, while outlining future use-inspired research directions to bridge theory and practice.</div>
<div class="mono" style="margin-top:8px">本综述针对强化学习研究与应用部署之间的差距，其动机源于现实挑战，如环境交互受限以及系统需适应不断变化的条件。作者将实际应用框架化为三个组成部分：部署期间的在线学习与优化、部署后的离线分析以及为持续改进而重复的部署周期，回顾了统计强化学习方法以提升数据效用、样本效率和部署序列设计。主要实验结果强调了在最大化数据推断效用、提高在线学习效率和设计部署序列方面的进展，同时概述了未来以应用为导向的研究方向，旨在弥合理论与实践的鸿沟。</div>
</details>
</div>
<div class="card">
<div class="title">MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks</div>
<div class="meta-line">Authors: Zixuan Ke, Yifei Ming, Austin Xu, Ryan Chin, Xuan-Phi Nguyen, Prathyusha Jwalapuram, Semih Yavuz, Caiming Xiong, Shafiq Joty</div>
<div class="meta-line">First: 2026-01-21T04:57:02+00:00 · Latest: 2026-01-21T04:57:02+00:00</div>
<div class="meta-line">Comments: Preprint; Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14652v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14652v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAS-Orchestra：通过整体编排与受控基准理解并提升多智能体推理能力</div>
<div class="mono" style="margin-top:8px">尽管多智能体系统（MAS）通过智能体协作展现出更高智能潜力，但当前自动设计方法效果有限。其不足源于两个关键因素：（1）方法复杂性——现有编排采用顺序、代码级执行，限制了全局系统级整体推理，且难以随智能体复杂度扩展；（2）效能不确定性——部署MAS时缺乏与单智能体系统（SAS）相比的实际效益评估。我们提出MAS-Orchestra，一种训练时框架，将MAS编排建模为具有整体编排能力的函数调用强化学习问题，可一次性生成完整MAS。该框架将复杂的目标导向子智能体抽象为可调用函数，在隐藏内部执行细节的同时实现系统结构的全局推理。为严谨探究MAS何时及为何有效，我们提出MASBENCH受控基准，从深度、跨度、广度、并行性和鲁棒性五个维度刻画任务特性。分析表明，MAS的增益关键取决于任务结构、验证协议及编排器与子智能体能力，而非普遍适用。基于这些发现，MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准上取得持续提升。MAS-Orchestra与MASBENCH共同为追求多智能体智能提供了更优的训练与理解框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underperformance of current multi-agent systems (MAS) due to methodological complexity and uncertain efficacy compared to single-agent systems, this paper introduces MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration to generate entire systems at once, abstracting sub-agents as callable functions for global reasoning. To rigorously assess MAS benefits, the authors also propose MASBENCH, a controlled benchmark characterizing tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Experimental results reveal that MAS gains depend critically on task structure, verification protocols, and agent capabilities rather than being universal, and guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA.</div>
<div class="mono" style="margin-top:8px">针对当前多智能体系统（MAS）因方法复杂性和相比单智能体系统效能不确定而表现不佳的问题，本文提出了MAS-Orchestra，这是一个训练时框架，将MAS编排表述为具有整体编排功能的函数调用强化学习问题，一次性生成整个系统，并将面向目标的子智能体抽象为可调用函数以实现全局推理。为了严格评估MAS的优势，作者还提出了MASBENCH，这是一个受控基准，从深度、跨度、广度、并行性和鲁棒性五个维度刻画任务。实验结果表明，MAS的增益关键取决于任务结构、验证协议和智能体能力，而非普遍有效；基于这些洞察，MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准上取得了持续改进。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Reasoning for Cold-Start Item Recommendation</div>
<div class="meta-line">Authors: Shijun Li, Yu Wang, Jin Wang, Ying Li, Joydeep Ghosh, Anne Cocos</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2025-11-23T03:22:53+00:00 · Latest: 2026-01-21T04:46:33+00:00</div>
<div class="meta-line">Comments: Published on Proceedings of the ACM on Web Conference 2026 (WWW 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18261v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18261v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix&#x27;s production ranking model by up to 8% in certain cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向冷启动物品推荐的LLM推理方法</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）凭借其固有的推理能力和广泛的知识库，在改进推荐系统方面展现出巨大潜力。然而，现有研究主要关注具有丰富用户-物品交互数据的温启动场景，对于交互稀疏、传统协同过滤方法受限的冷启动场景则探索不足。为突破这一局限，我们针对Netflix领域提出专为冷启动物品推荐设计的新型推理策略。该方法利用LLMs的高级推理能力有效推断用户偏好，尤其适用于新引入或交互稀少的物品。我们系统评估了监督微调、基于强化学习的微调以及融合两种方法的混合策略，以优化推荐性能。基于真实数据的大量实验表明，该方法在冷启动推荐场景中显著提升了方法效能与实际表现。值得注意的是，基于推理的微调模型在特定情况下比Netflix生产级排序模型性能提升高达8%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underexplored challenge of cold-start item recommendation, where sparse user-item interactions hinder traditional methods, by leveraging the reasoning capabilities of Large Language Models (LLMs) to infer user preferences for new or rarely interacted items. The proposed method involves developing and evaluating novel reasoning strategies, including supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches, specifically within the Netflix domain. Experimental results on real-world data show that these reasoning-based fine-tuned models achieve significant performance improvements in cold-start scenarios, even outperforming Netflix&#x27;s production ranking model by up to 8% in certain cases.</div>
<div class="mono" style="margin-top:8px">本文针对冷启动物品推荐中用户-物品交互稀疏、传统协同过滤方法效果受限的挑战，利用大语言模型的内在推理能力和知识库来推断用户对新引入或交互稀少物品的偏好。所提出的方法在Netflix领域设计了新颖的推理策略，系统评估了监督微调、基于强化学习的微调以及两者结合的混合方法，以优化推荐性能。在真实数据上的大量实验表明，这些基于推理的微调模型在冷启动推荐场景中取得了显著的性能提升，在某些情况下甚至比Netflix的生产排序模型性能高出8%。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Reinforcement Learning from Neural Feedback: Mapping fNIRS Signals to Agent Performance</div>
<div class="meta-line">Authors: Julia Santaniello, Matthew Russell, Benson Jiang, Donatello Sassaroli, Robert Jacob, Jivko Sinapov</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-17T00:21:46+00:00 · Latest: 2026-01-21T04:22:56+00:00</div>
<div class="meta-line">Comments: Accepted to the Association for the Advancement of Artificial Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12844v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.12844v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating user feedback into the agent&#x27;s training process. This paper introduces a framework that guides agent training through implicit neural signals, with a focus on the neural classification problem. Our work presents and releases a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train multiple classifiers to predict varying levels of agent performance (optimal, suboptimal, or worst-case) from windows of preprocessed fNIRS features, achieving an average F1 score of 67% for binary and 46% for multi-class classification across conditions and domains. We also train multiple regressors to predict the degree of deviation between an agent&#x27;s chosen action and a set of near-optimal policy actions, providing a continuous measure of performance. Finally, we evaluate cross-subject generalization and show that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our results demonstrate that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future Reinforcement Learning from Neural Feedback (RLNF) systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向基于神经反馈的强化学习：将fNIRS信号映射至智能体性能</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）通过将用户反馈整合到智能体训练过程中，使其行为与人类偏好保持一致。本文提出一种通过隐式神经信号指导智能体训练的框架，重点关注神经信号分类问题。我们构建并发布了一个新颖的功能性近红外光谱（fNIRS）数据集，采集自25名人类参与者在三个领域（抓取放置机器人、月球着陆器、Flappy Bird）的神经信号记录。我们训练了多个分类器，通过预处理后的fNIRS特征窗口预测智能体性能的不同等级（最优、次优、最差），在跨条件与跨领域实验中，二分类与多分类的平均F1分数分别达到67%和46%。同时训练了多个回归器，用于预测智能体所选动作与近似最优策略动作之间的偏差程度，提供连续的性能度量。最后，我们评估了跨被试泛化能力，结果表明使用少量被试特定数据对预训练模型进行微调，可使二分类与多分类模型的平均F1分数分别提升17%和41%。我们的研究证明，将隐式fNIRS信号映射至智能体性能具有可行性且可优化，为未来基于神经反馈的强化学习（RLNF）系统奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the goal of advancing Reinforcement Learning from Human Feedback (RLHF) by using implicit neural signals instead of explicit human input to guide agent training. The method involves creating and releasing a novel fNIRS dataset from 25 participants across three task domains, and training classifiers and regressors to map preprocessed neural signal windows to discrete levels of agent performance (optimal, suboptimal, worst) or to a continuous measure of action deviation from a near-optimal policy. The main experimental results show that the classifiers achieved an average F1 score of 67% for binary and 46% for multi-class prediction, and that fine-tuning models with limited subject-specific data significantly improved cross-subject generalization, increasing F1 scores by 17% and 41% for the respective binary and multi-class models, thereby demonstrating the feasibility of a future Reinforcement Learning from Neural Feedback (RLNF) paradigm.</div>
<div class="mono" style="margin-top:8px">本文的动机是通过使用隐式神经信号而非显式人类输入来指导智能体训练，从而推进基于人类反馈的强化学习（RLHF）。其方法包括创建并发布一个来自25名参与者在三个任务领域的新型功能性近红外光谱（fNIRS）数据集，并训练分类器和回归器，将预处理后的神经信号窗口映射到智能体性能的离散级别（最优、次优、最差）或映射到与接近最优策略的动作偏差的连续度量。主要实验结果表明，分类器在二分类和多分类预测中分别达到了平均67%和46%的F1分数，并且使用有限的特定被试数据进行微调显著改善了跨被试泛化能力，使二分类和多分类模型的F1分数分别提高了17%和41%，从而证明了未来基于神经反馈的强化学习（RLNF）系统的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning</div>
<div class="meta-line">Authors: Cheol-Hui Lee, Hwa-Yeon Lee, Dong-Joo Kim</div>
<div class="meta-line">First: 2026-01-20T13:38:01+00:00 · Latest: 2026-01-21T03:55:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13964v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13964v2">PDF</a> · <a href="https://github.com/dlcjfgmlnasa/RL-BioAug">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10%) of labeled data to guide the agent&#x27;s policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69% and 8.80% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task--for example, Time Masking with a 62% probability for sleep stage classification and Crop &amp; Resize with a 77% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at https://github.com/dlcjfgmlnasa/RL-BioAug.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RL-BioAug：用于自监督脑电表征学习的标签高效强化学习</div>
<div class="mono" style="margin-top:8px">数据增强的质量是脑电任务中对比学习性能的关键决定因素。尽管该范式在利用未标记数据方面前景广阔，但由于脑电信号的非平稳性（统计特性随时间变化），静态或随机增强策略往往难以保留内在信息。为此，我们提出RL-BioAug框架，利用标签高效强化学习（RL）智能体自主确定最优增强策略。该方法仅使用少量（10%）标记数据指导智能体策略，使编码器能以严格自监督方式学习鲁棒表征。实验结果表明，RL-BioAug显著优于随机选择策略，在Sleep-EDFX和CHB-MIT数据集上的Macro-F1分数分别大幅提升9.69%和8.80%。值得注意的是，该智能体主要针对各任务选择最优策略——例如睡眠分期任务以62%概率选择时间掩蔽，癫痫检测任务以77%概率选择裁剪重缩放。本框架有望替代传统基于启发式的增强方法，建立自主数据增强新范式。源代码发布于https://github.com/dlcjfgmlnasa/RL-BioAug。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static or random data augmentation strategies for non-stationary EEG signals in contrastive learning, this paper proposes RL-BioAug, a framework that uses a label-efficient reinforcement learning agent to autonomously learn optimal augmentation policies. The method requires only 10% of labeled data to guide the RL agent, enabling the encoder to learn robust representations in a self-supervised manner. Experimental results on the Sleep-EDFX and CHB-MIT datasets show the framework significantly outperforms random augmentation, achieving improvements of 9.69% and 8.80% in Macro-F1 score, with the agent learning distinct optimal strategies like Time Masking for sleep stage classification and Crop &amp; Resize for seizure detection.</div>
<div class="mono" style="margin-top:8px">针对脑电图信号的非平稳特性导致静态或随机数据增强在对比学习中效果有限的问题，本文提出了RL-BioAug框架，其利用标签高效的强化学习代理自主学习最优增强策略。该方法仅需10%的标注数据来指导代理，使编码器能以自监督方式学习鲁棒表征。在Sleep-EDFX和CHB-MIT数据集上的实验结果表明，该框架显著优于随机增强策略，宏F1分数分别提升了9.69%和8.80%，且代理学会了针对不同任务的最优策略，例如在睡眠分期中62%概率选择时间掩码，在癫痫检测中77%概率选择裁剪与调整大小。</div>
</details>
</div>
<div class="card">
<div class="title">SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation</div>
<div class="meta-line">Authors: Xichen Zhang, Ziyi He, Yinghao Zhu, Sitong Wu, Shaozuo Yu, Meng Chu, Wenhu Zhang, Haoru Tan, Jiaya Jia</div>
<div class="meta-line">First: 2026-01-21T03:16:17+00:00 · Latest: 2026-01-21T03:16:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14615v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14615v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SearchGym：通过高保真低成本环境仿真引导现实世界搜索智能体</div>
<div class="mono" style="margin-top:8px">搜索智能体已成为解决开放式知识密集型推理任务的关键范式。然而，通过强化学习训练这类智能体面临核心矛盾：调用实时商业网络API成本过高，而依赖静态数据快照常因数据错位引入噪声。这种错位会产生扭曲的奖励信号，通过惩罚正确推理或奖励幻觉行为破坏训练稳定性。为此，我们提出SearchGym仿真环境，旨在引导鲁棒的搜索智能体。该环境采用严谨的生成流程构建可验证知识图谱与对齐文档库，确保每个推理任务均基于事实且严格可解。基于此可控环境，我们进一步提出SearchGym-RL课程学习方法，通过净化反馈逐步优化智能体策略，从基础交互演进至复杂长程规划。在Llama和Qwen系列模型上的大量实验显示出优异的仿真到现实泛化能力。值得注意的是，在SearchGym中训练的Qwen2.5-7B-Base模型在九项多样化基准测试中，平均相对超越网络增强型ASearcher基线10.6%。实验结果验证了高保真仿真可作为扩展性强、成本效益高的搜索智能体开发方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training search agents via Reinforcement Learning, where direct interaction with live web APIs is costly and static data snapshots often yield misaligned, noisy rewards that destabilize training. To overcome this, the authors introduce SearchGym, a simulation environment that constructs a verifiable knowledge graph and aligned document corpus to ensure tasks are factually grounded and solvable, alongside SearchGym-RL, a curriculum learning method that progressively optimizes agent policies with purified feedback. Experimental results on models like Llama and Qwen show strong sim-to-real generalization, with the trained Qwen2.5-7B-Base model outperforming the ASearcher baseline by an average of 10.6% across nine benchmarks, validating the approach as a scalable and cost-effective solution.</div>
<div class="mono" style="margin-top:8px">本文针对通过强化学习训练搜索代理的挑战，即直接与实时网络API交互成本高昂，而静态数据快照常导致数据错位，产生噪声奖励信号，从而破坏训练稳定性。为解决此问题，作者提出了SearchGym，这是一个模拟环境，通过构建可验证的知识图谱和对齐的文档语料库，确保任务基于事实且可严格求解，并引入了SearchGym-RL课程学习方法，利用纯化反馈逐步优化代理策略。在Llama和Qwen等模型上的实验显示了强大的模拟到现实泛化能力，训练后的Qwen2.5-7B-Base模型在九个不同基准测试中平均相对优于ASearcher基线10.6%，验证了该方法作为可扩展且高成本效益解决方案的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Context-Picker: Dynamic context selection using multi-stage reinforcement learning</div>
<div class="meta-line">Authors: Siyuan Zhu, Chengdong Xu, Kaiqiang Ke, Chao Yu</div>
<div class="meta-line">First: 2025-12-16T14:52:11+00:00 · Latest: 2026-01-21T02:41:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14465v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.14465v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In long-context question answering, selecting the appropriate scope of context for a query remains a key and unresolved challenge. Insufficient context can lead to missing essential information, whereas excessive context often introduces noise and degrades answer quality. Conventional methods, such as retrieving a fixed number of passages or applying reranking, struggle to dynamically determine which context to include. This is especially problematic for factoid questions, which typically depend only on a few precise pieces of evidence. To overcome this limitation, we propose Context-Picker, a reasoning-aware framework that reframes context selection as the task of identifying a minimal sufficient evidence subset, moving beyond conventional similarity-based ranking. Context-Picker uses a human-inspired two-stage reinforcement learning schedule: stage 1 focuses on improving the recall rate of critical passages, and stage 2 prioritizes pruning redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines ``minimal sufficient sets&quot; via a Leave-One-Out (LOO) procedure, providing dense and task-aligned supervision. Experiments on five long-context and multi-hop QA datasets demonstrate that our method outperforms strong RAG baselines and achieved higher answer accuracy. Ablation studies also indicate that our coarse-to-fine optimization schedule, the redundancy-aware reward shaping, along with the rationale generated by the policy, all contribute substantially to these gains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Context-Picker：基于多阶段强化学习的动态上下文选择方法</div>
<div class="mono" style="margin-top:8px">在长上下文问答中，为查询选择合适的上下文范围仍是关键且未解决的挑战。上下文不足可能导致遗漏关键信息，而过量上下文常引入噪声并降低答案质量。传统方法（如检索固定数量段落或重排序）难以动态确定应包含的上下文，这对通常仅依赖少量精确证据的事实型问题尤为突出。为突破此限制，我们提出Context-Picker——一个将上下文选择重构为识别最小充分证据子集任务的推理感知框架，超越了传统基于相似度的排序方法。该框架采用受人类启发的两阶段强化学习方案：第一阶段聚焦提升关键段落召回率，第二阶段优先剪枝冗余以提炼紧凑证据集。为解决奖励稀疏性问题，我们设计了通过留一法挖掘“最小充分集”的离线证据蒸馏流程，提供密集且任务对齐的监督。在五个长上下文与多跳问答数据集上的实验表明，本方法优于强RAG基线且获得更高答案准确率。消融研究证实，从粗到细的优化方案、冗余感知的奖励塑造以及策略生成的推理依据均对性能提升有实质贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of dynamically selecting the optimal context scope for long-context question answering, where fixed retrieval or reranking methods often include either insufficient or noisy information, particularly harming factoid questions that rely on precise evidence. The proposed Context-Picker framework reframes context selection as identifying a minimal sufficient evidence subset, employing a two-stage reinforcement learning approach: the first stage improves recall of critical passages, and the second prunes redundancy to distill a compact set, supported by an offline evidence distillation pipeline using a Leave-One-Out procedure to provide dense supervision. Experimental results on five long-context and multi-hop QA datasets show that the method outperforms strong RAG baselines in answer accuracy, with ablations confirming the contributions of the coarse-to-fine optimization, redundancy-aware reward shaping, and policy-generated rationales.</div>
<div class="mono" style="margin-top:8px">本文针对长上下文问答中动态选择最佳上下文范围的挑战，传统固定检索或重排序方法常包含信息不足或噪声，尤其损害依赖精确证据的事实型问题。提出的Context-Picker框架将上下文选择重构为识别最小充分证据子集的任务，采用两阶段强化学习方法：第一阶段提升关键段落的召回率，第二阶段剪枝冗余以提炼紧凑证据集，并通过基于留一法的离线证据蒸馏流程提供密集监督。在五个长上下文和多跳问答数据集上的实验表明，该方法在答案准确性上优于强RAG基线，消融研究证实了由粗到细的优化调度、冗余感知奖励塑造以及策略生成的理由均对性能提升有重要贡献。</div>
</details>
</div>
<div class="card">
<div class="title">A Finite Expression Method for Solving High-Dimensional Committor Problems</div>
<div class="meta-line">Authors: Zezheng Song, Maria K. Cameron, Haizhao Yang</div>
<div class="meta-line">First: 2023-06-21T13:43:59+00:00 · Latest: 2026-01-21T01:30:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2306.12268v3">Abs</a> · <a href="https://arxiv.org/pdf/2306.12268v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transition path theory (TPT) is a mathematical framework for quantifying rare transition events between a pair of selected metastable states $A$ and $B$. Central to TPT is the committor function, which describes the probability to hit the metastable state $B$ prior to $A$ from any given starting point of the phase space. Once the committor is computed, the transition channels and the transition rate can be readily found. The committor is the solution to the backward Kolmogorov equation with appropriate boundary conditions. However, solving it is a challenging task in high dimensions due to the need to mesh a whole region of the ambient space. In this work, we explore the finite expression method (FEX, Liang and Yang (2022)) as a tool for computing the committor. FEX approximates the committor by an algebraic expression involving a fixed finite number of nonlinear functions and binary arithmetic operations. The optimal nonlinear functions, the binary operations, and the numerical coefficients in the expression template are found via reinforcement learning. The FEX-based committor solver is tested on several high-dimensional benchmark problems. It gives comparable or better results than neural network-based solvers. Most importantly, FEX is capable of correctly identifying the algebraic structure of the solution which allows one to reduce the committor problem to a low-dimensional one and find the committor with any desired accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>求解高维承诺子问题的有限表达式方法</div>
<div class="mono" style="margin-top:8px">过渡路径理论（TPT）是一种量化选定亚稳态A与B之间稀有跃迁事件的数学框架。其核心是承诺子函数，该函数描述了从相空间任意起点出发、在到达A之前首次抵达B的概率。一旦计算出承诺子，即可直接得到跃迁通道和跃迁速率。承诺子是满足特定边界条件的后向柯尔莫哥洛夫方程的解，但在高维空间中求解极具挑战性，因为需要对整个环境空间区域进行网格剖分。本研究探索了有限表达式方法（FEX，Liang与Yang，2022）作为计算承诺子的工具。FEX通过包含固定数量非线性函数和二元算术运算的代数表达式来逼近承诺子，并利用强化学习优化表达式模板中的非线性函数、二元运算及数值系数。基于FEX的承诺子求解器在多个高维基准问题上进行了测试，其结果与基于神经网络的求解器相当或更优。最重要的是，FEX能准确识别解的代数结构，从而将承诺子问题降维处理，并以任意所需精度求解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of solving high-dimensional committor problems in transition path theory, which are essential for quantifying rare transitions between metastable states, this paper explores the finite expression method (FEX) as a computational tool. The method approximates the committor function using algebraic expressions with a fixed number of nonlinear functions and binary operations, optimized via reinforcement learning to determine the structure and coefficients. Experimental results on high-dimensional benchmarks show that FEX achieves comparable or superior performance to neural network-based solvers and can correctly identify the solution&#x27;s algebraic structure, enabling reduction to low-dimensional problems and high-accuracy committor computation.</div>
<div class="mono" style="margin-top:8px">针对过渡路径理论中高维承诺子问题求解的挑战，该研究探索了有限表达式方法作为计算工具。该方法通过固定数量的非线性函数和二元运算的代数表达式来近似承诺子函数，并利用强化学习优化表达式结构和系数。在高维基准测试中，该方法取得了与基于神经网络的求解器相当或更好的结果，并能正确识别解的代数结构，从而将问题降至低维并实现高精度计算。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforced Inverse Scattering</div>
<div class="meta-line">Authors: Hanyang Jiang, Yuehaw Khoo, Haizhao Yang</div>
<div class="meta-line">First: 2022-06-08T22:56:09+00:00 · Latest: 2026-01-21T01:30:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2206.04186v3">Abs</a> · <a href="https://arxiv.org/pdf/2206.04186v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse wave scattering aims at determining the properties of an object using data on how the object scatters incoming waves. In order to collect information, sensors are put in different locations to send and receive waves from each other. The choice of sensor positions and incident wave frequencies determines the reconstruction quality of scatterer properties. This paper introduces reinforcement learning to develop precision imaging that decides sensor positions and wave frequencies adaptive to different scatterers in an intelligent way, thus obtaining a significant improvement in reconstruction quality with limited imaging resources. Extensive numerical results will be provided to demonstrate the superiority of the proposed method over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化逆散射</div>
<div class="mono" style="margin-top:8px">逆波散射旨在利用物体散射入射波的数据来确定其特性。为收集信息，传感器被置于不同位置以相互发送和接收波。传感器位置与入射波频率的选择决定了散射体特性的重建质量。本文引入强化学习，开发出一种能智能适应不同散射体的自适应传感器位置与波频率决策的精密成像方法，从而在有限成像资源下显著提升重建质量。大量数值结果将验证所提方法相较于现有方法的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge in inverse wave scattering of optimizing limited sensor positions and wave frequencies to improve the reconstruction quality of scatterer properties. It introduces a reinforcement learning method that adaptively and intelligently selects these imaging parameters tailored to different scatterers. Experimental results from extensive numerical simulations demonstrate that this approach significantly outperforms existing methods, achieving superior reconstruction quality with constrained imaging resources.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决逆波散射中如何优化有限的传感器位置和波频率以提高散射体特性重建质量的挑战。它引入了一种强化学习方法，能够自适应地、智能地为不同散射体选择这些成像参数。大量数值实验结果表明，该方法在有限成像资源下显著优于现有方法，实现了更优的重建质量。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability</div>
<div class="meta-line">Authors: Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang</div>
<div class="meta-line">First: 2026-01-14T07:52:14+00:00 · Latest: 2026-01-21T00:48:31+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures. Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09261v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09261v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner&#x27;s own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner&#x27;s internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学会信任经验：不可观测反馈可靠性下的学习监控-信任-调节框架</div>
<div class="mono" style="margin-top:8px">在反馈可靠性不可观测的条件下学习，提出了超越优化鲁棒性的独特挑战：系统必须决定是否从经验中学习，而不仅仅是学习如何稳定。我们将此设定研究为不可观测可靠性下的认知可识别性，其中每个经验具有潜在可信度，可靠与不可靠反馈在局部可能无法区分，且数据由学习者自身不断演变的信念与行为在闭环中生成。在此框架下，标准鲁棒学习可能稳定收敛，却形成高置信度但系统错误的信念。
我们提出元认知调节作为实用应对方案：一个内省的二级控制环路，从学习者内部动态的内生证据中推断经验可信度。我们将其形式化为模块化的监控-信任-调节分解，并通过自诊断机制实例化——该机制维护缓慢变化的经验信任变量，以软性调节学习更新，无需外部可靠性标签或显式损坏模型。
实证研究表明，在本文探讨的不可观测可靠性机制中，自诊断与认知可识别性的提升相关。在强化学习中，它实现了校准式怀疑能力，并在系统损坏的奖励下恢复学习；在监督学习中，它揭示了关键分离现象：性能恢复不意味着认知恢复。准确率可能回升，而内部信念动态仍被早期误导数据锁定，这种失败仅能通过内省诊断检测。监控-信任-调节框架与自诊断共同为不可观测可靠性下的自主学习提供了系统性抽象框架与具体设计模板。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of learning when feedback reliability is unobservable, a problem termed Epistemic Identifiability under Unobservable Reliability (EIUR), where standard robust methods can converge to confidently wrong beliefs. The proposed solution is a metacognitive Monitor-Trust-Regulator (MTR) framework, which uses introspective self-diagnosis to infer experience credibility from the learner&#x27;s internal dynamics, softly modulating updates without external labels. Experimental results in reinforcement and supervised learning show that this approach improves epistemic identifiability, enabling calibrated skepticism and revealing that performance recovery can mask persistent belief errors detectable only through introspection.</div>
<div class="mono" style="margin-top:8px">该论文研究了在反馈可靠性不可观测情况下的学习挑战，即不可观测可靠性下的认知可识别性问题，其中标准鲁棒方法可能稳定地收敛到错误信念。提出的解决方案是一个元认知的监控-信任-调节器框架，通过内省式自我诊断从学习者内部动态推断经验可信度，无需外部标签即可软调节学习更新。在强化学习和监督学习中的实验结果表明，该方法提高了认知可识别性，实现了校准的怀疑态度，并揭示了性能恢复可能掩盖仅通过内省才能检测到的持久信念错误。</div>
</details>
</div>
<div class="card">
<div class="title">Deceptive Sequential Decision-Making via Regularized Policy Optimization</div>
<div class="meta-line">Authors: Yerin Kim, Alexander Benvenuti, Bo Chen, Mustafa Karabag, Abhishek Kulkarni, Nathaniel D. Bastian, Ufuk Topcu, Matthew Hale</div>
<div class="meta-line">First: 2025-01-30T23:41:40+00:00 · Latest: 2026-01-21T00:03:43+00:00</div>
<div class="meta-line">Comments: 18 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.18803v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.18803v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous systems are increasingly expected to operate in the presence of adversaries, though adversaries may infer sensitive information simply by observing a system. Therefore, present a deceptive sequential decision-making framework that not only conceals sensitive information, but actively misleads adversaries about it. We model autonomous systems as Markov decision processes, with adversaries using inverse reinforcement learning to recover reward functions. To counter them, we present three regularization strategies for policy synthesis problems that actively deceive an adversary about a system&#x27;s reward. ``Diversionary deception&#x27;&#x27; leads an adversary to draw any false conclusion about the system&#x27;s reward function. ``Targeted deception&#x27;&#x27; leads an adversary to draw a specific false conclusion about the system&#x27;s reward function. ``Equivocal deception&#x27;&#x27; leads an adversary to infer that the real reward and a false reward both explain the system&#x27;s behavior. We show how each form of deception can be implemented in policy optimization problems and analytically bound the loss in total accumulated reward induced by deception. Next, we evaluate these developments in a multi-agent setting. We show that diversionary, targeted, and equivocal deception all steer the adversary to false beliefs while still attaining a total accumulated reward that is at least 98% of its optimal, non-deceptive value.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于正则化策略优化的欺骗性序贯决策</div>
<div class="mono" style="margin-top:8px">自主系统日益需要在对抗环境中运行，而对手仅通过观察系统即可推断敏感信息。为此，本文提出一种欺骗性序贯决策框架，不仅能隐藏敏感信息，还能主动误导对手。我们将自主系统建模为马尔可夫决策过程，假设对手使用逆强化学习来恢复奖励函数。为应对此威胁，我们提出三种策略综合问题的正则化策略，可主动误导对手对系统奖励的认知：&#x27;转移性欺骗&#x27;使对手对系统奖励函数得出任意错误结论；&#x27;定向欺骗&#x27;使对手对系统奖励函数得出特定错误结论；&#x27;模糊性欺骗&#x27;使对手推断真实奖励与虚假奖励均能解释系统行为。我们展示了每种欺骗形式在策略优化问题中的实现方法，并通过解析界定了欺骗行为导致的总累积奖励损失。随后在多智能体环境中评估这些方法，证明转移性、定向性和模糊性欺骗均能引导对手形成错误信念，同时使总累积奖励至少达到最优非欺骗值的98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for autonomous systems to protect sensitive information from adversarial inference, this paper introduces a deceptive sequential decision-making framework that actively misleads adversaries about a system&#x27;s true objectives. The method models systems as Markov decision processes and counters adversaries using inverse reinforcement learning by incorporating three regularization strategies into policy optimization: diversionary, targeted, and equivocal deception, each designed to manipulate adversary beliefs differently. Experimental results demonstrate that all three deception strategies successfully steer adversaries to false conclusions while maintaining at least 98% of the optimal non-deceptive reward, with analytical bounds provided on the reward loss induced by deception.</div>
<div class="mono" style="margin-top:8px">本文针对自主系统需保护敏感信息免受对手推断的问题，提出了一种欺骗性序贯决策框架，旨在主动误导对手对系统真实目标的理解。方法将系统建模为马尔可夫决策过程，并通过在策略优化中引入三种正则化策略来应对使用逆强化学习的对手：转移性欺骗、目标性欺骗和模糊性欺骗，每种策略以不同方式操纵对手信念。实验结果表明，所有欺骗策略均能成功引导对手得出错误结论，同时保持至少98%的最优非欺骗奖励，并分析了欺骗导致的奖励损失的理论界限。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Execution-Grounded Automated AI Research</div>
<div class="meta-line">Authors: Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Candès, Diyi Yang, Tatsunori Hashimoto</div>
<div class="meta-line">First: 2026-01-20T22:35:44+00:00 · Latest: 2026-01-20T22:35:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14525v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14525v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向执行驱动的自动化人工智能研究</div>
<div class="mono" style="margin-top:8px">自动化人工智能研究在加速科学发现方面潜力巨大，但当前大型语言模型常生成看似合理却无效的构想。执行驱动可能有助于解决此问题，但自动化执行的可行性及模型能否从执行反馈中学习尚不明确。为此，我们首先构建了自动化执行器来实施构想，并通过大规模并行GPU实验验证其有效性；随后将LLM预训练与后训练两个现实研究问题转化为执行环境，证明该执行器能实现前沿LLM生成的大部分构想。我们分析了两种从执行反馈中学习的方法：进化搜索与强化学习。执行引导的进化搜索具备样本高效性——仅用十个搜索周期，即在后训练任务中发现显著超越GRPO基线的方法（69.4%对48.0%），在预训练任务中发现优于nanoGPT基线的方案（19.7分钟对35.9分钟）。前沿LLM在搜索中常生成有意义的算法构想，但易早期饱和且仅偶尔呈现扩展趋势。而基于执行奖励的强化学习则遭遇模式坍塌：虽能提升构想模型的平均奖励，却因模型收敛于简单构想而未能突破上限。我们通过对执行构想与训练动态的深入分析，为未来执行驱动的自动化AI研究提供支撑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates automated AI research, motivated by the need to move beyond large language models (LLMs) generating plausible but ineffective ideas by grounding them in actual execution. The method involves building an automated executor to implement ideas and run large-scale GPU experiments, applying it to two research problems: LLM pre-training and post-training. Key experimental results show that execution-guided evolutionary search is sample-efficient, finding methods that significantly outperform baselines within ten search epochs, while reinforcement learning from execution reward suffers from mode collapse, improving average reward but not the upper-bound performance.</div>
<div class="mono" style="margin-top:8px">本文研究自动化AI科研，其动机是大型语言模型（LLM）常生成看似合理但无效的想法，需通过实际执行来加以验证。方法上构建了一个自动执行器来实现想法并运行大规模GPU实验，应用于LLM预训练和后训练两个实际问题。主要实验结果表明，执行引导的进化搜索样本效率高，在十个搜索周期内找到的方法显著优于基线；而基于执行奖励的强化学习则出现模式崩溃，虽提升了平均奖励但未能突破性能上限。</div>
</details>
</div>
<div class="card">
<div class="title">Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree</div>
<div class="meta-line">Authors: Leyi Zhao, Weijie Huang, Yitong Guo, Jiang Bian, Chenghong Wang, Xuhong Zhang</div>
<div class="meta-line">First: 2026-01-20T22:32:52+00:00 · Latest: 2026-01-20T22:32:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14523v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14523v1">PDF</a> · <a href="https://github.com/annihi1ation/phylo_evolve">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的系统发育树进化式代码优化</div>
<div class="mono" style="margin-top:8px">为现代GPU优化科学计算算法是一个劳动密集的迭代过程，涉及在复杂的软硬件栈中反复进行代码修改、基准测试和调优。近期研究探索了基于大语言模型（LLM）辅助的进化式自动代码优化方法，但这些方法主要依赖结果导向的选择和随机突变，未能充分利用迭代优化过程中产生的丰富轨迹信息。我们提出PhyloEvolve系统，这是一个将面向GPU的算法优化重构为上下文强化学习（ICRL）问题的LLM智能体框架。该框架支持基于优化轨迹的经验复用，无需模型重新训练。PhyloEvolve将算法蒸馏技术与基于提示的决策变换器集成到迭代工作流中，将算法修改序列与性能反馈作为首要学习信号。为组织优化历史，我们引入系统发育树表示法来捕捉算法变体间的继承、分化和重组，支持回溯追踪、跨谱系迁移和结果复现。该系统融合精英轨迹池、多岛并行探索和容器化执行机制，以在异构硬件上平衡探索与利用能力。我们在偏微分方程求解器、流形学习和谱图算法等科学计算任务上评估PhyloEvolve，其运行效率、内存利用率和正确性均持续优于基线方法与进化算法。代码已发布于：https://github.com/annihi1ation/phylo_evolve</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to automate the labor-intensive process of optimizing scientific computing algorithms for modern GPUs, addressing the limitations of existing LLM-assisted evolutionary methods that underutilize trajectory information. The proposed method, PhyloEvolve, reframes code optimization as an In-Context Reinforcement Learning problem, integrating Algorithm Distillation and prompt-based Decision Transformers to reuse optimization experience without retraining, and employs a phylogenetic tree representation to organize optimization history for backtracking and cross-lineage transfer. Experimental results on workloads like PDE solvers and spectral graph algorithms show that PhyloEvolve consistently improves runtime, memory efficiency, and correctness over baseline and evolutionary methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是自动化针对现代GPU优化科学计算算法的劳动密集型过程，解决现有基于大语言模型的进化方法未能充分利用轨迹信息的局限。所提出的方法PhyloEvolve将代码优化重新定义为上下文强化学习问题，结合算法蒸馏和基于提示的决策变换器，在不重新训练模型的情况下重用优化经验，并采用系统发育树表示来组织优化历史以支持回溯和跨谱系迁移。在偏微分方程求解器和谱图算法等任务上的实验结果表明，PhyloEvolve在运行时间、内存效率和正确性方面均优于基线方法和进化方法。</div>
</details>
</div>
<div class="card">
<div class="title">Marvel: Accelerating Safe Online Reinforcement Learning with Finetuned Offline Policy</div>
<div class="meta-line">Authors: Keru Chen, Honghao Wei, Zhigang Deng, Sen Lin</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research (TMLR), 2026</div>
<div class="meta-line">First: 2024-12-05T18:51:18+00:00 · Latest: 2026-01-20T20:06:01+00:00</div>
<div class="meta-line">Comments: Accepted by Transactions on Machine Learning Research (TMLR), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.04426v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.04426v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The high costs and risks involved in extensive environment interactions hinder the practical application of current online safe reinforcement learning (RL) methods. While offline safe RL addresses this by learning policies from static datasets, the performance therein is usually limited due to reliance on data quality and challenges with out-of-distribution (OOD) actions. Inspired by recent successes in offline-to-online (O2O) RL, it is crucial to explore whether offline safe RL can be leveraged to facilitate faster and safer online policy learning, a direction that has yet to be fully investigated. To fill this gap, we first demonstrate that naively applying existing O2O algorithms from standard RL would not work well in the safe RL setting due to two unique challenges: \emph{erroneous Q-estimations}, resulted from offline-online objective mismatch and offline cost sparsity, and \emph{Lagrangian mismatch}, resulted from difficulties in aligning Lagrange multipliers between offline and online policies. To address these challenges, we introduce \textbf{Marvel}, a novel framework for O2O safe RL, comprising two key components that work in concert: \emph{Value Pre-Alignment} to align the Q-functions with the underlying truth before online learning, and \emph{Adaptive PID Control} to effectively adjust the Lagrange multipliers during online finetuning. Extensive experiments demonstrate that Marvel significantly outperforms existing baselines in both reward maximization and safety constraint satisfaction. By introducing the first policy-finetuning based framework for O2O safe RL, which is compatible with many offline and online safe RL methods, our work has the great potential to advance the field towards more efficient and practical safe RL solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Marvel：基于离线策略微调的在线安全强化学习加速框架</div>
<div class="mono" style="margin-top:8px">当前在线安全强化学习方法因需要大量环境交互而面临高成本和高风险，限制了实际应用。离线安全强化学习虽通过静态数据集学习策略规避了交互需求，但其性能受限于数据质量和对分布外动作的敏感性。受离线到在线强化学习近期进展的启发，探索如何利用离线安全强化学习实现更快速、更安全的在线策略学习成为关键方向，但该领域尚未得到充分研究。为填补这一空白，本文首先揭示直接套用标准强化学习的离线到在线算法在安全强化学习场景中效果不佳，原因在于两大独特挑战：由离线-在线目标失配和离线成本稀疏性导致的\emph{Q值估计偏差}，以及由离线与在线策略拉格朗日乘数对齐困难引发的\emph{拉格朗日失配}。针对这些挑战，我们提出\textbf{Marvel}——一种创新的离线到在线安全强化学习框架，包含两个协同工作的核心组件：在线学习前对齐Q函数与真实值的\emph{价值预对齐}模块，以及在线微调期间动态调整拉格朗日乘数的\emph{自适应PID控制}模块。大量实验表明，Marvel在奖励最大化和安全约束满足方面均显著优于现有基线方法。作为首个基于策略微调的离线到在线安全强化学习框架（兼容多种离线/在线安全强化学习方法），本工作有望推动该领域向更高效、更实用的安全强化学习解决方案迈进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the high cost and risk of online safe reinforcement learning (RL) by proposing a framework to accelerate safe online learning through fine-tuned offline policies, as purely offline methods suffer from data quality limitations and out-of-distribution actions. The method, named Marvel, tackles challenges like erroneous Q-estimations and Lagrangian mismatch via two components: Value Pre-Alignment to correct Q-functions before online learning and Adaptive PID Control to adjust Lagrange multipliers during fine-tuning. Experimental results show that Marvel outperforms existing baselines in both reward maximization and safety constraint satisfaction, offering a compatible framework for advancing practical safe RL solutions.</div>
<div class="mono" style="margin-top:8px">本文针对在线安全强化学习成本高、风险大的问题，提出通过微调离线策略来加速安全在线学习的框架，因为纯离线方法受限于数据质量和分布外动作。该方法名为Marvel，通过两个组件解决错误Q估计和拉格朗日乘子不匹配等挑战：在线学习前的价值预对齐以校正Q函数，以及在线微调期间的自适应PID控制以调整拉格朗日乘子。实验结果表明，Marvel在奖励最大化和安全约束满足方面均显著优于现有基线，为推进实用安全强化学习提供了兼容性框架。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Large Language Models for Black-Box Optimization</div>
<div class="meta-line">Authors: Ye Yuan, Can, Chen, Zipeng Sun, Dinghuai Zhang, Christopher Pal, Xue Liu</div>
<div class="meta-line">First: 2026-01-20T19:59:29+00:00 · Latest: 2026-01-20T19:59:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14446v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14446v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline black-box optimization (BBO) aims to find optimal designs based solely on an offline dataset of designs and their labels. Such scenarios frequently arise in domains like DNA sequence design and robotics, where only a few labeled data points are available. Traditional methods typically rely on task-specific proxy or generative models, overlooking the in-context learning capabilities of pre-trained large language models (LLMs). Recent efforts have adapted autoregressive LLMs to BBO by framing task descriptions and offline datasets as natural language prompts, enabling direct design generation. However, these designs often contain bidirectional dependencies, which left-to-right models struggle to capture. In this paper, we explore diffusion LLMs for BBO, leveraging their bidirectional modeling and iterative refinement capabilities. This motivates our in-context denoising module: we condition the diffusion LLM on the task description and the offline dataset, both formatted in natural language, and prompt it to denoise masked designs into improved candidates. To guide the generation toward high-performing designs, we introduce masked diffusion tree search, which casts the denoising process as a step-wise Monte Carlo Tree Search that dynamically balances exploration and exploitation. Each node represents a partially masked design, each denoising step is an action, and candidates are evaluated via expected improvement under a Gaussian Process trained on the offline dataset. Our method, dLLM, achieves state-of-the-art results in few-shot settings on design-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散大语言模型的黑盒优化</div>
<div class="mono" style="margin-top:8px">离线黑盒优化旨在仅基于设计及其标签的离线数据集寻找最优设计，此类场景常见于DNA序列设计和机器人学等仅有少量标注数据的领域。传统方法通常依赖任务特定的代理或生成模型，忽视了预训练大语言模型的上下文学习能力。近期研究通过将任务描述和离线数据集构建为自然语言提示，将自回归大语言模型适配于黑盒优化，实现直接设计生成。然而，这些设计常包含双向依赖关系，而左至右模型难以有效捕捉。本文探索利用扩散大语言模型的双向建模和迭代优化能力进行黑盒优化，由此提出上下文去噪模块：将自然语言格式的任务描述和离线数据集作为条件输入扩散大语言模型，引导其对掩码设计进行去噪以生成改进方案。为引导生成高性能设计，我们提出掩码扩散树搜索，将去噪过程构建为逐步蒙特卡洛树搜索，动态平衡探索与利用：节点表示部分掩码设计，去噪步骤作为动作，候选方案通过基于离线数据集训练的高斯过程计算期望提升进行评估。我们的dLLM方法在设计基准测试的少样本场景中取得了最先进的成果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses offline black-box optimization (BBO) in data-scarce domains like DNA sequence design, where traditional methods overlook the in-context learning of large language models (LLMs). The authors propose dLLM, a method that employs diffusion LLMs to capture bidirectional dependencies in designs through an in-context denoising module conditioned on natural language task descriptions and datasets, enhanced by a masked diffusion tree search for balancing exploration and exploitation during iterative refinement. Experimental results demonstrate that dLLM achieves state-of-the-art performance in few-shot settings on the design-bench benchmark.</div>
<div class="mono" style="margin-top:8px">本文针对DNA序列设计等数据稀缺领域的离线黑盒优化问题，传统方法常忽略大语言模型的上下文学习能力。作者提出dLLM方法，利用扩散大语言模型通过基于自然语言任务描述和数据集的上下文去噪模块捕捉设计中的双向依赖，并结合掩码扩散树搜索在迭代优化中平衡探索与利用。实验结果表明，dLLM在design-bench基准测试的少样本设置中取得了最先进的性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
