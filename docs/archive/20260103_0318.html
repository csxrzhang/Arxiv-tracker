<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-03 03:18</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260103_0318</div>
    <div class="row"><div class="card">
<div class="title">Scaling Open-Ended Reasoning to Predict the Future</div>
<div class="meta-line">Authors: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</div>
<div class="meta-line">First: 2025-12-31T18:59:51+00:00 · Latest: 2025-12-31T18:59:51+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25070v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25070v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展开放式推理以预测未来</div>
<div class="mono" style="margin-top:8px">高风险决策涉及对未来不确定性的推理。本研究训练语言模型对开放式预测问题进行预测。为扩大训练数据规模，我们基于每日新闻报道的全球事件，采用全自动精细筛选方法合成新的预测问题。我们在自建数据集OpenForesight上训练Qwen3思维模型。为防止训练和评估过程中的未来信息泄露，预测系统全程使用离线新闻语料进行数据生成与检索。通过小规模验证集指导，我们证明了检索机制的优势及改进的强化学习奖励函数。最终预测系统在2025年5月至8月期间进行留出测试。我们的专用模型OpenForecaster 8B性能媲美规模更大的专有模型，其训练显著提升了预测的准确性、校准度和一致性。研究发现预测训练带来的校准改进可泛化至主流基准测试。我们开源全部模型、代码与数据，以促进语言模型预测研究的广泛开展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of high-stakes decision-making by training language models for open-ended future prediction, motivated by the need for scalable reasoning under uncertainty. The method involves automatically synthesizing a large dataset of forecasting questions from daily news using a carefully curated, offline corpus to prevent data leakage, and training the Qwen3 thinking models on this OpenForesight dataset with enhancements like retrieval and an improved reward function for reinforcement learning. Experimental results on held-out tests from May to August 2025 show that the specialized OpenForecaster 8B model matches the performance of much larger proprietary models, improving prediction accuracy, calibration, and consistency, with calibration gains generalizing to other benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对高风险决策中不确定性下的推理需求，旨在训练语言模型进行开放式未来预测。方法上，通过自动化流程从日常新闻中合成大规模预测问题数据集，使用离线新闻语料以避免信息泄露，并在OpenForesight数据集上训练Qwen3思维模型，结合检索和改进的强化学习奖励函数。实验结果表明，在2025年5月至8月的保留测试中，专门的OpenForecaster 8B模型性能媲美更大的专有模型，提升了预测准确性、校准性和一致性，且校准改进可泛化至其他基准测试。</div>
</details>
</div>
<div class="card">
<div class="title">Many Minds from One Model: Bayesian Transformers for Population Intelligence</div>
<div class="meta-line">Authors: Diji Yang, Yi Zhang</div>
<div class="meta-line">First: 2025-12-31T18:56:02+00:00 · Latest: 2025-12-31T18:56:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25063v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25063v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.
  B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单模型生众智：面向群体智能的贝叶斯Transformer</div>
<div class="mono" style="margin-top:8px">尽管规模庞大且成效显著，现代Transformer几乎都被训练为单一思维系统：优化过程产生一组确定性参数，代表对数据的单一功能假设。受&#x27;智能源于众智&#x27;理念启发，我们提出群体贝叶斯Transformer（B-Trans），将标准大语言模型转化为贝叶斯Transformer模型，支持从单组预训练权重中采样生成多样且连贯的模型实例。B-Trans通过将归一化层中类偏置偏移量视为具有高斯变分近似的随机变量，引入贝叶斯启发的后验代理，在不训练完整贝叶斯神经网络的前提下诱导模型行为分布。从该代理采样可获得行为多样且保持通用能力的模型实例集合。为保持单次生成内的连贯性，我们在序列层级冻结采样噪声，确保跨词元的时间一致性。B-Trans支持群体级决策，通过聚合采样个体的预测显著增强探索能力。在零样本生成、可验证奖励强化学习（RLVR）及无显式标签强化学习的实验中，B-Trans有效利用群体智慧，在获得更优任务性能的同时，相比确定性基线产生更卓越的语义多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the idea that intelligence emerges from many minds rather than a single deterministic model, this paper proposes Population Bayesian Transformers (B-Trans), a method to convert a standard Large Language Model into a Bayesian Transformer. The approach treats bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, creating a posterior proxy that allows sampling diverse model instances from a single pre-trained weight set while maintaining coherence via sequence-level noise freezing. Experimental results in zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels show that B-Trans enhances semantic diversity and achieves superior task performance through population-level decision-making compared to deterministic baselines.</div>
<div class="mono" style="margin-top:8px">受智能源于众多思维而非单一确定性模型这一理念的启发，本文提出了群体贝叶斯变换器（B-Trans），这是一种将标准大语言模型转换为贝叶斯变换器的方法。该方法将归一化层中的偏置类偏移视为具有高斯变分近似的随机变量，构建了一个后验代理，从而允许从单一预训练权重集中采样多样化的模型实例，同时通过序列级噪声冻结保持生成一致性。在零样本生成、带可验证奖励的强化学习（RLVR）以及无显式标签的强化学习等实验结果表明，B-Trans通过群体级决策机制，在提升语义多样性的同时，相比确定性基线实现了更优的任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning</div>
<div class="meta-line">Authors: Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke Hüllermeier</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-31T18:21:52+00:00 · Latest: 2025-12-31T18:21:52+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25023v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResponseRank：通过偏好强度学习实现数据高效奖励建模</div>
<div class="mono" style="margin-top:8px">强化学习人类反馈（RLHF）常用的二元选择仅能传递偏好方向。人们可能选择苹果而非橙子、香蕉而非葡萄，但哪种偏好更强？强度对于不确定性下的决策和偏好模型泛化至关重要，却难以可靠测量。响应时间、标注者间一致性等元数据可作为强度代理指标，但常存在噪声和混杂因素。我们提出ResponseRank以应对从噪声强度信号中学习的挑战。该方法利用代理信号的相对差异，通过推断的偏好强度对成对比较的响应进行排序。为控制系统性变异，我们仅在精心构建的分层内进行局部信号比较。这能在对强度信号做最少假设的前提下，稳健学习与强度排序一致的效用差异。我们的贡献有三方面：（1）ResponseRank——一种通过局部有效相对强度信号稳健学习偏好强度的新方法；（2）在合成偏好学习（模拟响应时间）、语言建模（标注者一致性）、强化学习控制任务（模拟回合回报）等多样化任务中提升样本效率与鲁棒性的实证证据；（3）皮尔逊距离相关性——将基数效用学习与序数准确性分离的新度量指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitation of binary preference data in reinforcement learning from human feedback, which lacks information on the strength of preferences crucial for decision-making and generalization. The proposed method, ResponseRank, addresses this by learning from noisy proxy signals for strength, such as response times or annotator agreement, using relative differences within carefully constructed strata to rank responses by inferred preference strength. Experimental results across synthetic preference learning, language modeling, and RL control tasks demonstrate improved sample efficiency and robustness, with the introduction of the Pearson Distance Correlation metric to evaluate cardinal utility learning separately from ordinal accuracy.</div>
<div class="mono" style="margin-top:8px">本文的动机在于强化学习人类反馈中二元偏好数据的局限性，即缺乏对决策和泛化至关重要的偏好强度信息。提出的方法ResponseRank通过从响应时间或标注者一致性等噪声代理信号中学习，在精心构建的层内使用相对差异来按推断的偏好强度对响应进行排序。在合成偏好学习、语言建模和强化学习控制任务上的实验结果表明，该方法提高了样本效率和鲁棒性，并引入了皮尔逊距离相关性这一新指标，以独立评估基数效用学习与序数准确性。</div>
</details>
</div>
<div class="card">
<div class="title">MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control</div>
<div class="meta-line">Authors: Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She</div>
<div class="meta-line">First: 2025-12-31T16:36:44+00:00 · Latest: 2025-12-31T16:36:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24955v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MSACL：基于李雅普诺夫证书的多步演员-评论家学习框架及其在指数稳定控制中的应用</div>
<div class="mono" style="margin-top:8px">在无模型强化学习中实现可证明的稳定性仍具挑战性，尤其是在探索与严格安全性之间的平衡方面。本文提出MSACL框架，通过多步李雅普诺夫证书学习，将指数稳定性理论与最大熵强化学习相结合。与依赖复杂奖励工程的方法不同，MSACL利用离策略多步数据学习满足理论稳定性条件的李雅普诺夫证书。通过引入指数稳定性标签（ESL）和λ加权聚合机制，该框架有效平衡多步学习中的偏差-方差权衡。策略优化由稳定性感知的优势函数引导，确保所学策略促进李雅普诺夫函数快速下降。我们在六个基准测试中评估MSACL，包括稳定性和非线性跟踪任务，证明其优于现有基于李雅普诺夫的强化学习算法。MSACL在简单奖励下实现指数稳定性和快速收敛，同时对不确定性具有显著鲁棒性，并能泛化至未见轨迹。敏感性分析确定多步视野n=20可作为跨不同系统的稳健默认值。通过将李雅普诺夫理论与离策略演员-评论家框架结合，MSACL为可验证的安全学习控制奠定基础。源代码与基准环境将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of achieving provable stability in model-free reinforcement learning by introducing MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. The method utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions, employing Exponential Stability Labels and a λ-weighted aggregation mechanism to balance bias-variance trade-offs, with policy optimization guided by a stability-aware advantage function. Experimental results across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrate MSACL&#x27;s superiority over state-of-the-art Lyapunov-based RL algorithms, achieving exponential stability, rapid convergence under simple rewards, and robustness to uncertainties with generalization to unseen trajectories, while sensitivity analysis establishes a multi-step horizon of n=20 as a robust default.</div>
<div class="mono" style="margin-top:8px">本文针对无模型强化学习中实现可证明稳定性的挑战，提出了MSACL框架，该框架通过多步李雅普诺夫证书学习，将指数稳定性理论与最大熵强化学习相结合。该方法利用离策略多步数据学习满足理论稳定性条件的李雅普诺夫证书，采用指数稳定性标签和λ加权聚合机制平衡偏差-方差权衡，并通过稳定性感知的优势函数指导策略优化。在包括稳定性和非线性跟踪任务的六个基准测试中，实验结果表明MSACL优于现有的基于李雅普诺夫的强化学习算法，实现了在简单奖励下的指数稳定性和快速收敛，对不确定性具有显著鲁棒性并能泛化到未见轨迹，同时敏感性分析确定多步视野n=20为跨不同系统的鲁棒默认值。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Deployment Improves Planning Skills in LLMs</div>
<div class="meta-line">Authors: Augusto B. Corrêa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, André G. Pereira, Yarin Gal</div>
<div class="meta-line">First: 2025-12-31T16:03:14+00:00 · Latest: 2025-12-31T16:03:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24940v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models&#x27; deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代部署提升大语言模型的规划能力</div>
<div class="mono" style="margin-top:8px">研究表明，通过迭代部署大语言模型（每轮模型均基于用户从前一轮模型部署中精心筛选的数据进行微调），能显著改变最终模型的特性。在多种规划领域测试该机制后，我们观察到规划能力的大幅提升：后续模型展现出涌现泛化能力，能发现比初始模型更长的规划方案。理论分析表明，迭代部署本质上是在外层循环中实现了强化学习训练（而非作为有意识的模型训练环节），并隐含了奖励函数。这种与强化学习的关联具有双重重要意义：其一，对人工智能安全领域而言，由于迭代部署隐含的奖励函数未明确定义，可能对未来模型部署的特性产生意外影响；其二，该机制可视为显式强化学习的替代训练范式，其依赖数据筛选而非显式奖励信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how iterative deployment of large language models (LLMs), where each new model is fine-tuned on user-curated data from the previous deployment, can significantly alter model properties. The method is tested across planning domains, leading to substantial improvements in planning skills, with later models demonstrating emergent generalization by discovering much longer plans than their predecessors. The main experimental results confirm these gains, and a theoretical analysis connects the process to an implicit reinforcement learning (RL) mechanism, highlighting implications for AI safety due to the undefined reward function and presenting it as an alternative training regime to explicit RL.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLMs）的迭代部署方法，即每个新模型都在用户从前一次部署中精心策划的数据上进行微调，从而显著改变模型特性。该方法在多个规划领域进行测试，结果表明规划技能得到实质性提升，后续模型表现出涌现的泛化能力，能发现比初始模型长得多的规划方案。主要实验结果证实了这些改进，理论分析将该过程与隐式强化学习（RL）机制联系起来，强调了由于奖励函数未明确定义而对AI安全的影响，并将其视为显式RL的一种替代训练方案。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks</div>
<div class="meta-line">Authors: Shota Suzuki, Satoshi Ono</div>
<div class="meta-line">Venue: IEICE Transactions on Information and Systems, Vol.E108.D, No. 6, pp. 640-643, 2025</div>
<div class="meta-line">First: 2025-12-31T11:30:28+00:00 · Latest: 2025-12-31T11:30:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24793v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24793v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural architecture search (NAS), which automates the architectural design process of deep neural networks (DNN), has attracted increasing attention. Multimodal DNNs that necessitate feature fusion from multiple modalities benefit from NAS due to their structural complexity; however, constructing an architecture for multimodal DNNs through NAS requires a substantial amount of labeled training data. Thus, this paper proposes a self-supervised learning (SSL) method for architecture search of multimodal DNNs. The proposed method applies SSL comprehensively for both the architecture search and model pretraining processes. Experimental results demonstrated that the proposed method successfully designed architectures for DNNs from unlabeled training data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多模态深度神经网络的自监督神经架构搜索</div>
<div class="mono" style="margin-top:8px">神经架构搜索（NAS）通过自动化深度神经网络（DNN）的架构设计过程，正受到日益广泛的关注。多模态DNN因需融合多模态特征而结构复杂，故能受益于NAS技术；然而，通过NAS构建多模态DNN架构通常需要大量标注训练数据。为此，本文提出一种面向多模态DNN架构搜索的自监督学习（SSL）方法。该方法将SSL全面应用于架构搜索与模型预训练两个阶段。实验结果表明，所提方法能成功利用无标注训练数据完成DNN架构设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to automate the design of complex multimodal deep neural networks without relying on large labeled datasets, this paper introduces a self-supervised learning approach for neural architecture search. The method comprehensively applies self-supervision to both the architecture search process and the pretraining of models, enabling the discovery of effective network structures directly from unlabeled data. Experimental results confirm that the proposed technique successfully designs architectures for multimodal DNNs using only unlabeled training data, demonstrating its practical utility in data-scarce scenarios.</div>
<div class="mono" style="margin-top:8px">本文的动机是无需依赖大量标注数据即可自动化设计复杂的多模态深度神经网络，为此提出了一种用于神经架构搜索的自监督学习方法。该方法将自监督学习全面应用于架构搜索过程和模型预训练，从而能够直接从无标注数据中发现有效的网络结构。实验结果表明，所提出的技术仅使用无标注训练数据就成功为多模态深度神经网络设计了架构，证明了其在数据稀缺场景中的实用价值。</div>
</details>
</div>
<div class="card">
<div class="title">Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation</div>
<div class="meta-line">Authors: Takeru Kusakabe, Yudai Hirose, Mashiho Mukaida, Satoshi Ono</div>
<div class="meta-line">First: 2025-12-31T11:30:03+00:00 · Latest: 2025-12-31T11:30:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24792v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24792v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks (DNNs) remain vulnerable to adversarial attacks that cause misclassification when specific perturbations are added to input images. This vulnerability also threatens the reliability of DNN-based monocular depth estimation (MDE) models, making robustness enhancement a critical need in practical applications. To validate the vulnerability of DNN-based MDE models, this study proposes a projection-based adversarial attack method that projects perturbation light onto a target object. The proposed method employs physics-in-the-loop (PITL) optimization -- evaluating candidate solutions in actual environments to account for device specifications and disturbances -- and utilizes a distributed covariance matrix adaptation evolution strategy. Experiments confirmed that the proposed method successfully created adversarial examples that lead to depth misestimations, resulting in parts of objects disappearing from the target scene.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于投影的对抗攻击：采用物理在环优化针对单目深度估计</div>
<div class="mono" style="margin-top:8px">深度神经网络（DNNs）在面对输入图像添加特定扰动时仍易受对抗攻击而导致误分类，这一脆弱性同样威胁着基于DNN的单目深度估计（MDE）模型的可靠性，使得鲁棒性增强成为实际应用中的关键需求。为验证基于DNN的MDE模型的脆弱性，本研究提出一种基于投影的对抗攻击方法，将扰动光线投射至目标物体。该方法采用物理在环（PITL）优化——通过在真实环境中评估候选解以考虑设备规格与干扰因素，并利用分布式协方差矩阵自适应进化策略。实验证实，所提方法成功生成了导致深度估计错误的对抗样本，致使目标场景中物体部分区域消失。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of deep neural networks to adversarial perturbations and the critical need to enhance robustness in monocular depth estimation models, this paper proposes a projection-based adversarial attack method that projects perturbation light onto a target object. The method employs physics-in-the-loop optimization, which evaluates candidate solutions in real-world environments to account for device specifications and disturbances, and utilizes a distributed covariance matrix adaptation evolution strategy. Experimental results confirmed that the proposed method successfully generated adversarial examples causing depth misestimations, leading to parts of objects disappearing from the target scene.</div>
<div class="mono" style="margin-top:8px">本研究针对深度神经网络易受对抗性攻击的脆弱性，以及提升单目深度估计模型鲁棒性的实际需求，提出了一种基于投影的对抗攻击方法，将扰动光线投射到目标物体上。该方法采用物理在环优化，通过在真实环境中评估候选解来考虑设备规格和干扰，并利用分布式协方差矩阵自适应进化策略。实验结果表明，所提方法成功生成了导致深度估计错误的对抗样本，使得目标场景中的物体部分消失。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification</div>
<div class="meta-line">Authors: Mikhael Djajapermana, Moritz Reiber, Daniel Mueller-Gritschneder, Ulf Schlichtmann</div>
<div class="meta-line">First: 2025-11-04T20:52:56+00:00 · Latest: 2025-12-31T10:37:03+00:00</div>
<div class="meta-line">Comments: Presented at ITEM workshop co-located with ECML PKDD 2024, Vilnius LT</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02992v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.02992v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向微型机器学习图像分类的混合卷积与视觉Transformer神经架构搜索空间</div>
<div class="mono" style="margin-top:8px">卷积神经网络与视觉Transformer的混合架构在性能上已超越纯卷积或纯Transformer架构。然而，由于这些架构参数量大、计算成本高，难以部署于微型机器学习场景。本文提出一种面向神经架构搜索的新型混合卷积-Transformer搜索空间，旨在为图像分类任务寻找高效的混合架构。该搜索空间涵盖可学习局部与全局信息的混合卷积-Transformer模块，以及创新的可搜索池化层模块，以实现高效的特征图降维。在CIFAR10数据集上的实验表明，在严格的模型大小限制下，本搜索空间生成的混合卷积-Transformer架构在精度与推理速度上均优于基于ResNet的微型机器学习模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient hybrid CNN-Vision Transformer architectures suitable for tinyML deployment, this paper introduces a novel Neural Architecture Search (NAS) search space that combines hybrid CNN-ViT blocks to capture both local and global information, along with a searchable pooling block for optimized feature map reduction. The method enables automated discovery of compact models. Experimental results on CIFAR10 demonstrate that the discovered architectures achieve superior accuracy and faster inference speeds compared to ResNet-based tinyML models under strict size constraints.</div>
<div class="mono" style="margin-top:8px">本文旨在为微型机器学习部署设计高效的混合卷积神经网络与视觉Transformer架构，提出了一种新的神经架构搜索空间，该空间融合了混合CNN-ViT块以学习局部和全局信息，并引入了可搜索的池化块以实现高效的特征图降维。该方法能够自动搜索紧凑的模型结构。在CIFAR10数据集上的实验结果表明，在严格的模型大小限制下，所发现的混合架构在准确性和推理速度上均优于基于ResNet的微型机器学习模型。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Offline Reinforcement Learning with Corruption Robustness</div>
<div class="meta-line">Authors: Nam Phuong Tran, Andi Nika, Goran Radanovic, Long Tran-Thanh, Debmalya Mandal</div>
<div class="meta-line">First: 2025-12-31T10:28:25+00:00 · Latest: 2025-12-31T10:28:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24768v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24768v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate robustness to strong data corruption in offline sparse reinforcement learning (RL). In our setting, an adversary may arbitrarily perturb a fraction of the collected trajectories from a high-dimensional but sparse Markov decision process, and our goal is to estimate a near optimal policy. The main challenge is that, in the high-dimensional regime where the number of samples $N$ is smaller than the feature dimension $d$, exploiting sparsity is essential for obtaining non-vacuous guarantees but has not been systematically studied in offline RL. We analyse the problem under uniform coverage and sparse single-concentrability assumptions. While Least Square Value Iteration (LSVI), a standard approach for robust offline RL, performs well under uniform coverage, we show that integrating sparsity into LSVI is unnatural, and its analysis may break down due to overly pessimistic bonuses. To overcome this, we propose actor-critic methods with sparse robust estimator oracles, which avoid the use of pointwise pessimistic bonuses and provide the first non-vacuous guarantees for sparse offline RL under single-policy concentrability coverage. Moreover, we extend our results to the contaminated setting and show that our algorithm remains robust under strong contamination. Our results provide the first non-vacuous guarantees in high-dimensional sparse MDPs with single-policy concentrability coverage and corruption, showing that learning a near-optimal policy remains possible in regimes where traditional robust offline RL techniques may fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏离线强化学习的抗干扰鲁棒性研究</div>
<div class="mono" style="margin-top:8px">本文研究了离线稀疏强化学习（RL）中对强数据干扰的鲁棒性问题。在我们的设定中，对手可能任意扰动从高维稀疏马尔可夫决策过程中收集的部分轨迹，目标是估计接近最优的策略。主要挑战在于，在样本数$N$小于特征维度$d$的高维场景下，利用稀疏性对于获得非平凡保证至关重要，但离线RL中尚未系统研究此问题。我们在均匀覆盖和稀疏单集中性假设下分析该问题。虽然鲁棒离线RL的标准方法——最小二乘值迭代（LSVI）在均匀覆盖下表现良好，但我们发现将稀疏性融入LSVI并不自然，其分析可能因过于悲观的奖励修正而失效。为此，我们提出了基于稀疏鲁棒估计器预言机的行动者-评论家方法，避免了逐点悲观奖励修正，首次为单策略集中性覆盖下的稀疏离线RL提供了非平凡保证。此外，我们将结果扩展到污染场景，证明算法在强污染下仍保持鲁棒性。本研究首次为具有单策略集中性覆盖和干扰的高维稀疏MDP提供了非平凡保证，表明在传统鲁棒离线RL技术可能失效的场景中，学习接近最优策略仍然是可行的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robust offline reinforcement learning in high-dimensional sparse Markov decision processes where data may be partially corrupted by an adversary. The motivation stems from the need to estimate near-optimal policies despite adversarial perturbations to a fraction of trajectories, particularly in sample-scarce regimes where the number of samples is smaller than the feature dimension, making sparsity exploitation critical. The method introduces actor-critic approaches that integrate sparse robust estimator oracles, avoiding the overly pessimistic bonuses of traditional methods like Least Square Value Iteration, which struggles under sparse single-concentrability assumptions. Experimental results demonstrate that the proposed algorithms achieve non-vacuous guarantees for learning near-optimal policies under single-policy concentrability coverage and remain robust even with strong data contamination, outperforming traditional robust offline RL techniques in high-dimensional sparse settings.</div>
<div class="mono" style="margin-top:8px">本文研究了在高维稀疏马尔可夫决策过程中，面对数据可能被对手部分污染的鲁棒离线强化学习问题。其动机在于，在样本数量少于特征维度的稀缺样本场景下，需要利用稀疏性来估计近似最优策略，同时应对轨迹数据的部分对抗性扰动。方法上，提出了结合稀疏鲁棒估计器oracle的actor-critic方法，避免了传统最小二乘值迭代等方法在稀疏单策略集中性假设下过于悲观的奖励估计问题。实验结果表明，所提算法在单策略集中性覆盖和强数据污染条件下，仍能获得非平凡保证，成功学习到近似最优策略，优于传统鲁棒离线强化学习技术在高维稀疏环境中的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow</div>
<div class="meta-line">Authors: Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei, Ruohan Zhang</div>
<div class="meta-line">First: 2025-12-31T10:25:24+00:00 · Latest: 2025-12-31T10:25:24+00:00</div>
<div class="meta-line">Comments: Project website: https://dream2flow.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24766v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24766v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dream2flow.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dream2Flow：通过三维物体流桥接视频生成与开放世界操控</div>
<div class="mono" style="margin-top:8px">生成式视频建模已成为零样本推理开放世界操控中合理物理交互的有力工具。然而，将此类人为引导的运动转化为机器人系统所需的底层动作仍具挑战。我们观察到，给定初始图像和任务指令，这些模型擅长合成合理的物体运动。因此，我们提出Dream2Flow框架，通过三维物体流作为中间表示，桥接视频生成与机器人控制。该方法从生成视频中重建三维物体运动，并将操控任务表述为物体轨迹跟踪。通过将状态变化与实现变化的执行器分离，Dream2Flow克服了具身化鸿沟，使预训练视频模型能够零样本引导操控多种类别物体——包括刚性、铰接式、可变形及颗粒状物体。通过轨迹优化或强化学习，Dream2Flow将重建的三维物体流转化为可执行的底层指令，无需任务特定演示。仿真与真实实验表明，三维物体流是适配视频生成模型至开放世界机器人操控的通用可扩展接口。视频与可视化内容详见项目网站。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to bridge the gap between generative video models, which can reason about plausible physical interactions, and the low-level action requirements of robotic systems for open-world manipulation. The method, Dream2Flow, introduces a framework that uses 3D object flow as an intermediate representation; it reconstructs 3D object motions from videos generated by pre-trained models and formulates manipulation as object trajectory tracking, enabling zero-shot guidance. The main experimental results demonstrate that Dream2Flow successfully converts these reconstructed flows into executable low-level commands via trajectory optimization or reinforcement learning, allowing manipulation of diverse object categories—rigid, articulated, deformable, and granular—in both simulation and real-world settings without task-specific demonstrations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于弥合生成视频模型（能够推理合理的物理交互）与机器人系统在开放世界操作中所需的低级动作之间的差距。其方法Dream2Flow提出了一个以3D物体流作为中间表示的框架：它从预训练模型生成的视频中重建3D物体运动，并将操作任务转化为物体轨迹跟踪，从而实现零样本引导。主要实验结果表明，Dream2Flow通过轨迹优化或强化学习，成功将这些重建的流转换为可执行的低级指令，能够在仿真和现实世界中操作多种物体类别（包括刚性、铰接、可变形和颗粒物体），且无需任务特定的演示。</div>
</details>
</div>
<div class="card">
<div class="title">AINav: Large Language Model-Based Adaptive Interactive Navigation</div>
<div class="meta-line">Authors: Kangjie Zhou, Yao Mu, Haoyang Song, Yi Zeng, Pengying Wu, Han Gao, Chang Liu</div>
<div class="meta-line">First: 2025-03-29T02:17:52+00:00 · Latest: 2025-12-31T09:40:34+00:00</div>
<div class="meta-line">Comments: 13 pages, 12 figures, accepted to IEEE Robotics &amp; Automation Magazine</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.22942v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.22942v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic navigation in complex environments remains a critical research challenge. Traditional navigation methods focus on optimal trajectory generation within fixed free workspace, therefore struggling in environments lacking viable paths to the goal, such as disaster zones or cluttered warehouses. To address this problem, we propose AINav, an adaptive interactive navigation approach that proactively interacts with environments to create feasible paths to achieve originally unreachable goals. Specifically, we present a primitive skill tree for task planning with large language models (LLMs), facilitating effective reasoning to determine interaction objects and sequences. To ensure robust subtask execution, we adopt reinforcement learning to pre-train a comprehensive skill library containing versatile locomotion and interaction behaviors for motion planning. Furthermore, we introduce an adaptive replanning approach featuring two LLM-based modules: an advisor serving as a flexible replanning trigger and an arborist for autonomous plan adjustment. Integrated with the tree structure, the replanning mechanism allows for convenient node addition and pruning, enabling rapid plan adaptation in a priori unknown environments. Comprehensive simulations and experiments have demonstrated AINav&#x27;s effectiveness and adaptivity in diverse scenarios. The supplementary video is available at: https://youtu.be/CjXm5KFx9AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AINav：基于大语言模型的自适应交互式导航</div>
<div class="mono" style="margin-top:8px">复杂环境下的机器人导航仍是关键研究挑战。传统导航方法聚焦于固定自由工作空间内的最优轨迹生成，因此在缺乏可行路径抵达目标的环境（如灾区或杂乱仓库）中表现不佳。为解决此问题，我们提出AINav——一种自适应交互式导航方法，通过主动与环境交互来创建可行路径，以实现原本无法抵达的目标。具体而言，我们构建了用于大语言模型任务规划的基元技能树，以促进有效推理来确定交互对象与序列。为确保子任务执行的鲁棒性，我们采用强化学习预训练包含多样化运动与交互行为的综合技能库用于运动规划。此外，我们引入一种自适应重规划方法，其包含两个基于大语言模型的模块：作为灵活重规划触发器的顾问模块，以及用于自主计划调整的规划树管理模块。该重规划机制与树形结构相结合，支持便捷的节点增删，从而能在先验未知环境中实现快速计划调整。综合仿真与实验验证了AINav在多场景下的有效性与适应性。补充视频详见：https://youtu.be/CjXm5KFx9AI。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable robotic navigation in complex environments where traditional methods fail due to a lack of viable paths, such as in disaster zones or cluttered spaces. The proposed method, AINav, combines large language models (LLMs) for high-level task planning—using a primitive skill tree to reason about interaction objects and sequences—with reinforcement learning to pre-train a versatile skill library for robust motion execution, and it features an adaptive replanning mechanism with LLM-based modules for dynamic plan adjustment. Experimental results from comprehensive simulations demonstrate the system&#x27;s effectiveness and adaptability in diverse scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决机器人在复杂环境（如灾区或杂乱仓库）中因缺乏可行路径而传统导航方法失效的问题。提出的AINav方法结合大型语言模型进行高层任务规划，通过原始技能树推理交互对象与序列，并采用强化学习预训练包含多种运动与交互行为的技能库以保障鲁棒执行，同时引入基于大型语言模型的自适应重规划模块实现动态调整。综合仿真实验表明，该系统在多种场景下均表现出有效性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting</div>
<div class="meta-line">Authors: Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao</div>
<div class="meta-line">First: 2025-12-31T08:10:03+00:00 · Latest: 2025-12-31T08:10:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24702v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24702v1">PDF</a> · <a href="https://github.com/AHideoKuzeA/Evol-SAM3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &quot;generate-then-segment&quot; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &quot;Generate-Evaluate-Evolve&quot; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>演进而非训练：基于进化提示的零样本推理分割</div>
<div class="mono" style="margin-top:8px">推理分割要求模型解析依赖上下文的复杂语言查询以实现像素级定位。当前主流方法严重依赖监督微调或强化学习，但前者存在灾难性遗忘和领域依赖问题，后者常受训练不稳定性和对预定义奖励函数僵化依赖的制约。尽管近期免训练方法规避了这些训练负担，但其本质上受限于静态推理范式——通常依赖单次“生成-分割”链，存在推理深度不足、无法自我纠正语言幻觉或空间误判的缺陷。本文突破这些限制，提出EVOL-SAM3这一新型零样本框架，将推理分割重构为推理时进化搜索过程。该框架通过“生成-评估-进化”循环迭代优化提示假设种群，引入视觉竞技场进行无参考成对竞争评估，设计语义变异算子注入多样性并修正语义错误，同时采用异构竞技场模块融合几何先验与语义推理以确保最终选择的鲁棒性。大量实验表明，EVOL-SAM3在零样本设置下不仅显著超越静态基线，更在挑战性ReasonSeg基准上大幅优于全监督前沿方法。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of supervised fine-tuning and reinforcement learning, which suffer from issues like catastrophic forgetting and training instability, and the insufficient reasoning depth of static training-free methods, this paper proposes EVOL-SAM3, a zero-shot framework that reformulates reasoning segmentation as an evolutionary search process. The method employs a &quot;Generate-Evaluate-Evolve&quot; loop, where a population of prompt hypotheses is iteratively refined using a Visual Arena for fitness evaluation via pairwise tournaments and a Semantic Mutation operator to inject diversity and correct errors, with a Heterogeneous Arena integrating geometric priors for final selection. Experimental results show that EVOL-SAM3 substantially outperforms static baselines and surpasses fully supervised state-of-the-art methods on the ReasonSeg benchmark in a zero-shot setting.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决监督微调和强化学习方法存在的灾难性遗忘、训练不稳定等问题，以及静态免训练方法推理深度不足的局限，提出了EVOL-SAM3这一零样本框架，将推理分割重新定义为推理时的进化搜索过程。该方法采用“生成-评估-进化”循环，通过视觉竞技场进行无参考成对竞赛以评估提示假设的适应性，并利用语义突变算子注入多样性和纠正错误，同时异构竞技场整合几何先验进行最终选择。实验结果表明，EVOL-SAM3在零样本设置下显著优于静态基线方法，并在ReasonSeg基准测试中超越了完全监督的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation</div>
<div class="meta-line">Authors: Yury Kolomeytsev, Dmitry Golembiovsky</div>
<div class="meta-line">First: 2025-12-31T05:58:57+00:00 · Latest: 2025-12-31T05:58:57+00:00</div>
<div class="meta-line">Comments: 22 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24651v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24651v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous mobile robots operating in complex, dynamic environments face the dual challenge of navigating large-scale, structurally diverse spaces with static obstacles while safely interacting with various moving agents. Traditional graph-based planners excel at long-range pathfinding but lack reactivity, while Deep Reinforcement Learning (DRL) methods demonstrate strong collision avoidance but often fail to reach distant goals due to a lack of global context. We propose Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), a hybrid framework that bridges this gap. Our approach utilizes a graph-based global planner to generate a path, which is integrated into a local DRL policy via a sequence of checkpoints encoded in both the state space and reward function. To ensure social compliance, the local planner employs an entity-aware reward structure that dynamically adjusts safety margins and penalties based on the semantic type of surrounding agents. We validate the proposed method through extensive testing in a realistic simulation environment derived from real-world map data. Comprehensive experiments demonstrate that HMP-DRL consistently outperforms other methods, including state-of-the-art approaches, in terms of key metrics of robot navigation: success rate, collision rate, and time to reach the goal. Overall, these findings confirm that integrating long-term path guidance with semantically-aware local control significantly enhances both the safety and reliability of autonomous navigation in complex human-centric settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的混合运动规划在移动机器人导航中的应用</div>
<div class="mono" style="margin-top:8px">在复杂动态环境中运行的自主移动机器人面临双重挑战：既要在大规模、结构多样的静态障碍物空间中导航，又要安全地与各类移动智能体交互。传统基于图的规划器擅长长距离路径规划但缺乏实时反应能力，而深度强化学习方法虽展现出优异的避障能力，却常因缺乏全局环境感知而难以抵达远距离目标。本文提出基于深度强化学习的混合运动规划框架，通过将基于图的全局规划器生成的路径（以状态空间和奖励函数中编码的检查点序列形式）整合至局部DRL策略中，有效弥补了上述缺陷。为保障社会合规性，局部规划器采用实体感知的奖励结构，根据周围智能体的语义类型动态调整安全边界与惩罚机制。基于真实地图数据构建的仿真环境测试表明，HMP-DRL在机器人导航关键指标（成功率、碰撞率、抵达时间）上均优于现有先进方法。研究证实，将长期路径引导与语义感知的局部控制相结合，能显著提升以人为中心的复杂场景中自主导航的安全性与可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of autonomous mobile robot navigation in complex, dynamic environments, where traditional graph-based planners lack reactivity and Deep Reinforcement Learning (DRL) methods often fail to reach distant goals due to limited global context. The proposed solution, Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), integrates a graph-based global planner that provides a long-range path with a local DRL policy guided by checkpoints and an entity-aware reward structure for socially compliant interactions with moving agents. Experimental validation in a realistic simulation environment shows that HMP-DRL outperforms state-of-the-art methods, achieving higher success rates, lower collision rates, and reduced time to reach goals, thereby enhancing both safety and reliability in human-centric settings.</div>
<div class="mono" style="margin-top:8px">本文针对自主移动机器人在复杂动态环境中导航的挑战，传统基于图的规划器缺乏反应性，而深度强化学习方法因缺乏全局上下文常无法到达远距离目标。提出的混合运动规划与深度强化学习方法，通过整合提供长距离路径的基于图全局规划器与由检查点引导的局部深度强化学习策略，并采用实体感知奖励结构以实现与移动智能体的社会合规交互。在基于真实地图数据的仿真环境中的综合实验表明，该方法在成功率、碰撞率和到达目标时间等关键指标上均优于现有先进方法，显著提升了以人为中心的复杂场景中自主导航的安全性和可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison</div>
<div class="meta-line">Authors: Yoonho Lee, Joseph Boen, Chelsea Finn</div>
<div class="meta-line">First: 2025-11-11T07:14:13+00:00 · Latest: 2025-12-31T05:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07919v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.07919v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反馈下降：基于成对比较的开放式文本优化框架</div>
<div class="mono" style="margin-top:8px">本文提出《反馈下降》框架，通过结构化文本反馈（而非仅依赖标量奖励）优化文本产物——包括提示词、代码和分子结构。该方法保留详细评析而非压缩为二元偏好，拓宽了偏好学习中的信息瓶颈，实现在文本空间而非权重空间的定向优化。研究表明，上下文学习可将结构化反馈转化为类梯度的方向性信息，从而实现针对性编辑。与先前将判断压缩为单比特的方法不同，本框架的评估器为每次比较配对文本反馈，形成高带宽监督。迭代循环完全在推理阶段完成，无需修改模型权重，且与任务无关。我们在三个不同领域评估反馈下降框架，发现其性能优于最先进的提示优化方法（GEPA）、强化学习方法（GRPO、REINVENT），甚至超越专用图基分子优化器。在DOCKSTRING分子发现基准测试中，该框架识别出的新型类药分子超越包含26万种化合物的数据库中99.9%的分子，覆盖六个蛋白质靶点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Feedback Descent, a framework motivated by the need to optimize text artifacts like prompts, code, and molecules using detailed textual feedback rather than limited scalar rewards, thereby addressing the information bottleneck in preference learning. The method leverages in-context learning to convert structured feedback into gradient-like directional signals, enabling targeted text edits through an inference-time iteration loop that is task-agnostic and does not require weight updates. Experimental results demonstrate that Feedback Descent outperforms state-of-the-art methods in prompt optimization, reinforcement learning, and molecular design, notably identifying novel drug-like molecules above the 99.9th percentile on the DOCKSTRING benchmark across six protein targets.</div>
<div class="mono" style="margin-top:8px">本文提出了Feedback Descent框架，其动机在于通过详细的文本反馈而非有限的标量奖励来优化提示、代码和分子等文本产物，从而解决偏好学习中的信息瓶颈问题。该方法利用上下文学习将结构化反馈转化为类似梯度的方向性信号，通过无需权重更新的推理时迭代循环实现针对性文本编辑，且与任务无关。实验结果表明，Feedback Descent在提示优化、强化学习和分子设计方面均优于现有先进方法，尤其在DOCKSTRING分子发现基准测试中，针对六个蛋白质靶点识别出了超越26万多个化合物数据库中99.9%百分位的新型类药物分子。</div>
</details>
</div>
<div class="card">
<div class="title">Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?</div>
<div class="meta-line">Authors: Zijian Zhao, Sen Li</div>
<div class="meta-line">First: 2025-09-26T13:15:18+00:00 · Latest: 2025-12-31T05:05:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03257v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03257v2">PDF</a> · <a href="https://github.com/RS2002/Triple-BERT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate real-time challenge of bundling and matching passengers-each with distinct origins and destinations-to available vehicles, all while navigating significant system uncertainties. Due to the extensive observation space arising from the large number of drivers and orders, order dispatching, though fundamentally a centralized task, is often addressed using Multi-Agent Reinforcement Learning (MARL). However, independent MARL methods fail to capture global information and exhibit poor cooperation among workers, while Centralized Training Decentralized Execution (CTDE) MARL methods suffer from the curse of dimensionality. To overcome these challenges, we propose Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method designed specifically for large-scale order dispatching on ride-sharing platforms. Built on a variant TD3, our approach addresses the vast action space through an action decomposition strategy that breaks down the joint action probability into individual driver action probabilities. To handle the extensive observation space, we introduce a novel BERT-based network, where parameter reuse mitigates parameter growth as the number of drivers and orders increases, and the attention mechanism effectively captures the complex relationships among the large pool of driver and orders. We validate our method using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves approximately an 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our code, trained model parameters, and processed data are publicly available at the repository https://github.com/RS2002/Triple-BERT .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Triple-BERT：网约车平台订单调度真的需要多智能体强化学习吗？</div>
<div class="mono" style="margin-top:8px">以Uber和Lyft为代表的按需出行平台面临实时捆绑匹配乘客与车辆的复杂挑战，需在系统高度不确定性的环境下处理海量订单与司机的动态匹配。由于司机与订单规模庞大导致观测空间巨大，订单调度虽本质是集中式任务，却常采用多智能体强化学习（MARL）方法。然而独立MARL方法难以获取全局信息且智能体协作性差，而集中训练分散执行（CTDE）类MARL方法又受维度灾难制约。为此，我们提出Triple-BERT——一种专为大规模网约车订单调度设计的集中式单智能体强化学习方法。该方法基于TD3算法变体，通过动作分解策略将联合动作概率拆分为独立司机动作概率以应对巨大动作空间；针对庞大观测空间，我们设计了基于BERT的新型网络架构，其参数复用机制可抑制参数随司机订单数量增长，注意力机制则能有效捕捉海量司机与订单间的复杂关联。基于曼哈顿真实网约车数据的实验表明，Triple-BERT相较当前最优方法实现约11.95%的综合性能提升，其中接单量增加4.26%，接驾时间降低22.25%。代码、训练模型及处理数据已开源：https://github.com/RS2002/Triple-BERT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the complex order dispatch problem in ride-sharing platforms, where the large scale of drivers and orders makes traditional Multi-Agent Reinforcement Learning (MARL) approaches inefficient due to poor cooperation or high dimensionality. The authors propose Triple-BERT, a centralized single-agent reinforcement learning method based on TD3, which decomposes the joint action space into individual driver actions and employs a BERT-based network with parameter sharing and attention mechanisms to efficiently handle the vast observation space and capture complex driver-order relationships. Experimental validation on a real-world Manhattan ride-hailing dataset shows that Triple-BERT outperforms state-of-the-art methods, achieving an 11.95% overall improvement, a 4.26% increase in served orders, and a 22.25% reduction in pickup times.</div>
<div class="mono" style="margin-top:8px">本文针对网约车平台中大规模订单调度问题展开研究，传统多智能体强化学习方法因难以协调或维度灾难而效率低下。为此，作者提出了Triple-BERT方法，这是一种基于TD3的集中式单智能体强化学习框架，通过动作分解策略将联合动作空间拆分为单个司机动作，并采用基于BERT的网络结构，利用参数复用和注意力机制有效处理大规模观察空间并捕捉司机与订单间的复杂关系。在纽约曼哈顿的真实网约车数据集上的实验表明，该方法优于现有最优方法，整体性能提升约11.95%，成功派单量增加4.26%，接驾时间减少22.25%。</div>
</details>
</div>
<div class="card">
<div class="title">One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms</div>
<div class="meta-line">Authors: Zijian Zhao, Sen Li</div>
<div class="meta-line">First: 2025-07-21T08:04:31+00:00 · Latest: 2025-12-31T05:00:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.15351v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.15351v2">PDF</a> · <a href="https://github.com/RS2002/OSPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Order dispatch is a critical task in ride-sharing systems with Autonomous Vehicles (AVs), directly influencing efficiency and profits. Recently, Multi-Agent Reinforcement Learning (MARL) has emerged as a promising solution to this problem by decomposing the large state and action spaces among individual agents, effectively addressing the Curse of Dimensionality (CoD) in transportation market, which is caused by the substantial number of vehicles, passengers, and orders. However, conventional MARL-based approaches heavily rely on accurate estimation of the value function, which becomes problematic in large-scale, highly uncertain environments. To address this issue, we propose two novel methods that bypass value function estimation, leveraging the homogeneous property of AV fleets. First, we draw an analogy between AV fleets and groups in Group Relative Policy Optimization (GRPO), adapting it to the order dispatch task. By replacing the Proximal Policy Optimization (PPO) baseline with the group average reward-to-go, GRPO eliminates critic estimation errors and reduces training bias. Inspired by this baseline replacement, we further propose One-Step Policy Optimization (OSPO), demonstrating that the optimal policy can be trained using only one-step group rewards under a homogeneous fleet. Experiments on a real-world ride-hailing dataset show that both GRPO and OSPO achieve promising performance across all scenarios, efficiently optimizing pickup times and the number of served orders using simple Multilayer Perceptron (MLP) networks. Furthermore, OSPO outperforms GRPO in all scenarios, attributed to its elimination of bias caused by the bounded time horizon of GRPO. Our code, trained models, and processed data are provided at https://github.com/RS2002/OSPO .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步足矣：基于单步策略优化的多智能体强化学习在网约车平台订单调度中的应用</div>
<div class="mono" style="margin-top:8px">订单调度是自动驾驶车辆网约车系统中的关键任务，直接影响运营效率和利润。近年来，多智能体强化学习通过将庞大的状态和动作空间分解至个体智能体，有效缓解了由海量车辆、乘客和订单引发的交通市场维度灾难，成为该问题的潜力解决方案。然而，传统基于多智能体强化学习的方法严重依赖价值函数的精确估计，这在规模庞大、高度不确定的环境中会产生问题。为解决此问题，我们提出两种绕过价值函数估计的新方法，利用自动驾驶车队的同质特性。首先，我们将自动驾驶车队类比为组相对策略优化中的群组，并将其适配至订单调度任务。通过用群组平均累积奖励替代近端策略优化基线，组相对策略优化消除了评论家估计误差并减少了训练偏差。受此基线替换启发，我们进一步提出单步策略优化，证明在同质车队条件下仅需单步群组奖励即可训练最优策略。在真实网约车数据集上的实验表明，组相对策略优化和单步策略优化在所有场景中均取得优异性能，仅使用简单的多层感知机网络即可有效优化接驾时间和完成订单数。此外，单步策略优化因消除了组相对策略优化有限时间视野导致的偏差，在所有场景中均优于组相对策略优化。我们的代码、训练模型和处理数据已发布于 https://github.com/RS2002/OSPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of order dispatch in ride-sharing systems with autonomous vehicles, where traditional multi-agent reinforcement learning methods suffer from value function estimation errors in large-scale uncertain environments. The authors propose two novel methods that bypass value function estimation by leveraging fleet homogeneity: Group Relative Policy Optimization (GRPO), which adapts group-based policy optimization to replace the critic with group average reward-to-go, and One-Step Policy Optimization (OSPO), which demonstrates that optimal policies can be trained using only one-step group rewards. Experiments on real-world ride-hailing data show that both methods achieve promising performance in optimizing pickup times and served orders, with OSPO outperforming GRPO by eliminating bias from bounded time horizons.</div>
<div class="mono" style="margin-top:8px">本文针对自动驾驶车辆共享出行系统中的订单调度问题，传统多智能体强化学习方法在大规模不确定环境中存在价值函数估计误差的挑战。作者提出了两种绕过价值函数估计的新方法，利用车队同质性：群体相对策略优化（GRPO）将基于群体的策略优化应用于订单调度，用群体平均未来奖励替代评论家；以及一步策略优化（OSPO），证明仅使用一步群体奖励即可训练最优策略。在真实网约车数据集上的实验表明，两种方法在优化接载时间和完成订单数方面均表现优异，其中OSPO通过消除GRPO有限时间范围带来的偏差，在所有场景中表现更优。</div>
</details>
</div>
<div class="card">
<div class="title">Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</div>
<div class="meta-line">Authors: Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun</div>
<div class="meta-line">First: 2025-12-31T04:17:36+00:00 · Latest: 2025-12-31T04:17:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24615v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24615v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \textbf{Workflow} mode for standard tasks and a \textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Youtu-Agent：通过自动化生成与混合策略优化提升智能体生产力</div>
<div class="mono" style="margin-top:8px">现有大语言模型（LLM）智能体框架面临两大挑战：高配置成本与静态能力局限。构建高质量智能体通常需要大量人工进行工具集成与提示工程，而已部署的智能体难以适应动态环境，且需昂贵微调。为此，我们提出Youtu-Agent——一个面向LLM智能体自动化生成与持续演进的模块化框架。该框架采用结构化配置系统，将执行环境、工具集与上下文管理解耦，支持灵活复用与自动化合成。我们引入两种生成范式：面向标准任务的“工作流”模式，以及面向复杂非标准需求的“元智能体”模式，可自动生成工具代码、提示词与配置。此外，Youtu-Agent建立了混合策略优化系统：（1）“智能体实践”模块支持智能体通过上下文优化积累经验、提升性能，无需参数更新；（2）“智能体强化学习”模块集成分布式训练框架，支持对任意Youtu-Agent进行端到端、大规模、可扩展的稳定强化学习。实验表明，Youtu-Agent在WebWalkerQA（71.47%）和GAIA（72.8%）基准上使用开源权重模型达到最优性能。自动化生成流程工具合成成功率超81%，实践模块在AIME 2024/2025上分别提升性能2.7%与5.4%。智能体强化学习训练在7B参数LLM上实现40%加速且性能稳定提升，在数学与通用/多跳问答基准上分别将代码推理与搜索能力提升达35%和21%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high manual configuration costs and static capabilities of existing LLM agent frameworks, this paper introduces Youtu-Agent, a modular framework for automated agent generation and continuous evolution. The method features a structured configuration system and two generation paradigms—Workflow and Meta-Agent—for automated synthesis of tools and prompts, alongside a hybrid policy optimization system combining in-context experience accumulation (Agent Practice) and scalable reinforcement learning (Agent RL). Experimental results show state-of-the-art performance on WebWalkerQA (71.47%) and GAIA (72.8%), an over 81% tool synthesis success rate, performance gains on AIME benchmarks (+2.7% to +5.4%), and Agent RL achieving a 40% speedup with up to 35% and 21% improvements on coding/reasoning and searching tasks.</div>
<div class="mono" style="margin-top:8px">针对现有大语言模型智能体框架配置成本高、能力静态化的挑战，本文提出了Youtu-Agent，一个用于自动化生成和持续演进的模块化框架。该方法采用结构化配置系统，提供工作流和元智能体两种生成范式，以自动合成工具和提示，并建立了混合策略优化系统，结合上下文经验积累（智能体实践）和可扩展强化学习（智能体RL）。实验结果表明，该框架在WebWalkerQA（71.47%）和GAIA（72.8%）上达到先进水平，工具合成成功率超过81%，在AIME基准上性能提升2.7%至5.4%，且智能体RL实现了40%的加速，在数学和通用/多跳问答基准上的编码/推理与搜索能力分别提升高达35%和21%。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization</div>
<div class="meta-line">Authors: Dong Qiu, Duo Xu, Limengxi Yue</div>
<div class="meta-line">First: 2025-12-31T03:59:18+00:00 · Latest: 2025-12-31T03:59:18+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICFTIC 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24609v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习增强型LLM智能体在协同决策与性能优化中的应用</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在语言任务中表现优异，但在多智能体环境中常缺乏协同意识，难以优化全局性能。本文提出一种强化学习增强的LLM智能体框架，将协作建模为去中心化部分可观测马尔可夫决策过程（Dec-POMDP），采用集中训练与分散执行（CTDE）机制。我们引入群体相对策略优化（GRPO），在训练期间结合全局信号联合优化智能体策略，并设计平衡任务质量、速度与协调成本的简化联合奖励函数。在协同写作与编程基准测试中，该框架相比单智能体基线实现任务处理速度提升3倍，写作任务中结构/风格一致性达98.7%，编程任务测试通过率达74.6%。该方法持续超越主流多智能体LLM基线，为复杂工作流中的可靠协作提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of Large Language Models (LLMs) in multi-agent collaboration, where they often lack cooperative awareness and struggle to optimize global performance. To overcome this, the authors propose a reinforcement learning-augmented LLM agent framework that models cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and employs centralized training with decentralized execution (CTDE). They introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies using global signals during training, alongside a simplified joint reward balancing task quality, speed, and coordination costs. Experimental results on collaborative writing and coding benchmarks show the framework achieves a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding, consistently outperforming strong multi-agent LLM baselines.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在多智能体协作中缺乏合作意识、难以优化全局性能的局限性展开研究。为解决这一问题，作者提出了一个强化学习增强的大语言模型智能体框架，将协作建模为去中心化部分可观测马尔可夫决策过程，并采用集中训练与分散执行的策略。他们引入了群体相对策略优化方法，在训练期间利用全局信号联合优化智能体策略，同时设计了一个平衡任务质量、速度和协调成本的简化联合奖励函数。在协作写作和编码基准测试中，该框架相比单智能体基线实现了任务处理速度3倍的提升，写作任务达到98.7%的结构/风格一致性，编码任务获得74.6%的测试通过率，持续优于现有的强大多智能体大语言模型基线。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning</div>
<div class="meta-line">Authors: Shanyu Han, Yangbo He, Yang Liu</div>
<div class="meta-line">First: 2025-12-31T03:13:22+00:00 · Latest: 2025-12-31T03:13:22+00:00</div>
<div class="meta-line">Comments: 63 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24580v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24580v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a novel framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty. We define two distinct yet coupled risk measures: an inner risk measure addressing state and cost randomness and an outer risk measure capturing transition dynamics uncertainty. Our framework unifies and generalizes most existing RL frameworks by permitting general coherent risk measures for both inner and outer risk measures. Within this framework, we construct a risk-sensitive robust Markov decision process (RSRMDP), derive its Bellman equation, and provide error analysis under a given posterior distribution. We further develop a Bayesian Dynamic Programming (Bayesian DP) algorithm that alternates between posterior updates and value iteration. The approach employs an estimator for the risk-based Bellman operator that combines Monte Carlo sampling with convex optimization, for which we prove strong consistency guarantees. Furthermore, we demonstrate that the algorithm converges to a near-optimal policy in the training environment and analyze both the sample complexity and the computational complexity under the Dirichlet posterior and CVaR. Finally, we validate our approach through two numerical experiments. The results exhibit excellent convergence properties while providing intuitive demonstrations of its advantages in both risk-sensitivity and robustness. Empirically, we further demonstrate the advantages of the proposed algorithm through an application on option hedging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒贝叶斯动态规划用于策略风险敏感强化学习</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的风险敏感强化学习（RSRL）框架，该框架融合了对状态转移不确定性的鲁棒性。我们定义了两个不同但耦合的风险度量：一个内部风险度量处理状态与成本的随机性，一个外部风险度量捕捉转移动态的不确定性。通过允许内部与外部风险度量采用一般的相干风险度量，我们的框架统一并推广了大多数现有强化学习框架。在此框架内，我们构建了风险敏感鲁棒马尔可夫决策过程（RSRMDP），推导出其贝尔曼方程，并在给定后验分布下提供了误差分析。我们进一步开发了一种贝叶斯动态规划（Bayesian DP）算法，该算法在后验更新与值迭代之间交替进行。该方法采用了一种基于风险的贝尔曼算子估计器，该估计器结合了蒙特卡洛采样与凸优化，我们证明了其强一致性保证。此外，我们证明了该算法在训练环境中收敛至接近最优的策略，并分析了在狄利克雷后验与条件风险价值（CVaR）下的样本复杂度和计算复杂度。最后，我们通过两个数值实验验证了所提方法。结果展示了优异的收敛特性，同时直观地证明了其在风险敏感性与鲁棒性方面的优势。实证中，我们进一步通过期权对冲应用展示了所提算法的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reinforcement learning that is both risk-sensitive and robust to uncertainties in transition dynamics, this paper introduces a novel framework that defines separate inner and outer coherent risk measures to address cost randomness and transition uncertainty, respectively. The method constructs a risk-sensitive robust Markov decision process (RSRMDP), derives its Bellman equation, and develops a Bayesian Dynamic Programming algorithm that alternates posterior updates with value iteration, using a Monte Carlo and convex optimization-based estimator for the risk-based Bellman operator with proven consistency. Experimental results from two numerical studies, including an option hedging application, demonstrate the algorithm&#x27;s strong convergence and advantages in balancing risk sensitivity with robustness.</div>
<div class="mono" style="margin-top:8px">本文的动机是开发一种既能对风险敏感又能抵御状态转移不确定性的强化学习框架，通过定义分别处理成本随机性和动态不确定性的内外双层相干风险度量来实现。方法上构建了风险敏感鲁棒马尔可夫决策过程，推导其贝尔曼方程，并提出一种贝叶斯动态规划算法，该算法交替进行后验更新与值迭代，并采用结合蒙特卡洛采样与凸优化的风险贝尔曼算子估计器，具有严格的一致性保证。主要实验结果基于两项数值实验（包括期权对冲应用）显示，该算法收敛性良好，并在风险敏感性与鲁棒性方面展现出直观优势。</div>
</details>
</div>
<div class="card">
<div class="title">From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme</div>
<div class="meta-line">Authors: Xueyan Li, Yingyi Xue, Mengjie Jiang, Qingzi Zhu, Yazhe Niu</div>
<div class="meta-line">First: 2025-12-31T01:35:49+00:00 · Latest: 2025-12-31T01:35:49+00:00</div>
<div class="meta-line">Comments: 46 pages, 20 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24555v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24555v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating humorous memes is a challenging multimodal task that moves beyond direct image-to-caption supervision. It requires a nuanced reasoning over visual content, contextual cues, and subjective humor. To bridge this gap between visual perception and humorous punchline creation, we propose HUMOR}, a novel framework that guides VLMs through hierarchical reasoning and aligns them with group-wise human preferences. First, HUMOR employs a hierarchical, multi-path Chain-of-Thought (CoT): the model begins by identifying a template-level intent, then explores diverse reasoning paths under different contexts, and finally anchors onto a high-quality, context-specific path. This CoT supervision, which traces back from ground-truth captions, enhances reasoning diversity. We further analyze that this multi-path exploration with anchoring maintains a high expected humor quality, under the practical condition that high-quality paths retain significant probability mass. Second, to capture subjective humor, we train a pairwise reward model that operates within groups of memes sharing the same template. Following established theory, this approach ensures a consistent and robust proxy for human preference, even with subjective and noisy labels. The reward model then enables a group-wise reinforcement learning optimization, guaranteeing providing a theoretical guarantee for monotonic improvement within the trust region. Extensive experiments show that HUMOR empowers various VLMs with superior reasoning diversity, more reliable preference alignment, and higher overall meme quality. Beyond memes, our work presents a general training paradigm for open-ended, human-aligned multimodal generation, where success is guided by comparative judgment within coherent output group.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从感知到笑点：赋能视觉语言模型掌握开放场景表情包艺术</div>
<div class="mono" style="margin-top:8px">生成幽默表情包是一项超越直接图像-标题监督的复杂多模态任务，需要对视觉内容、上下文线索和主观幽默进行细致推理。为弥合视觉感知与幽默笑点创作间的鸿沟，我们提出HUMOR框架，通过分层推理引导视觉语言模型，并将其与群体化人类偏好对齐。首先，HUMOR采用分层多路径思维链：模型先识别模板级意图，随后在不同语境下探索多样化推理路径，最终锚定高质量的情境特定路径。这种从真实标注回溯的思维链监督增强了推理多样性。我们进一步分析表明，在高质量路径保持显著概率质量的现实条件下，这种带锚定的多路径探索能维持较高的期望幽默质量。其次，为捕捉主观幽默，我们训练了在共享相同模板的表情包群组内运作的成对奖励模型。依据成熟理论，该方法即使面对主观且含噪声的标注，也能确保获得稳健一致的人类偏好代理。该奖励模型进而支持群组化强化学习优化，为信任域内的单调改进提供理论保证。大量实验表明，HUMOR能赋予各类视觉语言模型更卓越的推理多样性、更可靠的偏好对齐能力及更高的整体表情包质量。超越表情包领域，本研究为开放式、人类对齐的多模态生成提供了通用训练范式，其成功关键在于对连贯输出群组内的比较判断机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of generating humorous memes, which requires nuanced multimodal reasoning beyond simple captioning, this paper introduces HUMOR, a framework that enhances Vision-Language Models (VLMs) through hierarchical reasoning and group-wise human preference alignment. The method employs a multi-path Chain-of-Thought approach where the model first identifies a template-level intent, explores diverse reasoning paths, and anchors to a high-quality context-specific path, supervised by ground-truth captions to boost diversity; it further uses a pairwise reward model trained on meme groups to capture subjective humor, enabling group-wise reinforcement learning with theoretical guarantees. Experimental results demonstrate that HUMOR equips various VLMs with improved reasoning diversity, better preference alignment, and higher overall meme quality, offering a general paradigm for open-ended, human-aligned multimodal generation.</div>
<div class="mono" style="margin-top:8px">本文针对生成幽默表情包这一超越简单图像描述的多模态挑战，提出了HUMOR框架，旨在通过分层推理和群体偏好对齐增强视觉语言模型。方法采用多路径思维链，模型先识别模板级意图，探索多样推理路径，并锚定高质量上下文路径，基于真实标注监督以提升多样性；同时使用在表情包组上训练的成对奖励模型捕捉主观幽默，实现具有理论保证的群体强化学习。实验结果表明，HUMOR能赋予多种视觉语言模型更优的推理多样性、更可靠的偏好对齐及更高的整体表情包质量，为开放式、人类对齐的多模态生成提供了通用训练范式。</div>
</details>
</div>
<div class="card">
<div class="title">From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning</div>
<div class="meta-line">Authors: Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera</div>
<div class="meta-line">First: 2025-12-31T00:36:03+00:00 · Latest: 2025-12-31T00:36:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24532v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24532v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从基础模块到规划：基于强化学习的大语言模型多步空间推理</div>
<div class="mono" style="margin-top:8px">大语言模型的空间推理能力因在导航与规划领域的应用而日益受到关注。尽管具备强大的通用语言能力，大语言模型在结构化环境中的空间变换与多步规划任务上仍存在困难。本文提出一种两阶段方法，将空间推理分解为基础模块及其组合过程：首先通过监督微调学习旋转、平移、缩放等基础空间变换，使模型掌握空间物理规律；随后冻结该物理感知模型，在GRPO框架内训练轻量级LoRA适配器，以闭环方式学习组合基础模块实现谜题环境中的多步规划。为支持该流程，我们构建了ASCII艺术数据集及对应的强化学习环境。实验表明，在具有显式状态更新的动态环境和需依赖内部状态跨步推理的静态环境中，本方法均持续优于基线模型（包括通用主干模型、物理感知模型及端到端强化学习模型）。此外，相较于从零开始的端到端强化学习，该方法收敛更快且训练更稳定。最后，我们通过注意力模式分析验证了微调对空间理解能力的实质性提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling large language models (LLMs) to perform multi-step spatial reasoning, which is crucial for navigation and planning tasks, as current models struggle with spatial transformations and planning in structured environments. The proposed method employs a two-stage approach: first, supervised fine-tuning teaches the model basic spatial physics through atomic transformations like rotation and translation; second, with the physics-aware model frozen, lightweight LoRA adapters are trained using the GRPO reinforcement learning framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, supported by a synthesized ASCII-art dataset and environment. Experimental results show that this approach consistently outperforms baselines, including the generic backbone and end-to-end RL models, in both Dynamic and Static environments, with faster convergence and more stable training, and attention pattern analysis indicates meaningful improvements in spatial understanding.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在空间推理中难以处理空间变换和多步规划的问题展开研究，这对于导航和规划应用至关重要。所提出的方法采用两阶段策略：首先，通过监督微调让模型学习旋转、平移等基本空间物理变换；其次，在冻结这一具备物理感知的模型后，使用GRPO强化学习框架训练轻量级LoRA适配器，以学习在基于谜题的环境中组合这些基础模块进行多步规划的策略，并辅以合成的ASCII艺术数据集和环境。实验结果表明，该方法在动态和静态环境中均优于基线模型，包括通用骨干模型和端到端强化学习模型，且收敛更快、训练更稳定，注意力模式分析也证实了空间理解能力的实质性提升。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-fidelity Bayesian Optimization: A Review</div>
<div class="meta-line">Authors: Bach Do, Ruda Zhang</div>
<div class="meta-line">Venue: AIAA Journal 63:6 (2025) 2286-2322</div>
<div class="meta-line">First: 2023-11-21T23:22:11+00:00 · Latest: 2025-12-31T00:31:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2311.13050v3">Abs</a> · <a href="https://arxiv.org/pdf/2311.13050v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resided at the intersection of multi-fidelity optimization (MFO) and Bayesian optimization (BO), MF BO has found a niche in solving expensive engineering design optimization problems, thanks to its advantages in incorporating physical and mathematical understandings of the problems, saving resources, addressing exploitation-exploration trade-off, considering uncertainty, and processing parallel computing. The increasing number of works dedicated to MF BO suggests the need for a comprehensive review of this advanced optimization technique. In this paper, we survey recent developments of two essential ingredients of MF BO: Gaussian process (GP) based MF surrogates and acquisition functions. We first categorize the existing MF modeling methods and MFO strategies to locate MF BO in a large family of surrogate-based optimization and MFO algorithms. We then exploit the common properties shared between the methods from each ingredient of MF BO to describe important GP-based MF surrogate models and review various acquisition functions. By doing so, we expect to provide a structured understanding of MF BO. Finally, we attempt to reveal important aspects that require further research for applications of MF BO in solving intricate yet important design optimization problems, including constrained optimization, high-dimensional optimization, optimization under uncertainty, and multi-objective optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多保真度贝叶斯优化：综述</div>
<div class="mono" style="margin-top:8px">多保真度贝叶斯优化（MF BO）结合了多保真度优化（MFO）与贝叶斯优化（BO），凭借其在融入问题物理与数学理解、节约资源、平衡探索与利用、处理不确定性及并行计算方面的优势，已成为解决昂贵工程设计优化问题的重要方法。随着相关研究日益增多，对这一先进优化技术进行全面综述的需求日益凸显。本文系统回顾了MF BO的两个核心要素：基于高斯过程（GP）的多保真度代理模型与采集函数。首先通过分类现有MF建模方法与MFO策略，将MF BO置于基于代理的优化及MFO算法体系中进行定位；进而基于各要素方法的共性，阐述重要的GP多保真度代理模型并综述各类采集函数，以构建对MF BO的结构化认知。最后，针对MF BO在复杂关键设计优化问题（包括约束优化、高维优化、不确定性优化及多目标优化）中的应用，探讨了需进一步研究的重要方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to efficiently solve expensive engineering design problems, this review paper surveys multi-fidelity Bayesian optimization (MF BO), which combines multi-fidelity optimization with Bayesian optimization to leverage cheaper low-fidelity data and manage the exploration-exploitation trade-off. The method focuses on two core components: Gaussian process-based multi-fidelity surrogate models for approximating objective functions across fidelity levels, and acquisition functions for guiding the optimization process. The main experimental results, derived from synthesizing the literature, categorize existing MF modeling techniques and optimization strategies, providing a structured overview of the field and identifying key research gaps in areas like constrained, high-dimensional, and multi-objective optimization.</div>
<div class="mono" style="margin-top:8px">本文旨在高效解决昂贵的工程设计优化问题，综述了多保真度贝叶斯优化方法，该方法结合多保真度优化与贝叶斯优化，以利用低成本低保真度数据并平衡探索与利用。其方法核心聚焦于两个组成部分：基于高斯过程的多保真度代理模型，用于跨保真度层级近似目标函数；以及采集函数，用于指导优化过程。通过综合文献，主要实验成果对现有的多保真度建模技术和优化策略进行了分类，提供了该领域的结构化概览，并指出了在约束优化、高维优化及多目标优化等重要方向上的研究空白。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics</div>
<div class="meta-line">Authors: Akash Samanta, Sheldon Williamson</div>
<div class="meta-line">First: 2025-12-30T19:57:52+00:00 · Latest: 2025-12-30T19:57:52+00:00</div>
<div class="meta-line">Comments: This preprint focuses on the theoretical framework and diagnostic behavior. Comprehensive experimental validation in application-specific settings is deferred to a companion experimental study</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24445v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Building on this framework, we derive diagnostic-driven instantiations including a stabilized supervised optimizer, a diagnostic-regulated actor-critic scheme, and a diagnostic-conditioned learned optimizer. Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to temporal-difference error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏差-噪声-对齐诊断的自适应学习框架</div>
<div class="mono" style="margin-top:8px">部署于非平稳与安全关键环境的学习系统常面临学习动态演化时的不稳定性、收敛缓慢或脆弱适应等问题。现有优化、强化学习与元学习方法虽能适应梯度统计量，却普遍忽略误差信号本身的时间结构。本文提出一种诊断驱动的自适应学习框架，通过将误差演化分解为三个原则性分量进行显式建模：捕捉持续漂移的偏差、捕捉随机波动的噪声、以及捕捉导致超调的重现方向激励的对齐量。这些诊断指标通过损失函数或时序差分误差轨迹的轻量级统计量在线计算，且独立于模型架构与任务领域。研究表明，所提出的偏差-噪声-对齐分解为监督优化、演员-评论家强化学习及可学习优化器提供了统一控制框架。基于此框架，我们推导出诊断驱动的具体实现方案：包括稳定化监督优化器、诊断调节型演员-评论家机制及诊断条件化可学习优化器。在标准平滑性假设下，我们为所有案例建立了有效更新的有界性与稳定性证明。演员-评论家学习中的典型诊断示例揭示了所提信号如何根据时序差分误差结构调节适应过程。总体而言，本研究将误差演化提升为自适应学习的一类核心对象，为动态环境中的可靠学习提供了可解释、轻量化的理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the instability and slow convergence of learning systems in nonstationary environments, this paper introduces a diagnostic-driven adaptive learning framework that models error evolution through a decomposition into bias, noise, and alignment components. The method computes these diagnostics online from lightweight statistics of error trajectories, independent of model architecture or task domain, and uses them to derive stabilized optimizers for supervised learning, actor-critic reinforcement learning, and learned optimizers. Main experimental results, under standard smoothness assumptions, demonstrate bounded effective updates and stability, with illustrative examples in actor-critic learning showing how the diagnostics modulate adaptation based on temporal-difference error structure.</div>
<div class="mono" style="margin-top:8px">针对非平稳环境中学习系统的不稳定性和收敛缓慢问题，本文提出了一种诊断驱动的自适应学习框架，通过将误差分解为偏差、噪声和对齐分量来建模误差演化。该方法从误差轨迹的轻量级统计中在线计算这些诊断指标，独立于模型架构或任务领域，并利用它们推导出用于监督学习、演员-评论家强化学习和学习优化器的稳定优化器。在标准平滑性假设下，主要实验结果证明了有效更新的有界性和稳定性，演员-批评家学习中的代表性示例展示了诊断如何根据时序差分误差结构调节自适应过程。</div>
</details>
</div>
<div class="card">
<div class="title">Lagrangian Index Policy for Restless Bandits with Average Reward</div>
<div class="meta-line">Authors: Konstantin Avrachenkov, Vivek S. Borkar, Pratik Shah</div>
<div class="meta-line">First: 2024-12-17T08:03:53+00:00 · Latest: 2025-12-30T19:29:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.12641v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.12641v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the Lagrangian Index Policy (LIP) for restless multi-armed bandits with long-run average reward. In particular, we compare the performance of LIP with the performance of the Whittle Index Policy (WIP), both heuristic policies known to be asymptotically optimal under certain natural conditions. Even though in most cases their performances are very similar, in the cases when WIP shows bad performance, LIP continues to perform very well. We then propose reinforcement learning algorithms, both tabular and NN-based, to obtain online learning schemes for LIP in the model-free setting. The proposed reinforcement learning schemes for LIP require significantly less memory than the analogous schemes for WIP. We calculate analytically the Lagrangian index for the restart model, which applies to the optimal web crawling and the minimization of the weighted age of information. We also give a new proof of asymptotic optimality in case of homogeneous arms as the number of arms goes to infinity, based on exchangeability and de Finetti&#x27;s theorem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平均奖励下的不安定多臂老虎机拉格朗日指数策略</div>
<div class="mono" style="margin-top:8px">本文研究了针对长期平均奖励的不安定多臂老虎机问题的拉格朗日指数策略（LIP）。特别地，我们将LIP与Whittle指数策略（WIP）的性能进行了比较，这两种启发式策略在特定自然条件下均具有渐近最优性。尽管在多数情况下两者性能相近，但在WIP表现不佳的场景中，LIP仍能保持优异性能。我们进一步提出了基于表格和神经网络的强化学习算法，为无模型环境下的LIP实现在线学习方案。相较于WIP的同类方案，所提出的LIP强化学习方案显著降低了内存需求。通过解析计算重启模型的拉格朗日指数，我们将其应用于最优网络爬取和加权信息年龄最小化问题。此外，基于可交换性和德菲内蒂定理，我们为同质臂情形（当臂数量趋于无穷时）提供了渐近最优性的新证明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient heuristics in restless multi-armed bandits with average reward, this paper analyzes the Lagrangian Index Policy (LIP) and compares it to the Whittle Index Policy (WIP), both of which are asymptotically optimal under certain conditions. The method involves theoretical analysis and the proposal of model-free reinforcement learning algorithms, both tabular and neural network-based, to implement LIP online. The main experimental results show that LIP performs well even when WIP performs poorly, and the proposed learning schemes for LIP require significantly less memory than those for WIP, with analytical calculations provided for specific models like restart applications in web crawling and age of information minimization, alongside a new proof of asymptotic optimality for homogeneous arms.</div>
<div class="mono" style="margin-top:8px">本文针对具有平均奖励的躁动多臂老虎机问题，旨在研究拉格朗日指数策略（LIP）的性能，并与惠特尔指数策略（WIP）进行比较，这两种启发式策略在特定条件下具有渐近最优性。方法上，通过理论分析并提出了无模型强化学习算法（包括表格型和神经网络型）来在线实现LIP。主要实验结果表明，即使在WIP表现不佳的情况下，LIP仍能保持良好性能，且所提出的LIP学习方案比WIP方案所需内存显著减少；论文还解析计算了重启模型（适用于最优网络爬取和加权信息年龄最小化）的拉格朗日指数，并基于可交换性和德菲内蒂定理给出了同质臂情况下的渐近最优性新证明。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
