<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-18 03:58</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260218_0358</div>
    <div class="row"><div class="card">
<div class="title">Privileged Information Distillation for Language Models</div>
<div class="meta-line">Authors: Emiliano Penaloza, Dheeraj Vattikonda, Nicolas Gontier, Alexandre Lacoste, Laurent Charlin, Massimo Caccia</div>
<div class="meta-line">First: 2026-02-04T18:46:17+00:00 · Latest: 2026-02-16T18:57:38+00:00</div>
<div class="meta-line">Comments: Abstract border should have been purple</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04942v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.04942v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, which typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable, but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically, we find that π-Distill and, in some cases, OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型的特权信息蒸馏</div>
<div class="mono" style="margin-top:8px">训练阶段的特权信息（PI）能使语言模型在原本会失败的任务上取得成功，这使其成为困难、长周期场景下强化学习的强大工具。然而，将借助PI习得的能力迁移至推理时无法使用PI的策略，仍是一个根本性挑战。我们在为多轮智能体环境蒸馏前沿模型的背景下研究该问题——这类环境通常隐藏内部推理过程，仅暴露行动轨迹。这使得标准蒸馏流程失效，因为成功行为可观测，但推理过程不可见。为此，我们提出π-Distill：一种联合师生目标函数，使用同一模型同步训练PI条件化教师模型与无条件化学生模型。同时，我们还提出策略上自蒸馏（OPSD）——通过强化学习训练学生模型，并在其与PI条件化教师模型间施加反向KL惩罚的替代方案。实验表明，这两种算法均能有效利用仅含行动信息的PI蒸馏前沿智能体。具体而言，π-Distill（及某些场景下的OPSD）在多个智能体基准测试、模型架构和PI形式中，均优于假设能获取完整思维链监督的行业标准流程（监督微调后接强化学习）。我们通过深入分析补充实验结果，重点解析π-Distill实现PI有效学习的关键因素，并阐明OPSD具备竞争力的适用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of distilling capabilities from language models trained with privileged information (PI), which is unavailable at inference, particularly in multi-turn agentic environments where only action trajectories are observable. It introduces two methods: π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously, and On-Policy Self-Distillation (OPSD), which uses reinforcement learning with a reverse KL-penalty. Experimental results show that both methods effectively distill frontier agents using action-only PI, outperforming standard practices like supervised fine-tuning followed by RL across multiple benchmarks, with π-Distill generally performing well and OPSD being competitive in some cases.</div>
<div class="mono" style="margin-top:8px">本文研究了在推理时无法获取特权信息（PI）的情况下，如何从训练时使用PI的语言模型中蒸馏能力，特别是在仅能观察行动轨迹的多轮智能体环境中。提出了两种方法：π-Distill，一种联合教师-学生目标，同时训练PI条件化教师和无条件学生；以及基于策略的自蒸馏（OPSD），使用带有反向KL惩罚的强化学习。实验结果表明，这两种方法利用仅行动PI有效蒸馏了前沿智能体，在多个基准测试中优于监督微调后接RL的标准做法，其中π-Distill表现普遍良好，OPSD在某些情况下具有竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Cold-Start Personalization via Training-Free Priors from Structured World Models</div>
<div class="meta-line">Authors: Avinandan Bose, Shuyue Stella Li, Faeze Brahman, Pang Wei Koh, Simon Shaolei Du, Yulia Tsvetkov, Maryam Fazel, Lin Xiao, Asli Celikyilmaz</div>
<div class="meta-line">First: 2026-02-16T18:52:13+00:00 · Latest: 2026-02-16T18:52:13+00:00</div>
<div class="meta-line">Comments: 24 pages, 4 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15012v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15012v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users&#x27; stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结构化世界模型免训练先验的冷启动个性化方法</div>
<div class="mono" style="margin-top:8px">冷启动个性化需要在缺乏用户历史数据时通过交互推断用户偏好。核心挑战是路径选择问题：每项任务涉及数十个偏好维度，但个体用户仅关注少数维度，且关键维度因人而异。在有限提问次数下，无结构的提问将遗漏重要维度。强化学习虽是自然建模方式，但在多轮交互中，其最终奖励无法利用偏好数据的因子化、按准则划分的结构，实践中学习到的策略会退化为忽略用户响应的静态提问序列。我们提出将冷启动偏好获取分解为离线结构学习与在线贝叶斯推断：Pep（基于先验的偏好获取）框架离线从完整画像中学习偏好相关性的结构化世界模型，在线执行免训练的贝叶斯推断以选择信息量最大的问题并预测完整偏好画像（包括未询问的维度）。该框架对下游求解器具有模块化特性，仅需简单的信念模型。在医学、数学、社会及常识推理任务中，Pep生成响应与用户声明偏好的对齐度达80.8%（强化学习为68.5%），交互次数减少3-5倍。当两名用户对同一问题给出不同答案时，Pep调整后续问题的概率为39-62%（强化学习为0-28%）。该模型仅需约1万参数（强化学习需80亿），表明冷启动获取的瓶颈在于利用偏好数据因子化结构的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of cold-start personalization, where systems must infer user preferences without prior data, by framing it as a routing problem to identify the few relevant dimensions among many. The proposed method, Pep, decomposes the task into offline learning of a structured world model that captures preference correlations from complete profiles, followed by online Bayesian inference to select informative questions and predict full profiles, including unasked dimensions. Experimental results across medical, mathematical, social, and commonsense reasoning domains show that Pep achieves 80.8% alignment with user preferences compared to 68.5% for reinforcement learning baselines, requires 3-5 times fewer interactions, adapts follow-up questions more frequently based on user responses, and does so with only about 10K parameters versus 8 billion for RL, demonstrating efficiency and effectiveness.</div>
<div class="mono" style="margin-top:8px">本文针对冷启动个性化问题展开研究，该问题要求在无用户历史数据的情况下通过交互推断用户偏好，核心挑战在于从众多偏好维度中识别少数关键维度。所提出的方法Pep将任务分解为离线学习和在线推理：离线阶段从完整配置文件中学习捕捉偏好相关性的结构化世界模型，在线阶段则通过无训练的贝叶斯推理选择信息性问题并预测完整偏好配置文件，包括未询问的维度。在医学、数学、社会和常识推理领域的实验结果表明，Pep与用户偏好的对齐度达到80.8%，优于强化学习基线的68.5%，交互次数减少3-5倍，能根据用户回答更频繁地调整后续问题，且仅需约1万个参数，而强化学习需要80亿个参数，体现了其在利用偏好数据结构化信息方面的优越性。</div>
</details>
</div>
<div class="card">
<div class="title">Evolution Strategies at the Hyperscale</div>
<div class="meta-line">Authors: Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio León Villares, Anya Sims, Clarisse Wibault, Dmitry Samsonov, Dylan Cope, Jarek Liesen, Kang Li, Lukas Seier, Theo Wolf, Uljad Berdica, Valentin Mohl, Alexander David Goldie, Aaron Courville, Karin Sevegnani, Shimon Whiteson, Jakob Nicolaus Foerster</div>
<div class="meta-line">First: 2025-11-20T18:56:05+00:00 · Latest: 2026-02-16T18:01:18+00:00</div>
<div class="meta-line">Comments: 76 pages, 15 figures, Website at https://eshyperscale.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16652v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16652v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://eshyperscale.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evolution Strategies (ES) is a class of powerful black-box optimisation methods that are highly parallelisable and can handle non-differentiable and noisy objectives. However, naïve ES becomes prohibitively expensive at scale on GPUs due to the low arithmetic intensity of batched matrix multiplications with unstructured random perturbations. We introduce Evolution Guided GeneRal Optimisation via Low-rank Learning (EGGROLL), which improves arithmetic intensity by structuring individual perturbations as rank-$r$ matrices, resulting in a hundredfold increase in training speed for billion-parameter models at large population sizes, achieving up to 91% of the throughput of pure batch inference. We provide a rigorous theoretical analysis of Gaussian ES for high-dimensional parameter objectives, investigating conditions needed for ES updates to converge in high dimensions. Our results reveal a linearising effect, and proving consistency between EGGROLL and ES as parameter dimension increases. Our experiments show that EGGROLL: (1) enables the stable pretraining of nonlinear recurrent language models that operate purely in integer datatypes, (2) is competitive with GRPO for post-training LLMs on reasoning tasks, and (3) does not compromise performance compared to ES in tabula rasa RL settings, despite being faster.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超大规模下的进化策略</div>
<div class="mono" style="margin-top:8px">进化策略（ES）是一类强大的黑盒优化方法，具有高度可并行性，能够处理不可微且含噪声的目标函数。然而，由于非结构化随机扰动的批量矩阵乘法算术强度较低，朴素ES在GPU上大规模运行时成本过高。我们提出了基于低秩学习的进化引导通用优化方法（EGGROLL），通过将个体扰动构建为秩-$r$矩阵来提高算术强度，使得十亿参数模型在大种群规模下的训练速度提升百倍，达到纯批量推理吞吐量的91%。我们对高维参数目标的高斯ES进行了严格的理论分析，探讨了ES更新在高维空间中收敛所需的条件。研究结果揭示了线性化效应，并证明了EGGROLL与ES在参数维度增加时的一致性。实验表明EGGROLL：（1）能够稳定预训练纯整数数据类型的非线性循环语言模型；（2）在推理任务的后训练LLM中与GRPO具有竞争力；（3）在空白强化学习场景中，尽管速度更快，但性能未逊于ES。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the computational inefficiency of traditional Evolution Strategies (ES) when scaling to billion-parameter models on GPUs, due to the low arithmetic intensity of operations with unstructured random perturbations. The method introduces EGGROLL, which structures individual perturbations as low-rank matrices to dramatically improve arithmetic intensity and training speed, while providing a theoretical analysis of ES convergence in high dimensions. Key experimental results demonstrate that EGGROLL achieves up to 91% of the throughput of pure batch inference, enables stable pretraining of integer-based recurrent language models, remains competitive with methods like GRPO for post-training LLMs on reasoning, and matches ES performance in reinforcement learning without sacrificing speed.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决传统进化策略在GPU上扩展到十亿参数模型时，由于非结构化随机扰动操作算术强度低而导致的计算效率低下问题。方法上提出了EGGROLL，通过将个体扰动构建为低秩矩阵来显著提高算术强度和训练速度，并对高维参数空间中ES的收敛性进行了理论分析。主要实验结果表明，EGGROLL在大型种群规模下实现了高达纯批量推理91%的吞吐量，能够稳定预训练纯整数数据类型的非线性循环语言模型，在大语言模型推理任务的后训练上与GRPO等方法竞争力相当，并且在强化学习场景中在不损失速度的前提下保持了与ES相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">DiffusionNFT: Online Diffusion Reinforcement with Forward Process</div>
<div class="meta-line">Authors: Kaiwen Zheng, Huayu Chen, Haotian Ye, Haoxiang Wang, Qinsheng Zhang, Kai Jiang, Hang Su, Stefano Ermon, Jun Zhu, Ming-Yu Liu</div>
<div class="meta-line">Venue: ICLR 2026 Oral</div>
<div class="meta-line">First: 2025-09-19T16:09:33+00:00 · Latest: 2026-02-16T17:14:06+00:00</div>
<div class="meta-line">Comments: ICLR 2026 Oral</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16117v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.16117v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffusionNFT：基于前向过程的在线扩散强化学习</div>
<div class="mono" style="margin-top:8px">在线强化学习（RL）已成为语言模型后训练的核心方法，但其在扩散模型中的扩展因似然函数难以处理而面临挑战。现有研究通过离散化逆向采样过程实现GRPO式训练，但仍存在求解器限制、前向-逆向不一致性、以及与无分类器引导（CFG）结合复杂等根本缺陷。本文提出扩散负感知微调（DiffusionNFT），一种基于流匹配直接在前向过程优化扩散模型的新型在线RL范式。DiffusionNFT通过对比正负样本来定义隐式策略改进方向，自然地将强化信号融入监督学习目标。该框架支持任意黑盒求解器训练，无需似然估计，且策略优化仅需干净图像而非采样轨迹。在直接对比中，DiffusionNFT比FlowGRPO效率提升高达25倍，且无需CFG。例如，DiffusionNFT在1千步内将GenEval分数从0.24提升至0.98，而FlowGRPO需超过5千步并依赖CFG才达到0.95。通过整合多个奖励模型，DiffusionNFT在各项基准测试中显著提升了SD3.5-Medium的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces DiffusionNFT, a novel online reinforcement learning paradigm for diffusion models that addresses the challenges of applying traditional RL methods, which struggle with intractable likelihoods and complex integrations like classifier-free guidance. The method optimizes diffusion models directly on the forward process via flow matching, contrasting positive and negative generations to define an implicit policy improvement direction, thereby incorporating reinforcement signals into a supervised learning objective without needing likelihood estimation or sampling trajectories. Experimental results show that DiffusionNFT is up to 25 times more efficient than prior methods like FlowGRPO, improving the GenEval score from 0.24 to 0.98 within 1,000 steps without classifier-free guidance, and it significantly boosts the performance of models such as SD3.5-Medium across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文提出了DiffusionNFT，一种用于扩散模型的新型在线强化学习范式，旨在解决传统强化学习方法因难以处理似然估计和复杂集成（如无分类器引导）而面临的挑战。该方法通过流匹配直接在正向过程中优化扩散模型，通过对比正负生成样本来定义隐式的策略改进方向，从而将强化信号融入监督学习目标，无需似然估计或采样轨迹。实验结果表明，DiffusionNFT比FlowGRPO等现有方法效率提升高达25倍，在无需无分类器引导的情况下，仅用1,000步就将GenEval分数从0.24提升至0.98，并显著提升了SD3.5-Medium模型在多个基准测试中的性能。</div>
</details>
</div>
<div class="card">
<div class="title">MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design</div>
<div class="meta-line">Authors: Gen Zhou, Sugitha Janarthanan, Lianghong Chen, Pingzhao Hu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-16T17:01:47+00:00 · Latest: 2026-02-16T17:01:47+00:00</div>
<div class="meta-line">Comments: This paper is published in ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14926v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14926v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a &#x27;black box&#x27;. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAC-AMP：面向多目标抗菌肽设计的闭环多智能体协作系统</div>
<div class="mono" style="margin-top:8px">为应对全球抗菌素耐药性威胁，抗菌肽因其对抗耐药病原体的强效潜力备受关注。尽管人工智能已应用于抗菌肽的发现与设计，但现有模型多采用僵化或模糊的评分方法，难以平衡活性、毒性与新颖性等关键目标，导致结果难以解释与优化。随着大语言模型能力的快速演进，基于此类模型的多智能体协作系统在复杂科学设计场景中展现出巨大潜力。为此，我们提出MAC-AMP——一个面向多目标抗菌肽设计的闭环多智能体协作系统。该系统采用全自主模拟同行评审-自适应强化学习框架，仅需任务描述和示例数据集即可设计新型抗菌肽。本研究的创新在于引入具有跨领域可迁移性的闭环多智能体系统，在支持多目标优化的同时保持可解释性（非“黑箱”）。实验表明，MAC-AMP能有效优化多个关键分子特性，在抗菌活性、抗菌肽相似性、毒性合规及结构可靠性方面均显著优于现有生成模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to combat antimicrobial resistance and the limitations of existing AI models in balancing multiple design objectives like activity, toxicity, and novelty, this paper proposes MAC-AMP, a closed-loop multi-agent collaboration system based on large language models for multi-objective antimicrobial peptide design. The method employs a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to generate novel peptides, emphasizing explainability and cross-domain transferability rather than operating as a black box. Experimental results demonstrate that MAC-AMP outperforms other generative models by effectively optimizing key molecular properties, achieving exceptional performance in antibacterial activity, peptide likeliness, toxicity compliance, and structural reliability.</div>
<div class="mono" style="margin-top:8px">为应对全球抗菌素耐药性威胁，并针对现有人工智能模型在平衡活性、毒性和新颖性等多重设计目标上的不足，本文提出了MAC-AMP，一个基于大语言模型的闭环多智能体协作系统，用于多目标抗菌肽设计。该方法采用完全自主的模拟同行评审-自适应强化学习框架，仅需任务描述和示例数据集即可生成新型肽，强调可解释性和跨领域可迁移性，而非黑箱操作。实验结果表明，MAC-AMP在优化关键分子特性方面优于其他生成模型，在抗菌活性、肽似然性、毒性合规性和结构可靠性上均取得了优异表现。</div>
</details>
</div>
<div class="card">
<div class="title">BFS-PO: Best-First Search for Large Reasoning Models</div>
<div class="meta-line">Authors: Fiorenzo Parascandolo, Wenhui Tan, Enver Sangineto, Ruihua Song, Rita Cucchiara</div>
<div class="meta-line">First: 2026-02-16T16:53:41+00:00 · Latest: 2026-02-16T16:53:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14917v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BFS-PO：面向大型推理模型的最佳优先搜索算法</div>
<div class="mono" style="margin-top:8px">大型推理模型（如OpenAI o1和DeepSeek-R1）在长推理链任务中表现出色，但这也导致计算成本显著增加并产生冗长输出，即“过度思考”现象。强化学习算法（如GRPO/DAPO）往往会加剧该趋势。本文提出BFS-PO算法，采用最佳优先搜索探索策略缓解此问题：通过基于最大熵节点的回溯机制寻找最短正确答案，在训练中逐步生成更简短响应，从而学习生成简洁推理链。实验表明，BFS-PO能在提升模型准确率的同时缩短答案长度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the overthinking problem in Large Reasoning Models (LRMs), where models like OpenAI o1 and DeepSeek-R1 generate excessively long reasoning chains, increasing computational costs and verbosity, often worsened by RL algorithms such as GRPO/DAPO. The method proposed, BFS-PO, employs a Best-First Search exploration strategy with backtracking based on maximum entropy nodes to find the shortest correct answer, training the model to produce progressively shorter responses. Experimental results across various benchmarks and base LRMs demonstrate that BFS-PO simultaneously improves model accuracy and reduces answer length.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型推理模型中的过度思考问题，即如OpenAI o1和DeepSeek-R1等模型在推理任务中生成过长推理链，导致计算成本增加和输出冗长，且GRPO/DAPO等强化学习算法常加剧此现象。所提出的BFS-PO方法采用最佳优先搜索探索策略，基于最大熵节点进行回溯以寻找最短正确答案，通过训练逐步缩短响应来学习生成简洁推理链。在不同基准测试和基础大型推理模型上的实验结果表明，BFS-PO能同时提高模型准确率并缩短答案长度。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Bayesian Optimisation with Unbounded Corruptions</div>
<div class="meta-line">Authors: Abdelhamid Ezzerg, Ilija Bogunovic, Jeremias Knoblauch</div>
<div class="meta-line">First: 2025-11-19T10:28:56+00:00 · Latest: 2026-02-16T16:22:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.15315v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.15315v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/4})$ and $O(T^{1/7})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB&#x27;s regret bounds match those of the standard GP-UCB algorithm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无界扰动下的鲁棒贝叶斯优化</div>
<div class="mono" style="margin-top:8px">贝叶斯优化对极端异常值极为敏感。现有可证明鲁棒的方法通常假设累积扰动预算有界，这使得它们即使面对单个足够大的扰动也毫无防御能力。为解决此问题，我们引入一种新型对抗模型，其预算仅受扰动频率限制，而不受扰动幅度限制。随后我们推导出RCGP-UCB算法，该算法将著名的置信上界方法与鲁棒共轭高斯过程相结合。我们提出了RCGP-UCB的稳定版本与自适应版本，并证明它们在面对最多$O(T^{1/4})$和$O(T^{1/7})$个可能无限幅度的扰动时仍能实现次线性遗憾。这种鲁棒性几乎无需代价：在没有异常值时，RCGP-UCB的遗憾界与标准GP-UCB算法保持一致。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the critical vulnerability of Bayesian Optimization to extreme outliers, where existing robust methods fail against even a single corruption of large magnitude, this paper introduces a new adversary model bounded only in corruption frequency, not magnitude. The method develops RCGP-UCB, an algorithm that combines the upper confidence bound approach with a Robust Conjugate Gaussian Process, offering stable and adaptive versions. Experimental results, supported by theoretical analysis, demonstrate that these algorithms achieve sublinear regret under up to O(T^{1/4}) and O(T^{1/7}) corruptions of potentially infinite magnitude, while maintaining performance comparable to standard GP-UCB in outlier-free scenarios.</div>
<div class="mono" style="margin-top:8px">针对贝叶斯优化对极端异常值高度脆弱的问题，现有鲁棒方法通常假设累积污染有界，无法应对单次大幅污染，本文提出了一种仅限制污染频率而非幅度的新对抗模型。方法上，通过结合上置信界与鲁棒共轭高斯过程，开发了RCGP-UCB算法，并提供了稳定和自适应版本。实验与理论分析表明，该算法在最多O(T^{1/4})和O(T^{1/7})次可能无限幅度的污染下仍能实现次线性遗憾，且在无异常值情况下，其遗憾界与标准GP-UCB算法相当。</div>
</details>
</div>
<div class="card">
<div class="title">On the Learning Dynamics of RLVR at the Edge of Competence</div>
<div class="meta-line">Authors: Yu Huang, Zixin Wen, Yuejie Chi, Yuting Wei, Aarti Singh, Yingbin Liang, Yuxin Chen</div>
<div class="meta-line">First: 2026-02-16T16:03:08+00:00 · Latest: 2026-02-16T16:03:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14872v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14872v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model&#x27;s capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论能力边界处RLVR的学习动态机制</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是推动大型推理模型近期突破的主要动力。然而，仅基于最终结果的奖励如何帮助克服长程推理障碍仍是一个谜团。为理解这一机制，我们建立了针对组合推理任务的Transformer强化学习训练动态理论。该理论揭示了RLVR的有效性如何受难度谱平滑度的调控：当数据存在难度突变时，学习过程会出现顿悟型相变，在重新取得进展前产生长期平台期；反之，平滑的难度谱会引发接力效应——较简单问题上持续的梯度信号将模型能力提升至可处理更难问题的水平，从而实现稳定持续的改进。我们的理论阐释了RLVR如何在能力边界提升性能，并表明适当设计的数据混合能产生可扩展的增益。作为技术贡献，本研究将有限群上的傅里叶分析工具发展并适配至当前场景，最终通过合成实验对预测机制进行了实证验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the learning dynamics of reinforcement learning with verifiable rewards (RLVR) to understand how outcome-based rewards can overcome long-horizon reasoning barriers in large models. The authors develop a theoretical framework for transformer training on compositional tasks, showing that effectiveness depends on the smoothness of the difficulty spectrum in data: abrupt difficulty discontinuities cause grokking-like plateaus, while a smooth spectrum enables a relay effect where easier problems progressively elevate capabilities to tackle harder ones, leading to steady improvement. The theory, supported by synthetic experiments, explains RLVR&#x27;s role at the edge of competence and suggests that well-designed data mixtures can yield scalable gains, with technical contributions adapting Fourier analysis on finite groups.</div>
<div class="mono" style="margin-top:8px">本文研究了基于可验证奖励的强化学习（RLVR）的学习动态，旨在理解仅基于最终结果的奖励如何帮助克服长程推理的障碍。作者针对组合推理任务中的Transformer训练提出了一个理论框架，表明其有效性取决于数据中难度谱的平滑性：难度突变会导致类似顿悟的停滞期，而平滑的难度谱则能产生接力效应，即较简单问题上的持续梯度信号提升模型能力，使其逐步解决更难问题，实现稳定改进。该理论通过合成实验验证，解释了RLVR在能力边缘的作用，并指出精心设计的数据混合可带来可扩展的收益，技术贡献包括将有限群上的傅里叶分析工具适配到该场景中。</div>
</details>
</div>
<div class="card">
<div class="title">Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning</div>
<div class="meta-line">Authors: Ilia Mahrooghi, Aryo Lotfi, Emmanuel Abbe</div>
<div class="meta-line">First: 2026-02-16T16:01:27+00:00 · Latest: 2026-02-16T16:01:27+00:00</div>
<div class="meta-line">Comments: 21 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question&#x27;s difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student&#x27;s performance on seen samples, the teacher continuously adapts to the student&#x27;s evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Goldilocks强化学习：通过调节任务难度规避稀疏奖励的推理困境</div>
<div class="mono" style="margin-top:8px">强化学习已成为解锁大语言模型推理能力的重要范式，但依赖稀疏奖励导致该过程样本效率极低——模型需在极少反馈下探索巨大搜索空间。传统课程学习虽尝试按复杂度排序数据以缓解此问题，但针对特定模型的最佳排序往往难以确定。为此，我们提出Goldilocks：一种新型教师驱动数据采样策略，旨在预测每个问题对学生模型的难度。教师模型依据Goldilocks原则（难度适中）为学生选择适宜问题，同时使用GRPO训练学生模型。通过持续分析学生在已见样本上的表现，教师能动态适应其能力演进。在OpenMathReasoning数据集上，Goldilocks数据采样策略在相同计算预算下显著提升了标准GRPO训练模型的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the sample inefficiency of reinforcement learning for reasoning in large language models caused by sparse rewards. It introduces Goldilocks, a teacher-driven data sampling strategy that predicts question difficulty for a student model, applying the Goldilocks principle to select questions that are neither too easy nor too hard, and trains the student using GRPO while the teacher adapts based on the student&#x27;s performance. Experimental results on the OpenMathReasoning dataset show that this approach improves model performance over standard GRPO under the same computational budget.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型推理中因稀疏奖励导致的强化学习样本效率低下问题，提出了一种名为Goldilocks的教师驱动数据采样策略。该方法通过预测学生模型对问题的难度，依据Goldilocks原则选择难度适中的问题，并利用GRPO训练学生模型，同时教师模型根据学生表现动态调整。在OpenMathReasoning数据集上的实验结果表明，在相同计算资源下，该策略相比标准GRPO提升了模型性能。</div>
</details>
</div>
<div class="card">
<div class="title">Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment</div>
<div class="meta-line">Authors: Elias Malomgré, Pieter Simoens</div>
<div class="meta-line">First: 2026-02-16T15:40:10+00:00 · Latest: 2026-02-16T15:40:10+00:00</div>
<div class="meta-line">Comments: Accepted for the AAMAS 2026 Blue Sky Ideas track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14844v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14844v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent&#x27;s policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无交互逆强化学习：面向持久对齐的数据中心化框架</div>
<div class="mono" style="margin-top:8px">人工智能对齐的重要性日益凸显，但现有方法存在将安全目标与智能体策略相纠缠的结构性缺陷。基于人类反馈的强化学习和直接偏好优化等方法会产生不透明、一次性使用的对齐产物，我们称之为“对齐浪费”。本文提出无交互逆强化学习方法，将对齐产物学习与策略优化解耦，生成可检查、可编辑且模型无关的奖励模型。此外，我们引入“对齐飞轮”——一种人机协同的生命周期，通过自动化审计与精炼迭代强化奖励模型。该架构将安全性从一次性消耗转化为持久、可验证的工程资产。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the structural flaw in current AI alignment methods, which entangle safety objectives with the agent&#x27;s policy and produce opaque, single-use artifacts termed Alignment Waste, this paper proposes Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, yielding an inspectable and model-agnostic reward model. The method is complemented by the Alignment Flywheel, a human-in-the-loop lifecycle for iterative hardening through automated audits and refinement. Experimental results demonstrate that this framework transforms safety into a durable, verifiable engineering asset, as evidenced by its acceptance for the AAMAS 2026 Blue Sky Ideas track.</div>
<div class="mono" style="margin-top:8px">本文的动机在于当前AI对齐方法存在结构性缺陷，即将安全目标与智能体策略纠缠，产生被称为“对齐浪费”的不透明、一次性产物。为此，论文提出了无交互逆强化学习，将对齐产物学习与策略优化解耦，生成可检查、可编辑且模型无关的奖励模型。方法还引入了对齐飞轮，这是一种人机协同的生命周期，通过自动化审计和精炼迭代强化奖励模型。实验结果表明，该框架将安全转化为持久、可验证的工程资产，这从其被AAMAS 2026蓝天空想轨道接收得到印证。</div>
</details>
</div>
<div class="card">
<div class="title">Virne: A Comprehensive Benchmark for RL-based Network Resource Allocation in NFV</div>
<div class="meta-line">Authors: Tianfu Wang, Liwei Deng, Xi Chen, Junyang Wang, Huiguo He, Zhengyu Hu, Wei Wu, Leilei Ding, Qilin Fan, Hui Xiong</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-07-25T12:58:32+00:00 · Latest: 2026-02-16T14:54:58+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.19234v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.19234v2">PDF</a> · <a href="https://github.com/GeminiLight/virne">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Virne：面向NFV中基于强化学习的网络资源分配的综合基准测试框架</div>
<div class="mono" style="margin-top:8px">资源分配（RA）是网络功能虚拟化（NFV）这一变革性网络范式中实现高效服务部署的关键。近年来，基于深度强化学习（RL）的方法在应对这一复杂性方面展现出巨大潜力。然而，缺乏系统化的基准测试框架和深入分析，不仅阻碍了对新兴网络的探索和更鲁棒算法的开发，还导致了评估结果的不一致。本文提出了Virne，一个针对NFV-RA问题的综合基准测试框架，重点支持基于深度RL的方法。Virne为多样化网络场景（包括云、边缘和5G环境）提供可定制的模拟，并采用模块化、可扩展的实现流程，支持超过30种不同类型的方法，同时涵盖除有效性之外的实际评估维度，如可扩展性、泛化能力和效率。此外，我们通过大量实验进行深入分析，为高效实现提供性能权衡方面的宝贵见解，并为未来研究方向提供可行指导。总体而言，凭借其多样化的模拟、丰富的实现和广泛的评估能力，Virne可作为推进NFV-RA方法和深度RL应用的综合基准。代码已公开于https://github.com/GeminiLight/virne。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the lack of a systematic benchmarking framework for deep Reinforcement Learning (RL) methods applied to the complex Network Function Virtualization Resource Allocation (NFV-RA) problem, which hinders consistent evaluation and algorithm development. The method introduces Virne, a comprehensive benchmarking framework that provides customizable simulations for diverse network scenarios like cloud and 5G, and features a modular pipeline supporting over 30 different methods. The main experimental results from extensive analysis offer insights into performance trade-offs, including scalability and generalization, providing actionable guidance for future research and establishing Virne as a tool to advance NFV-RA and deep RL applications.</div>
<div class="mono" style="margin-top:8px">本工作的动机是，针对网络功能虚拟化资源分配这一复杂问题，当前缺乏一个系统性的基准测试框架来评估深度强化学习方法，这阻碍了一致的性能比较和更鲁棒算法的开发。方法上，论文提出了Virne这一综合性基准框架，它提供针对云、边缘和5G等多样化网络场景的可定制模拟，并采用模块化、可扩展的实现流程，支持超过30种不同类型的算法。主要实验结果通过大量实验进行了深入分析，揭示了算法在可扩展性、泛化性等方面的性能权衡，为高效实现提供了宝贵见解，并为未来研究方向提供了实用指导，使Virne能作为推动NFV-RA方法和深度强化学习应用的综合基准。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning via Self-Distillation</div>
<div class="meta-line">Authors: Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</div>
<div class="meta-line">First: 2026-01-28T17:45:12+00:00 · Latest: 2026-02-16T14:49:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20802v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20802v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#x27;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自蒸馏的强化学习</div>
<div class="mono" style="margin-top:8px">大型语言模型在代码和数学等可验证领域越来越多地通过强化学习进行后训练。然而，当前基于可验证奖励的强化学习方法仅从每次尝试的标量结果奖励中学习，形成了严重的信用分配瓶颈。许多可验证环境实际能提供丰富的文本反馈（如运行时错误或评判评估），用以解释尝试失败的原因。我们将此设定形式化为具有丰富反馈的强化学习，并提出自蒸馏策略优化方法，该方法将标记化反馈转化为密集学习信号，无需外部教师或显式奖励模型。SDPO将基于反馈的当前模型视为自教师，并将其反馈感知的下一标记预测蒸馏回策略中。通过这种方式，SDPO利用了模型在上下文中回溯识别自身错误的能力。在科学推理、工具使用和LiveCodeBench v6的竞技编程任务中，SDPO相比强RLVR基线显著提升了样本效率和最终准确率。值得注意的是，在仅返回标量反馈的标准RLVR环境中，SDPO通过将成功轨迹作为失败尝试的隐式反馈，同样超越了基线方法。最后，在测试阶段对单个问题应用SDPO可加速困难二元奖励任务的探索，以比k最佳采样或多轮对话少3倍的尝试次数达到相同的发现概率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitation of current reinforcement learning with verifiable rewards (RLVR) methods, which rely only on scalar outcome rewards and suffer from credit assignment problems, despite many verifiable environments providing rich textual feedback like error messages. To address this, the authors introduce Self-Distillation Policy Optimization (SDPO), a method that leverages this textual feedback by treating the model conditioned on feedback as a self-teacher and distilling its feedback-informed next-token predictions back into the policy, thereby enabling the model to learn from its own retrospective mistake identification. Experimental results across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6 show that SDPO improves sample efficiency and final accuracy over strong RLVR baselines, even outperforming them in standard scalar-feedback environments by using successful rollouts as implicit feedback, and it accelerates discovery on difficult tasks with fewer attempts compared to methods like best-of-k sampling.</div>
<div class="mono" style="margin-top:8px">本文的动机在于当前基于可验证奖励的强化学习方法仅依赖标量结果奖励，存在严重的信用分配瓶颈，而许多可验证环境实际提供了丰富的文本反馈（如运行时错误或评估说明）。为此，作者提出了自蒸馏策略优化方法，该方法将模型在反馈条件下的输出视为自我教师，并将其基于反馈的下一令牌预测蒸馏回策略中，从而利用模型在上下文中自我识别错误的能力。在科学推理、工具使用和LiveCodeBench v6的竞争性编程等实验结果表明，SDPO在样本效率和最终准确率上均优于现有RLVR基线，甚至在仅提供标量反馈的标准环境中，通过使用成功轨迹作为隐式反馈也表现更优，并且在困难二元奖励任务上能以更少的尝试次数加速发现过程，达到与最佳k采样或多轮对话相当的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Extending Multi-Source Bayesian Optimization With Causality Principles</div>
<div class="meta-line">Authors: Luuk Jacobs, Mohammad Ali Javidian</div>
<div class="meta-line">First: 2026-02-16T14:38:16+00:00 · Latest: 2026-02-16T14:38:16+00:00</div>
<div class="meta-line">Comments: An extended abstract version of this work was accepted for the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14791v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于因果原理扩展多源贝叶斯优化</div>
<div class="mono" style="margin-top:8px">多源贝叶斯优化（MSBO）是传统贝叶斯优化（BO）框架的变体，适用于通过仿真、代理模型或真实实验等多信息源优化目标黑箱函数的场景。然而传统MSBO假设目标函数的输入变量独立同分布，在临床实验或政策制定等可获得因果信息并可实施干预的场景中效果受限。在单源领域，因果贝叶斯优化（CBO）将因果原理融入标准BO，能更好建模变量依赖关系，从而实现更精确的优化、更优的决策制定以及低成本信息源的高效利用。本文提出在多源领域对MSBO与CBO方法进行原理性整合，融合二者优势以提升高维问题的优化效率并降低计算复杂度。我们阐述了因果与多源贝叶斯优化的理论基础，论证其协同作用如何支撑我们提出的多源因果贝叶斯优化（MSCBO）算法。通过在含不同噪声水平的合成与真实数据集上对比MSCBO与其基础模型的性能，凸显了MSCBO的鲁棒性与适用性。研究表明，将MSBO与CBO的因果原理结合有助于实现降维、降低运算成本，最终提升收敛速度、性能与可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional Multi-Source Bayesian Optimization (MSBO), which assumes independent input variables and thus underperforms in scenarios with available causal information like clinical trials, this paper integrates causality principles from Causal Bayesian Optimization (CBO) into the multi-source domain. The method proposes a Multi-Source Causal Bayesian Optimization (MSCBO) algorithm that synergistically combines MSBO and CBO to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. Experimental results on synthetic and real-world datasets with varying noise levels demonstrate that MSCBO outperforms its foundational counterparts, improving convergence speed, performance, and scalability by facilitating dimensionality reduction and lowering operational costs.</div>
<div class="mono" style="margin-top:8px">本文的动机在于传统多源贝叶斯优化（MSBO）假设输入变量独立同分布，在存在因果信息（如临床试验）的场景中效果有限，因此将因果贝叶斯优化（CBO）的因果原理整合到多源领域。方法上提出了多源因果贝叶斯优化（MSCBO）算法，协同结合MSBO和CBO，以提升高维问题中的优化效率并降低计算复杂度。在具有不同噪声水平的合成和真实数据集上的实验结果表明，MSCBO优于其基础对应方法，通过促进降维和降低操作成本，提高了收敛速度、性能和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Flow Is All You Need to Sample Out-of-Distribution Chemical Spaces</div>
<div class="meta-line">Authors: Nianze Tao, Minori Abe</div>
<div class="meta-line">First: 2024-12-16T04:43:54+00:00 · Latest: 2026-02-16T14:03:22+00:00</div>
<div class="meta-line">Comments: 34 pages, 14 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.11439v5">Abs</a> · <a href="https://arxiv.org/pdf/2412.11439v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating novel molecules with higher properties than the training space, namely the out-of-distribution generation, is important for de novo drug design. However, it is not easy for distribution learning-based models, for example diffusion models, to solve this challenge as these methods are designed to fit the distribution of training data as close as possible. In this paper, we show that Bayesian flow network, especially ChemBFN model, is capable of intrinsically generating high quality out-of-distribution samples that meet several scenarios. A reinforcement learning strategy is added to the ChemBFN and a controllable ordinary differential equation solver-like generating process is employed that accelerate the sampling processes. Most importantly, we introduce a semi-autoregressive strategy during training and inference that enhances the model performance and surpass the state-of-the-art models. A theoretical analysis of out-of-distribution generation in ChemBFN with semi-autoregressive approach is included as well.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯流网络是采样分布外化学空间的唯一所需</div>
<div class="mono" style="margin-top:8px">生成性质优于训练空间的新分子，即分布外生成，对于从头药物设计至关重要。然而，基于分布学习的模型（如扩散模型）难以应对此挑战，因为这些方法旨在尽可能拟合训练数据的分布。本文证明，贝叶斯流网络（特别是ChemBFN模型）能够本质性地生成满足多种场景的高质量分布外样本。我们在ChemBFN中引入了强化学习策略，并采用类似可控常微分方程求解器的生成过程以加速采样。最重要的是，我们在训练和推理阶段采用半自回归策略，显著提升模型性能并超越现有最优模型。文中还包含对ChemBFN半自回归方法分布外生成的理论分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating novel molecules with superior properties beyond the training distribution, a key task in de novo drug design where standard diffusion models often struggle due to their focus on fitting existing data. The method introduces ChemBFN, a Bayesian flow network enhanced with a reinforcement learning strategy and a controllable ODE solver-like process to accelerate sampling, alongside a semi-autoregressive training and inference approach to boost performance. Experimental results demonstrate that ChemBFN effectively produces high-quality out-of-distribution samples across multiple scenarios, outperforming state-of-the-art models, with theoretical analysis supporting its efficacy.</div>
<div class="mono" style="margin-top:8px">本文针对在训练分布之外生成具有更优性质的新分子这一挑战展开研究，这在从头药物设计中至关重要，而标准扩散模型因专注于拟合现有数据往往难以解决。方法上提出了ChemBFN，一种贝叶斯流网络，结合了强化学习策略和类似可控ODE求解器的生成过程以加速采样，并采用半自回归训练和推理策略来提升性能。实验结果表明，ChemBFN能在多种场景下有效生成高质量的外分布样本，超越了现有最先进模型，同时提供了理论分析以支持其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay</div>
<div class="meta-line">Authors: Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-05T17:55:43+00:00 · Latest: 2026-02-16T13:48:46+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05316v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.05316v4">PDF</a> · <a href="https://github.com/ASTRAL-Group/data-efficient-llm-rl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism inspired by experience replay in traditional RL. This technique reuses recent rollouts, lowering per-step computation while maintaining stable updates. Experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 23% to 62% while reaching the same level of performance as the original GRPO algorithm. Our code is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过难度导向的在线数据选择与轨迹回放提升大语言模型强化学习微调的数据效率</div>
<div class="mono" style="margin-top:8px">强化学习已成为微调大语言模型的有效方法，尤其在提升其推理能力方面。然而，强化学习微调仍高度消耗资源，现有研究大多忽视了数据效率问题。本文提出两种提升大语言模型强化学习微调数据效率的技术：难度导向的在线数据选择与轨迹回放。我们引入自适应难度概念来指导在线数据选择，优先选择中等难度、更可能产生有效学习信号的问题。为高效估计自适应难度，我们开发了基于注意力的框架，仅需对少量参考问题集进行轨迹推演，其余问题的难度则通过与该集合的相似度进行估计。为进一步降低推演成本，我们借鉴传统强化学习中的经验回放机制，提出轨迹回放技术，通过复用近期轨迹在保持更新稳定性的同时降低单步计算量。在6组大语言模型与数据集的实验表明，本方法将强化学习微调时间减少23%至62%，同时达到与原始GRPO算法相当的性能。代码发布于https://github.com/ASTRAL-Group/data-efficient-llm-rl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high computational cost of reinforcement learning (RL) fine-tuning for large language models (LLMs) by proposing two techniques to improve data efficiency. The method introduces difficulty-targeted online data selection, which uses an attention-based framework to estimate and prioritize questions of moderate adaptive difficulty for more informative training, and a rollout replay mechanism that reuses recent rollouts to reduce per-step computation. Experimental results across six LLM-dataset combinations demonstrate that this approach reduces fine-tuning time by 23% to 62% while achieving performance comparable to the original GRPO algorithm.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习微调的高计算成本问题，提出了两种提升数据效率的技术。方法包括难度导向的在线数据选择，通过一个基于注意力的框架来估计并优先选择中等难度的问题以提供更有信息量的训练信号，以及受传统强化学习经验回放启发的轨迹回放机制，通过重用近期轨迹来降低单步计算量。在六个大语言模型与数据集组合上的实验结果表明，该方法将微调时间减少了23%至62%，同时达到了与原始GRPO算法相当的性能水平。</div>
</details>
</div>
<div class="card">
<div class="title">A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting</div>
<div class="meta-line">Authors: Ons Saadallah, Mátyás andó, Tamás Gábor Orosz</div>
<div class="meta-line">First: 2026-02-01T21:26:57+00:00 · Latest: 2026-02-16T13:31:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01445v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01445v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向时序预测超参数优化的元知识增强型大语言模型框架</div>
<div class="mono" style="margin-top:8px">超参数优化（HPO）对深度学习模型性能至关重要，但其计算成本高昂且可解释性差，在时序预测中尤为突出。贝叶斯优化（BO）作为主流方法，通常独立处理调优任务且决策依据不透明。大语言模型（LLM）的最新进展为将结构化先验知识与推理融入优化流程提供了新途径。本文提出LLM-AutoOpt——一种融合BO与基于LLM的上下文推理的混合HPO框架。该框架将数据集元特征、模型描述、历史优化结果及目标指标编码为LLM提示中的结构化元知识，并利用BO初始化搜索以缓解冷启动问题。该设计实现了上下文感知的稳定超参数优化，同时揭示了决策背后的推理逻辑。在多变量时序预测基准测试中，LLM-AutoOpt相比无元知识的BO和LLM基线方法，展现出更优的预测性能与更强的优化过程可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational expense and limited interpretability of hyperparameter optimization (HPO) in time-series forecasting, where standard Bayesian Optimization (BO) lacks contextual reasoning. To address this, the authors propose LLM-AutoOpt, a hybrid framework that integrates BO with large language models (LLMs) by encoding dataset meta-features, model descriptions, and historical outcomes as structured meta-knowledge in prompts, using BO to initialize searches and reduce cold-start issues. Experimental results on a multivariate time series benchmark show that LLM-AutoOpt outperforms BO and LLM baselines in predictive performance while offering more interpretable optimization behavior.</div>
<div class="mono" style="margin-top:8px">本文的动机在于时间序列预测中超参数优化（HPO）计算成本高且可解释性有限，而标准的贝叶斯优化（BO）缺乏上下文推理能力。为此，作者提出了LLM-AutoOpt，一个混合框架，通过将数据集元特征、模型描述和历史结果编码为提示中的结构化元知识，将BO与大型语言模型（LLM）结合，利用BO初始化搜索以减少冷启动问题。在多变量时间序列基准测试中，实验结果表明，LLM-AutoOpt在预测性能上优于BO和缺乏元知识的LLM基线，同时提供了更可解释的优化行为。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis</div>
<div class="meta-line">Authors: Matthieu Mastio, Paul Saves, Benoit Gaudou, Nicolas Verstaevel</div>
<div class="meta-line">Venue: AAMAS 2026, Paphos, IFAAMAS, 10 pages</div>
<div class="meta-line">First: 2025-12-19T13:24:43+00:00 · Latest: 2026-02-16T13:27:34+00:00</div>
<div class="meta-line">Comments: AAMAS CC-BY 4.0 licence. Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis. Full paper. In Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 - 29, 2026, IFAAMAS, 10 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17979v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17979v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers&#x27; strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间双重拍卖市场中的自适应智能体：工业共生涌现的建模</div>
<div class="mono" style="margin-top:8px">工业共生通过促使企业重新利用残余资源来促进循环性，但其涌现受到社会空间摩擦的制约，这些摩擦影响着成本、匹配机会和市场效率。现有模型常忽视空间结构、市场设计与自适应企业行为之间的相互作用，限制了我们对共生在何处及如何产生的理解。我们开发了一个基于智能体的模型，其中异质企业通过空间嵌入的双重拍卖市场交易副产品，价格和数量从局部互动中内生涌现。借助强化学习，企业调整其竞价策略以最大化利润，同时考虑运输成本、处置惩罚和资源稀缺性。模拟实验揭示了分散交易在何种经济和空间条件下趋于稳定高效的结果。反事实遗憾分析表明卖方策略接近纳什均衡，而敏感性分析则突显了空间结构和市场参数如何共同调控循环性。该模型为探索旨在协调企业激励与可持续性目标的政策干预提供了基础，并更广泛地展示了在空间受限市场中自适应智能体如何实现分散协调。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand how industrial symbiosis emerges despite socio-spatial frictions that hinder resource exchange, this paper develops an agent-based model where heterogeneous firms trade byproducts in a spatially embedded double-auction market. The method employs reinforcement learning to enable firms to adaptively optimize bidding strategies, accounting for transport costs, disposal penalties, and resource scarcity, with prices and quantities emerging endogenously from local interactions. Experimental results show that decentralized exchanges can converge to stable, efficient outcomes under certain economic and spatial conditions, with sellers&#x27; strategies approaching a near Nash equilibrium, and sensitivity analyses reveal how spatial structures and market parameters jointly govern circularity, providing a basis for policy interventions to align firm incentives with sustainability goals.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究工业共生如何在阻碍资源交换的社会空间摩擦下形成，为此开发了一个基于智能体的模型，其中异质企业在空间嵌入的双重拍卖市场中交易副产品。方法上采用强化学习使企业能自适应地优化投标策略，考虑运输成本、处置惩罚和资源稀缺性，价格和数量从局部互动中内生涌现。实验结果表明，在特定经济和空间条件下，分散交换可收敛至稳定高效的结果，卖方策略接近纳什均衡，敏感性分析揭示了空间结构和市场参数如何共同影响循环性，为制定政策干预以协调企业激励与可持续目标提供了基础。</div>
</details>
</div>
<div class="card">
<div class="title">ManeuverNet: A Soft Actor-Critic Framework for Precise Maneuvering of Double-Ackermann-Steering Robots with Optimized Reward Functions</div>
<div class="meta-line">Authors: Kohio Deflesselle, Mélodie Daniel, Aly Magassouba, Miguel Aranda, Olivier Ly</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-16T13:19:04+00:00 · Latest: 2026-02-16T13:19:04+00:00</div>
<div class="meta-line">Comments: 8 pages, 5, figures, Accepted for 2026 IEEE International Conference on Robotics &amp; Automation (ICRA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14726v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14726v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous control of double-Ackermann-steering robots is essential in agricultural applications, where robots must execute precise and complex maneuvers within a limited space. Classical methods, such as the Timed Elastic Band (TEB) planner, can address this problem, but they rely on parameter tuning, making them highly sensitive to changes in robot configuration or environment and impractical to deploy without constant recalibration. At the same time, end-to-end deep reinforcement learning (DRL) methods often fail due to unsuitable reward functions for non-holonomic constraints, resulting in sub-optimal policies and poor generalization. To address these challenges, this paper presents ManeuverNet, a DRL framework tailored for double-Ackermann systems, combining Soft Actor-Critic with CrossQ. Furthermore, ManeuverNet introduces four specifically designed reward functions to support maneuver learning. Unlike prior work, ManeuverNet does not depend on expert data or handcrafted guidance. We extensively evaluate ManeuverNet against both state-of-the-art DRL baselines and the TEB planner. Experimental results demonstrate that our framework substantially improves maneuverability and success rates, achieving more than a 40% gain over DRL baselines. Moreover, ManeuverNet effectively mitigates the strong parameter sensitivity observed in the TEB planner. In real-world trials, ManeuverNet achieved up to a 90% increase in maneuvering trajectory efficiency, highlighting its robustness and practical applicability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ManeuverNet：基于优化奖励函数的双阿克曼转向机器人精确操控软演员-评论家框架</div>
<div class="mono" style="margin-top:8px">双阿克曼转向机器人的自主控制在农业应用中至关重要，机器人需在有限空间内执行精确复杂的机动动作。传统方法（如定时弹性带规划器）虽能解决此问题，但依赖参数调优，对机器人配置或环境变化极为敏感，且需持续重新标定才能部署。同时，端到端深度强化学习方法常因奖励函数不适用于非完整约束而失效，导致策略次优且泛化能力差。为应对这些挑战，本文提出ManeuverNet——专为双阿克曼系统设计的深度强化学习框架，融合软演员-评论家与CrossQ算法，并引入四种专门设计的奖励函数以支持机动学习。与先前研究不同，ManeuverNet不依赖专家数据或人工引导。我们通过大量实验将ManeuverNet与前沿深度强化学习基线及定时弹性带规划器进行对比，结果表明该框架显著提升了机动性与成功率，较深度强化学习基线提高超40%，且有效缓解了定时弹性带规划器的强参数敏感性。实际测试中，ManeuverNet的机动轨迹效率提升最高达90%，凸显了其鲁棒性与实际应用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces ManeuverNet, a deep reinforcement learning framework designed to address the challenge of precise autonomous maneuvering for double-Ackermann-steering robots in constrained agricultural spaces, where classical methods like the Timed Elastic Band planner are overly sensitive to parameter tuning and existing DRL approaches suffer from poor generalization due to unsuitable reward functions. The method employs a Soft Actor-Critic algorithm combined with CrossQ and incorporates four specifically engineered reward functions to effectively learn control policies without relying on expert demonstrations. Experimental results show that ManeuverNet significantly outperforms both DRL baselines and the classical planner, achieving over a 40% improvement in success rates, greatly reducing parameter sensitivity, and demonstrating up to a 90% increase in trajectory efficiency in real-world tests, confirming its robustness and practical utility.</div>
<div class="mono" style="margin-top:8px">本文提出了ManeuverNet，一个专为双阿克曼转向机器人设计的深度强化学习框架，旨在解决其在农业受限空间中精确自主机动的挑战，传统方法如定时弹性带规划器对参数调整过于敏感，而现有深度强化学习方法则因奖励函数不适用于非完整约束导致泛化能力差。该方法采用结合CrossQ的柔性演员-评论家算法，并引入了四种专门设计的奖励函数，从而无需专家数据即可有效学习控制策略。实验结果表明，ManeuverNet显著优于现有的深度强化学习基准和传统规划器，成功率提升超过40%，极大降低了参数敏感性，并在真实世界测试中实现了高达90%的轨迹效率提升，证明了其鲁棒性和实际应用价值。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs</div>
<div class="meta-line">Authors: Lunjun Zhang, Ryan Chen, Bradly C. Stadie</div>
<div class="meta-line">First: 2026-02-16T12:34:27+00:00 · Latest: 2026-02-16T12:34:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14697v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14697v1">PDF</a> · <a href="https://github.com/LunjunZhang/E-SPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化系统提示学习可促进大语言模型的强化学习</div>
<div class="mono" style="margin-top:8px">构建能够从经验中自主进化的智能体系统是人工智能的长期目标。当前大语言模型主要通过两种机制实现自我改进：基于上下文更新的自我反思机制，以及基于权重更新的强化学习机制。本研究提出进化系统提示学习方法，通过联合优化模型上下文与模型权重实现协同改进。在每轮强化学习迭代中，该方法并行选取多个系统提示执行推演，针对各提示条件更新模型权重，并通过大语言模型驱动的突变与交叉操作对提示种群进行进化更新。每个系统提示均设有基于批量迭代内相对表现更新的TrueSkill评分，用于进化选择。该方法促使陈述性知识（编码于提示）与程序性知识（编码于权重）形成自然分工，在推理与智能体任务中均取得性能提升。例如在易到难（AIME→BeyondAIME）泛化场景中，该方法将强化学习成功率从38.8%提升至45.1%，同时优于反思式提示进化方法（40.0%）。总体而言，研究结果表明强化学习与系统提示进化的耦合能持续提升样本效率与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the goal of developing autonomous AI systems that can self-improve, this paper introduces Evolutionary System Prompt Learning (E-SPL), a method that jointly optimizes model contexts and weights by coupling reinforcement learning (RL) with evolutionary updates to a population of system prompts. The method operates by selecting multiple prompts per RL iteration, running parallel rollouts, applying RL updates to model weights conditioned on each prompt, and evolving the prompt population via LLM-driven mutation and crossover, guided by TrueSkill ratings based on relative performance. Experimental results demonstrate that E-SPL improves performance on reasoning and agentic tasks, notably increasing RL success rates from 38.8% to 45.1% in an easy-to-hard generalization setting and outperforming reflective prompt evolution, thereby enhancing sample efficiency and generalization.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发能够自主从经验中自我改进的AI智能体系统，提出了进化系统提示学习（E-SPL）方法，该方法通过将强化学习与系统提示的进化更新相结合，共同优化模型上下文和模型权重。其方法是在每个强化学习迭代中选择多个系统提示并行运行交互轨迹，对每个提示条件下的模型权重进行强化学习更新，并通过基于大语言模型的突变和交叉来进化提示种群，使用基于相对性能的TrueSkill评分进行选择。主要实验结果表明，E-SPL在推理和智能体任务上提升了性能，例如在从易到难的泛化设置中，将强化学习成功率从38.8%提高至45.1%，并优于反思性提示进化方法，从而提高了样本效率和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses</div>
<div class="meta-line">Authors: Attila Lischka, Balázs Kulcsár</div>
<div class="meta-line">First: 2026-02-16T12:04:14+00:00 · Latest: 2026-02-16T12:04:14+00:00</div>
<div class="meta-line">Comments: 29 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14676v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14676v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GREAT-EER：面向应急疏散响应的图边注意力网络</div>
<div class="mono" style="margin-top:8px">城市区域应急疏散需求可能源于人为因素（如恐怖袭击或工业事故）或自然灾害，后者因气候变化而日益频发。因此，开发高效快速的疏散规划方法至关重要。本研究提出并定义了公交疏散定向问题（BEOP），这是一个NP难组合优化问题，目标是在预设的短时间内通过公交车从受灾区域疏散尽可能多的人员。采用公交疏散旨在缓解纯依赖私家车疏散方案引发的交通拥堵与混乱。为解决BEOP，我们提出一种基于深度强化学习与图学习的方法，该方法经训练后可实现快速推理，能在秒级时间内生成疏散路径。我们通过混合整数线性规划（MILP）模型对疏散方案的优化间隙进行理论界定。为验证方法有效性，我们基于真实道路网络与行程时间构建了旧金山疏散场景实验。结果表明，该方法能获得接近最优的解决方案质量，并能进一步探究在预设疏散时间内，为达成特定公交疏散配额所需的最小车辆数，同时保持合理的计算耗时。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing frequency of natural disasters and man-made emergencies requiring urban evacuations, this paper introduces the Bus Evacuation Orienteering Problem (BEOP), an NP-hard optimization challenge aimed at maximizing bus-based evacuations within a limited time to reduce congestion compared to car-only scenarios. The method employs a deep reinforcement learning approach with graph learning, specifically a Graph Edge Attention Network, to rapidly generate evacuation routes with fast inference speeds. Experimental validation using real-world San Francisco road networks demonstrates that the approach achieves near-optimal solution quality, efficiently determines the required number of evacuation vehicles to meet quotas, and maintains practical runtimes, with performance bounds provided via an MILP formulation.</div>
<div class="mono" style="margin-top:8px">本文的动机是应对日益频繁的自然灾害和人为紧急事件对城市疏散的需求，提出了公交疏散定向问题（BEOP），这是一个NP难组合优化问题，旨在有限时间内通过公交疏散最大化人员数量，以减少纯汽车疏散的拥堵和混乱。方法采用基于图学习的深度强化学习，具体利用图边注意力网络，以实现快速推理并生成疏散路线。基于旧金山真实道路网络的实验验证表明，该方法能获得接近最优的解决方案质量，有效确定满足预设疏散配额所需的车辆数量，并保持合理的运行时间，同时通过混合整数线性规划模型提供了性能界限。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR</div>
<div class="meta-line">Authors: Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang</div>
<div class="meta-line">First: 2025-09-02T17:22:46+00:00 · Latest: 2026-02-16T11:42:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02522v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.02522v2">PDF</a> · <a href="https://github.com/ritzz-ai/PACS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, inherent to RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while providing more stable and efficient training. Extensive experiments demonstrate that PACS significantly outperforms strong open-source models and RLVR baselines, yielding substantial average gains of $\textbf{+8.26\%}$ (4B) and $\textbf{+9.57\%}$ (8B) over base models offering a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于监督学习框架的RLVR隐式行动者-评论者耦合方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）的最新进展使大语言模型（LLMs）能够应对数学和编程等复杂推理任务。尽管前景广阔，但RLVR范式仍面临重大挑战：现有方法常受限于稀疏奖励信号和不稳定的策略梯度更新，这是基于RL方法的固有难题。为此，我们提出$\textbf{PACS}$——一种通过$\textbf{S}$监督学习框架实现隐式$\textbf{P}$行动者-$\textbf{C}$评论者$\textbf{A}$耦合的新型RLVR框架。通过将结果奖励视为可预测标签，我们将RLVR问题重构为对策略模型参数化评分函数的监督学习任务，并采用交叉熵损失进行优化。梯度分析表明，该监督式框架在提供更稳定高效训练的同时，本质恢复了经典策略梯度更新。大量实验证明，PACS显著优于主流开源模型与RLVR基线，在4B和8B基础模型上分别实现$\textbf{+8.26\%}$和$\textbf{+9.57\%}$的平均性能提升，为LLMs的可验证奖励后训练提供了新路径。代码与数据已在https://github.com/ritzz-ai/PACS开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of sparse rewards and unstable training in Reinforcement Learning with Verifiable Rewards (RLVR) for large language models, this paper introduces PACS, a framework that implicitly couples the actor and critic by reformulating RLVR as a supervised learning task. The method treats the reward outcome as a predictable label and optimizes a policy-parameterized score function using cross-entropy loss, which gradient analysis shows recovers the policy gradient while ensuring more stable and efficient training. Experimental results demonstrate that PACS substantially outperforms existing open-source models and RLVR baselines, achieving average accuracy gains of +8.26% and +9.57% for 4B and 8B parameter models, respectively, offering a promising approach for post-training LLMs with verifiable rewards.</div>
<div class="mono" style="margin-top:8px">针对可验证奖励强化学习（RLVR）中大型语言模型面临的奖励稀疏和训练不稳定问题，本文提出了PACS框架，通过将RLVR重构为监督学习任务来实现演员与评论家的隐式耦合。该方法将奖励结果视为可预测标签，并使用交叉熵损失优化由策略参数化的评分函数，梯度分析表明该监督公式能恢复经典策略梯度更新，同时提供更稳定高效的训练。实验结果表明，PACS显著优于现有开源模型和RLVR基线，在4B和8B参数模型上分别实现了平均+8.26%和+9.57%的准确率提升，为基于可验证奖励的大语言模型后训练提供了有效途径。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application</div>
<div class="meta-line">Authors: Thomas Hickling, Maxwell Hogan, Abdulla Tammam, Nabil Aouf</div>
<div class="meta-line">First: 2025-02-27T17:53:16+00:00 · Latest: 2026-02-16T11:36:33+00:00</div>
<div class="meta-line">Comments: 22 Pages, 24 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.20326v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.20326v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents the first end-to-end framework that combines guidance, navigation, and centralised task allocation for multiple UAVs performing autonomous search-and-rescue (SAR) in GNSS-denied indoor environments. A Twin Delayed Deep Deterministic Policy Gradient controller is trained with an Artificial Potential Field (APF) reward that blends attractive and repulsive potentials with continuous control, accelerating convergence and yielding smoother, safer trajectories than distance-only baselines. Collaborative mission assignment is solved by a deep Graph Attention Network that, at each decision step, reasons over the drone-task graph to produce near-optimal allocations with negligible on-board compute. To arrest the notorious Z-drift of indoor LiDAR-SLAM, we fuse depth-camera altimetry with IMU vertical velocity in a lightweight complementary filter, giving centimetre-level altitude stability without external beacons. The resulting system was deployed on two 1m-class quad-rotors and flight-tested in a cluttered, multi-level disaster mock-up designed for the NATO-Sapience Autonomous Cooperative Drone Competition. Compared with prior DRL guidance that remains largely in simulation, our framework demonstrates an ability to navigate complex indoor environments, securing first place in the 2024 event. These results demonstrate that APF-shaped DRL and GAT-driven cooperation can translate to reliable real-world SAR operations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的协作无人机自主决策：搜救现实应用</div>
<div class="mono" style="margin-top:8px">本文首次提出端到端框架，将引导、导航与集中式任务分配相结合，用于多架无人机在GNSS拒止室内环境执行自主搜救任务。采用融合引力与斥力势场的连续控制人工势场奖励训练双延迟深度确定性策略梯度控制器，相比仅基于距离的基线方法，该设计加速收敛并生成更平滑、更安全的轨迹。协作任务分配通过深度图注意力网络实现，该网络在每个决策步骤对无人机-任务图进行推理，以可忽略的机载计算量生成近似最优分配方案。为抑制室内LiDAR-SLAM固有的Z轴漂移，通过轻量级互补滤波器融合深度相机测高与IMU垂直速度，在不依赖外部信标情况下实现厘米级高度稳定性。该系统部署于两架1米级四旋翼无人机，在专为北约Sapience自主协作无人机竞赛设计的杂乱多层灾害模拟场景中完成飞行测试。与多数仍停留在仿真阶段的现有DRL引导方法相比，本框架展现出在复杂室内环境导航的能力，荣获2024年赛事冠军。结果表明：APF赋形的DRL与GAT驱动的协作机制可转化为可靠的实际搜救操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for autonomous cooperative UAVs in GPS-denied indoor search-and-rescue missions, this paper introduces an end-to-end framework integrating guidance, navigation, and centralized task allocation. The method employs a Twin Delayed Deep Deterministic Policy Gradient controller trained with an Artificial Potential Field reward for smooth trajectory planning and a deep Graph Attention Network for efficient drone-task assignment, alongside a complementary filter fusing depth-camera and IMU data to mitigate LiDAR-SLAM altitude drift. Experimental results from real-world deployment on two quad-rotors in a cluttered disaster mock-up show the system achieves centimeter-level altitude stability and secured first place in a 2024 competition, demonstrating reliable performance in complex indoor environments.</div>
<div class="mono" style="margin-top:8px">本文针对GPS拒止室内环境中协同无人机自主搜救的需求，提出了一个集引导、导航与集中任务分配于一体的端到端框架。方法采用基于人工势场奖励训练的双延迟深度确定性策略梯度控制器实现平滑轨迹规划，利用深度图注意力网络进行高效的无人机-任务分配，并结合深度相机与IMU数据的互补滤波器以抑制LiDAR-SLAM的高度漂移。在杂乱多层灾难模拟场景中，两个四旋翼无人机的实际飞行测试结果表明，该系统实现了厘米级高度稳定性，并在2024年比赛中获得第一名，验证了其在复杂室内环境中的可靠操作能力。</div>
</details>
</div>
<div class="card">
<div class="title">Heterogeneous RBCs via Deep Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Federico Gabriele, Aldo Glielmo, Marco Taboga</div>
<div class="meta-line">Venue: Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026, https://cyprusconferences.org/aamas2026/)</div>
<div class="meta-line">First: 2025-10-14T08:26:18+00:00 · Latest: 2026-02-16T10:49:57+00:00</div>
<div class="meta-line">Comments: 14 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.12272v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.12272v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current macroeconomic models with agent heterogeneity can be broadly divided into two main groups. Heterogeneous-agent general equilibrium (GE) models, such as those based on Heterogeneous Agent New Keynesian (HANK) or Krusell-Smith (KS) approaches, rely on GE and &#x27;rational expectations&#x27;, somewhat unrealistic assumptions that make the models very computationally cumbersome, which in turn limits the amount of heterogeneity that can be modelled. In contrast, agent-based models (ABMs) can flexibly encompass a large number of arbitrarily heterogeneous agents, but typically require the specification of explicit behavioural rules, which can lead to a lengthy trial-and-error model-development process. To address these limitations, we introduce MARL-BC, a framework that integrates deep multi-agent reinforcement learning (MARL) with real business cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover textbook RBC results when using a single agent; (2) recover the results of the mean-field KS model using a large number of identical agents; and (3) effectively simulate rich heterogeneity among agents, a hard task for traditional GE approaches. Our framework can be thought of as an ABM if used with a variety of heterogeneous interacting agents, and can reproduce GE results in limit cases. As such, it is a step towards a synthesis of these often opposed modelling paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度多智能体强化学习的异质性实际经济周期模型</div>
<div class="mono" style="margin-top:8px">当前具有智能体异质性的宏观经济模型主要可分为两大类。异质性智能体一般均衡模型（如基于异质性智能体新凯恩斯模型或克鲁塞尔-史密斯方法）依赖一般均衡与&#x27;理性预期&#x27;假设，这些假设存在一定非现实性，导致模型计算极为繁重，进而限制了可建模的异质性程度。相比之下，基于智能体的模型虽能灵活容纳大量任意异质智能体，但通常需设定明确的行为规则，这可能导致冗长的试错式模型开发过程。为突破这些局限，我们提出了MARL-BC框架，将深度多智能体强化学习与实际经济周期模型相结合。我们证明该框架能够：（1）在单智能体场景下复现经典实际经济周期结果；（2）通过大量同质智能体重现平均场克鲁塞尔-史密斯模型结果；（3）有效模拟智能体间丰富的异质性——这对传统一般均衡方法而言是艰巨任务。当使用多种异质交互智能体时，本框架可视为基于智能体的模型，并能在极限情况下复现一般均衡结果。因此，这是向两种常对立建模范式融合迈出的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing macroeconomic models—where heterogeneous-agent general equilibrium models are computationally cumbersome and rely on unrealistic assumptions, while agent-based models require explicit behavioral rules and trial-and-error development—this paper introduces MARL-BC, a framework integrating deep multi-agent reinforcement learning with real business cycle models. The method employs MARL to enable agents to learn adaptive behaviors through interaction, avoiding the need for pre-specified rules or rational expectations. Experimental results demonstrate that MARL-BC successfully recovers textbook RBC outcomes with a single agent, replicates mean-field Krusell-Smith model results with many identical agents, and effectively simulates rich agent heterogeneity, which is challenging for traditional general equilibrium approaches, thereby bridging agent-based and general equilibrium modeling paradigms.</div>
<div class="mono" style="margin-top:8px">针对现有宏观经济模型的局限性——异质性主体一般均衡模型计算繁琐且依赖不现实假设，而基于主体的模型需要明确行为规则和试错开发——本文提出了MARL-BC框架，将深度多智能体强化学习与真实经济周期模型相结合。该方法利用多智能体强化学习使主体通过交互学习自适应行为，避免了预先指定规则或理性预期。实验结果表明，MARL-BC在单个主体下成功复现了经典真实经济周期结果，在多个相同主体下复制了平均场克鲁塞尔-史密斯模型的结果，并有效模拟了丰富的异质性，这是传统一般均衡方法难以实现的，从而弥合了基于主体模型与一般均衡模型之间的鸿沟。</div>
</details>
</div>
<div class="card">
<div class="title">A representational framework for learning and encoding structurally enriched trajectories in complex agent environments</div>
<div class="meta-line">Authors: Corina Catarau-Cotutiu, Esther Mondragon, Eduardo Alonso</div>
<div class="meta-line">First: 2025-03-17T14:04:27+00:00 · Latest: 2026-02-16T10:37:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.13194v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.13194v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability of artificial intelligence agents to make optimal decisions and generalise them to different domains and tasks is compromised in complex scenarios. One way to address this issue has focused on learning efficient representations of the world and on how the actions of agents affect them in state-action transitions. Whereas such representations are procedurally efficient, they lack structural richness. To address this problem, we propose to enhance the agent&#x27;s ontology and extend the traditional conceptualisation of trajectories to provide a more nuanced view of task execution. Structurally Enriched Trajectories (SETs) extend the encoding of sequences of states and their transitions by incorporating hierarchical relations between objects, interactions, and affordances. SETs are built as multi-level graphs, providing a detailed representation of the agent dynamics and a transferable functional abstraction of the task. SETs are integrated into an architecture, Structurally Enriched Trajectory Learning and Encoding (SETLE), that employs a heterogeneous graph-based memory structure of multi-level relational dependencies essential for generalisation. We demonstrate that SETLE can support downstream tasks, enabling agents to recognise task relevant structural patterns across CREATE and MiniGrid environments. Finally, we integrate SETLE with reinforcement learning and show measurable improvements in downstream performance, including breakthrough success rates in complex, sparse-reward tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复杂智能体环境中结构增强轨迹学习与编码的表征框架</div>
<div class="mono" style="margin-top:8px">人工智能智能体在复杂场景中做出最优决策并将其泛化至不同领域和任务的能力受到限制。现有方法侧重于学习世界的有效表征及智能体行为在状态-动作转换中的影响，虽具过程效率但缺乏结构丰富性。为此，我们提出通过增强智能体本体并扩展传统轨迹概念化方式，为任务执行提供更精细的视角。结构增强轨迹通过融入对象层级关系、交互与功能可供性，扩展了状态序列及其转换的编码方式。该框架构建为多层图结构，既能详细表征智能体动态，又可形成任务的可迁移功能抽象。我们将其整合至结构增强轨迹学习与编码架构中，该架构采用基于异质图的多级关系依赖记忆结构，这对泛化能力至关重要。实验表明，该架构能支持下游任务，使智能体在CREATE和MiniGrid环境中识别任务相关的结构模式。最后，通过将架构与强化学习结合，我们在下游任务性能上实现了可量化的提升，包括在复杂稀疏奖励任务中取得突破性成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for artificial intelligence agents to make optimal decisions and generalize in complex environments, this paper addresses the limitation of procedurally efficient but structurally poor representations by proposing Structurally Enriched Trajectories (SETs). The method extends traditional state-action trajectories into multi-level graphs that encode hierarchical relations among objects, interactions, and affordances, integrated into an architecture called SETLE which uses a heterogeneous graph-based memory for learning relational dependencies. Experimental results demonstrate that SETLE enables agents to recognize task-relevant structural patterns across CREATE and MiniGrid environments, and when combined with reinforcement learning, it leads to measurable performance improvements, including higher success rates in complex, sparse-reward tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决人工智能智能体在复杂环境中做出最优决策并实现泛化能力时，因现有表示方法虽过程高效但结构贫乏而受限的问题。方法上，提出了结构增强轨迹，将传统的状态-动作轨迹扩展为多层级图，编码对象、交互和功能可供性之间的层次关系，并整合进一个名为SETLE的架构中，该架构利用基于异质图的记忆结构来学习关系依赖。主要实验结果表明，SETLE能使智能体在CREATE和MiniGrid环境中识别任务相关的结构模式，且与强化学习结合后，在下游任务中带来了可衡量的性能提升，包括在复杂、稀疏奖励任务中实现了突破性的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">A Bayesian Approach to Low-Discrepancy Subset Selection</div>
<div class="meta-line">Authors: Nathan Kirk</div>
<div class="meta-line">First: 2026-02-16T10:11:07+00:00 · Latest: 2026-02-16T10:11:07+00:00</div>
<div class="meta-line">Comments: 13 pages, 3 figures, mODa14</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14607v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-discrepancy designs play a central role in quasi-Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP-hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP-hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种低差异子集选择的贝叶斯方法</div>
<div class="mono" style="margin-top:8px">低差异设计在拟蒙特卡洛方法中占据核心地位，并日益影响机器学习、机器人学和计算机图形学等多个领域。近年来，一种称为子集选择的低差异构造方法备受关注。该方法旨在从大规模总体中，基于差异目标函数最优地选取一个低差异小子集。已知该问题的某些变体属于NP难问题。本文首次证明，针对核差异的子集选择问题同样为NP难。受此难解性启发，我们提出一种基于深度嵌入核的贝叶斯优化方法，用于解决子集选择问题。实验表明该BO算法能有效最小化差异度量，且该框架可广泛适用于各类设计准则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the NP-hardness of selecting low-discrepancy subsets from large populations, which is crucial for applications in quasi-Monte Carlo and machine learning, this paper proposes a Bayesian Optimization (BO) method using deep embedding kernels to tackle this computationally intractable problem. The method efficiently searches for subsets that minimize kernel-based discrepancy measures. Experimental results demonstrate that the BO framework effectively reduces discrepancy and is broadly applicable to various design criteria.</div>
<div class="mono" style="margin-top:8px">本文针对从大规模群体中选择低差异子集这一NP难问题（在准蒙特卡洛和机器学习中至关重要），提出了一种基于深度嵌入核的贝叶斯优化方法，以应对计算上的难解性。该方法能高效搜索最小化核差异度量的子集。实验结果表明，该贝叶斯优化框架能有效降低差异，并广泛适用于多种设计准则。</div>
</details>
</div>
<div class="card">
<div class="title">From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism</div>
<div class="meta-line">Authors: Sarthak Wanjari</div>
<div class="meta-line">First: 2026-02-09T13:48:25+00:00 · Latest: 2026-02-16T09:49:12+00:00</div>
<div class="meta-line">Comments: 10 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08655v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08655v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds. Current solutions necessitate a trade-off between computational efficiency and performance. Methods like CQL offer rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair, our method injects OOD conservatism via reward shaping with a O(1) training overhead to the training loop. Evaluated on the D4RL MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed standard-deviation by 4 times. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, it achieves 86.4% terminal agreement with clinicians compared to IQL&#x27;s 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从机器人学到脓毒症治疗：基于几何悲观主义的离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）旨在从静态数据集中恢复最优策略，但其仍易高估分布外（OOD）动作，尤其在断裂稀疏的数据流形中。现有方法需在计算效率与性能间权衡：CQL等方法虽提供严格保守性但需巨大算力，而基于期望分位数的高效方法（如IQL）常无法修正病态数据集上的OOD误差，退化为行为克隆。本研究提出几何悲观主义——一种模块化、计算高效的框架，通过基于状态-动作嵌入空间中k近邻距离的密度惩罚来增强标准IQL。通过预计算应用于各状态-动作对的惩罚项，本方法以O(1)训练开销通过奖励塑形注入OOD保守性。在D4RL MuJoCo基准测试中，我们的Geo-IQL方法在敏感不稳定的medium-replay任务上超越标准IQL超过18分，同时将随机种子间标准差降低4倍，且在稳定流形上性能无衰减。关键的是，我们在MIMIC-III脓毒症重症监护数据集上验证了算法：当标准IQL退化为行为克隆时，Geo-IQL展现出主动策略改进，在保持安全约束下与临床医生终局决策吻合率达86.4%（IQL为75%）。结果表明几何悲观主义能为关键现实决策系统安全克服局部最优提供必要正则化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in offline reinforcement learning where existing methods either demand high computational resources or fail to correct out-of-distribution errors in sparse datasets, leading to suboptimal policies. The authors propose Geometric Pessimism, a modular framework that enhances the efficient IQL method by incorporating a density-based penalty derived from k-nearest-neighbor distances in the state-action embedding space, applied via reward shaping with minimal training overhead. Experimental results on the D4RL MuJoCo benchmark show that Geo-IQL outperforms standard IQL by over 18 points on sensitive tasks and reduces performance variability, while on the MIMIC-III Sepsis dataset, it achieves active policy improvement with 86.4% terminal agreement with clinicians, compared to IQL&#x27;s 75%, demonstrating effective regularization for real-world applications.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中现有方法要么计算成本高昂，要么在稀疏数据集中无法纠正分布外误差导致策略次优的问题。作者提出几何悲观主义，这是一个模块化框架，通过基于状态-动作嵌入空间中k近邻距离的密度惩罚来增强高效的IQL方法，并以最小训练开销通过奖励塑形应用。在D4RL MuJoCo基准测试中，Geo-IQL在敏感任务上比标准IQL性能高出超过18分并降低了性能波动；在MIMIC-III脓毒症数据集上，它实现了积极的策略改进，与临床医生的终点一致性达到86.4%，而IQL仅为75%，证明了其在现实世界关键决策系统中的有效正则化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow</div>
<div class="meta-line">Authors: Minh Nguyen</div>
<div class="meta-line">First: 2026-02-16T09:35:25+00:00 · Latest: 2026-02-16T09:35:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14587v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14587v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于哈密顿流解耦的连续时间强化学习</div>
<div class="mono" style="margin-top:8px">从金融到机器人领域的许多现实控制问题，均在连续时间中演化，涉及非均匀、事件驱动的决策。基于固定步长贝尔曼更新的标准离散时间强化学习在此场景中面临困难：随着时间间隔缩小，Q函数会坍缩为价值函数V，导致动作排序失效。现有连续时间方法通过优势率函数q重新引入动作信息，但需借助复杂的鞅损失或正交约束来保证最优性，这些约束对测试过程的选择敏感，且将V和q耦合为庞大复杂的优化问题，难以稳定训练。为克服这些局限，本文提出一种新型解耦连续时间行动者-评判者算法，采用交替更新机制：q通过V的扩散生成器学习，而V通过基于哈密顿的价值流更新，该价值流在无穷小时间步下仍保持信息有效性，避免了标准最大/软最大值备份的失效。理论上，我们通过新的概率论证证明了严格收敛性，绕开了基于生成器的哈密顿量在无穷范数下缺乏贝尔曼式收缩性的难题。实证表明，在挑战性连续控制基准和真实交易任务中，本方法优于现有连续时间及主流离散时间基线，单季度实现21%的收益，近乎两倍于次优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying reinforcement learning to continuous-time control problems, where standard discrete-time methods fail as time intervals shrink because the Q-function collapses to the value function, losing action discrimination. To overcome this, the authors propose a decoupled continuous-time actor-critic algorithm that separates the learning of the advantage-rate function q and the value function V: q is derived from diffusion generators on V, while V is updated via a Hamiltonian-based value flow that remains effective under infinitesimal time steps. Experimental results demonstrate that this method outperforms existing continuous-time and discrete-time baselines on continuous-control benchmarks and a real-world trading task, achieving a 21% profit over a single quarter, nearly doubling the performance of the second-best approach.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在连续时间控制问题中的应用挑战，指出标准离散时间方法在时间间隔趋近于零时，Q函数会退化为价值函数，从而丧失动作区分能力。为解决这一问题，作者提出了一种解耦的连续时间行动者-评论者算法，将优势率函数q和价值函数V的学习分离：q通过V的扩散生成器学习，而V则通过基于哈密顿量的价值流更新，该方法在无限小时间步下仍保持有效性。实验结果表明，该方法在连续控制基准测试和真实世界交易任务中优于现有连续时间和离散时间基线，单季度实现21%的利润，几乎是次优方法的两倍。</div>
</details>
</div>
<div class="card">
<div class="title">RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch</div>
<div class="meta-line">Authors: Isam Vrce, Andreas Kassler, Gökçe Aydos</div>
<div class="meta-line">First: 2026-02-16T09:17:29+00:00 · Latest: 2026-02-16T09:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14578v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RNM-TD3：从零开始的N:M半结构化稀疏强化学习</div>
<div class="mono" style="margin-top:8px">稀疏性是一种经过深入研究的深度神经网络压缩技术，可在不影响性能的前提下实现压缩。在深度强化学习中，仅保留原始权重5%的神经网络经过训练后，其性能损失相较于稠密网络仍可控制在最低限度。然而，现有方法多依赖非结构化细粒度稀疏，其不规则计算模式限制了硬件加速潜力。结构化粗粒度稀疏虽能实现硬件加速，但通常会导致性能下降并增加剪枝复杂度。本研究首次在强化学习领域探索N:M结构化稀疏方案，在压缩率、性能与硬件效率间取得平衡。该框架在离线策略强化学习（TD3）中为所有网络实施行级N:M稀疏训练，保持与支持N:M稀疏矩阵运算的加速器兼容。连续控制基准测试表明，在50%-75%稀疏度（如2:4和1:4）下，我们提出的N:M稀疏智能体RNM-TD3性能优于稠密版本，其中在Ant环境中2:4稀疏度下性能提升达14%。即使在87.5%稀疏度（1:8）下，RNM-TD3仍保持竞争力，同时具备潜在训练加速优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing sparsity techniques in deep reinforcement learning (DRL), where unstructured fine-grained sparsity hinders hardware acceleration and structured coarse-grained sparsity often degrades performance. The method introduces RNM-TD3, a framework that enforces row-wise N:M semi-structured sparsity throughout training in off-policy RL (specifically TD3), ensuring compatibility with accelerators supporting such sparse operations. Experimental results on continuous-control benchmarks demonstrate that RNM-TD3 outperforms dense counterparts at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% performance increase in the Ant environment at 2:4 sparsity, and remains competitive even at 87.5% sparsity (1:8) while enabling potential training speedups.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决深度强化学习中现有稀疏化技术的局限性，即非结构化细粒度稀疏化阻碍硬件加速，而结构化粗粒度稀疏化常导致性能下降。方法上提出了RNM-TD3框架，在离线策略强化学习（TD3）中全程实施行级N:M半结构化稀疏化，确保与支持此类稀疏操作的加速器兼容。在连续控制基准测试中的实验结果表明，RNM-TD3在50%-75%稀疏度（如2:4和1:4）下优于稠密模型，在Ant环境中2:4稀疏度时性能提升高达14%，即使在87.5%稀疏度（1:8）下仍保持竞争力，同时有望实现训练加速。</div>
</details>
</div>
<div class="card">
<div class="title">Fluid-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Shishir Sharma, Doina Precup, Theodore J. Perkins</div>
<div class="meta-line">First: 2026-02-16T08:37:46+00:00 · Latest: 2026-02-16T08:37:46+00:00</div>
<div class="meta-line">Comments: Published in the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14559v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14559v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>流体智能体强化学习</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）的主要研究焦点一直是探讨嵌入环境中的固定数量智能体之间的交互。然而，在现实世界中，智能体的数量既非固定也非预先可知。此外，智能体可以决定创建其他智能体（例如，细胞可能分裂，或公司可能拆分部门）。本文提出一个允许智能体创建其他智能体的框架，我们称之为流体智能体环境。我们提出了流体智能体博弈的博弈论解概念，并在此框架内实证评估了多种MARL算法的性能。实验包括经典基准的流体变体，如捕食者-猎物和基于等级的觅食环境，其中智能体可动态生成，以及我们引入的新环境，突显流动性如何能解锁固定群体设置中无法观察到的新颖解决策略。我们证明该框架能产生动态调整规模以适应环境需求的智能体团队。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of traditional multi-agent reinforcement learning (MARL) in handling environments where the number of agents is not fixed or known in advance, this paper introduces a fluid-agent framework that allows agents to dynamically create other agents. The method involves defining game-theoretic solution concepts for such fluid-agent games and empirically evaluating several MARL algorithms within this framework, using adapted benchmarks like Predator-Prey and Level-Based Foraging, as well as a new custom environment. The main experimental results demonstrate that this approach enables agent teams to dynamically adjust their population size in response to environmental demands, unlocking novel solution strategies not possible in fixed-population settings.</div>
<div class="mono" style="margin-top:8px">本文的动机源于传统多智能体强化学习（MARL）无法处理智能体数量不固定或未知的动态环境，因此提出了一个流体智能体框架，允许智能体动态创建其他智能体。方法包括为这类流体智能体博弈定义博弈论解概念，并在该框架内对多种MARL算法进行实证评估，使用了改编的基准环境（如捕食者-猎物和基于等级的觅食）以及一个新的自定义环境。主要实验结果表明，该方法能使智能体团队根据环境需求动态调整其规模，从而解锁在固定群体设置中无法实现的新颖解决策略。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Width Neural Networks</div>
<div class="meta-line">Authors: Federico Errica, Henrik Christiansen, Viktor Zaverkin, Mathias Niepert, Francesco Alesiani</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-01-27T09:25:56+00:00 · Latest: 2026-02-16T08:15:11+00:00</div>
<div class="meta-line">Comments: International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.15889v5">Abs</a> · <a href="https://arxiv.org/pdf/2501.15889v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For almost 70 years, researchers have typically selected the width of neural networks&#x27; layers either manually or through automated hyperparameter tuning methods such as grid search and, more recently, neural architecture search. This paper challenges the status quo by introducing an easy-to-use technique to learn an unbounded width of a neural network&#x27;s layer during training. The method jointly optimizes the width and the parameters of each layer via standard backpropagation. We apply the technique to a broad range of data domains such as tables, images, text, sequences, and graphs, showing how the width adapts to the task&#x27;s difficulty. A by product of our width learning approach is the easy truncation of the trained network at virtually zero cost, achieving a smooth trade-off between performance and compute resources. Alternatively, one can dynamically compress the network until performances do not degrade. In light of recent foundation models trained on large datasets, requiring billions of parameters and where hyper-parameter tuning is unfeasible due to huge training costs, our approach introduces a viable alternative for width learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自适应宽度神经网络</div>
<div class="mono" style="margin-top:8px">近70年来，研究者通常通过人工选择或网格搜索等自动化超参数调优方法（近年来更多采用神经架构搜索）来确定神经网络各层的宽度。本文提出一种易于使用的技术，可在训练过程中学习神经网络层的无界宽度，从而挑战这一传统范式。该方法通过标准反向传播联合优化每层的宽度与参数。我们将该技术应用于表格、图像、文本、序列及图结构等广泛数据领域，展示了宽度如何根据任务难度自适应调整。该宽度学习方法的副产品是能够以近乎零成本对训练后的网络进行截断，实现性能与计算资源间的平滑权衡；亦可动态压缩网络直至性能不出现衰减。鉴于近期基于海量数据训练的基础模型需数十亿参数，且因训练成本巨大导致超参数调优难以实施，本方法为宽度学习提供了可行的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the long-standing reliance on manual or computationally intensive methods for determining neural network layer widths, this paper introduces a technique to learn an unbounded layer width directly during training via standard backpropagation. The method jointly optimizes width and network parameters, adapting to task difficulty across diverse data domains including tables, images, text, sequences, and graphs. Experimental results demonstrate that the approach enables easy network truncation with minimal cost, offering a smooth performance-compute trade-off and dynamic compression without performance degradation, providing a viable alternative to hyperparameter tuning for large-scale models.</div>
<div class="mono" style="margin-top:8px">针对长期以来依赖手动或计算密集型方法确定神经网络层宽度的现状，本文提出了一种在训练过程中通过标准反向传播直接学习无边界层宽度的技术。该方法联合优化宽度和网络参数，能适应表格、图像、文本、序列和图等多种数据域的任务难度。实验结果表明，该方法能以极低成本实现网络截断，在性能和计算资源之间提供平滑权衡，并支持动态压缩而不降低性能，为大规模模型的超参数调优提供了可行替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs</div>
<div class="meta-line">Authors: Baorong Shi, Bo Cui, Boyuan Jiang, Deli Yu, Fang Qian, Haihua Yang, Huichao Wang, Jiale Chen, Jianfei Pan, Jieqiong Cao, Jinghao Lin, Kai Wu, Lin Yang, Shengsheng Yao, Tao Chen, Xiaojun Xiao, Xiaozhong Ji, Xu Wang, Yijun He, Zhixiong Yang</div>
<div class="meta-line">First: 2026-02-13T08:19:38+00:00 · Latest: 2026-02-16T08:13:49+00:00</div>
<div class="meta-line">Comments: XIAOHE Medical AI team. Currently, the model is exclusively available on XIAOHE AI Doctor, accessible via both the App Store and the Douyin Mini Program</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12705v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12705v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedXIAOHE：构建医疗多模态大语言模型的完整方案</div>
<div class="mono" style="margin-top:8px">我们推出MedXIAOHE——一个旨在提升真实临床场景中通用医疗理解与推理能力的医疗视觉语言基础模型。该模型在多项医疗基准测试中取得最先进性能，并在多种能力上超越领先的闭源多模态系统。为实现这一目标，我们提出一种实体感知的持续预训练框架，通过组织异构医疗语料库来拓宽知识覆盖范围并减少长尾差距（如罕见疾病）。针对医疗专家级推理与交互，MedXIAOHE通过强化学习和工具增强的智能体训练融合了多样化医疗推理模式，支持具有可验证决策轨迹的多步骤诊断推理。为提升实际应用的可靠性，模型整合了用户偏好评估标准、证据驱动的推理机制以及低幻觉长文本报告生成功能，并增强了对医疗指令的遵循度。本报告旨在记录我们的实践设计选择、规模化洞察与评估框架，以期激发进一步研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for advanced general-purpose medical understanding and reasoning in real-world clinical applications, this paper introduces MedXIAOHE, a medical vision-language foundation model. The method employs an entity-aware continual pretraining framework to organize heterogeneous medical corpora, broadening knowledge coverage and reducing long-tail gaps, and incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training for multi-step diagnostic reasoning. Experimentally, MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks, surpassing leading closed-source multimodal systems on multiple capabilities, while also demonstrating improved reliability through user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation.</div>
<div class="mono" style="margin-top:8px">本研究旨在推动现实世界临床应用中通用医疗理解和推理的发展，提出了医疗视觉语言基础模型MedXIAOHE。方法上，采用实体感知的持续预训练框架组织异构医疗语料，以拓宽知识覆盖并减少长尾差距，并通过强化学习和工具增强的智能体训练融入多样医疗推理模式，实现可验证决策轨迹的多步诊断推理。主要实验结果表明，MedXIAOHE在多样医疗基准测试中取得了最先进的性能，在多项能力上超越了领先的闭源多模态系统，同时通过用户偏好准则、证据驱动的推理和低幻觉长报告生成，提高了在实际应用中的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward</div>
<div class="meta-line">Authors: Renjun Xu, Yang Yan</div>
<div class="meta-line">First: 2026-02-12T21:33:25+00:00 · Latest: 2026-02-16T07:44:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12430v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12430v2">PDF</a> · <a href="https://github.com/scienceaix/agentskills">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The transition from monolithic language models to modular, skill-equipped agents marks a defining shift in how large language models (LLMs) are deployed in practice. Rather than encoding all procedural knowledge within model weights, agent skills -- composable packages of instructions, code, and resources that agents load on demand -- enable dynamic capability extension without retraining. It is formalized in a paradigm of progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP). This survey provides a comprehensive treatment of the agent skills landscape, as it has rapidly evolved during the last few months. We organize the field along four axes: (i) architectural foundations, examining the {SKILL.md} specification, progressive context loading, and the complementary roles of skills and MCP; (ii) skill acquisition, covering reinforcement learning with skill libraries, autonomous skill discovery (SEAgent), and compositional skill synthesis; (iii) deployment at scale, including the computer-use agent (CUA) stack, GUI grounding advances, and benchmark progress on OSWorld and SWE-bench; and (iv) security, where recent empirical analyses reveal that 26.1% of community-contributed skills contain vulnerabilities, motivating our proposed Skill Trust and Lifecycle Governance Framework -- a four-tier, gate-based permission model that maps skill provenance to graduated deployment capabilities. We identify seven open challenges -- from cross-platform skill portability to capability-based permission models -- and propose a research agenda for realizing trustworthy, self-improving skill ecosystems. Unlike prior surveys that broadly cover LLM agents or tool use, this work focuses specifically on the emerging skill abstraction layer and its implications for the next generation of agentic systems. Project repo: https://github.com/scienceaix/agentskills</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型的智能体技能：架构、获取、安全与未来路径</div>
<div class="mono" style="margin-top:8px">从单一语言模型向模块化、具备技能的智能体转变，标志着大语言模型在实际部署方式上的根本性变革。智能体技能——即智能体按需加载的指令、代码和资源的可组合包——无需重新训练即可实现动态能力扩展，而非将所有程序性知识编码在模型权重中。这一范式通过渐进式披露、可移植技能定义以及与模型上下文协议的集成得以形式化。本综述全面梳理了在过去几个月快速演进的智能体技能领域现状。我们围绕四个维度组织该领域：（一）架构基础，检视技能规范、渐进式上下文加载以及技能与模型上下文协议的互补作用；（二）技能获取，涵盖基于技能库的强化学习、自主技能发现与组合式技能合成；（三）规模化部署，包括计算机使用智能体技术栈、图形用户界面基础技术进展以及在操作系统世界和软件工程基准测试上的评估进展；（四）安全领域，近期实证分析显示26.1%的社区贡献技能存在漏洞，这促使我们提出技能信任与生命周期治理框架——一个基于四层门控权限的模型，将技能来源映射至分级部署能力。我们识别出从跨平台技能可移植性到基于能力的权限模型等七大开放挑战，并提出了实现可信、自我改进的技能生态系统的研究议程。与先前广泛涵盖大语言模型智能体或工具使用的综述不同，本研究聚焦于新兴的技能抽象层及其对下一代智能体系统的影响。项目仓库：https://github.com/scienceaix/agentskills</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey paper is motivated by the shift from monolithic large language models (LLMs) to modular agents equipped with on-demand, composable skills, enabling dynamic capability extension without retraining. The method involves formalizing this paradigm through progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP), organizing the landscape along architectural foundations, skill acquisition, deployment at scale, and security. Key experimental results include benchmark progress on OSWorld and SWE-bench, and a security analysis revealing that 26.1% of community-contributed skills contain vulnerabilities, leading to a proposed Skill Trust and Lifecycle Governance Framework.</div>
<div class="mono" style="margin-top:8px">本综述论文的动机是从单体大语言模型向配备按需组合技能的模块化智能体转变，以实现无需重新训练的动态能力扩展。其方法是通过渐进式披露、可移植技能定义以及与模型上下文协议（MCP）的集成来形式化这一范式，并从架构基础、技能获取、规模化部署和安全性四个维度梳理该领域。主要实验结果包括在OSWorld和SWE-bench基准测试上的进展，以及一项安全分析显示26.1%的社区贡献技能存在漏洞，从而提出了一个技能信任与生命周期治理框架。</div>
</details>
</div>
<div class="card">
<div class="title">TWISTED-RL: Hierarchical Skilled Agents for Knot-Tying without Human Demonstrations</div>
<div class="meta-line">Authors: Guy Freund, Tom Jurgenson, Matan Sudry, Erez Karpas</div>
<div class="meta-line">First: 2026-02-16T07:21:02+00:00 · Latest: 2026-02-16T07:21:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14526v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic knot-tying represents a fundamental challenge in robotics due to the complex interactions between deformable objects and strict topological constraints. We present TWISTED-RL, a framework that improves upon the previous state-of-the-art in demonstration-free knot-tying (TWISTED), which smartly decomposed a single knot-tying problem into manageable subproblems, each addressed by a specialized agent. Our approach replaces TWISTED&#x27;s single-step inverse model that was learned via supervised learning with a multi-step Reinforcement Learning policy conditioned on abstract topological actions rather than goal states. This change allows more delicate topological state transitions while avoiding costly and ineffective data collection protocols, thus enabling better generalization across diverse knot configurations. Experimental results demonstrate that TWISTED-RL manages to solve previously unattainable knots of higher complexity, including commonly used knots such as the Figure-8 and the Overhand. Furthermore, the increase in success rates and drop in planning time establishes TWISTED-RL as the new state-of-the-art in robotic knot-tying without human demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TWISTED-RL：无需人类演示的分层技能代理结绳系统</div>
<div class="mono" style="margin-top:8px">机器人结绳因涉及可变形物体的复杂交互与严格拓扑约束，是机器人学的基础性难题。本文提出TWISTED-RL框架，该框架改进了此前无需演示的结绳技术（TWISTED）——原方法通过智能分解将单一结绳任务转化为可处理的子问题，并由专用代理分别处理。我们的方法将TWISTED中通过监督学习训练的单步逆模型，替换为基于抽象拓扑动作（而非目标状态）的多步强化学习策略。这一改进实现了更精细的拓扑状态转换，同时避免了高成本低效的数据收集流程，从而提升了不同绳结构型的泛化能力。实验结果表明，TWISTED-RL能够完成先前无法实现的高复杂度绳结，包括八字结、反手结等常用绳结。成功率的提升与规划时间的减少，共同确立了TWISTED-RL在无人类演示的机器人结绳领域的新标杆地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complex challenge of robotic knot-tying with deformable objects and topological constraints, this work introduces TWISTED-RL, a hierarchical framework that improves upon a prior method by replacing a single-step inverse model with multi-step reinforcement learning policies conditioned on abstract topological actions. This methodological shift enables more delicate state transitions and avoids inefficient data collection, leading to better generalization. The main experimental results show that TWISTED-RL successfully ties previously unattainable, higher-complexity knots like the Figure-8 and Overhand, achieving higher success rates and reduced planning time to set a new state-of-the-art in demonstration-free robotic knot-tying.</div>
<div class="mono" style="margin-top:8px">针对机器人打结任务中柔性物体交互复杂和拓扑约束严格的挑战，本研究提出了TWISTED-RL框架，它在先前方法的基础上，将基于监督学习的单步逆模型替换为以抽象拓扑动作为条件的多步强化学习策略。这一方法改进实现了更精细的状态转换，避免了低效的数据收集，从而提升了泛化能力。主要实验结果表明，TWISTED-RL能够成功完成先前无法实现的高复杂度绳结，如八字结和单结，同时提高了成功率并减少了规划时间，确立了其在无需人工示教的机器人打结任务中的最新技术水平。</div>
</details>
</div>
<div class="card">
<div class="title">Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC</div>
<div class="meta-line">Authors: Dennis Gross</div>
<div class="meta-line">First: 2026-02-16T06:37:34+00:00 · Latest: 2026-02-16T06:37:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14505v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14505v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC&#x27;s capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient&#x27;s evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC&#x27;s integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用COOL-MC对脓毒症治疗策略进行形式化验证与解释</div>
<div class="mono" style="margin-top:8px">在医疗领域，安全且可解释的序贯决策至关重要，然而用于优化脓毒症治疗的强化学习策略往往不透明且难以验证。传统概率模型检测器需处理完整状态空间，对于大型MDP不可行，且无法解释已习得策略的决策依据。COOL-MC封装了模型检测器Storm，并新增三项核心功能：仅构建训练策略诱导的可达状态空间，生成更小的离散时间马尔可夫链，使得在完整MDP分析不可行时仍可验证；自动为状态标注具有临床意义的原子命题；将可解释性方法与概率计算树逻辑查询相结合，揭示治疗轨迹中驱动决策的关键特征。我们在ICU-Sepsis MDP（基于约17,000份脓毒症患者记录的基准数据集）上验证COOL-MC的功能，将其作为脓毒症治疗策略形式化分析的案例研究。通过完整MDP验证建立严格边界，训练出具有最优生存概率的安全强化学习策略，并借助PCTL验证和诱导DTMC的可解释性分析其行为。分析发现，训练策略主要依赖既往用药史而非患者实时病情变化——这一传统评估难以察觉的缺陷，通过COOL-MC整合的形式化验证与可解释性得以暴露。本研究表明，COOL-MC可作为临床医生在部署前研究和调试脓毒症治疗策略的有效工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for safe and interpretable sequential decision-making in healthcare, particularly for opaque reinforcement learning policies in sepsis treatment, this paper introduces COOL-MC, a tool that enhances formal verification. The method wraps the Storm model checker to construct only the reachable state space of a trained policy, enabling verification on a tractable discrete-time Markov chain, automatically labels states with clinical propositions, and integrates explainability via probabilistic computation tree logic queries. Experimental results on the ICU-Sepsis MDP, derived from 17,000 patient records, show that COOL-MC establishes hard bounds via full MDP verification, trains a safe RL policy achieving optimal survival probability, and reveals critical insights, such as the policy&#x27;s overreliance on dosing history rather than patient condition, which standard evaluation misses.</div>
<div class="mono" style="margin-top:8px">本文针对医疗保健中需要安全且可解释的序列决策问题，特别是脓毒症治疗中强化学习策略的不透明性，提出了COOL-MC工具以增强形式化验证。该方法通过封装Storm模型检查器，仅构建训练策略的可达状态空间，从而在可处理的离散时间马尔可夫链上进行验证，自动用临床命题标记状态，并整合基于概率计算树逻辑查询的可解释性方法。在基于约17,000份患者记录的ICU-Sepsis MDP上的实验结果表明，COOL-MC通过完整MDP验证建立了硬性边界，训练出实现最优生存概率的安全强化学习策略，并揭示了关键发现，例如策略过度依赖用药历史而非患者病情变化，这是标准评估所无法发现的。</div>
</details>
</div>
<div class="card">
<div class="title">TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Hao Ding, Zhichuan Yang, Weijie Ge, Ziqin Gao, Chaoyi Lu, Lei Zhao</div>
<div class="meta-line">First: 2026-02-16T05:46:47+00:00 · Latest: 2026-02-16T05:46:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14482v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TikArt：基于强化学习的孔径引导细粒度视觉推理观测方法</div>
<div class="mono" style="margin-top:8px">本研究针对多模态大语言模型中的细粒度视觉推理问题，提出TikArt（思维孔径）方法。该方法将多步视觉语言推理构建为对感兴趣区域的决策过程，通过“思考-孔径-观测”循环交替进行语言生成与两种孔径操作：Zoom提取矩形区域，Segment调用SAM2获取不规则目标的掩码区域。每次操作后模型需生成显式观测，将局部视觉线索转化为持久语言记忆。基于Qwen3-VL-8B构建的TikArt采用AGRPO强化学习算法进行策略优化，该算法采用两阶段课程学习：先预热分割操作，再联合优化视觉数学、细粒度VQA和分割任务，其奖励机制将任务成功率与孔径操作有效性耦合。在V*、HR-Bench-4K/8K、MME-RealWorld-Lite、MMStar、RefCOCO和ReasonSeg数据集上的实验表明，该方法在基础模型上取得稳定性能提升，并为高分辨率推理生成可解释的孔径轨迹。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fine-grained visual reasoning in multimodal large language models (MLLMs), where critical details in small or cluttered image regions are often lost with standard global encoding. The authors introduce TikArt, an agent that frames reasoning as a sequential decision process guided by aperture actions—Zoom for rectangular crops and Segment using SAM2 for mask-based crops—within a Think-Aperture-Observe loop that converts local visual information into explicit linguistic observations. Built upon Qwen3-VL-8B and optimized with a two-stage curriculum reinforcement learning algorithm called AGRPO, TikArt demonstrates consistent performance improvements over its backbone model across multiple benchmarks including V*, HR-Bench, MME-RealWorld-Lite, and segmentation tasks, while producing interpretable action trajectories for high-resolution reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型中的细粒度视觉推理问题，旨在解决关键证据常因微小物体、杂乱区域或细微标记在单一全局图像编码中丢失的挑战。研究者提出了TikArt，这是一个基于孔径引导的智能体，它将多步视觉语言推理构建为对感兴趣区域的决策过程，通过“思考-孔径-观察”循环交替进行语言生成与两种孔径操作：Zoom提取矩形裁剪，Segment调用SAM2获取基于掩码的不规则目标裁剪。该模型在每次操作后必须生成明确的观察结果，将局部视觉线索转化为持久的语言记忆。基于Qwen3-VL-8B构建，TikArt使用名为AGRPO的两阶段课程强化学习算法优化其推理策略，实验在V*、HR-Bench-4K/8K、MME-RealWorld-Lite等多个基准测试上显示其性能持续超越骨干模型，并为高分辨率推理提供了可解释的孔径操作轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">Making Slow Thinking Faster: Compressing LLM Chain-of-Thought via Step Entropy</div>
<div class="meta-line">Authors: Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, Qiang Xu</div>
<div class="meta-line">First: 2025-08-05T11:48:18+00:00 · Latest: 2026-02-16T05:36:50+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.03346v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.03346v2">PDF</a> · <a href="https://github.com/staymylove/COT_Compresstion_via_Step_entropy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies \emph{the informational contribution of individual reasoning steps} to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly improves LLM inference efficiency while preserving accuracy, paving the way for more scalable LLM deployments and a better understanding of their internal reasoning. The code and data are released in https://github.com/staymylove/COT_Compresstion_via_Step_entropy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>加速慢思考：基于步骤熵的LLM思维链压缩</div>
<div class="mono" style="margin-top:8px">采用思维链提示的大语言模型在复杂推理任务中表现出色，但会生成包含大量冗余的冗长思考过程，导致推理成本增加和效率降低。我们提出一种基于步骤熵的新型思维链压缩框架，该指标通过量化个体推理步骤的信息贡献来识别冗余。通过理论分析和数学推理基准的广泛实证验证，我们证明低熵步骤确实具有高度冗余性。实验表明，在DeepSeek-R1-7B、14B和Qwen3-8B模型上，可修剪高达80%的低熵中间步骤，且最终答案准确率仅轻微下降。这一发现与随机或高熵修剪形成鲜明对比，后者会严重损害推理性能。基于此，我们提出结合监督微调与群组相对策略优化的两阶段训练策略，使大语言模型能通过策略性插入[SKIP]标记，在推理过程中自主学习生成压缩思维链。该方法在保持准确性的同时显著提升推理效率，为大规模部署和深入理解模型内部推理机制开辟了新路径。代码与数据已发布于https://github.com/staymylove/COT_Compresstion_via_Step_entropy。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that Chain-of-Thought (CoT) prompting in Large Language Models (LLMs) produces verbose and redundant reasoning steps, which increases inference costs. The method introduces a compression framework based on step entropy, a metric to quantify the informational contribution of each step, and a two-stage training strategy combining Supervised Fine-Tuning and Group Relative Policy Optimization to teach models to generate compressed CoTs using [SKIP] tokens. The main experimental results show that pruning up to 80% of low-entropy steps causes only minor accuracy degradation on mathematical reasoning benchmarks for models like DeepSeek-R1 and Qwen3, whereas pruning high-entropy steps severely harms performance, validating the framework&#x27;s efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大语言模型（LLM）使用思维链（CoT）提示时产生的冗长且冗余的推理步骤所导致的高推理成本问题。方法上，提出了一个基于步骤熵的压缩框架，该指标用于量化单个推理步骤的信息贡献度，并结合监督微调与群体相对策略优化的两阶段训练策略，使模型学会在推理时使用[SKIP]标记生成压缩的思维链。主要实验结果表明，在DeepSeek-R1和Qwen3等模型的数学推理基准测试中，剪枝高达80%的低熵步骤仅导致最终答案准确率轻微下降，而剪枝高熵步骤则会严重损害推理性能，从而验证了该框架在提升效率方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization</div>
<div class="meta-line">Authors: Subhojyoti Mukherjee, Viet Dac Lai, Raghavendra Addanki, Ryan Rossi, Seunghyun Yoon, Trung Bui, Anup Rao, Jayakumar Subramanian, Branislav Kveton</div>
<div class="meta-line">First: 2025-06-08T01:59:30+00:00 · Latest: 2026-02-16T05:33:56+00:00</div>
<div class="meta-line">Comments: Advances in Neural Information Processing Systems 38</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06964v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.06964v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) is a variant of RL where the policy is learned from a previously collected dataset of trajectories and rewards. In our work, we propose a practical approach to offline RL with large language models (LLMs). We recast the problem as reward-weighted fine-tuning, which can be solved using similar techniques to supervised fine-tuning (SFT). To showcase the value of our approach, we apply it to learning short-horizon question-answering policies of a fixed length, where the agent reasons about potential answers or asks clarifying questions. Our work stands in a stark contrast to state-of-the-art methods in this domain, based on SFT and direct preference optimization, which have additional hyper-parameters and do not directly optimize for rewards. We compare to them empirically, and report major gains in both optimized rewards and language quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于奖励加权微调的对话优化离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）是一种从预先收集的轨迹与奖励数据集中学习策略的RL变体。本研究提出一种面向大语言模型（LLM）的实用离线RL方法，将问题重构为奖励加权微调，其求解技术可与监督微调（SFT）类比。为验证方法价值，我们将其应用于固定长度的短视程问答策略学习，使智能体能对潜在答案进行推理或提出澄清问题。与当前基于SFT和直接偏好优化的前沿方法相比，我们的方法无需额外超参数且直接优化奖励目标，实验结果表明其在奖励优化和语言质量方面均取得显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of applying offline reinforcement learning to large language models in a practical manner, particularly for conversational tasks. The method proposed reformulates offline RL as a reward-weighted fine-tuning problem, enabling the use of techniques similar to supervised fine-tuning without the complexities of traditional RL algorithms. Experimental results on short-horizon question-answering policies demonstrate that this approach achieves significant improvements in both optimized rewards and language quality compared to state-of-the-art methods like supervised fine-tuning and direct preference optimization, which often involve more hyperparameters and do not directly optimize for rewards.</div>
<div class="mono" style="margin-top:8px">本文的动机在于如何将离线强化学习实际应用于大型语言模型，特别是在对话任务中。所提出的方法将离线强化学习重新定义为奖励加权的微调问题，使得能够使用类似于监督微调的技术，而无需传统强化学习算法的复杂性。在短视界问答策略上的实验结果表明，与基于监督微调和直接偏好优化的先进方法相比，该方法在优化奖励和语言质量方面均取得了显著提升，这些先进方法通常涉及更多超参数且不直接优化奖励。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Transferability: A Two-Stage Reinforcement Learning Approach for Enhancing Quadruped Robots&#x27; Performance in U-Shaped Stair Climbing</div>
<div class="meta-line">Authors: Baixiao Huang, Baiyu Huang, Yu Hou</div>
<div class="meta-line">First: 2026-02-16T05:19:06+00:00 · Latest: 2026-02-16T05:19:06+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, International Conference on Computing in Civil Engineering (i3CE 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14473v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quadruped robots are employed in various scenarios in building construction. However, autonomous stair climbing across different indoor staircases remains a major challenge for robot dogs to complete building construction tasks. In this project, we employed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize a robot&#x27;s performance on U-shaped stairs. The training robot-dog modality, Unitree Go2, was first trained to climb stairs on Isaac Lab&#x27;s pyramid-stair terrain, and then to climb a U-shaped indoor staircase using the learned policies. This project explores end-to-end RL methods that enable robot dogs to autonomously climb stairs. The results showed (1) the successful goal reached for robot dogs climbing U-shaped stairs with a stall penalty, and (2) the transferability from the policy trained on U-shaped stairs to deployment on straight, L-shaped, and spiral stair terrains, and transferability from other stair models to deployment on U-shaped terrain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习可迁移性：一种两阶段强化学习方法用于提升四足机器人在U型楼梯攀爬中的性能</div>
<div class="mono" style="margin-top:8px">四足机器人被广泛应用于建筑施工现场。然而，在不同室内楼梯间实现自主攀爬仍是机器人完成建筑施工任务的主要挑战。本项目采用一种两阶段端到端深度强化学习方法，优化机器人在U型楼梯上的性能。训练机器人模型（Unitree Go2）首先在Isaac Lab的金字塔阶梯地形上学习攀爬，随后运用习得的策略攀爬U型室内楼梯。本项目探索了使机器人能够自主攀爬楼梯的端到端强化学习方法。结果表明：（1）在引入停滞惩罚机制下，机器人成功实现了U型楼梯攀爬的目标；（2）在U型楼梯上训练的策略可迁移部署至直梯、L型梯及螺旋阶梯地形，且其他楼梯模型训练的策略也可迁移部署至U型楼梯地形。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling quadruped robots to autonomously climb diverse indoor staircases, such as U-shaped stairs, for construction applications. The authors propose a two-stage end-to-end deep reinforcement learning method, where the robot is first trained on a simulated pyramid-stair terrain and then on a target U-shaped staircase to refine its policy. Experimental results demonstrate that the approach successfully allows the robot to reach its goal on U-shaped stairs and exhibits strong policy transferability, as policies learned for U-shaped stairs generalize to straight, L-shaped, and spiral stairs, and vice versa.</div>
<div class="mono" style="margin-top:8px">本文针对四足机器人在建筑场景中自主攀爬室内楼梯（如U型楼梯）的挑战展开研究。作者提出了一种两阶段的端到端深度强化学习方法，首先在模拟金字塔楼梯地形上训练机器人，然后在目标U型楼梯上微调策略以优化性能。实验结果表明，该方法成功实现了机器人在U型楼梯上的目标抵达，并且策略展现出良好的可迁移性：针对U型楼梯训练的策略能迁移部署到直梯、L型梯和螺旋楼梯，其他楼梯模型训练的策略也能迁移到U型地形上。</div>
</details>
</div>
<div class="card">
<div class="title">Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems</div>
<div class="meta-line">Authors: Furkan Mumcu, Yasin Yilmaz</div>
<div class="meta-line">First: 2026-02-16T05:17:58+00:00 · Latest: 2026-02-16T05:17:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14471v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14471v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent&#x27;s private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>社会加权对齐：多智能体大语言模型系统的博弈论框架</div>
<div class="mono" style="margin-top:8px">在共享环境中部署大语言模型（LLM）智能体时，个体对齐与集体稳定性之间存在根本性张力：局部理性决策可能产生负外部性，从而降低系统整体性能。本文提出社会加权对齐（SWA），这是一种博弈论框架，通过社会权重$λ\in[0,1]$在智能体私有目标与群体福利估计值之间进行插值，从而修改推理阶段的决策机制。在包含$n$个智能体且拥塞强度为$β$的共享资源拥塞博弈中，我们证明SWA会引致临界阈值$λ^*=(n-β)/(n-1)$，当超过该阈值时，智能体在过载状态下不再具有增加需求的边际激励，从而产生从持续拥塞到接近容量稳定运行的相变。我们进一步提出无需参数更新或多智能体强化学习的SWA推理时算法实例，并通过多智能体仿真实证验证了预测的阈值行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the conflict between individual alignment and collective stability when multiple LLM agents operate in shared environments, where self-interested decisions can cause negative externalities and degrade overall system performance. It introduces Socially-Weighted Alignment (SWA), a game-theoretic framework that adjusts inference-time decision-making by balancing an agent&#x27;s private objective with estimated group welfare through a social weight parameter λ. In a shared-resource congestion game, theoretical analysis reveals a critical threshold λ* = (n-β)/(n-1) that triggers a phase transition from persistent congestion to stable operation near capacity when exceeded, and an inference-time algorithm is proposed without requiring parameter updates or multi-agent reinforcement learning, with simulations empirically confirming the threshold behavior.</div>
<div class="mono" style="margin-top:8px">本文针对多个大语言模型智能体在共享环境中运行时个体对齐与集体稳定性之间的冲突问题展开研究，其中自利决策可能导致负外部性并降低系统整体性能。作者提出了社会加权对齐框架，这是一种博弈论方法，通过社会权重参数λ在推理时平衡智能体的私人目标与估计的群体福利。在一个共享资源拥堵博弈中，理论分析表明存在一个临界阈值λ* = (n-β)/(n-1)，当λ超过该阈值时，系统会从持续拥堵转变为接近容量的稳定运行，实现相变；同时提出了一种无需参数更新或多智能体强化学习的推理时算法，并通过多智能体仿真实验验证了该阈值行为。</div>
</details>
</div>
<div class="card">
<div class="title">LACONIC: Length-Aware Constrained Reinforcement Learning for LLM</div>
<div class="meta-line">Authors: Chang Liu, Yiran Zhao, Lawrence Liu, Yaoqi Ye, Csaba Szepesvári, Lin F. Yang</div>
<div class="meta-line">First: 2026-02-16T05:09:40+00:00 · Latest: 2026-02-16T05:09:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14468v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LACONIC：面向大语言模型的长度感知约束强化学习方法</div>
<div class="mono" style="margin-top:8px">强化学习通过奖励驱动的训练增强了大语言模型的能力，但该过程可能导致生成冗长响应，增加推理延迟与计算开销。现有长度控制方法多依赖固定的启发式奖励塑形，易与任务目标冲突且需脆弱调参。本研究提出LACONIC——一种在训练中强制执行目标令牌预算的强化学习方法。该方法通过结合任务奖励与基于长度的成本构建增强目标来更新策略模型，并动态调整成本规模以平衡简洁性与任务性能，在保持任务奖励的同时实现鲁棒的长度控制。理论分析为该方法提供支撑。在数学推理模型与数据集的实验中，LACONIC在保持或提升pass@1指标的同时将输出长度缩减超50%，在通用知识与多语言基准测试中以减少44%令牌量的条件保持跨域性能。该方法无需修改推理过程即可集成至标准RL微调流程，部署开销极低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for LACONIC stems from the issue that reinforcement learning (RL) for large language models (LLMs) often leads to excessively long responses, increasing latency and computational cost, while prior length-control methods rely on brittle heuristic tuning. The method introduces a constrained RL approach that enforces a target token budget by augmenting the policy objective with an adaptively scaled length-based cost, theoretically balancing brevity and task performance. Experimentally, on mathematical reasoning tasks, LACONIC reduces output length by over 50% while preserving or improving pass@1 accuracy, and it maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens, requiring no inference changes.</div>
<div class="mono" style="margin-top:8px">LACONIC的提出动机在于，针对大语言模型的强化学习常导致生成长度过长的响应，增加延迟和计算开销，而现有长度控制方法依赖脆弱的启发式调整。该方法采用一种约束强化学习方法，通过将自适应调整的长度成本与任务奖励结合来更新策略，以强制实现目标令牌预算，理论上平衡了简洁性与任务性能。实验结果表明，在数学推理任务中，LACONIC将输出长度减少超过50%，同时保持或提高了pass@1准确率；在通用知识和多语言基准测试中，它以减少44%的令牌数保持了性能，且无需推理阶段改动。</div>
</details>
</div>
<div class="card">
<div class="title">Online reinforcement learning via sparse Gaussian mixture model Q-functions</div>
<div class="meta-line">Authors: Minh Vu, Konstantinos Slavakis</div>
<div class="meta-line">First: 2025-09-18T03:37:11+00:00 · Latest: 2026-02-16T02:50:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14585v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.14585v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a structured and interpretable online policy-iteration framework for reinforcement learning (RL), built around the novel class of sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work that trained GMM-QFs offline, the proposed framework develops an online scheme that leverages streaming data to encourage exploration. Model complexity is regulated through sparsification by Hadamard overparametrization, which mitigates overfitting while preserving expressiveness. The parameter space of S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing for principled parameter updates via online gradient descent on a smooth objective. Numerical experiments show that S-GMM-QFs match or even outperform dense deep RL (DeepRL) methods on standard benchmarks while using significantly fewer parameters. Moreover, they maintain strong performance even in low-parameter regimes where sparsified DeepRL methods fail to generalize.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于稀疏高斯混合模型Q函数的在线强化学习</div>
<div class="mono" style="margin-top:8px">本文提出了一种结构清晰且可解释的在线策略迭代强化学习框架，其核心是新型稀疏高斯混合模型Q函数（S-GMM-QFs）。该框架在离线训练GMM-QFs的先前工作基础上，开发了一种利用流数据促进探索的在线方案。通过哈达玛过参数化进行稀疏化来调控模型复杂度，在保持表达力的同时缓解过拟合。S-GMM-QFs的参数空间天然具备黎曼流形结构，支持在光滑目标函数上通过在线梯度下降进行理论完备的参数更新。数值实验表明，在标准基准测试中，S-GMM-QFs使用显著更少的参数即可达到甚至超越密集深度强化学习方法。此外，在稀疏化深度强化学习方法泛化失效的低参数区域，S-GMM-QFs仍能保持强劲性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for structured and interpretable online reinforcement learning methods, introducing a novel framework based on sparse Gaussian mixture model Q-functions (S-GMM-QFs). The method extends prior offline GMM-QF work to an online setting, leveraging streaming data for exploration and using Hadamard overparametrization for sparsification to control model complexity and prevent overfitting, with parameter updates performed via online gradient descent on a Riemannian manifold. Experimental results demonstrate that S-GMM-QFs achieve comparable or superior performance to dense deep RL methods on benchmarks with far fewer parameters and maintain robustness in low-parameter regimes where sparsified deep RL methods fail.</div>
<div class="mono" style="margin-top:8px">本文旨在开发结构化和可解释的在线强化学习方法，提出了基于稀疏高斯混合模型Q函数（S-GMM-QFs）的新框架。该方法将先前的离线GMM-QF工作扩展到在线场景，利用流数据进行探索，并通过Hadamard过参数化进行稀疏化以控制模型复杂度和防止过拟合，参数更新通过在黎曼流形上进行在线梯度下降实现。实验结果表明，S-GMM-QFs在标准基准测试中与密集深度强化学习方法性能相当甚至更优，且参数数量显著减少，在低参数区域仍保持强健性能，而稀疏化的深度强化学习方法则无法泛化。</div>
</details>
</div>
<div class="card">
<div class="title">AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation</div>
<div class="meta-line">Authors: Morgan Byrd, Donghoon Baek, Kartik Garg, Hyunyoung Jung, Daesol Cho, Maks Sorokin, Robert Wright, Sehoon Ha</div>
<div class="meta-line">First: 2026-02-16T00:29:53+00:00 · Latest: 2026-02-16T00:29:53+00:00</div>
<div class="meta-line">Comments: Website: https://morganbyrd03.github.io/adaptmanip/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://morganbyrd03.github.io/adaptmanip/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaptManip：通过在线循环状态估计学习自适应全身物体抓取与递送</div>
<div class="mono" style="margin-top:8px">本文提出自适应全身移动操作框架AdaptManip，使人形机器人能够完全自主地执行集成导航、物体抓取与递送任务。与以往依赖人类演示且易受干扰的模仿学习方法不同，AdaptManip旨在通过强化学习训练鲁棒的移动操作策略，无需人类演示或遥操作数据。该框架包含三个耦合组件：(1) 循环物体状态估计器，在有限视野和遮挡条件下实时追踪被操作物体；(2) 全身基座策略，通过残差操作控制实现鲁棒移动与稳定物体抓取递送；(3) 基于LiDAR的机器人全局位置估计器，提供抗漂移定位能力。所有组件均通过强化学习在仿真中训练，并以零样本方式部署至实体硬件。实验表明，AdaptManip在适应性和整体成功率上显著优于包括模仿学习方法在内的基线方法，且精确的物体状态估计能在遮挡环境下提升操作性能。我们进一步在人形机器人上实现了完全自主的真实环境导航、物体抓取与递送演示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces AdaptManip, a reinforcement learning framework for autonomous humanoid robots to perform integrated navigation, object lifting, and delivery without relying on human demonstrations. The method employs a recurrent object state estimator for real-time tracking under occlusions, a whole-body policy for locomotion with residual manipulation control, and a LiDAR-based global position estimator, all trained in simulation and deployed zero-shot. Experiments show that AdaptManip outperforms imitation learning baselines in adaptability and success rate, with robust object state estimation enhancing manipulation under occlusion, and it demonstrates full autonomy in real-world tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了AdaptManip，一种无需人类演示、基于强化学习的框架，用于人形机器人自主执行导航、物体抓取与递送任务。该方法包含一个在遮挡下实时跟踪物体的循环状态估计器、一个用于运动并带残余操控控制的全身体策略，以及一个基于激光雷达的全局位置估计器，所有组件均在仿真中训练并零样本部署到真实硬件。实验结果表明，AdaptManip在适应性和整体成功率上显著优于基于模仿学习的基线方法，准确的对象状态估计提升了遮挡下的操控性能，并在真实人形机器人上实现了完全自主的导航、抓取与递送。</div>
</details>
</div>
<div class="card">
<div class="title">WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control</div>
<div class="meta-line">Authors: Mehran Aghabozorgi, Alireza Moazeni, Yanshu Zhang, Ke Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-15T23:53:16+00:00 · Latest: 2026-02-15T23:53:16+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026. OpenReview: https://openreview.net/forum?id=mzLOnTb3WH</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14351v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14351v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WIMLE：基于不确定性感知与IMLE的世界模型，用于样本高效的连续控制</div>
<div class="mono" style="margin-top:8px">基于模型的强化学习虽具备样本效率优势，但在实践中常因以下问题表现不佳：模型误差累积、单模态世界模型对多模态动态的平均化处理，以及过度自信的预测偏差。本文提出WIMLE方法，将隐式最大似然估计（IMLE）扩展至基于模型的强化学习框架，通过集成与隐变量采样实现无需迭代采样的随机多模态世界模型学习及预测不确定性估计。训练过程中，WIMLE依据预测置信度加权合成状态转移，在保留有效模型推演的同时抑制不确定预测的偏差，实现稳定学习。在涵盖DeepMind Control、MyoSuite和HumanoidBench的40个连续控制任务中，WIMLE展现出卓越的样本效率，其渐近性能优于或媲美主流无模型及基于模型的基线方法。尤其在Humanoid-run任务中，样本效率较最强基线提升超50%；在HumanoidBench中成功解决14项任务中的8项（对比BRO的4项与SimbaV2的5项）。这些结果凸显了基于IMLE的多模态建模与不确定性感知加权对稳定模型强化学习的重要价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the practical shortcomings of model-based reinforcement learning, such as compounding model error and overconfident predictions from unimodal world models, this paper introduces WIMLE, a method that integrates Implicit Maximum Likelihood Estimation (IMLE) to learn stochastic, multi-modal world models without iterative sampling and estimates predictive uncertainty via ensembles and latent sampling. The method weights synthetic transitions by predicted confidence during training to preserve useful rollouts while attenuating bias from uncertain predictions, enabling stable learning. Experimental results across 40 continuous-control tasks in DeepMind Control, MyoSuite, and HumanoidBench demonstrate superior sample efficiency and competitive asymptotic performance, with WIMLE improving sample efficiency by over 50% on Humanoid-run and solving 8 of 14 tasks on HumanoidBench, outperforming strong baselines.</div>
<div class="mono" style="margin-top:8px">针对基于模型的强化学习中存在的模型误差累积、单模态世界模型对多模态动态平均以及预测过度自信导致学习偏差等实际问题，本文提出了WIMLE方法，该方法将隐式最大似然估计（IMLE）扩展到基于模型的强化学习框架中，以学习无需迭代采样的随机多模态世界模型，并通过集成和潜在采样来估计预测不确定性。在训练过程中，WIMLE根据预测置信度对合成状态转移进行加权，以保留有用的模型推演同时减少不确定预测带来的偏差，从而实现稳定学习。在涵盖DeepMind Control、MyoSuite和HumanoidBench的40个连续控制任务上的实验结果表明，WIMLE具有卓越的样本效率和竞争性的渐进性能，在Humanoid-run任务上样本效率相对最强基线提升超过50%，在HumanoidBench上解决了14个任务中的8个，优于现有强基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dual Goal Representations</div>
<div class="meta-line">Authors: Seohong Park, Deepinder Mann, Sergey Levine</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-08T07:07:39+00:00 · Latest: 2026-02-15T23:51:55+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.06714v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.06714v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we introduce dual goal representations for goal-conditioned reinforcement learning (GCRL). A dual goal representation characterizes a state by &quot;the set of temporal distances from all other states&quot;; in other words, it encodes a state through its relations to every other state, measured by temporal distance. This representation provides several appealing theoretical properties. First, it depends only on the intrinsic dynamics of the environment and is invariant to the original state representation. Second, it contains provably sufficient information to recover an optimal goal-reaching policy, while being able to filter out exogenous noise. Based on this concept, we develop a practical goal representation learning method that can be combined with any existing GCRL algorithm. Through diverse experiments on the OGBench task suite, we empirically show that dual goal representations consistently improve offline goal-reaching performance across 20 state- and pixel-based tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双重目标表示</div>
<div class="mono" style="margin-top:8px">本研究针对目标条件强化学习（GCRL）提出了双重目标表示方法。该表示通过“从所有其他状态出发的时序距离集合”来刻画状态特征，即利用状态与环境中所有其他状态之间的时序距离关系进行编码。此表示具备若干理想的理论特性：首先，它仅依赖于环境的内在动态特性，且对原始状态表示具有不变性；其次，该表示可证明包含恢复最优目标达成策略的充分信息，同时能过滤外生噪声。基于此概念，我们开发了一种可与现有任意GCRL算法结合的实际目标表示学习方法。通过在OGBench任务套件上的多样化实验，我们实证表明双重目标表示在20个基于状态和像素的任务中持续提升离线目标达成性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for robust and informative goal representations in goal-conditioned reinforcement learning (GCRL), this paper introduces dual goal representations, which characterize a state by the set of temporal distances to all other states, thereby encoding relational information based on environment dynamics. The method leverages this theoretically grounded representation, which is invariant to the original state encoding and filters exogenous noise, to develop a practical learning approach that can be integrated with any existing GCRL algorithm. Experimental results on the OGBench suite demonstrate that the proposed dual representations consistently enhance offline goal-reaching performance across 20 diverse state-based and pixel-based tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机是为目标条件强化学习（GCRL）构建鲁棒且信息丰富的目标表示，提出了对偶目标表示方法，该方法通过一个状态到所有其他状态的时间距离集合来刻画该状态，从而基于环境动态编码关系信息。该方法利用这种具有理论依据的表示——其对原始状态编码具有不变性并能过滤外生噪声——开发了一种可与任何现有GCRL算法结合使用的实用学习框架。在OGBench任务套件上的实验结果表明，所提出的对偶表示一致地提升了20个基于状态和基于像素的多样化任务中的离线目标达成性能。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Instruction Following in RL via Structured LTL Representations</div>
<div class="meta-line">Authors: Mathias Jackermeier, Mattia Giuri, Jacques Cloete, Alessandro Abate</div>
<div class="meta-line">First: 2026-02-15T23:22:50+00:00 · Latest: 2026-02-15T23:22:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14344v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14344v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结构化线性时序逻辑表示的强化学习零样本指令跟随</div>
<div class="mono" style="margin-top:8px">本研究探讨多任务强化学习中的指令跟随问题，要求智能体在训练中未见过的新任务上实现零样本执行。线性时序逻辑（LTL）作为一种强大的框架，近期被用于描述结构化、时间延展的任务。现有方法虽能训练通用策略，却常难以有效捕捉LTL规范中固有的丰富逻辑与时间结构。为此，我们提出一种学习结构化任务表示的新方法，以促进训练与泛化。该方法通过基于任务有限自动机构建的布尔公式序列来调节策略，采用分层神经架构编码公式的逻辑结构，并引入注意力机制使策略能够推理未来子目标。在多种复杂环境中的实验表明，本方法具有强大的泛化能力和优越性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of zero-shot instruction following in multi-task reinforcement learning, where agents must execute novel tasks not encountered during training. The motivation stems from the limitations of existing methods in capturing the complex logical and temporal structures of tasks specified via linear temporal logic (LTL). To overcome this, the authors propose a method that conditions policies on sequences of Boolean formulae derived from task automata, employing a hierarchical neural architecture with an attention mechanism to encode logical structure and reason about future subgoals. Experimental results across diverse complex environments show that this approach achieves strong generalization and outperforms prior techniques.</div>
<div class="mono" style="margin-top:8px">本文研究了多任务强化学习中的零样本指令跟随问题，即智能体需执行训练中未见过的新任务。现有方法虽能训练通用策略，但难以有效捕捉线性时序逻辑规范中丰富的逻辑与时间结构。为此，作者提出一种新方法，通过基于任务自动机构建的布尔公式序列来调节策略，采用分层神经架构和注意力机制编码逻辑结构并推理未来子目标。在多种复杂环境中的实验表明，该方法具有强大的泛化能力，性能优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Zhi Zhang, Zhen Han, Costas Mavromatis, Qi Zhu, Yunyi Zhang, Sheng Guan, Dingmin Wang, Xiong Zhou, Shuai Wang, Soji Adeshina, Vassilis Ioannidis, Huzefa Rangwala</div>
<div class="meta-line">First: 2026-02-15T23:14:05+00:00 · Latest: 2026-02-15T23:14:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14338v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14338v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>训练更少，学习更多：基于群组的强化学习自适应高效轨迹优化</div>
<div class="mono" style="margin-top:8px">强化学习在大语言模型后训练中扮演核心角色。现有方法中，群组相对策略优化被广泛采用，尤其适用于带可验证奖励的强化学习微调。在GRPO中，每个查询提示大语言模型生成一组固定规模N的轨迹。当群组内所有轨迹结果相同（全对或全错）时，群组归一化优势值变为零，导致梯度信号缺失并浪费微调算力。本文提出自适应高效轨迹优化，作为GRPO的增强方法。AERO采用自适应轨迹策略，应用选择性剔除以策略性剪枝轨迹，并维护贝叶斯后验分布以避免零优势死区。在三种模型配置上的实验表明，AERO在不牺牲性能的前提下提升了计算效率：在相同总轨迹预算下，平均减少约48%的总训练算力，单步耗时缩短约45%。尽管计算量大幅降低，AERO在Pass@8和Avg@8指标上均达到或超越GRPO，为基于强化学习的大语言模型对齐提供了实用、可扩展且计算高效的策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses inefficiencies in Group Relative Policy Optimization (GRPO), a reinforcement learning method used for large language model fine-tuning, where fixed-size groups of rollouts can yield zero gradient signals when all rollouts share identical outcomes, wasting computational resources. To overcome this, the authors propose Adaptive Efficient Rollout Optimization (AERO), which enhances GRPO by employing an adaptive rollout strategy, selective rejection to prune uninformative rollouts, and a Bayesian posterior to avoid zero-advantage scenarios. Experimental results across three model configurations show that AERO reduces total training compute by approximately 48% and decreases wall-clock time per step by about 45% on average, while maintaining or improving performance metrics such as Pass@8 and Avg@8, thereby offering a more compute-efficient approach for RL-based alignment.</div>
<div class="mono" style="margin-top:8px">本文针对用于大语言模型微调的强化学习方法——组相对策略优化（GRPO）中的效率问题，即当固定大小的组中所有rollout结果完全一致时，梯度信号为零，导致计算资源浪费。为解决此问题，作者提出了自适应高效rollout优化（AERO），通过采用自适应rollout策略、选择性拒绝以剪除无信息rollout，以及使用贝叶斯后验来避免零优势区域，从而改进GRPO。在三种模型配置上的实验结果表明，AERO将总训练计算量减少了约48%，平均每步墙上时间缩短了约45%，同时在Pass@8和Avg@8等性能指标上保持或优于GRPO，为基于强化学习的对齐提供了一种更高效的计算策略。</div>
</details>
</div>
<div class="card">
<div class="title">When is Offline Policy Selection Sample Efficient for Reinforcement Learning?</div>
<div class="meta-line">Authors: Vincent Liu, Prabhat Nagarajan, Andrew Patterson, Martha White</div>
<div class="meta-line">First: 2023-12-04T21:35:13+00:00 · Latest: 2026-02-15T22:55:29+00:00</div>
<div class="meta-line">Comments: AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.02355v2">Abs</a> · <a href="https://arxiv.org/pdf/2312.02355v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning algorithms often require careful hyperparameter tuning. Before deployment, we need to select amongst a set of candidate policies. However, there is limited understanding about the fundamental limits of this offline policy selection (OPS) problem. In this work we provide clarity on when sample efficient OPS is possible, primarily by connecting OPS to off-policy policy evaluation (OPE) and Bellman error (BE) estimation. We first show a hardness result, that in the worst case, OPS is just as hard as OPE, by proving a reduction of OPE to OPS. As a result, no OPS method can be more sample efficient than OPE in the worst case. We then connect BE estimation to the OPS problem, showing how BE can be used as a tool for OPS. While BE-based methods generally require stronger requirements than OPE, when those conditions are met they can be more sample efficient. Building on this insight, we propose a BE method for OPS, called Identifiable BE Selection (IBES), that has a straightforward method for selecting its own hyperparameters. We conclude with an empirical study comparing OPE and IBES, and by showing the difficulty of OPS on an offline Atari benchmark dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线强化学习中策略选择何时具备样本效率？</div>
<div class="mono" style="margin-top:8px">离线强化学习算法通常需要细致的超参数调优。在部署前，我们需要从一组候选策略中进行选择。然而，目前对离线策略选择（OPS）问题的基本限制理解有限。本研究通过将OPS与离策略策略评估（OPE）及贝尔曼误差（BE）估计相关联，明确了样本高效OPS可行的条件。首先，我们通过证明OPE到OPS的归约，展示了一个硬度结果：在最坏情况下，OPS与OPE同样困难，因此任何OPS方法在最坏情况下都不可能比OPE更样本高效。随后，我们将BE估计与OPS问题联系起来，说明BE如何作为OPS的工具。尽管基于BE的方法通常需要比OPE更强的条件，但当这些条件满足时，它们可能更具样本效率。基于这一见解，我们提出了一种用于OPS的BE方法——可识别贝尔曼误差选择（IBES），该方法具备简洁的超参数选择机制。最后，通过实证研究比较OPE与IBES，并展示了在离线Atari基准数据集上OPS的难度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the sample efficiency of offline policy selection (OPS) in reinforcement learning, motivated by the need to reliably choose among candidate policies without online interaction. The method connects OPS to off-policy policy evaluation (OPE) and Bellman error (BE) estimation, first proving a worst-case reduction showing OPS is as hard as OPE, thus establishing a fundamental efficiency limit. It then demonstrates that BE-based methods can be more sample-efficient under certain conditions, leading to the proposal of Identifiable BE Selection (IBES), which includes a hyperparameter selection strategy. Experimental results from an empirical study and an offline Atari benchmark confirm the theoretical hardness and compare the performance of OPE and IBES.</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习中离线策略选择（OPS）的样本效率问题，其动机是在无需在线交互的情况下可靠地从候选策略中进行选择。方法上将OPS与离线策略评估（OPE）和贝尔曼误差（BE）估计联系起来，首先通过归约证明在最坏情况下OPS与OPE同样困难，从而确立了基本的效率极限。随后指出在特定条件下，基于BE的方法可以具有更高的样本效率，并提出了可识别贝尔曼误差选择（IBES）方法，该方法包含超参数选择策略。在经验研究和离线Atari基准数据集上的实验结果验证了理论上的困难性，并比较了OPE与IBES的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study</div>
<div class="meta-line">Authors: Hani Beirami, M M Manjurul Islam</div>
<div class="meta-line">First: 2026-02-15T22:10:11+00:00 · Latest: 2026-02-15T22:10:11+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14322v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14322v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent&#x27;s actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于共形信号时序逻辑的强化学习鲁棒控制：案例研究</div>
<div class="mono" style="margin-top:8px">本研究探讨形式化时序逻辑规约如何提升航空航天应用中强化学习控制的安全性与鲁棒性。基于开源AeroBench F-16仿真基准，我们训练近端策略优化智能体调节发动机油门并跟踪指令空速。控制目标被编码为信号时序逻辑要求：在每次机动最后阶段将空速维持在预设区间。为实现运行时规约执行，我们提出共形STL防护罩，通过在线共形预测过滤强化学习智能体的动作。我们在三种设置下进行对比：（1）PPO基线，（2）PPO+传统基于规则的STL防护罩，（3）PPO+所提共形防护罩，测试场景包含标称工况及涉及气动模型失配、执行器速率限制、测量噪声和任务中设定点突变的极端应力工况。实验表明共形防护罩在保持接近基线性能的同时确保STL满足度，且比传统防护罩提供更强的鲁棒性保证。这些结果证明，将形式化规约监测与数据驱动的强化学习控制相结合，能显著提升挑战性环境下自主飞行控制的可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to improve the safety and robustness of reinforcement learning (RL) control in aerospace by integrating formal temporal logic specifications. The method employs a Proximal Policy Optimization (PPO) agent trained in an F-16 simulation, with the control objective encoded as a Signal Temporal Logic (STL) requirement for airspeed regulation. A novel conformal STL shield, using online conformal prediction to filter actions, is introduced and compared against a baseline PPO and a classical rule-based shield under both nominal and stressed conditions. Experimental results demonstrate that the conformal shield maintains STL satisfaction and near-baseline performance while offering stronger robustness guarantees than the classical alternative, thereby enhancing autonomous flight control reliability.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过整合形式化时序逻辑规范，提升强化学习在航空航天控制中的安全性与鲁棒性。方法采用在F-16仿真中训练的近端策略优化（PPO）智能体，将控制目标编码为保持空速的信号时序逻辑（STL）要求，并引入一种新颖的保形STL屏蔽器，利用在线保形预测来过滤动作，与基线PPO及传统基于规则的屏蔽器在标称和压力场景下进行对比。实验结果表明，该保形屏蔽器在维持STL满足度和接近基线性能的同时，提供了比传统屏蔽器更强的鲁棒性保证，从而显著增强了自主飞行控制在挑战性环境中的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning</div>
<div class="meta-line">Authors: Kris Shengjun Dong, Sahil Modi, Dima Nikiforov, Sana Damani, Edward Lin, Siva Kumar Sastry Hari, Christos Kozyrakis</div>
<div class="meta-line">First: 2026-02-15T19:48:43+00:00 · Latest: 2026-02-15T19:48:43+00:00</div>
<div class="meta-line">Comments: 15 pages, 33 pages with appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14293v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KernelBlaster：基于记忆增强上下文强化学习的持续跨任务CUDA优化</div>
<div class="mono" style="margin-top:8px">跨多代GPU架构优化CUDA代码具有挑战性，因为实现峰值性能需深入探索日益复杂且硬件特定的优化空间。传统编译器受限于固定启发式规则，而微调大语言模型成本高昂。然而，现有CUDA代码优化的智能体工作流难以聚合历史探索知识，导致采样偏差和次优解。我们提出KernelBlaster，一种记忆增强上下文强化学习框架，旨在提升基于LLM的GPU编程智能体的CUDA优化搜索能力。该框架通过将知识积累至可检索的持久化CUDA知识库，使智能体能够从经验中学习，并对未来任务做出系统化决策。我们设计了一种基于性能剖析与文本梯度的新型智能体流程，用于CUDA代码生成与优化，以实现跨代GPU架构的高性能表现。KernelBlaster引导LLM智能体系统化探索超越简单重写的高潜力优化策略。相较于PyTorch基线，本方法在KernelBench 1、2、3级测试中分别实现1.43倍、2.50倍和1.50倍的几何平均加速。我们开源了KernelBlaster智能体框架，附带测试工具、验证组件及可复现的评估流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of optimizing CUDA code across evolving GPU architectures, where traditional compilers rely on fixed heuristics and fine-tuning LLMs is costly, while existing agentic workflows struggle to accumulate prior knowledge, leading to biased sampling. It introduces KernelBlaster, a Memory-Augmented In-context Reinforcement Learning framework that builds a Persistent CUDA Knowledge Base to enable LLM-based agents to learn from experience and make informed decisions, employing a profile-guided, textual-gradient-based flow to explore high-potential optimization strategies beyond simple rewrites. Experimentally, the method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3 respectively compared to a PyTorch baseline, and the framework is released as open-source with evaluation tools.</div>
<div class="mono" style="margin-top:8px">本文针对跨代GPU架构优化CUDA代码的挑战，传统编译器依赖固定启发式方法且微调大语言模型成本高，而现有智能体工作流难以积累先验知识导致采样偏差和次优解。提出了KernelBlaster，一种记忆增强的上下文强化学习框架，通过构建持久化CUDA知识库使基于大语言模型的智能体能够从经验中学习并做出系统化决策，采用基于配置文件和文本梯度的流程探索超越简单重写的高潜力优化策略。实验结果表明，相比PyTorch基线，该方法在KernelBench级别1、2和3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速，该框架已作为开源智能体系统发布并包含测试工具和可复现评估流程。</div>
</details>
</div>
<div class="card">
<div class="title">Regularized Top-$k$: A Bayesian Framework for Gradient Sparsification</div>
<div class="meta-line">Authors: Ali Bereyhi, Ben Liang, Gary Boudreau, Ali Afana</div>
<div class="meta-line">Venue: IEEE Transactions on Signal Processing, vol. 73, pp. 4463 - 4478, 2025</div>
<div class="meta-line">First: 2025-01-10T00:32:46+00:00 · Latest: 2026-02-15T19:03:46+00:00</div>
<div class="meta-line">Comments: This paper has been published in IEEE Transactions on Signal Processing, vol. 73, pp. 4463 - 4478, 2025. The present arXiv version contains additional experimental results. 27 pages, 8 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.05633v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.05633v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Error accumulation is effective for gradient sparsification in distributed settings: initially-unselected gradient entries are eventually selected as their accumulated error exceeds a certain level. The accumulation essentially behaves as a scaling of the learning rate for the selected entries. Although this property prevents the slow-down of lateral movements in distributed gradient descent, it can deteriorate convergence in some settings. This work proposes a novel sparsification scheme that controls the learning rate scaling of error accumulation. The development of this scheme follows two major steps: first, gradient sparsification is formulated as an inverse probability (inference) problem, and the Bayesian optimal sparsification mask is derived as a maximum-a-posteriori estimator. Using the prior distribution inherited from Top-k, we derive a new sparsification algorithm which can be interpreted as a regularized form of Top-k. We call this algorithm regularized Top-k (RegTop-k). It utilizes past aggregated gradients to evaluate posterior statistics of the next aggregation. It then prioritizes the local accumulated gradient entries based on these posterior statistics. We validate our derivation through various numerical experiments. In distributed linear regression, it is observed that while Top-k remains at a fixed distance from the global optimum, RegTop-k converges to the global optimum at significantly higher compression ratios. We further demonstrate the generalization of this observation by employing RegTop-k in distributed training of ResNet-18 on CIFAR-10, as well as fine-tuning of multiple computer vision models on the ImageNette dataset. Our numerical results confirm that as the compression ratio increases, RegTop-k sparsification noticeably outperforms Top-k.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正则化Top-$k$：梯度稀疏化的贝叶斯框架</div>
<div class="mono" style="margin-top:8px">误差累积在分布式环境中对梯度稀疏化具有显著效果：初始未被选中的梯度分量会随着累积误差超过特定阈值而被选中。这种累积本质上表现为对选中分量学习率的缩放。虽然该特性避免了分布式梯度下降中横向移动的减速，但在某些场景下可能恶化收敛性。本研究提出了一种新型稀疏化方案，可控制误差累积对学习率的缩放效应。该方案的开发遵循两大步骤：首先将梯度稀疏化建模为逆概率（推断）问题，并推导出贝叶斯最优稀疏化掩码作为最大后验估计量。通过继承Top-k的先验分布，我们推导出可解释为正则化Top-k形式的新稀疏化算法，称为正则化Top-k（RegTop-k）。该算法利用历史聚合梯度评估下一次聚合的后验统计量，进而基于这些后验统计量对本地累积梯度分量进行优先级排序。我们通过多组数值实验验证了推导结果：在分布式线性回归中，Top-k始终与全局最优解保持固定距离，而RegTop-k能在更高压缩比下收敛至全局最优解。我们进一步通过在CIFAR-10数据集上分布式训练ResNet-18，以及在ImageNette数据集上微调多个计算机视觉模型，验证了该结论的普适性。数值结果表明，随着压缩比提升，RegTop-k稀疏化性能显著优于Top-k。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the convergence deterioration caused by error accumulation in distributed gradient sparsification methods like Top-k, where unselected gradients accumulate and eventually get selected, effectively scaling the learning rate. The authors propose a Bayesian framework that formulates gradient sparsification as an inverse probability problem, deriving a maximum-a-posteriori estimator to create a new algorithm called regularized Top-k (RegTop-k), which uses past aggregated gradients to compute posterior statistics and prioritize local gradient entries accordingly. Experimental results in distributed linear regression show that RegTop-k converges to the global optimum at high compression ratios where Top-k stagnates, and this advantage generalizes to training ResNet-18 on CIFAR-10 and fine-tuning vision models on ImageNette, with RegTop-k consistently outperforming Top-k as compression increases.</div>
<div class="mono" style="margin-top:8px">本文针对分布式梯度稀疏化方法（如Top-k）中误差累积导致的收敛恶化问题，其中未选中的梯度会累积并最终被选中，这实质上改变了学习率缩放。作者提出一个贝叶斯框架，将梯度稀疏化表述为逆概率问题，通过最大后验估计推导出一种新算法——正则化Top-k（RegTop-k），该算法利用历史聚合梯度计算后验统计量，从而优先选择局部梯度条目。在分布式线性回归实验中，RegTop-k在高压缩比下能收敛到全局最优解，而Top-k则停滞不前；这一优势在CIFAR-10上训练ResNet-18以及在ImageNette数据集上微调视觉模型时得到验证，结果表明随着压缩比提高，RegTop-k的性能显著优于Top-k。</div>
</details>
</div>
<div class="card">
<div class="title">GRAIL: Goal Recognition Alignment through Imitation Learning</div>
<div class="meta-line">Authors: Osher Elhadad, Felipe Meneguzzi, Reuth Mirsky</div>
<div class="meta-line">First: 2026-02-15T17:45:03+00:00 · Latest: 2026-02-15T17:45:03+00:00</div>
<div class="meta-line">Comments: Accepted for publication at AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14252v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14252v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding an agent&#x27;s goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor&#x27;s true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GRAIL：通过模仿学习实现目标识别对齐</div>
<div class="mono" style="margin-top:8px">从智能体行为中理解其目标是使AI系统与人类意图对齐的基础。现有目标识别方法通常依赖于最优的目标导向策略表示，这可能与行为者的真实行为存在差异，从而阻碍对其目标的准确识别。为弥补这一不足，本文提出通过模仿学习实现目标识别对齐（GRAIL），该方法利用模仿学习和逆强化学习，直接从（可能次优的）示范轨迹中为每个候选目标学习一个目标导向策略。通过单次前向传播，用每个学习到的目标导向策略对观察到的部分轨迹进行评分，GRAIL在保留经典目标识别一次性推理能力的同时，利用能够捕捉次优及系统性偏差行为的学习策略。在评估领域中，GRAIL在系统性偏差最优行为下将F1分数提升超过0.5，在次优行为下获得约0.1-0.3的增益，在噪声最优轨迹下实现高达0.4的改进，同时在完全最优场景中保持竞争力。这项工作为在不确定环境中解释智能体目标的可扩展且鲁棒的模型提供了贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to accurately infer an agent&#x27;s goals from its behavior for AI alignment, noting that existing methods rely on optimal policy representations which may mismatch suboptimal or biased demonstrations. The proposed method, GRAIL, uses imitation learning and inverse reinforcement learning to learn a goal-directed policy for each candidate goal directly from demonstration trajectories, enabling one-shot inference by scoring partial trajectories against these policies. Experimental results show that GRAIL significantly improves F1-scores, with gains over 0.5 under systematically biased optimal behavior, 0.1-0.3 under suboptimal behavior, and up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings.</div>
<div class="mono" style="margin-top:8px">本文的动机是为了实现AI对齐，需要从智能体行为中准确推断其目标，但现有方法依赖于最优策略表示，可能与次优或有偏差的演示不匹配。提出的GRAIL方法利用模仿学习和逆强化学习，直接从演示轨迹中为每个候选目标学习目标导向策略，通过将部分轨迹与这些策略评分实现一次性推理。实验结果表明，GRAIL显著提高了F1分数，在系统偏差最优行为下增益超过0.5，在次优行为下增益约0.1-0.3，在噪声最优轨迹下增益高达0.4，同时在完全最优设置中保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents</div>
<div class="meta-line">Authors: Zheng Chu, Xiao Wang, Jack Hong, Huiming Fan, Yuqi Huang, Yue Yang, Guohai Xu, Chenxiao Zhao, Cheng Xiang, Shengchao Hu, Dongdong Kuang, Ming Liu, Bing Qin, Xing Yu</div>
<div class="meta-line">First: 2026-02-15T17:04:46+00:00 · Latest: 2026-02-15T17:04:46+00:00</div>
<div class="meta-line">Comments: https://redsearchagent.github.io/index/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14234v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14234v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://redsearchagent.github.io/index/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>REDSearcher：面向长程搜索智能体的可扩展且经济高效的框架</div>
<div class="mono" style="margin-top:8px">大语言模型正从通用知识引擎转向现实问题求解器，但针对深度搜索任务的优化仍具挑战。核心瓶颈在于高质量搜索轨迹与奖励信号的极端稀疏性，这源于可扩展长程任务构建的困难，以及涉及外部工具调用的高交互成本。为应对这些挑战，我们提出REDSearcher——一个通过协同设计复杂任务合成、中期训练与后期训练来实现可扩展搜索智能体优化的统一框架。具体而言，REDSearcher引入以下改进：（1）将任务构建为双约束优化问题，通过图拓扑与证据分散度精确控制任务难度，实现复杂高质量任务的可扩展生成；（2）引入工具增强查询以鼓励主动使用工具而非被动回忆；（3）在中期训练中强化核心原子能力（知识、规划、函数调用），大幅降低为下游训练收集高质量轨迹的成本；（4）构建本地模拟环境，为强化学习实验提供快速低成本的算法迭代。在纯文本与多模态搜索智能体基准测试中，本方法均取得最先进性能。为促进长程搜索智能体的未来研究，我们将公开10K条高质量复杂文本搜索轨迹、5K条多模态轨迹、1K文本强化学习查询集，以及代码与模型检查点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for REDSearcher stems from the challenge of optimizing large language models for long-horizon search tasks, where high-quality training data and rewards are scarce due to the difficulty of scalable task creation and costly interactive rollouts. The method introduces a unified framework that co-designs task synthesis, mid-training, and post-training, featuring dual-constrained optimization for scalable complex task generation, tool-augmented queries to promote proactive tool use, and a local simulated environment for low-cost reinforcement learning iteration. Main experimental results show that REDSearcher achieves state-of-the-art performance on both text-only and multimodal search-agent benchmarks, and the authors plan to release extensive datasets including 10K text and 5K multimodal trajectories to support future research.</div>
<div class="mono" style="margin-top:8px">REDSearcher的动机源于优化大语言模型进行长程搜索任务的挑战，其中高质量训练数据和奖励信号因可扩展任务构建的困难和高成本交互式调用而稀缺。该方法提出了一个统一框架，协同设计任务合成、中期训练和后期训练，包括通过双约束优化实现可扩展的复杂任务生成、工具增强查询以鼓励主动工具使用，以及本地模拟环境支持低成本强化学习迭代。主要实验结果表明，REDSearcher在纯文本和多模态搜索代理基准测试中均达到了最先进的性能，作者计划发布包含1万条文本和5千条多模态轨迹的广泛数据集以支持未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS</div>
<div class="meta-line">Authors: Cong Wang, Changfeng Gao, Yang Xiang, Zhihao Du, Keyu An, Han Zhao, Qian Chen, Xiangang Li, Yingming Gao, Ya Li</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-12-04T08:12:49+00:00 · Latest: 2026-02-15T17:01:33+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04552v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.04552v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lrwinr.github.io/RRPO-CosyVoice">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RRPO：基于大语言模型的鲁棒奖励策略优化情感语音合成</div>
<div class="mono" style="margin-top:8px">DiffRO等可微分强化学习框架为可控文本转语音提供了强大方法，但在情感控制等精细任务中易受奖励攻击影响。策略模型可能通过生成声学伪影来利用普通奖励模型获取虚假奖励，却会损害感知质量。为此，我们提出鲁棒奖励策略优化框架，采用混合正则化方案构建鲁棒奖励模型，使其奖励信号更可靠地符合人类感知，促使策略放弃有害捷径并学习真实情感的复杂特征。消融实验证实了奖励模型的增强鲁棒性，其跨语言泛化能力即为明证。主观评估表明，该鲁棒奖励模型有效缓解了奖励攻击，在情感表现力和自然度上均显著超越所有基线。演示页面：https://lrwinr.github.io/RRPO-CosyVoice。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of differentiable reinforcement learning frameworks like DiffRO to reward hacking in emotion-controlled text-to-speech (TTS), where policies exploit acoustic artifacts to achieve spurious rewards at the cost of perceptual quality, this paper proposes Robust Reward Policy Optimization (RRPO). The method employs a hybrid regularization scheme to develop a robust reward model (RM) whose signals are more reliably aligned with human perception, compelling the policy to learn genuine emotional features instead of detrimental shortcuts. Experimental results from an ablation study confirm the enhanced robustness and strong cross-lingual generalization of the RM, while subjective evaluations demonstrate that RRPO effectively mitigates reward hacking, leading to significant improvements in emotional expressiveness and naturalness over all baselines.</div>
<div class="mono" style="margin-top:8px">针对可微分强化学习框架在情感控制文本到语音任务中易受奖励攻击的问题，即策略模型可能通过生成声学伪影获取虚假奖励而损害感知质量，本文提出了鲁棒奖励策略优化方法。该方法采用混合正则化方案，构建一个奖励信号与人类感知更可靠对齐的鲁棒奖励模型，迫使策略学习真实情感特征而非有害捷径。消融实验证实了该奖励模型增强的鲁棒性和强大的跨语言泛化能力，主观评估表明该方法有效缓解了奖励攻击，在情感表现力和自然度上均显著优于所有基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding</div>
<div class="meta-line">Authors: Fengxiang Wang, Mingshuo Chen, Yueying Li, Yajie Yang, Yuhao Zhou, Di Wang, Yifan Zhang, Haoyu Wang, Haiyan Zhao, Hongda Sun, Long Lan, Jun Song, Yulin Wang, Jing Zhang, Wenlong Zhang, Bo Du</div>
<div class="meta-line">First: 2026-02-15T16:40:33+00:00 · Latest: 2026-02-15T16:40:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14225v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14225v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) &quot;pre-warming&quot; on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本先于视觉：分阶段知识注入对超高分辨率遥感理解中具身RLVR至关重要</div>
<div class="mono" style="margin-top:8px">超高分辨率遥感的多模态推理常受视觉证据获取的制约：模型需在庞大像素空间中定位微小的任务相关区域。虽然使用放大工具的具身可验证奖励强化学习提供了解决路径，但我们发现标准强化学习在没有结构化领域先验的情况下难以驾驭这些广阔视觉空间。本文通过对照实验比较冷启动监督微调、RLVR与具身RLVR在超高分辨率遥感基准上的表现，得出反直觉结论：高质量的地球科学纯文本问答是提升超高分辨率视觉推理能力的主要驱动力。尽管不含图像，领域文本注入了引导视觉证据检索所需的概念、机制解释与决策规则。基于此，我们提出分阶段知识注入方案：（1）通过可扩展的知识图谱验证地球科学文本问答进行冷启动，植入推理结构；（2）在监督微调阶段对相同困难样本进行“预热”，以稳定并增强后续基于工具的强化学习。该方法在XLRS-Bench上达到60.40%的Pass@1，显著超越GPT-5.2、Gemini 3.0 Pro、Intern-S1等更大规模通用模型，创下新最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of visual evidence acquisition in ultra-high-resolution remote sensing, where models must locate tiny relevant regions in massive pixel spaces. The authors investigate agentic reinforcement learning with verifiable rewards (RLVR) and find that standard RL struggles without structured priors; counter-intuitively, they demonstrate that high-quality Earth-science text-only question-answering is a primary driver of visual reasoning gains, as it injects necessary concepts and decision rules. Their proposed method employs a staged knowledge injection: first cold-starting with knowledge-graph-verified text QA to instill reasoning structures, then pre-warming on hard image-text examples during supervised fine-tuning to stabilize subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on the XLRS-Bench, significantly outperforming larger general-purpose models and setting a new state-of-the-art.</div>
<div class="mono" style="margin-top:8px">本文针对超高分辨率遥感理解中视觉证据获取的挑战，即模型需要在海量像素空间中定位微小的相关区域。作者研究了基于可验证奖励的智能体强化学习，发现标准强化学习在没有结构化先验时难以应对；反直觉的是，他们证明高质量的地球科学纯文本问答是提升视觉推理能力的主要驱动力，因为它注入了必要的概念和决策规则。所提出的方法采用分阶段知识注入：首先通过知识图谱验证的文本问答进行冷启动以建立推理结构，然后在监督微调阶段对困难的图像-文本示例进行预热，以稳定后续基于工具的强化学习。该方法在XLRS-Bench上实现了60.40%的Pass@1，显著优于更大的通用模型，并确立了新的最先进水平。</div>
</details>
</div>
<div class="card">
<div class="title">GeoEyes: On-Demand Visual Focusing for Evidence-Grounded Understanding of Ultra-High-Resolution Remote Sensing Imagery</div>
<div class="meta-line">Authors: Fengxiang Wang, Mingshuo Chen, Yueying Li, Yajie Yang, Yifan Zhang, Long Lan, Xue Yang, Hongda Sun, Yulin Wang, Di Wang, Jun Song, Jing Zhang, Bo Du</div>
<div class="meta-line">First: 2026-02-15T15:50:55+00:00 · Latest: 2026-02-15T15:50:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14201v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The &quot;thinking-with-images&quot; paradigm enables multimodal large language models (MLLMs) to actively explore visual scenes via zoom-in tools. This is essential for ultra-high-resolution (UHR) remote sensing VQA, where task-relevant cues are sparse and tiny. However, we observe a consistent failure mode in existing zoom-enabled MLLMs: Tool Usage Homogenization, where tool calls collapse into task-agnostic patterns, limiting effective evidence acquisition. To address this, we propose GeoEyes, a staged training framework consisting of (1) a cold-start SFT dataset, UHR Chain-of-Zoom (UHR-CoZ), which covers diverse zooming regimes, and (2) an agentic reinforcement learning method, AdaZoom-GRPO, that explicitly rewards evidence gain and answer improvement during zoom interactions. The resulting model learns on-demand zooming with proper stopping behavior and achieves substantial improvements on UHR remote sensing benchmarks, with 54.23% accuracy on XLRS-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoEyes：面向证据驱动的超高分辨率遥感图像理解的按需视觉聚焦</div>
<div class="mono" style="margin-top:8px">“图像伴随思考”范式使多模态大语言模型（MLLMs）能够通过放大工具主动探索视觉场景。这对于超高分辨率（UHR）遥感视觉问答（VQA）至关重要，因为任务相关线索稀疏且微小。然而，我们观察到现有支持放大的MLLMs存在一致的失效模式：工具使用同质化，即工具调用退化为与任务无关的模式，限制了有效证据获取。为此，我们提出GeoEyes——一个分阶段训练框架，包含：（1）冷启动监督微调数据集UHR链式放大（UHR-CoZ），涵盖多样化的放大模式；（2）自主强化学习方法AdaZoom-GRPO，在放大交互过程中显式奖励证据增益与答案改进。最终模型学会按需放大并具备恰当的停止行为，在UHR遥感基准测试中取得显著提升，在XLRS-Bench上达到54.23%的准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the failure of existing zoom-enabled multimodal large language models (MLLMs) to perform task-specific visual exploration in ultra-high-resolution (UHR) remote sensing visual question answering, where relevant details are often sparse and tiny, this paper introduces GeoEyes to address tool usage homogenization. The method employs a staged training framework: first, a cold-start supervised fine-tuning dataset (UHR Chain-of-Zoom) covering diverse zooming patterns, followed by an agentic reinforcement learning approach (AdaZoom-GRPO) that explicitly rewards evidence gain and answer improvement during zoom interactions. Experimental results show that the model learns on-demand zooming with proper stopping behavior, achieving substantial improvements on UHR remote sensing benchmarks, including 54.23% accuracy on XLRS-Bench.</div>
<div class="mono" style="margin-top:8px">本文针对现有支持缩放的多模态大语言模型在超高分辨率遥感视觉问答中，因任务相关线索稀疏微小而出现的工具使用同质化问题，提出了GeoEyes框架以实现按需视觉聚焦。方法采用分阶段训练策略：首先利用涵盖多种缩放模式的冷启动监督微调数据集（UHR Chain-of-Zoom），然后通过一种代理强化学习方法（AdaZoom-GRPO），在缩放交互中明确奖励证据获取和答案改进。主要实验结果表明，该模型学会了按需缩放并具备适当的停止行为，在超高分辨率遥感基准测试上取得显著提升，如在XLRS-Bench上达到54.23%的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling</div>
<div class="meta-line">Authors: Yiran Guo, Zhongjian Qiao, Yingqi Xie, Jie Liu, Dan Ye, Ruiqing Zhang, Shuang Qiu, Lijie Xu</div>
<div class="meta-line">First: 2026-02-15T14:44:15+00:00 · Latest: 2026-02-15T14:44:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14169v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14169v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于枢轴驱动重采样的大语言模型强化学习深度密集探索方法</div>
<div class="mono" style="margin-top:8px">在大语言模型的强化学习中，有效探索是核心挑战：如何在有限采样预算下，从广阔的自然语言序列空间中发现高质量轨迹。现有方法存在明显局限：GRPO仅从根节点采样，导致高概率轨迹饱和，而对深层易错状态的探索不足；基于树结构的方法将预算盲目分散于平凡或不可恢复状态，引发采样稀释问题，既难以发现罕见正确后缀，又破坏局部基线的稳定性。为此，我们提出深度密集探索策略，将探索聚焦于失败轨迹中的可恢复深层状态——即“枢轴状态”。我们通过DEEP-GRPO实现该策略，其包含三项关键创新：(1) 轻量级数据驱动效用函数，自动平衡可恢复性与深度偏差以识别枢轴状态；(2) 在枢轴处进行局部密集重采样，提升发现后续正确轨迹的概率；(3) 双流优化目标，将全局策略学习与局部修正更新解耦。数学推理基准测试表明，本方法在GRPO、树结构方法及其他强基线模型上均取得稳定优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of effective exploration in reinforcement learning for large language models, where existing methods either saturate high-probability trajectories or waste sampling budget on trivial states, failing to uncover rare correct solutions. To overcome this, the authors propose Deep Dense Exploration (DDE), a strategy that focuses on pivot states—deep, recoverable points within unsuccessful trajectories—and instantiate it with DEEP-GRPO, which introduces a data-driven utility function to identify pivots, local dense resampling to discover correct subsequent paths, and a dual-stream optimization objective to decouple global and local learning. Experimental results on mathematical reasoning benchmarks show that this method consistently outperforms GRPO, tree-based approaches, and other strong baselines.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习中的有效探索挑战，现有方法要么饱和于高概率轨迹，要么在无关状态上浪费采样预算，难以发现罕见的正确解。为此，作者提出了深度密集探索策略，专注于不成功轨迹中的可恢复深度状态（即枢纽状态），并通过DEEP-GRPO实现该策略，其核心创新包括：使用轻量级数据驱动效用函数自动平衡可恢复性和深度偏差以识别枢纽，在枢纽处进行局部密集重采样以增加发现正确后续轨迹的概率，以及采用双流优化目标分离全局策略学习和局部纠正更新。在数学推理基准测试上的实验结果表明，该方法一致优于GRPO、基于树的方法及其他强基线。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Improving World Modelling with Latent Actions</div>
<div class="meta-line">Authors: Yifu Qiu, Zheng Zhao, Waylon Li, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti</div>
<div class="meta-line">First: 2026-02-05T19:04:41+00:00 · Latest: 2026-02-15T14:22:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06130v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06130v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_θ(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_φ(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model&#x27;s log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于隐式动作的自改进世界建模</div>
<div class="mono" style="margin-top:8px">世界的内在建模——预测在动作Z作用下从先前状态X到下一状态Y的转移——对LLM和VLM的推理与规划至关重要。学习此类模型通常需要代价高昂的动作标注轨迹。我们提出SWIRL框架，通过将动作视为隐变量，并交替进行前向世界建模FWM（P_θ(Y|X,Z)）与逆动力学建模IDM（Q_φ(Z|X,Y)），实现仅从状态序列中学习的自改进机制。SWIRL迭代两个阶段：（1）变分信息最大化：更新FWM以生成能最大化隐动作与给定先验状态间条件互信息的下一状态，增强可识别一致性；（2）ELBO最大化：更新IDM以解释观测到的状态转移，实现有效的坐标上升优化。两个模型均采用强化学习（具体为GRPO）训练，以另一冻结模型的对数概率作为奖励信号。我们为两个更新阶段提供了理论可学习性保证，并在多环境中评估SWIRL：单轮/多轮开放世界视觉动态环境，以及面向物理、网络和工具调用的合成文本环境。SWIRL在AURORABench上提升16%，ByteMorph提升28%，WorldPredictionBench提升16%，StableToolBench提升14%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SWIRL, a self-improving framework designed to learn world models from state-only sequences without costly action-labeled data, motivated by the need for efficient internal modelling in LLMs and VLMs. The method treats actions as latent variables and alternates between Forward World Modelling (FWM) and Inverse Dynamics Modelling (IDM), using variational information maximization and ELBO maximization in a coordinate ascent approach, with both models trained via reinforcement learning using log-probability rewards. Experimental results show that SWIRL achieves significant performance gains, including 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench, across various visual and textual environments.</div>
<div class="mono" style="margin-top:8px">本文提出了SWIRL，一种自改进框架，旨在无需昂贵动作标注数据的情况下从仅状态序列中学习世界模型，其动机在于为LLMs和VLMs实现高效内部建模的需求。该方法将动作视为潜变量，在正向世界建模和逆向动力学建模之间交替，采用变分信息最大化和ELBO最大化的坐标上升策略，并通过强化学习以对数概率为奖励训练两个模型。实验结果表明，SWIRL在多个视觉和文本环境中取得了显著性能提升，包括在AURORABench上提升16%、ByteMorph上提升28%、WorldPredictionBench上提升16%以及StableToolBench上提升14%。</div>
</details>
</div>
<div class="card">
<div class="title">Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning</div>
<div class="meta-line">Authors: Chaeeun Lee, T. Michael Yates, Pasquale Minervini, T. Ian Simpson</div>
<div class="meta-line">First: 2026-02-15T14:21:21+00:00 · Latest: 2026-02-15T14:21:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14160v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14160v1">PDF</a> · <a href="https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向可靠临床推理的过程监督多智能体强化学习</div>
<div class="mono" style="margin-top:8px">临床决策需对异质证据进行细致推理并提供可追溯的论证。尽管近期基于大语言模型的多智能体系统展现出潜力，但其主要优化结果准确性，忽视了符合临床标准的过程化推理。基因-疾病有效性评估是这一问题的关键现实案例，专家需综合多元生物医学证据以判断基因是否与疾病存在因果关联。为此，我们提出一种工具化智能体强化学习框架，其具备双重目标：(i) 通过过程级监督确保推理遵循有效临床路径；(ii) 借助分层多智能体系统实现高效协同。在ClinGen数据集上的评估表明：仅使用结果奖励时，采用GRPO训练的Qwen3-4B监督智能体的多智能体系统将最终结果准确率从基础模型监督的0.195大幅提升至0.732，但过程对齐性较差（F1值0.392）；而结合过程与结果奖励后，采用GRPO训练监督智能体的系统在将结果准确率提升至0.750的同时，显著改善过程保真度至F1值0.520。代码已开源：https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for clinical reasoning systems that are not only accurate but also follow valid, traceable processes, focusing on the task of gene-disease validity curation where existing multi-agent systems often prioritize outcome over procedural correctness. The authors propose a process-supervised multi-agent reinforcement learning framework that employs a hierarchical agent structure and uses Group Relative Policy Optimization (GRPO) to train a supervisor agent, applying rewards for both the final outcome and the reasoning process alignment. Experimental results on the ClinGen dataset demonstrate that using combined process and outcome rewards leads to superior performance, achieving an outcome accuracy of 0.750 and a process alignment F1 score of 0.520, compared to outcome-only rewards which yield higher accuracy but significantly lower process fidelity.</div>
<div class="mono" style="margin-top:8px">本文针对临床推理系统不仅需要结果准确，还需遵循有效、可追溯流程的需求，聚焦于基因-疾病有效性评估任务，现有多智能体系统常重结果而轻过程正确性。作者提出了一种过程监督的多智能体强化学习框架，采用分层智能体结构，并利用组相对策略优化（GRPO）训练监督智能体，对最终结果和推理过程对齐同时给予奖励。在ClinGen数据集上的实验结果表明，结合过程和结果奖励能获得更优性能，实现了0.750的结果准确率和0.520的过程对齐F1分数，而仅使用结果奖励虽能提高准确率，但过程保真度显著较低。</div>
</details>
</div>
<div class="card">
<div class="title">Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion</div>
<div class="meta-line">Authors: Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">First: 2026-01-06T17:52:02+00:00 · Latest: 2026-02-15T14:19:30+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03213v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.03213v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像扩散中的批评者引导强化学习遗忘</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型的机器遗忘旨在移除特定概念，同时保持整体实用性。现有扩散遗忘方法通常依赖监督权重编辑或全局惩罚；强化学习方法虽灵活，但常优化稀疏轨迹末端奖励，导致高方差更新和弱信用分配。我们提出一个通用的强化学习框架用于扩散遗忘，将去噪视为序列决策过程，并引入带噪声步奖励的时间步感知批评者。具体而言，我们在噪声潜在空间上训练基于CLIP的奖励预测器，利用其每步信号计算优势估计，以更新反向扩散核的策略梯度。该算法易于实现，支持离策略重用，并可集成至标准文本到图像主干网络。在多个概念上，本方法达到优于或可比基线模型的遗忘效果，同时保持图像质量和良性提示保真度；消融实验表明：(i) 每步批评者与(ii) 噪声条件奖励是稳定性和有效性的关键。我们公开代码和评估脚本，以促进基于强化学习的扩散遗忘研究的可复现性和未来探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of machine unlearning in text-to-image diffusion models, aiming to remove specific concepts without degrading overall model utility. The authors propose a reinforcement learning framework that treats the denoising process as a sequential decision problem, introducing a timestep-aware critic with per-step rewards conditioned on noisy latents to improve credit assignment and reduce variance. Experimental results demonstrate that this method achieves effective concept forgetting comparable to or better than existing baselines while preserving image quality and fidelity to benign prompts, with ablations confirming the importance of per-step critics and noisy-conditioned rewards for stable performance.</div>
<div class="mono" style="margin-top:8px">本文针对文本到图像扩散模型中的机器遗忘问题，旨在移除特定概念同时保持模型整体性能。研究者提出一个强化学习框架，将去噪过程视为序列决策问题，引入基于时间步的评论家模型，利用噪声潜在空间中的每步奖励来改进信用分配并降低方差。实验结果表明，该方法在多个概念上实现了与现有基线相当或更好的遗忘效果，同时保持了图像质量和对良性提示的保真度，消融研究证实了每步评论家和噪声条件奖励对稳定性和有效性的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Optimization and Regularization Under Arbitrary Objectives</div>
<div class="meta-line">Authors: Jared N. Lakhani, Etienne Pienaar</div>
<div class="meta-line">First: 2025-11-24T19:03:43+00:00 · Latest: 2026-02-15T13:56:11+00:00</div>
<div class="meta-line">Comments: 74 pages, 29 figures, 16 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.19628v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.19628v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>任意目标函数下的优化与正则化</div>
<div class="mono" style="margin-top:8px">本研究探讨了将马尔可夫链蒙特卡洛（MCMC）方法应用于任意目标函数的局限性，重点关注一种交替使用Metropolis-Hastings与Gibbs采样的双块MCMC框架。尽管此类方法常被认为能实现数据驱动的正则化而具有优势，但我们证明其性能关键取决于所采用似然形式的锐度。通过引入锐度参数并探索与目标函数成比例的替代似然形式，我们揭示了似然曲率如何同时控制样本内性能及训练数据推断的正则化程度。实证研究在强化学习任务中展开：包括导航问题与井字棋游戏。研究最后通过独立分析，探讨了源自经典二十一点游戏的任意目标函数在极端似然锐度下的影响，其中双块MCMC框架的第一块被迭代优化步骤替代。所得混合方法的性能与原MCMC框架几乎一致，表明过高的似然锐度会有效将后验质量坍缩至单一主导模态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the limitations of using two-block MCMC methods, which combine Metropolis-Hastings and Gibbs sampling, for optimizing arbitrary objective functions, motivated by the need to understand when such data-driven regularization approaches are effective. The method introduces a sharpness parameter to analyze how the curvature of alternative likelihood formulations, proportional to the target objective, influences performance. Experimental results on reinforcement learning tasks, including navigation and tic-tac-toe, show that performance and regularization depend critically on likelihood sharpness; a separate analysis on blackjack, using a hybrid approach with an optimization step, reveals that extreme sharpness collapses the posterior to a single mode, matching the original MCMC&#x27;s performance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了将结合Metropolis-Hastings与Gibbs采样的双块MCMC方法用于优化任意目标函数的局限性，动机在于理解此类数据驱动正则化方法何时有效。方法引入锐度参数，分析与目标成比例的替代似然形式的曲率如何影响性能。在强化学习任务（包括导航和井字棋）上的实验结果表明，性能和正则化关键取决于似然锐度；在二十一点游戏上的单独分析采用带优化步骤的混合方法，显示极端锐度会使后验坍缩到单一主导模式，性能与原MCMC框架几乎相同。</div>
</details>
</div>
<div class="card">
<div class="title">Fast Graph Generation via Autoregressive Noisy Filtration Modeling</div>
<div class="meta-line">Authors: Markus Krimmel, Jenna Wiens, Karsten Borgwardt, Dexiong Chen</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, 2026</div>
<div class="meta-line">First: 2025-02-04T15:35:25+00:00 · Latest: 2026-02-15T12:19:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.02415v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.02415v2">PDF</a> · <a href="https://github.com/BorgwardtLab/anfm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing graph generative models often face a critical trade-off between sample quality and generation speed. We introduce Autoregressive Noisy Filtration Modeling (ANFM), a flexible autoregressive framework that addresses both challenges. ANFM leverages filtration, a concept from topological data analysis, to transform graphs into short sequences of subgraphs. We identify exposure bias as a potential hurdle in autoregressive graph generation and propose noise augmentation and reinforcement learning as effective mitigation strategies, which allow ANFM to learn both edge addition and deletion operations. This unique capability enables ANFM to correct errors during generation by modeling non-monotonic graph sequences. Our results show that ANFM matches state-of-the-art diffusion models in quality while offering over 100 times faster inference, making it a promising approach for high-throughput graph generation. The source code is publicly available at https://github.com/BorgwardtLab/anfm .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自回归噪声过滤建模的快速图生成</div>
<div class="mono" style="margin-top:8px">现有图生成模型常在样本质量与生成速度间面临关键权衡。本文提出自回归噪声过滤建模（ANFM），一种灵活的自回归框架，可同时应对这两项挑战。ANFM利用拓扑数据分析中的过滤概念，将图转化为短序列的子图。我们指出自回归图生成中曝光偏差的潜在障碍，并提出噪声增强与强化学习作为有效缓解策略，使ANFM能同时学习边的添加与删除操作。这一独特能力通过建模非单调图序列，使ANFM能在生成过程中修正错误。实验表明，ANFM在质量上媲美当前最优扩散模型，同时推理速度提升超百倍，为高通量图生成提供了可行方案。源代码已公开于https://github.com/BorgwardtLab/anfm。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the trade-off between quality and speed in existing graph generative models, this paper introduces Autoregressive Noisy Filtration Modeling (ANFM), a framework that transforms graphs into short sequences of subgraphs using filtration from topological data analysis. The method addresses exposure bias through noise augmentation and reinforcement learning, enabling it to model both edge addition and deletion for non-monotonic sequences. Experimental results demonstrate that ANFM achieves state-of-the-art quality comparable to diffusion models while providing over 100 times faster inference, making it highly efficient for high-throughput graph generation.</div>
<div class="mono" style="margin-top:8px">针对现有图生成模型在生成质量与速度之间的权衡问题，本文提出了自回归噪声过滤建模（ANFM），该框架利用拓扑数据分析中的过滤概念将图转换为短子图序列。该方法通过噪声增强和强化学习解决自回归中的暴露偏差问题，从而能够建模边的添加和删除操作以处理非单调序列。实验结果表明，ANFM在质量上达到了与扩散模型相当的最先进水平，同时推理速度提升超过100倍，为高通量图生成提供了一种高效方法。</div>
</details>
</div>
<div class="card">
<div class="title">ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving</div>
<div class="meta-line">Authors: Sejin Kim, Hayan Choi, Seokki Lee, Sundong Kim</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-11-14T08:52:53+00:00 · Latest: 2026-02-15T10:35:59+00:00</div>
<div class="meta-line">Comments: KDD 2026 (Datasets and Benchmarks) accepted</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11079v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.11079v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input-output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARCTraj：面向抽象问题求解的人类推理轨迹数据集与基准</div>
<div class="mono" style="margin-top:8px">本文提出ARCTraj——一个用于通过抽象与推理语料库（ARC）中的复杂视觉任务建模人类推理的数据集与方法框架。尽管ARC已激发大量关于抽象推理的研究，现有方法多依赖静态输入输出监督，难以揭示推理随时间展开的过程。ARCTraj通过记录时序有序的对象级操作填补这一空白，这些操作捕捉了人类如何迭代地将输入转化为输出，揭示了传统数据集忽略的中间推理步骤。该数据集通过O2ARC网页界面采集，包含约10,000条轨迹，涵盖ARC-AGI-1基准中400个训练任务，每条轨迹均标注任务标识、时间戳和成功标签。研究进一步构建了统一推理流程，涵盖数据采集、操作抽象、马尔可夫决策过程（MDP）建模及下游学习，支持与强化学习（如PPO）、生成建模（如世界模型、GFlowNets、扩散智能体）及序列建模方法（如决策变换器）的集成。对空间选择、色彩归因和策略收敛的分析揭示了人类推理的结构性与多样性。这些工作共同使ARCTraj成为研究类人推理的可解释结构化基础，推动可解释性、对齐性与泛化智能的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static input-output supervision in the Abstraction and Reasoning Corpus (ARC), which obscures the step-by-step process of human reasoning, this paper introduces ARCTraj, a dataset and framework for modeling human reasoning trajectories. The method involves collecting temporally ordered, object-level action sequences via the O2ARC interface, encompassing around 10,000 trajectories from 400 ARC training tasks, and defines a unified pipeline including action abstraction and Markov decision process formulation to integrate with various learning approaches like reinforcement learning and sequence modeling. Experimental analyses reveal insights into human reasoning patterns, such as spatial selection and strategic convergence, establishing ARCTraj as a foundation for advancing explainable and generalizable intelligence.</div>
<div class="mono" style="margin-top:8px">针对抽象与推理语料库（ARC）中静态输入输出监督方法难以揭示人类逐步推理过程的局限性，本文提出了ARCTraj数据集和框架，旨在建模人类在复杂视觉任务中的推理轨迹。该方法通过O2ARC界面收集了约10,000条来自400个ARC训练任务的时序有序、对象级动作序列，并构建了一个统一流程，包括动作抽象和马尔可夫决策过程建模，以兼容强化学习、序列建模等多种学习方法。实验分析揭示了人类在空间选择、策略收敛等方面的推理模式，使ARCTraj成为推动可解释和通用人工智能发展的结构化基础。</div>
</details>
</div>
<div class="card">
<div class="title">ESPO: Entropy Importance Sampling Policy Optimization</div>
<div class="meta-line">Authors: Yuepeng Sheng, Yuwei Huang, Shuman Liu, Anxiang Zeng, Haibo Zhang</div>
<div class="meta-line">First: 2025-11-29T14:09:38+00:00 · Latest: 2026-02-15T10:13:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00499v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00499v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a central component of post-training for large language models (LLMs), particularly for complex reasoning tasks that require stable optimization over long generation horizons. However, achieving performance at scale often introduces a fundamental trade-off between training stability and training efficiency. Token-level optimization applies fine-grained updates at the individual units, but is prone to high variance in gradient estimation, which can result in unstable training dynamics. In contrast, Sequence-level optimization often relies on aggressive clipping mechanisms to ensure stable updates. However, such design may discard a large fraction of valid training samples, leading to inefficient gradient utilization and reduced training efficiency. We refer to this phenomenon as gradient underutilization. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that aims to combine fine-grained updates with stable training. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy Grouping Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy Adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging mathematical benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESPO：基于熵重要性采样的策略优化方法</div>
<div class="mono" style="margin-top:8px">强化学习已成为大语言模型后训练的核心组成部分，尤其适用于需要长序列生成稳定优化的复杂推理任务。然而，规模化性能提升常面临训练稳定性与训练效率之间的根本性权衡：词元级优化虽能实现细粒度更新，但梯度估计方差较高易导致训练不稳定；序列级优化常依赖激进截断机制确保稳定更新，却可能丢弃大量有效训练样本，造成梯度利用不足与训练效率下降。本研究提出基于熵重要性采样的策略优化框架，通过预测熵将序列分解为不同组别，实现（1）熵分组重要性采样以捕捉序列内异质性，（2）熵自适应截断以基于模型不确定性动态分配置信区域。数学推理基准测试表明，该框架不仅能加速收敛，更在具有挑战性的数学基准上取得最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the trade-off between training stability and efficiency in reinforcement learning for large language models, where token-level optimization suffers from high variance and sequence-level methods underutilize gradients due to aggressive clipping. The authors propose Entropy Importance Sampling Policy Optimization (ESPO), which groups sequence tokens by predictive entropy to enable entropy-based importance sampling for capturing intra-sequence heterogeneity and adaptive clipping that dynamically adjusts trust regions based on model uncertainty. Experimental results on mathematical reasoning benchmarks show that ESPO accelerates convergence and achieves state-of-the-art accuracy, effectively improving performance on challenging tasks.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习中训练稳定性与效率之间的权衡问题展开研究，其中词元级优化存在高方差问题，而序列级方法因激进裁剪导致梯度利用不足。作者提出了熵重要性采样策略优化方法，通过基于预测熵对序列词元进行分组，实现了熵分组重要性采样以捕捉序列内异质性，以及基于模型不确定性的熵自适应裁剪来动态分配信任区域。在数学推理基准测试上的实验结果表明，该方法不仅加速了收敛，还取得了最先进的准确率，显著提升了在复杂数学任务上的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Gradient with Adaptive Entropy Annealing for Continual Fine-Tuning</div>
<div class="meta-line">Authors: Yaqian Zhang, Bernhard Pfahringer, Eibe Frank, Albert Bifet</div>
<div class="meta-line">First: 2026-02-15T10:05:03+00:00 · Latest: 2026-02-15T10:05:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14078v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their success, large pretrained vision models remain vulnerable to catastrophic forgetting when adapted to new tasks in class-incremental settings. Parameter-efficient fine-tuning (PEFT) alleviates this by restricting trainable parameters, yet most approaches still rely on cross-entropy (CE) loss, a surrogate for the 0-1 loss, to learn from new data. We revisit this choice and revive the true objective (0-1 loss) through a reinforcement learning perspective. By formulating classification as a one-step Markov Decision Process, we derive an Expected Policy Gradient (EPG) method that directly minimizes misclassification error with a low-variance gradient estimation. Our analysis shows that CE can be interpreted as EPG with an additional sample-weighting mechanism: CE encourages exploration by emphasizing low-confidence samples, while EPG prioritizes high-confidence ones. Building on this insight, we propose adaptive entropy annealing (aEPG), a training strategy that transitions from exploratory (CE-like) to exploitative (EPG-like) learning. aEPG-based methods outperform CE-based methods across diverse benchmarks and with various PEFT modules. More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应熵退火的持续微调策略梯度方法</div>
<div class="mono" style="margin-top:8px">尽管大型预训练视觉模型已取得显著成功，但在类别增量场景中适应新任务时仍易受灾难性遗忘影响。参数高效微调通过限制可训练参数缓解此问题，但多数方法仍依赖交叉熵损失（0-1损失的替代目标）从新数据中学习。本研究重新审视这一选择，通过强化学习视角恢复真实目标（0-1损失）。通过将分类建模为单步马尔可夫决策过程，我们推导出可直接最小化误分类误差的低方差梯度估计方法——期望策略梯度。分析表明，交叉熵可解释为附加样本加权机制的EPG：交叉熵通过强调低置信度样本鼓励探索，而EPG优先处理高置信度样本。基于此洞见，我们提出自适应熵退火训练策略，实现从探索性（类交叉熵）到利用性（类EPG）学习的过渡。基于aEPG的方法在多种基准测试和不同PEFT模块中均优于基于交叉熵的方法。更广泛地，我们评估了多种熵正则化方法，证明输出预测分布的低熵特性可增强预训练视觉模型的适应能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses catastrophic forgetting in continual fine-tuning of large pretrained vision models by moving beyond the standard cross-entropy loss. The authors adopt a reinforcement learning perspective, formulating classification as a one-step Markov Decision Process to derive an Expected Policy Gradient (EPG) method that directly minimizes the 0-1 misclassification error. Their key innovation is adaptive entropy annealing (aEPG), a strategy that transitions training from an exploratory, CE-like phase to an exploitative, EPG-like phase. Experimental results show that aEPG-based methods outperform CE-based approaches across multiple benchmarks with various parameter-efficient fine-tuning modules, and they further demonstrate that lower output entropy enhances model adaptation.</div>
<div class="mono" style="margin-top:8px">本文针对大型预训练视觉模型在持续微调中的灾难性遗忘问题，提出了超越标准交叉熵损失的方法。作者从强化学习视角出发，将分类建模为一步马尔可夫决策过程，推导出能直接最小化0-1误分类误差的期望策略梯度方法。其核心创新是自适应熵退火策略，该策略使训练从探索性的类交叉熵阶段过渡到利用性的类期望策略梯度阶段。实验结果表明，基于自适应熵退火的方法在多种参数高效微调模块和基准测试中均优于基于交叉熵的方法，并进一步证明较低的输出熵能增强预训练视觉模型的适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling</div>
<div class="meta-line">Authors: Yang Liu, Jiaqi Li, Zilong Zheng</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-10T10:31:21+00:00 · Latest: 2026-02-15T09:10:49+00:00</div>
<div class="meta-line">Comments: ICLR 2026 camera ready, 28 pages, 10 figures, 15 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08672v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08672v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rule-based reasoning is acknowledged as one of the fundamental problems of reasoning. While recent studies show that large reasoning models (LRMs) have remarkable reasoning capabilities enhanced by reinforcement learning (RL), real applications still face severe challenges due to variations in rule formats, types, and complexity. To mitigate this issue, we introduce RuleReasoner, an effective method for rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach in RL. Specifically, RuleReasoner resamples each training batch by updating the domain weights based on historical rewards. This facilitates domain balance and active learning schedules for RL, obviating static mix-training engineered by human. Evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($Δ$4.1% on eight ID tasks and $Δ$10.4% on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RuleReasoner：通过领域感知动态采样实现基于规则的强化推理</div>
<div class="mono" style="margin-top:8px">基于规则的推理被公认为推理的基本问题之一。尽管近期研究表明，大型推理模型（LRMs）通过强化学习（RL）显著增强了推理能力，但由于规则格式、类型和复杂性的差异，实际应用仍面临严峻挑战。为缓解此问题，我们提出了RuleReasoner，这是一种通过精心策划的任务集合和RL中新颖的领域感知动态采样方法实现高效规则推理的方法。具体而言，RuleReasoner通过基于历史奖励更新领域权重来重新采样每个训练批次，从而促进RL的领域平衡和主动学习调度，避免了人工设计的静态混合训练。在分布内（ID）和分布外（OOD）基准测试中的评估显示，RuleReasoner显著优于前沿LRMs（在八个ID任务上Δ4.1%，在三个OOD任务上较OpenAI-o1提升Δ10.4%）。值得注意的是，与先前方法相比，我们的方法还展现出更高的计算效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying large reasoning models to rule-based reasoning tasks, where performance is hindered by diverse rule formats and complexities. To overcome this, the authors propose RuleReasoner, a method that uses a curated set of tasks and a novel domain-aware dynamic sampling technique in reinforcement learning, which dynamically adjusts domain weights based on historical rewards to balance training and enhance learning efficiency. Experimental results demonstrate that RuleReasoner significantly outperforms leading large reasoning models, achieving improvements of 4.1% on in-distribution tasks and 10.4% on out-of-distribution tasks, while also being more computationally efficient.</div>
<div class="mono" style="margin-top:8px">本文针对基于规则的推理任务中，由于规则格式、类型和复杂性多样导致大型推理模型性能受限的问题，提出了一种解决方案。作者引入了RuleReasoner方法，该方法通过精心策划的任务集和一种新颖的领域感知动态采样技术，在强化学习中根据历史奖励动态调整领域权重，以平衡训练并提升学习效率。实验结果表明，RuleReasoner在分布内任务上性能提升4.1%，在分布外任务上提升10.4%，显著优于前沿大型推理模型，同时具有更高的计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL</div>
<div class="meta-line">Authors: Zhennan Jiang, Shangqing Zhou, Yutong Jiang, Zefang Huang, Mingjie Wei, Yuhui Chen, Tianxing Zhou, Zhen Guo, Hao Lin, Quanlu Zhang, Yu Wang, Haoran Li, Chao Yu, Dongbin Zhao</div>
<div class="meta-line">First: 2026-02-15T03:48:20+00:00 · Latest: 2026-02-15T03:48:20+00:00</div>
<div class="meta-line">Comments: 21pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13977v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WoVR：作为后训练VLA策略强化学习可靠模拟器的世界模型</div>
<div class="mono" style="margin-top:8px">强化学习（RL）有望为视觉-语言-动作（VLA）模型解锁超越模仿学习的能力，但其对海量真实世界交互的需求阻碍了在物理机器人上的直接部署。近期研究尝试使用学习得到的世界模型作为策略优化的模拟器，但闭环想象推演不可避免地受到幻觉和长时程误差累积的影响。此类误差不仅降低视觉保真度，还会破坏优化信号，促使策略利用模型的不准确性而非真正的任务进展。我们提出WoVR，一种用于后训练VLA策略的可靠基于世界模型的强化学习框架。WoVR不假设完美的世界模型，而是明确规范RL如何与不完美的想象动态交互：通过可控的动作条件视频世界模型提升推演稳定性，利用关键帧初始化推演重塑想象交互以降低有效误差深度，并通过世界模型-策略协同进化保持策略与模拟器的一致性。在LIBERO基准测试和真实机器人操作上的大量实验表明，WoVR实现了稳定的长时程想象推演和有效的策略优化，将LIBERO平均成功率从39.95%提升至69.2%（+29.3个百分点），真实机器人成功率从61.7%提升至91.7%（+30.0个百分点）。这些结果证明，当幻觉被明确控制时，学习得到的世界模型可作为强化学习的实用模拟器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable effective reinforcement learning (RL) for Vision-Language-Action (VLA) policies on physical robots by overcoming the prohibitive cost of real-world interaction and the unreliability of prior world model simulators, which suffer from hallucination and error accumulation that corrupt the RL optimization signal. The method, WoVR, is a framework that explicitly regulates RL interaction with an imperfect world model by using a controllable action-conditioned video model, reducing error depth via Keyframe-Initialized Rollouts, and maintaining alignment through World Model-Policy co-evolution. The main experimental results show that WoVR significantly improves policy performance, increasing the average success rate on the LIBERO benchmark from 39.95% to 69.2% and on real-robot manipulation from 61.7% to 91.7%, demonstrating that learned world models can serve as practical simulators when their imperfections are controlled.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过克服真实世界交互的高成本以及先前世界模型模拟器存在的幻觉和误差累积问题，来为视觉-语言-动作策略在物理机器人上实现有效的强化学习，因为这些缺陷会破坏强化学习的优化信号。所提出的方法WoVR是一个框架，它通过使用可控的动作条件视频模型、通过关键帧初始化滚动来减少误差深度，并通过世界模型与策略协同进化来保持对齐，从而显式地调节强化学习与不完美世界模型的交互。主要实验结果表明，WoVR显著提升了策略性能，将LIBERO基准测试的平均成功率从39.95%提高到69.2%，并将真实机器人操作的成功率从61.7%提升至91.7%，证明当模型缺陷得到控制时，学习得到的世界模型可以作为实用的模拟器。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction</div>
<div class="meta-line">Authors: Yusong Wu, Stephen Brade, Aleksandra Teng Ma, Tia-Jane Fowler, Enning Yang, Berker Banar, Aaron Courville, Natasha Jaques, Cheng-Zhi Anna Huang</div>
<div class="meta-line">First: 2025-11-22T02:12:41+00:00 · Latest: 2026-02-15T02:28:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.17879v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.17879v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player&#x27;s future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking&#x27;&#x27;, affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成对抗式后训练缓解实时人机音乐交互中的奖励破解问题</div>
<div class="mono" style="margin-top:8px">大多数生成式AI应用采用顺序交互模式，即用户输入提示后等待响应，反应时间和适应性并非关键因素。相比之下，实时即兴演奏是一种需要实时协调与适应的协作式交互，参与者无法预知对方后续动作，同时需保持多样性以维持创作流。强化学习后训练通过同策略交互实现有效适应，但常因利用基于连贯性的奖励而降低输出多样性。这种被称为“奖励破解”的崩溃现象影响众多RL后训练流程，在实时即兴演奏中尤为有害，因为音乐创作依赖动态变化与相互响应。本文提出一种基于策略生成轨迹的新型对抗训练方法，以缓解旋律到和弦伴奏的RL后训练中的奖励破解问题。协同进化的判别器将策略轨迹与数据分布分离，策略在最大化判别器输出的同时兼顾连贯性奖励，从而避免坍缩至平凡输出。我们通过模拟实验（使用固定测试旋律与学习型旋律智能体）评估伴奏质量与输出多样性，并部署实时交互系统与专业音乐家开展用户研究。定量评估与用户反馈表明，该方法在输出多样性、和声连贯性、适应速度及用户能动性方面均有提升。本研究为缓解生成序列模型RL后训练的奖励破解问题提供了一种简洁有效的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward hacking in reinforcement learning (RL) post-training for generative AI, where models exploit coherence rewards at the cost of output diversity, which is particularly detrimental in live, creative interactions like musical jamming. The authors propose an adversarial training method that uses a co-evolving discriminator to distinguish policy-generated trajectories from the original data distribution, encouraging the policy to maximize both discriminator output and coherence rewards to avoid collapse. Experimental results from simulation with fixed and learned melody agents, along with a user study involving expert musicians in a real-time system, show that the method improves output diversity, harmonic coherence, adaptation speed, and user agency compared to standard RL post-training approaches.</div>
<div class="mono" style="margin-top:8px">本文针对生成式人工智能在强化学习后训练中出现的奖励破解问题展开研究，该问题导致模型为追求连贯性奖励而牺牲输出多样性，这在现场音乐即兴等创造性互动中尤为有害。作者提出了一种对抗性训练方法，通过一个协同演化的判别器来区分策略生成的轨迹与原始数据分布，促使策略同时最大化判别器输出和连贯性奖励，从而避免输出崩溃。在模拟环境中使用固定和学习的旋律代理进行的实验，以及包含专家音乐家的实时系统用户研究表明，该方法相较于标准强化学习后训练，在输出多样性、和声连贯性、适应速度和用户自主性方面均有提升。</div>
</details>
</div>
<div class="card">
<div class="title">MURPHY: Multi-Turn GRPO for Self Correcting Code Generation</div>
<div class="meta-line">Authors: Chanakya Ekbote, Vijay Lingam, Sujay Sanghavi, Jun Huan, Behrooz Omidvar-Tehrani, Anoop Deoras, Stefano Soatto</div>
<div class="meta-line">First: 2025-11-11T05:03:22+00:00 · Latest: 2026-02-15T02:11:41+00:00</div>
<div class="meta-line">Comments: 21 pages, 2 figures, 8 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07833v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.07833v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards(RLVR) has emerged as a powerful framework for enhancing the reasoning capabilities of large language models (LLMs). However, existing approaches such as Group Relative Policy Optimization (GRPO) and its variants, while effective on reasoning benchmarks, struggle with agentic tasks that require iterative decision-making. We introduce MURPHY, a multi-turn RLVR framework that incorporates execution feedback directly into training, extending GRPO to optimize over multi-turn trajectories where models iteratively refine solutions. MURPHY combines a feedback conditioned rollout tree with trajectory-level credit assignment, and uses pruning to reduce the cost of multi-turn optimization. Evaluations on code generation benchmarks with two model families show that MURPHY consistently improves multi-iteration performance, achieving up to an 8% absolute gain in pass@1 over compute-matched GRPO baselines, and outperforming the prior leading method that incorporates multi-turn execution feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MURPHY：用于自校正代码生成的多轮GRPO框架</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为增强大语言模型推理能力的有效框架。然而，现有方法如组相对策略优化及其变体虽然在推理基准测试中表现良好，但在需要迭代决策的智能体任务上存在局限。本文提出MURPHY——一种将执行反馈直接融入训练的多轮RLVR框架，通过扩展GRPO来优化模型迭代优化解决方案的多轮轨迹。MURPHY结合了反馈条件化的展开树与轨迹级信用分配机制，并采用剪枝技术降低多轮优化成本。在两个模型系列的代码生成基准测试中，MURPHY持续提升多轮迭代性能，在pass@1指标上较计算量匹配的GRPO基线实现最高8%的绝对增益，且优于先前融合多轮执行反馈的领先方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for MURPHY arises from the limitations of existing Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO, which are effective for reasoning but struggle with iterative, agentic tasks such as code generation requiring self-correction. The method introduces a multi-turn RLVR framework that extends GRPO by incorporating execution feedback directly into training, using a feedback-conditioned rollout tree with trajectory-level credit assignment and pruning to optimize over multi-turn trajectories efficiently. Main experimental results on code generation benchmarks with two model families show that MURPHY consistently improves multi-iteration performance, achieving up to an 8% absolute gain in pass@1 over compute-matched GRPO baselines and outperforming prior leading methods that use multi-turn execution feedback.</div>
<div class="mono" style="margin-top:8px">MURPHY的提出动机源于现有可验证奖励强化学习（RLVR）方法（如GRPO）的局限性，这些方法在推理任务上有效，但在需要迭代决策的智能体任务（如需要自我纠正的代码生成）中表现不佳。该方法引入了一个多轮次RLVR框架，通过将执行反馈直接纳入训练来扩展GRPO，采用反馈条件化的展开树和轨迹级信用分配，并结合剪枝技术来高效优化多轮次轨迹。在代码生成基准测试中使用两个模型系列的主要实验结果表明，MURPHY持续提升了多轮迭代性能，在计算匹配的GRPO基线上实现了高达8%的pass@1绝对增益，并优于先前结合多轮执行反馈的领先方法。</div>
</details>
</div>
<div class="card">
<div class="title">QuRL: Efficient Reinforcement Learning with Quantized Rollout</div>
<div class="meta-line">Authors: Yuhang Li, Reena Elangovan, Xin Dong, Priyadarshini Panda, Brucek Khailany</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-15T01:48:10+00:00 · Latest: 2026-02-15T01:48:10+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13953v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QuRL：基于量化推演的高效强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习已成为训练推理大语言模型的主流范式。然而，由于大语言模型的自回归解码特性，推演过程成为强化学习训练的效率瓶颈，最高可占总训练时间的70%。本研究提出量化强化学习方法，通过量化执行器加速推演过程。我们解决了QuRL中的两大挑战：首先提出自适应截断范围技术，根据全精度执行器与量化执行器间的策略比率动态调整截断比例，这对避免长期训练崩溃至关重要；其次针对权重更新问题——强化学习步骤间的权重变化极小导致量化操作难以有效捕捉，我们通过不变缩放技术降低量化噪声并增强权重更新。在DeepScaleR和DAPO数据集上进行的INT8与FP8量化实验表明，该方法可实现训练期间推演速度提升20%至80%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the efficiency bottleneck in reinforcement learning with verifiable rewards (RLVR) for large language models, where the autoregressive rollout process consumes up to 70% of training time. To accelerate this, the authors propose Quantized Reinforcement Learning (QuRL), which employs a quantized actor for faster rollouts, tackling challenges through an Adaptive Clipping Range to prevent training collapse by dynamically adjusting clipping based on policy ratios, and an invariant scaling technique to mitigate quantization noise and enhance weight updates. Experimental results on DeepScaleR and DAPO with INT8 and FP8 quantization show that QuRL achieves 20% to 80% faster rollout during training.</div>
<div class="mono" style="margin-top:8px">本文针对可验证奖励强化学习在大型语言模型训练中的效率瓶颈问题，其中自回归展开过程占用高达70%的训练时间。为加速此过程，作者提出量化强化学习方法，使用量化执行器进行快速展开，通过自适应裁剪范围根据全精度与量化执行器间的策略比率动态调整裁剪以防止训练崩溃，并采用不变缩放技术减少量化噪声并增强权重更新。在DeepScaleR和DAPO上进行的INT8和FP8量化实验表明，该方法在训练期间实现了20%至80%的展开速度提升。</div>
</details>
</div>
<div class="card">
<div class="title">Experiential Reinforcement Learning</div>
<div class="meta-line">Authors: Taiwei Shi, Sihao Chen, Bowen Jiang, Linxin Song, Longqi Yang, Jieyu Zhao</div>
<div class="meta-line">First: 2026-02-15T01:23:48+00:00 · Latest: 2026-02-15T01:23:48+00:00</div>
<div class="meta-line">Comments: 26 pages, 9 tables, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13949v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13949v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>体验式强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为语言模型从环境奖励或反馈中学习的核心方法。实践中，环境反馈通常稀疏且延迟。从这类信号中学习具有挑战性，因为语言模型必须隐式推断观察到的失败应如何转化为未来迭代中的行为调整。我们提出体验式强化学习，这是一种将显式的体验-反思-巩固循环嵌入强化学习过程的训练范式。给定任务后，模型生成初始尝试，接收环境反馈，并产生引导优化二次尝试的反思，其成功经验被强化并内化至基础策略中。该过程将反馈转化为结构化的行为修正，提升探索效率并稳定优化，同时在部署时无需额外推理成本即可保持收益。在稀疏奖励控制环境和智能体推理基准测试中，ERL相较于强基线强化学习方法持续提升学习效率和最终性能，在复杂多步环境中实现高达+81%的增益，在工具使用推理任务中实现高达+11%的增益。这些结果表明，将显式自我反思整合到策略训练中，为将反馈转化为持久行为改进提供了实用机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Experiential Reinforcement Learning (ERL), motivated by the challenge of sparse and delayed environmental feedback in reinforcement learning for language models, which makes it difficult to translate failures into behavioral changes. The method embeds an explicit experience-reflection-consolidation loop where the model generates an initial attempt, receives feedback, produces a reflection to guide a refined second attempt, and reinforces the successful outcome into the base policy, thereby converting feedback into structured revision without extra inference cost. Experimental results show that ERL improves learning efficiency and final performance over strong baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks, demonstrating that explicit self-reflection stabilizes optimization and enhances durable behavioral improvement.</div>
<div class="mono" style="margin-top:8px">本文提出了经验强化学习（ERL），其动机在于强化学习中环境反馈通常稀疏且延迟，使得语言模型难以将观察到的失败转化为行为改变。该方法嵌入了一个显式的经验-反思-巩固循环：模型生成初始尝试、接收环境反馈、产生反思以指导改进的第二次尝试，并将成功结果强化到基础策略中，从而将反馈转化为结构化行为修订且无需额外推理成本。实验结果表明，ERL在稀疏奖励控制环境和智能推理基准测试中，相比强基线方法持续提升了学习效率和最终性能，在复杂多步环境中增益高达+81%，在使用工具推理任务中增益达+11%，表明显式自我反思为将反馈转化为持久行为改进提供了实用机制。</div>
</details>
</div>
<div class="card">
<div class="title">Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Wenlong Deng, Yi Ren, Yushu Li, Boying Gong, Danica J. Sutherland, Xiaoxiao Li, Christos Thrampoulidis</div>
<div class="meta-line">Venue: ICML 2025 best paper</div>
<div class="meta-line">First: 2025-10-04T04:49:44+00:00 · Latest: 2026-02-15T01:21:25+00:00</div>
<div class="meta-line">Comments: Full version of submission to 2nd AI for Math Workshop@ ICML 2025 (best paper)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03669v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.03669v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token&#x27;s influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO&#x27;s learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>令牌隐藏奖励：引导群体相对深度强化学习中的探索-利用权衡</div>
<div class="mono" style="margin-top:8px">具有可验证奖励的强化学习显著提升了大型语言模型的推理能力，但如何显式引导训练偏向探索或利用仍是开放问题。本文提出令牌隐藏奖励（THR），这是一种在群体相对策略优化（GRPO）框架下量化各令牌对正确答案似然影响的令牌级指标。研究发现训练动态由少数具有高绝对值THR的令牌主导：正THR令牌增强正确输出的置信度（偏向利用），负THR令牌为替代输出保留概率空间（促进探索）。基于此，我们提出THR引导的重新加权算法，通过调节GRPO的学习信号显式控制训练偏向。该算法在多样数学推理基准测试中验证有效：放大正THR令牌可提升贪婪解码准确率（偏向利用），反向操作则提高Pass@K指标（促进探索）。算法可无缝集成GSPO等其他RL目标，并适用于Llama等不同架构。THR为RL调优的LLM提供了动态控制探索-利用权衡的细粒度机制，为推理密集型应用的定向微调提供了新工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of explicitly steering exploration versus exploitation in reinforcement learning for large language models, where verifiable rewards have advanced reasoning but lack fine-grained control. It introduces Token Hidden Reward (THR), a token-level metric that quantifies each token&#x27;s influence on correct response likelihood under Group Relative Policy Optimization (GRPO), revealing that training is dominated by tokens with high absolute THR values: positive THR tokens boost confidence in correct outputs (favoring exploitation), while negative ones preserve probability mass for alternatives (enabling exploration). Based on this, the authors propose a THR-guided reweighting algorithm to modulate GRPO&#x27;s learning signals, biasing training toward exploitation or exploration as needed. Experimental results on diverse math reasoning benchmarks show that amplifying positive THR tokens improves greedy-decoding accuracy for exploitation, whereas weakening them enhances Pass@K accuracy for exploration, with the algorithm also integrating seamlessly with other RL objectives like GSPO and generalizing across architectures such as Llama, establishing THR as a principled mechanism for dynamic control in RL-tuned LLMs.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型强化学习中如何显式引导探索与利用的挑战展开研究，尽管可验证奖励已提升推理能力，但缺乏细粒度控制。作者提出了令牌隐藏奖励（THR），这是一种在组相对策略优化（GRPO）下量化每个令牌对正确响应可能性影响的令牌级指标，发现训练动态由高绝对THR值的少数令牌主导：正THR令牌增强正确输出的置信度（偏向利用），而负THR令牌为替代输出保留概率质量（支持探索）。基于此，作者设计了一种THR引导的重加权算法，以调整GRPO的学习信号，根据需要偏向利用或探索。在多样数学推理基准上的实验结果表明，放大正THR令牌可提高贪婪解码准确率以促进利用，而削弱它们则能提升Pass@K准确率以支持探索，该算法还能无缝集成其他RL目标（如GSPO）并泛化至Llama等架构，从而确立了THR作为RL调优LLM中动态控制探索与利用的原则性机制。</div>
</details>
</div>
<div class="card">
<div class="title">You Can Learn Tokenization End-to-End with Reinforcement Learning</div>
<div class="meta-line">Authors: Sam Dauncey, Roger Wattenhofer</div>
<div class="meta-line">First: 2026-02-15T00:31:24+00:00 · Latest: 2026-02-15T00:31:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13940v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization is a hardcoded compression step which remains in the training pipeline of Large Language Models (LLMs), despite a general trend towards architectures becoming increasingly end-to-end. Prior work has shown promising results at scale in bringing this compression step inside the LLMs&#x27; architecture with heuristics to draw token boundaries, and also attempts to learn these token boundaries with straight-through estimates, which treat the problem of drawing discrete token boundaries as a continuous one. We show that these token boundaries can instead be learned using score function estimates, which have tighter theoretical guarantees due to directly optimizing the problem of drawing discrete token boundaries to minimize loss. We observe that techniques from reinforcement learning, such as time discounting, are necessary to reduce the variance of this score function sufficiently to make it practicable. We demonstrate that the resultant method outperforms prior proposed straight-through estimates, both qualitatively and quantitatively at the $100$ million parameter scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过强化学习实现端到端分词学习</div>
<div class="mono" style="margin-top:8px">分词作为硬编码的压缩步骤，尽管大语言模型（LLMs）架构整体呈现端到端化趋势，但其训练流程中仍保留这一环节。先前研究通过启发式方法划定词元边界，将压缩步骤融入LLMs架构并取得规模化成果，亦有尝试使用直通估计学习词元边界——将离散边界划定问题视为连续问题处理。本文提出，可采用具有更严格理论保证的得分函数估计来学习词元边界，该方法通过直接优化离散边界划定以最小化损失。研究发现，需引入强化学习技术（如时间折扣）来充分降低得分函数方差，使其具备实用性。实验表明，该方法在1亿参数规模上，无论定性还是定量评估均优于先前提出的直通估计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the observation that tokenization remains a hardcoded, non-end-to-end component in modern LLM pipelines, unlike other architectural trends. The method proposes learning discrete token boundaries end-to-end using reinforcement learning techniques, specifically score function estimates with time discounting to reduce variance, instead of prior heuristic or straight-through gradient approaches. The main experimental result is that this reinforcement learning-based method outperforms previous straight-through estimation techniques both qualitatively and quantitatively in models at the 100 million parameter scale.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到，与现代大语言模型架构日益端到端的趋势不同，分词仍是一个硬编码的非端到端组件。该方法提出使用强化学习技术，特别是结合时间折扣的得分函数估计，来端到端地学习离散的词元边界，以替代先前基于启发式或直通梯度估计的方法。主要实验结果表明，这种基于强化学习的方法在1亿参数规模的模型上，在定性和定量评估中均优于先前提出的直通估计技术。</div>
</details>
</div>
<div class="card">
<div class="title">Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning</div>
<div class="meta-line">Authors: Zhimin Zhao</div>
<div class="meta-line">First: 2026-02-15T00:14:31+00:00 · Latest: 2026-02-15T00:14:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13934v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13934v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为何是代码，为何是现在：可学习性、可计算性与机器学习的真实局限</div>
<div class="mono" style="margin-top:8px">代码生成比强化学习进展更稳定，主要因为代码具有使其可学习的信息结构。代码在每个标记处提供密集、局部、可验证的反馈，而大多数强化学习问题则不然。这种反馈质量的差异并非二元对立而是渐进的。我们基于信息结构提出五级可学习性层次，并论证机器学习进展的上限更取决于任务是否可学习，而非模型规模。该层次建立在计算问题三个属性（可表达性、可计算性、可学习性）的形式化区分之上。我们确立了它们之间的两两关系，包括哪些蕴含关系成立、哪些失效，并提出使结构差异显式化的统一模板。分析揭示了为何代码的监督学习可预测地扩展而强化学习不能，以及为何&#x27;仅靠扩展就能解决剩余机器学习挑战&#x27;这一普遍假设值得审视。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the observation that code generation advances more reliably than reinforcement learning, attributing this to the superior information structure of code which provides dense, local, and verifiable feedback per token. The method involves proposing a five-level hierarchy of learnability based on information structure, formally distinguishing expressibility, computability, and learnability, and analyzing their relationships through a unified template. The main experimental results are conceptual, showing that the ceiling on machine learning progress depends more on a task&#x27;s inherent learnability than on model scaling, thereby explaining why supervised learning on code scales predictably while reinforcement learning often does not.</div>
<div class="mono" style="margin-top:8px">本文的动机源于观察到代码生成比强化学习进展更可靠，这归因于代码具有优越的信息结构，能为每个标记提供密集、局部且可验证的反馈。方法上，作者提出了一个基于信息结构的五级可学习性层次，形式化区分了可表达性、可计算性和可学习性，并通过统一模板分析了它们之间的关系。主要实验结果属于概念性分析，表明机器学习进展的上限更多取决于任务本身的可学习性而非模型规模，从而解释了为什么代码的监督学习可预测地扩展，而强化学习则往往不能。</div>
</details>
</div>
<div class="card">
<div class="title">From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen</div>
<div class="meta-line">First: 2026-02-14T22:31:49+00:00 · Latest: 2026-02-14T22:31:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13912v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13912v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs&#x27; limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从像素到策略：强化语言模型的空间推理能力以实现内容感知的版面设计</div>
<div class="mono" style="margin-top:8px">我们提出LaySPA——一种强化学习框架，通过显式且可解释的空间推理能力赋能大语言模型，实现内容感知的图形版面设计。LaySPA解决两大核心挑战：大语言模型有限的空间推理能力，以及设计决策过程缺乏透明度。我们摒弃像素级操作，将版面设计重构为结构化文本空间环境中的策略学习问题，该环境显式编码画布几何、元素属性及元素间关系。LaySPA生成包含可解释推理轨迹与结构化版面规范的双层输出，实现透明可控的设计决策。版面设计策略通过多目标空间评估机制进行优化，该机制将版面质量解构为几何有效性、关系连贯性与美学一致性，并采用相对群体优化方法进行训练，以稳定开放设计空间中的学习过程。实验表明，LaySPA在提升结构有效性与视觉质量方面，不仅超越规模更大的专有大语言模型，其性能更可与专业SOTA版面生成器相媲美，同时所需标注样本更少、延迟更低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by large language models&#x27; limited spatial reasoning and opaque design decisions in layout tasks, this paper introduces LaySPA, a reinforcement learning framework that reformulates graphic layout design as policy learning over a structured textual spatial environment encoding canvas geometry, element attributes, and relationships. The method produces interpretable reasoning traces and structured layout specifications, optimized via a multi-objective spatial critique for geometric validity, relational coherence, and aesthetic consistency, using relative group optimization to stabilize training. Experimental results show LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and matching specialized state-of-the-art layout generators with fewer annotated samples and lower latency.</div>
<div class="mono" style="margin-top:8px">针对大语言模型在布局任务中空间推理能力有限且设计决策不透明的问题，本文提出了LaySPA强化学习框架，将图形布局设计重新定义为在结构化文本空间环境上的策略学习，该环境显式编码画布几何、元素属性和相互关系。该方法生成可解释的推理轨迹和结构化布局规范，通过多目标空间评判器优化几何有效性、关系一致性和美学一致性，并采用相对群体优化稳定开放设计空间的训练。实验结果表明，LaySPA提升了结构有效性和视觉质量，优于更大的专有大语言模型，性能与专用先进布局生成器相当，同时需要更少的标注样本和更低的延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay</div>
<div class="meta-line">Authors: Gabriel Romio, Mateus Begnini Melchiades, Bruno Castro da Silva, Gabriel de Oliveira Ramos</div>
<div class="meta-line">First: 2026-02-14T19:55:11+00:00 · Latest: 2026-02-14T19:55:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13865v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13865v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent&#x27;s direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object&#x27;s final state (standard HER), 2HER also generates goals from the agent&#x27;s effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在稀疏奖励环境中通过后见经验回放实现选项学习</div>
<div class="mono" style="margin-top:8px">分层强化学习（HRL）框架如选项评论家（OC）和多更新选项评论家（MOC）在学习可重用选项方面取得了显著进展。然而，这些方法在稀疏奖励的多目标环境中表现不佳，因为行动需要与时间上遥远的结果相关联。为解决这一局限，我们首先提出了MOC-HER，将后见经验回放（HER）机制集成到MOC框架中。通过从已实现结果中重新标记目标，MOC-HER能够解决原始MOC难以处理的稀疏奖励环境。但该方法在物体操控任务中仍显不足，因为奖励取决于物体是否到达目标，而非智能体的直接交互，这使得HRL智能体难以发现如何与物体交互。为克服此问题，我们引入了双重目标后见经验回放（2HER），这一新颖扩展创建了两组虚拟目标：除了基于物体最终状态重新标记目标（标准HER），2HER还从智能体效应器位置生成目标，同时奖励智能体与物体交互及完成任务。在机器人操控环境中的实验结果表明，MOC-2HER的成功率高达90%，而MOC和MOC-HER均低于11%。这些结果凸显了我们的双重目标重标记策略在稀疏奖励多目标任务中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of hierarchical reinforcement learning (HRL) methods like Option-Critic underperforming in sparse-reward, multi-goal environments, where linking actions to distant outcomes is difficult. To overcome this, the authors first integrate Hindsight Experience Replay (HER) into the Multi-updates Option Critic framework, creating MOC-HER, which relabels goals from achieved outcomes to improve performance in sparse rewards. However, for object manipulation tasks where rewards depend on object states rather than agent actions, they propose Dual Objectives Hindsight Experience Replay (2HER), a novel extension that generates two sets of virtual goals: one based on the object&#x27;s final state and another on the agent&#x27;s effector positions, thereby rewarding both interaction and task completion. Experimental results in robotic manipulation environments demonstrate that MOC-2HER achieves success rates up to 90%, significantly outperforming MOC and MOC-HER, which both scored below 11%, validating the effectiveness of the dual objective relabeling strategy.</div>
<div class="mono" style="margin-top:8px">本文针对分层强化学习（HRL）方法在稀疏奖励、多目标环境中表现不佳的问题展开研究，这类环境中动作与远期结果难以关联。为解决此问题，作者首先将后见经验回放（HER）机制集成到多更新选项批评家框架中，提出MOC-HER，通过重标注已实现目标来改善稀疏奖励下的性能。然而，对于奖励依赖于物体状态而非智能体动作的物体操控任务，他们进一步提出了双重目标后见经验回放（2HER），这是一种新颖的扩展方法，生成两组虚拟目标：一组基于物体的最终状态，另一组基于智能体效应器位置，从而同时奖励交互行为和任务完成。在机器人操控环境中的实验结果表明，MOC-2HER的成功率高达90%，显著优于成功率均低于11%的MOC和MOC-HER，验证了双重目标重标注策略的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
