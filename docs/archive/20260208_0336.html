<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-08 03:36</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260208_0336</div>
    <div class="row"><div class="card">
<div class="title">Can vision language models learn intuitive physics from interaction?</div>
<div class="meta-line">Authors: Luca M. Schulze Buschoff, Konstantinos Voudouris, Can Demircan, Eric Schulz</div>
<div class="meta-line">First: 2026-02-05T18:59:20+00:00 · Latest: 2026-02-05T18:59:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06033v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06033v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型能否通过交互学习直观物理？</div>
<div class="mono" style="margin-top:8px">预训练的视觉语言模型对物理世界缺乏良好的直觉。近期研究表明，监督微调能提升模型在简单物理任务上的表现，但微调后的模型并未学到可泛化至新情境的稳健物理规则。基于认知科学研究，我们假设模型需通过与环境交互来正确学习其物理动态。我们使用强化学习训练模型，使其通过环境交互进行学习。尽管交互学习能提升模型在任务内的表现，但未能形成具有泛化性的物理直觉。研究发现，在单一任务上训练的模型无法可靠地泛化至相关任务，即使这些任务共享视觉统计特征与物理原理，且无论模型是否通过交互训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether vision-language models can acquire robust physical intuitions through interaction, motivated by their known limitations in understanding the physical world and the failure of supervised fine-tuning to yield generalizable rules. The method involves training models using reinforcement learning to interact with an environment, aiming to learn physical dynamics directly from experience. The main experimental results show that while interaction improves performance within the specific training task, it does not lead to models that generalize reliably to related tasks sharing similar visual and physical properties, indicating a lack of learned generalizable physical intuitions.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型能否通过交互学习获得稳健的物理直觉，其动机在于现有模型在理解物理世界方面存在不足，且监督微调未能产生可泛化的物理规则。研究方法采用强化学习训练模型与环境交互，旨在从经验中直接学习物理动态。主要实验结果表明，尽管交互提高了模型在特定训练任务中的表现，但模型无法可靠地泛化到具有相似视觉统计和物理原理的相关任务中，这表明模型未能学习到可泛化的物理直觉。</div>
</details>
</div>
<div class="card">
<div class="title">Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</div>
<div class="meta-line">Authors: Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan</div>
<div class="meta-line">First: 2026-02-05T18:58:32+00:00 · Latest: 2026-02-05T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06029v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>好奇心即知识：基于主动推断的自洽学习与无遗憾优化</div>
<div class="mono" style="margin-top:8px">主动推断通过最小化期望自由能，借助好奇心系数平衡认知价值（信息增益）与实用价值（任务性能），统一了探索与利用。然而，这种平衡何时能同时实现连贯的学习与高效的决策一直不明确：好奇心不足可能导致短视的利用并阻碍不确定性消解，而过度的好奇心则可能引发不必要的探索与遗憾。我们首次为最小化期望自由能的智能体建立了理论保证，证明单一条件——足够的好奇心——即可同时确保自洽学习（贝叶斯后验一致性）和无遗憾优化（累积遗憾有界）。分析揭示了该机制如何依赖于初始不确定性、可识别性与目标对齐性，从而在一个理论框架内将主动推断与经典贝叶斯实验设计及贝叶斯优化联系起来。我们进一步将这些理论转化为实用设计准则，用于在混合学习-优化问题中调节认知-实用权衡，并通过真实实验验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of balancing exploration and exploitation in active inference, where the Expected Free Energy (EFE) objective uses a curiosity coefficient to trade off information gain and task performance. The authors provide the first theoretical guarantee that setting sufficient curiosity ensures both self-consistent Bayesian learning and no-regret decision-making, linking active inference to Bayesian experimental design and optimization. Experimental validation supports practical guidelines for tuning this trade-off in hybrid learning-optimization scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对主动推理中探索与利用的平衡问题展开研究，其中期望自由能目标通过好奇心系数权衡信息增益与任务性能。作者首次提出理论保证，表明设置足够的好奇心可同时确保自洽的贝叶斯学习和无遗憾决策，从而将主动推理与贝叶斯实验设计和优化联系起来。实验验证为混合学习-优化场景中调整这一权衡提供了实用指导。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</div>
<div class="meta-line">Authors: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang</div>
<div class="meta-line">First: 2026-02-05T18:57:09+00:00 · Latest: 2026-02-05T18:57:09+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/BudgetMem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06025v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06025v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/BudgetMem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向运行时智能体内存的查询感知预算层级路由学习</div>
<div class="mono" style="margin-top:8px">内存对超越单上下文窗口运行的大语言模型（LLM）智能体日益关键，但现有系统多依赖离线、查询无关的内存构建方式，效率低下且可能丢失查询关键信息。尽管运行时内存利用是自然替代方案，但先前研究常产生显著开销，且对性能-成本权衡缺乏显式控制。本文提出\textbf{BudgetMem}——一个支持显式查询感知性能-成本控制的运行时智能体内存框架。该框架将内存处理构建为多模块结构，每个模块提供三种预算层级（即\textsc{低}/\textsc{中}/\textsc{高}）。轻量级路由器通过强化学习训练的紧凑神经策略，执行跨模块的预算层级路由以平衡任务性能与内存构建成本。以BudgetMem为统一测试平台，我们研究实现预算层级的三种互补策略：实现方式（方法复杂度）、推理机制（推断行为）和容量配置（模块模型规模）。在LoCoMo、LongMemEval和HotpotQA数据集上的实验表明：在优先性能的场景（即高预算设置）下，BudgetMem超越强基线模型；在严格预算约束下能提供更优的精度-成本边界。进一步分析揭示了不同层级策略的优劣特性，明确了各策略维度在不同预算区间实现最佳权衡的适用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces BudgetMem, a runtime memory framework for LLM agents motivated by the inefficiency and query-agnostic nature of existing offline memory systems, which lack explicit control over performance-cost trade-offs. The method structures memory into modules with three budget tiers (Low/Mid/High) and employs a lightweight neural router trained with reinforcement learning to perform query-aware, budget-tier routing across modules, balancing task performance and memory construction cost. Experimental results on LoCoMo, LongMemEval, and HotpotQA show that BudgetMem outperforms baselines in high-budget settings and achieves better accuracy-cost frontiers under tight budgets, with analysis revealing the distinct trade-offs of different tiering strategies based on implementation, reasoning, and capacity.</div>
<div class="mono" style="margin-top:8px">本文提出了BudgetMem，这是一个用于大型语言模型智能体的运行时记忆框架，其动机在于现有离线记忆系统效率低下且与查询无关，缺乏对性能与成本权衡的明确控制。该方法将记忆组织为具有三个预算层级（低/中/高）的模块，并采用通过强化学习训练的轻量级神经路由器，执行跨模块的查询感知预算层级路由，以平衡任务性能和记忆构建成本。在LoCoMo、LongMemEval和HotpotQA上的实验结果表明，BudgetMem在高预算设置下优于基线方法，并在严格预算下实现了更好的准确率-成本前沿，分析还揭示了基于实现、推理和容量的不同层级策略在不同预算制度下的独特权衡优势。</div>
</details>
</div>
<div class="card">
<div class="title">On Computation and Reinforcement Learning</div>
<div class="meta-line">Authors: Raj Ghugare, Michał Bortkiewicz, Alicja Ziarko, Benjamin Eysenbach</div>
<div class="meta-line">First: 2026-02-05T18:45:57+00:00 · Latest: 2026-02-05T18:45:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05999v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05999v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论计算与强化学习</div>
<div class="mono" style="margin-top:8px">强化学习策略可用的计算量如何影响其学习效果？使用固定参数量的策略是否仍能从额外计算中获益？标准强化学习框架缺乏正式回答这些问题的理论工具。实践中，深度强化学习策略常被参数化为静态架构的神经网络，混淆了计算量与参数量。本文形式化定义了计算受限策略，并证明使用更多计算的策略能解决计算较少策略无法处理的问题，且能泛化至更长时域的任务。基于算法学习和无模型规划的前期研究，我们提出一种能灵活利用可变计算量的最小化架构。实验与理论相互印证：在涵盖在线与离线强化学习的31项任务中，该架构（1）仅通过增加计算量即可获得更强性能，（2）与使用多达5倍参数的标准前馈网络或深度残差网络相比，在长时域测试任务中展现出更优的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how the computational budget, distinct from parameter count, influences reinforcement learning (RL) performance and generalization. To formally address this, the authors introduce a framework for compute-bounded policies and propose a minimal architecture capable of utilizing variable computation. Experimental results across 31 diverse RL tasks demonstrate that this architecture achieves stronger performance with increased compute and better generalization to longer-horizon tasks compared to standard networks with significantly more parameters.</div>
<div class="mono" style="margin-top:8px">本文研究了计算资源（区别于参数数量）如何影响强化学习的性能与泛化能力。为正式探讨此问题，作者提出了计算受限策略的框架，并设计了一种能够利用可变计算量的最小化架构。在31个不同的强化学习任务上的实验结果表明，该架构通过使用更多计算资源获得了更强的性能，并且在更长视野的测试任务上，比参数数量多出5倍的标准前馈网络或深度残差网络具有更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access</div>
<div class="meta-line">Authors: Daniel Ebi, Gaspard Lambrechts, Damien Ernst, Klemens Böhm</div>
<div class="meta-line">First: 2025-09-30T09:32:20+00:00 · Latest: 2026-02-05T18:21:20+00:00</div>
<div class="meta-line">Comments: 11 pages, 26 pages total, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26000v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26000v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Asymmetric actor-critic methods are widely used in partially observable reinforcement learning, but typically assume full state observability to condition the critic during training, which is often unrealistic in practice. We introduce the informed asymmetric actor-critic framework, allowing the critic to be conditioned on arbitrary state-dependent privileged signals without requiring access to the full state. We show that any such privileged signal yields unbiased policy gradient estimates, substantially expanding the set of admissible privileged information. This raises the problem of selecting the most adequate privileged information in order to improve learning. For this purpose, we propose two novel informativeness criteria: a dependence-based test that can be applied prior to training, and a criterion based on improvements in value prediction accuracy that can be applied post-hoc. Empirical results on partially observable benchmark tasks and synthetic environments demonstrate that carefully selected privileged signals can match or outperform full-state asymmetric baselines while relying on strictly less state information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知情非对称行动者-评论者：利用超越全状态访问的特权信号</div>
<div class="mono" style="margin-top:8px">非对称行动者-评论者方法在部分可观测强化学习中广泛应用，但通常假设评论者在训练期间能基于全状态条件进行学习，这在实践中往往不现实。我们提出知情非对称行动者-评论者框架，允许评论者基于任意与状态相关的特权信号进行条件化，而无需访问完整状态。我们证明任何此类特权信号均可产生无偏策略梯度估计，从而显著扩展了可采纳特权信息的范围。这引出了如何选择最合适特权信息以提升学习效果的问题。为此，我们提出两种新颖的信息量准则：一种可在训练前应用的基于依赖关系的检验方法，以及一种基于价值预测精度改进的事后评估准则。在部分可观测基准任务和合成环境中的实验结果表明，精心选择的特权信号在依赖严格更少状态信息的情况下，能够达到或超越全状态非对称基线方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of standard asymmetric actor-critic methods, which rely on full state access for the critic during training—an often impractical assumption in partially observable environments. To overcome this, the authors propose an informed asymmetric actor-critic framework that allows the critic to be conditioned on arbitrary state-dependent privileged signals, without requiring full state observability, and prove that such signals yield unbiased policy gradient estimates. They further introduce two novel criteria for selecting effective privileged information: a dependence-based pre-training test and a post-hoc value prediction accuracy measure. Experimental results on partially observable benchmarks and synthetic tasks show that well-chosen privileged signals can match or exceed the performance of full-state baselines while using strictly less state information.</div>
<div class="mono" style="margin-top:8px">本文针对标准非对称行动者-评论家方法在训练中依赖完全状态访问的局限性提出改进，这在部分可观测环境中往往不切实际。为此，作者提出了一个知情非对称行动者-评论家框架，允许评论家基于任意与状态相关的特权信号进行条件化，而无需访问完整状态，并证明了此类信号能产生无偏的策略梯度估计。为进一步选择有效的特权信息，他们提出了两个新颖的信息量准则：一种可训练前应用的基于依赖性的测试，以及一种基于价值预测准确性改进的事后准则。在部分可观测基准任务和合成环境上的实验结果表明，精心选择的特权信号能够匹配甚至超越完全状态基线方法的性能，同时依赖更少的状态信息。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Share: Selective Memory for Efficient Parallel Agentic Systems</div>
<div class="meta-line">Authors: Joseph Fioresi, Parth Parag Kulkarni, Ashmal Vayani, Song Wang, Mubarak Shah</div>
<div class="meta-line">First: 2026-02-05T18:20:21+00:00 · Latest: 2026-02-05T18:20:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://joefioresi718.github.io/LTS_webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学会共享：面向高效并行智能体系统的选择性记忆机制</div>
<div class="mono" style="margin-top:8px">智能体系统通过协调多个智能体进行迭代推理、调用工具并交换中间结果来解决复杂任务。为提高鲁棒性与求解质量，近期研究采用并行运行的多个智能体团队来探索多样化推理路径。然而并行执行会带来显著计算开销：当不同团队独立处理相似子问题或执行同类步骤时，会重复进行大量重叠计算。为突破此局限，本文提出&#x27;学会共享&#x27;（LTS）——一种用于并行智能体框架的习得式共享记忆机制，可在控制上下文增长的同时实现跨团队选择性信息复用。LTS构建了全局共享记忆库与轻量控制器，后者通过逐步强化学习结合使用感知的信用分配机制进行训练，从而精准识别跨并行执行过程的全局有效信息。在AssistantBench与GAIA基准测试中，LTS在保持或提升任务性能的同时显著降低总体运行时间，验证了习得式记忆准入是提升并行智能体系统效率的有效策略。项目页面：https://joefioresi718.github.io/LTS_webpage/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the computational inefficiency in parallel agentic systems, where multiple independent teams often redundantly process similar sub-problems, leading to substantial overlapping computation. The method proposed, Learning to Share (LTS), introduces a learned shared-memory mechanism with a global memory bank and a lightweight controller trained via stepwise reinforcement learning to selectively admit intermediate agent steps for cross-team reuse, thereby controlling context growth. Main experimental results on AssistantBench and GAIA benchmarks demonstrate that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, confirming its effectiveness in enhancing system efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决并行智能体系统中的计算效率低下问题，即多个独立团队常冗余处理相似子问题，导致大量重复计算。所提出的方法“学习共享”（LTS）引入了一种基于学习的共享内存机制，包含一个全局内存库和一个轻量级控制器，该控制器通过逐步强化学习训练，以选择性允许中间智能体步骤跨团队重用，从而控制上下文增长。在AssistantBench和GAIA基准上的主要实验结果表明，与无内存并行基线相比，LTS显著降低了总体运行时间，同时保持或提升了任务性能，验证了其提升系统效率的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-02-05T18:03:03+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v2">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题发现新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时执行强化学习，使大型语言模型能够持续训练，但此时训练经验专门针对测试问题。这种持续学习形式非常特殊，因为其目标是产生一个卓越解决方案而非多个平均良好的方案，且旨在解决当前特定问题而非泛化至其他问题。因此，我们的学习目标和搜索子程序被设计为优先考虑最有潜力的解决方案。我们将此方法称为“测试时训练发现法”。遵循先前研究，我们专注于具有连续奖励的问题。我们报告了在数学、GPU内核工程、算法设计和生物学领域尝试的所有问题的结果。TTT-Discover在几乎所有问题上都创造了新的最优解：（i）埃尔德什最小重叠问题与自相关不等式；（ii）GPUMode内核竞赛（比先前最优方案快达2倍）；（iii）过往AtCoder算法竞赛；（iv）单细胞分析中的去噪问题。我们的解决方案均经过专家或组织者评审。所有结果均使用开源模型OpenAI gpt-oss-120b实现，并可通过我们公开的代码复现，而先前最优结果需依赖封闭前沿模型。测试时训练通过Thinking Machines的Tinker API执行，每个问题成本仅数百美元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the goal of using AI to achieve new state-of-the-art solutions for scientific problems, this paper introduces Test-Time Training to Discover (TTT-Discover), a method that performs reinforcement learning at test time to allow a large language model (LLM) to continually train on experience specific to a given problem, prioritizing the most promising solutions. The approach focuses on problems with continuous rewards and is designed to produce one great solution rather than average performance across many. Experimental results demonstrate that TTT-Discover sets new state-of-the-art performance across multiple domains: it advances solutions in Erdős&#x27; minimum overlap problem and an autocorrelation inequality in mathematics, achieves up to 2x speedup in a GPU kernel engineering competition, excels in past AtCoder algorithm competitions, and improves denoising in single-cell biology analysis, with all solutions verified by experts or organizers. Notably, these results are achieved using an open model (OpenAI gpt-oss-120b) and publicly available code, contrasting with prior work that relied on closed frontier models, with test-time training costs kept to a few hundred dollars per problem.</div>
<div class="mono" style="margin-top:8px">本文旨在利用人工智能为科学问题实现新的最先进解决方案，提出了测试时训练发现方法（TTT-Discover），该方法在测试时通过强化学习使大型语言模型能够针对特定问题持续训练，并优先探索最有希望的解决方案。该方法专注于具有连续奖励的问题，旨在产生单个优秀解而非平均性能。实验结果表明，TTT-Discover在多个领域取得了新的最先进成果：在数学中改进了埃尔德什最小重叠问题和自相关不等式，在GPU内核工程竞赛中实现了比先前技术快2倍的速度，在过去的AtCoder算法竞赛中表现出色，并在单细胞生物学分析中提升了去噪性能，所有解决方案均经过专家或组织者审核。值得注意的是，这些成果使用开源模型（OpenAI gpt-oss-120b）和公开代码实现，与之前依赖封闭前沿模型的工作形成对比，且每个问题的测试时训练成本仅为几百美元。</div>
</details>
</div>
<div class="card">
<div class="title">$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment</div>
<div class="meta-line">Authors: Rajdeep Haldar, Lantao Mei, Guang Lin, Yue Xing, Qifan Song</div>
<div class="meta-line">First: 2026-02-05T18:01:52+00:00 · Latest: 2026-02-05T18:01:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05946v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05946v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$f$-GRPO 及其拓展：基于散度的通用大语言模型对齐强化学习算法</div>
<div class="mono" style="margin-top:8px">近期研究表明，偏好对齐目标可视为对齐（选中）与未对齐（拒绝）响应分布间的散度估计器。本研究将这一基于散度的视角拓展至通用对齐场景，例如仅依赖环境奖励的强化学习验证奖励设置。在此统一框架下，我们提出了$f$-群组相对策略优化——一类基于策略的强化学习方法，以及$f$-混合对齐损失——一种混合策略目标，二者均基于$f$-散度的变分表示构建，适用于通用大语言模型对齐。我们提供了理论保证，证明这些目标能提升对齐后的平均奖励。在实验中，我们在强化学习验证奖励（数学推理）和偏好对齐（安全对齐）任务上验证了框架的有效性，相较于现有方法展现出更优的性能与灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the insight that preference alignment objectives can be viewed as divergence estimators, this work extends a divergence-based framework to general language model alignment settings, including reinforcement learning with verifiable rewards (RLVR) where only environmental feedback is present. The method introduces two novel classes of objectives: $f$-GRPO, an on-policy reinforcement learning approach, and $f$-HAL, a hybrid on/off-policy loss, both derived from the variational representation of $f$-divergences to align models. The main experimental results demonstrate the framework&#x27;s effectiveness and flexibility, showing superior performance over existing methods in both RLVR tasks, such as math reasoning, and preference alignment tasks, like safety alignment, with theoretical guarantees of reward improvement.</div>
<div class="mono" style="margin-top:8px">本研究受偏好对齐目标可视为分布散度估计器的启发，将基于散度的框架扩展至通用大语言模型对齐场景，包括仅依赖环境奖励的强化学习验证奖励（RLVR）任务。方法上提出了两类新目标：基于$f$散度变分表示的在线策略强化学习方法$f$-GRPO，以及混合在线/离线策略的损失函数$f$-HAL，用于模型对齐。主要实验结果验证了该框架的有效性和灵活性，在数学推理等RLVR任务和安全对齐等偏好对齐任务上均优于现有方法，并理论保证了奖励提升。</div>
</details>
</div>
<div class="card">
<div class="title">Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training</div>
<div class="meta-line">Authors: Zhenghao Xu, Qin Lu, Changlong Yu, Tuo Zhao</div>
<div class="meta-line">First: 2026-02-05T17:44:28+00:00 · Latest: 2026-02-05T17:44:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05933v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05933v1">PDF</a> · <a href="https://github.com/horizon-rl/OpenKimi">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略镜像下降中对数配分函数的近似诱导大语言模型后训练的隐式正则化</div>
<div class="mono" style="margin-top:8px">策略镜像下降通过迭代求解KL正则化的策略改进子问题，为强化学习提供了理论框架。尽管该方法已用于训练Kimi K1.5/K2等先进大语言模型，但理想的闭式PMD更新需要可靠的对数配分函数估计，这在大语言模型广阔动作空间中有限采样条件下极具挑战。本文研究一种名为PMD-mean的实用算法，该算法用采样策略下的平均奖励近似对数配分项，并在对数策略空间执行回归。我们刻画了PMD-mean的总体解，证明其隐式优化了具有自适应混合KL-χ²正则项的镜像下降子问题。额外的χ²正则化约束了大幅概率变化，在期望奖励较低时产生更保守的更新，并增强了对有限样本估计误差的鲁棒性。数学推理任务实验表明，PMD-mean以更高的稳定性与时间效率获得优越性能。这些发现深化了对PMD-mean的理解，并为大语言模型强化学习算法的理论改进指明了路径。代码发布于https://github.com/horizon-rl/OpenKimi。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of reliably estimating the partition function in policy mirror descent (PMD) for LLM reinforcement learning, which is difficult due to limited rollouts in large action spaces. The method introduces PMD-mean, a practical algorithm that approximates the log-partition term using the mean reward under the sampling policy and performs regression in log-policy space. Experimental results on math reasoning tasks demonstrate that PMD-mean achieves superior performance with improved stability and time efficiency, as the implicit adaptive mixed KL-χ² regularization it induces produces more conservative updates and enhances robustness against estimation errors.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决在大语言模型强化学习中，由于动作空间巨大且样本有限，策略镜像下降（PMD）的配分函数估计困难的问题。方法上提出了PMD-mean算法，该算法利用采样策略下的平均奖励来近似对数配分项，并在对数策略空间进行回归。在数学推理任务上的实验结果表明，PMD-mean实现了更优的性能，并具有更好的稳定性和时间效率，其隐含的自适应混合KL-χ²正则化能产生更保守的更新，增强了对估计误差的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem</div>
<div class="meta-line">Authors: Eva Andrés</div>
<div class="meta-line">First: 2026-02-05T17:32:14+00:00 · Latest: 2026-02-05T17:32:14+00:00</div>
<div class="meta-line">Comments: 22 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05920v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05920v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Transformer的量子强化学习求解带容量约束的车辆路径问题</div>
<div class="mono" style="margin-top:8px">本文通过比较经典与量子强化学习方法，研究带容量约束的车辆路径问题。研究实现了经典、全量子和混合三种优势行动者-评论者智能体变体，集成Transformer架构，通过自注意力与交叉注意力机制捕捉车辆、客户与配送中心间的关联关系。实验聚焦多车辆容量约束场景，设置20个客户与4辆车辆，进行十次独立运行。性能通过路径距离、路径紧凑度和路径重叠度进行评估。结果表明三种方法均能学习有效路径策略，但量子增强模型优于经典基线，产生更稳健的路径组织，其中混合架构在距离、紧凑度和路径重叠度上综合表现最佳。除量化改进外，定性可视化显示量子模型能生成更具结构性和连贯性的路径方案。这些发现凸显了混合量子-经典强化学习模型处理CVRP等复杂组合优化问题的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to solve the complex combinatorial optimization challenge of the Capacitated Vehicle Routing Problem (CVRP). The method employs Reinforcement Learning, specifically an Advantage Actor-Critic agent, implemented in classical, full quantum, and hybrid variants, and integrates transformer architectures with attention mechanisms to model relationships between vehicles, clients, and the depot. Experimental results on a scenario with 20 clients and 4 vehicles over ten runs show that all three approaches learn effective policies, but quantum-enhanced models, particularly the hybrid architecture, outperform the classical baseline in routing distance, compactness, and overlap, yielding more robust and structured solutions.</div>
<div class="mono" style="margin-top:8px">本文旨在解决带容量约束的车辆路径问题这一复杂的组合优化挑战。研究方法采用强化学习，具体为优势演员-评论家智能体，并实现了经典、全量子和混合三种变体，同时集成了具有注意力机制的Transformer架构以建模车辆、客户和仓库之间的关系。在包含20个客户和4辆车的场景上进行十次独立实验的结果表明，所有三种方法都能学习到有效的策略，但量子增强模型，特别是混合架构，在路径距离、紧凑性和重叠度方面优于经典基线，产生了更稳健和结构化的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to summarize user information for personalized reinforcement learning from human feedback</div>
<div class="meta-line">Authors: Hyunji Nam, Yanming Wan, Mickel Liu, Peter Ahnn, Jianxun Lian, Natasha Jaques</div>
<div class="meta-line">First: 2025-07-17T23:48:51+00:00 · Latest: 2026-02-05T17:13:23+00:00</div>
<div class="meta-line">Comments: 10 pages for main text, 10 pages for appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.13579v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.13579v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As everyday use cases of large language model (LLM) AI assistants have expanded, it is becoming increasingly important to personalize responses to align to different users&#x27; preferences and goals. While reinforcement learning from human feedback (RLHF) is effective at improving LLMs to be generally more helpful and fluent, it does not account for variability across users, as it models the entire user population with a single reward model, meaning it assumes that everyone&#x27;s preferences are the same. We present a novel framework, Preference Learning Using Summarization (PLUS), that uses reinforcement learning (RL) to learn to produce text-based summaries of each user&#x27;s preferences, characteristics, and past conversations. These summaries condition the reward model, enabling it to make personalized predictions about the types of responses valued by each user. Both the user-summarization model and reward model are trained simultaneously, creating an online co-adaptation loop. We show that in contrast to the standard Bradley-Terry model, summaries produced by PLUS capture diverse aspects of user preferences, achieving a 11-77/% improvement in reward model accuracy. Key strengths of PLUS are: (1) robust performance with new users and conversation topics, achieving a 25\% improvement over the best personalized reward model technique used for RLHF; (2) zero-shot personalization with state-of-the-art proprietary models like GPT-4 (e.g., PLUS-summary-conditioned responses achieved a 72\% win rate compared to 28% for default GPT-4o); (3) learning from flexible user contexts beyond preference labels, and (4) interpretable representation of users, enabling greater transparency and user control in pluralistic LLM alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习总结用户信息以实现基于人类反馈的个性化强化学习</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLM）AI助手在日常应用中的扩展，个性化响应用户偏好与目标变得日益重要。基于人类反馈的强化学习（RLHF）虽能有效提升LLM的通用助益性与流畅度，但未考虑用户间的差异性，因其使用单一奖励模型建模全体用户，隐含假设所有人偏好相同。本文提出一种新颖框架——基于总结的偏好学习（PLUS），利用强化学习（RL）生成基于文本的用户偏好、特征及历史对话摘要。这些摘要作为奖励模型的调节条件，使其能针对每位用户进行个性化响应价值预测。用户摘要模型与奖励模型同步训练，形成在线协同适应循环。实验表明，相较于标准Bradley-Terry模型，PLUS生成的摘要能捕捉用户偏好的多元维度，将奖励模型准确率提升11-77%。PLUS的核心优势包括：（1）对新用户与对话主题具有强健性能，较RLHF最佳个性化奖励模型技术提升25%；（2）实现与GPT-4等前沿专有模型的零样本个性化交互（例如：PLUS摘要调节的响应胜率达72%，而默认GPT-4o仅为28%）；（3）支持超越偏好标签的灵活用户上下文学习；（4）提供可解释的用户表征，增强多元化LLM对齐的透明度与用户可控性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that standard reinforcement learning from human feedback (RLHF) fails to personalize large language model responses because it uses a single reward model for all users, ignoring individual preferences. The proposed method, called Preference Learning Using Summarization (PLUS), introduces a novel framework where reinforcement learning is used to generate text-based summaries of each user&#x27;s preferences and history; these summaries then condition a reward model to make personalized predictions. Key experimental results show that PLUS significantly improves reward model accuracy by 11-77%, outperforms the best personalized RLHF technique by 25% with new users and topics, and enables zero-shot personalization, such as achieving a 72% win rate against default GPT-4o, while also providing interpretable user representations.</div>
<div class="mono" style="margin-top:8px">这项工作的动机在于，标准的基于人类反馈的强化学习（RLHF）使用单一奖励模型，无法针对不同用户的偏好进行个性化响应。为此，研究提出了名为偏好学习与摘要（PLUS）的新框架，该方法利用强化学习生成基于文本的用户偏好与历史摘要，并用这些摘要来调节奖励模型以实现个性化预测。主要实验结果表明，PLUS将奖励模型准确率提升了11-77%，在新用户和新话题上比最佳个性化RLHF技术性能提高25%，并能实现零样本个性化，例如在与默认GPT-4o的对比中以72%的胜率获胜，同时提供了可解释的用户表示。</div>
</details>
</div>
<div class="card">
<div class="title">DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training</div>
<div class="meta-line">Authors: Dingwei Zhu, Zhiheng Xi, Shihan Dou, Jiahan Li, Chenhao Huang, Junjie Ye, Sixian Li, Mingxu Chai, Yuhui Wang, Yajie Yang, Ming Zhang, Jiazheng Zhang, Shichun Liu, Caishuang Huang, Yunke Zhang, Yuran Wang, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang</div>
<div class="meta-line">First: 2026-02-05T17:07:42+00:00 · Latest: 2026-02-05T17:07:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05890v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05890v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DFPO：通过分布流扩展价值建模，实现稳健且可泛化的大语言模型后训练</div>
<div class="mono" style="margin-top:8px">在现实环境中训练强化学习系统仍面临监督噪声大和领域外泛化能力差的挑战，尤其在大语言模型后训练中。现有分布强化学习方法通过多分位数点建模价值以提升稳健性，但各分位数仍作为独立标量学习，导致价值表征粒度粗糙、缺乏对状态信息的细粒度条件化，难以应对复杂及领域外场景。本文提出DFPO（带条件风险与一致性控制的分布价值流策略优化），一种稳健的分布强化学习框架，将价值建模为跨时间步的连续流。通过学习价值流场而非孤立分位数预测，DFPO扩展了价值建模维度，捕获更丰富的状态信息以实现更精准的优势估计。为在噪声反馈下稳定训练，DFPO进一步整合了沿价值流轨迹的条件风险控制与一致性约束。在对话、数学推理及科学任务上的实验表明，DFPO在噪声监督下优于PPO、FlowRL等基线方法，显著提升了训练稳定性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenges of noisy supervision and poor out-of-domain generalization in reinforcement learning for LLM post-training, where existing distributional RL methods often produce coarse-grained value representations. To address this, the authors propose DFPO, a framework that models values as continuous flows across time steps instead of independent scalar quantiles, enabling richer state conditioning and more accurate advantage estimation; it further incorporates conditional risk control and consistency constraints to stabilize training under noise. Experimental results on dialogue, math reasoning, and scientific tasks demonstrate that DFPO outperforms baselines like PPO and FlowRL in robustness and generalization under noisy supervision.</div>
<div class="mono" style="margin-top:8px">本文的动机是针对大语言模型后训练中强化学习面临的噪声监督和域外泛化能力差的问题，现有分布强化学习方法常产生粗粒度的价值表示。为此，作者提出了DFPO框架，通过将价值建模为跨时间步的连续流而非独立的标量分位数，以捕获更丰富的状态信息并实现更准确的优势估计；该框架还整合了条件风险控制和价值流轨迹的一致性约束，以在噪声下稳定训练。在对话、数学推理和科学任务上的实验结果表明，DFPO在噪声监督下优于PPO、FlowRL等基线方法，展现出更强的训练稳定性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</div>
<div class="meta-line">Authors: Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He</div>
<div class="meta-line">First: 2026-02-05T17:01:09+00:00 · Latest: 2026-02-05T17:01:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05885v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05885v1">PDF</a> · <a href="https://www.github.com/hkust-nlp/KernelGYM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dr. Kernel：面向Triton内核生成的正则化强化学习方法</div>
<div class="mono" style="margin-top:8px">高质量内核对可扩展AI系统至关重要，使大语言模型能生成此类代码将推动AI发展。然而，训练大语言模型完成此任务需要充足数据、鲁棒环境，且该过程常易受奖励破解与惰性优化影响。模型可能破解训练奖励，优先追求表面正确性而非实质性加速。本文系统研究强化学习在内核生成中的应用。首先设计KernelGYM——支持奖励破解检测、多轮交互数据收集与长期强化学习训练的分布式GPU环境。基于此，研究有效的多轮强化学习方法，发现GRPO中因自包含导致的策略梯度偏差问题。为此提出轮级强化学习留一法，为多轮强化学习提供无偏优势估计。为缓解惰性优化，引入失配校正以提升训练稳定性，并提出基于性能剖析的奖励机制与拒绝采样方法以克服该问题。训练所得Dr.Kernel-14B模型在Kernelbench中达到与Claude-4.5-Sonnet相当的性能。最后研究Dr.Kernel-14B的序列测试时扩展：在KernelBench Level-2子集中，31.6%生成内核实现较Torch参考至少1.2倍加速，超越Claude-4.5-Sonnet（26.7%）与GPT-5（28.6%）；当跨所有轮次选择最佳候选时，该加速率进一步提升至47.8%。全部资源（环境、训练代码、模型及数据集）已开源：https://www.github.com/hkust-nlp/KernelGYM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of using reinforcement learning (RL) to train large language models (LLMs) for generating high-performance GPU kernels, a task prone to reward hacking and lazy optimization where models may prioritize trivial correctness over actual speedup. The method introduces KernelGYM, a robust distributed GPU environment for multi-turn RL training and reward hacking checks, and proposes Turn-level Reinforce-Leave-One-Out (TRLOO) to correct biased policy gradients, alongside profiling-based rewards and rejection sampling to combat lazy optimization. Experimental results show that the resulting model, Dr.Kernel-14B, achieves competitive performance with Claude-4.5-Sonnet, and on a specific benchmark subset, 31.6% of its generated kernels achieve at least a 1.2x speedup over a reference, surpassing other models, with the rate rising to 47.8% when selecting the best candidate across multiple turns.</div>
<div class="mono" style="margin-top:8px">本文针对使用强化学习训练大语言模型生成高性能GPU内核的挑战展开研究，该任务易出现奖励破解和惰性优化问题，即模型可能优先考虑表面正确性而非实际加速效果。方法上提出了KernelGYM，这是一个用于多轮强化学习训练和奖励破解检查的鲁棒分布式GPU环境，并设计了Turn-level Reinforce-Leave-One-Out（TRLOO）来纠正有偏的策略梯度，同时结合基于性能剖析的奖励和拒绝采样以缓解惰性优化。实验结果表明，所得模型Dr.Kernel-14B的性能与Claude-4.5-Sonnet相当，在特定基准测试子集上，其生成的内核中有31.6%实现了至少1.2倍于参考代码的加速，优于其他模型，且当在多轮中选取最佳候选时，该加速比例进一步提升至47.8%。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning</div>
<div class="meta-line">Authors: Wenquan Lu, Hai Huang, Randall Balestriero</div>
<div class="meta-line">First: 2026-02-03T06:59:42+00:00 · Latest: 2026-02-05T16:51:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03190v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03190v2">PDF</a> · <a href="https://github.com/wenquanlu/prompt-augmentation-GRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 45.2 per-benchmark accuracy and 51.8 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示增强技术扩展GRPO在数学推理任务中的训练规模</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）等强化学习算法已展现出提升大语言模型数学推理能力的巨大潜力。然而，先前研究普遍观察到强化学习后训练过程中会出现熵崩溃现象，表现为策略熵单调递减，最终导致训练失稳与崩溃。因此，现有方法大多将训练限制在较短周期（通常5-20轮），制约了持续探索并阻碍策略的进一步优化。此外，几乎所有先前研究在训练期间都依赖单一固定的推理提示模板。本研究提出提示增强训练策略，通过引导模型在不同模板与格式下生成推理轨迹，有效提升训练轨迹的多样性。实验表明，在无KL正则项的情况下，提示增强技术能在固定数据集上实现训练时长的稳定扩展，并使模型能够适应低熵状态而避免过早崩溃。实证结果显示，采用提示增强技术在MATH Level 3-5数据集上训练的Qwen2.5-Math-1.5B模型取得了最先进的性能，在AIME24、AMC、MATH500、Minerva及OlympiadBench等标准数学推理基准测试中，分别达到45.2%的基准准确率和51.8%的题目准确率。代码与模型检查点已开源：https://github.com/wenquanlu/prompt-augmentation-GRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the entropy collapse and limited training horizons observed in group-relative policy optimization (GRPO) for enhancing large language models&#x27; mathematical reasoning. The proposed method, prompt augmentation, increases rollout diversity by training the model to generate reasoning traces under varied templates and formats, which stabilizes extended training without requiring KL regularization. Experimental results show that a Qwen2.5-Math-1.5B model trained with this strategy on the MATH Level 3-5 dataset achieves state-of-the-art performance, with 45.2 per-benchmark and 51.8 per-question accuracy on key mathematical reasoning benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究针对使用群体相对策略优化（GRPO）增强大语言模型数学推理能力时出现的熵崩溃和训练周期受限问题，提出了提示增强方法。该方法通过让模型在多样化的模板和格式下生成推理轨迹来增加训练数据的多样性，从而在不依赖KL正则化的情况下实现了稳定的长时间训练。实验结果表明，在MATH Level 3-5数据集上使用提示增强训练的Qwen2.5-Math-1.5B模型取得了最先进的性能，在AIME24、AMC等关键数学推理基准测试中分别达到了45.2的基准平均准确率和51.8的题目平均准确率。</div>
</details>
</div>
<div class="card">
<div class="title">The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton&#x27;s Laws in Financial Deep Reinforcement Learning (RL) Algorithms</div>
<div class="meta-line">Authors: Trang Thoi, Hung Tran, Tram Thoi, Huaiyang Zhong</div>
<div class="meta-line">First: 2026-02-01T18:48:33+00:00 · Latest: 2026-02-05T16:48:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01388v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (DRL), a subset of machine learning focused on sequential decision-making, has emerged as a powerful approach for tackling financial trading problems. In finance, DRL is commonly used either to generate discrete trade signals or to determine continuous portfolio allocations. In this work, we propose a novel reinforcement learning framework for portfolio optimization that incorporates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into several DRL algorithms. The approach replaces conventional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs) in both actor and critic components-utilizing learnable B-spline univariate functions to achieve parameter-efficient and more interpretable function approximation. During actor updates, we introduce a physics-informed regularization loss that promotes second-order temporal consistency between observed return dynamics and the action-induced portfolio adjustments. The proposed framework is evaluated across three equity markets-China, Vietnam, and the United States, covering both emerging and developed economies. Across all three markets, PIKAN-based agents consistently deliver higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and more favorable drawdown characteristics compared to both standard DRL baselines and classical online portfolio-selection methods. This yields more stable training, higher Sharpe ratios, and superior performance compared to traditional DRL counterparts. The approach is particularly valuable in highly dynamic and noisy financial markets, where conventional DRL often suffers from instability and poor generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强型物理信息Kolmogorov-Arnold网络：牛顿定律在金融深度强化学习算法中的应用</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）作为机器学习中专注于序列决策的子领域，已成为解决金融交易问题的有力方法。在金融领域，DRL通常用于生成离散交易信号或确定连续投资组合配置。本研究提出了一种新颖的强化学习框架，将物理信息Kolmogorov-Arnold网络（PIKANs）融入多种DRL算法以优化投资组合。该方法在行动者和评论者组件中用Kolmogorov-Arnold网络（KANs）替代传统多层感知器，利用可学习的B样条单变量函数实现参数高效且更可解释的函数逼近。在行动者更新过程中，我们引入了物理信息正则化损失，以促进观测收益动态与行动引发的投资组合调整之间的二阶时间一致性。所提框架在中国、越南和美国三个股票市场（涵盖新兴与发达经济体）进行评估。在所有市场中，基于PIKAN的智能体相比标准DRL基线和经典在线投资组合选择方法，持续实现更高的累计与年化收益、更优的夏普与卡尔玛比率，以及更有利的回撤特性。相较于传统DRL方法，该框架带来更稳定的训练、更高的夏普比率和更卓越的性能。该方法在高度动态且噪声显著的金融市场中尤其有价值，因为传统DRL常面临不稳定和泛化能力不足的问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more stable and interpretable deep reinforcement learning (DRL) in financial portfolio optimization, this paper introduces a novel framework that integrates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into DRL algorithms. The method replaces standard multilayer perceptrons in actor and critic networks with Kolmogorov-Arnold Networks, which use learnable B-spline functions for efficient and interpretable approximation, and adds a physics-informed regularization loss during actor updates to enforce second-order temporal consistency between returns and portfolio adjustments. Experimental results across equity markets in China, Vietnam, and the United States show that PIKAN-based agents achieve higher cumulative and annualized returns, better Sharpe and Calmar ratios, and improved drawdown characteristics compared to baseline DRL and classical portfolio methods, leading to more stable training and superior performance in dynamic financial environments.</div>
<div class="mono" style="margin-top:8px">针对金融投资组合优化中深度强化学习（DRL）稳定性与可解释性不足的问题，本文提出了一种将物理信息柯尔莫哥洛夫-阿诺德网络（PIKANs）集成到DRL算法中的新框架。该方法用柯尔莫哥洛夫-阿诺德网络替代了执行器和评论器网络中的传统多层感知器，通过可学习的B样条函数实现高效且可解释的近似，并在执行器更新时引入物理信息正则化损失，以强制收益与投资组合调整之间的二阶时间一致性。在中国、越南和美国股票市场的实验结果表明，基于PIKAN的智能体相比基线DRL和经典投资组合方法，获得了更高的累计和年化收益、更优的夏普与卡尔玛比率，以及更好的回撤特性，从而在动态金融市场中实现了更稳定的训练和更卓越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models</div>
<div class="meta-line">Authors: Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, Shaohui Lin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-03-09T20:06:45+00:00 · Latest: 2026-02-05T16:21:33+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. Code is available at https://github.com/Osilly/Vision-R1</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.06749v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.06749v3">PDF</a> · <a href="https://github.com/Osilly/Vision-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model&#x27;s ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. Scaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4% and 78.2% MathVista benchmark scores, respectively. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vision-R1：激励多模态大语言模型中的推理能力</div>
<div class="mono" style="margin-top:8px">DeepSeek-R1-Zero已成功证明仅通过强化学习（RL）即可在大语言模型中涌现推理能力。受此突破启发，我们探索如何利用RL增强多模态大语言模型（MLLM）的推理能力。然而，由于缺乏高质量多模态推理数据，直接使用RL训练难以激活MLLM中的复杂推理能力（如质疑与反思）。为此，我们提出推理型MLLM——Vision-R1以提升多模态推理能力。具体而言，我们首先通过模态桥接与数据过滤技术，利用现有MLLM和DeepSeek-R1构建了无需人工标注的20万规模高质量多模态思维链数据集Vision-R1-cold，作为Vision-R1的冷启动初始化数据。为缓解冷启动后过度思考导致的优化难题，我们提出渐进式思维抑制训练（PTST）策略，并采用硬格式化结果奖励函数配合分组相对策略优化（GRPO），在1万规模多模态数学数据集上逐步优化模型学习正确复杂推理过程的能力。综合实验表明，我们的模型在多项多模态数学推理基准上平均提升约6%。Vision-R1-7B在广泛使用的MathVista基准上达到73.5%准确率，仅比领先的推理模型OpenAI O1低0.4%。通过增加RL训练中的多模态数学数据量，Vision-R1-32B和Vision-R1-72B分别在MathVista基准上获得76.4%和78.2%的分数。数据集与代码发布于：https://github.com/Osilly/Vision-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the success of RL in fostering reasoning in LLMs, this work aims to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) through RL, but faces a data scarcity challenge. The method involves first constructing a high-quality, unannotated 200K multimodal Chain-of-Thought dataset (Vision-R1-cold) using existing models for cold-start initialization, and then applying a Progressive Thinking Suppression Training strategy with Group Relative Policy Optimization to refine reasoning on a 10K math dataset. The main experimental results show that the proposed Vision-R1 model achieves an average improvement of ~6% on various multimodal math reasoning benchmarks, with the 7B parameter version reaching 73.5% accuracy on MathVista, close to the leading model, and larger 32B and 72B versions achieving 76.4% and 78.2%, respectively.</div>
<div class="mono" style="margin-top:8px">受强化学习在大型语言模型中成功激发推理能力的启发，本研究旨在通过强化学习提升多模态大语言模型的推理能力，但面临高质量数据缺乏的挑战。方法上，首先利用现有模型构建了一个无需人工标注的20万规模多模态思维链数据集用于冷启动，随后采用渐进式思维抑制训练策略与分组相对策略优化，在一个1万规模的多模态数学数据集上精炼推理过程。主要实验结果表明，所提出的Vision-R1模型在多个多模态数学推理基准上平均提升约6%，其中70亿参数版本在MathVista基准上达到73.5%的准确率，接近领先模型，而320亿和720亿参数版本则分别取得了76.4%和78.2%的成绩。</div>
</details>
</div>
<div class="card">
<div class="title">TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu, Gong Chen, Ziqi Gao, Min Peng</div>
<div class="meta-line">First: 2026-02-05T16:08:36+00:00 · Latest: 2026-02-05T16:08:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05818v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05818v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TKG-Thinker：基于智能体强化学习的时序知识图谱动态推理方法</div>
<div class="mono" style="margin-top:8px">时序知识图谱问答（TKGQA）旨在利用时序知识库回答时间敏感问题。尽管大语言模型（LLM）在TKGQA中展现出巨大潜力，但现有提示策略主要在两方面限制了其效能：一是在复杂时序约束下易产生推理幻觉；二是静态提示缺乏与时序知识图谱（TKG）环境的动态交互优化，限制了模型自主性与泛化能力。为此，我们提出\textbf{TKG-Thinker}——一种具备自主规划与自适应检索能力的新型智能体，专为时序知识图谱推理设计。该模型通过双阶段训练策略实现与TKG的动态多轮交互式深度时序推理：首先采用思维链数据进行监督微调以植入核心规划能力，再通过强化学习阶段利用多维度奖励优化复杂时序约束下的推理策略。在三个开源LLM的基准数据集实验中，TKG-Thinker取得了最先进的性能，并在复杂TKGQA场景中表现出强泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current Large Language Models (LLM) prompting strategies for Temporal Knowledge Graph Question Answering (TKGQA), which suffer from reasoning hallucinations and lack of dynamic interaction with knowledge graphs. The authors propose TKG-Thinker, an agent that uses a dual-training strategy combining Supervised Fine-Tuning with chain-of-thought data and Reinforcement Learning with multi-dimensional rewards to enable autonomous planning and adaptive retrieval for dynamic, multi-turn reasoning over Temporal Knowledge Graphs. Experiments on benchmark datasets demonstrate that TKG-Thinker achieves state-of-the-art performance and shows strong generalization across complex TKGQA settings.</div>
<div class="mono" style="margin-top:8px">本文针对当前大语言模型在时序知识图谱问答中的提示策略存在推理幻觉和缺乏与知识图谱动态交互的局限性，提出了一种名为TKG-Thinker的智能体。该方法采用双重训练策略，结合了基于思维链数据的监督微调和基于多维奖励的强化学习，以赋予模型自主规划和自适应检索能力，从而实现与时序知识图谱的动态多轮推理。在基准数据集上的实验结果表明，TKG-Thinker取得了最先进的性能，并在复杂的时序知识图谱问答场景中展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Selecting Hyperparameters for Tree-Boosting</div>
<div class="meta-line">Authors: Floris Jan Koster, Fabio Sigrist</div>
<div class="meta-line">First: 2026-02-05T15:44:42+00:00 · Latest: 2026-02-05T15:44:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05786v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05786v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>树提升超参数选择</div>
<div class="mono" style="margin-top:8px">树提升是广泛用于表格数据的机器学习技术，但其样本外精度高度依赖多个超参数。本文通过59个回归与分类数据集，实证比较了随机网格搜索、树结构Parzen估计器（TPE）、高斯过程贝叶斯优化（GP-BO）、Hyperband、序列模型算法配置（SMAC）及确定性全网格搜索等主流超参数优化方法。研究发现SMAC方法显著优于其他方法，并进一步观察到：（i）精确调参需超过100次试验；（ii）使用默认超参数会导致模型精度严重不足；（iii）所有考察的超参数均对树提升精度有实质影响，不存在更重要的核心子集；（iv）在回归任务中，通过早停法确定提升迭代次数比将其纳入搜索空间能获得更精确的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the critical impact of hyperparameter selection on the out-of-sample accuracy of tree-boosting models, a popular technique for tabular data. The method involves an empirical comparison of several hyperparameter optimization approaches, including random grid search, TPE, GP-BO, Hyperband, SMAC, and deterministic full grid search, across 59 regression and classification datasets. The main experimental results show that the SMAC method clearly outperforms all others, and key findings indicate that accurate tuning requires over 100 trials, default hyperparameters lead to poor performance, all hyperparameters significantly affect accuracy, and using early stopping for boosting iterations is beneficial for regression tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于超参数选择对树提升模型（一种广泛使用的表格数据技术）样本外精度的关键影响。研究方法是对多种超参数优化方法进行实证比较，包括随机网格搜索、TPE、GP-BO、Hyperband、SMAC和确定性全网格搜索，在59个回归和分类数据集上进行测试。主要实验结果表明，SMAC方法明显优于其他所有方法，关键发现包括：准确调参需要超过100次试验、默认超参数会导致性能不佳、所有超参数都对精度有实质影响，以及在回归任务中使用早停法确定提升迭代次数效果更好。</div>
</details>
</div>
<div class="card">
<div class="title">A Policy Gradient-Based Sequence-to-Sequence Method for Time Series Prediction</div>
<div class="meta-line">Authors: Qi Sima, Xinze Zhang, Yukun Bao, Siyue Yang, Liang Shen</div>
<div class="meta-line">First: 2024-06-14T00:24:29+00:00 · Latest: 2026-02-05T15:42:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.09643v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.09643v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sequence-to-sequence architectures built upon recurrent neural networks have become a standard choice for multi-step-ahead time series prediction. In these models, the decoder produces future values conditioned on contextual inputs, typically either actual historical observations (ground truth) or previously generated predictions. During training, feeding ground-truth values helps stabilize learning but creates a mismatch between training and inference conditions, known as exposure bias, since such true values are inaccessible during real-world deployment. On the other hand, using the model&#x27;s own outputs as inputs at test time often causes errors to compound rapidly across prediction steps. To mitigate these limitations, we introduce a new training paradigm grounded in reinforcement learning: a policy gradient-based method to learn an adaptive input selection strategy for sequence-to-sequence prediction models. Auxiliary models first synthesize plausible input candidates for the decoder, and a trainable policy network optimized via policy gradients dynamically chooses the most beneficial inputs to maximize long-term prediction performance. Empirical evaluations on diverse time series datasets confirm that our approach enhances both accuracy and stability in multi-step forecasting compared to conventional methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略梯度的序列到序列时间序列预测方法</div>
<div class="mono" style="margin-top:8px">基于循环神经网络的序列到序列架构已成为多步时间序列预测的标准选择。在这些模型中，解码器根据上下文输入生成未来值，这些输入通常是实际历史观测值（真实值）或先前生成的预测值。训练时使用真实值有助于稳定学习，但会导致训练与推断条件不匹配（称为暴露偏差），因为实际部署时无法获取真实值。另一方面，在测试阶段使用模型自身输出作为输入往往会导致误差在预测步骤中快速累积。为缓解这些局限，我们提出一种基于强化学习的新训练范式：采用策略梯度方法，为序列到序列预测模型学习自适应输入选择策略。辅助模型首先生成解码器的合理候选输入，通过策略梯度优化的可训练策略网络动态选择最有利的输入，以最大化长期预测性能。在多样化时间序列数据集上的实证评估表明，与传统方法相比，我们的方法在多步预测中同时提升了准确性与稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the exposure bias and error accumulation problems in conventional sequence-to-sequence time series forecasting, where training with ground-truth inputs mismatches inference conditions. The authors propose a reinforcement learning solution: a policy gradient method that learns an adaptive input selection strategy, using auxiliary models to generate candidate inputs and a policy network to dynamically choose the most beneficial ones to optimize long-term prediction. Experiments across diverse datasets demonstrate that this approach improves both accuracy and stability in multi-step forecasting compared to standard methods.</div>
<div class="mono" style="margin-top:8px">本文针对传统序列到序列时间序列预测中存在的曝光偏差和误差累积问题，即训练时使用真实值输入与推理条件不匹配。作者提出了一种基于强化学习的解决方案：采用策略梯度方法学习自适应输入选择策略，利用辅助模型生成候选输入，并通过策略网络动态选择最有利的输入以优化长期预测性能。在多个数据集上的实验表明，与传统方法相比，该方法提高了多步预测的准确性和稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Distributional Reinforcement Learning with Diffusion Bridge Critics</div>
<div class="meta-line">Authors: Shutong Ding, Yimiao Zhou, Ke Hu, Mokai Pan, Shan Zhong, Yanwei Fu, Jingya Wang, Ye Shi</div>
<div class="meta-line">First: 2026-02-05T15:40:14+00:00 · Latest: 2026-02-05T15:40:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05783v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散桥评论家的分布强化学习方法</div>
<div class="mono" style="margin-top:8px">基于扩散的强化学习方法在连续控制任务中展现出广阔前景，但现有研究多聚焦于扩散策略的应用，而扩散评论家尚未得到充分探索。实际上，由于策略优化本质上依赖于评论家，准确的价值估计远比策略表达能力更为重要。此外，鉴于多数强化学习任务的随机性，采用分布模型描述评论家已被证实更为合适。基于此，我们提出一种基于扩散桥评论家的新型分布强化学习方法。该方法直接对Q值的逆累积分布函数进行建模，借助扩散桥强大的分布匹配能力，能够精确捕捉价值分布并避免其坍缩为平凡的高斯分布。同时，我们进一步推导出解析积分公式以解决离散化误差问题，这对价值估计至关重要。据我们所知，这是首次将扩散桥模型应用于评论家的工作。值得注意的是，该方法具备即插即用特性，可集成至多数现有强化学习框架。在MuJoCo机器人控制基准测试中的实验结果表明，该方法优于以往的分布评论家模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the importance of accurate value estimation in reinforcement learning and the need for distributional critics to capture task stochasticity, this paper introduces Diffusion Bridge Critics (DBC), a novel distributional RL method that models the inverse cumulative distribution function of Q-values using a diffusion bridge to prevent collapse into trivial distributions. The method addresses discretization errors via an analytic integral formula and is designed as a plug-and-play component compatible with existing RL frameworks. Experimental results on MuJoCo robot control benchmarks show that DBC outperforms prior distributional critic models.</div>
<div class="mono" style="margin-top:8px">本文的动机在于强化学习中精确价值估计的重要性，以及需要分布化评论家来捕捉任务随机性，为此提出了扩散桥评论家（DBC）这一新颖的分布化强化学习方法，它使用扩散桥建模Q值的逆累积分布函数，以防止坍缩为平凡分布。该方法通过解析积分公式处理离散化误差，并设计为即插即用组件，可与现有强化学习框架集成。在MuJoCo机器人控制基准测试中的实验结果表明，DBC优于先前的分布化评论家模型。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-Domain Offline Policy Adaptation via Selective Transition Correction</div>
<div class="meta-line">Authors: Mengbei Yan, Jiafei Lyu, Shengjie Sun, Zhongjian Qiao, Jingwen Yang, Zichuan Lin, Deheng Ye, Xiu Li</div>
<div class="meta-line">First: 2026-02-05T15:37:29+00:00 · Latest: 2026-02-05T15:37:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05776v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于选择性转移修正的跨领域离线策略自适应</div>
<div class="mono" style="margin-top:8px">在强化学习（RL）中，如何在动态特性不匹配的领域间进行策略自适应仍是一个关键挑战。本文研究跨领域离线强化学习，即通过访问来自相似源领域的离线数据集来增强基于目标领域数据集的策略学习。由于可能存在动态特性不匹配，直接合并两个数据集可能导致次优性能。现有方法通常通过源领域转移过滤或奖励修正来缓解此问题，但这可能导致对宝贵源领域数据的利用不足。为此，我们提出将源领域数据修正为目标领域数据。具体而言，我们利用逆策略模型和奖励模型来修正源转移的动作与奖励，显式实现与目标动态特性的对齐。由于有限数据可能导致模型训练不准确，我们进一步采用前向动态模型来保留比原始转移更匹配目标动态特性的修正样本。基于此，我们提出选择性转移修正（STC）算法，实现对源领域数据的可靠利用以进行策略自适应。在具有动态特性迁移的多种环境中的实验表明，STC相较于现有基线方法取得了更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of adapting reinforcement learning policies across domains with mismatched dynamics, specifically in cross-domain offline RL where a source domain dataset is available alongside a target domain dataset. The motivation is to avoid suboptimal performance from directly merging datasets due to dynamics mismatches, while overcoming limitations of existing methods like filtering or reward modification that underutilize source data. The proposed method, Selective Transition Correction (STC), corrects source domain transitions by using an inverse policy model and a reward model to align actions and rewards with target dynamics, and employs a forward dynamics model to selectively retain corrected samples that better match the target. Experimental results across various environments with dynamics shifts show that STC outperforms existing baselines, demonstrating superior performance in policy adaptation.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中策略在动态不匹配的跨域适应问题展开研究，特别是在跨域离线强化学习场景下，可利用源域数据集辅助目标域策略学习。其动机是避免因动态不匹配直接合并数据集导致的性能下降，并克服现有方法（如过滤或奖励修改）对源域数据利用不足的局限。所提出的方法——选择性转移校正（STC），通过逆策略模型和奖励模型校正源域转移中的动作和奖励以对齐目标动态，并利用前向动态模型选择性保留比原始转移更匹配目标动态的校正样本。在多种动态偏移环境中的实验结果表明，STC优于现有基线方法，在策略适应方面表现出更优的性能。</div>
</details>
</div>
<div class="card">
<div class="title">RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism</div>
<div class="meta-line">Authors: Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai, Jing Long, Tianyun Zhao, Mingxi Luo, Chen Zhou, Yucheng Guo, Qiming Yang, Wanting Xu, Wen Huang, Yunxuan Ma, Hongke Zhao, Likang Wu, Xiaotie Deng, Xi Xiao, Sheng Wen, Yicheng Gong, Junwu Xiong</div>
<div class="meta-line">First: 2026-02-05T15:30:23+00:00 · Latest: 2026-02-05T15:30:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05765v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05765v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#x27;s excellent scalability under most conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RL-VLA$^3$：基于全异步化的强化学习视觉语言动作模型加速框架</div>
<div class="mono" style="margin-top:8px">近年来，视觉-语言-动作模型已成为实现通用具身智能的关键路径，但其训练效率已成为主要瓶颈。尽管现有基于强化学习的训练框架（如RLinf）能提升模型泛化能力，但仍依赖同步执行，导致环境交互、策略生成与模型更新阶段存在严重的资源利用不足和吞吐量限制。为突破这一局限，本文首次提出并实现了一个覆盖环境交互、轨迹生成到策略更新全流程的全异步策略训练框架。通过系统借鉴大模型强化学习中的异步优化思想，本框架设计了多层次解耦架构，包括环境交互与轨迹收集的异步并行化、策略生成的流式执行以及训练更新的解耦调度。我们在多种VLA模型与环境中验证了方法的有效性：在LIBERO基准测试中，相比现有同步策略，框架最高可实现59.25%的吞吐量提升；深度优化解耦策略后，吞吐量增幅可达126.67%。通过消融实验验证了各异步组件的有效性，在8至256张GPU上的扩展律测试表明该方法在多数条件下具备优异的可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the training efficiency bottleneck in Vision-Language-Action (VLA) models for embodied intelligence, where existing synchronous reinforcement learning frameworks cause severe resource underutilization, this paper proposes RL-VLA³, a fully-asynchronous policy training framework. The method systematically decouples and asynchronizes the entire pipeline—including environment interaction, trajectory collection (rollout), and policy updates—inspired by asynchronous optimization in large model RL, employing a multi-level decoupled architecture with streaming execution and decoupled scheduling. Experimental results on the LIBERO benchmark show throughput improvements of up to 59.25% over synchronous strategies, with further optimization achieving up to 126.67% gains, validated through ablation studies and demonstrating excellent scalability across 8 to 256 GPUs.</div>
<div class="mono" style="margin-top:8px">本文针对实现具身智能的视觉-语言-动作（VLA）模型训练效率瓶颈问题，即现有同步强化学习框架导致资源利用率低下，提出了RL-VLA³这一全异步策略训练框架。该方法受大模型强化学习中异步优化思想启发，采用多级解耦架构，将环境交互、轨迹收集和策略更新等全流程异步化，设计了异步并行交互、流式策略生成与解耦调度更新。在LIBERO基准测试中，实验结果表明，相比现有同步策略，该框架实现了高达59.25%的吞吐量提升，深度优化分离策略后更可提升126.67%，并通过消融研究验证了各异步组件的有效性，在8至256个GPU上的扩展性验证显示了其优秀的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Inject: Automated Prompt Injection via Reinforcement Learning</div>
<div class="meta-line">Authors: Xin Chen, Jie Zhang, Florian Tramer</div>
<div class="meta-line">First: 2026-02-05T15:14:46+00:00 · Latest: 2026-02-05T15:14:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05746v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05746v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习注入：基于强化学习的自动化提示词注入攻击</div>
<div class="mono" style="margin-top:8px">提示词注入是大语言模型智能体最关键的漏洞之一，但从优化角度实现高效自动化攻击的研究仍十分有限。现有方法严重依赖人工红队和手工构建的提示词，限制了其扩展性和适应性。我们提出AutoInject——一种强化学习框架，能生成通用、可迁移的对抗性后缀，同时优化攻击成功率与良性任务效用保持。我们的黑盒方法支持基于查询的优化及对未知模型与任务的迁移攻击。仅使用15亿参数的对抗后缀生成器，我们就在AgentDojo基准测试中成功攻破了包括GPT 5 Nano、Claude Sonnet 3.5和Gemini 2.5 Flash在内的前沿系统，为自动化提示词注入研究建立了更强基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the critical vulnerability of prompt injection in LLM agents and the limitations of existing human-dependent, non-scalable attack methods, this paper introduces AutoInject, a reinforcement learning framework designed to automate the generation of universal and transferable adversarial suffixes. The method jointly optimizes for high attack success and preserved utility on benign tasks in a black-box setting, enabling both query-based optimization and transfer attacks to unseen models. Experimental results demonstrate that using only a 1.5B parameter generator, the approach successfully compromises frontier systems like GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a new strong baseline for automated prompt injection research.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大型语言模型智能体中提示注入这一关键安全漏洞，以及现有方法严重依赖人工红队和手工提示、可扩展性与适应性不足的问题。为此，研究提出了AutoInject，一个基于强化学习的框架，旨在自动生成通用且可迁移的对抗性后缀，其方法在优化攻击成功率的同时，确保良性任务上的效用保持，并支持黑盒环境下的查询优化及对未见模型和任务的迁移攻击。主要实验结果表明，仅使用一个15亿参数的生成器，该方法便在AgentDojo基准测试上成功攻破了包括GPT 5 Nano、Claude Sonnet 3.5和Gemini 2.5 Flash在内的前沿系统，为自动化提示注入研究建立了更强的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification</div>
<div class="meta-line">Authors: Taoye Yin, Haoyuan Hu, Yaxin Fan, Xinhao Chen, Xinya Wu, Kai Deng, Kezun Zhang, Feng Wang</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-05T14:49:05+00:00 · Latest: 2026-02-05T14:49:05+00:00</div>
<div class="meta-line">Comments: accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05723v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05723v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过细粒度知识验证缓解金融检索增强生成中的幻觉问题</div>
<div class="mono" style="margin-top:8px">在金融检索增强生成（RAG）系统中，由于金融领域信息的时效性，模型常依赖检索文档生成准确回答。尽管检索文档有助于填补知识空白，但模型生成的回答仍存在与检索信息相矛盾的幻觉现象。为缓解这种不一致性，我们提出一种融合细粒度知识验证的强化学习框架（RLFKV）。该方法将金融回答分解为原子知识单元，评估每个单元的正确性以计算细粒度忠实度奖励。该奖励提供更精确的优化信号，从而提升与检索文档的一致性。此外，为防止奖励欺骗行为（如过度简化的回复），我们引入信息量奖励机制，激励策略模型至少保留与基线模型同等数量的知识单元。在公开金融数据描述（FDD）任务及我们新提出的FDD-ANT数据集上的实验均显示稳定改进，验证了方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the persistent issue of hallucinations in financial Retrieval-Augmented Generation (RAG) systems, where generated responses contradict retrieved documents despite their use for accuracy, this paper introduces a Reinforcement Learning framework with Fine-grained Knowledge Verification (RLFKV). The method decomposes financial responses into atomic knowledge units to compute a fine-grained faithful reward for each unit, providing precise optimization signals to improve alignment with retrieved information; it also incorporates an informativeness reward to prevent overly concise replies by encouraging the retention of knowledge units. Experimental results on the Financial Data Description (FDD) task and a new FDD-ANT dataset show consistent improvements, confirming the approach&#x27;s effectiveness in mitigating hallucinations.</div>
<div class="mono" style="margin-top:8px">针对金融检索增强生成（RAG）系统中生成响应与检索信息相矛盾的幻觉问题，本文提出了一个结合细粒度知识验证的强化学习框架（RLFKV）。该方法将金融响应分解为原子知识单元，通过计算每个单元的细粒度忠实度奖励来提供精确的优化信号，以增强与检索文档的一致性；同时引入信息量奖励以防止回复过于简略，鼓励保留与基础模型相当的知识单元。在公开的金融数据描述（FDD）任务和新提出的FDD-ANT数据集上的实验结果表明，该方法持续提升了性能，有效缓解了幻觉现象。</div>
</details>
</div>
<div class="card">
<div class="title">Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification</div>
<div class="meta-line">Authors: Tianyi Wang, Long Li, Hongcan Guo, Yibiao Chen, Yixia Li, Yong Wang, Yun Chen, Guanhua Chen</div>
<div class="meta-line">First: 2026-02-05T14:41:57+00:00 · Latest: 2026-02-05T14:41:57+00:00</div>
<div class="meta-line">Comments: 17 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05717v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05717v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model&#x27;s full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model&#x27;s high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>锚定策略优化：通过支撑集约束校正缓解探索坍缩</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）日益被视为一种树剪枝机制。然而，我们发现一种称为递归空间收缩的系统性病理现象，这是一种由正向锐化与负向挤压的复合动力学驱动的不可逆坍缩，导致有效备选方案的采样概率趋近于零。虽然KL正则化旨在缓解此问题，但其施加了严格的形状匹配约束，迫使策略模仿参考模型的完整密度分布，从而与正确性所需的锐化过程产生梯度冲突。我们提出锚定策略优化（APO），将范式从全局形状匹配转向支撑集覆盖。通过基于参考模型高置信度支撑集定义安全流形，APO允许为提升效率而进行激进锐化，同时在误差校正阶段选择性调用恢复力以防止坍缩。我们从理论上推导出APO可作为梯度对齐机制来最大化支撑集覆盖，实现重新激活有效分支的弹性恢复。在数学基准测试中的实证评估表明，APO打破了精度-多样性权衡，在显著提升Pass@1指标的同时，恢复了传统策略梯度方法通常丢失的Pass@K多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a systemic pathology in reinforcement learning with verifiable rewards called Recursive Space Contraction, where the policy&#x27;s sampling probability of valid alternatives collapses irreversibly due to combined positive sharpening and negative squeezing dynamics. To mitigate this, the authors propose Anchored Policy Optimization, which shifts the regularization paradigm from global shape matching via KL divergence to ensuring support coverage, defining a safe manifold based on the reference model&#x27;s high-confidence support to allow aggressive sharpening while selectively applying restorative forces during error correction. Experimental results on mathematical benchmarks show that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 scores while restoring the Pass@K diversity typically lost by standard policy gradient methods.</div>
<div class="mono" style="margin-top:8px">本文针对可验证奖励强化学习中的一种系统性病理问题——递归空间收缩，即由于正锐化和负挤压的动态组合导致策略对有效替代方案的采样概率发生不可逆的崩溃。为缓解此问题，作者提出了锚定策略优化方法，将正则化范式从基于KL散度的全局形状匹配转向确保支持覆盖，依据参考模型的高置信度支持定义安全流形，从而允许积极的锐化，同时在纠错时选择性地施加恢复力以防止崩溃。在数学基准上的实验结果表明，APO打破了准确性与多样性的权衡，显著提高了Pass@1分数，并恢复了标准策略梯度方法通常丢失的Pass@K多样性。</div>
</details>
</div>
<div class="card">
<div class="title">DeepAgent: A General Reasoning Agent with Scalable Toolsets</div>
<div class="meta-line">Authors: Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, Zhicheng Dou</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2025-10-24T16:24:01+00:00 · Latest: 2026-02-05T14:08:05+00:00</div>
<div class="meta-line">Comments: Accepted by WWW 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21618v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.21618v3">PDF</a> · <a href="https://github.com/RUC-NLPIR/DeepAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To manage long-horizon interactions, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepAgent：具备可扩展工具集的通用推理智能体</div>
<div class="mono" style="margin-top:8px">大型推理模型已展现出强大的问题解决能力，但现实任务常需外部工具与长程交互。现有智能体框架多遵循预定义流程，限制了自主性与全局任务完成度。本文提出DeepAgent，一种端到端深度推理智能体，可在单一连贯的推理过程中执行自主思考、工具发现与动作执行。为管理长程交互，我们引入自主记忆折叠机制，将过往交互压缩为结构化的情景记忆、工作记忆与工具记忆，在保留关键信息的同时减少误差累积。为实现高效稳定的通用工具使用，我们开发了端到端强化学习策略ToolPO，利用LLM模拟的API并通过工具调用优势归因对工具调用令牌进行细粒度信用分配。在八个基准测试（包括通用工具使用任务（ToolBench、API-Bank、TMDB、Spotify、ToolHop）与下游应用（ALFWorld、WebShop、GAIA、HLE））上的大量实验表明，DeepAgent在标注工具和开放集工具检索场景中均持续优于基线模型。代码与演示见https://github.com/RUC-NLPIR/DeepAgent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for DeepAgent stems from the limitations of existing agent frameworks that rely on predefined workflows, which hinder autonomous and global task completion in real-world scenarios requiring external tools and long-horizon interactions. The method introduces an end-to-end deep reasoning agent that integrates autonomous thinking, tool discovery, and action execution within a single reasoning process, featuring an autonomous memory folding mechanism to compress past interactions into structured memories and a reinforcement learning strategy called ToolPO for efficient tool use via LLM-simulated APIs and fine-grained credit assignment. Main experimental results show that DeepAgent consistently outperforms baselines across eight benchmarks, including general tool-use tasks like ToolBench and downstream applications such as ALFWorld, in both labeled-tool and open-set tool retrieval scenarios.</div>
<div class="mono" style="margin-top:8px">DeepAgent的动机源于现有智能体框架依赖预定义工作流的局限性，这阻碍了在需要外部工具和长程交互的现实场景中实现自主和全局任务完成。该方法提出了一种端到端的深度推理智能体，将自主思考、工具发现和行动执行集成在单一推理过程中，通过自主记忆折叠机制将过往交互压缩为结构化记忆，并采用名为ToolPO的强化学习策略，利用LLM模拟API和细粒度信用分配来高效使用工具。主要实验结果表明，在包括ToolBench等通用工具使用任务和ALFWorld等下游应用在内的八个基准测试中，DeepAgent在标记工具和开放集工具检索场景下均持续优于基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Variable Search Stepsize for Randomized Local Search in Multi-Objective Combinatorial Optimization</div>
<div class="meta-line">Authors: Xuepeng Ren, Maocai Wang, Guangming Dai, Zimin Liang, Qianrong Liu, Shengxiang Yang, Miqing Li</div>
<div class="meta-line">First: 2026-02-05T13:59:05+00:00 · Latest: 2026-02-05T13:59:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05675v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Over the past two decades, research in evolutionary multi-objective optimization has predominantly focused on continuous domains, with comparatively limited attention given to multi-objective combinatorial optimization problems (MOCOPs). Combinatorial problems differ significantly from continuous ones in terms of problem structure and landscape. Recent studies have shown that on MOCOPs multi-objective evolutionary algorithms (MOEAs) can even be outperformed by simple randomised local search. Starting with a randomly sampled solution in search space, randomised local search iteratively draws a random solution (from an archive) to perform local variation within its neighbourhood. However, in most existing methods, the local variation relies on a fixed neighbourhood, which limits exploration and makes the search easy to get trapped in local optima. In this paper, we present a simple yet effective local search method, called variable stepsize randomized local search (VS-RLS), which adjusts the stepsize during the search. VS-RLS transitions gradually from a broad, exploratory search in the early phases to a more focused, fine-grained search as the search progresses. We demonstrate the effectiveness and generalizability of VS-RLS through extensive evaluations against local search and MOEAs methods on diverse MOCOPs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多目标组合优化中随机局部搜索的变步长策略</div>
<div class="mono" style="margin-top:8px">过去二十年间，进化多目标优化研究主要集中于连续域，对多目标组合优化问题的关注相对有限。组合问题在问题结构与解空间特征上与连续问题存在显著差异。近期研究表明，在多目标组合优化问题上，简单的随机局部搜索甚至能超越多目标进化算法。随机局部搜索从搜索空间中随机采样初始解，迭代地从存档中随机选取解进行邻域内的局部变异。然而，现有方法大多依赖固定邻域进行局部变异，这限制了探索能力且易使搜索陷入局部最优。本文提出一种简洁高效的局部搜索方法——变步长随机局部搜索，通过在搜索过程中动态调整步长，实现从早期广泛探索到后期精细搜索的渐进过渡。通过对多种多目标组合优化问题开展局部搜索与多目标进化算法的对比实验，验证了该方法的有效性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited attention given to multi-objective combinatorial optimization problems (MOCOPs) compared to continuous domains, and the observation that simple randomized local search can outperform more complex multi-objective evolutionary algorithms on such problems, this paper introduces a variable stepsize randomized local search (VS-RLS) method. The method addresses the limitation of fixed neighbourhoods in existing local search by dynamically adjusting the stepsize, transitioning from broad exploration early on to fine-grained exploitation later. Experimental evaluations across diverse MOCOPs demonstrate that VS-RLS is effective and generalizable, outperforming both standard local search and MOEAs.</div>
<div class="mono" style="margin-top:8px">本文的动机在于多目标组合优化问题相较于连续域问题研究不足，且简单随机局部搜索在此类问题上可能优于复杂多目标进化算法。为此，论文提出了一种变步长随机局部搜索方法，通过动态调整搜索步长，克服了固定邻域搜索易陷入局部最优的限制，实现了从早期广泛探索到后期精细开发的过渡。在多种多目标组合优化问题上的广泛实验表明，该方法具有有效性和通用性，性能优于传统局部搜索和多目标进化算法。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Anthony Kobanda, Rémy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer</div>
<div class="meta-line">First: 2024-12-19T14:00:03+00:00 · Latest: 2026-02-05T13:36:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.14865v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.14865v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略的层次子空间：面向持续离线强化学习</div>
<div class="mono" style="margin-top:8px">本文研究持续强化学习场景，要求智能体在不断适应新任务的同时保持已习得技能，核心挑战在于避免遗忘历史知识并确保随任务数量增长的可扩展性。这类问题在自主机器人与视频游戏仿真中尤为突出，特别是在拓扑结构或运动学特性易变的导航任务中。为此，我们提出HiSPO——一种专为基于离线数据的导航场景持续学习设计的层次化框架。该方法利用神经网络构建的策略子空间，在保留已有知识的同时实现对新任务的灵活高效适应。通过严谨的实验研究，我们在经典MuJoCo迷宫环境与复杂类游戏导航仿真中验证了方法的有效性，其在传统持续学习指标（尤其是内存使用效率与适应能力）上均展现出竞争优势与良好适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of continual offline reinforcement learning, where an agent must adapt to new tasks without forgetting previously learned skills, a critical issue in domains like autonomous robotics and video game navigation. The authors propose HiSPO, a hierarchical framework that utilizes distinct policy subspaces within neural networks to facilitate flexible adaptation to new tasks while preserving past knowledge. Experimental results in MuJoCo maze environments and complex video game simulations demonstrate that HiSPO achieves competitive performance, with notable efficiency in memory usage and adaptability according to standard continual learning metrics.</div>
<div class="mono" style="margin-top:8px">本文针对持续离线强化学习中的挑战，即智能体需适应新任务而不遗忘已学技能，这在自主机器人和视频游戏导航等领域尤为关键。作者提出了HiSPO这一分层框架，利用神经网络中的不同策略子空间来实现对新任务的灵活适应，同时保留已有知识。在MuJoCo迷宫环境和复杂视频游戏模拟中的实验结果表明，HiSPO在性能上具有竞争力，尤其在内存使用效率和适应性方面，符合经典持续学习指标的要求。</div>
</details>
</div>
<div class="card">
<div class="title">UAV Trajectory Optimization via Improved Noisy Deep Q-Network</div>
<div class="meta-line">Authors: Zhang Hengyu, Maryam Cheraghy, Liu Wei, Armin Farhadi, Meysam Soltanpour, Zhong Zhuoqing</div>
<div class="meta-line">First: 2026-02-05T13:23:47+00:00 · Latest: 2026-02-05T13:23:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05644v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05644v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes an Improved Noisy Deep Q-Network (Noisy DQN) to enhance the exploration and stability of Unmanned Aerial Vehicle (UAV) when applying deep reinforcement learning in simulated environments. This method enhances the exploration ability by combining the residual NoisyLinear layer with an adaptive noise scheduling mechanism, while improving training stability through smooth loss and soft target network updates. Experiments show that the proposed model achieves faster convergence and up to $+40$ higher rewards compared to standard DQN and quickly reach to the minimum number of steps required for the task 28 in the 15 * 15 grid navigation environment set up. The results show that our comprehensive improvements to the network structure of NoisyNet, exploration control, and training stability contribute to enhancing the efficiency and reliability of deep Q-learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于改进噪声深度Q网络的无人机轨迹优化</div>
<div class="mono" style="margin-top:8px">本文提出一种改进的噪声深度Q网络（Noisy DQN），用于增强无人机在模拟环境中应用深度强化学习时的探索能力与稳定性。该方法通过将残差噪声线性层与自适应噪声调度机制相结合来提升探索能力，同时通过平滑损失函数和柔性目标网络更新提高训练稳定性。实验表明，在15×15网格导航环境设置中，所提模型相比标准DQN实现了更快的收敛速度，奖励值提升最高达+40，并能快速达到任务28所需的最小步数。结果表明，我们对NoisyNet网络结构、探索控制和训练稳定性的综合改进，有助于提升深度Q学习的效率与可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to enhance exploration and stability in deep reinforcement learning for UAV trajectory optimization. The method introduces an Improved Noisy Deep Q-Network that combines a residual NoisyLinear layer with adaptive noise scheduling to boost exploration, alongside smooth loss and soft target network updates to improve training stability. Experimental results in a simulated 15x15 grid navigation environment demonstrate that the model achieves faster convergence, up to +40 higher rewards compared to standard DQN, and quickly reaches the minimum required steps for the task, indicating improved efficiency and reliability.</div>
<div class="mono" style="margin-top:8px">本文旨在提升深度强化学习在无人机轨迹优化中的探索能力和稳定性。方法提出了一种改进的噪声深度Q网络，通过结合残差噪声线性层和自适应噪声调度机制来增强探索，同时采用平滑损失和软目标网络更新以提高训练稳定性。在模拟的15x15网格导航环境中的实验结果表明，该模型相比标准DQN实现了更快的收敛速度、高达+40的奖励提升，并能快速达到任务所需的最小步数，从而验证了其在效率和可靠性上的综合改进。</div>
</details>
</div>
<div class="card">
<div class="title">Rewards as Labels: Revisiting RLVR from a Classification Perspective</div>
<div class="meta-line">Authors: Zepeng Zhai, Meilin Chen, Jiaxuan Zhao, Junlang Qian, Lei Shen, Yuan Lu</div>
<div class="meta-line">First: 2026-02-05T13:11:36+00:00 · Latest: 2026-02-05T13:11:36+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05630v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05630v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励作为标签：从分类视角重审可验证奖励强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）近期通过提供明确的规则监督，显著提升了大型语言模型在复杂推理任务中的能力。在RLVR方法中，GRPO及其变体取得了优异的实证性能。尽管成果显著，我们发现这些方法存在正样本梯度错配与负样本梯度主导问题，导致策略更新效率低下且非最优。为解决这些问题，我们提出“奖励作为标签”（REAL）框架，将可验证奖励重新定义为分类标签而非标量权重，从而将策略优化重构为分类问题。在此基础上，我们进一步引入锚定逻辑值以增强策略学习。分析表明，REAL能产生单调有界的梯度加权机制，实现回合间梯度分配的平衡，有效缓解上述错配问题。在数学推理基准上的大量实验表明，REAL提升了训练稳定性，并持续超越GRPO及DAPO等强效变体。在15亿参数模型中，REAL的Pass@1平均指标较DAPO提升6.7%。该优势在70亿参数模型中进一步扩大，REAL分别超越DAPO和GSPO达6.2%和1.7%。值得注意的是，即使采用基础二元交叉熵损失，REAL仍保持稳定，平均性能超越DAPO达4.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses inefficiencies in Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO, identifying issues of Gradient Misassignment in Positives and Gradient Domination in Negatives that lead to suboptimal policy updates. The authors propose Rewards as Labels (REAL), a novel framework that reinterprets verifiable rewards as categorical labels instead of scalar weights, thereby reformulating policy optimization as a classification problem and introducing anchor logits to enhance learning. Experimental results on mathematical reasoning benchmarks demonstrate that REAL improves training stability and outperforms strong baselines, achieving average Pass@1 improvements of 6.7% over DAPO on a 1.5B model, with gains scaling to 6.2% and 1.7% over DAPO and GSPO respectively on a 7B model.</div>
<div class="mono" style="margin-top:8px">本文针对可验证奖励强化学习方法中的低效问题，如GRPO及其变体存在的正样本梯度错配和负样本梯度主导，提出了名为REAL的新框架。该方法将可验证奖励重新定义为分类标签而非标量权重，从而将策略优化重构为分类问题，并引入锚定逻辑值以增强学习。在数学推理基准上的实验表明，REAL提高了训练稳定性并显著优于现有方法，在1.5B模型上平均Pass@1比DAPO提升6.7%，在7B模型上继续领先DAPO和GSPO分别达6.2%和1.7%。</div>
</details>
</div>
<div class="card">
<div class="title">Mode-Dependent Rectification for Stable PPO Training</div>
<div class="meta-line">Authors: Mohamad Mohamad, Francesco Ponzio, Xavier Descombes</div>
<div class="meta-line">First: 2026-02-05T12:54:19+00:00 · Latest: 2026-02-05T12:54:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05619v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05619v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mode-dependent architectural components (layers that behave differently during training and evaluation, such as Batch Normalization or dropout) are commonly used in visual reinforcement learning but can destabilize on-policy optimization. We show that in Proximal Policy Optimization (PPO), discrepancies between training and evaluation behavior induced by Batch Normalization lead to policy mismatch, distributional drift, and reward collapse. We propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO under mode-dependent layers without architectural changes. Experiments across procedurally generated games and real-world patch-localization tasks demonstrate that MDR consistently improves stability and performance, and extends naturally to other mode-dependent layers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模式依赖校正的稳定PPO训练方法</div>
<div class="mono" style="margin-top:8px">模式依赖架构组件（如批归一化或丢弃层等在训练与评估阶段表现不同的层）在视觉强化学习中广泛应用，但可能破坏同策略优化的稳定性。本文证明，在近端策略优化（PPO）中，批归一化引起的训练-评估行为差异会导致策略失配、分布漂移和奖励崩溃。我们提出模式依赖校正（MDR）——一种轻量级双阶段训练流程，可在不改变架构的前提下稳定模式依赖层下的PPO训练。在程序生成游戏和现实世界补丁定位任务中的实验表明，MDR能持续提升训练稳定性与性能，并可自然扩展至其他模式依赖层。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the instability in Proximal Policy Optimization (PPO) training caused by mode-dependent layers like Batch Normalization, which behave differently during training and evaluation, leading to policy mismatch and reward collapse. The authors propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO without modifying the network architecture. Experimental results on procedurally generated games and real-world patch-localization tasks show that MDR consistently enhances training stability and improves performance, and it generalizes effectively to other mode-dependent layers.</div>
<div class="mono" style="margin-top:8px">本文针对在视觉强化学习中常用的模式依赖层（如批归一化）导致近端策略优化训练不稳定的问题，这些层在训练和评估时行为差异会引发策略失配和奖励崩溃。作者提出了模式依赖校正方法，这是一种轻量级的双阶段训练流程，无需改变网络架构即可稳定PPO训练。在程序生成游戏和现实世界补丁定位任务上的实验表明，该方法能持续提升训练稳定性和性能，并可自然扩展到其他模式依赖层。</div>
</details>
</div>
<div class="card">
<div class="title">Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation</div>
<div class="meta-line">Authors: Zhiqi Yu, Zhangquan Chen, Mengting Liu, Heye Zhang, Liangqiong Qu</div>
<div class="meta-line">First: 2026-02-05T11:07:14+00:00 · Latest: 2026-02-05T11:07:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05548v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示隐式优势对称性：为何GRPO在探索与难度适应中面临困境</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR），尤其是GRPO，已成为激发大语言模型推理能力的标准方法。然而，其在探索效率和难度适应性方面的表现仍是待解难题。本研究指出，这些瓶颈源于组间相对优势估计（GRAE）固有的隐式优势对称性。该对称性引发两个关键局限：（i）在组层面，正确与错误轨迹间严格的权重对称性使未采样动作的对数概率保持不变，从而阻碍了对新颖正确解的探索；（ii）在样本层面，算法隐式优先处理中等难度样本，未能适应难度聚焦的非平稳需求。通过受控实验，我们揭示这种对称特性并非最优，并得出两个关键洞见：（i）非对称抑制正确轨迹的优势能促进必要探索；（ii）通过类课程学习策略——初期优先处理简单样本再逐步转向复杂样本——可最大化学习效率。基于这些发现，我们提出非对称GRAE（A-GRAE），动态调节探索激励与样本难度聚焦。在七个基准测试上的实验表明，A-GRAE能持续提升GRPO及其变体在大语言模型和多模态大语言模型上的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the exploration and difficulty adaptation inefficiencies in GRPO, a prominent method for Reinforcement Learning with Verifiable Rewards (RLVR) in LLM reasoning. The authors identify the root cause as an implicit advantage symmetry in the Group Relative Advantage Estimation (GRAE) component, which limits exploration by not updating unsampled action logits and fails to adaptively prioritize samples of varying difficulty. Through controlled experiments, they demonstrate that asymmetrically suppressing advantages for correct trajectories promotes better exploration and that a curriculum-like focus, transitioning from simple to complex samples, enhances learning. Motivated by these insights, they propose Asymmetric GRAE (A-GRAE), a method that dynamically adjusts exploration incentives and difficulty focus, and show its consistent improvements over GRPO and its variants across seven benchmarks for both LLMs and MLLMs.</div>
<div class="mono" style="margin-top:8px">本文研究了GRPO方法在大型语言模型推理的强化学习可验证奖励框架中，存在的探索与难度适应效率低下的问题。作者指出其根本原因在于组相对优势估计中隐含的优势对称性，这种对称性限制了探索能力，无法更新未采样动作的对数概率，且不能自适应地优先处理不同难度的样本。通过受控实验，他们证明非对称地抑制正确轨迹的优势可以促进更好的探索，而采用从简单样本逐渐过渡到复杂样本的课程式学习策略能提升学习效率。基于这些发现，他们提出了非对称组相对优势估计方法，该方法能动态调整探索激励和难度关注焦点，并在七个基准测试中，针对大型语言模型和多模态大模型，一致地改进了GRPO及其变体的性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Framework for Rethinking Policy Divergence Measures in GRPO</div>
<div class="meta-line">Authors: Qingyuan Wu, Yuhui Wang, Simon Sinong Zhan, Yanning Dai, Shilong Deng, Sarra Habchi, Qi Zhu, Matthias Gallé, Chao Huang</div>
<div class="meta-line">First: 2026-02-05T09:56:16+00:00 · Latest: 2026-02-05T09:56:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05494v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05494v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GRPO中策略差异度量的统一框架重构</div>
<div class="mono" style="margin-top:8px">基于验证奖励的强化学习已成为提升大语言模型推理能力的关键范式。现有GRPO及其变体等RLVR方法多通过似然比截断约束策略差异以确保稳定更新。本文提出统一截断框架，以广义策略差异概念统摄现有方法，涵盖似然比与KL散度并扩展至其他度量。该框架为系统分析不同策略差异度量如何影响探索与性能提供理论依据。我们进一步提出KL3估计器——一种方差缩减的KL散度蒙特卡洛估计器，作为关键策略差异约束。理论证明基于KL3的约束在数学上等效于非对称比率截断，能将概率质量重新分配至高置信度动作，在保持GRPO类方法简洁性的同时增强探索能力。数学推理基准实验表明，将KL3估计器融入GRPO可同步提升训练稳定性与最终性能，印证了策略优化中理论化策略差异约束的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to systematically understand and improve policy divergence constraints in reinforcement learning with verified reward (RLVR) methods like GRPO, which are crucial for enhancing large language models&#x27; reasoning. The authors propose a unified clipping framework that generalizes existing methods by incorporating various policy divergence measures, including likelihood ratios and KL divergences, and they specifically identify and analyze the KL3 estimator as a variance-reduced Monte Carlo estimator for KL divergence. The main experimental results on mathematical reasoning benchmarks show that integrating the KL3 estimator into GRPO leads to improved training stability and final performance, validating the framework&#x27;s effectiveness in promoting exploration while maintaining simplicity.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要系统性地理解和改进基于验证奖励的强化学习（RLVR）方法（如GRPO）中的策略差异约束，这些方法对提升大语言模型的推理能力至关重要。作者提出了一个统一的裁剪框架，通过纳入包括似然比和KL散度在内的多种策略差异度量来泛化现有方法，并特别将KL3估计器识别为一种方差减少的KL散度蒙特卡洛估计器进行分析。在数学推理基准上的主要实验结果表明，将KL3估计器整合到GRPO中能提高训练稳定性和最终性能，验证了该框架在保持方法简洁性的同时促进探索的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Log2Motion: Biomechanical Motion Synthesis from Touch Logs</div>
<div class="meta-line">Authors: Michał Patryk Miazga, Hannah Bussmann, Antti Oulasvirta, Patrick Ebel</div>
<div class="meta-line">First: 2026-01-28T21:04:19+00:00 · Latest: 2026-02-05T09:24:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21043v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21043v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Log2Motion：基于触摸日志的生物力学运动合成</div>
<div class="mono" style="margin-top:8px">移动设备的大规模触摸数据难以揭示其背后的交互过程。尽管生物力学仿真能阐明运动控制机制，但尚未应用于触摸交互研究。为填补这一空白，我们提出一项新颖的计算任务：直接从日志合成合理运动。核心创新在于采用强化学习驱动的肌肉骨骼正向仿真，生成与触摸日志事件一致的生物力学合理运动序列。通过将软件模拟器集成至物理仿真环境，实现了生物力学模型对真实应用的实时操控。Log2Motion能够从触摸日志合成丰富的用户运动数据，包括动作、速度、精度及用力程度的估计。我们通过动作捕捉实验数据与既有研究对比验证生成运动的合理性，并在大规模数据集中展示了该方法的有效性。生物力学运动合成为理解日志数据提供了新途径，揭示了触摸交互背后的人体工学与运动控制机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the abundance of touch log data that lacks insight into the physical interactions that produced it, this paper introduces Log2Motion, a method to synthesize biomechanically plausible human motion directly from touchscreen event logs. The core method employs a reinforcement learning-driven musculoskeletal forward simulation integrated with a software emulator within a physics simulator, enabling biomechanical models to interact with real applications in real-time. Experimental validation against motion capture data and prior findings demonstrates that the synthesized movements are plausible and can estimate metrics like motion trajectory, speed, accuracy, and effort, offering a novel approach to understanding the ergonomics and motor control in large-scale touch interaction datasets.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决触控日志数据丰富但无法揭示其背后物理交互过程的问题，提出了Log2Motion方法，用于直接从触摸屏事件日志合成生物力学上合理的人体运动。该方法的核心是结合强化学习的肌肉骨骼前向模拟，并通过软件模拟器集成到物理仿真器中，使生物力学模型能够实时操作真实应用程序。实验通过将合成运动与动作捕捉数据及先前研究结果进行比较，验证了生成运动的合理性，并能估计运动轨迹、速度、准确性和努力程度等指标，为大规模触控交互数据中的工效学和运动控制理解提供了新途径。</div>
</details>
</div>
<div class="card">
<div class="title">ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation</div>
<div class="meta-line">Authors: Yiwen Duan, Jing Ye, Xinpei Zhao</div>
<div class="meta-line">First: 2026-02-05T09:20:23+00:00 · Latest: 2026-02-05T09:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05472v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05472v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALIVE：通过对抗学习与指导性言语评估唤醒大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）实现专家级推理能力的探索长期受制于顽固的“奖励瓶颈”：传统强化学习（RL）依赖标量奖励机制，这种机制扩展成本高昂、跨领域适应性脆弱，且对解决方案的内在逻辑缺乏洞察。这种对外部贫乏信号的依赖阻碍了模型对推理原则形成深刻、自洽的理解。本文提出ALIVE（基于指导性言语评估的对抗学习），一种无需人工干预的对齐框架，它超越标量奖励优化，转向内在推理能力的习得。ALIVE以“认知协同”原则为基础，将问题提出、求解与评判统一于单一策略模型中，从而内化正确性的逻辑框架。通过将对抗学习与指导性言语反馈相结合，ALIVE使模型能够直接从原始语料中内化评估标准，有效将外部评判转化为内生的推理能力。在数学推理、代码生成和通用逻辑推理基准上的实证评估表明，ALIVE能持续缓解奖励信号的局限性。在相同数据和算力条件下，该框架实现了准确率提升、跨领域泛化能力显著增强以及更高的自我纠错率。这些结果表明，推理三元组（提出-求解-评判）形成了能力增长的自我维持轨迹，使ALIVE成为无需人工监督、可扩展的通用推理对齐基础框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional reinforcement learning&#x27;s scalar rewards, which are costly, brittle, and blind to solution logic, this paper introduces ALIVE, a framework that integrates adversarial learning with instructive verbal feedback to internalize reasoning principles. The method unifies problem posing, solving, and judging within a single model, enabling it to learn evaluative criteria directly from raw corpora without external supervision. Experimental results across mathematical reasoning, code generation, and logical inference benchmarks show that ALIVE achieves higher accuracy, better cross-domain generalization, and improved self-correction rates compared to traditional approaches, demonstrating scalable reasoning alignment.</div>
<div class="mono" style="margin-top:8px">针对传统强化学习依赖标量奖励所导致的成本高、鲁棒性差且无法理解解决方案内在逻辑的问题，本文提出了ALIVE框架，通过对抗性学习和指导性语言反馈使模型内化推理原则。该方法将问题提出、解决和评判统一于单一策略模型中，使其能够直接从原始语料中学习评估标准，无需外部监督。在数学推理、代码生成和逻辑推理基准上的实验结果表明，ALIVE在相同数据和计算条件下实现了更高的准确率、显著的跨领域泛化能力以及更强的自我纠正能力，为无需人工监督的通用推理对齐提供了可扩展的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Optimization is Not Enough: Why Problem Formulation Deserves Equal Attention</div>
<div class="meta-line">Authors: Iván Olarte Rodríguez, Gokhan Serhat, Mariusz Bujny, Fabian Duddeck, Thomas Bäck, Elena Raponi</div>
<div class="meta-line">First: 2026-02-05T09:15:19+00:00 · Latest: 2026-02-05T09:15:19+00:00</div>
<div class="meta-line">Comments: 25 pages, 11 figures, 4 table, submitted at Conference on Evolutionary Computation, Theory and Applications (ECTA) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05466v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05466v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Black-box optimization is increasingly used in engineering design problems where simulation-based evaluations are costly and gradients are unavailable. In this context, the optimization community has largely analyzed algorithm performance in context-free setups, while not enough attention has been devoted to how problem formulation and domain knowledge may affect the optimization outcomes. We address this gap through a case study in the topology optimization of laminated composite structures, formulated as a black-box optimization problem. Specifically, we consider the design of a cantilever beam under a volume constraint, intending to minimize compliance while optimizing both the structural topology and fiber orientations. To assess the impact of problem formulation, we explicitly separate topology and material design variables and compare two strategies: a concurrent approach that optimizes all variables simultaneously without leveraging physical insight, and a sequential approach that optimizes variables of the same nature in stages. Our results show that context-agnostic strategies consistently lead to suboptimal or non-physical designs. In contrast, the sequential strategy yields better-performing and more interpretable solutions. These findings underscore the value of incorporating, when available, domain knowledge into the optimization process and motivate the development of new black-box benchmarks that reward physically informed and context-aware optimization strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>优化并非万能：为何问题建模同样值得重视</div>
<div class="mono" style="margin-top:8px">黑箱优化在基于仿真的评估成本高昂且梯度不可得的工程设计问题中应用日益广泛。在此背景下，优化领域的研究大多聚焦于无上下文环境下的算法性能分析，而对问题建模与领域知识如何影响优化结果的关注不足。本文以层合复合材料结构的拓扑优化为例，将其构建为黑箱优化问题，以填补这一研究空白。具体而言，我们以体积约束下的悬臂梁设计为研究对象，旨在最小化柔度并同时优化结构拓扑与纤维取向。为评估问题建模的影响，我们明确分离拓扑变量与材料设计变量，并比较两种策略：一种是同时优化所有变量且不利用物理洞察的并行方法，另一种是分阶段优化同类变量的顺序方法。结果表明，无视上下文的策略始终导致次优或非物理的设计；而顺序策略则能产生性能更优、可解释性更强的解。这些发现强调了在优化过程中融入领域知识（若可获得）的价值，并激励开发能奖励具备物理洞察与上下文感知的优化策略的新型黑箱基准测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper argues that in black-box optimization for engineering design, where simulations are expensive and gradients are unavailable, the community has overly focused on algorithm performance in isolation while neglecting the critical role of problem formulation informed by domain knowledge. To demonstrate this, the authors conduct a case study on the topology optimization of laminated composite cantilever beams, comparing a concurrent optimization strategy that treats all design variables simultaneously without physical insight against a sequential strategy that optimizes variables of the same nature in stages. The experimental results reveal that the context-agnostic concurrent approach consistently produces suboptimal or non-physical designs, whereas the sequential strategy yields superior, better-performing, and more interpretable solutions, highlighting the necessity of integrating domain knowledge into the optimization process.</div>
<div class="mono" style="margin-top:8px">本文指出，在仿真成本高昂且梯度不可用的工程设计的黑盒优化中，学术界过度关注算法本身的性能，而忽视了基于领域知识的问题表述的关键作用。为证明这一点，作者以层合复合材料悬臂梁的拓扑优化为例进行研究，比较了两种策略：一种是不利用物理洞察、同时优化所有设计变量的并发方法，另一种是按变量性质分阶段优化的顺序方法。实验结果表明，不考虑背景的并发方法持续产生次优或不切实际的设计，而顺序策略则能生成性能更优、更易解释的解决方案，这强调了将领域知识融入优化过程的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR</div>
<div class="meta-line">Authors: Jiaying Zhang, Lei Shi, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He</div>
<div class="meta-line">First: 2026-01-14T10:41:34+00:00 · Latest: 2026-02-05T09:12:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09361v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09361v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoRA：面向RLVR的几何感知低秩自适应方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）对推进大规模推理模型至关重要。然而，现有参数高效方法（如PiSSA和MiLoRA）专为监督微调（SFT）设计，未考虑RLVR特有的优化动态与几何结构。直接应用这些方法会导致谱崩溃和优化不稳定，严重限制模型性能。同时，利用更新稀疏性的替代方法因非结构化计算在现代硬件上面临显著效率瓶颈。为解决这些问题，我们提出GeoRA（几何感知低秩自适应），该方法利用RL更新子空间的各向异性与可压缩特性。GeoRA通过在几何约束子空间内进行奇异值分解（SVD）提取主方向来初始化适配器，同时冻结残差分量。此方法保留了预训练几何结构，并通过稠密算子实现高效GPU计算。在Qwen和Llama上的实验表明，GeoRA缓解了几何失准导致的优化瓶颈，在关键数学基准测试中持续超越现有低秩基线，达到最先进（SOTA）水平。此外，GeoRA在领域外任务中展现出优异的泛化能力与抗灾难性遗忘鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of existing parameter-efficient fine-tuning methods like PiSSA and MiLoRA, which are designed for supervised fine-tuning and fail to account for the unique optimization dynamics and geometric structures in Reinforcement Learning with Verifiable Rewards (RLVR), leading to spectral collapse and instability. To address this, the authors propose GeoRA, a geometry-aware low-rank adaptation method that initializes adapters by extracting principal directions via Singular Value Decomposition within a constrained subspace, preserving pre-trained geometric structures and enabling efficient GPU computation. Experimental results on models such as Qwen and Llama show that GeoRA mitigates optimization bottlenecks, outperforms established low-rank baselines on mathematical benchmarks with state-of-the-art performance, and demonstrates superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有参数高效微调方法如PiSSA和MiLoRA专为监督微调设计，无法适应可验证奖励强化学习（RLVR）中独特的优化动态和几何结构，导致谱崩溃和不稳定性。为此，作者提出GeoRA，一种几何感知的低秩适应方法，通过在几何约束子空间中使用奇异值分解提取主方向来初始化适配器，从而保留预训练几何结构并实现高效的GPU计算。在Qwen和Llama等模型上的实验结果表明，GeoRA缓解了由几何错位引起的优化瓶颈，在关键数学基准测试中超越了现有低秩基线，取得了最先进的性能，并在域外任务中表现出优异的泛化能力和对灾难性遗忘的抵抗力。</div>
</details>
</div>
<div class="card">
<div class="title">When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL</div>
<div class="meta-line">Authors: Jan Malte Töpperwien, Aditya Mohan, Marius Lindauer</div>
<div class="meta-line">First: 2026-02-05T09:08:17+00:00 · Latest: 2026-02-05T09:08:17+00:00</div>
<div class="meta-line">Comments: 27 pages, 19 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05459v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05459v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\approx$ 20\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习超参数何时呈现良性特征？离线目标条件强化学习研究</div>
<div class="mono" style="margin-top:8px">深度强化学习中的超参数敏感性常被视为不可避免，但其本质究竟源于强化学习问题本身，还是特定训练机制所加剧，尚未明确。本研究在离线目标条件强化学习框架下探讨该问题——该场景中数据分布固定，且可通过数据质量的计划性偏移显式控制非平稳性。我们同时考察了平稳与非平稳机制下的不同数据质量，并涵盖两种代表性算法：HIQL（基于自举时序差分学习）与QRL（拟度量表示学习）。整体而言，即使在受控非平稳条件下，观测到的超参数配置变化鲁棒性也显著高于在线强化学习常见报告水平。当存在适量专家数据（约20%）时，QRL保持广泛稳定的近最优区域，而HIQL则呈现尖锐的最优点且随训练阶段显著漂移。为解释此差异，我们引入目标间梯度对齐诊断方法，发现自举目标会产生更强的破坏性梯度干扰，这与超参数敏感性直接相关。这些结果表明：训练过程中对超参数配置的高敏感性并非强化学习的必然属性，而是由自举机制动态特性所放大，这为设计更鲁棒的算法目标提供了路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether the notorious hyperparameter sensitivity in deep reinforcement learning is an inherent problem or is exacerbated by specific training mechanisms, focusing on offline goal-conditioned RL where data distributions are fixed and non-stationarity can be controlled. The study examines two algorithms, HIQL and QRL, across stationary and non-stationary regimes with varying data qualities, finding substantially greater robustness to hyperparameter changes than typically seen in online RL, especially when modest expert data is present. Key results show that QRL maintains broad stable performance regions, while HIQL exhibits sharp, drifting optima, with analysis via a novel gradient alignment diagnostic linking bootstrapping dynamics to destructive gradient interference and heightened sensitivity, suggesting pathways for more robust algorithm design.</div>
<div class="mono" style="margin-top:8px">本文研究了深度强化学习中超参数敏感性的根源，探讨其是固有难题还是由特定训练机制加剧，聚焦于离线目标条件强化学习，其中数据分布固定且非平稳性可受控调节。研究在静态和非静态机制下检验了HIQL和QRL两种算法在不同数据质量下的表现，发现其对超参数变化的鲁棒性显著高于在线强化学习，尤其在少量专家数据存在时。主要实验结果表明，QRL保持广泛稳定的近优区域，而HIQL则呈现尖锐且随训练阶段漂移的最优点，通过引入的梯度对齐诊断分析，发现自举目标会导致更强的破坏性梯度干扰，直接关联超参数敏感性，这为设计更鲁棒的算法目标提供了途径。</div>
</details>
</div>
<div class="card">
<div class="title">Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options</div>
<div class="meta-line">Authors: Joongkyu Lee, Seouh-won Yi, Min-hwan Oh</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-21T15:11:01+00:00 · Latest: 2026-02-05T09:06:17+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18713v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.18713v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL&#x27;s recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\tilde{O}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter&#x27;s norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $Ω\left( \frac{d}{K \sqrt{T}} \right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越两两比较的偏好强化学习：多选项的效益</div>
<div class="mono" style="margin-top:8px">本研究旨在提升样本效率，探讨在线偏好强化学习（PbRL）。尽管受PbRL近期实证成功（尤其在大型语言模型对齐方面）推动的理论研究日益增多，但现有工作多局限于两两比较。少数近期研究（Zhu等人2023、Mukherjee等人2024、Thekumparampil等人2024）尝试使用多重比较与排序反馈，但其性能保证未能随反馈长度增加而提升，甚至可能恶化，未能充分利用更丰富的信息。为填补此空白，我们采用Plackett-Luce（PL）模型处理动作子集的排序反馈，提出M-AUPO算法——通过最大化所提供子集内的平均不确定性来选择多个动作。理论证明M-AUPO的次优性间隙为$\tilde{O}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$，其中$T$为总轮数，$d$为特征维度，$|S_t|$为第$t$轮子集大小。该结果表明：更大子集直接提升性能，且边界避免了以往多数工作中对未知参数范数的指数依赖这一根本局限。此外，我们建立了近乎匹配的下界$Ω\left( \frac{d}{K \sqrt{T}} \right)$（$K$为最大子集尺寸）。据我们所知，这是PbRL领域首个在排序反馈场景中明确展示样本效率随子集尺寸提升的理论成果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the sample inefficiency in online preference-based reinforcement learning (PbRL) by moving beyond pairwise comparisons to leverage richer ranking feedback from multiple options. The authors propose the M-AUPO algorithm, which selects subsets of actions by maximizing average uncertainty within the offered set and employs the Plackett-Luce model to handle ranking feedback. Theoretical analysis shows that M-AUPO achieves a suboptimality gap that improves with larger subset sizes, avoiding exponential dependence on unknown parameters, and a near-matching lower bound confirms the benefit of using multiple comparisons for enhanced sample efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对在线偏好强化学习中的样本效率问题，提出超越两两比较、利用多选项排序反馈的方法。研究者设计了M-AUPO算法，通过最大化所选动作子集内的平均不确定性来选取动作，并采用Plackett-Luce模型处理排序反馈。理论结果表明，该算法的次优性间隙随子集规模增大而改善，避免了以往工作中对未知参数范数的指数依赖，且近乎匹配的下界证实了使用多选项比较能提升样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy</div>
<div class="meta-line">Authors: Hongze Tan, Zihan Wang, Jianfei Pan, Jinghao Lin, Hao Wang, Yifan Wu, Tao Chen, Zhihang Zheng, Zhihao Tang, Haihua Yang</div>
<div class="meta-line">First: 2025-08-06T11:42:47+00:00 · Latest: 2026-02-05T08:04:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.04349v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.04349v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) is pivotal for enhancing Large Language Model (LLM) reasoning, yet mainstream algorithms such as GRPO and DAPO remain constrained by a coarse-grained credit assignment paradigm, where all tokens within the same response receive the identical reward. In this paper, we propose Dynamic Entropy Weighting, systematically define entropy-based weight ratios $\frac{H_{i,t}}{\sum_{k=1}^{n} H_{k,t}}$ and similar variants to redistribute rewards and get fine-grained rewards through two new algorithms: Group Token Policy Optimization (GTPO), which assigns an entropy-weighted reward to each token and synthesizes token-specific advantage function to drive the model toward optimal path, and the analogous algorithm Sequence-Level GRPO (GRPO-S), which extends this design to the sequence level and exhibits superior stability in long Chain-of-Thought (CoT) reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GTPO与GRPO-S：基于策略熵的令牌级与序列级奖励塑形</div>
<div class="mono" style="margin-top:8px">强化学习（RL）对提升大语言模型（LLM）推理能力至关重要，但主流算法如GRPO和DAPO仍受限于粗粒度信用分配范式——同一响应中的所有令牌均获得相同奖励。本文提出动态熵加权方法，系统定义基于熵的权重比$\frac{H_{i,t}}{\sum_{k=1}^{n} H_{k,t}}$及其变体，通过两种新算法实现奖励重分配与细粒度奖励获取：组令牌策略优化（GTPO）为每个令牌分配熵加权奖励，并综合令牌特定优势函数驱动模型趋向最优路径；序列级GRPO（GRPO-S）将此设计延伸至序列层面，在长链思维（CoT）推理任务中展现出更优的稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of coarse-grained credit assignment in existing reinforcement learning algorithms for large language models, where all tokens in a response receive identical rewards, this paper introduces Dynamic Entropy Weighting to enable fine-grained reward redistribution. The method systematically defines entropy-based weight ratios to develop two new algorithms: Group Token Policy Optimization (GTPO), which assigns entropy-weighted rewards per token and uses token-specific advantages to guide the model, and Sequence-Level GRPO (GRPO-S), which applies a similar design at the sequence level. Experimental results demonstrate that these approaches, particularly GRPO-S, offer superior stability in long Chain-of-Thought reasoning tasks by providing more precise reward signals.</div>
<div class="mono" style="margin-top:8px">针对现有强化学习算法在大语言模型中存在的粗粒度信用分配问题，即同一响应中的所有令牌获得相同奖励，本文提出了动态熵加权方法以实现细粒度奖励再分配。该方法系统定义了基于熵的权重比率，并由此开发了两种新算法：组令牌策略优化（GTPO），为每个令牌分配熵加权奖励并利用令牌特定优势函数引导模型；以及序列级GRPO（GRPO-S），在序列层面应用类似设计。实验结果表明，这些方法能提供更精确的奖励信号，其中GRPO-S在长链思维推理任务中表现出更优的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Generative AI for Intent-Driven Network Management in 6G RAN: A Case Study on the Mamba Model</div>
<div class="meta-line">Authors: Md Arafat Habib, Medhat Elsayed, Yigit Ozcan, Pedro Enrique Iturria-Rivera, Majid Bavand, Melike Erol-Kantarci</div>
<div class="meta-line">First: 2025-08-08T18:06:52+00:00 · Latest: 2026-02-05T07:07:22+00:00</div>
<div class="meta-line">Comments: Paper submitted to IEEE for possible publication. The contents of this paper may change at any time</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06616v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.06616v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the emergence of 6G, mobile networks are becoming increasingly heterogeneous and dynamic, necessitating advanced automation for efficient management. Intent-Driven Networks (IDNs) address this by translating high-level intents into optimization policies. Large Language Models (LLMs) can enhance this process by understanding complex human instructions, enabling adaptive and intelligent automation. Given the rapid advancements in Generative AI (GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated Radio Access Network (RAN) environments is both timely and critical. This article provides such a survey, along with a case study on a selective State-Space Model (SSM)-enabled IDN architecture that integrates GenAI across three key stages: intent processing, intent validation, and intent execution. For the first time in the literature, we propose a hierarchical framework built on Mamba-SSM that introduces GenAI across all stages of the IDN pipeline. We further present a case study demonstrating that the proposed Mamba architecture significantly improves network performance through intelligent automation, surpassing existing IDN approaches. In a multi-cell 5G/6G scenario, the proposed architecture reduces quality of service drift by up to 70%, improves throughput by up to 80 Mbps, and lowers inference time to 60-70 ms, outperforming GenAI, reinforcement learning, and non-machine learning baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向6G无线接入网意图驱动网络管理的生成式人工智能：基于Mamba模型的案例研究</div>
<div class="mono" style="margin-top:8px">随着6G时代的到来，移动网络日益呈现异构化与动态化特征，亟需先进的自动化管理技术。意图驱动网络通过将高层业务意图转化为优化策略来应对这一挑战。大型语言模型能够理解复杂的人类指令，从而增强这一过程的适应性与智能化水平。鉴于生成式人工智能的快速发展，对基于LLM的解耦式无线接入网环境下的IDN架构进行全面综述具有重要时效性与关键意义。本文不仅提供了此类综述，还通过案例研究展示了一种基于选择性状态空间模型的IDN架构，该架构将生成式人工智能整合至意图处理、意图验证与意图执行三大关键阶段。本研究首次在文献中提出基于Mamba-SSM构建的分层框架，实现了生成式人工智能在IDN全流程的贯通。案例研究表明，所提出的Mamba架构通过智能自动化显著提升网络性能：在多小区5G/6G场景中，服务质量漂移降低达70%，吞吐量提升最高80Mbps，推理时间缩短至60-70毫秒，其性能全面超越生成式人工智能、强化学习及非机器学习基线方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for advanced automation in the increasingly complex and dynamic 6G Radio Access Network (RAN), this paper surveys and proposes a Generative AI-enhanced Intent-Driven Network (IDN) architecture to translate high-level human intents into efficient network policies. The method introduces a novel hierarchical framework built on the Mamba State-Space Model (SSM), integrating Generative AI across the three core stages of intent processing, validation, and execution for the first time. In a multi-cell 5G/6G experimental case study, the proposed architecture demonstrates superior performance, reducing quality of service drift by up to 70%, improving throughput by up to 80 Mbps, and achieving faster inference times of 60-70 ms compared to existing Generative AI, reinforcement learning, and non-machine learning baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机源于6G无线接入网日益复杂和动态化，亟需高级自动化管理，为此研究并提出了一个生成式人工智能增强的意图驱动网络架构，旨在将高层人类意图转化为高效的网络策略。该方法首次提出了一个基于Mamba状态空间模型的分层框架，将生成式人工智能集成到意图处理、验证和执行这三个核心阶段。在一个多小区5G/6G场景的案例研究中，所提出的架构展现出卓越性能，与现有的生成式人工智能、强化学习及非机器学习基线方法相比，能将服务质量漂移降低高达70%，吞吐量提升高达80 Mbps，并将推理时间缩短至60-70毫秒。</div>
</details>
</div>
<div class="card">
<div class="title">Variance Reduction Based Experience Replay for Policy Optimization</div>
<div class="meta-line">Authors: Hua Zheng, Wei Xie, M. Ben Feng, Keilung Choy</div>
<div class="meta-line">First: 2026-02-05T06:58:28+00:00 · Latest: 2026-02-05T06:58:28+00:00</div>
<div class="meta-line">Comments: 24 pages, 4 figures. arXiv admin note: text overlap with arXiv:2208.12341</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05379v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05379v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective reinforcement learning (RL) for complex stochastic systems requires leveraging historical data collected in previous iterations to accelerate policy optimization. Classical experience replay treats all past observations uniformly and fails to account for their varying contributions to learning. To overcome this limitation, we propose Variance Reduction Experience Replay (VRER), a principled framework that selectively reuses informative samples to reduce variance in policy gradient estimation. VRER is algorithm-agnostic and integrates seamlessly with existing policy optimization methods, forming the basis of our sample-efficient off-policy algorithm, Policy Gradient with VRER (PG-VRER). Motivated by the lack of rigorous theoretical analysis of experience replay, we develop a novel framework that explicitly captures dependencies introduced by Markovian dynamics and behavior-policy interactions. Using this framework, we establish finite-time convergence guarantees for PG-VRER and reveal a fundamental bias-variance trade-off: reusing older experience increases bias but simultaneously reduces gradient variance. Extensive empirical experiments demonstrate that VRER consistently accelerates policy learning and improves performance over state-of-the-art policy optimization algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于方差缩减的经验回放策略优化方法</div>
<div class="mono" style="margin-top:8px">针对复杂随机系统的有效强化学习需要利用历史数据加速策略优化。传统经验回放均等处理所有过往观测，未考虑其对学习贡献的差异。为克服此局限，本文提出方差缩减经验回放（VRER）框架，通过选择性复用信息样本降低策略梯度估计方差。VRER与算法无关，可无缝集成现有策略优化方法，并由此构建出样本高效的离策略算法PG-VRER。针对经验回放缺乏理论分析的问题，我们建立了能显式刻画马尔可夫动态与行为策略交互依赖关系的分析框架，据此证明PG-VRER的有限时间收敛性，并揭示偏差-方差权衡规律：复用早期经验会增加偏差但能降低梯度方差。大量实验表明VRER能持续加速策略学习，其性能优于当前最优策略优化算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more efficient reinforcement learning in complex stochastic systems, this paper addresses the uniform treatment of past data in classical experience replay by proposing Variance Reduction Experience Replay (VRER), a framework that selectively reuses informative samples to reduce variance in policy gradient estimation. The method is algorithm-agnostic and integrates with existing policy optimization techniques, leading to the development of PG-VRER, and is supported by a novel theoretical framework that captures dependencies from Markovian dynamics and behavior-policy interactions to establish finite-time convergence guarantees and reveal a bias-variance trade-off. Experimental results demonstrate that VRER consistently accelerates policy learning and outperforms state-of-the-art algorithms.</div>
<div class="mono" style="margin-top:8px">本文针对复杂随机系统中强化学习效率提升的需求，指出经典经验回放均匀处理历史数据的不足，提出了方差缩减经验回放（VRER）框架，通过选择性重用信息样本来降低策略梯度估计的方差。该方法与算法无关，可无缝集成到现有策略优化方法中，并由此开发了PG-VRER算法；同时，作者构建了一个新的理论框架，以捕捉马尔可夫动态和行为策略交互引入的依赖关系，从而建立了有限时间收敛保证，并揭示了偏差-方差的权衡关系。大量实验结果表明，VRER能持续加速策略学习，并在性能上优于当前最先进的策略优化算法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Plan &amp; Schedule with Reinforcement-Learned Bimanual Robot Skills</div>
<div class="meta-line">Authors: Weikang Wan, Fabio Ramos, Xuning Yang, Caelan Garrett</div>
<div class="meta-line">First: 2025-10-29T15:39:53+00:00 · Latest: 2026-02-05T06:48:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25634v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25634v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon contact-rich bimanual manipulation presents a significant challenge, requiring complex coordination involving a mixture of parallel execution and sequential collaboration between arms. In this paper, we introduce a hierarchical framework that frames this challenge as an integrated skill planning &amp; scheduling problem, going beyond purely sequential decision-making to support simultaneous skill invocation. Our approach is built upon a library of single-arm and bimanual primitive skills, each trained using Reinforcement Learning (RL) in GPU-accelerated simulation. We then train a Transformer-based planner on a dataset of skill compositions to act as a high-level scheduler, simultaneously predicting the discrete schedule of skills as well as their continuous parameters. We demonstrate that our method achieves higher success rates on complex, contact-rich tasks than end-to-end RL approaches and produces more efficient, coordinated behaviors than traditional sequential-only planners.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习双手机器人技能的学习规划与调度方法</div>
<div class="mono" style="margin-top:8px">长时程密集接触的双手机器人操作面临重大挑战，需要双臂在并行执行与顺序协作间实现复杂协调。本文提出一种分层框架，将该挑战构建为集成技能规划与调度问题，突破纯顺序决策限制，支持技能同时调用。该方法基于单臂与双臂基础技能库构建，各技能均通过GPU加速仿真环境下的强化学习训练获得。随后，我们在技能组合数据集上训练基于Transformer的规划器作为高层调度器，同步预测离散技能调度序列及其连续参数。实验表明，本方法在复杂密集接触任务中比端到端强化学习方法获得更高成功率，且比传统纯顺序规划器产生更高效协调的行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of long-horizon contact-rich bimanual manipulation, which demands intricate coordination between robot arms, by proposing a hierarchical framework that integrates skill planning and scheduling to allow simultaneous skill execution rather than purely sequential actions. The method involves creating a library of single-arm and bimanual primitive skills trained with Reinforcement Learning in simulation, and then training a Transformer-based planner on skill compositions to predict both discrete skill schedules and continuous parameters. Experimental results show that this approach outperforms end-to-end Reinforcement Learning methods in success rates on complex tasks and generates more efficient, coordinated behaviors compared to traditional sequential planners.</div>
<div class="mono" style="margin-top:8px">本文针对长时程、接触密集的双臂操作挑战，该任务需要双臂进行复杂的协调，包括并行执行与顺序协作，提出了一种分层框架，将问题构建为集成的技能规划与调度问题，支持同时调用技能而非纯顺序决策。方法基于通过强化学习在仿真中训练的单臂和双臂原始技能库，并训练一个基于Transformer的规划器在技能组合数据集上作为高级调度器，同时预测离散的技能调度和连续参数。实验结果表明，该方法在复杂接触密集任务上比端到端强化学习方法获得更高的成功率，并比传统纯顺序规划器产生更高效、协调的行为。</div>
</details>
</div>
<div class="card">
<div class="title">GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL</div>
<div class="meta-line">Authors: Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang</div>
<div class="meta-line">First: 2026-02-05T05:44:48+00:00 · Latest: 2026-02-05T05:44:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05323v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05323v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to &quot;stitch&quot; optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAS：增强生成模型辅助离线安全强化学习的奖励-成本平衡</div>
<div class="mono" style="margin-top:8px">离线安全强化学习（OSRL）旨在仅使用预收集数据集学习策略，在满足约束条件的同时实现序列决策的高性能。受生成模型（GMs）强大能力的启发，近期研究将OSRL中的决策重新表述为条件生成过程，即GMs根据预定义的奖励和成本值生成理想动作。然而，GM辅助方法在OSRL中面临两大挑战：（1）缺乏从数据集内次优轨迹中“拼接”最优状态转移的能力；（2）难以平衡奖励目标与成本目标，尤其在二者冲突时。为解决这些问题，我们提出目标辅助拼接（GAS）算法，该算法旨在增强拼接能力的同时有效平衡奖励最大化与约束满足。为提升拼接能力，GAS首先在状态转移层面对数据集进行增强和重标注，从而能够从次优轨迹构建高质量轨迹。GAS还引入新颖的目标函数，用于从数据集中估计可达到的最优奖励与成本目标。这些目标函数通过在重标注增强数据集上使用期望回归进行训练，使GAS能够适应更广泛的奖励-成本回报组合，并相比人工指定值实现更好的奖励最大化与约束满足权衡。估计的目标随后指导策略训练，确保在约束设置下的稳健性能。此外，为提升训练稳定性与效率，我们重构数据集以实现更均匀的奖励-成本回报分布。实证结果验证了GAS的有效性，其在平衡奖励最大化与约束满足方面展现出优于现有方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses two key challenges in offline safe reinforcement learning (OSRL) with generative models: the inability to stitch optimal transitions from suboptimal trajectories and the difficulty in balancing conflicting reward and cost objectives. The proposed Goal-Assisted Stitching (GAS) algorithm enhances stitching by augmenting and relabeling the dataset at the transition level to construct high-quality trajectories, and it improves reward-cost balance by introducing goal functions trained via expectile regression to estimate optimal achievable goals, which then guide policy training. Experimental results show that GAS achieves a superior trade-off between reward maximization and constraint satisfaction compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对生成模型辅助的离线安全强化学习中的两个关键挑战展开研究：一是难以从次优轨迹中拼接出最优转移，二是难以平衡相互冲突的奖励与成本目标。提出的目标辅助拼接算法通过增强和重标注数据集来构建高质量轨迹以提升拼接能力，并引入基于期望回归训练的目标函数来估计最优可达目标，从而更好地平衡奖励与成本，进而指导策略训练。实验结果表明，该算法在奖励最大化与约束满足的权衡上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">A Differential and Pointwise Control Approach to Reinforcement Learning</div>
<div class="meta-line">Authors: Minh Nguyen, Chandrajit Bajaj</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-04-24T03:11:12+00:00 · Latest: 2026-02-05T05:28:08+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.15617v4">Abs</a> · <a href="https://arxiv.org/pdf/2404.15617v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) in continuous state-action spaces remains challenging in scientific computing due to poor sample efficiency and lack of pathwise physical consistency. We introduce Differential Reinforcement Learning (Differential RL), a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation. This induces a Hamiltonian structure that embeds physics priors and ensures consistent trajectories without requiring explicit constraints. To implement Differential RL, we develop Differential Policy Optimization (dfPO), a pointwise, stage-wise algorithm that refines local movement operators along the trajectory for improved sample efficiency and dynamic alignment. We establish pointwise convergence guarantees, a property not available in standard RL, and derive a competitive theoretical regret bound of $\mathcal{O}(K^{5/6})$. Empirically, dfPO outperforms standard RL baselines on representative scientific computing tasks, including surface modeling, grid control, and molecular dynamics, under low-data and physics-constrained conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习的微分与逐点控制方法</div>
<div class="mono" style="margin-top:8px">连续状态-动作空间中的强化学习（RL）在科学计算中仍面临挑战，主要源于样本效率低下和缺乏路径物理一致性。我们提出微分强化学习（Differential RL），这是一种通过微分对偶公式从连续时间控制角度重构RL的新框架。该框架引入哈密顿结构，嵌入物理先验并确保轨迹一致性，无需显式约束。为实现微分强化学习，我们开发了微分策略优化（dfPO），这是一种沿轨迹精化局部移动算子的逐点、分阶段算法，以提升样本效率和动态对齐能力。我们建立了逐点收敛保证（标准RL不具备的特性），并推导出$\mathcal{O}(K^{5/6})$的竞争性理论遗憾界。实验表明，在低数据和物理约束条件下，dfPO在表面建模、网格控制和分子动力学等代表性科学计算任务上均优于标准RL基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of poor sample efficiency and lack of physical consistency in continuous control reinforcement learning for scientific computing, this paper introduces Differential Reinforcement Learning, a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation to embed physics priors. The method implements this through Differential Policy Optimization (dfPO), a pointwise, stage-wise algorithm that refines local movement operators along trajectories. Main experimental results demonstrate that dfPO outperforms standard RL baselines on scientific computing tasks like surface modeling and molecular dynamics under low-data conditions, while also providing theoretical convergence guarantees and a competitive regret bound.</div>
<div class="mono" style="margin-top:8px">针对科学计算中连续控制强化学习样本效率低和缺乏物理一致性的挑战，本文提出了微分强化学习这一新框架，通过微分对偶公式从连续时间控制角度重构强化学习，以嵌入物理先验。该方法通过微分策略优化算法实现，这是一种逐点、分阶段的算法，可沿轨迹优化局部运动算子。主要实验结果表明，在表面建模和分子动力学等科学计算任务中，该算法在低数据条件下优于标准强化学习基线，同时提供了理论收敛保证和具有竞争力的遗憾界。</div>
</details>
</div>
<div class="card">
<div class="title">Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates</div>
<div class="meta-line">Authors: Chengxiao Wang, Haoze Wu, Gagandeep Singh</div>
<div class="meta-line">First: 2026-02-05T05:08:01+00:00 · Latest: 2026-02-05T05:08:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05311v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05311v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可证明鲁棒的神经李雅普诺夫-屏障证书的形式化综合</div>
<div class="mono" style="margin-top:8px">神经李雅普诺夫与屏障证书近年来已成为验证深度强化学习控制器安全性与稳定性属性的有力工具。然而，现有方法仅在固定理想无扰动力学下提供保证，限制了其在现实应用中因不确定性导致动力学偏离时的可靠性。本文研究在系统动力学受扰动下仍能保持保证的鲁棒神经李雅普诺夫屏障证书的综合问题。我们形式化定义了鲁棒李雅普诺夫屏障函数，并基于Lipschitz连续性提出了确保对有限扰动鲁棒性的充分条件。通过对抗训练、Lipschitz邻域约束和全局Lipschitz正则化，我们设计了强制执行这些条件的实用训练目标。在倒立摆和二维对接这两个实际相关环境中验证了方法：前者是广泛研究的基准，后者是自主系统中的安全关键任务。实验表明，相较于基线方法，我们的方法在强扰动下将可证明鲁棒界提升最高达4.6倍，实证成功率提高最高达2.4倍。结果证明了在动力学扰动下训练鲁棒神经证书对安全强化学习的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to ensure safety and stability of deep reinforcement learning controllers under real-world dynamic uncertainties, this paper proposes a method for synthesizing robust neural Lyapunov-barrier certificates that remain valid despite perturbations. The approach formally defines robust certificates and establishes sufficient conditions based on Lipschitz continuity, then enforces these through adversarial training, Lipschitz neighborhood bounds, and global Lipschitz regularization during training. Experimental validation on the Inverted Pendulum and 2D Docking tasks shows that the method significantly improves certified robustness bounds by up to 4.6 times and empirical success rates under strong perturbations by up to 2.4 times compared to baselines, demonstrating its effectiveness for safe RL in perturbed dynamics.</div>
<div class="mono" style="margin-top:8px">针对深度强化学习控制器在现实动态不确定性下需确保安全与稳定性的需求，本文提出了一种合成鲁棒神经李雅普诺夫-屏障证书的方法，使其在扰动下仍保持有效性。该方法正式定义了鲁棒证书，并基于Lipschitz连续性建立了充分条件，进而通过对抗训练、Lipschitz邻域边界和全局Lipschitz正则化在训练中强制执行这些条件。在倒立摆和二维对接任务上的实验验证表明，相比基线方法，该方法将认证鲁棒性边界提升了最高4.6倍，在强扰动下的实证成功率提高了最高2.4倍，证明了其在扰动动态中实现安全强化学习的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities</div>
<div class="meta-line">Authors: Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov, Ivan Oseledets</div>
<div class="meta-line">First: 2026-02-05T04:06:55+00:00 · Latest: 2026-02-05T04:06:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05281v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>回归基础：通过生成概率重新审视强化学习在大型语言模型推理中的探索机制</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习已成为提升大型语言模型推理能力的关键范式。然而，标准策略优化方法（如组相对策略优化）常收敛于低熵策略，导致严重的模式坍缩和输出多样性受限。我们从采样概率动态的角度分析此问题，发现标准目标函数会过度强化最高似然路径，从而抑制有效的替代推理链。为此，我们提出一种新颖的优势重加权机制，旨在均衡所有正确答案的置信度水平。通过将提示困惑度与答案置信度融入优势估计，该方法动态重塑奖励信号以削弱过度自信推理路径的梯度更新，同时将概率质量重新分配给未充分探索的正确解。实证结果表明，我们的方法在保持竞争力的准确率的同时，显著提升了生成多样性和响应熵，有效实现了推理任务中探索与利用的优越权衡。在Qwen2.5和DeepSeek模型上的数学与编程基准测试表明，ProGRPO显著缓解了熵坍缩问题。具体而言，在Qwen2.5-7B模型上，本方法在Pass@1指标上超越GRPO 5.7%，在Pass@32指标上更显著领先13.9%，凸显了其在生成多样化正确推理路径方面的卓越能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of mode collapse and limited output diversity in reinforcement learning for LLM reasoning, which occurs when standard methods like GRPO converge to low-entropy policies by disproportionately reinforcing high-likelihood paths. To mitigate this, the authors propose an Advantage Re-weighting Mechanism (ARM) that incorporates Prompt Perplexity and Answer Confidence into advantage estimation, dynamically reshaping rewards to attenuate gradients for over-confident paths and redistribute probability toward under-explored correct solutions. Experimental results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that the method, ProGRPO, significantly mitigates entropy collapse, outperforming GRPO by 5.7% in Pass@1 and 13.9% in Pass@32 on Qwen2.5-7B, thereby enhancing generative diversity while maintaining competitive accuracy.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在大型语言模型推理中因标准方法（如GRPO）收敛至低熵策略而导致模式崩溃和输出多样性受限的问题，提出了一种优势重加权机制（ARM）。该方法通过将提示困惑度和答案置信度融入优势估计，动态调整奖励信号，以削弱过度自信推理路径的梯度更新，并将概率质量重新分配给未被充分探索的正确解。在Qwen2.5和DeepSeek模型上的数学与编码基准实验表明，所提方法ProGRPO显著缓解了熵崩溃，在Qwen2.5-7B上，其Pass@1和Pass@32分别比GRPO提升了5.7%和13.9%，从而在保持竞争力的准确率的同时有效提升了生成多样性。</div>
</details>
</div>
<div class="card">
<div class="title">CARL: Focusing Agentic Reinforcement Learning on Critical Actions</div>
<div class="meta-line">Authors: Leyang Shen, Yang Zhang, Chun Kai Ling, Xiaoyan Zhao, Tat-Seng Chua</div>
<div class="meta-line">First: 2025-12-04T16:15:46+00:00 · Latest: 2026-02-05T03:39:41+00:00</div>
<div class="meta-line">Comments: 17 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04949v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04949v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for long-horizon agentic reasoning. CARL leverages entropy as a heuristic proxy for action criticality and achieves focused training by assigning rewards to high-criticality actions while excluding low-criticality actions from model updates, avoiding noisy credit assignment and redundant computation. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency across diverse evaluation settings. The source code will be publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CARL：聚焦关键动作的智能体强化学习</div>
<div class="mono" style="margin-top:8px">能够通过与环境的多次交互完成复杂任务的智能体已成为热门研究方向。然而，在这种多步交互场景中，传统群体级策略优化算法因其默认假设每个动作贡献均等而偏离现实，导致效果欠佳。我们的分析表明，仅少数关键动作对最终结果起决定性作用。基于此发现，我们提出CARL——一种专为长周期智能体推理设计的、聚焦关键动作的强化学习算法。CARL以熵作为动作关键性的启发式代理指标，通过为高关键性动作分配奖励，并将低关键性动作排除在模型更新之外，实现聚焦式训练，从而避免噪声信用分配与冗余计算。大量实验表明，CARL在多种评估场景中均实现了更强的性能与更高的效率。源代码将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of standard reinforcement learning in multi-step agentic tasks, where treating all actions as equally important leads to suboptimal policy optimization. The authors propose CARL, an algorithm that identifies critical actions using entropy as a proxy, focusing training updates on these high-impact actions while ignoring low-criticality ones to improve credit assignment and computational efficiency. Experimental results show that CARL outperforms baseline methods in both performance and efficiency across various long-horizon reasoning scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对多步智能体任务中传统强化学习效率低下的问题，指出其将每个动作视为同等重要的假设与现实中仅少数关键动作决定最终结果的情况不符。为此，作者提出了CARL算法，该算法利用熵作为动作关键性的启发式指标，在训练中专注于高关键性动作的奖励分配，并排除低关键性动作的模型更新，从而避免噪声信用分配和冗余计算。大量实验表明，CARL在多种评估设置下均实现了更强的性能和更高的效率。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretability by Design for Efficient Multi-Objective Reinforcement Learning</div>
<div class="meta-line">Authors: Qiyue Xia, Tianwei Wang, J. Michael Herrmann</div>
<div class="meta-line">First: 2025-06-04T14:52:18+00:00 · Latest: 2026-02-05T02:52:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04022v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.04022v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective reinforcement learning (MORL) aims at optimising several, often conflicting goals to improve the flexibility and reliability of RL in practical tasks. This is typically achieved by finding a set of diverse, non-dominated policies that form a Pareto front in the performance space. We introduce LLE-MORL, an approach that achieves interpretability by design by utilising a training scheme based on the local relationship between the parameter space and the performance space. By exploiting a locally linear map between these spaces, our method provides an interpretation of policy parameters in terms of the objectives, and this structured representation enables an efficient search within contiguous solution domains, allowing for the rapid generation of high-quality solutions without extensive retraining. Experiments across diverse continuous control domains demonstrate that LLE-MORL consistently achieves higher Pareto front quality and efficiency than state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高效多目标强化学习的可解释性设计</div>
<div class="mono" style="margin-top:8px">多目标强化学习（MORL）旨在优化多个常相互冲突的目标，以提升强化学习在实际任务中的灵活性与可靠性。该方法通常通过寻找一组构成性能空间帕累托前沿的多样化非支配策略来实现。本文提出LLE-MORL方法，通过利用参数空间与性能空间局部关联性的训练机制，实现设计层面的可解释性。该方法通过构建两空间间的局部线性映射，以目标函数视角解释策略参数，这种结构化表征支持在连续解空间中进行高效搜索，无需大量重复训练即可快速生成高质量解。在多类连续控制领域的实验表明，LLE-MORL在帕累托前沿质量与求解效率方面均持续优于现有先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the need for interpretable and efficient multi-objective reinforcement learning (MORL), where optimizing multiple conflicting goals is crucial for practical applications. The proposed method, LLE-MORL, achieves interpretability by design through a training scheme that leverages a locally linear map between the policy parameter space and the performance space, enabling clear interpretation of parameters in terms of objectives and facilitating efficient search within contiguous solution domains. Experimental results across various continuous control tasks show that LLE-MORL consistently outperforms state-of-the-art methods in both Pareto front quality and computational efficiency, allowing rapid generation of high-quality solutions without extensive retraining.</div>
<div class="mono" style="margin-top:8px">本文针对多目标强化学习（MORL）中可解释性和效率的需求展开研究，旨在优化多个常冲突的目标以提升实际任务的灵活性和可靠性。所提出的LLE-MORL方法通过利用策略参数空间与性能空间之间的局部线性映射，实现了设计上的可解释性，从而能够依据目标解释参数，并在连续解域中进行高效搜索。在多种连续控制领域的实验结果表明，LLE-MORL在帕累托前沿质量和效率上均一致优于现有先进方法，无需大量重新训练即可快速生成高质量解。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints</div>
<div class="meta-line">Authors: Evan Chen, Wenzhi Fang, Shiqiang Wang, Christopher Brinton</div>
<div class="meta-line">First: 2026-01-29T23:27:15+00:00 · Latest: 2026-02-05T02:29:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00166v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00166v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预算约束下本地语言模型与云端卸载决策的联合持续学习</div>
<div class="mono" style="margin-top:8px">本地部署的小型语言模型必须在严格的内存与计算约束下持续支持多样化任务，这使得选择性依赖云端大型语言模型成为必然。在持续学习过程中调控云端辅助具有挑战性：基于朴素奖励的强化学习常导致不稳定的卸载行为，且会因任务分布变化加剧灾难性遗忘。我们提出DA-GRPO——一种融合双重优势的组相对策略优化扩展方法，将云端使用约束直接纳入优势计算，避免固定奖励塑造和外部路由模型。该设计使本地模型能联合学习任务能力与协作行为，让云端请求在遵循预设辅助预算的前提下自然产生于训练后阶段。在数学推理与代码生成基准测试中，DA-GRPO相比现有协作与路由方法提升了切换后准确率，显著减少遗忘，并保持稳定的云端使用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling Small Language Models (SLMs) to continually learn diverse tasks under local resource constraints while selectively offloading queries to cloud-based Large Language Models (LLMs) within a strict usage budget. The proposed method, DA-GRPO, extends Group Relative Policy Optimization by integrating cloud-usage constraints directly into advantage computation, allowing the local model to jointly learn task competence and collaboration behavior without relying on fixed reward shaping or external routing models. Experimental results on mathematical reasoning and code generation benchmarks demonstrate that DA-GRPO improves post-switch accuracy, significantly reduces catastrophic forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.</div>
<div class="mono" style="margin-top:8px">本文旨在解决小型语言模型在本地资源受限条件下持续学习多样化任务时，如何在严格使用预算内选择性地将查询卸载到云端大型语言模型的挑战。所提出的方法DA-GRPO扩展了组相对策略优化，将云端使用约束直接纳入优势计算，使本地模型能够联合学习任务能力和协作行为，无需依赖固定的奖励塑造或外部路由模型。在数学推理和代码生成基准测试上的实验结果表明，与先前的协作和基于路由的方法相比，DA-GRPO提高了切换后的准确性，显著减少了灾难性遗忘，并保持了稳定的云端使用。</div>
</details>
</div>
<div class="card">
<div class="title">HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation</div>
<div class="meta-line">Authors: Puyue Wang, Jiawei Hu, Yan Gao, Junyan Wang, Yu Zhang, Gillian Dobbie, Tao Gu, Wafa Johal, Ting Dang, Hong Jia</div>
<div class="meta-line">First: 2026-02-04T10:41:23+00:00 · Latest: 2026-02-05T02:24:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04412v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04412v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tonywang-0517.github.io/hord/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher&#x27;s robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at https://tonywang-0517.github.io/hord/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HoRD：基于历史条件强化学习与在线蒸馏的鲁棒人形机器人控制</div>
<div class="mono" style="margin-top:8px">人形机器人在动力学、任务规范或环境设置的微小变化下可能出现性能显著下降。本文提出HoRD，一种面向领域偏移下鲁棒人形控制的两阶段学习框架。首先，通过历史条件强化学习训练高性能教师策略，该策略从近期状态-动作轨迹推断潜在动力学上下文，以在线适应多样随机化动力学。其次，执行在线蒸馏，将教师的鲁棒控制能力迁移至基于Transformer的学生策略，该策略以稀疏的根关节相对三维关键点轨迹为输入。通过结合历史条件适应与在线蒸馏，HoRD使单一策略能够零样本适应未见领域，无需逐领域重新训练。大量实验表明，HoRD在鲁棒性与迁移性上优于强基线方法，尤其在未见领域和外部扰动下表现突出。代码与项目页面详见 https://tonywang-0517.github.io/hord/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of humanoid robots to performance degradation under slight domain shifts, such as changes in dynamics or task specifications. To enhance robustness, the authors propose HoRD, a two-stage framework that first trains a teacher policy using history-conditioned reinforcement learning, enabling it to infer latent dynamics from recent trajectories and adapt online to randomized conditions. This teacher&#x27;s capabilities are then distilled into a transformer-based student policy via online distillation, allowing the student to operate on sparse joint keypoints and achieve zero-shot adaptation to unseen domains without retraining. Experimental results demonstrate that HoRD surpasses baseline methods in robustness and transfer performance, particularly when facing unseen domains and external perturbations.</div>
<div class="mono" style="margin-top:8px">该论文针对人形机器人在动态或任务规范等轻微领域变化下性能下降的问题，提出了一种增强鲁棒性的方法。作者设计了HoRD这一两阶段框架：首先通过历史条件强化学习训练教师策略，使其能从近期轨迹推断潜在动态并在线适应随机化条件；然后通过在线蒸馏将教师策略的鲁棒控制能力迁移到基于Transformer的学生策略中，该学生策略基于稀疏的关节关键点运行，无需针对每个领域重新训练即可实现零样本适应。实验结果表明，HoRD在鲁棒性和迁移性能上优于基线方法，尤其在未见领域和外部扰动下表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture</div>
<div class="meta-line">Authors: Wenyun Li, Wenjie Huang, Zejian Deng, Chen Sun</div>
<div class="meta-line">First: 2025-06-14T12:18:19+00:00 · Latest: 2026-02-05T02:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12474v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.12474v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate driving behavior modeling is fundamental to safe and efficient trajectory prediction, yet remains challenging in complex traffic scenarios. This paper presents a novel Inverse Reinforcement Learning (IRL) framework that captures human-like decision-making by inferring diverse reward functions, enabling robust cross-scenario adaptability. The learned reward function is utilized to maximize the likelihood of output by integrating Mamba blocks for efficient long-sequence dependency modeling with graph attention networks to encode spatial interactions among traffic agents. Comprehensive evaluations on urban intersections and roundabouts demonstrate that the proposed method not only outperforms various popular approaches in terms of prediction accuracy but also achieves 2.3 times higher generalization performance to unseen scenarios compared to other baselines, achieving adaptability in Out-of-Distribution settings that is competitive with fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Mamba-图架构的逆强化学习通用轨迹预测方法</div>
<div class="mono" style="margin-top:8px">精确的驾驶行为建模是实现安全高效轨迹预测的基础，但在复杂交通场景中仍具挑战。本文提出一种新颖的逆强化学习框架，通过推断多样化奖励函数来捕捉类人决策机制，实现跨场景的鲁棒适应性。该框架将Mamba模块用于高效长序列依赖建模，并结合图注意力网络编码交通参与者间的空间交互，利用习得的奖励函数最大化输出似然。在城市交叉口和环岛场景的综合评估表明，该方法不仅在预测精度上优于多种主流方法，且对未见场景的泛化性能达到其他基线的2.3倍，在分布外场景中取得的适应性可与微调方法相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of achieving accurate and generalizable trajectory prediction in complex traffic scenarios, where modeling human-like decision-making is crucial. The method introduces an Inverse Reinforcement Learning framework that infers diverse reward functions to capture nuanced driving behaviors, integrating Mamba blocks for efficient long-sequence dependency modeling and graph attention networks to encode spatial interactions among traffic agents. Experimental results on urban intersections and roundabouts show that the approach outperforms popular baselines in prediction accuracy and achieves 2.3 times higher generalization performance to unseen scenarios, demonstrating robust adaptability in Out-of-Distribution settings comparable to fine-tuning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决复杂交通场景中轨迹预测的准确性和泛化性挑战，其中模拟类人决策至关重要。该方法提出了一种逆强化学习框架，通过推断多样化的奖励函数来捕捉精细的驾驶行为，并整合Mamba模块以高效建模长序列依赖，同时利用图注意力网络编码交通参与者间的空间交互。在城区交叉口和环岛的全面评估表明，该方法不仅在预测准确性上优于多种主流方法，而且在未见场景中的泛化性能比其他基线高出2.3倍，展示了在分布外设置中与微调相竞争的强大适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">Your Latent Reasoning is Secretly Policy Improvement Operator</div>
<div class="meta-line">Authors: Arip Asadulaev, Rayan Banerjee, Fakhri Karray, Martin Takac</div>
<div class="meta-line">First: 2025-11-21T01:54:23+00:00 · Latest: 2026-02-05T01:45:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16886v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.16886v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, small models with latent recursion have obtained promising results on complex reasoning tasks. These results are typically explained by the theory that such recursion increases a networks depth, allowing it to compactly emulate the capacity of larger models. However, the performance of recursively added layers remains behind the capabilities of one pass models with the same feed forward depth. This means that in the looped version, not every recursive step effectively contributes to depth. This raises the question: when and why does latent reasoning improve performance, and when does it result in dead compute? In our work, we analyze the algorithms that latent reasoning provides answer to this question. We show that latent reasoning can be formalized as a classifier free guidance and policy improvement algorithm. Building on these insights, we propose to use a training schemes from reinforcement learning and diffusion methods for latent reasoning models. Using the Tiny Recursive Model as our testbed, we show that with our modifications we can avoid dead compute steps and reduce the total number of forward passes by 18x while maintaining performance. Broadly speaking, we show how a policy improvement perspective on recursive steps can explain model behavior and provide insights for further improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在推理实为策略改进算子</div>
<div class="mono" style="margin-top:8px">近期，采用潜在递归的小型模型在复杂推理任务中取得了显著成果。通常认为，这种递归通过增加网络深度，使模型能紧凑模拟更大规模模型的能力。然而，递归叠加层的性能仍落后于同等前馈深度的单次推理模型，表明并非每个递归步骤都能有效贡献深度。这引出一个核心问题：潜在推理何时及为何能提升性能，何时会导致无效计算？本研究通过算法分析解答该问题，证明潜在推理可形式化为无分类器引导与策略改进算法。基于此洞见，我们提出将强化学习与扩散方法的训练机制应用于潜在推理模型。以微型递归模型为测试平台，改进后的模型在保持性能的同时，避免了无效计算步骤，并将前向传播总次数减少18倍。总体而言，本研究通过策略改进视角阐释递归步骤的行为机制，并为模型优化提供了新思路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper investigates why latent reasoning in small recursive models sometimes fails to improve performance, a phenomenon termed &#x27;dead compute&#x27;. Motivated by the discrepancy between recursive layers and equivalent-depth feedforward models, the authors formalize latent reasoning as a classifier-free guidance and policy improvement algorithm. Their method adapts training schemes from reinforcement learning and diffusion models to enhance recursive steps. Experimental results on the Tiny Recursive Model demonstrate that these modifications avoid dead compute, reducing forward passes by 18x while maintaining task performance, thereby validating the policy improvement perspective for optimizing latent reasoning.</div>
<div class="mono" style="margin-top:8px">本文研究了小型递归模型中潜在推理有时无法提升性能的原因，即所谓“无效计算”现象。其动机源于递归层与同等深度前馈模型之间的性能差距，作者将潜在推理形式化为无分类器引导和策略改进算法。方法上借鉴了强化学习和扩散模型的训练方案来优化递归步骤。在Tiny Recursive Model上的实验结果表明，这些改进能避免无效计算，在保持任务性能的同时将前向传播次数减少18倍，从而验证了策略改进视角对优化潜在推理的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: John Yan, Michael Yu, Yuqi Sun, Alexander Duffy, Tyler Marques, Matthew Lyle Olson</div>
<div class="meta-line">First: 2026-02-05T01:21:22+00:00 · Latest: 2026-02-05T01:21:22+00:00</div>
<div class="meta-line">Comments: authors 1, 2 and 3 contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05183v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05183v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent&#x27;s system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的多智能体强化学习的数据中心可解释性研究</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）日益在复杂的强化学习多智能体环境中进行训练，导致行为在训练过程中的变化难以理解。稀疏自编码器（SAEs）近期被证明对数据中心可解释性具有实用价值。本研究通过应用预训练的SAEs及LLM摘要生成方法，分析了复杂环境《全面外交》中的大规模强化学习训练过程。我们提出了Meta-Autointerp方法，用于将SAE特征聚类为关于训练动态的可解释假设。我们发现了细粒度行为（包括角色扮演模式、退化输出、语言切换）以及高层战略行为与环境特定缺陷。通过自动化评估，我们验证了90%已发现的SAE元特征具有显著性，并发现了一种令人意外的奖励破解行为。然而，两项用户研究表明，即使是主观有趣且看似有用的SAE特征，也可能对人类毫无价值甚至产生负面影响，LLM生成的假设大多也存在类似问题。但部分SAE衍生的假设对下游任务具有预测价值。我们通过增强未训练智能体的系统提示进一步验证，使其得分提升+14.2%。总体而言，我们证明SAEs与LLM摘要生成器能为智能体行为提供互补视角，共同构建了一个实用框架，为未来确保LLM训练全过程可信行为的数据中心可解释性研究奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the difficulty of understanding behavioral changes in large language models (LLMs) trained in complex multi-agent reinforcement learning environments, this paper introduces Meta-Autointerp, a method that groups sparse autoencoder (SAE) features into interpretable hypotheses about training dynamics. The approach applies pretrained SAEs and LLM-summarizer methods to analyze training runs from the Full-Press Diplomacy environment, discovering fine-grained behaviors like role-playing and high-level strategic patterns. Experimental results show that 90% of discovered SAE meta-features are significant, including a reward hacking behavior, and while many features and LLM hypotheses are not helpful to humans, a subset proves predictively useful, with one validation improving an untrained agent&#x27;s score by 14.2%, demonstrating that SAEs and LLM-summarizers offer complementary views for data-centric interpretability.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型在复杂多智能体强化学习环境中训练时行为变化难以理解的问题，提出了Meta-Autointerp方法，将稀疏自编码器特征分组为关于训练动态的可解释假设。该方法应用预训练的稀疏自编码器和LLM摘要器分析Full-Press Diplomacy环境中的训练过程，发现了细粒度的角色扮演行为和高层战略模式等。实验结果表明，90%发现的稀疏自编码器元特征具有显著性，包括奖励黑客行为；尽管许多特征和LLM生成的假设对人类帮助有限，但其中一部分具有预测实用性，例如通过增强未训练智能体的系统提示使其得分提升14.2%，证明了稀疏自编码器和LLM摘要器为数据为中心的可解释性提供了互补视角。</div>
</details>
</div>
<div class="card">
<div class="title">EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization</div>
<div class="meta-line">Authors: Kevin Han, Yuhang Zhou, Mingze Gao, Gedi Zhou, Serena Li, Abhishek Kumar, Xiangjun Fan, Weiwei Li, Lizhu Zhang</div>
<div class="meta-line">First: 2026-02-05T00:33:02+00:00 · Latest: 2026-02-05T00:33:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05165v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy&#x27;s accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford&#x27;s online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EBPO：基于经验贝叶斯收缩的组相对策略优化稳定方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已被证明能有效提升大语言模型（LLMs）的推理能力。然而，主流方法如组相对策略优化（GRPO）面临关键的稳定性挑战：在计算受限（小组规模较小）时估计器方差较高，且在饱和失效场景（所有响应均产生相同的零奖励）中梯度信号趋于消失。为此，我们提出经验贝叶斯策略优化（EBPO），这是一种通过利用策略累积的全局统计量来正则化局部组基线的新框架。EBPO采用收缩估计器动态平衡局部组统计量与通过Welford在线算法更新的全局先验，而非孤立估计基线。理论上，我们证明相比GRPO，EBPO能严格保证更低的均方误差（MSE）、有界的熵衰减，并在失效场景中提供非消失的惩罚信号。实证中，EBPO在包括AIME和OlympiadBench在内的多种基准测试中持续优于GRPO及其他成熟基线。值得注意的是，EBPO展现出卓越的训练稳定性，即使在小规模组设置下仍能实现高性能提升，并能显著受益于难度分层课程学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Empirical Bayes Policy Optimization (EBPO) to address stability issues in Group Relative Policy Optimization (GRPO) for reinforcement learning with verifiable rewards, where GRPO suffers from high variance with small group sizes and vanishing gradients when all responses yield zero rewards. The method regularizes local group baselines by dynamically shrinking them toward a global prior updated online, theoretically ensuring lower mean squared error, bounded entropy decay, and non-vanishing penalty signals. Experimental results show EBPO outperforms GRPO and other baselines on benchmarks like AIME and OlympiadBench, offering greater training stability and performance gains even with small groups, and benefits from difficulty-stratified curriculum learning.</div>
<div class="mono" style="margin-top:8px">本文提出经验贝叶斯策略优化（EBPO），以解决可验证奖励强化学习中组相对策略优化（GRPO）的稳定性问题，其中GRPO在小组规模下存在高方差且在全部响应奖励为零时梯度消失。该方法通过将局部组基线动态收缩至在线更新的全局先验来进行正则化，从理论上保证了更低的均方误差、有界的熵衰减和非消失的惩罚信号。实验结果表明，EBPO在AIME和OlympiadBench等基准测试中优于GRPO及其他基线方法，即使小组规模较小也能提供更高的训练稳定性和性能提升，并受益于难度分层课程学习。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning</div>
<div class="meta-line">Authors: Ahmed Attia, Alham Fikri Aji</div>
<div class="meta-line">First: 2026-01-18T18:44:49+00:00 · Latest: 2026-02-04T23:26:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12535v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12535v2">PDF</a> · <a href="https://github.com/Copticoder/thesis-nllb-bootstrap-grpo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving. The code is available on github: https://github.com/Copticoder/thesis-nllb-bootstrap-grpo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过往返式强化学习改进低资源机器翻译</div>
<div class="mono" style="margin-top:8px">随着低资源语言社区平行数据的收集，低资源机器翻译日益受到关注，但许多改进低资源机器翻译的潜在方法仍有待探索。本研究基于No Language Left Behind（NLLB）系列模型，采用往返式自举方法，探索了在低资源环境下通过自监督强化学习进行翻译微调的技术。该方法先将英语翻译为目标低资源语言，再回译至英语，并以重构英语句子的chrF++和BLEU组合作为奖励函数。基于NLLB-MD数据集，我们对6亿参数和13亿参数的NLLB模型进行评估，在以下语言中观察到一致性的性能提升：中部艾马拉语、弗留利语、沃洛夫语和俄语。对翻译输出的定性分析表明，其流畅度与语义保真度均有所提高。我们认为该方法可进一步受益于模型规模扩展，使模型能更充分地利用预训练知识并持续自我改进。代码已发布于GitHub：https://github.com/Copticoder/thesis-nllb-bootstrap-grpo。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of improving machine translation for low-resource languages by proposing a self-supervised reinforcement learning method. The motivation stems from the need to leverage existing models more effectively despite scarce parallel data. The method employs round-trip bootstrapping using NLLB models, translating English to a target low-resource language and back, with a reward function based on chrF++ and BLEU scores for the reconstructed English. Experimental results on languages including Central Aymara, Friulian, Wolof, and Russian using NLLB-MD datasets show consistent improvements in translation fluency and semantic fidelity, with the approach benefiting from model scale.</div>
<div class="mono" style="margin-top:8px">本文针对低资源语言机器翻译的改进挑战，提出了一种自监督强化学习方法。其动机是在平行数据稀缺的情况下，更有效地利用现有模型。该方法采用基于NLLB模型的往返自举技术，将英语翻译成目标低资源语言后再译回英语，并使用chrF++和BLEU分数作为重建英语句子的奖励函数。在NLLB-MD数据集上对包括中部艾马拉语、弗留利语、沃洛夫语和俄语等语言的实验结果表明，翻译的流畅性和语义保真度均得到一致提升，且该方法能从模型规模中获益。</div>
</details>
</div>
<div class="card">
<div class="title">Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning</div>
<div class="meta-line">Authors: Ethan Rathbun, Wo Wei Lin, Alina Oprea, Christopher Amato</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-04T22:17:23+00:00 · Latest: 2026-02-04T22:17:23+00:00</div>
<div class="meta-line">Comments: 10 pages main body, ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05089v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05089v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger&#x27;&#x27;, leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent&#x27;s training pipeline, enabling them to both alter and observe agent&#x27;s rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze&#x27;&#x27; which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze&#x27;s effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>警惕不可信模拟器——强化学习中的无奖励后门攻击</div>
<div class="mono" style="margin-top:8px">模拟环境是强化学习（RL）成功的关键，使从业者和研究人员无需在真实硬件上进行昂贵实验即可训练决策智能体。然而，模拟器仍是一个安全盲区，恶意开发者可通过修改其发布的模拟器动态实现有害目的。为此，本文揭示了一种新型威胁，展示了如何利用模拟器动态向RL智能体隐蔽植入动作级后门。该后门使攻击者能在观察到预定义“触发器”时，可靠地激活智能体中的目标动作，可能导致危险后果。传统后门攻击受限于其强威胁模型，假设攻击者几乎完全控制智能体训练流程，既能修改也能观察智能体奖励。由于这些假设在模拟器中难以实现，本文提出新型攻击“Daze”，该攻击能在不修改甚至不观察奖励的情况下，可靠且隐蔽地向面向真实世界任务的RL智能体植入后门。我们通过形式化证明验证了Daze在通用RL任务中保证攻击成功的有效性，并在离散与连续动作空间领域进行了广泛实证评估。此外，我们首次展示了RL后门攻击可迁移至真实机器人硬件。这些进展推动了对RL训练流程全组件安全防护的进一步研究，以防范恶意攻击。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses a security vulnerability in reinforcement learning (RL) by introducing a novel backdoor attack that exploits untrusted simulators. The motivation stems from the fact that simulators, while essential for cost-effective RL training, are a security blind spot, allowing adversaries to maliciously alter environment dynamics. The proposed method, named Daze, implants action-level backdoors into RL agents without requiring control over or observation of the agent&#x27;s rewards, overcoming limitations of traditional attacks that assume strong adversary control. Experimental results demonstrate Daze&#x27;s effectiveness in both discrete and continuous action spaces, with formal proofs of success, and include the first documented transfer of an RL backdoor attack to real robotic hardware, highlighting the need for securing the entire RL training pipeline.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的安全漏洞，提出了一种利用不可信模拟器的新型后门攻击。其动机在于模拟器虽然是强化学习经济高效训练的关键，但存在安全盲点，允许攻击者恶意修改环境动态。所提出的方法名为Daze，能够在无需控制或观察智能体奖励的情况下，向其植入动作级后门，克服了传统攻击需假设攻击者具有强控制力的局限。实验结果表明，Daze在离散和连续动作空间中均有效，并提供了成功的形式化证明，还包括了首次记录的后门攻击向真实机器人硬件的迁移案例，强调了保护整个强化学习训练流程的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling</div>
<div class="meta-line">Authors: Parsa Vares</div>
<div class="meta-line">First: 2026-02-04T22:16:50+00:00 · Latest: 2026-02-04T22:16:50+00:00</div>
<div class="meta-line">Comments: Master&#x27;s Thesis, University of Luxembourg in collaboration with Luxembourg Institute of Science and Technology (LIST). Supervised by Prof. Jun Pang and Dr. Eloi Durant</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05087v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05087v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent&#x27;s decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoDiscover：基于图感知汤普森采样的强化学习推荐系统，用于解决主动学习中的冷启动不平衡挑战</div>
<div class="mono" style="margin-top:8px">系统文献综述是循证研究的基础，但随着科学成果的增长，人工筛选日益成为瓶颈。筛选过程面临相关研究出现率低、专家决策稀缺且成本高昂的问题。传统主动学习系统虽能提供帮助，但通常依赖固定查询策略选择未标注文档。这些静态策略无法随时间自适应，且忽略了科学文献网络的关系结构。本论文提出AutoDiscover框架，将主动学习重构为由自适应智能体驱动的在线决策问题。文献被建模为包含文档、作者和元数据关系的异质图。异质图注意力网络学习节点表征，折扣汤普森采样智能体利用该表征动态管理查询策略组合。通过实时人机协同标注，智能体在非平稳的综述动态中平衡探索与利用，其中策略效用随时间变化。在26个数据集的SYNERGY基准测试中，AutoDiscover实现了比静态主动学习基线更高的筛选效率。关键创新在于：智能体通过从极少量初始标签进行引导式发现，缓解了静态方法难以应对的冷启动问题。同时，我们开发了开源可视化分析仪表盘TS-Insight，用于解释、验证和诊断智能体决策。这些成果共同提升了在专家标注稀缺和相关研究低出现率条件下的系统文献综述筛选效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This thesis addresses the inefficiency of manual screening in systematic literature reviews, where relevant studies are rare and expert labels are scarce, by introducing AutoDiscover, a reinforcement learning framework that treats active learning as an online decision-making problem. The method models literature as a heterogeneous graph, uses a Heterogeneous Graph Attention Network to learn node representations, and employs a Discounted Thompson Sampling agent to dynamically select query strategies, adapting to non-stationary review dynamics. Experimental results on the SYNERGY benchmark show that AutoDiscover outperforms static active learning baselines in screening efficiency and effectively mitigates cold-start issues with minimal initial labels, while an accompanying visual analytics tool, TS-Insight, aids in interpreting the agent&#x27;s decisions.</div>
<div class="mono" style="margin-top:8px">本论文针对系统文献综述中人工筛选效率低下、相关研究稀少且专家标注稀缺的问题，提出了AutoDiscover框架，该框架将主动学习重构为一个在线决策问题，由强化学习智能体驱动。方法将文献建模为异构图以捕捉文档、作者和元数据间的关系，利用异构图注意力网络学习节点表示，并采用折扣汤普森采样智能体动态管理查询策略组合，以适应非平稳的综述动态。在SYNERGY基准的26个数据集上实验表明，AutoDiscover在筛选效率上优于静态主动学习基线，并能通过极少初始标注有效缓解冷启动问题，同时配套的可视化分析工具TS-Insight有助于解释智能体的决策过程。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking</div>
<div class="meta-line">Authors: Vinal Asodia, Iman Sharifi, Saber Fallah</div>
<div class="meta-line">First: 2026-02-04T21:56:27+00:00 · Latest: 2026-02-04T21:56:27+00:00</div>
<div class="meta-line">Comments: 12 pages, 7 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05079v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05079v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于向量语义表征与符号推理的强化学习增强方法及其在以人为中心的自主紧急制动系统中的应用</div>
<div class="mono" style="margin-top:8px">现有基于摄像头的深度强化学习方法存在双重问题：鲜少将高层场景上下文整合至特征表征中，且依赖僵化的固定奖励函数。为解决这些挑战，本文提出一种新型流程，生成融合语义、空间与形状信息的神经符号特征表征，重点关注安全关键道路使用者的空间增强动态实体特征。同时提出一种通过符号推理模块平衡人类价值的软一阶逻辑奖励函数。该方法从分割图中提取语义与空间谓词，并应用于语言规则以获得奖励权重。在CARLA仿真环境中的定量实验表明，相较于基线表征与奖励方案，所提出的神经符号表征与软一阶逻辑奖励函数在不同交通密度和遮挡水平下均提升了策略鲁棒性及安全相关性能指标。研究证明，将整体表征与软推理融入强化学习可支持自动驾驶系统实现更具情境感知能力且符合人类价值的决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing camera-based deep reinforcement learning methods, which often lack high-level scene context and rely on rigid reward functions, this paper introduces a novel pipeline combining neuro-symbolic feature representation and a Soft First-Order Logic reward function. The method integrates semantic, spatial, and shape information with spatially boosted features of dynamic entities, while using symbolic reasoning to balance human values through linguistic rules derived from segmentation maps. Experimental results in the CARLA simulator demonstrate that this approach enhances policy robustness and safety metrics across diverse traffic densities and occlusion levels, outperforming baseline methods and supporting more context-aware, value-aligned autonomous driving decisions.</div>
<div class="mono" style="margin-top:8px">针对现有基于摄像头的深度强化学习方法缺乏高层场景上下文且依赖固定奖励函数的不足，本文提出了一种结合神经符号特征表示和软一阶逻辑奖励函数的新流程。该方法整合了语义、空间和形状信息，并增强了动态实体的空间特征，同时通过从分割图提取的语义空间谓词应用语言规则进行符号推理，以平衡人类价值。在CARLA模拟环境中的定量实验表明，相较于基线方法，该神经符号表示和软一阶逻辑奖励函数在不同交通密度和遮挡水平下提高了策略鲁棒性和安全相关性能指标，证明了整合整体表示和软推理能促进自动驾驶中更情境感知和价值对齐的决策。</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance</div>
<div class="meta-line">Authors: Agni Bandyopadhyay, Gunther Waxenegger-Wilfing</div>
<div class="meta-line">First: 2026-02-04T21:49:20+00:00 · Latest: 2026-02-04T21:49:20+00:00</div>
<div class="meta-line">Comments: Accpeted at Conference: 15th IAA Symposium on Small Satellites for Earth System Observation At: Berlin</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05075v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的多碎片交会任务规划优化：融合燃料补给与自适应碰撞规避</div>
<div class="mono" style="margin-top:8px">随着地球轨道环境日益拥挤，主动碎片清除任务面临确保安全运行与降低在轨碰撞风险的双重挑战。本研究提出一种基于强化学习的框架，旨在增强主动碎片清除任务中的自适应碰撞规避能力，特别针对采用微小卫星的多碎片清除场景。微小卫星凭借其灵活性、成本效益和机动性优势，正逐渐成为动态任务（如碎片清除）的理想平台。
该框架在现有多碎片交会研究基础上，整合了燃料补给策略、高效任务规划与自适应碰撞规避机制，以优化航天器交会操作。研究采用掩码近端策略优化算法，使智能体能根据实时轨道条件动态调整机动策略，核心考量包括燃料效率、主动碰撞区域规避及动态轨道参数优化。
强化学习智能体通过训练掌握多碎片目标的高效交会序列规划能力，在整合必要燃料补给节点的同时优化燃料消耗与任务时长。研究采用基于铱星33号碎片数据集的仿真场景进行评估，涵盖多种轨道构型与碎片分布以验证方案的鲁棒性与适应性。结果表明，相较于传统启发式方法，该强化学习框架在提升任务效率的同时显著降低了碰撞风险。
本工作为复杂多碎片清除任务规划提供了可扩展的解决方案，并可推广至自主空间任务规划中的其他多目标交会问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing challenge of orbital debris and the need for safe, efficient active debris removal (ADR) missions, this study introduces a reinforcement learning framework that optimizes multi-debris rendezvous planning for small satellites. The method employs a masked Proximal Policy Optimization algorithm to dynamically plan mission sequences, integrating refueling strategies and adaptive collision avoidance to adjust maneuvers based on real-time orbital conditions. Experimental results using simulated scenarios from the Iridium 33 debris dataset demonstrate that the approach reduces collision risk and improves fuel and time efficiency compared to traditional heuristic methods, offering a scalable solution for autonomous space mission planning.</div>
<div class="mono" style="margin-top:8px">本研究针对日益严重的轨道碎片问题以及主动碎片清除任务对安全高效操作的需求，提出了一种基于强化学习的框架，用于优化小型卫星的多碎片交会规划。该方法采用掩码近端策略优化算法，动态规划任务序列，整合了燃料补给策略和自适应碰撞规避，以根据实时轨道条件调整机动操作。基于铱星33碎片数据集的模拟实验结果表明，与传统启发式方法相比，该框架降低了碰撞风险，并提高了燃料和时间效率，为自主空间任务规划提供了一个可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">StagePilot: A Deep Reinforcement Learning Agent for Stage-Controlled Cybergrooming Simulation</div>
<div class="meta-line">Authors: Heajun An, Qi Zhang, Minqian Liu, Xinyi Zhang, Sang Won Lee, Lifu Huang, Pamela J. Wisniewski, Jin-Hee Cho</div>
<div class="meta-line">First: 2026-02-04T21:22:45+00:00 · Latest: 2026-02-04T21:22:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05060v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cybergrooming is an evolving threat to youth, necessitating proactive educational interventions. We propose StagePilot, an offline RL-based dialogue agent that simulates the stage-wise progression of grooming behaviors for prevention training. StagePilot selects conversational stages using a composite reward that balances user sentiment and goal proximity, with transitions constrained to adjacent stages for realism and interpretability. We evaluate StagePilot through LLM-based simulations, measuring stage completion, dialogue efficiency, and emotional engagement. Results show that StagePilot generates realistic and coherent conversations aligned with grooming dynamics. Among tested methods, the IQL+AWAC agent achieves the best balance between strategic planning and emotional coherence, reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StagePilot：一种用于阶段控制型网络诱骗模拟的深度强化学习智能体</div>
<div class="mono" style="margin-top:8px">网络诱骗是对青少年的持续威胁，亟需主动式教育干预。本文提出StagePilot——一种基于离线强化学习的对话智能体，通过模拟诱骗行为的阶段性演进用于预防训练。该智能体采用综合奖励机制（平衡用户情感与目标接近度）选择对话阶段，并通过限制阶段间相邻转移以保持真实性与可解释性。我们基于大语言模型仿真评估StagePilot，测量阶段完成度、对话效率及情感参与度。结果表明，StagePilot能生成符合诱骗动态的真实连贯对话。在测试方法中，IQL+AWAC智能体在策略规划与情感连贯性间取得最佳平衡，其抵达最终阶段的频率较基线提升43%，同时保持70%以上的情感对齐度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for proactive educational tools against cybergrooming threats to youth, this paper introduces StagePilot, an offline reinforcement learning dialogue agent designed to simulate the stage-wise progression of grooming behaviors for prevention training. The method employs a composite reward balancing user sentiment and goal proximity, with transitions constrained to adjacent stages to ensure realism and interpretability. Experimental evaluation using LLM-based simulations demonstrates that StagePilot generates realistic and coherent conversations aligned with grooming dynamics, with the IQL+AWAC agent achieving the best performance by reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.</div>
<div class="mono" style="margin-top:8px">针对网络诱拐对青少年的持续威胁，本文提出StagePilot，一种基于离线强化学习的对话智能体，旨在模拟诱拐行为的阶段性进展以用于预防培训。该方法采用复合奖励机制，平衡用户情感与目标接近度，并通过限制阶段间相邻转换来确保真实性与可解释性。基于大语言模型的实验评估表明，StagePilot能生成符合诱拐动态的真实连贯对话，其中IQL+AWAC智能体在策略规划与情感一致性上表现最佳，相比基线方法到达最终阶段的频率提高达43%，同时保持超过70%的情感对齐度。</div>
</details>
</div>
<div class="card">
<div class="title">ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation</div>
<div class="meta-line">Authors: Songyuan Zhang, Oswin So, H. M. Sabbir Ahmad, Eric Yang Yu, Matthew Cleaveland, Mitchell Black, Chuchu Fan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-04T21:03:11+00:00 · Latest: 2026-02-04T21:03:11+00:00</div>
<div class="meta-line">Comments: 24 pages, 17 figures; Accepted by the fourteenth International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05051v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05051v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReFORM：通过噪声操纵实现支撑集约束离线强化学习的反射流方法</div>
<div class="mono" style="margin-top:8px">离线强化学习旨在从行为策略生成的固定数据集中学习最优策略，无需额外环境交互。该场景下的常见挑战是分布外误差，即策略偏离训练分布时产生的误差。现有方法通过惩罚统计距离项使策略接近行为策略，但这会限制策略改进且无法完全避免分布外动作。另一挑战是最优策略分布可能呈多模态且难以表征。近期研究采用扩散或流策略解决此问题，但如何在保持策略表达能力的同时避免分布外误差仍不明确。本文提出ReFORM——一种基于流策略的离线强化学习方法，通过结构设计强制执行约束更宽松的支撑集条件。该方法首先学习具有有界源分布的行为克隆流策略以捕捉动作分布的支撑集，随后优化反射流为行为克隆流生成有界噪声（同时保持支撑集）以最大化性能。在OGBench基准的40项挑战性任务中（涵盖不同质量数据集且所有任务使用恒定超参数），ReFORM在性能曲线图上全面超越所有经过人工调参的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address two key challenges in offline reinforcement learning: the out-of-distribution error that arises when policies deviate from the training data, and the difficulty of representing multimodal optimal policy distributions. The method, named ReFORM, introduces a flow-based approach that enforces a support constraint by first learning a behavior cloning flow policy to capture the action distribution&#x27;s support, and then optimizing a reflected flow that generates bounded noise to maximize performance while staying within the support. Experimental results on 40 tasks from the OGBench benchmark show that ReFORM, using constant hyperparameters across all tasks, outperforms all baselines with hand-tuned hyperparameters on performance profile curves.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线强化学习中的两个关键挑战：策略偏离训练数据时产生的分布外误差，以及表示多模态最优策略分布的困难。所提出的方法ReFORM采用基于流的策略，通过首先学习一个行为克隆流策略来捕捉动作分布的支撑集，然后优化一个反射流，该流生成有界噪声以最大化性能，同时保持在支撑集内。在OGBench基准的40个任务上的实验结果表明，ReFORM在所有任务中使用恒定超参数，在性能曲线图上优于所有手动调优超参数的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Laplacian Representations for Decision-Time Planning</div>
<div class="meta-line">Authors: Dikshant Shehmar, Matthew Schlegel, Matthew E. Taylor, Marlos C. Machado</div>
<div class="meta-line">First: 2026-02-04T20:34:50+00:00 · Latest: 2026-02-04T20:34:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05031v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拉普拉斯表示在决策时规划中的应用</div>
<div class="mono" style="margin-top:8px">在基于模型的强化学习中，利用学习到的模型进行规划仍是一个核心挑战。决策时规划中，状态表示至关重要，其需支持局部代价计算并保持长时程结构。本文证明，拉普拉斯表示通过捕捉多时间尺度的状态空间距离，为规划提供了有效的潜在空间。该表示保留了有意义的距离，自然地将长时程问题分解为子目标，并缓解了长预测时域中产生的误差累积。基于这些特性，我们提出了分层规划算法ALPS，并在OGBench基准测试中的一系列离线目标条件强化学习任务上验证了其优于常用基线方法的表现，该基准此前主要由无模型方法主导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of decision-time planning in model-based reinforcement learning, where effective state representations are needed to compute local costs while maintaining long-horizon structure. The method proposes using Laplacian representations, which capture multi-scale state-space distances to preserve meaningful metrics, decompose long-horizon problems into subgoals, and reduce compounding errors. Experimental results on offline goal-conditioned RL tasks from OGBench show that the introduced hierarchical algorithm, ALPS, outperforms common baselines, surpassing previous model-free approaches.</div>
<div class="mono" style="margin-top:8px">本文针对基于模型的强化学习中的决策时规划挑战，旨在寻找能支持局部成本计算并保持长时程结构的有效状态表示。方法提出使用拉普拉斯表示，通过捕捉多时间尺度的状态空间距离来保留有意义的度量、将长时程问题分解为子目标，并减少长期预测中的误差累积。在OGBench的离线目标条件强化学习任务上的实验结果表明，所引入的分层规划算法ALPS优于常用基线，超越了先前以无模型方法为主的性能。</div>
</details>
</div>
<div class="card">
<div class="title">GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA</div>
<div class="meta-line">Authors: Zhichao Wang</div>
<div class="meta-line">First: 2025-10-27T21:18:19+00:00 · Latest: 2026-02-04T19:28:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23868v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.23868v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine \textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT minimizes the discrepancy between implicit and explicit reward models. It combines three key ideas: (1) the online multi-response generation and normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the implicit-explicit reward alignment principle of UNA. By jointly normalizing the implicit and explicit rewards, GIFT eliminates an otherwise intractable term that prevents effective use of implicit rewards. This normalization transforms the complex reward maximization objective into a simple mean squared error (MSE) loss between the normalized reward functions, converting a non-convex optimization problem into a convex, stable, and analytically differentiable formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy and thus retains exploration capability. Compared to GRPO, it requires fewer hyperparameters, converges faster, and generalizes better with significantly reduced training overfitting. Empirically, GIFT achieves superior reasoning and alignment performance on mathematical benchmarks while remaining computationally efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GIFT：基于群体相对隐式微调整合GRPO、DPO与UNA</div>
<div class="mono" style="margin-top:8px">本文提出一种新颖的强化学习框架——\textbf{群体相对隐式微调}（GIFT），用于对齐大语言模型。与PPO或GRPO直接最大化累积奖励不同，GIFT通过最小化隐式与显式奖励模型之间的差异实现优化。该框架融合三项核心思想：（1）GRPO的在线多响应生成与归一化机制；（2）DPO的隐式奖励建模方法；（3）UNA的隐式-显式奖励对齐原则。通过对隐式与显式奖励进行联合归一化，GIFT消除了阻碍隐式奖励有效使用的复杂项，将原本非凸的奖励最大化目标转化为归一化奖励函数间的均方误差损失，从而构建出凸优化、稳定且可解析微分的训练目标。相较于DPO、UNA等离线方法，GIFT保持在线策略特性并保留探索能力；相比GRPO，其超参数更少、收敛更快、泛化能力更强且显著降低训练过拟合。实验表明，GIFT在数学推理基准测试中取得卓越的对齐性能，同时保持高效计算特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GIFT, a reinforcement learning framework designed to align large language models by addressing the limitations of existing methods like PPO, GRPO, DPO, and UNA. Its motivation is to avoid directly maximizing cumulative rewards, instead minimizing the discrepancy between implicit and explicit reward models through a combination of GRPO&#x27;s online multi-response generation, DPO&#x27;s implicit reward formulation, and UNA&#x27;s alignment principle. The method jointly normalizes implicit and explicit rewards, transforming the optimization into a convex mean squared error loss, which simplifies training and enhances stability. Experimental results show that GIFT outperforms prior methods in reasoning and alignment on mathematical benchmarks, with faster convergence, better generalization, reduced overfitting, and maintained computational efficiency while retaining on-policy exploration.</div>
<div class="mono" style="margin-top:8px">本文提出了GIFT，一种用于对齐大语言模型的强化学习框架，旨在克服PPO、GRPO、DPO和UNA等现有方法的局限性。其动机是通过结合GRPO的在线多响应生成、DPO的隐式奖励公式和UNA的对齐原则，避免直接最大化累积奖励，转而最小化隐式与显式奖励模型之间的差异。该方法对隐式和显式奖励进行联合归一化，将优化问题转化为凸的均方误差损失，从而简化训练并提高稳定性。实验结果表明，GIFT在数学基准测试中实现了更优的推理和对齐性能，具有更快的收敛速度、更好的泛化能力、减少的训练过拟合，并在保持计算效率的同时保留了在线策略探索能力。</div>
</details>
</div>
<div class="card">
<div class="title">Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives</div>
<div class="meta-line">Authors: Ioannis Anagnostides, Itai Zilberstein, Zachary W. Sollie, Arman Kilic, Tuomas Sandholm</div>
<div class="meta-line">First: 2026-02-04T19:24:06+00:00 · Latest: 2026-02-04T19:24:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04990v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04990v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamental barrier: incentives. In this position paper, we highlight that organ allocation is not merely a static optimization problem, but rather a complex game involving transplant centers, clinicians, and regulators. Focusing on US adult heart transplant allocation, we identify critical incentive misalignments across the decision-making pipeline, and present data showing that they are having adverse consequences today. Our main position is that the next generation of allocation policies should be incentive aware. We outline a research agenda for the machine learning community, calling for the integration of mechanism design, strategic classification, causal inference, and social choice to ensure robustness, efficiency, and fairness in the face of strategic behavior from the various constituent groups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>立场：心脏移植分配政策优化的机器学习应考量激励因素</div>
<div class="mono" style="margin-top:8px">稀缺供体器官的分配是医疗领域最具影响力的算法挑战之一。尽管该领域正迅速从僵化的规则系统转向机器学习和数据驱动的优化，我们认为当前方法常忽视一个根本障碍：激励。本文强调，器官分配不仅是静态优化问题，更是涉及移植中心、临床医生和监管机构的复杂博弈。聚焦美国成人心脏移植分配，我们指出决策流程中关键的激励错配，并通过数据证明其已产生不利后果。核心立场是：新一代分配政策应具备激励意识。我们为机器学习界提出研究议程，呼吁整合机制设计、策略分类、因果推断与社会选择理论，以确保在各方策略行为下实现鲁棒性、效率与公平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This position paper argues that current machine learning approaches to optimizing heart transplant allocation in the US overlook the critical role of incentives and strategic behavior among transplant centers and clinicians, treating it as a static optimization problem rather than a complex game. The authors propose that next-generation policies must be incentive-aware, advocating for a research agenda that integrates mechanism design, strategic classification, and causal inference to ensure robustness and fairness. They support this by presenting data showing that existing incentive misalignments in the decision pipeline are already causing adverse consequences in practice.</div>
<div class="mono" style="margin-top:8px">这篇立场论文指出，当前美国心脏移植分配中基于机器学习的优化方法忽视了移植中心和临床医生等主体的激励与策略行为，错误地将其视为静态优化问题而非复杂博弈。作者主张下一代分配政策必须具备激励感知能力，并呼吁整合机制设计、策略分类和因果推断等工具进行研究，以确保系统的鲁棒性与公平性。他们通过展示决策流程中激励错位已导致实际负面后果的数据，支撑了这一核心观点。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
