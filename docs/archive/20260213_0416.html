<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-13 04:16</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260213_0416</div>
    <div class="row"><div class="card">
<div class="title">Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows</div>
<div class="meta-line">Authors: Shaswat Garg, Matin Moezzi, Brandon Da Silva</div>
<div class="meta-line">First: 2026-02-11T18:54:48+00:00 · Latest: 2026-02-11T18:54:48+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures, IEEE International Conference on Robotics and Automation 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11142v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offline or data-scarce regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive normalizing flow policies at both the high- and low-levels of the hierarchy is introduced. This design enables tractable log-likelihood computation, efficient sampling, and the ability to model rich multimodal behaviors. New theoretical guarantees are derived, including explicit KL-divergence bounds for Real-valued non-volume preserving (RealNVP) policies and PAC-style sample efficiency results, showing that NF-HIQL preserves stability while improving generalization. Empirically, NF-HIQL is evaluted across diverse long-horizon tasks in locomotion, ball-dribbling, and multi-step manipulation from OGBench. NF-HIQL consistently outperforms prior goal-conditioned and hierarchical baselines, demonstrating superior robustness under limited data and highlighting the potential of flow-based architectures for scalable, data-efficient hierarchical reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于归一化流的数据高效分层目标条件强化学习</div>
<div class="mono" style="margin-top:8px">分层目标条件强化学习（H-GCRL）通过将复杂长程任务分解为结构化子目标，为处理此类任务提供了强大框架。然而，其实际应用受限于数据效率低下与策略表达能力不足，尤其在离线或数据稀缺场景中。本研究提出基于归一化流的分层隐式Q学习（NF-HIQL），该创新框架在分层结构的高层与底层均采用表达能力更强的归一化流策略替代单峰高斯策略。该设计实现了可处理的似然对数计算、高效采样及对丰富多模态行为的建模能力。研究推导出新的理论保证，包括实值非体积保持（RealNVP）策略的显式KL散度边界与PAC式样本效率结果，证明NF-HIQL在保持稳定性的同时提升了泛化能力。在OGBench的运动控制、运球及多步操作等多样化长程任务上的实验表明，NF-HIQL持续优于现有目标条件与分层基线方法，在有限数据下展现出更强的鲁棒性，凸显了基于流的架构对可扩展、数据高效分层强化学习的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the poor data efficiency and limited expressivity of hierarchical goal-conditioned reinforcement learning (H-GCRL) in offline or data-scarce settings. The authors introduce NF-HIQL, a framework that replaces unimodal Gaussian policies with expressive normalizing flow policies at both hierarchical levels, enabling tractable log-likelihood computation, efficient sampling, and the modeling of multimodal behaviors. The method is supported by new theoretical guarantees, including KL-divergence bounds and PAC-style sample efficiency results, and empirical evaluations on long-horizon locomotion, ball-dribbling, and manipulation tasks from OGBench show that NF-HIQL consistently outperforms prior baselines, demonstrating superior robustness with limited data.</div>
<div class="mono" style="margin-top:8px">本文针对分层目标条件强化学习（H-GCRL）在离线或数据稀缺场景下数据效率低、策略表达能力有限的问题，提出了NF-HIQL框架。该方法在分层结构的高层和底层均使用归一化流策略替代单峰高斯策略，实现了可处理的似然计算、高效采样和对多模态行为的建模。研究提供了包括KL散度边界和PAC式样本效率结果在内的新理论保证，并在OGBench的长时程运动、运球和操作任务上进行了实证评估。结果表明，NF-HIQL在数据有限的情况下始终优于现有基线，展现出更强的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Capabilities of Reinforcement Learning via Text Feedback</div>
<div class="meta-line">Authors: Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T18:56:56+00:00 · Latest: 2026-02-11T18:43:26+00:00</div>
<div class="meta-line">Comments: 43 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02482v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02482v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本反馈拓展强化学习能力</div>
<div class="mono" style="margin-top:8px">强化学习在大型语言模型后训练中的成功源于一个信息量极低的来源：每次训练轮次仅提供单比特信息，如二元奖励或偏好标签。另一极端，蒸馏方法虽能提供密集监督，但需要成本高昂且难以扩展的演示样本。本研究将文本反馈作为一种中间信号进行探索：它比标量奖励更丰富，又比完整演示更经济。文本反馈是人类互动的自然模式，已在众多现实场景中广泛存在，用户、标注者和自动评估系统常以此评判语言模型输出。为规模化利用文本反馈，我们形式化了一个多轮强化学习框架——基于文本反馈的强化学习（RLTF），其中训练阶段可获得文本反馈，但推理阶段则无。因此，模型必须学会内化反馈以提升测试阶段的单轮表现。为此，我们提出两种方法：自蒸馏（RLTF-SD），训练单轮策略以匹配其自身基于反馈的第二轮生成结果；以及反馈建模（RLTF-FM），通过预测反馈作为辅助目标。我们对两种方法进行了理论分析，并在推理谜题、竞赛数学和创意写作任务上进行了实证评估。结果表明，两种方法在多个基准测试中均持续优于强基线，凸显了强化学习结合规模化丰富监督源的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of existing reinforcement learning (RL) approaches for large language model (LLM) post-training, which rely on either sparse binary rewards or costly full demonstrations. To address this, the authors propose using text feedback as an intermediate supervision signal, formalizing a multi-turn RL setup called RL from Text Feedback (RLTF) where feedback is available only during training. They introduce two methods: Self Distillation (RLTF-SD), which aligns the single-turn policy with its own feedback-conditioned second-turn outputs, and Feedback Modeling (RLTF-FM), which predicts feedback as an auxiliary task. Experimental results on reasoning puzzles, competition math, and creative writing tasks demonstrate that both methods consistently outperform strong baselines, showcasing the effectiveness of leveraging rich textual feedback for scalable RL.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有强化学习（RL）在大语言模型（LLM）后训练中的局限性，即依赖稀疏的二元奖励或成本高昂的完整演示。为解决这一问题，作者提出使用文本反馈作为中间监督信号，形式化了一个多轮RL设置，称为“来自文本反馈的强化学习”（RLTF），其中反馈仅在训练期间可用。他们引入了两种方法：自蒸馏（RLTF-SD），使单轮策略与自身反馈条件下的第二轮输出对齐；以及反馈建模（RLTF-FM），将反馈预测作为辅助任务。在推理谜题、竞赛数学和创意写作任务上的实验结果表明，这两种方法均持续优于强基线，凸显了利用丰富文本反馈进行可扩展RL的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Reinhard Heckel, Mahdi Soltanolkotabi, Christos Thramboulidis</div>
<div class="meta-line">First: 2026-02-11T18:39:42+00:00 · Latest: 2026-02-11T18:39:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可验证奖励的强化学习中的非对称提示加权方法</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习推动了近期大语言模型后训练的发展，尤其在推理任务中。策略优化算法为给定提示生成多个响应，并根据奖励对相应梯度进行有效加权。主流算法如GRPO、DAPO和RLOO主要关注模糊提示（即具有中等成功概率的提示），同时降低对极简单和极难提示的梯度权重。本文提出一种非对称提示加权方法，对经验成功概率较低甚至为零的提示赋予更高权重。研究发现，非对称加权特别有利于从零开始的强化学习（如R1-Zero），因为此类训练需跨越较宽的准确率范围；而在后SFT强化学习中，由于模型已具备较高初始准确率，其增益相对有限。本文还提供了理论分析，刻画了在固定更新预算下，将成功概率从初始水平提升至目标准确率所需时间最小化的最优提示权重。在低成功率场景中，由于有效响应稀缺且响应成本占主导，这些最优权重呈现非对称性——通过提升低成功概率提示的权重，加速了有效时间收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of efficiently training large language models via reinforcement learning with verifiable rewards, particularly in low-success regimes where informative responses are scarce. The authors propose asymmetric prompt weighting, a method that assigns higher gradient weights to prompts with low or zero empirical success probability, contrasting with existing algorithms that focus on ambiguous prompts. Experimental results show that this asymmetric approach significantly accelerates training in from-scratch RL scenarios, such as R1-Zero, by improving convergence speed when initial accuracy is low, though it offers less benefit in post-SFT RL where models start with high accuracy. Theoretical analysis supports that optimal weights in low-success regimes are asymmetric, upweighting low probabilities to minimize the time needed to reach target accuracy under a fixed update budget.</div>
<div class="mono" style="margin-top:8px">本文针对基于可验证奖励的强化学习在训练大型语言模型时面临的效率挑战，特别是在低成功率区域中信息性响应稀缺的问题。作者提出了非对称提示加权方法，该方法为经验成功率低或为零的提示分配更高的梯度权重，与现有专注于模糊提示的算法形成对比。实验结果表明，这种非对称方法在从零开始的强化学习场景（如R1-Zero）中显著加速了训练，通过提高低初始准确率下的收敛速度，但在后SFT强化学习中，由于模型起始准确率较高，其益处较小。理论分析表明，在低成功率区域，最优权重是非对称的，通过上加权低概率提示，可以在固定更新预算下最小化达到目标准确率所需的时间。</div>
</details>
</div>
<div class="card">
<div class="title">Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaoyang Wang, Canwen Xu, Boyi Liu, Yite Wang, Siwei Han, Zhewei Yao, Huaxiu Yao, Yuxiong He</div>
<div class="meta-line">First: 2026-02-10T18:55:41+00:00 · Latest: 2026-02-11T18:20:25+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10090v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10090v2">PDF</a> · <a href="https://github.com/Snowflake-Labs/agent-world-model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能体世界模型：面向智能体强化学习的无限合成环境</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的最新进展使自主智能体能够执行需要与工具和环境进行多轮交互的复杂任务。然而，由于缺乏多样且可靠的环境，此类智能体训练的规模化受到限制。本文提出智能体世界模型（AWM），一种完全合成的环境生成流程。利用该流程，我们扩展至覆盖日常场景的1000个环境，智能体可在其中与丰富的工具集（平均每个环境35个工具）交互并获得高质量观测。值得注意的是，这些环境由代码驱动并依托数据库支持，相比LLM模拟的环境能提供更可靠、一致的状态转换。此外，与从真实环境收集轨迹相比，它们能实现更高效的智能体交互。为验证该资源的有效性，我们对多轮工具使用智能体进行了大规模强化学习训练。得益于完全可执行的环境和可访问的数据库状态，我们还能设计可靠的奖励函数。在三个基准测试上的实验表明，仅在合成环境中训练（而非特定基准环境）能产生强大的分布外泛化能力。代码发布于https://github.com/Snowflake-Labs/agent-world-model。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation in scaling autonomous agent training due to a lack of diverse and reliable environments by proposing Agent World Model (AWM), a fully synthetic environment generation pipeline. The method creates 1,000 code-driven, database-backed environments covering everyday scenarios with rich toolsets, offering more reliable state transitions than LLM-simulated ones and enabling efficient agent interaction. Experimental results from large-scale reinforcement learning on three benchmarks demonstrate that training exclusively in these synthetic environments yields strong out-of-distribution generalization, supported by reliable reward functions derived from executable environments and accessible database states.</div>
<div class="mono" style="margin-top:8px">本文针对自主智能体训练因缺乏多样可靠环境而难以扩展的问题，提出了Agent World Model（AWM），一种完全合成的环境生成流程。该方法创建了1000个代码驱动、数据库支持的日常场景环境，配备丰富的工具集，比基于大语言模型模拟的环境提供更可靠的状态转换，并支持高效的智能体交互。在三个基准测试上的大规模强化学习实验表明，仅在这些合成环境中训练即可产生强大的分布外泛化能力，这得益于可执行环境和可访问数据库状态所设计的可靠奖励函数。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away</div>
<div class="meta-line">Authors: Soumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Furong Huang, Dinesh Manocha, Amrit Singh Bedi</div>
<div class="meta-line">First: 2026-02-11T18:09:17+00:00 · Latest: 2026-02-11T18:09:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11096v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11096v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix (&quot;Wait, think safely&quot;) only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理模型的安全恢复仅需少量早期引导步骤</div>
<div class="mono" style="margin-top:8px">基于强化学习的显式思维链后训练（如GRPO）提升了多模态大规模推理模型的推理能力，但近期研究表明这会同时削弱安全对齐并增加越狱成功率。我们提出SafeThink，一种轻量级推理时防御方法，将安全恢复视为满足性约束而非最大化目标。SafeThink通过安全奖励模型监控演进的推理轨迹，仅在安全阈值被突破时有条件地注入优化后的短校正前缀（“等等，请安全思考”）。在六款开源多模态推理模型和四个越狱基准测试（JailbreakV-28K、Hades、FigStep、MM-SafetyBench）的评估中，SafeThink将攻击成功率降低30-60%（例如LlamaV-o1在JailbreakV-28K上从63.33%降至5.74%，R1-Onevision在Hades上从69.07%降至5.65%），同时保持推理性能（MathVista准确率从65.20%微降至65.00%）。实验的关键实证发现是：安全恢复通常仅需少量引导步骤——在前1-3个推理步骤中进行干预，便足以将完整生成过程导向安全结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that reinforcement learning-based post-training for explicit chain-of-thought reasoning in multimodal large-scale reasoning models (MLRMs) often degrades safety alignment and increases jailbreak vulnerability, this paper introduces SafeThink, a lightweight inference-time defense method. The approach treats safety recovery as a satisficing constraint, monitoring the reasoning trace with a safety reward model and conditionally injecting a short optimized corrective prefix (e.g., &quot;Wait, think safely&quot;) only when a safety threshold is violated. Experimental results across six open-source MLRMs and four jailbreak benchmarks demonstrate that SafeThink reduces attack success rates by 30-60%—for instance, lowering LlamaV-o1&#x27;s rate from 63.33% to 5.74% on JailbreakV-28K—while largely preserving reasoning performance, such as maintaining MathVista accuracy at 65.00%. A key finding is that safety recovery typically requires only a few early steering steps, with intervention in the first 1-3 reasoning steps often sufficient to redirect generations toward safe completions.</div>
<div class="mono" style="margin-top:8px">本文的动机源于观察到，基于强化学习的显式思维链后训练在提升多模态大规模推理模型（MLRMs）推理能力的同时，往往会削弱其安全对齐性并增加越狱成功率。为此，论文提出了SafeThink这一轻量级推理时防御方法，该方法将安全恢复视为一个满足性约束而非最大化目标，通过安全奖励模型监控推理过程，仅在安全阈值被违反时条件性地注入一个简短的优化纠正前缀（如“等等，请安全思考”）。在六个开源MLRMs和四个越狱基准（如JailbreakV-28K、Hades）上的实验结果表明，SafeThink能将攻击成功率降低30-60%（例如，LlamaV-o1在JailbreakV-28K上的成功率从63.33%降至5.74%），同时基本保持推理性能（如MathVista准确率从65.20%微降至65.00%）。一个关键实证发现是，安全恢复通常只需少数早期引导步骤，在最初1-3个推理步骤中进行干预往往足以将整个生成过程转向安全完成。</div>
</details>
</div>
<div class="card">
<div class="title">DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning</div>
<div class="meta-line">Authors: Yicheng Chen, Zerun Ma, Xinchen Xie, Yining Li, Kai Chen</div>
<div class="meta-line">First: 2026-02-11T17:56:15+00:00 · Latest: 2026-02-11T17:56:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11089v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11089v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate \emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME&#x27;25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DataChef：通过强化学习为LLM适配烹饪最优数据配方</div>
<div class="mono" style="margin-top:8px">在当前大语言模型（LLM）的发展中，大规模高质量训练数据的构建是模型性能的主要驱动力。其中关键环节是“数据配方”——即通过数据处理流水线将原始数据源转化为训练语料库。尽管LLM已越来越多地用于自动化单个数据处理步骤（如数据合成与过滤），但数据配方的整体设计仍高度依赖人工，需要大量专业知识和迭代优化。为填补这一空白，我们提出了面向LLM适配的“端到端数据配方生成”框架：给定目标基准测试和可用数据源池，模型需输出完整的数据配方，使基础LLM适配目标任务。我们提出的DataChef-32B模型采用在线强化学习方法，通过代理奖励函数预测候选配方的下游性能。在六项保留任务中，DataChef-32B生成的实用配方达到了与专家人工构建配方相当的下游性能。值得注意的是，DataChef-32B生成的配方成功将Qwen3-1.7B-Base适配至数学领域，在AIME&#x27;25测试中取得66.7分，超越了原版Qwen3-1.7B。这项工作为自动化LLM训练及开发自进化AI系统提供了新思路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the labor-intensive and expertise-dependent manual design of data recipes for LLM adaptation, this paper introduces DataChef-32B, a method that formulates end-to-end data recipe generation as a reinforcement learning problem, using a proxy reward to predict downstream performance from candidate recipes. The approach automates the creation of data processing pipelines from raw sources to optimize LLM training for target benchmarks. Experimental results across six held-out tasks show that DataChef-32B produces recipes achieving performance comparable to human-expert designs, with a notable example adapting Qwen3-1.7B-Base to the math domain, scoring 66.7 on AIME&#x27;25 and surpassing the original model.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型适配中数据配方设计依赖人工、耗时费力的问题，本文提出了DataChef-32B方法，将端到端数据配方生成构建为强化学习任务，通过代理奖励预测候选配方在下游任务的表现来自动化生成数据处理流程。该方法基于可用数据源和目标基准，自动优化训练语料库的构建。在六个保留任务上的实验结果表明，DataChef-32B生成的配方达到了与人类专家设计相当的性能，特别是在数学领域适配Qwen3-1.7B-Base模型时，在AIME&#x27;25基准上获得66.7分，超越了原模型性能。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable Attention-Based Multi-Agent PPO for Latency Spike Resolution in 6G RAN Slicing</div>
<div class="meta-line">Authors: Kavan Fatehi, Mostafa Rahmani Ghourtani, Amir Sonee, Poonam Yadav, Alessandra M Russo, Hamed Ahmadi, Radu Calinescu</div>
<div class="meta-line">First: 2026-02-11T17:44:03+00:00 · Latest: 2026-02-11T17:44:03+00:00</div>
<div class="meta-line">Comments: This work has been accepted to appear in the IEEE International Conference on Communications (ICC)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11076v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11076v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sixth-generation (6G) radio access networks (RANs) must enforce strict service-level agreements (SLAs) for heterogeneous slices, yet sudden latency spikes remain difficult to diagnose and resolve with conventional deep reinforcement learning (DRL) or explainable RL (XRL). We propose \emph{Attention-Enhanced Multi-Agent Proximal Policy Optimization (AE-MAPPO)}, which integrates six specialized attention mechanisms into multi-agent slice control and surfaces them as zero-cost, faithful explanations. The framework operates across O-RAN timescales with a three-phase strategy: predictive, reactive, and inter-slice optimization.
  A URLLC case study shows AE-MAPPO resolves a latency spike in $18$ms, restores latency to $0.98$ms with $99.9999\%$ reliability, and reduces troubleshooting time by $93\%$ while maintaining eMBB and mMTC continuity. These results confirm AE-MAPPO&#x27;s ability to combine SLA compliance with inherent interpretability, enabling trustworthy and real-time automation for 6G RAN slicing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向6G无线接入网切片时延尖峰消解的基于可解释注意力机制的多智能体PPO方法</div>
<div class="mono" style="margin-top:8px">第六代（6G）无线接入网（RAN）必须为异构切片执行严格的服务等级协议（SLA），但突发的时延尖峰问题仍难以通过传统深度强化学习（DRL）或可解释强化学习（XRL）进行诊断与解决。本文提出《注意力增强的多智能体近端策略优化（AE-MAPPO）》，该方法将六种专用注意力机制集成至多智能体切片控制中，并将其呈现为零成本、高保真的解释机制。该框架通过预测、响应和切片间优化三阶段策略，在O-RAN各时间尺度上运行。超可靠低时延通信案例研究表明，AE-MAPPO可在18毫秒内消解时延尖峰，以99.9999%的可靠性将时延恢复至0.98毫秒，并将故障排查时间减少93%，同时保持增强移动宽带和海量机器类通信的业务连续性。这些结果证实AE-MAPPO能够将SLA合规性与内在可解释性相结合，为6G RAN切片提供可信赖的实时自动化解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to diagnose and resolve sudden latency spikes while enforcing strict service-level agreements in 6G radio access network slicing, this paper proposes an Attention-Enhanced Multi-Agent Proximal Policy Optimization (AE-MAPPO) method. The approach integrates six specialized attention mechanisms into multi-agent slice control across predictive, reactive, and inter-slice optimization phases to provide faithful, zero-cost explanations. In a URLLC case study, experimental results demonstrate that AE-MAPPO resolves a latency spike in 18ms, restores latency to 0.98ms with 99.9999% reliability, reduces troubleshooting time by 93%, and maintains continuity for other slice types.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决6G无线接入网络切片中突发延迟尖峰的诊断与修复难题，同时确保严格的服务水平协议，提出了注意力增强的多智能体近端策略优化方法。该方法将六种专用注意力机制集成到多智能体切片控制中，通过预测、反应和切片间优化三阶段策略提供零成本、可信的解释。在URLLC案例实验中，该方法在18毫秒内解决延迟尖峰，将延迟恢复至0.98毫秒并实现99.9999%的可靠性，故障排除时间减少93%，且保持了其他切片类型的业务连续性。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tienie Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-11T17:42:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于图像对话的内省式视觉思考</div>
<div class="mono" style="margin-top:8px">当前的大型视觉语言模型通常依赖基于单次视觉编码的纯文本推理，这常导致细粒度视觉信息丢失。近期提出的“图像思考”方法尝试通过外部工具或代码操作图像来缓解此限制，但生成的视觉状态往往与语言语义关联不足，损害了有效的跨模态对齐——尤其在需要跨远距离区域或多图像进行视觉语义或几何关系推理时。为解决这些问题，我们提出“图像对话”框架，将视觉操作重构为语言引导的特征调制。在表达性语言提示的引导下，模型动态地对多图像区域进行联合重编码，实现语言推理与视觉状态更新的紧密耦合。我们在ViLaVT中实例化了这一范式，该新型大型视觉语言模型配备了专为此类交互式视觉推理设计的动态视觉编码器，并通过结合监督微调与强化学习的双阶段课程训练，以促进有效推理行为。在八个基准测试上的广泛实验表明，ViLaVT实现了显著且一致的性能提升，在复杂多图像和基于视频的空间推理任务上增益尤为突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of current large vision-language models, which often lose fine-grained visual details due to single-pass encoding and struggle with aligning visual states to linguistic semantics in complex reasoning tasks. The method introduces &#x27;chatting with images&#x27;, a framework that uses language prompts to guide dynamic re-encoding of multiple image regions, implemented in ViLaVT—a model with a dynamic vision encoder trained via a two-stage curriculum of supervised fine-tuning and reinforcement learning. Main experimental results show that ViLaVT achieves strong, consistent improvements across eight benchmarks, with notable gains on complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前大型视觉语言模型的局限性，这些模型因单次视觉编码常丢失细粒度视觉信息，且在复杂推理任务中难以将视觉状态与语言语义有效对齐。方法上提出了“与图像对话”框架，通过语言提示引导对多图像区域的动态重新编码，并在ViLaVT模型中实现，该模型配备了专为交互式视觉推理设计的动态视觉编码器，并通过监督微调和强化学习的两阶段课程进行训练。主要实验结果表明，ViLaVT在八个基准测试中均取得显著且一致的性能提升，尤其在复杂的多图像和基于视频的空间推理任务上表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning</div>
<div class="meta-line">Authors: Hang Wu, Yujun Cai, Zehao Li, Haonan Ge, Bowen Sun, Junsong Yuan, Yiwei Wang</div>
<div class="meta-line">First: 2026-01-30T04:45:43+00:00 · Latest: 2026-02-11T17:26:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00181v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00181v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CamReasoner：通过结构化空间推理增强摄像机运动理解</div>
<div class="mono" style="margin-top:8px">理解摄像机动态是视频空间智能的基础支柱。然而，现有多模态模型主要将此任务视为黑盒分类，常因依赖表层视觉模式而非几何线索而混淆物理性质不同的运动。我们提出CamReasoner框架，将摄像机运动理解重构为结构化推理过程，以弥合感知与影视逻辑之间的鸿沟。该方法以观察-思考-回答范式为核心，迫使模型在显式推理模块中解码轨迹和视锥体等时空线索。为培养此能力，我们构建了包含1.8万条SFT推理链和3.8万条RL反馈样本的大规模推理轨迹数据集。值得注意的是，我们首次在该领域采用强化学习实现逻辑对齐，确保运动推理基于物理几何而非上下文猜测。通过将强化学习应用于观察-思考-回答推理范式，CamReasoner有效抑制幻觉生成，并在多个基准测试中取得最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing multimodal models that treat camera movement understanding as a black-box classification and often confuse motions by relying on superficial patterns, this paper introduces CamReasoner, a framework that reformulates the task as a structured inference process to bridge perception and cinematic logic. The method centers on an Observation-Thinking-Answer paradigm, which decodes spatio-temporal cues like trajectories and view frustums within an explicit reasoning block, and is trained using a Large-scale Inference Trajectory Suite with 18k SFT chains and 38k RL feedback samples, where RL is innovatively applied for logical alignment. The main experimental results show that CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks by grounding motion inferences in physical geometry.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有多模态模型将相机运动理解视为黑盒分类，常依赖表面视觉模式混淆不同运动，因此提出CamReasoner框架，将该任务重构为结构化推理过程以连接感知与电影逻辑。方法核心是观察-思考-回答范式，在显式推理块中解码轨迹和视锥等时空线索，并通过包含1.8万条SFT推理链和3.8万条RL反馈样本的大规模推理轨迹套件进行训练，其中首次在该领域使用强化学习实现逻辑对齐。主要实验结果表明，CamReasoner能有效抑制幻觉，通过将运动推理基于物理几何，在多个基准测试中取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models</div>
<div class="meta-line">Authors: Xinyu Yuan, Yan Qiao, Zonghui Wang, Wenzhi Chen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-11T17:24:49+00:00 · Latest: 2026-02-11T17:24:49+00:00</div>
<div class="meta-line">Comments: Published as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11057v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11057v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered &quot;agent&quot;, and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (&lt;10\% performance degradation under link failures or flow bursts), demonstrating MLM&#x27;s generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分而治之，协调制胜：运用多模态语言模型求解多商品流问题</div>
<div class="mono" style="margin-top:8px">多商品流（MCF）问题是网络流与组合优化的基础课题，在交通、通信、物流等领域具有广泛应用。当前分配系统的快速扩张对现有优化引擎在平衡最优性与可解性方面提出了挑战。本文提出Pram——首个基于机器学习的方法，利用多模态语言模型（MLM）的推理能力解决服务提供商亟需权衡的困境。Pram的核心设计包括：（1）通过将原问题分解为局部子问题，由MLM驱动的“智能体”快速计算高质量分配方案；（2）采用多智能体强化学习算法协调子问题以确保全局一致性。理论证明Pram通过上下文学习执行梯度下降，可在MCF问题族内收敛至最优解。实证研究表明，在真实数据集与公共拓扑上，Pram性能与线性规划求解器相当（部分场景甚至超越），且运行时间显著降低（提速1-2个数量级）。此外，Pram展现出强鲁棒性（链路故障或流量突发时性能衰减&lt;10%），验证了MLM对未预见事件的泛化能力。该方法具有目标无关性，可无缝集成主流分配系统，为未来网络提供实用可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to balance optimality and tractability in large-scale allocation systems like transportation and logistics, this paper introduces Pram, a method that uses multimodal language models (MLMs) to solve multi-commodity flow problems. The approach divides the problem into local subproblems handled by MLM agents and harmonizes them via multi-agent reinforcement learning to ensure global consistency. Theoretically, Pram converges to optimal solutions, and experiments on real-world datasets show it matches or exceeds linear programming solvers in performance while being 10 to 100 times faster, with strong robustness to network disruptions.</div>
<div class="mono" style="margin-top:8px">本文针对交通、物流等大规模分配系统中优化性与可处理性难以平衡的问题，提出了Pram方法，利用多模态语言模型解决多商品流问题。该方法将原问题分解为局部子问题，由多模态语言模型代理处理，并通过多智能体强化学习协调以确保全局一致性。理论证明Pram能收敛至最优解，在真实数据集上的实验表明，其性能媲美甚至超越线性规划求解器，且运行速度快10到100倍，对链路故障或流量突发等意外事件表现出强鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Goal-Conditioned Reinforcement Learning from Sub-Optimal Data on Metric Spaces</div>
<div class="meta-line">Authors: Alfredo Reichlin, Miguel Vasco, Hang Yin, Danica Kragic</div>
<div class="meta-line">First: 2024-02-16T16:46:53+00:00 · Latest: 2026-02-11T17:20:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.10820v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.10820v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of learning optimal behavior from sub-optimal datasets for goal-conditioned offline reinforcement learning under sparse rewards, invertible actions and deterministic transitions. To mitigate the effects of \emph{distribution shift}, we propose MetricRL, a method that combines metric learning for value function approximation with weighted imitation learning for policy estimation. MetricRL avoids conservative or behavior-cloning constraints, enabling effective learning even in severely sub-optimal regimes. We introduce distance monotonicity as a key property linking metric representations to optimality and design an objective that explicitly promotes it. Empirically, MetricRL consistently outperforms prior state-of-the-art goal-conditioned RL methods in recovering near-optimal behavior from sub-optimal offline data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>度量空间上基于次优数据的目标条件强化学习</div>
<div class="mono" style="margin-top:8px">本研究针对稀疏奖励、可逆动作与确定性转移条件下的目标条件离线强化学习，探索从次优数据集中学习最优行为的问题。为缓解分布偏移的影响，我们提出MetricRL方法，该方法将价值函数逼近的度量学习与策略估计的加权模仿学习相结合。MetricRL避免了保守约束或行为克隆限制，即使在严重次优条件下也能实现有效学习。我们引入距离单调性作为连接度量表示与最优性的关键性质，并设计了显式促进该性质的优化目标。实验表明，在从次优离线数据恢复近似最优行为方面，MetricRL持续优于现有最先进的目标条件强化学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of learning optimal goal-conditioned policies from sub-optimal offline datasets, where distribution shift and sparse rewards pose significant difficulties. The proposed method, MetricRL, integrates metric learning for value function approximation with weighted imitation learning for policy estimation, avoiding overly conservative constraints. A key innovation is the introduction of distance monotonicity, a property linking metric representations to optimality, which is explicitly promoted in the learning objective. Experimental results demonstrate that MetricRL consistently outperforms prior state-of-the-art methods in recovering near-optimal behavior from sub-optimal data.</div>
<div class="mono" style="margin-top:8px">本文研究了在稀疏奖励、可逆动作和确定性转移条件下，如何从次优离线数据中学习目标条件的最优策略。为缓解分布偏移的影响，作者提出了MetricRL方法，该方法将用于价值函数近似的度量学习与用于策略估计的加权模仿学习相结合，避免了保守或行为克隆的约束。其核心创新是引入了距离单调性这一关键属性，将度量表示与最优性联系起来，并在目标函数中明确促进该属性。实验结果表明，MetricRL在从次优离线数据中恢复接近最优行为方面，持续优于先前最先进的目标条件强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Bizhe Bai, Xinyue Wang, Peng Ye, Tao Chen</div>
<div class="meta-line">First: 2026-01-30T13:10:30+00:00 · Latest: 2026-02-11T16:53:48+00:00</div>
<div class="meta-line">Comments: 17 pages, 10 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02555v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02555v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) improves LLM reasoning, yet growing evidence indicates an exploration ceiling: it often reweights existing solution traces rather than discovering new strategies, limiting gains under large sampling budgets (e.g., pass-at-256). We address this limitation with PSN-RLVR, which perturbs policy parameters before rollout generation to induce temporally consistent, trajectory-level exploration that better preserves long-horizon chain-of-thought coherence than action-space noise. To mitigate the resulting sampling-update mismatch, we incorporate truncated importance sampling (TIS). To avoid expensive KL-based adaptive noise control, we propose a computationally efficient real-time adaptive noise scheduler driven by a lightweight surrogate that combines semantic diversity with normalized self-certainty. Instantiated on GRPO, a widely used RLVR method, PSN-GRPO consistently expands the effective reasoning capability boundary across multiple mathematical reasoning benchmarks and model families, yielding higher pass-at-k under large sampling budgets and outperforming prior exploration-oriented RLVR methods (e.g., Pass-at-k-style training) while remaining orthogonal and thus composable for additional gains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于参数空间噪声的探索学习：强化学习中可验证奖励的参数空间噪声深度解析</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）虽能提升大语言模型的推理能力，但日益增多的证据表明其存在探索瓶颈：该方法往往仅对现有解轨迹进行重加权，而非发现新策略，导致在大规模采样预算（如256次采样通过率）下收益受限。我们提出PSN-RLVR解决此问题，该方法在轨迹生成前扰动策略参数，以诱导时间一致、轨迹层级的探索，相比动作空间噪声更能保持长程思维链的连贯性。为缓解由此产生的采样-更新失配，我们引入截断重要性采样（TIS）。为避免基于KL散度的自适应噪声控制带来的高昂计算成本，我们提出一种计算高效的自适应噪声调度器，其通过结合语义多样性与归一化自置信度的轻量代理指标实现实时调控。基于广泛使用的RLVR方法GRPO实例化后，PSN-GRPO在多个数学推理基准和模型家族中持续扩展了有效推理能力边界，在大规模采样预算下获得更高的k次采样通过率，优于先前的探索导向型RLVR方法（如Pass-at-k式训练），同时保持正交性，可进一步组合其他方法以获得额外增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the exploration ceiling in Reinforcement Learning with Verifiable Rewards (RLVR), where existing methods often reweight known solution traces rather than discovering novel strategies, limiting performance gains under large sampling budgets. The proposed method, PSN-RLVR, introduces parameter-space noise to perturb policy parameters before rollout generation, enabling temporally consistent, trajectory-level exploration that better maintains long-horizon chain-of-thought coherence compared to action-space noise. To handle sampling-update mismatch, truncated importance sampling is incorporated, and a computationally efficient real-time adaptive noise scheduler is developed using a lightweight surrogate metric combining semantic diversity and normalized self-certainty, avoiding expensive KL-based control. Experimental results on GRPO, a widely used RLVR method, show that PSN-GRPO consistently expands effective reasoning boundaries across multiple mathematical reasoning benchmarks and model families, achieving higher pass-at-k scores under large sampling budgets and outperforming prior exploration-oriented RLVR approaches while remaining orthogonal for potential composition with other methods.</div>
<div class="mono" style="margin-top:8px">本文针对可验证奖励的强化学习（RLVR）中的探索瓶颈问题，现有方法常重新加权已知解轨迹而非发现新策略，限制了在大采样预算下的性能提升。提出的PSN-RLVR方法通过在策略生成前扰动策略参数，引入参数空间噪声，实现时间一致、轨迹级的探索，相比动作空间噪声能更好地保持长程思维链的连贯性。为处理采样与更新的不匹配，方法结合了截断重要性采样，并利用结合语义多样性和归一化自确定性的轻量代理指标，开发了计算高效的自适应噪声调度器，避免了基于KL的昂贵控制。在广泛使用的RLVR方法GRPO上的实验结果表明，PSN-GRPO在多个数学推理基准和模型家族中持续扩展了有效推理能力边界，在大采样预算下实现了更高的pass-at-k分数，优于先前的探索导向RLVR方法，同时保持正交性以便与其他方法组合使用。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, Wanli Ouyang, Feng Zhao</div>
<div class="meta-line">First: 2025-10-01T17:58:05+00:00 · Latest: 2026-02-11T16:48:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01304v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.01304v3">PDF</a> · <a href="https://github.com/yuzeng0-0/AGILE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向视觉语言模型感知与推理能力增强的自主拼图交互学习</div>
<div class="mono" style="margin-top:8px">尽管当前大型视觉语言模型在多模态理解与推理方面取得进展，但其基础感知与推理能力仍存在局限。具体而言，即使在简单拼图任务上，现有模型表现接近随机水平，揭示了核心感知与推理能力的不足。虽然高质量视觉语言数据可提升这些能力，但其稀缺性与有限可扩展性构成显著制约。为此，我们提出AGILE（自主拼图交互学习）方法，通过将拼图求解构建为交互过程，使模型能逐步与环境互动。在每一步中，模型根据当前状态生成可执行代码以执行动作，同时环境提供细粒度视觉反馈引导任务完成。通过这种观察与交互的迭代循环，模型借助探索与反馈持续提升感知与推理能力。实验表明，AGILE不仅在多种复杂度拼图任务上显著提升性能（如在2×2设置下准确率从9.5%提升至82.8%），还在9项通用视觉任务上展现出强泛化能力，平均提升达3.1%。这些结果表明模型在感知与推理能力上均获得显著增强。本研究为推进多模态模型的推理与泛化能力开辟了新途径，并为多模态强化学习数据稀缺问题提供了高效可扩展的解决方案。代码与数据集已开源：https://github.com/yuzeng0-0/AGILE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited perceptual and reasoning abilities of current Vision-Language Models (VLMs), which perform poorly even on simple jigsaw tasks, this paper introduces AGILE, an Agentic Jigsaw Interaction Learning method. The approach formulates jigsaw solving as an interactive process where the model generates executable code to act based on the current state and receives fine-grained visual feedback from the environment, enabling iterative improvement through exploration. Experimental results show that AGILE significantly boosts performance on jigsaw tasks, increasing accuracy from 9.5% to 82.8% in a 2x2 setting, and generalizes well to 9 general vision tasks with an average improvement of 3.1%, demonstrating enhanced perceptual and reasoning capabilities while addressing data scarcity in multimodal reinforcement learning.</div>
<div class="mono" style="margin-top:8px">针对当前视觉语言模型在感知和推理能力上的不足，即使在简单拼图任务上也表现不佳，本文提出了AGILE方法，即一种基于智能体交互的拼图学习框架。该方法将拼图求解构建为一个交互过程，模型根据当前状态生成可执行代码进行操作，并从环境中获得细粒度视觉反馈，通过探索实现迭代式能力提升。实验结果表明，AGILE显著提高了拼图任务性能，在2x2设置下准确率从9.5%提升至82.8%，并在9个通用视觉任务上平均提升3.1%，展现出增强的感知与推理能力，同时为多模态强化学习数据稀缺问题提供了高效可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Mediated Guidance of MARL Systems</div>
<div class="meta-line">Authors: Philipp D. Siedler, Ian Gemp</div>
<div class="meta-line">First: 2025-03-16T20:16:13+00:00 · Latest: 2026-02-11T16:37:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.13553v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.13553v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In complex multi-agent environments, achieving efficient learning and desirable behaviours is a significant challenge for Multi-Agent Reinforcement Learning (MARL) systems. This work explores the potential of combining MARL with Large Language Model (LLM)-mediated interventions to guide agents toward more desirable behaviours. Specifically, we investigate how LLMs can be used to interpret and facilitate interventions that shape the learning trajectories of multiple agents. We experimented with two types of interventions, referred to as controllers: a Natural Language (NL) Controller and a Rule-Based (RB) Controller. The RB Controller showed a stronger impact than the NL Controller, which uses a small (7B/8B) LLM to simulate human-like interventions. Our findings indicate that agents particularly benefit from early interventions, leading to more efficient training and higher performance. Both intervention types outperform the baseline without interventions, highlighting the potential of LLM-mediated guidance to accelerate training and enhance MARL performance in challenging environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM介导的多智能体强化学习系统引导</div>
<div class="mono" style="margin-top:8px">在复杂多智能体环境中，实现高效学习与理想行为是多智能体强化学习系统面临的重要挑战。本研究探索将MARL与大型语言模型介导的干预相结合，以引导智能体朝向更理想行为。具体而言，我们研究了如何利用LLM解释并实施干预，以塑造多智能体的学习轨迹。实验采用两种干预控制器：自然语言控制器与基于规则的控制器。结果显示，基于规则的控制器比使用小型LLM模拟类人干预的自然语言控制器影响更显著。研究发现早期干预对智能体尤为有益，可实现更高效的训练与更优性能。两种干预方式均优于无干预基线，表明LLM介导的引导在加速训练与提升MARL系统在复杂环境中的性能方面具有潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of achieving efficient learning and desirable behaviors in complex multi-agent environments using Multi-Agent Reinforcement Learning (MARL). The method explores combining MARL with Large Language Model (LLM)-mediated interventions to guide agents, specifically investigating how LLMs can interpret and facilitate interventions to shape learning trajectories. The main experimental results show that two types of intervention controllers—a Natural Language (NL) Controller using a small LLM and a Rule-Based (RB) Controller—were tested, with the RB Controller having a stronger impact; both controllers outperformed a baseline without interventions, and early interventions were found to lead to more efficient training and higher performance.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决复杂多智能体环境中使用多智能体强化学习（MARL）实现高效学习和理想行为的挑战。方法上探索了将MARL与大型语言模型（LLM）介导的干预相结合以引导智能体，具体研究了LLM如何解释和促进干预来塑造学习轨迹。主要实验结果表明，测试了两种干预控制器——使用小型LLM的自然语言（NL）控制器和基于规则（RB）的控制器，其中RB控制器影响更强；两种控制器均优于无干预的基线，且早期干预能带来更高效的训练和更高的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Optimal Reinforcement Learning under Safety Filtering</div>
<div class="meta-line">Authors: Donggeon David Oh, Duy P. Nguyen, Haimin Hu, Jaime F. Fisac</div>
<div class="meta-line">First: 2025-10-20T20:20:10+00:00 · Latest: 2026-02-11T16:22:56+00:00</div>
<div class="meta-line">Comments: Accepted for publication in the proceedings of The International Association for Safe &amp; Ethical AI (IASEAI) 2026; 17 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18082v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.18082v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) enable its use on increasingly complex tasks, but the lack of formal safety guarantees still limits its application in safety-critical settings. A common practical approach is to augment the RL policy with a safety filter that overrides unsafe actions to prevent failures during both training and deployment. However, safety filtering is often perceived as sacrificing performance and hindering the learning process. We show that this perceived safety-performance tradeoff is not inherent and prove, for the first time, that enforcing safety with a sufficiently permissive safety filter does not degrade asymptotic performance. We formalize RL safety with a safety-critical Markov decision process (SC-MDP), which requires categorical, rather than high-probability, avoidance of catastrophic failure states. Additionally, we define an associated filtered MDP in which all actions result in safe effects, thanks to a safety filter that is considered to be a part of the environment. Our main theorem establishes that (i) learning in the filtered MDP is safe categorically, (ii) standard RL convergence carries over to the filtered MDP, and (iii) any policy that is optimal in the filtered MDP-when executed through the same filter-achieves the same asymptotic return as the best safe policy in the SC-MDP, yielding a complete separation between safety enforcement and performance optimization. We validate the theory on Safety Gymnasium with representative tasks and constraints, observing zero violations during training and final performance matching or exceeding unfiltered baselines. Together, these results shed light on a long-standing question in safety-filtered learning and provide a simple, principled recipe for safe RL: train and deploy RL policies with the most permissive safety filter that is available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全过滤下可证明最优的强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）的最新进展使其能够应用于日益复杂的任务，但缺乏形式化安全保证仍限制其在安全关键场景中的应用。常见的实用方法是通过安全过滤器增强RL策略，在训练和部署期间覆盖不安全动作以防止故障。然而，安全过滤常被视为以牺牲性能为代价并阻碍学习过程。我们证明这种安全与性能的权衡并非固有特性，并首次证明：使用足够宽松的安全过滤器强制执行安全性不会降低渐近性能。我们通过安全关键马尔可夫决策过程（SC-MDP）形式化RL安全性，该模型要求绝对避免（而非高概率避免）灾难性故障状态。此外，我们定义了关联的过滤MDP，其中所有动作均产生安全效果——这得益于被视为环境组成部分的安全过滤器。我们的主要定理证明：（i）在过滤MDP中学习具有绝对安全性；（ii）标准RL收敛性可迁移至过滤MDP；（iii）任何在过滤MDP中最优的策略（通过相同过滤器执行）均能在SC-MDP中获得与最佳安全策略相同的渐近回报，从而实现安全执行与性能优化的完全分离。我们在Safety Gymnasium平台上通过代表性任务和约束验证理论，训练期间观察到零违规，最终性能匹配或超越未过滤基线。这些结果共同阐明了安全过滤学习中长期存在的问题，并为安全RL提供了简洁而原则性的方案：使用可用的最宽松安全过滤器进行RL策略的训练与部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for formal safety guarantees in reinforcement learning (RL) for safety-critical applications, this paper challenges the perceived trade-off between safety and performance when using safety filters. The method introduces a safety-critical Markov decision process (SC-MDP) to model categorical safety and a filtered MDP where a safety filter is embedded in the environment, proving theoretically that learning in this filtered setting is safe and does not degrade asymptotic optimality. Experimental validation on Safety Gymnasium tasks shows zero safety violations during training and final performance that matches or exceeds unfiltered baselines, demonstrating a principled separation of safety enforcement from performance optimization.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在安全关键应用中缺乏形式化安全保证的问题，旨在挑战使用安全过滤器时普遍认为的安全与性能之间存在权衡的观点。方法上引入了安全关键马尔可夫决策过程（SC-MDP）来建模绝对安全性，并构建了一个将安全过滤器嵌入环境的过滤MDP，从理论上证明了在此过滤设置下学习是安全的且不会降低渐近最优性。在Safety Gymnasium任务上的实验验证显示训练过程中零安全违规，最终性能达到或超过无过滤基线，为安全强化学习提供了一种将安全执行与性能优化分离的原则性方案。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuning GPT-5 for GPU Kernel Generation</div>
<div class="meta-line">Authors: Ali Tehrani, Yahya Emara, Essam Wissam, Wojciech Paluch, Waleed Atallah, Łukasz Dudziak, Mohamed S. Abdelfattah</div>
<div class="meta-line">First: 2026-02-11T16:22:54+00:00 · Latest: 2026-02-11T16:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora&#x27;s environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>微调GPT-5用于GPU内核生成</div>
<div class="mono" style="margin-top:8px">开发高效的GPU内核对扩展现代AI系统至关重要，但由于复杂的硬件架构和需要专业优化知识，这仍是一项复杂任务。尽管大语言模型在通用顺序代码生成方面展现出强大能力，但在GPU代码生成中面临重大挑战：高质量标注训练数据稀缺、生成合成方案时的编译器偏差，以及跨硬件代际的泛化能力有限。这使得监督微调难以成为改进当前大语言模型的可扩展方法。相比之下，强化学习提供了一种数据高效且自适应的替代方案，但需要相关工具支持、精心选择训练问题及稳健的评估环境。我们展示了Makora为前沿模型强化学习微调构建的环境与工具，并报告了微调GPT-5生成Triton代码的结果。在单次尝试设置中，相较于基线GPT-5，我们的微调模型将内核正确率从43.7%提升至77.0%（+33.3个百分点），在超越TorchInductor的问题比例上从14.8%增至21.8%（+7个百分点），同时在KernelBench基准上超越了先前最优模型。当集成至完整编码智能体时，该模型能解决扩展版KernelBench测试集中高达97.4%的问题，在72.9%的问题上优于PyTorch TorchInductor编译器，几何平均加速比达2.12倍。我们的研究表明，通过强化学习进行针对性后训练能够释放大语言模型在高度专业化技术领域的潜力——这些领域传统监督学习受限于数据可用性，从而为AI辅助加速器编程开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complexity of developing efficient GPU kernels for AI systems and the limitations of supervised fine-tuning due to scarce high-quality data and poor generalization, this work employs reinforcement learning (RL) as a data-efficient alternative. The method involves fine-tuning GPT-5 using the Makora environment and tools specifically designed for RL, targeting Triton code generation. Experimental results show that the fine-tuned model significantly improves single-attempt kernel correctness from 43.7% to 77.0% and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8%, while achieving state-of-the-art performance on KernelBench; when deployed as a full coding agent, it solves up to 97.4% of problems in an expanded suite, outperforming TorchInductor on 72.9% with a 2.12x geometric mean speedup.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决为AI系统开发高效GPU内核的复杂性，以及由于高质量标注数据稀缺和泛化能力不足导致监督微调受限的问题，采用强化学习作为数据高效的替代方法。该方法利用专为强化学习设计的Makora环境和工具对GPT-5进行微调，以生成Triton代码。实验结果表明，微调后的模型在单次尝试中将内核正确率从43.7%提升至77.0%，并将优于TorchInductor的问题比例从14.8%提高到21.8%，同时在KernelBench上达到最先进性能；作为完整编码代理部署时，它能在扩展测试集中解决高达97.4%的问题，在72.9%的问题上超越TorchInductor，几何平均加速比为2.12倍。</div>
</details>
</div>
<div class="card">
<div class="title">Finding Kissing Numbers with Game-theoretic Reinforcement Learning</div>
<div class="meta-line">Authors: Chengdong Ma, Théo Tao Zhaowei, Pengyu Li, Minghao Liu, Haojun Chen, Zihao Mao, Yuan Cheng, Yuan Qi, Yaodong Yang</div>
<div class="meta-line">First: 2025-11-17T14:02:00+00:00 · Latest: 2026-02-11T16:16:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13391v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.13391v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem is the local analogue of Hilbert&#x27;s 18th problem, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry, dimensional structure variability, and combinatorial explosion beyond Go limit the scalability and generality of existing methods. Here we model the problem as a two-player matrix completion game and train the reinforcement learning system, PackingStar, to play the games. The matrix entries represent pairwise cosines of sphere center vectors. One player fills entries while another corrects suboptimal ones to improve exploration quality, cooperatively maximizing the matrix size, corresponding to the kissing number. These matrices are decomposed into representative substructures, providing diverse bases and structural constraints that steer subsequent games and make extremely large spaces tractable. PackingStar surpasses records from dimensions 25 to 31 and sets new lower bounds for generalized kissing numbers under various angular constraints. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in other dimensions. Notably, some configurations challenge long-held antipodal paradigms, revealing algebraic correspondences with finite simple groups as well as geometric relationships across dimensions. Inspired by these patterns, humans devised further improved constructions. These results demonstrate AI&#x27;s power to explore high-dimensional spaces beyond human intuition via extreme-scale reinforcement learning and open new pathways for the Kissing Number Problem and broader geometry research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于博弈论强化学习的接吻数求解</div>
<div class="mono" style="margin-top:8px">自艾萨克·牛顿于1694年首次研究接吻数问题以来，确定中心球周围非重叠球体的最大数量始终是基础性难题。该问题是希尔伯特第18问题的局部类比，连接几何学、数论与信息论。尽管通过格与编码已取得重要进展，但高维几何的不规则性、维度结构可变性以及超越围棋的组合爆炸限制了现有方法的可扩展性与普适性。本研究将该问题建模为双人矩阵补全博弈，训练强化学习系统PackingStar进行博弈。矩阵元素表示球心向量间的成对余弦值。一方填充元素，另一方修正次优元素以提升探索质量，协同最大化矩阵规模（对应接吻数）。这些矩阵被分解为代表性子结构，提供多样化基与结构约束以引导后续博弈，使极大空间可处理。PackingStar在25至31维度打破记录，并为多种角度约束下的广义接吻数设定新下界。它在13维度实现1971年以来首次超越有理结构的突破，在其他维度发现超6000种新结构。值得注意的是，部分构型挑战了长期存在的对径范式，揭示了与有限单群的代数对应关系及跨维度几何关联。受这些模式启发，人类设计出进一步优化的构造。这些成果证明人工智能能通过极大规模强化学习探索超越人类直觉的高维空间，为接吻数问题及更广泛的几何研究开辟新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the long-standing Kissing Number Problem in high-dimensional geometry, which seeks the maximum number of non-overlapping spheres that can touch a central sphere, this paper introduces a game-theoretic reinforcement learning method called PackingStar to overcome the combinatorial explosion and structural variability that limit traditional approaches. The method frames the problem as a two-player matrix completion game, where one player fills matrix entries representing pairwise cosines of sphere center vectors and another corrects suboptimal entries, cooperatively maximizing matrix size to find dense packings; these matrices are decomposed into substructures to guide exploration. Experimentally, PackingStar surpasses existing records from dimensions 25 to 31, sets new lower bounds for generalized kissing numbers, achieves the first breakthrough beyond 1971 results in 13 dimensions, and discovers over 6000 new structures, some challenging antipodal paradigms and revealing algebraic-geometric correspondences, ultimately enabling human-inspired improvements.</div>
<div class="mono" style="margin-top:8px">本文针对高维几何中经典的接吻数问题——即寻找一个中心球周围可容纳的非重叠球的最大数量，受限于传统方法在组合爆炸和结构可变性上的不足，提出了一种名为PackingStar的博弈论强化学习方法。该方法将问题建模为双玩家矩阵补全游戏：一名玩家填充表示球心向量间余弦的矩阵条目，另一名玩家修正次优条目，通过合作最大化矩阵规模以寻找密集堆积；这些矩阵被分解为代表性子结构以指导探索。实验结果表明，PackingStar在25至31维上超越了现有记录，为广义接吻数设定了新的下界，在13维上实现了自1971年以来的首次突破，并发现了6000多个新结构，其中一些挑战了长期存在的对径范式，揭示了代数与几何的对应关系，进而启发了人工构造的进一步改进。</div>
</details>
</div>
<div class="card">
<div class="title">HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba</div>
<div class="meta-line">Authors: Yinuo Wang, Yuanyang Qi, Jinzhao Zhou, Pengxiang Meng, Xiaowen Tao</div>
<div class="meta-line">First: 2025-09-22T17:19:55+00:00 · Latest: 2026-02-11T16:00:35+00:00</div>
<div class="meta-line">Comments: 12 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18046v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.18046v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">End-to-end reinforcement learning (RL) for humanoid locomotion is appealing for its compact perception-action mapping, yet practical policies often suffer from training instability, inefficient feature fusion, and high actuation cost. We present HuMam, a state-centric end-to-end RL framework that employs a single-layer Mamba encoder to fuse robot-centric states with oriented footstep targets and a continuous phase clock. The policy outputs joint position targets tracked by a low-level PD loop and is optimized with PPO. A concise six-term reward balances contact quality, swing smoothness, foot placement, posture, and body stability while implicitly promoting energy saving. On the JVRC-1 humanoid in mc-mujoco, HuMam consistently improves learning efficiency, training stability, and overall task performance over a strong feedforward baseline, while reducing power consumption and torque peaks. To our knowledge, this is the first end-to-end humanoid RL controller that adopts Mamba as the fusion backbone, demonstrating tangible gains in efficiency, stability, and control economy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HuMam：基于Mamba端到端深度强化学习的人形机器人运动控制</div>
<div class="mono" style="margin-top:8px">人形机器人运动的端到端强化学习因其紧凑的感知-动作映射而备受关注，但实际策略常面临训练不稳定、特征融合效率低和高驱动成本等问题。本文提出HuMam——一种以状态为中心的端到端强化学习框架，采用单层Mamba编码器融合机器人中心状态、定向步态目标与连续相位时钟。策略输出由底层PD回路跟踪的关节位置目标，并通过PPO算法优化。简洁的六项奖励函数平衡接触质量、摆动平滑度、足部放置、姿态和身体稳定性，同时隐式促进节能。在mc-mujoco的JVRC-1人形机器人上，HuMam相较于强前馈基线持续提升学习效率、训练稳定性和整体任务性能，同时降低功耗和扭矩峰值。据我们所知，这是首个采用Mamba作为融合骨干的端到端人形机器人强化学习控制器，在效率、稳定性和控制经济性方面均取得实质性提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for HuMam is to address the training instability, inefficient feature fusion, and high actuation costs common in end-to-end reinforcement learning for humanoid locomotion. The method employs a state-centric framework with a single-layer Mamba encoder to fuse robot-centric states, oriented footstep targets, and a continuous phase clock, using PPO for optimization and a PD loop to track joint position targets, supported by a concise six-term reward function. Main experimental results on the JVRC-1 humanoid in mc-mujoco show that HuMam consistently improves learning efficiency, training stability, and overall task performance over a strong baseline while reducing power consumption and torque peaks, marking the first use of Mamba as a fusion backbone in this context.</div>
<div class="mono" style="margin-top:8px">HuMam的动机是解决人形机器人端到端强化学习中常见的训练不稳定、特征融合效率低和驱动成本高的问题。该方法采用以状态为中心的框架，通过单层Mamba编码器融合机器人中心状态、定向步态目标和连续相位时钟，利用PPO进行优化，并通过PD回路跟踪关节位置目标，辅以简洁的六项奖励函数。在mc-mujoco的JVRC-1人形机器人上的主要实验结果表明，HuMam相比强基线持续提升了学习效率、训练稳定性和整体任务性能，同时降低了功耗和扭矩峰值，这是首次在该领域将Mamba用作融合骨干。</div>
</details>
</div>
<div class="card">
<div class="title">Uni-DPO: A Unified Paradigm for Dynamic Preference Optimization of LLMs</div>
<div class="meta-line">Authors: Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, Min Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-11T17:58:05+00:00 · Latest: 2026-02-11T15:06:07+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026. Code &amp; models: https://github.com/pspdada/Uni-DPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10054v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.10054v3">PDF</a> · <a href="https://github.com/pspdada/Uni-DPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct Preference Optimization (DPO) has emerged as a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based methods typically treat all preference pairs equally, overlooking substantial variations in data quality and learning difficulty, which leads to inefficient data utilization and suboptimal performance. To address this limitation, we propose Uni-DPO, a unified dynamic preference optimization framework that jointly considers (a) the inherent quality of preference pairs and (b) the model&#x27;s evolving performance during training. By adaptively reweighting samples based on both factors, Uni-DPO enables more effective use of preference data and achieves superior performance. Extensive experiments across models and benchmarks demonstrate the effectiveness and generalization of Uni-DPO. On textual tasks, Gemma-2-9B-IT fine-tuned with Uni-DPO surpasses the leading LLM, Claude 3 Opus, by 6.7 points on Arena-Hard. On mathematical and multimodal tasks, Uni-DPO consistently outperforms baseline methods across all benchmarks, providing strong empirical evidence of its effectiveness and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Uni-DPO：大语言模型动态偏好优化的统一范式</div>
<div class="mono" style="margin-top:8px">直接偏好优化（DPO）因其简洁高效，已成为基于人类反馈的强化学习（RLHF）的核心技术。然而，现有基于DPO的方法通常平等对待所有偏好对，忽视了数据质量和学习难度的显著差异，导致数据利用效率低下和性能欠佳。为解决这一局限，我们提出Uni-DPO——一个统一的动态偏好优化框架，该框架同时考虑（a）偏好对的固有质量与（b）模型在训练过程中的动态表现。通过基于这两个因素自适应调整样本权重，Uni-DPO能更有效地利用偏好数据并取得更优性能。跨模型与基准测试的大量实验验证了Uni-DPO的有效性和泛化能力：在文本任务中，经Uni-DPO微调的Gemma-2-9B-IT模型在Arena-Hard基准上超越领先大语言模型Claude 3 Opus达6.7分；在数学与多模态任务中，Uni-DPO在所有基准测试中均持续优于基线方法，为其有效性与鲁棒性提供了坚实实证依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for Uni-DPO stems from the limitation in standard Direct Preference Optimization (DPO) methods, which treat all preference pairs equally and thus inefficiently use data with varying quality and learning difficulty. The proposed method introduces a unified dynamic framework that adaptively reweights training samples by jointly considering the inherent quality of preference pairs and the model&#x27;s evolving performance during training. Main experimental results show that Uni-DPO fine-tuned models achieve superior performance; for instance, on textual tasks, Gemma-2-9B-IT surpasses Claude 3 Opus by 6.7 points on Arena-Hard, and it consistently outperforms baselines on mathematical and multimodal benchmarks, demonstrating effectiveness and robustness.</div>
<div class="mono" style="margin-top:8px">Uni-DPO的提出动机源于现有直接偏好优化方法通常平等对待所有偏好对，忽略了数据质量和学习难度的显著差异，导致数据利用效率低下和性能欠佳。该方法引入了一个统一的动态偏好优化框架，通过联合考虑偏好对的固有质量和模型在训练过程中的动态表现，自适应地重新加权样本。主要实验结果表明，Uni-DPO微调的模型性能优异；例如在文本任务中，Gemma-2-9B-IT在Arena-Hard上比领先的Claude 3 Opus高出6.7分，在数学和多模态任务的所有基准测试中也一致优于基线方法，证明了其有效性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs</div>
<div class="meta-line">Authors: Yisen Gao, Jiaxin Bai, Tianshi Zheng, Qingyun Sun, Ziwei Zhang, Xingcheng Fu, Jianxin Li, Yangqiu Song</div>
<div class="meta-line">First: 2025-05-27T09:36:47+00:00 · Latest: 2026-02-11T14:57:15+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20948v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20948v2">PDF</a> · <a href="https://github.com/HKUST-KnowComp/CtrlHGen">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Abductive reasoning in knowledge graphs aims to generate plausible logical hypotheses from observed entities, with broad applications in areas such as clinical diagnosis and scientific discovery. However, due to a lack of controllability, a single observation may yield numerous plausible but redundant or irrelevant hypotheses on large-scale knowledge graphs. To address this limitation, we introduce the task of controllable hypothesis generation to improve the practical utility of abductive reasoning. This task faces two key challenges when controlling for generating long and complex logical hypotheses: hypothesis space collapse and hypothesis oversensitivity. To address these challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation framework for abductive reasoning over knowledge graphs, trained in a two-stage paradigm including supervised learning and subsequent reinforcement learning. To mitigate hypothesis space collapse, we design a dataset augmentation strategy based on sub-logical decomposition, enabling the model to learn complex logical structures by leveraging semantic patterns in simpler components. To address hypothesis oversensitivity, we incorporate smoothed semantic rewards including Dice and Overlap scores, and introduce a condition-adherence reward to guide the generation toward user-specified control constraints. Extensive experiments on three benchmark datasets demonstrate that our model not only better adheres to control conditions but also achieves superior semantic similarity performance compared to baselines. Our code is available at https://github.com/HKUST-KnowComp/CtrlHGen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识图谱溯因推理的可控逻辑假设生成</div>
<div class="mono" style="margin-top:8px">知识图谱中的溯因推理旨在从观测实体生成合理的逻辑假设，在临床诊断与科学发现等领域具有广泛应用。然而，由于缺乏可控性，单一观测在大规模知识图谱上可能产生大量合理但冗余或无关的假设。为突破此局限，我们引入可控假设生成任务以提升溯因推理的实用价值。该任务在控制生成长复杂逻辑假设时面临两大挑战：假设空间坍缩与假设过度敏感。为此，我们提出CtrlHGen——一种面向知识图谱溯因推理的可控逻辑假设生成框架，采用包含监督学习与后续强化学习的双阶段训练范式。为缓解假设空间坍缩，我们设计了基于子逻辑分解的数据集增强策略，使模型能通过简单组件的语义模式学习复杂逻辑结构。针对假设过度敏感问题，我们引入包含Dice与Overlap分数的平滑语义奖励，并新增条件遵循奖励以引导生成过程贴合用户指定的控制约束。在三个基准数据集上的大量实验表明，相较于基线模型，我们的方法不仅能更好地遵循控制条件，同时实现了更优的语义相似性性能。代码已开源：https://github.com/HKUST-KnowComp/CtrlHGen。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of controllability in abductive reasoning over knowledge graphs, where single observations often produce many redundant or irrelevant logical hypotheses, limiting practical utility. The authors propose CtrlHGen, a framework for controllable hypothesis generation trained via a two-stage paradigm of supervised and reinforcement learning. To tackle challenges of hypothesis space collapse and oversensitivity, they employ a dataset augmentation strategy based on sub-logical decomposition and incorporate smoothed semantic rewards alongside a condition-adherence reward. Experimental results on three benchmark datasets show that CtrlHGen better adheres to control conditions and achieves superior semantic similarity compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">本文针对知识图谱溯因推理中缺乏可控性的问题，即单一观察常产生大量冗余或不相关的逻辑假设，限制了实际应用。作者提出了CtrlHGen框架，通过监督学习和强化学习两阶段训练实现可控假设生成。为应对假设空间塌缩和过度敏感挑战，该方法采用基于子逻辑分解的数据集增强策略，并引入平滑语义奖励及条件遵循奖励。在三个基准数据集上的实验表明，该模型能更好地遵循控制条件，并在语义相似性上优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins</div>
<div class="meta-line">Authors: Qian Zuo, Zhiyong Wang, Fengxiang He</div>
<div class="meta-line">First: 2026-02-11T14:54:26+00:00 · Latest: 2026-02-11T14:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10917v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study safe online reinforcement learning in Constrained Markov Decision Processes (CMDPs) under strong regret and violation metrics, which forbid error cancellation over time. Existing primal-dual methods that achieve sublinear strong reward regret inevitably incur growing strong constraint violation or are restricted to average-iterate convergence due to inherent oscillations. To address these limitations, we propose the Flexible safety Domain Optimization via Margin-regularized Exploration (FlexDOME) algorithm, the first to provably achieve near-constant $\tilde{O}(1)$ strong constraint violation alongside sublinear strong regret and non-asymptotic last-iterate convergence. FlexDOME incorporates time-varying safety margins and regularization terms into the primal-dual framework. Our theoretical analysis relies on a novel term-wise asymptotic dominance strategy, where the safety margin is rigorously scheduled to asymptotically majorize the functional decay rates of the optimization and statistical errors, thereby clamping cumulative violations to a near-constant level. Furthermore, we establish non-asymptotic last-iterate convergence guarantees via a policy-dual Lyapunov argument. Experiments corroborate our theoretical findings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于衰减安全裕度的在线约束马尔可夫决策过程：近恒定强违反与末次迭代收敛</div>
<div class="mono" style="margin-top:8px">本研究在强遗憾和强违反度量下探索约束马尔可夫决策过程中的安全在线强化学习，该度量禁止误差随时间抵消。现有实现次线性强奖励遗憾的原对偶方法，由于固有振荡性，必然导致增长的强约束违反或仅限于平均迭代收敛。为突破这些局限，我们提出基于裕度正则化探索的柔性安全域优化算法，该算法首次可证明地实现近恒定$\tilde{O}(1)$强约束违反，同时保持次线性强遗憾与非渐近末次迭代收敛。该算法将时变安全裕度与正则化项融入原对偶框架。理论分析依托新颖的逐项渐近主导策略：通过严格调度安全裕度使其渐近主导优化误差与统计误差的函数衰减率，从而将累积违反控制在近恒定水平。此外，我们通过策略-对偶李雅普诺夫论证建立了非渐近末次迭代收敛保证。实验数据验证了理论结论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of achieving both sublinear strong regret and near-constant strong constraint violation in safe online reinforcement learning for Constrained Markov Decision Processes (CMDPs), where existing primal-dual methods often suffer from growing violation or only average-iterate convergence. The proposed FlexDOME algorithm introduces time-varying safety margins and regularization into the primal-dual framework, employing a novel term-wise asymptotic dominance strategy to schedule the safety margin so it asymptotically majorizes optimization and statistical error decay rates, thereby clamping cumulative violations. Theoretical analysis proves that FlexDOME achieves near-constant strong violation, sublinear strong regret, and non-asymptotic last-iterate convergence, with experimental results supporting these findings.</div>
<div class="mono" style="margin-top:8px">本文针对约束马尔可夫决策过程（CMDPs）中的安全在线强化学习问题，旨在同时实现次线性强遗憾和接近恒定的强约束违反，而现有对偶原始方法常导致违反增长或仅平均迭代收敛。提出的FlexDOME算法将对偶原始框架与时变安全边界及正则化项结合，采用一种新颖的逐项渐近主导策略来调度安全边界，使其渐近主导优化和统计误差的衰减率，从而将累积违反控制在接近恒定水平。理论分析证明FlexDOME可实现接近恒定的强违反、次线性强遗憾和非渐近的最后迭代收敛，实验结杲验证了理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">PreferThinker: Reasoning-based Personalized Image Preference Assessment</div>
<div class="meta-line">Authors: Shengqi Xu, Xinpeng Zhou, Yabo Zhang, Ming Liu, Tao Liang, Tianyu Zhang, Yalong Bai, Zuxuan Wu, Wangmeng Zuo</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-11-01T16:19:51+00:00 · Latest: 2026-02-11T14:53:46+00:00</div>
<div class="meta-line">Comments: This paper is accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00609v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00609v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Personalized image preference assessment aims to evaluate an individual user&#x27;s image preferences by relying only on a small set of reference images as prior information. Existing methods mainly focus on general preference assessment, training models with large-scale data to tackle well-defined tasks such as text-image alignment. However, these approaches struggle to handle personalized preference because user-specific data are scarce and not easily scalable, and individual tastes are often diverse and complex. To overcome these challenges, we introduce a common preference profile that serves as a bridge across users, allowing large-scale user data to be leveraged for training profile prediction and capturing complex personalized preferences. Building on this idea, we propose a reasoning-based personalized image preference assessment framework that follows a \textit{predict-then-assess} paradigm: it first predicts a user&#x27;s preference profile from reference images, and then provides interpretable, multi-dimensional scores and assessments of candidate images based on the predicted profile. To support this, we first construct a large-scale Chain-of-Thought (CoT)-style personalized assessment dataset annotated with diverse user preference profiles and high-quality CoT-style reasoning, enabling explicit supervision of structured reasoning. Next, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase to empower the model with structured reasoning capabilities, followed by reinforcement learning to incentivize the model to explore more reasonable assessment paths and enhance generalization. Furthermore, we propose a similarity-aware prediction reward to encourage better prediction of the user&#x27;s preference profile, which facilitates more reasonable assessments exploration. Extensive experiments demonstrate the superiority of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PreferThinker：基于推理的个性化图像偏好评估</div>
<div class="mono" style="margin-top:8px">个性化图像偏好评估旨在仅依赖少量参考图像作为先验信息，评估个体用户的图像偏好。现有方法主要关注通用偏好评估，通过大规模数据训练模型以处理文本-图像对齐等明确任务，但难以应对个性化偏好，因为用户特定数据稀缺且不易扩展，且个体偏好通常多样复杂。为克服这些挑战，本文引入一种跨用户的通用偏好档案作为桥梁，利用大规模用户数据训练档案预测并捕捉复杂的个性化偏好。基于此，提出一种基于推理的个性化图像偏好评估框架，遵循“预测后评估”范式：首先从参考图像预测用户偏好档案，然后基于预测档案为候选图像提供可解释的多维评分与评估。为此，首先构建了一个大规模思维链风格的个性化评估数据集，标注了多样用户偏好档案与高质量思维链推理，以支持结构化推理的显式监督。其次，采用两阶段训练策略：冷启动监督微调阶段赋予模型结构化推理能力，随后通过强化学习激励模型探索更合理的评估路径并增强泛化性。此外，提出相似性感知预测奖励机制，以促进更准确的用户偏好档案预测，从而推动更合理的评估探索。大量实验证明了所提方法的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of personalized image preference assessment, where existing methods trained on large-scale general data struggle with scarce and diverse user-specific tastes. To overcome this, the authors propose a reasoning-based framework that first predicts a user&#x27;s preference profile from a few reference images and then uses this profile to provide interpretable, multi-dimensional assessments of candidate images. This is supported by a large-scale Chain-of-Thought-style dataset and a two-stage training strategy combining supervised fine-tuning and reinforcement learning with a similarity-aware reward. Experimental results demonstrate the method&#x27;s superiority in capturing complex personalized preferences.</div>
<div class="mono" style="margin-top:8px">本文针对个性化图像偏好评估的挑战，现有基于大规模通用数据训练的方法难以处理稀缺且多样的用户特定偏好。为此，作者提出一个基于推理的框架，首先从少量参考图像预测用户偏好画像，然后利用该画像对候选图像进行可解释的多维度评估。该方法通过构建大规模思维链风格数据集，并采用结合监督微调和强化学习的两阶段训练策略，引入相似性感知奖励以优化预测。实验结果表明，该方法在捕捉复杂个性化偏好方面具有优越性。</div>
</details>
</div>
<div class="card">
<div class="title">evortran: a modern Fortran package for genetic algorithms with applications from LHC data fitting to LISA signal reconstruction</div>
<div class="meta-line">Authors: Thomas Biekötter</div>
<div class="meta-line">Venue: SciPost Phys. Codebases 64 (2026)</div>
<div class="meta-line">First: 2025-07-08T15:22:22+00:00 · Latest: 2026-02-11T14:35:31+00:00</div>
<div class="meta-line">Comments: v1: 68 pages, 13 figures. evortran repository: https://gitlab.com/thomas.biekoetter/evortran v2: Added Python interface. pyevortran repository: https://gitlab.com/thomas.biekoetter/pyevortran, v3: matches published version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.06082v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.06082v3">PDF</a> · <a href="https://gitlab.com/thomas.biekoetter/evortran">Code1</a> · <a href="https://gitlab.com/thomas.biekoetter/pyevortran">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">evortran is a modern Fortran library designed for high-performance genetic algorithms and evolutionary optimization. evortran can be used to tackle a wide range of problems in high-energy physics and beyond, such as derivative-free parameter optimization, complex search taks, parameter scans and fitting experimental data under the presence of instrumental noise. The library is built as an fpm package with flexibility and efficiency in mind, while also offering a simple installation process, user interface and integration into existing Fortran (or Python) programs. evortran offers a variety of selection, crossover, mutation and elitism strategies, with which users can tailor an evolutionary algorithm to their specific needs. evortran supports different abstraction levels: from operating directly on individuals and populations, to running full evolutionary cycles, and even enabling migration between independently evolving populations to enhance convergence and maintain diversity. In this paper, we present the functionality of the evortran library, demonstrate its capabilities with example benchmark applications, and compare its performance with existing genetic algorithm frameworks. As physics-motivated applications, we use evortran to confront extended Higgs sectors with LHC data and to reconstruct gravitational wave spectra and the underlying physical parameters from LISA mock data, demonstrating its effectiveness in realistic, data-driven scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>evortran：一个现代Fortran遗传算法包，应用范围从LHC数据拟合到LISA信号重建</div>
<div class="mono" style="margin-top:8px">evortran是一个专为高性能遗传算法和进化优化设计的现代Fortran库。该库可用于解决高能物理及其他领域的广泛问题，例如无导数参数优化、复杂搜索任务、参数扫描以及在仪器噪声存在下的实验数据拟合。该库构建为fpm包，兼顾灵活性与效率，同时提供简易的安装流程、用户界面及与现有Fortran（或Python）程序的集成能力。evortran提供多种选择、交叉、突变和精英保留策略，用户可根据具体需求定制进化算法。该库支持不同抽象层级：从直接操作个体和种群，到运行完整的进化周期，甚至支持独立进化种群间的迁移以增强收敛性并保持多样性。本文介绍了evortran库的功能，通过基准应用示例展示其性能，并与现有遗传算法框架进行对比。在物理应用方面，我们使用evortran将扩展希格斯扇区与LHC数据进行比较，并从LISA模拟数据中重建引力波谱及基础物理参数，证明了其在现实数据驱动场景中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for developing evortran is to provide a high-performance, flexible, and easy-to-use modern Fortran library for genetic algorithms and evolutionary optimization, particularly targeting complex problems in high-energy physics and other domains. The method involves building the library as an fpm package with a modular design that offers various selection, crossover, mutation, and elitism strategies, supporting multiple abstraction levels from direct population manipulation to full evolutionary cycles with migration between populations. Main experimental results demonstrate its effectiveness through benchmark comparisons with existing frameworks and physics-motivated applications, including fitting extended Higgs sectors to LHC data and reconstructing gravitational wave spectra and parameters from LISA mock data, showcasing robust performance in realistic, data-driven scenarios.</div>
<div class="mono" style="margin-top:8px">开发evortran的动机是提供一个高性能、灵活且易于使用的现代Fortran库，用于遗传算法和进化优化，特别针对高能物理等领域的复杂问题。该方法将库构建为fpm包，采用模块化设计，提供多种选择、交叉、变异和精英保留策略，支持从直接种群操作到完整进化周期及种群间迁移的多层次抽象。主要实验结果通过基准测试与现有框架的比较以及物理应用案例证明了其有效性，包括将扩展希格斯扇区拟合到LHC数据、从LISA模拟数据重建引力波谱和参数，展示了在现实数据驱动场景中的强大性能。</div>
</details>
</div>
<div class="card">
<div class="title">Resource-Efficient Model-Free Reinforcement Learning for Board Games</div>
<div class="meta-line">Authors: Kazuki Ota, Takayuki Osa, Motoki Omura, Tatsuya Harada</div>
<div class="meta-line">First: 2026-02-11T14:25:38+00:00 · Latest: 2026-02-11T14:25:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10894v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10894v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their reproducibility. In this study, we propose a model-free reinforcement learning algorithm designed for board games to achieve more efficient learning. To validate the efficiency of the proposed method, we conducted comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. The results demonstrate that the proposed method achieves more efficient learning than existing methods across these environments. In addition, our extensive ablation study shows the importance of core techniques used in the proposed method. We believe that our efficient algorithm shows the potential of model-free reinforcement learning in domains traditionally dominated by search-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向棋盘游戏的资源高效无模型强化学习</div>
<div class="mono" style="margin-top:8px">棋盘游戏长期以来一直是人工智能领域复杂的决策基准。在该领域中，以AlphaZero为代表的基于搜索的强化学习方法取得了显著成功，但其巨大的计算需求被指出是阻碍方法可复现性的关键瓶颈。本研究提出一种专为棋盘游戏设计的无模型强化学习算法，以实现更高效的学习。为验证所提方法的效率，我们在五种棋盘游戏（动物将棋、加德纳象棋、围棋、六边形棋、黑白棋）上进行了全面实验。结果表明，所提方法在这些环境中均实现了比现有方法更高效的学习。此外，通过系统的消融实验，我们验证了所提方法中核心技术的必要性。我们相信，这一高效算法展现了无模型强化学习在传统由搜索方法主导的领域中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high computational costs of search-based reinforcement learning methods like AlphaZero, which hinder reproducibility in board game AI, this paper proposes a model-free reinforcement learning algorithm designed for more efficient learning. The method is validated through comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. Results show that the proposed approach achieves more efficient learning than existing methods across these environments, with ablation studies highlighting the importance of its core techniques, demonstrating the potential of model-free reinforcement learning in domains traditionally dominated by search-based approaches.</div>
<div class="mono" style="margin-top:8px">本研究针对AlphaZero等搜索型强化学习方法在棋盘游戏AI中计算成本高、可复现性差的问题，提出了一种旨在实现更高效学习的无模型强化学习算法。该方法通过在动物将棋、加德纳国际象棋、围棋、六边形棋和黑白棋五种棋盘游戏上进行全面实验验证。结果表明，所提方法在这些环境中均比现有方法实现了更高效的学习，消融实验进一步揭示了其核心技术的重要性，展现了无模型强化学习在传统上由搜索方法主导的领域中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents</div>
<div class="meta-line">Authors: Cong Pang, Xuyu Feng, Yujie Yi, Zixuan Chen, Jiawei Hong, Tiankuo Yao, Nang Yuan, Jiapeng Luo, Lewei Lu, Xin Lou</div>
<div class="meta-line">First: 2026-02-11T13:50:19+00:00 · Latest: 2026-02-11T13:50:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10863v1">PDF</a> · <a href="https://github.com/pc-inno/ICA_MM_deepsearch.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the strong performance achieved by reinforcement learning-trained information-seeking agents, learning in open-ended web environments remains severely constrained by low signal-to-noise feedback. Text-based parsers often discard layout semantics and introduce unstructured noise, while long-horizon training typically relies on sparse outcome rewards that obscure which retrieval actions actually matter. We propose a visual-native search framework that represents webpages as visual snapshots, allowing agents to leverage layout cues to quickly localize salient evidence and suppress distractors. To learn effectively from these high-dimensional observations, we introduce Information-Aware Credit Assignment (ICA), a post-hoc method that estimates each retrieved snapshot&#x27;s contribution to the final outcome via posterior analysis and propagates dense learning signals back to key search turns. Integrated with a GRPO-based training pipeline, our approach consistently outperforms text-based baselines on diverse information-seeking benchmarks, providing evidence that visual snapshot grounding with information-level credit assignment alleviates the credit-assignment bottleneck in open-ended web environments. The code and datasets will be released in https://github.com/pc-inno/ICA_MM_deepsearch.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ICA：面向视觉基础长程信息搜索智能体的信息感知信用分配方法</div>
<div class="mono" style="margin-top:8px">尽管强化学习训练的信息搜索智能体已取得优异性能，但在开放网络环境中的学习仍受限于低信噪比反馈。基于文本的解析器常忽略布局语义并引入非结构化噪声，而长程训练通常依赖稀疏结果奖励，难以识别关键检索动作。我们提出一种视觉原生搜索框架，将网页表示为视觉快照，使智能体能利用布局线索快速定位关键证据并抑制干扰信息。为从这些高维观测中有效学习，我们提出信息感知信用分配（ICA）——一种后验分析方法，通过后验分析估算每个检索快照对最终结果的贡献度，并将密集学习信号回溯传播至关键搜索节点。结合基于GRPO的训练流程，本方法在多种信息搜索基准测试中持续优于基于文本的基线，证明视觉快照基础与信息级信用分配能缓解开放网络环境中的信用分配瓶颈。代码与数据集发布于https://github.com/pc-inno/ICA_MM_deepsearch.git。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of low signal-to-noise feedback in reinforcement learning for information-seeking agents operating in open-ended web environments, where text-based parsers often lose layout semantics and sparse rewards obscure the importance of individual retrieval actions. The proposed method introduces a visual-native search framework that represents webpages as visual snapshots to leverage layout cues for localizing evidence and suppressing distractors, coupled with Information-Aware Credit Assignment (ICA), a post-hoc technique that estimates each snapshot&#x27;s contribution to the final outcome via posterior analysis to propagate dense learning signals to key search steps. Experimental results show that this approach, integrated with a GRPO-based training pipeline, consistently outperforms text-based baselines across diverse information-seeking benchmarks, demonstrating that visual grounding with information-level credit assignment alleviates the credit-assignment bottleneck in such environments.</div>
<div class="mono" style="margin-top:8px">本文针对开放式网络环境中信息寻求智能体在强化学习训练中面临的低信噪比反馈问题，其中基于文本的解析器常丢失布局语义，而稀疏奖励则模糊了各检索动作的重要性。所提出的方法引入了一种视觉原生搜索框架，将网页表示为视觉快照以利用布局线索定位关键证据并抑制干扰，并结合信息感知信用分配（ICA）这一事后技术，通过后验分析估计每个快照对最终结果的贡献，从而将密集学习信号传播至关键搜索步骤。实验结果表明，该方法与基于GRPO的训练流程结合后，在多种信息寻求基准测试中持续优于基于文本的基线，证明了视觉接地与信息级信用分配能缓解此类环境中的信用分配瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Model Design using Gated Neuron Selection in Telecom</div>
<div class="meta-line">Authors: Adam Orucu, Marcus Medhage, Farnaz Moradi, Andreas Johnsson, Sarunas Girdzijauskas</div>
<div class="meta-line">First: 2026-02-11T13:40:48+00:00 · Latest: 2026-02-11T13:40:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10854v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10854v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The telecommunications industry is experiencing rapid growth in adopting deep learning for critical tasks such as traffic prediction, signal strength prediction, and quality of service optimisation. However, designing neural network architectures for these applications remains challenging and time-consuming, particularly when targeting compact models suitable for resource-constrained network environments. Therefore, there is a need for automating the model design process to create high-performing models efficiently. This paper introduces TabGNS (Tabular Gated Neuron Selection), a novel gradient-based Neural Architecture Search (NAS) method specifically tailored for tabular data in telecommunications networks. We evaluate TabGNS across multiple telecommunications and generic tabular datasets, demonstrating improvements in prediction performance while reducing the architecture size by 51-82% and reducing the search time by up to 36x compared to state-of-the-art tabular NAS methods. Integrating TabGNS into the model lifecycle management enables automated design of neural networks throughout the lifecycle, accelerating deployment of ML solutions in telecommunications networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>电信领域基于门控神经元选择的自动化模型设计</div>
<div class="mono" style="margin-top:8px">电信行业在流量预测、信号强度预测和服务质量优化等关键任务中正快速采用深度学习技术。然而，为这些应用设计神经网络架构仍具挑战且耗时，尤其是在针对资源受限的网络环境设计紧凑模型时。因此，亟需自动化模型设计流程以高效创建高性能模型。本文提出TabGNS（表格门控神经元选择），这是一种专为电信网络表格数据定制的新型基于梯度的神经架构搜索方法。我们在多个电信及通用表格数据集上评估TabGNS，结果表明：相较于最先进的表格NAS方法，该方法在提升预测性能的同时，将架构规模缩减51-82%，搜索时间最多缩短36倍。将TabGNS集成至模型生命周期管理，可实现神经网络在全生命周期的自动化设计，从而加速机器学习解决方案在电信网络中的部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the telecommunications industry&#x27;s need for efficient deep learning models for tasks like traffic prediction and the challenge of manually designing compact architectures for resource-constrained environments, this paper introduces TabGNS, a gradient-based Neural Architecture Search method tailored for tabular data. The method automates model design by selecting neurons via gating mechanisms. Experimental results on telecom and generic tabular datasets show that TabGNS improves prediction performance while reducing architecture size by 51-82% and cutting search time by up to 36 times compared to state-of-the-art tabular NAS methods, facilitating faster deployment of ML solutions in networks.</div>
<div class="mono" style="margin-top:8px">本文的动机源于电信行业在流量预测等任务中对高效深度学习模型的需求，以及为资源受限环境手动设计紧凑架构的挑战，因此提出了TabGNS，一种专为表格数据定制的基于梯度的神经架构搜索方法。该方法通过门控机制选择神经元来自动化模型设计。在电信和通用表格数据集上的实验结果表明，与先进的表格NAS方法相比，TabGNS在提升预测性能的同时，将架构大小减少了51-82%，搜索时间缩短了高达36倍，从而加速了机器学习解决方案在网络中的部署。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: John Gardiner, Orlando Romero, Brendan Tivnan, Nicolò Dal Fabbro, George J. Pappas</div>
<div class="meta-line">First: 2026-02-09T18:01:40+00:00 · Latest: 2026-02-11T13:26:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08965v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08965v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中基于量子纠缠的协同学习</div>
<div class="mono" style="margin-top:8px">通信缺失是多智能体强化学习（MARL）协同面临的主要挑战。先前研究通过共享随机性（有时以关联设备形式）关联局部策略，以辅助分散决策。本研究首次提出训练MARL智能体利用共享量子纠缠作为协同资源的框架，其允许的通信无关关联策略类别远超单纯共享随机性。该框架受量子物理经典结论启发：在特定无通信单轮协作博弈中，共享量子纠缠可实现优于纯随机共享的策略，即存在量子优势。我们提出可微分策略参数化方法以优化量子测量，并设计将联合策略分解为量子协调器与分散局部执行器的新型策略架构。为验证方法有效性，我们首先证明在作为黑盒预言机的单轮博弈中，能通过纯经验学习获得量子优势策略；进而展示在分散部分可观测马尔可夫决策过程（Dec-POMDP）构建的序列决策问题中，该框架可学习具有量子优势的策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of coordination without communication in multi-agent reinforcement learning (MARL) and the known potential of quantum entanglement to outperform classical shared randomness in certain cooperative games. The method introduces a novel framework with a differentiable policy parameterization for optimizing quantum measurements and a policy architecture that combines a quantum coordinator with decentralized local actors. The main experimental results show that the method can learn strategies achieving a quantum advantage from experience in single-round black-box games and can also learn policies with quantum advantage in a sequential Dec-POMDP task.</div>
<div class="mono" style="margin-top:8px">本文的动机源于多智能体强化学习中无通信协调的挑战，以及量子纠缠在某些合作游戏中超越经典共享随机性的已知潜力。方法上，它引入了一个新颖的框架，包含用于优化量子测量的可微分策略参数化，以及一个将联合策略分解为量子协调器和分散局部执行者的策略架构。主要实验结果表明，该方法能够在单轮黑盒游戏中从经验中学习到实现量子优势的策略，并且也能在一个序列化的分散部分可观测马尔可夫决策过程任务中学习到具有量子优势的策略。</div>
</details>
</div>
<div class="card">
<div class="title">SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios</div>
<div class="meta-line">Authors: Yanan Wang, Renxi Wang, Yongxin Wang, Xuezhi Liang, Fajri Koto, Timothy Baldwin, Xiaodan Liang, Haonan Li</div>
<div class="meta-line">First: 2026-02-11T13:26:02+00:00 · Latest: 2026-02-11T13:26:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10840v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10840v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimuScene：面向物理场景模拟的代码生成训练与基准测试</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）在数学竞赛、复杂编程和科学推理等任务中已得到广泛研究，但其通过代码准确表征和模拟物理场景的能力仍待深入探索。我们提出SimuScene，首次系统性地训练和评估LLM在五个物理领域及52个物理概念上的场景模拟能力。我们构建了自动数据收集流程，并经过人工验证以确保质量。最终数据集包含7,659个物理场景，其中334个人工验证样本作为测试集。对10个前沿LLM的评估显示，即使最强模型也仅达到21.5%的通过率，印证了该任务的挑战性。最后，我们提出一种结合视觉奖励的强化学习框架，通过视觉语言模型作为评判器来训练文本模型。实验表明，使用本数据训练能提升代码驱动的物理模拟能力，同时显著增强通用代码生成性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored capability of large language models (LLMs) to simulate physical scenarios through code, this paper introduces SimuScene, a systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 concepts. The method involves building an automatic data collection pipeline with human verification, resulting in a dataset of 7,659 scenarios, including a test set of 334 human-verified examples. Experimental results show that even the strongest LLM achieves only a 21.5% pass rate, highlighting the task&#x27;s difficulty, and further experiments demonstrate that training with this data, enhanced by a reinforcement learning pipeline using a vision-language model as a visual reward judge, improves both physical simulation via code and general code generation performance.</div>
<div class="mono" style="margin-top:8px">本文的动机在于大型语言模型（LLMs）通过代码模拟物理场景的能力尚未得到充分探索，为此提出了SimuScene，这是首个在五个物理领域和52个物理概念上系统训练和评估LLMs模拟物理场景的研究。方法上，通过构建自动数据收集流程并辅以人工验证，创建了一个包含7,659个物理场景的数据集，其中测试集包含334个人工验证示例。主要实验结果表明，即使最强的LLM也仅达到21.5%的通过率，揭示了任务的挑战性；进一步实验显示，利用该数据集进行训练，并结合基于视觉语言模型作为视觉奖励评判器的强化学习流程，能有效提升通过代码的物理模拟能力以及通用代码生成性能。</div>
</details>
</div>
<div class="card">
<div class="title">Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns</div>
<div class="meta-line">Authors: Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai</div>
<div class="meta-line">First: 2025-09-25T13:11:35+00:00 · Latest: 2026-02-11T13:12:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21124v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.21124v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model&#x27;s reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过学习多样化的思维链模式扩展基础模型的推理潜力</div>
<div class="mono" style="margin-top:8px">近期，针对复杂数学推理的大型模型进展主要依赖于强化学习（RL）。在中训练阶段引入长思维链（CoT）数据已被证明能显著提升推理深度。然而，现有方法往往不加区分地使用CoT数据，未能解答何种数据类型最能有效增强模型推理能力这一关键问题。本文首次将基础模型的推理潜力定义为正确回答问题所需独立尝试次数的倒数，该指标与最终模型性能高度相关。我们进而提出利用富含高价值推理模式的多样化数据来扩展推理潜力。具体而言，我们从CoT序列中抽象出具有共性和归纳能力的原子推理模式，并以此构建富含价值推理模式的核心参考集。此外，我们提出一种融合推理模式链与词元熵的双粒度算法，高效地从数据池中筛选出与核心集对齐的高价值CoT数据（CoTP），从而有效训练模型掌握推理能力。仅使用100亿词元的CoTP数据，即可使850亿参数的混合专家（MoE）模型在具有挑战性的AIME 2024和2025测试中提升9.58%，并将下游RL性能上限提高7.81%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to identify which chain-of-thought (CoT) data types most effectively enhance reasoning in large models, this paper introduces the concept of reasoning potential—defined as the inverse of attempts needed for a correct answer—and proposes a method to expand it by leveraging diverse, high-value reasoning patterns. The method abstracts atomic reasoning patterns from CoT sequences to build a core reference set and employs a dual-granularity algorithm based on reasoning pattern chains and token entropy to efficiently select valuable CoT data (CoTP) for training. Experimental results show that using only 10B tokens of CoTP data improves an 85A6B Mixture-of-Experts model by 9.58% on challenging AIME benchmarks and raises the upper bound of downstream reinforcement learning performance by 7.81%.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决当前大模型推理训练中链式思维数据使用 indiscriminately 的问题，首次定义了推理潜力（即正确回答问题所需独立尝试次数的倒数），并提出通过利用多样化的高价值推理模式来扩展这一潜力。方法上，从链式思维序列中抽象出具有共性和归纳能力的原子推理模式，构建核心参考集，并采用基于推理模式链和令牌熵的双粒度算法，高效筛选与核心集对齐的高价值链式思维数据用于训练。实验结果表明，仅使用100亿令牌的此类数据，就能使一个85A6B混合专家模型在AIME 2024和2025挑战基准上提升9.58%，并将下游强化学习性能上限提高7.81%。</div>
</details>
</div>
<div class="card">
<div class="title">RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization</div>
<div class="meta-line">Authors: Linxuan Xia, Xiaolong Yang, Yongyuan Chen, Enyue Zhao, Deng Cai, Yasheng Wang, Boxi Wu</div>
<div class="meta-line">First: 2026-02-11T13:02:40+00:00 · Latest: 2026-02-11T13:02:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10819v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model&#x27;s generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model&#x27;s current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RePO：通过重述策略优化桥接在线策略学习与离线策略知识</div>
<div class="mono" style="margin-top:8px">将大语言模型（LLM）与领域特定数据对齐仍是一项基础性挑战。监督微调（SFT）提供了注入领域知识的直接途径，但往往会削弱模型的泛化能力。相比之下，在线策略强化学习（RL）能保持泛化性，却难以有效吸收超出模型当前推理水平的困难样本。近期的离线策略RL尝试提升了困难样本的利用率，但因强制向离线策略知识分布偏移而导致严重的训练不稳定。为协调离线策略知识的有效吸收与在线策略RL的稳定性，本文提出重述策略优化（RePO）。在RePO中，策略模型被引导先理解离线策略知识，再将其重述为符合自身风格与参数分布的轨迹。RePO动态地用这些重述的高质量轨迹替换低奖励的原始轨迹。该策略在严格保持在线策略训练动态的同时，引导模型走向正确推理路径。多个基准实验表明，RePO提升了困难样本利用率，优于现有基线方法，实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of aligning large language models with domain-specific knowledge without compromising their general capabilities, as existing methods like supervised fine-tuning can degrade generality while on-policy reinforcement learning struggles with hard samples. The proposed method, Rephrasing Policy Optimization (RePO), bridges on-policy learning and off-policy knowledge by prompting the model to comprehend off-policy knowledge and rephrase it into trajectories that match its own stylistic distribution, dynamically replacing low-reward rollouts with these high-quality rephrased paths to maintain training stability. Experimental results on multiple benchmarks show that RePO enhances hard-sample utilization and achieves state-of-the-art performance, outperforming prior baselines.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在领域特定知识对齐中面临的挑战，即监督微调可能损害模型通用性，而在线策略强化学习难以有效吸收超出当前推理水平的困难样本。提出的方法——重述策略优化（RePO），通过引导模型理解离线策略知识并将其重述为符合自身风格分布的轨迹，动态替换低奖励路径，从而在保持在线策略训练稳定性的同时吸收离线知识。在多个基准测试上的实验结果表明，RePO提高了困难样本的利用率，性能优于现有基线方法，达到了最先进水平。</div>
</details>
</div>
<div class="card">
<div class="title">Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training</div>
<div class="meta-line">Authors: Aojun Lu, Tao Feng, Hangjie Yuan, Wei Li, Yanan Sun</div>
<div class="meta-line">First: 2026-02-11T12:55:15+00:00 · Latest: 2026-02-11T12:55:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10815v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10815v1">PDF</a> · <a href="https://github.com/byyx666/DC-SFT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The adaptation of large-scale Vision-Language Models (VLMs) through post-training reveals a pronounced generalization gap: models fine-tuned with Reinforcement Learning (RL) consistently achieve superior out-of-distribution (OOD) performance compared to those trained with Supervised Fine-Tuning (SFT). This paper posits a data-centric explanation for this phenomenon, contending that RL&#x27;s generalization advantage arises from an implicit data filtering mechanism that inherently prioritizes medium-difficulty training samples. To test this hypothesis, we systematically evaluate the OOD generalization of SFT models across training datasets of varying difficulty levels. Our results confirm that data difficulty is a critical factor, revealing that training on hard samples significantly degrades OOD performance. Motivated by this finding, we introduce Difficulty-Curated SFT (DC-SFT), a straightforward method that explicitly filters the training set based on sample difficulty. Experiments show that DC-SFT not only substantially enhances OOD generalization over standard SFT, but also surpasses the performance of RL-based training, all while providing greater stability and computational efficiency. This work offers a data-centric account of the OOD generalization gap in VLMs and establishes a more efficient pathway to achieving robust generalization. Code is available at https://github.com/byyx666/DC-SFT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为何强化学习比监督微调泛化能力更强？从数据视角看视觉语言模型后训练</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（VLM）的后训练适配揭示出显著的泛化差距：与监督微调（SFT）训练的模型相比，通过强化学习（RL）微调的模型在分布外（OOD）性能上持续表现更优。本文从数据角度解释这一现象，认为RL的泛化优势源于其隐含的数据筛选机制，该机制天然优先选择中等难度的训练样本。为验证此假设，我们系统评估了SFT模型在不同难度训练数据集上的OOD泛化能力。结果证实数据难度是关键因素，表明使用困难样本训练会显著降低OOD性能。基于此发现，我们提出难度筛选监督微调（DC-SFT），这是一种根据样本难度显式过滤训练集的简明方法。实验表明，DC-SFT不仅显著提升了标准SFT的OOD泛化能力，其性能甚至超越基于RL的训练方法，同时具备更高的稳定性和计算效率。本研究为VLM的OOD泛化差距提供了数据视角的解释，并建立了实现稳健泛化的更高效路径。代码发布于 https://github.com/byyx666/DC-SFT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates why Reinforcement Learning (RL) post-training generalizes better than Supervised Fine-Tuning (SFT) for Vision-Language Models, proposing a data-centric explanation centered on training sample difficulty. The authors hypothesize that RL implicitly filters data, favoring medium-difficulty samples, and test this by evaluating SFT on datasets of varying difficulties, finding that hard samples degrade out-of-distribution performance. They then introduce Difficulty-Curated SFT (DC-SFT), which explicitly filters training data by difficulty, and show it outperforms both standard SFT and RL in generalization while being more stable and efficient.</div>
<div class="mono" style="margin-top:8px">本文研究了视觉语言模型后训练中强化学习比监督微调泛化能力更好的原因，提出了一个以数据为中心的解释，核心在于训练样本的难度。作者假设强化学习隐式地过滤数据，偏好中等难度样本，并通过在不同难度数据集上评估监督微调来验证，发现困难样本会损害分布外性能。为此，他们提出了难度筛选监督微调方法，显式地基于难度过滤训练数据，实验表明该方法在泛化性能上不仅超越了标准监督微调，也优于基于强化学习的训练，同时更具稳定性和计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">On the Optimal Reasoning Length for RL-Trained Language Models</div>
<div class="meta-line">Authors: Daisuke Nohara, Taishi Nakamura, Rio Yokota</div>
<div class="meta-line">First: 2026-02-10T09:45:42+00:00 · Latest: 2026-02-11T12:19:33+00:00</div>
<div class="meta-line">Comments: 15 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09591v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09591v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论强化学习训练语言模型的最优推理长度</div>
<div class="mono" style="margin-top:8px">强化学习显著提升了大语言模型的推理能力，但也会延长思维链输出，增加训练和推理的计算成本。尽管已有长度控制方法提出，但平衡效率与性能的最优输出长度仍不明确。本研究在Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B两个模型上比较了多种长度控制方法。结果表明：长度惩罚可能阻碍推理习得，而对具备强先验推理能力的模型进行适当调优的长度控制可提升效率。通过将先前研究延伸至强化学习训练策略，我们识别出两种失效模式：1）长输出增加离散度，2）短输出导致思考不足。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the observation that reinforcement learning (RL) improves reasoning in language models but often leads to excessively long and computationally costly chain-of-thought outputs, creating a need to balance efficiency and performance. The method involves comparing several length control techniques on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B, extending prior analysis to RL-trained policies. The main experimental results show that simple length penalties can hinder reasoning acquisition, whereas properly tuned control can improve efficiency for models with strong prior reasoning, and they identify two failure modes: long outputs increase answer dispersion, while short outputs cause under-thinking.</div>
<div class="mono" style="margin-top:8px">本研究动机源于强化学习虽能提升大语言模型的推理能力，却常导致思维链输出过长、增加计算成本，因此需要权衡效率与性能。方法上，本研究在Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B两个模型上比较了多种长度控制技术，并将先前分析扩展至强化学习训练的策略。主要实验结果表明，简单的长度惩罚会阻碍推理能力的习得，而对具有较强先验推理能力的模型进行适当调优的长度控制则可提升效率；同时研究识别出两种失效模式：输出过长会增加答案的分散性，而输出过短则导致思考不足。</div>
</details>
</div>
<div class="card">
<div class="title">Progress Constraints for Reinforcement Learning in Behavior Trees</div>
<div class="meta-line">Authors: Finn Rietz, Mart Kartašev, Petter Ögren, Johannes A. Stork</div>
<div class="meta-line">First: 2026-02-06T09:26:44+00:00 · Latest: 2026-02-11T12:15:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06525v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06525v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Behavior Trees (BTs) provide a structured and reactive framework for decision-making, commonly used to switch between sub-controllers based on environmental conditions. Reinforcement Learning (RL), on the other hand, can learn near-optimal controllers but sometimes struggles with sparse rewards, safe exploration, and long-horizon credit assignment. Combining BTs with RL has the potential for mutual benefit: a BT design encodes structured domain knowledge that can simplify RL training, while RL enables automatic learning of the controllers within BTs. However, naive integration of BTs and RL can lead to some controllers counteracting other controllers, possibly undoing previously achieved subgoals, thereby degrading the overall performance. To address this, we propose progress constraints, a novel mechanism where feasibility estimators constrain the allowed action set based on theoretical BT convergence results. Empirical evaluations in a 2D proof-of-concept and a high-fidelity warehouse environment demonstrate improved performance, sample efficiency, and constraint satisfaction, compared to prior methods of BT-RL integration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为树中强化学习的进展约束</div>
<div class="mono" style="margin-top:8px">行为树（BTs）为决策提供了结构化且反应灵敏的框架，常用于根据环境条件在子控制器之间切换。强化学习（RL）能够学习近似最优的控制器，但有时在稀疏奖励、安全探索和长期信用分配方面存在困难。将BTs与RL结合具有相互促进的潜力：BT设计编码了结构化领域知识，可简化RL训练，而RL则能自动学习BT内的控制器。然而，BTs与RL的简单集成可能导致某些控制器抵消其他控制器，可能破坏先前达成的子目标，从而降低整体性能。为解决此问题，我们提出进展约束，这是一种新颖机制，其中可行性估计器基于理论上的BT收敛结果来限制允许的动作集。在二维概念验证和高保真仓库环境中的实证评估表明，与先前的BT-RL集成方法相比，该方法在性能、样本效率和约束满足方面均有提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the complementary strengths of Behavior Trees (BTs), which offer structured decision-making, and Reinforcement Learning (RL), which learns optimal controllers but faces challenges like sparse rewards. The method introduces progress constraints, a novel integration mechanism that uses feasibility estimators derived from BT convergence theory to restrict the action set and prevent learned controllers from undoing previously achieved subgoals. Experimental results in both a 2D proof-of-concept and a high-fidelity warehouse simulation show that this approach improves overall performance, sample efficiency, and constraint satisfaction compared to prior BT-RL integration methods.</div>
<div class="mono" style="margin-top:8px">本文的动机在于结合行为树（BT）的结构化决策优势与强化学习（RL）学习最优控制器的能力，后者常面临稀疏奖励等问题。方法上提出了进度约束这一新颖机制，利用基于BT收敛理论推导的可行性估计器来限制动作集，防止学习到的控制器抵消已实现的子目标。在二维概念验证和高保真仓库环境中的实验结果表明，与先前的BT-RL集成方法相比，该方法提高了整体性能、样本效率和约束满足度。</div>
</details>
</div>
<div class="card">
<div class="title">A PBN-RL-XAI Framework for Discovering a &quot;Hit-and-Run&quot; Therapeutic Strategy in Melanoma</div>
<div class="meta-line">Authors: Zhonglin Liu</div>
<div class="meta-line">First: 2025-07-14T10:35:38+00:00 · Latest: 2026-02-11T11:49:35+00:00</div>
<div class="meta-line">Comments: 7 pages, 7 figures. Accepted by the IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2025. Code is available at https://github.com/Liu-Zhonglin/pbn-melanoma-project</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10136v6">Abs</a> · <a href="https://arxiv.org/pdf/2507.10136v6">PDF</a> · <a href="https://github.com/Liu-Zhonglin/pbn-melanoma-project">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Innate resistance to anti-PD-1 immunotherapy remains a major clinical challenge in metastatic melanoma, with the underlying molecular networks being poorly understood. To address this, we constructed a dynamic Probabilistic Boolean Network model using transcriptomic data from patient tumor biopsies to elucidate the regulatory logic governing therapy response. We then employed a reinforcement learning agent to systematically discover optimal, multi-step therapeutic interventions and used explainable artificial intelligence to mechanistically interpret the agent&#x27;s control policy. The analysis revealed that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2 protein (LOXL2) was the most effective strategy. Our explainable analysis showed that this &#x27;&#x27;hit-and-run&quot; intervention is sufficient to erase the molecular signature driving resistance, allowing the network to self-correct without requiring sustained intervention. This study presents a novel, time-dependent therapeutic hypothesis for overcoming immunotherapy resistance and provides a powerful computational framework for identifying non-obvious intervention protocols in complex biological systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于发现黑色素瘤“一击即走”治疗策略的PBN-RL-XAI框架</div>
<div class="mono" style="margin-top:8px">对抗PD-1免疫疗法的先天耐药性仍是转移性黑色素瘤的主要临床挑战，其背后的分子网络机制尚不明确。为此，我们利用患者肿瘤活检的转录组数据构建了一个动态概率布尔网络模型，以阐明调控治疗反应的逻辑规则。随后，我们采用强化学习智能体系统性地探索最优的多步治疗干预方案，并运用可解释人工智能机制化地解读智能体的控制策略。分析表明，对赖氨酰氧化酶样2蛋白（LOXL2）进行精确计时的四步短暂抑制是最有效的策略。我们的可解释性分析显示，这种“一击即走”的干预足以消除驱动耐药性的分子特征，使网络能够自我修正而无需持续干预。本研究提出了克服免疫疗法耐药性的新型时间依赖性治疗假说，并为识别复杂生物系统中非显而易见的干预方案提供了强大的计算框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the clinical challenge of innate resistance to anti-PD-1 immunotherapy in metastatic melanoma, this study developed a computational framework combining a dynamic Probabilistic Boolean Network model, reinforcement learning, and explainable AI to discover optimal therapeutic strategies. The method used transcriptomic data from patient biopsies to model regulatory networks, then employed a reinforcement learning agent to identify multi-step interventions, with explainable AI interpreting the agent&#x27;s decisions. The main experimental result revealed that a precisely timed, 4-step temporary inhibition of LOXL2 protein was the most effective strategy, showing this &quot;hit-and-run&quot; approach could erase the molecular signature of resistance and allow the network to self-correct without sustained intervention.</div>
<div class="mono" style="margin-top:8px">本研究针对转移性黑色素瘤中抗PD-1免疫疗法的先天耐药性这一临床挑战，开发了一个结合动态概率布尔网络模型、强化学习和可解释人工智能的计算框架，以发现最佳治疗策略。该方法利用患者活检的转录组数据构建调控网络模型，然后使用强化学习智能体识别多步骤干预方案，并通过可解释人工智能解释智能体的决策。主要实验结果表明，精确定时的四步短暂抑制LOXL2蛋白是最有效的策略，这种“打了就跑”的方法能够消除驱动耐药的分子特征，使网络自我纠正而无需持续干预。</div>
</details>
</div>
<div class="card">
<div class="title">A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer</div>
<div class="meta-line">Authors: Azkaa Nasir, Fatima Dossa, Muhammad Ahmed Atif, Mohammad Shahid Shaikh</div>
<div class="meta-line">First: 2026-02-10T14:18:03+00:00 · Latest: 2026-02-11T11:47:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09810v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09810v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (DDQN) and Dueling DQN influence transfer behavior across environments. Using CartPole as a source task and LunarLander as a structurally distinct target task, we evaluate a fixed layer-wise representation transfer protocol under identical hyperparameters and training conditions, with baseline agents trained from scratch used to contextualize transfer effects. Empirical results show that DDQN consistently avoids negative transfer under the examined setup and maintains learning dynamics comparable to baseline performance in the target environment. In contrast, Dueling DQN consistently exhibits negative transfer under identical conditions, characterized by degraded rewards and unstable optimization behavior. Statistical analysis across multiple random seeds confirms a significant performance gap under transfer. These findings suggest that architectural inductive bias is strongly associated with robustness to cross-environment transfer in value-based deep reinforcement learning under the examined transfer protocol.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双DQN与竞争DQN在跨环境迁移下的对照研究</div>
<div class="mono" style="margin-top:8px">深度强化学习中的迁移学习通常旨在提升稳定性并降低训练成本，但在显著领域偏移下也可能失效。本文通过对照实验研究双深度Q网络（DDQN）与竞争DQN的架构差异如何影响跨环境迁移行为。以CartPole作为源任务、LunarLander作为结构差异明显的目标任务，在相同超参数与训练条件下采用固定分层表征迁移方案，并以从头训练的基线智能体作为迁移效果的参照基准。实验结果表明：在既定实验设置下，DDQN能持续避免负迁移，且在目标环境中保持与基线相当的学习动态；相比之下，竞争DQN在相同条件下始终呈现负迁移现象，表现为奖励衰减与优化过程不稳定。基于多随机种子的统计分析证实了迁移条件下显著的性能差异。这些发现表明，在所考察的迁移方案中，架构归纳偏置与基于价值的深度强化学习在跨环境迁移的鲁棒性存在强关联。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the need to understand how architectural choices in deep reinforcement learning affect transfer learning robustness, particularly when domain shifts occur. The method involves a controlled empirical comparison between Double DQN and Dueling DQN, using CartPole as a source environment and LunarLander as a distinct target, with a fixed layer-wise transfer protocol under identical training conditions. The main experimental results show that Double DQN consistently avoids negative transfer and performs comparably to baseline agents trained from scratch, while Dueling DQN exhibits significant negative transfer with degraded rewards and unstable optimization, with statistical analysis confirming a significant performance gap.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究深度强化学习中架构选择对迁移学习鲁棒性的影响，特别是在领域差异显著时。方法上，通过控制实验比较了Double DQN和Dueling DQN，使用CartPole作为源环境、LunarLander作为目标环境，并采用固定的分层迁移协议和相同的训练条件。主要实验结果表明，Double DQN能持续避免负迁移，其性能与从头训练的基线代理相当，而Dueling DQN在相同条件下表现出明显的负迁移，奖励下降且优化不稳定，统计分析证实了显著的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Translate Policy to Language: Flow Matching Generated Rewards for LLM Explanations</div>
<div class="meta-line">Authors: Xinyi Yang, Liang Zeng, Heng Dong, Chao Yu, Xiaoran Wu, Huazhong Yang, Yu Wang, Milind Tambe, Tonghan Wang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-02-18T04:34:45+00:00 · Latest: 2026-02-11T10:54:33+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.12530v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.12530v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As humans increasingly share environments with diverse agents powered by RL, LLMs, and beyond, the ability to explain agent policies in natural language is vital for reliable coexistence. We introduce a general-purpose framework that trains explanation-generating LLMs via reinforcement learning from AI feedback, with distributional rewards generated by generative continuous normalizing flows (CNFs). CNFs capture the pluralistic and probabilistic nature of human judgments about explanations. Moreover, under mild assumptions, CNFs provably bound deviations from true human reward distributions when trained on noisy proxy rewards from LLMs. We design a specialized CNF architecture that selectively attends to linguistic cues in the decision context and explanations when generating rewards. Human and LLM evaluators find that our method delivers explanations that enable more accurate predictions of true agent decisions, exhibit greater logical soundness and actionability, and impose lower cognitive load than explanations trained with proxy LLM rewards or state-of-the-art RLHF and RLAIF baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略语言化：基于流匹配生成奖励的大语言模型解释方法</div>
<div class="mono" style="margin-top:8px">随着人类日益频繁地与基于强化学习、大语言模型等技术的多样化智能体共存，用自然语言解释智能体策略的能力对实现可靠协同至关重要。本文提出一种通用框架，通过人工智能反馈的强化学习训练解释生成型大语言模型，其分布奖励由生成式连续归一化流生成。连续归一化流能捕捉人类对解释评判的多元化和概率性特征。在温和假设下，当基于大语言模型噪声代理奖励训练时，连续归一化流可证明地约束与真实人类奖励分布的偏差。我们设计了专用连续归一化流架构，在生成奖励时选择性关注决策语境与解释中的语言线索。人类与大语言模型评估表明：相较于基于代理大语言模型奖励或前沿RLHF/RLAIF基线训练的解释，本方法生成的解释能更准确预测真实智能体决策，展现更强的逻辑严谨性与可操作性，并降低认知负荷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable human-agent coexistence, this paper introduces a framework to train LLMs to explain agent policies using reinforcement learning from AI feedback, where rewards are generated by continuous normalizing flows (CNFs) to capture the pluralistic nature of human judgment. The method employs a specialized CNF architecture that attends to linguistic cues in decision contexts and explanations, provably bounding deviations from true human reward distributions under mild assumptions. Experimental results show that explanations generated by this approach enable more accurate predictions of agent decisions, exhibit greater logical soundness and actionability, and impose lower cognitive load compared to baselines using proxy LLM rewards or state-of-the-art RLHF and RLAIF methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是确保人类与多样化智能体可靠共存，提出一个通用框架，通过基于人工智能反馈的强化学习来训练生成解释的大语言模型，其中奖励由连续归一化流生成，以捕捉人类判断的多元概率特性。该方法采用一种专门设计的连续归一化流架构，在生成奖励时选择性关注决策上下文和解释中的语言线索，并在温和假设下可证明地约束与真实人类奖励分布的偏差。实验结果表明，与使用代理大语言模型奖励或最先进的RLHF和RLAIF基线方法相比，该方法生成的解释能更准确地预测智能体决策，具有更强的逻辑合理性和可操作性，并降低了认知负荷。</div>
</details>
</div>
<div class="card">
<div class="title">Decentralized Reinforcement Learning for Multi-Agent Multi-Resource Allocation via Dynamic Cluster Agreements</div>
<div class="meta-line">Authors: Antonio Marino, Esteban Restrepo, Claudio Pacchierotti, Paolo Robuffo Giordano</div>
<div class="meta-line">Venue: IEEE Robotics and Automation Letters, 2025, 10 (8), pp.8123-8130</div>
<div class="meta-line">First: 2025-03-04T09:23:48+00:00 · Latest: 2026-02-11T10:47:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.02437v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.02437v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses the challenge of allocating heterogeneous resources among multiple agents in a decentralized manner. Our proposed method, Liquid-Graph-Time Clustering-IPPO, builds upon Independent Proximal Policy Optimization (IPPO) by integrating dynamic cluster consensus, a mechanism that allows agents to form and adapt local sub-teams based on resource demands. This decentralized coordination strategy reduces reliance on global information and enhances scalability. We evaluate LGTC-IPPO against standard multi-agent reinforcement learning baselines and a centralized expert solution across a range of team sizes and resource distributions. Experimental results demonstrate that LGTC-IPPO achieves more stable rewards, better coordination, and robust performance even as the number of agents or resource types increases. Additionally, we illustrate how dynamic clustering enables agents to reallocate resources efficiently also for scenarios with discharging resources.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于动态集群共识的去中心化多智能体多资源分配强化学习方法</div>
<div class="mono" style="margin-top:8px">本文研究了在去中心化环境下多智能体间异构资源分配的挑战。我们提出的方法——液态图-时间聚类-独立近端策略优化（LGTC-IPPO），在独立近端策略优化（IPPO）基础上，引入了动态集群共识机制，使智能体能根据资源需求动态形成并调整局部子团队。这种去中心化协调策略降低了对全局信息的依赖，提升了系统可扩展性。我们通过不同团队规模和资源分布的实验，将LGTC-IPPO与标准多智能体强化学习基线及集中式专家方案进行对比。结果表明，即使智能体数量或资源类型增加，LGTC-IPPO仍能实现更稳定的奖励、更优的协调能力和鲁棒性能。此外，我们还展示了动态聚类机制如何帮助智能体在资源衰减场景中实现高效资源再分配。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for decentralized, scalable methods to allocate heterogeneous resources among multiple agents without relying on global information. The proposed method, Liquid-Graph-Time Clustering-IPPO (LGTC-IPPO), extends Independent Proximal Policy Optimization by incorporating dynamic cluster consensus, where agents autonomously form and adapt local sub-teams based on real-time resource demands to coordinate actions. Experimental evaluations against multi-agent reinforcement learning baselines and a centralized expert show that LGTC-IPPO achieves more stable rewards, improved coordination, and robust performance as team sizes or resource types increase, including in scenarios with discharging resources.</div>
<div class="mono" style="margin-top:8px">本文旨在解决多智能体在去中心化环境下分配异构资源的挑战，以减少对全局信息的依赖并提升可扩展性。所提出的方法LGTC-IPPO基于独立近端策略优化，引入了动态集群共识机制，使智能体能根据资源需求动态形成和调整本地子团队以实现协调。实验结果表明，与标准多智能体强化学习基准和集中式专家方案相比，LGTC-IPPO在多种团队规模和资源分布下获得了更稳定的奖励、更好的协调性，并在智能体数量或资源类型增加时表现出鲁棒性能，包括资源消耗场景。</div>
</details>
</div>
<div class="card">
<div class="title">Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning</div>
<div class="meta-line">Authors: Andrés Holgado-Sánchez, Peter Vamplew, Richard Dazeley, Sascha Ossowski, Holger Billhardt</div>
<div class="meta-line">First: 2026-02-09T16:06:36+00:00 · Latest: 2026-02-11T10:09:11+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures. To be published in proceedings of the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). This is a full version that includes the supplementary material</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08835v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08835v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏好多目标强化学习的社会价值体系学习</div>
<div class="mono" style="margin-top:8px">价值感知人工智能应能识别人类价值观并适应不同用户的价值体系（基于价值的偏好）。这需要对价值观进行操作化处理，但可能存在设定偏差。价值观的社会性要求其表征需兼顾多用户需求，而价值体系虽具多样性，却在群体间呈现规律性模式。在序列决策中，已有研究通过多样化智能体的示范行为，针对不同目标或价值观进行个性化适配。然而，这些方法或依赖人工设计特征，或缺乏基于价值的可解释性及对多元用户偏好的适应能力。
我们提出基于聚类与偏好多目标强化学习的算法，用于在马尔可夫决策过程中学习智能体社会的价值对齐模型与价值体系。该方法联合学习社会衍生的价值对齐模型（基础表征）以及能简明表征社会不同用户群体（聚类）的价值体系集合。每个聚类包含代表成员价值偏好的价值体系，以及反映符合该价值体系行为的近似帕累托最优策略。我们在两个包含人类价值观的MDP环境中，将本方法与前沿PbMORL算法及基线模型进行对比评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling AI to recognize and adapt to diverse human value systems, which is crucial for value-aware AI but prone to misspecification. The authors propose a method that combines clustering and preference-based multi-objective reinforcement learning (PbMORL) to jointly learn value alignment models and a set of value systems representing different societal groups in Markov Decision Processes. Experimental results on two MDPs with human values demonstrate that their approach outperforms a state-of-the-art PbMORL algorithm and baselines, effectively capturing group-specific preferences and generating corresponding Pareto-optimal policies.</div>
<div class="mono" style="margin-top:8px">本文旨在解决人工智能识别和适应多样化人类价值体系的挑战，这对于价值感知AI至关重要但容易产生误判。作者提出了一种结合聚类和基于偏好的多目标强化学习的方法，在马尔可夫决策过程中联合学习价值对齐模型以及代表不同社会群体的价值体系。在包含人类价值的两个MDP上的实验结果表明，该方法优于现有的先进PbMORL算法和基线，能有效捕捉群体特定偏好并生成相应的帕累托最优策略。</div>
</details>
</div>
<div class="card">
<div class="title">Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation</div>
<div class="meta-line">Authors: Jie Jiang, Yangru Huang, Zeyu Wang, Changping Wang, Yuling Xiong, Jun Zhang, Huan Yu</div>
<div class="meta-line">First: 2026-02-11T09:57:36+00:00 · Latest: 2026-02-11T09:57:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10699v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10699v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将搜索资源投入高回报领域：面向生成式推荐的价值引导结构化采样与优化</div>
<div class="mono" style="margin-top:8px">基于自回归模型的生成式推荐将检索与排序统一至单一条件生成框架，但通过强化学习（RL）微调这类模型常面临概率与奖励的根本性错配。传统以似然为主导的解码方法（如束搜索）对局部高概率前缀存在短视偏好，导致两大关键缺陷：（1）探索不足——低概率分支中的高奖励项被过早剪枝且极少被采样；（2）优势压缩——共享高概率前缀的轨迹获得高度相关的奖励，组内方差极低，导致RL缺乏有效比较信号。为此，我们提出V-STAR框架，即价值引导采样与树状结构优势强化框架。V-STAR通过两个协同组件形成自进化闭环：首先，设计价值引导高效解码（VED）以识别关键节点并选择性深化高潜力前缀，在避免穷举树搜索的同时提升探索效率；其次，提出Sibling-GRPO方法，利用推导的树状拓扑计算兄弟节点相对优势，将学习信号聚焦于关键分支决策。在离线与在线数据集上的大量实验表明，V-STAR在严格延迟约束下优于现有基线模型，实现了更优的准确性与候选集多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the probability-reward mismatch in fine-tuning generative recommendation models with Reinforcement Learning (RL), where conventional decoding methods like beam search suffer from insufficient exploration of low-probability, high-reward items and advantage compression due to correlated rewards. The authors propose V-STAR, a framework combining Value-Guided Efficient Decoding to selectively deepen promising search branches and Sibling-GRPO, which uses tree-structured sibling-relative advantages to focus learning on decisive decisions. Experimental results on offline and online datasets show that V-STAR outperforms state-of-the-art methods, achieving higher accuracy and diversity while meeting strict latency constraints.</div>
<div class="mono" style="margin-top:8px">本文针对使用强化学习微调生成式推荐模型时存在的概率-奖励不匹配问题，传统解码方法如束搜索会因探索不足和优势压缩而失效。作者提出了V-STAR框架，它包含价值引导高效解码以选择性深化有潜力的搜索分支，以及利用树状结构的兄弟相对优势的Sibling-GRPO来聚焦学习信号于关键决策。在离线和在线数据集上的广泛实验表明，V-STAR优于现有先进方法，在严格延迟约束下实现了更高的准确性和候选集多样性。</div>
</details>
</div>
<div class="card">
<div class="title">VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training</div>
<div class="meta-line">Authors: Guobin Shen, Chenxiao Zhao, Xiang Cheng, Lei Huang, Xing Yu</div>
<div class="meta-line">First: 2026-02-11T09:48:08+00:00 · Latest: 2026-02-11T09:48:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10693v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10693v1">PDF</a> · <a href="https://github.com/FloyedShen/VESPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VESPO：面向稳定离策略大语言模型训练的变分序列级软策略优化</div>
<div class="mono" style="margin-top:8px">训练稳定性始终是大语言模型强化学习的核心挑战。策略陈旧性、异步训练以及训练与推理引擎间的失配，均会导致行为策略偏离当前策略，引发训练崩溃风险。重要性采样为这种分布偏移提供了理论修正，但存在高方差缺陷；现有解决方案（如词元级截断和序列级归一化）缺乏统一的理论基础。本文提出变分序列级软策略优化方法。通过将方差缩减融入提议分布的变分框架，VESPO推导出可直接作用于序列级重要性权重（无需长度归一化）的闭式重塑核函数。在数学推理基准测试中，VESPO能在高达64倍陈旧率及完全异步执行环境下保持训练稳定性，并在稠密模型与混合专家模型中均取得持续性能提升。代码已开源：https://github.com/FloyedShen/VESPO</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training instability in reinforcement learning for large language models, which arises from policy staleness, asynchronous training, and mismatches between training and inference engines. The authors propose VESPO, a method that integrates variance reduction into a variational framework over proposal distributions to derive a closed-form reshaping kernel for sequence-level importance weights, avoiding length normalization. Experimental results on mathematical reasoning benchmarks demonstrate that VESPO ensures stable training under high staleness ratios and asynchronous conditions, while consistently improving performance across both dense and Mixture-of-Experts models.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习中因策略陈旧、异步训练以及训练与推理引擎不匹配导致的训练不稳定问题展开研究。作者提出了VESPO方法，该方法将方差减少整合到关于提议分布的变分框架中，推导出一个直接作用于序列级重要性权重的闭式重塑核，无需进行长度归一化。在数学推理基准测试上的实验结果表明，VESPO能在高陈旧比和完全异步执行条件下保持训练稳定，并在稠密模型与专家混合模型上均实现了性能的持续提升。</div>
</details>
</div>
<div class="card">
<div class="title">CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs</div>
<div class="meta-line">Authors: Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully</div>
<div class="meta-line">First: 2026-02-10T18:51:39+00:00 · Latest: 2026-02-11T09:46:16+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10085v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10085v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/code-sharp/homepage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos at https://sites.google.com/view/code-sharp/homepage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CODE-SHARP：作为分层奖励程序的持续开放式技能发现与演进</div>
<div class="mono" style="margin-top:8px">开发能够开放式发现和学习新技能的智能体是人工智能领域的重大挑战。虽然强化学习为训练智能体掌握复杂技能提供了强大框架，但其通常依赖于人工设计的奖励函数。这对于开放式技能发现并不可行，因为有意义技能集合并非先验已知。尽管近期方法在自动化奖励函数设计方面展现出前景，但仍局限于为预定义任务优化奖励。为突破此限制，我们提出“作为分层奖励程序的持续开放式技能发现与演进”（CODE-SHARP），该创新框架利用基础模型开放式扩展并优化分层技能库——该库以代码形式构建为可执行奖励函数的有向图。实验表明，仅通过发现的SHARP技能生成奖励进行训练的目标条件智能体，能在Craftax环境中学会解决日益长时程的目标任务。当结合高层基于基础模型的规划器时，所发现的技能使单一目标条件智能体能够解决复杂的长时程任务，在平均性能上超越预训练智能体与任务专用专家策略超过134%。我们将开源代码并在https://sites.google.com/view/code-sharp/homepage提供补充视频。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling agents to autonomously discover and learn novel skills without predefined tasks, moving beyond traditional reinforcement learning&#x27;s reliance on hand-designed rewards. It introduces CODE-SHARP, a framework that uses Foundation Models to continuously expand and refine a hierarchical archive of skills, represented as executable reward programs in code. Experimental results in the Craftax environment demonstrate that an agent trained solely on these discovered skills can solve increasingly long-horizon goals, and when combined with a high-level planner, it outperforms pretrained agents and task-specific experts by over 134% on average in complex tasks.</div>
<div class="mono" style="margin-top:8px">本文旨在解决智能体在无需预定义任务的情况下自主发现和学习新技能的挑战，克服了传统强化学习依赖人工设计奖励的局限。它提出了CODE-SHARP框架，利用基础模型持续扩展和优化一个层次化技能库，其中技能以代码形式的可执行奖励程序表示。在Craftax环境中的实验结果表明，仅基于这些发现技能训练的智能体能够解决日益增长的长时程目标，当与高层规划器结合时，在复杂任务中平均性能超过预训练智能体和任务特定专家策略134%以上。</div>
</details>
</div>
<div class="card">
<div class="title">OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL</div>
<div class="meta-line">Authors: Jinjie Shen, Jing Wu, Yaxiong Wang, Lechao Cheng, Shengeng Tang, Tianrui Hui, Nan Pu, Zhun Zhong</div>
<div class="meta-line">First: 2026-02-11T09:41:36+00:00 · Latest: 2026-02-11T09:41:36+00:00</div>
<div class="meta-line">Comments: 38 pages, DeepFake Detection</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10687v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniVL-Guard：基于均衡强化学习的统一视觉语言伪造检测与定位框架</div>
<div class="mono" style="margin-top:8px">现有伪造检测方法通常局限于单模态或双模态场景，难以处理现实世界虚假信息中普遍存在的文本、图像与视频交错内容。为弥补这一空白，本文致力于构建一个统一的全域视觉语言伪造检测与定位框架。在此统一框架下，多模态间的交互作用与检测定位的双重要求引发了关键的“难度偏差”问题：较简单的真实性分类任务易主导梯度更新，导致多任务优化中细粒度定位性能欠佳。为此，我们提出OmniVL-Guard——一种基于均衡强化学习的全域视觉语言伪造检测与定位框架。该框架包含两大核心设计：自演进思维链生成与自适应奖励缩放策略优化。自演进思维链生成通过合成高质量推理路径，有效克服冷启动难题；基于此，自适应奖励缩放策略优化通过动态调节奖励尺度与任务权重，确保均衡的联合优化。大量实验表明，OmniVL-Guard显著优于现有最优方法，并在跨域场景中展现出零样本鲁棒泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing uni-modal or bi-modal forgery detection methods in handling the interleaved text, images, and videos common in real-world misinformation, this paper proposes a unified framework for omnibus vision-language forgery detection and grounding. The method introduces OmniVL-Guard, which employs a balanced reinforcement learning approach featuring two core designs: Self-Evolving Chain-of-Thought (CoT) Generation to synthesize high-quality reasoning paths and overcome cold-start challenges, and Adaptive Reward Scaling Policy Optimization (ARSPO) to dynamically modulate reward scales and task weights, thereby addressing the difficulty bias problem where simpler classification tasks dominate gradients during multi-task optimization. Experimental results show that OmniVL-Guard significantly outperforms state-of-the-art methods and demonstrates robust zero-shot generalization in out-of-domain scenarios.</div>
<div class="mono" style="margin-top:8px">针对现有单模态或双模态伪造检测方法难以处理现实世界中普遍存在的交错文本、图像和视频的局限性，本文旨在开发一个统一的框架，用于全方位的视觉-语言伪造检测与定位。该方法提出了OmniVL-Guard，采用平衡强化学习框架，其核心设计包括：自我演化的思维链生成，以合成高质量推理路径并克服冷启动问题；以及自适应奖励缩放策略优化，动态调整奖励规模和任务权重，从而解决多任务优化中较简单的分类任务主导梯度的难度偏差问题。大量实验表明，OmniVL-Guard显著优于现有最先进方法，并在域外场景中展现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Analysis of Control Bellman Residual Minimization for Markov Decision Problem</div>
<div class="meta-line">Authors: Donghwan Lee, Hyukjun Yang</div>
<div class="meta-line">First: 2026-01-26T10:58:27+00:00 · Latest: 2026-02-11T09:33:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18840v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18840v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Markov decision problems are most commonly solved via dynamic programming. Another approach is Bellman residual minimization, which directly minimizes the squared Bellman residual objective function. However, compared to dynamic programming, this approach has received relatively less attention, mainly because it is often less efficient in practice and can be more difficult to extend to model-free settings such as reinforcement learning. Nonetheless, Bellman residual minimization has several advantages that make it worth investigating, such as more stable convergence with function approximation for value functions. While Bellman residual methods for policy evaluation have been widely studied, methods for policy optimization (control tasks) have been scarcely explored. In this paper, we establish foundational results for the control Bellman residual minimization for policy optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>马尔可夫决策问题中控制贝尔曼残差最小化的分析</div>
<div class="mono" style="margin-top:8px">马尔可夫决策问题通常通过动态规划求解。另一种方法是贝尔曼残差最小化，它直接最小化平方贝尔曼残差目标函数。然而，与动态规划相比，该方法在实践中效率较低且更难扩展至无模型设置（如强化学习），因此关注度相对较低。尽管如此，贝尔曼残差最小化具有值得研究的优势，例如在价值函数逼近中收敛更稳定。虽然策略评估的贝尔曼残差方法已得到广泛研究，但策略优化（控制任务）的方法仍鲜有探索。本文为策略优化中的控制贝尔曼残差最小化建立了基础性理论结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the relative neglect of Bellman residual minimization for control tasks despite its potential advantages like stable convergence with function approximation, this paper establishes foundational results for its application to policy optimization in Markov decision problems. The method focuses on directly minimizing the squared Bellman residual objective, contrasting with dynamic programming approaches. The main experimental results provide theoretical groundwork, addressing the scarcity of exploration in control settings and highlighting its viability for policy optimization.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，尽管贝尔曼残差最小化在控制任务中具有如函数逼近下稳定收敛等优势，但相对较少被探索，因此为策略优化建立了基础性结果。该方法直接最小化平方贝尔曼残差目标函数，与动态规划方法形成对比。主要实验结果提供了理论框架，解决了控制设置中研究不足的问题，并强调了其在策略优化中的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations</div>
<div class="meta-line">Authors: Abdelrhman Shaheen, Anas Badr, Ali Abohendy, Hatem Alsaadawy, Nadine Alsayad, Ehab H. El-Shazly</div>
<div class="meta-line">First: 2025-02-14T17:06:34+00:00 · Latest: 2026-02-11T09:17:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.10303v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.10303v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has been widely used in many applications, particularly in gaming, which serves as an excellent training ground for AI models. Google DeepMind has pioneered innovations in this field, employing reinforcement learning algorithms, including model-based, model-free, and deep Q-network approaches, to create advanced AI models such as AlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial model, integrates supervised learning and reinforcement learning to master the game of Go, surpassing professional human players. AlphaGo Zero refines this approach by eliminating reliance on human gameplay data, instead utilizing self-play for enhanced learning efficiency. MuZero further extends these advancements by learning the underlying dynamics of game environments without explicit knowledge of the rules, achieving adaptability across various games, including complex Atari games. This paper reviews the significance of reinforcement learning applications in Atari and strategy-based games, analyzing these three models, their key innovations, training processes, challenges encountered, and improvements made. Additionally, we discuss advancements in the field of gaming, including MiniZero and multi-agent models, highlighting future directions and emerging AI models from Google DeepMind.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习在策略类与雅达利游戏中的应用：谷歌DeepMind创新成果综述</div>
<div class="mono" style="margin-top:8px">强化学习（RL）已在众多领域得到广泛应用，尤其在游戏领域，其为人工智能模型提供了绝佳的训练平台。谷歌DeepMind在该领域率先取得突破，通过运用包括基于模型、无模型及深度Q网络在内的强化学习算法，开发出AlphaGo、AlphaGo Zero和MuZero等先进AI模型。初始模型AlphaGo融合监督学习与强化学习，掌握了围棋技艺并超越人类职业选手。AlphaGo Zero进一步优化此方法，摒弃对人类对弈数据的依赖，转而通过自我对弈提升学习效率。MuZero则更进一步，无需明确规则知识即可学习游戏环境的内在动态，实现了在包括复杂雅达利游戏在内的多种游戏中的适应性。本文综述了强化学习在雅达利及策略类游戏中的应用价值，剖析了上述三种模型的核心创新、训练流程、面临挑战及改进方案。此外，我们还探讨了游戏领域的最新进展，包括MiniZero与多智能体模型，并展望了谷歌DeepMind未来研究方向及新兴AI模型的发展趋势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This review paper is motivated by the role of games as a key testing ground for reinforcement learning (RL) and aims to analyze Google DeepMind&#x27;s pioneering innovations in this domain. The method involves examining three landmark model architectures: AlphaGo, which combines supervised and RL learning; AlphaGo Zero, which uses purely self-play RL without human data; and MuZero, which learns environment dynamics without prior rule knowledge to achieve general game mastery. The main experimental results highlight that these models successively surpassed professional human players in Go and demonstrated superior performance and adaptability across diverse games, including complex Atari titles, thereby advancing the field toward more general and efficient AI systems.</div>
<div class="mono" style="margin-top:8px">本文的动机在于游戏作为强化学习的关键试验场，旨在分析Google DeepMind在该领域的开创性创新。研究方法聚焦于审视三种标志性模型架构：AlphaGo结合监督学习与强化学习；AlphaGo Zero摒弃人类对弈数据，仅通过自我对弈进行强化学习；MuZero则无需预先知晓规则，通过自主学习环境动态来实现通用游戏掌握。主要实验结果强调，这些模型相继在围棋中超越人类职业选手，并在包括复杂Atari游戏在内的多种游戏中展现出卓越的性能与适应性，从而推动了人工智能系统向更通用、高效的方向发展。</div>
</details>
</div>
<div class="card">
<div class="title">Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments</div>
<div class="meta-line">Authors: Aashwin Mishra, Matt Seaberg, Ryan Roussel, Daniel Ratner, Apurva Mehta</div>
<div class="meta-line">First: 2026-02-11T09:15:20+00:00 · Latest: 2026-02-11T09:15:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10670v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10670v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization (BO) is a powerful tool for optimizing complex non-linear systems. However, its performance degrades in high-dimensional problems with tightly coupled parameters and highly asymmetric objective landscapes, where rewards are sparse. In such needle-in-a-haystack scenarios, even advanced methods like trust-region BO (TurBO) often lead to unsatisfactory results. We propose a domain knowledge guided Bayesian Optimization approach, which leverages physical insight to fundamentally simplify the search problem by transforming coordinates to decouple input features and align the active subspaces with the primary search axes. We demonstrate this approach&#x27;s efficacy on a challenging 12-dimensional, 6-crystal Split-and-Delay optical system, where conventional approaches, including standard BO, TuRBO and multi-objective BO, consistently led to unsatisfactory results. When combined with an reverse annealing exploration strategy, this approach reliably converges to the global optimum. The coordinate transformation itself is the key to this success, significantly accelerating the search by aligning input co-ordinate axes with the problem&#x27;s active subspaces. As increasingly complex scientific instruments, from large telescopes to new spectrometers at X-ray Free Electron Lasers are deployed, the demand for robust high-dimensional optimization grows. Our results demonstrate a generalizable paradigm: leveraging physical insight to transform high-dimensional, coupled optimization problems into simpler representations can enable rapid and robust automated tuning for consistent high performance while still retaining current optimization algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>领域知识引导的贝叶斯优化在复杂科学仪器自主对准中的应用</div>
<div class="mono" style="margin-top:8px">贝叶斯优化是优化复杂非线性系统的强大工具，但在参数紧密耦合、目标函数高度不对称且回报稀疏的高维问题中，其性能会下降。在这类“大海捞针”场景中，即使如信任域贝叶斯优化等先进方法也常导致不理想结果。我们提出一种领域知识引导的贝叶斯优化方法，通过物理洞察对坐标进行变换，以解耦输入特征并使活跃子空间与主搜索轴对齐，从而从根本上简化搜索问题。我们在具有挑战性的12维六晶体分束延时光学系统上验证了该方法的有效性——传统方法（包括标准贝叶斯优化、信任域贝叶斯优化及多目标贝叶斯优化）均无法取得满意结果。结合反向退火探索策略后，该方法能稳定收敛至全局最优解。坐标变换本身是成功的关键，通过将输入坐标轴与问题活跃子空间对齐，显著加速了搜索过程。随着从大型望远镜到X射线自由电子激光器新型光谱仪等日益复杂的科学仪器的部署，对鲁棒性高维优化的需求日益增长。我们的研究展示了一种可推广的范式：利用物理洞察将高维耦合优化问题转化为更简洁的表征形式，能够在保留现有优化算法的同时，实现快速鲁棒的自动调参，从而持续获得高性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of Bayesian Optimization (BO) in high-dimensional, complex optimization problems with sparse rewards, such as aligning intricate scientific instruments. The method introduces a domain knowledge guided BO approach that uses physical insight to transform coordinates, decoupling input features and aligning active subspaces with search axes, combined with a reverse annealing exploration strategy. Experimental results on a challenging 12-dimensional optical system show that this approach reliably converges to the global optimum, outperforming conventional methods like standard BO and TuRBO, and demonstrates a generalizable paradigm for simplifying high-dimensional optimization.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决贝叶斯优化在高维、复杂且奖励稀疏的优化问题中的局限性，例如复杂科学仪器的自动对准。方法提出了一种领域知识引导的贝叶斯优化方法，利用物理洞察进行坐标变换，以解耦输入特征并将活动子空间与搜索轴对齐，并结合反向退火探索策略。在具有挑战性的12维光学系统上的实验结果表明，该方法能可靠地收敛到全局最优解，优于标准贝叶斯优化和TuRBO等传统方法，展示了一种简化高维优化问题的通用范式。</div>
</details>
</div>
<div class="card">
<div class="title">Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Yingxiao Huo, Satya Prakash Dash, Radu Stoican, Samuel Kaski, Mingfei Sun</div>
<div class="meta-line">First: 2026-01-26T16:02:18+00:00 · Latest: 2026-02-11T08:34:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18626v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.18626v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习中自然策略梯度的逆费舍尔矩阵秩-1近似方法</div>
<div class="mono" style="margin-top:8px">自然梯度因其快速收敛特性和协变权重更新特性，在深度强化学习中长期受到研究。然而，计算自然梯度需要在每次迭代中求逆费舍尔信息矩阵（FIM），这在计算上具有天然的高昂代价。本文提出一种高效且可扩展的自然策略优化技术，利用秩-1近似来替代完整的逆FIM计算。我们从理论上证明，在特定条件下，逆FIM的秩-1近似比策略梯度收敛更快，并且在某些条件下具有与随机策略梯度方法相同的样本复杂度。我们在多种环境中对本方法进行基准测试，结果表明其性能优于标准的演员-评论家方法和信赖域基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational challenge of inverting the Fisher Information Matrix (FIM) in natural policy gradient methods for deep reinforcement learning. The proposed method employs a rank-1 approximation to the inverse FIM to create a scalable natural policy optimization technique. Experimental results across diverse environments demonstrate that this approach outperforms standard actor-critic and trust-region baselines in performance.</div>
<div class="mono" style="margin-top:8px">本文的动机源于深度强化学习中自然策略梯度方法需要计算费舍尔信息矩阵逆矩阵所带来的巨大计算挑战。该方法采用对逆费舍尔信息矩阵的秩-1近似，提出了一种高效可扩展的自然策略优化技术。在多种环境下的实验结果表明，该方法在性能上优于标准的演员-评论员和信任域基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling</div>
<div class="meta-line">Authors: Zhibin Duan, Guowei Rong, Zhuo Li, Bo Chen, Mingyuan Zhou, Dandan Guo</div>
<div class="meta-line">First: 2026-02-11T08:14:11+00:00 · Latest: 2026-02-11T08:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10623v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10623v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models learned from human preferences are central to aligning large language models (LLMs) via reinforcement learning from human feedback, yet they are often vulnerable to reward hacking due to noisy annotations and systematic biases such as response length or style. We propose Bayesian Non-Negative Reward Model (BNRM), a principled reward modeling framework that integrates non-negative factor analysis into Bradley-Terry (BT) preference model. BNRM represents rewards through a sparse, non-negative latent factor generative process that operates at two complementary levels: instance-specific latent variables induce disentangled reward representations, while sparsity over global latent factors acts as an implicit debiasing mechanism that suppresses spurious correlations. Together, this disentanglement-then-debiasing structure enables robust uncertainty-aware reward learning. To scale BNRM to modern LLMs, we develop an amortized variational inference network conditioned on deep model representations, allowing efficient end-to-end training. Extensive empirical results demonstrate that BNRM substantially mitigates reward over-optimization, improves robustness under distribution shifts, and yields more interpretable reward decompositions than strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过贝叶斯非负奖励建模缓解RLHF中的奖励黑客问题</div>
<div class="mono" style="margin-top:8px">从人类偏好学习的奖励模型是通过人类反馈强化学习对齐大语言模型的核心，但由于标注噪声和系统偏差（如响应长度或风格），它们常易受奖励黑客攻击。我们提出贝叶斯非负奖励模型（BNRM），这是一个将非负因子分析整合到Bradley-Terry偏好模型中的原则性奖励建模框架。BNRM通过稀疏、非负的潜在因子生成过程表示奖励，该过程在两个互补层面运作：实例特定的潜在变量诱导解耦的奖励表示，而全局潜在因子的稀疏性作为隐式去偏机制抑制虚假相关性。这种先解耦后去偏的结构共同实现了稳健的、具有不确定性感知的奖励学习。为将BNRM扩展到现代大语言模型，我们开发了基于深度模型表征的摊销变分推断网络，支持高效的端到端训练。大量实证结果表明，相较于强基线模型，BNRM显著缓解了奖励过优化问题，提升了分布偏移下的鲁棒性，并产生了更具可解释性的奖励分解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward hacking in reinforcement learning from human feedback (RLHF), where reward models can exploit spurious correlations like response length due to noisy or biased preference annotations. The authors propose the Bayesian Non-Negative Reward Model (BNRM), a framework that integrates non-negative factor analysis into the Bradley-Terry preference model, using a sparse, non-negative latent factor generative process to disentangle reward representations at the instance level and debias at the global factor level, thereby enabling robust, uncertainty-aware reward learning. Experiments show that BNRM effectively mitigates reward over-optimization, enhances robustness under distribution shifts, and produces more interpretable reward decompositions compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习中的奖励黑客问题，即奖励模型可能因噪声或带偏见的偏好标注而利用如回复长度等虚假相关性。作者提出了贝叶斯非负奖励模型，该框架将非负因子分析整合到Bradley-Terry偏好模型中，通过稀疏、非负的潜在因子生成过程，在实例层面解耦奖励表示并在全局因子层面去偏，从而实现鲁棒且考虑不确定性的奖励学习。实验结果表明，与基线方法相比，该模型显著缓解了奖励过优化问题，提升了分布变化下的鲁棒性，并产生了更可解释的奖励分解。</div>
</details>
</div>
<div class="card">
<div class="title">The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward</div>
<div class="meta-line">Authors: Long Li, Zhijian Zhou, Jiaran Hao, Jason Klein Liu, Yanting Miao, Wei Pang, Xiaoyu Tan, Wei Chu, Zhe Wang, Shirui Pan, Chao Qu, Yuan Qi</div>
<div class="meta-line">First: 2025-09-09T06:34:32+00:00 · Latest: 2026-02-11T08:10:26+00:00</div>
<div class="meta-line">Comments: 25 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.07430v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.07430v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>散度选择：强化学习可验证奖励中缓解多样性崩溃的被忽视关键</div>
<div class="mono" style="margin-top:8px">在使用可验证奖励的强化学习（RLVR）微调大型语言模型（LLMs）时，一个核心悖论是：尽管单次尝试准确率（Pass@1）有所提升，但多次尝试性能（Pass@k）却经常下降，并常伴随灾难性遗忘——模型丧失先前习得的技能。尽管已有多种方法被提出，但散度项的选择与功能作为一种主动解决方案却惊人地未被审视。我们认为，标准的RLVR目标——无论是使用寻求模式的反向KL散度还是完全舍弃散度项的方法——都缺乏关键的知识保留机制。反向KL散度通过收窄策略主动加速了这种知识衰减，而舍弃散度项则无法防止模型偏离其多样化的知识基础。我们提出一个根本性的视角转变：将散度项本身作为解决方案。我们的框架——多样性保持混合强化学习（DPH-RL）——利用质量覆盖的f-散度（如正向KL散度和JS散度）作为复现机制。通过持续参考初始策略，该方法迫使模型保持广泛的解决方案覆盖范围。在数学和SQL生成任务上的大量实验表明，DPH-RL不仅解决了Pass@k性能下降问题，还同时提升了领域内外的Pass@1和Pass@k性能。此外，DPH-RL具有更高的训练效率，因为它使用生成函数计算f-散度，仅需从初始策略采样，无需在线参考模型。我们的工作揭示了改进RLVR的一个关键但被忽视的维度，证明正确选择散度度量是构建更通用、更多样化推理模型的强大工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the paradox in fine-tuning Large Language Models with Reinforcement Learning with Verifiable Reward (RLVR), where improvements in single-attempt accuracy (Pass@1) often lead to degraded multi-attempt performance (Pass@k) and catastrophic forgetting of diverse skills. The authors identify a neglected key: the choice of divergence term in the RL objective, arguing that standard approaches using reverse KL-divergence or no divergence accelerate knowledge decay. They propose a new framework, Diversity-Preserving Hybrid RL (DPH-RL), which uses mass-covering f-divergences like forward-KL as a rehearsal mechanism to continuously reference the initial policy, thereby preserving broad solution coverage. Experimental results on math and SQL generation tasks show that DPH-RL effectively mitigates Pass@k degradation, improves both Pass@1 and Pass@k in- and out-of-domain, and offers greater training efficiency by computing divergence via generator functions without needing an online reference model.</div>
<div class="mono" style="margin-top:8px">本文针对使用可验证奖励的强化学习微调大语言模型时的一个核心矛盾：单次尝试准确率提升常导致多次尝试性能下降及灾难性遗忘。作者指出一个被忽视的关键因素是强化学习目标中散度项的选择，认为标准方法使用的反向KL散度或无散度项会加速知识衰减。他们提出了一个新框架——多样性保持混合强化学习，利用前向KL等质量覆盖f-散度作为复习机制，持续参考初始策略以保持广泛的解覆盖。在数学和SQL生成任务上的实验表明，该框架有效缓解了多次尝试性能下降问题，同时提升了域内外的单次和多次尝试准确率，且通过生成函数计算散度提高了训练效率，无需在线参考模型。</div>
</details>
</div>
<div class="card">
<div class="title">Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model</div>
<div class="meta-line">Authors: Yisen Gao, Jiaxin Bai, Yi Huang, Xingcheng Fu, Qingyun Sun, Yangqiu Song</div>
<div class="meta-line">Venue: The Web Conference 2026</div>
<div class="meta-line">First: 2025-10-13T14:34:57+00:00 · Latest: 2026-02-11T08:10:05+00:00</div>
<div class="meta-line">Comments: Accepted by The Web Conference 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11462v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11462v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deductive and abductive reasoning are two critical paradigms for analyzing knowledge graphs, enabling applications from financial query answering to scientific discovery. Deductive reasoning on knowledge graphs usually involves retrieving entities that satisfy a complex logical query, while abductive reasoning generates plausible logical hypotheses from observations. Despite their clear synergistic potential, where deduction can validate hypotheses and abduction can uncover deeper logical patterns, existing methods address them in isolation. To bridge this gap, we propose DARK, a unified framework for Deductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion model capable of capturing the bidirectional relationship between queries and conclusions, DARK has two key innovations. First, to better leverage deduction for hypothesis refinement during abductive reasoning, we introduce a self-reflective denoising process that iteratively generates and validates candidate hypotheses against the observed conclusion. Second, to discover richer logical associations, we propose a logic-exploration reinforcement learning approach that simultaneously masks queries and conclusions, enabling the model to explore novel reasoning compositions. Extensive experiments on multiple benchmark knowledge graphs show that DARK achieves state-of-the-art performance on both deductive and abductive reasoning tasks, demonstrating the significant benefits of our unified approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于掩码扩散模型统一知识图谱中的演绎与溯因推理</div>
<div class="mono" style="margin-top:8px">演绎推理与溯因推理是分析知识图谱的两大关键范式，支撑着从金融查询应答到科学发现等多种应用。知识图谱的演绎推理通常涉及检索满足复杂逻辑查询的实体，而溯因推理则从观察中生成合理的逻辑假设。尽管二者具有明确的协同潜力——演绎可验证假设，溯因能揭示更深层的逻辑模式——现有方法仍孤立处理这两类任务。为弥合这一鸿沟，我们提出DARK框架，用于知识图谱中演绎与溯因推理的统一建模。作为一种能捕捉查询与结论双向关系的掩码扩散模型，DARK具备两项关键创新：首先，为在溯因推理中更好地利用演绎进行假设优化，我们引入自反思去噪过程，可针对观测结论迭代生成并验证候选假设；其次，为发现更丰富的逻辑关联，我们提出逻辑探索强化学习方法，通过同时掩码查询与结论，使模型能够探索新颖的推理组合。在多个基准知识图谱上的大量实验表明，DARK在演绎与溯因推理任务上均达到最先进性能，验证了统一方法的显著优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to unify deductive and abductive reasoning in knowledge graphs, as existing methods treat these synergistic paradigms in isolation. The authors propose DARK, a masked diffusion model framework that introduces a self-reflective denoising process for iterative hypothesis refinement during abduction and a logic-exploration reinforcement learning method to mask both queries and conclusions for discovering novel logical associations. Experimental results on multiple benchmark knowledge graphs demonstrate that DARK achieves state-of-the-art performance on both reasoning tasks, validating the benefits of the unified approach.</div>
<div class="mono" style="margin-top:8px">本文的动机在于统一知识图谱中的演绎推理和溯因推理，因为现有方法将这两种协同范式孤立处理。作者提出了DARK，一个掩码扩散模型框架，引入了自反思去噪过程用于在溯因推理中迭代优化假设，并提出一种逻辑探索强化学习方法，通过同时掩码查询和结论来发现新颖的逻辑组合。在多个基准知识图谱上的实验结果表明，DARK在两项推理任务上均取得了最先进的性能，验证了统一方法的显著优势。</div>
</details>
</div>
<div class="card">
<div class="title">Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</div>
<div class="meta-line">Authors: Linghao Zhu, Yiran Guan, Dingkang Liang, Jianzhong Ju, Zhenbo Luo, Bin Qin, Jian Luan, Yuliang Liu, Xiang Bai</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-08-07T17:53:47+00:00 · Latest: 2026-02-11T08:08:38+00:00</div>
<div class="meta-line">Comments: This paper has been accepted to ICLR 2026 Project page at: https://xenozlh.github.io/Shuffle-R1/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05612v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.05612v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xenozlh.github.io/Shuffle-R1/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Shuffle-R1：基于数据动态重排的多模态大语言模型高效强化学习框架</div>
<div class="mono" style="margin-top:8px">强化学习已成为提升多模态大语言模型推理能力的有效后训练范式。然而，当前强化学习流程常受两个未充分探讨问题导致的训练低效困扰：优势坍缩（批次中多数优势值趋近于零）与轨迹沉寂（贡献非零梯度的轨迹比例随时间递减）。这些问题导致梯度更新次优并阻碍长期学习效率。为解决这些问题，我们提出Shuffle-R1——一个通过动态重组轨迹采样与批次构建来提升强化学习微调效率的简洁而原理清晰的框架。该框架引入：（1）配对轨迹采样：选取具有显著优势差异的高对比度轨迹以提升梯度信号质量；（2）基于优势的轨迹重排：通过智能批次重组增加高价值轨迹的曝光度。在多个推理基准测试上的实验表明，本框架能以极低开销持续超越现有强化学习基线方法。这些结果凸显了以数据为中心的适应性策略对提升多模态大语言模型强化学习训练效率的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses training inefficiencies in reinforcement learning (RL) for multimodal large language models (MLLMs), specifically the issues of Advantage Collapsing and Rollout Silencing, which lead to suboptimal gradient updates. To improve RL fine-tuning efficiency, the authors propose Shuffle-R1, a data-centric framework that dynamically restructures trajectory sampling and batch composition through Pairwise Trajectory Sampling to select high-contrast trajectories and Advantage-based Trajectory Shuffle to increase exposure of valuable rollouts. Experimental results across multiple reasoning benchmarks demonstrate that Shuffle-R1 consistently outperforms strong RL baselines with minimal overhead, highlighting the effectiveness of data-centric adaptations for more efficient RL training in MLLMs.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLM）强化学习（RL）训练中的效率低下问题，特别是优势坍缩和轨迹静默这两个导致梯度更新次优的未充分探索问题。为提高RL微调效率，作者提出了Shuffle-R1，这是一个以数据为中心的框架，通过成对轨迹采样选择高对比度轨迹，以及基于优势的轨迹重排来增加有价值轨迹的曝光，从而动态重组轨迹采样和批次组成。在多个推理基准测试上的实验结果表明，Shuffle-R1以最小开销持续优于强RL基线，凸显了以数据为中心的调整对于提升MLLM中RL训练效率的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Online Causal Kalman Filtering for Stable and Effective Policy Optimization</div>
<div class="meta-line">Authors: Shuo He, Lang Feng, Xin Cheng, Lei Feng, Bo An</div>
<div class="meta-line">First: 2026-02-11T07:57:43+00:00 · Latest: 2026-02-11T07:57:43+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10609v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token&#x27;s IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线因果卡尔曼滤波实现稳定高效策略优化</div>
<div class="mono" style="margin-top:8px">大型语言模型的强化学习面临高方差词元级重要性采样（IS）比率问题，这会导致大规模策略优化失稳。为提升稳定性，现有方法通常对序列中所有词元使用固定的序列级IS比率，或单独调整各词元的IS比率，从而忽略了序列中词元间的时序离策略推导。本文首先通过实证发现，局部离策略偏差在词元层面存在结构性不一致，可能扭曲相邻词元间的策略梯度更新并导致训练崩溃。为解决该问题，我们提出用于稳定高效策略优化的在线因果卡尔曼滤波方法（KPO）。具体而言，我们将目标IS比率建模为跨词元演化的潜在状态，并应用卡尔曼滤波器基于历史词元状态在线自回归地更新该状态，无需依赖未来词元。经滤波处理的IS比率在保留词元级局部结构感知变化的同时，能显著平滑噪声尖峰，从而产生更稳定高效的策略更新。实验表明，在具有挑战性的数学推理数据集上，KPO相比现有最优方法取得了更优异的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the instability in reinforcement learning for large language models caused by high-variance token-level importance sampling ratios, which can lead to training collapse. The method proposed, Online Causal Kalman Filtering for Policy Optimization (KPO), models the importance sampling ratio as a latent state evolving across tokens and applies a Kalman filter to update it online and autoregressively, smoothing noise while preserving local token-wise structure. Experimental results show that KPO achieves superior performance on challenging math reasoning datasets compared to state-of-the-art methods, demonstrating more stable and effective policy optimization.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型强化学习中因令牌级重要性采样比率高方差导致的不稳定性问题，这可能引发训练崩溃。提出的方法是在线因果卡尔曼滤波策略优化（KPO），它将重要性采样比率建模为跨令牌演化的潜在状态，并应用卡尔曼滤波器在线自回归地更新该状态，以平滑噪声同时保留局部令牌结构。实验结果表明，在具有挑战性的数学推理数据集上，KPO相比最先进方法取得了更优的性能，实现了更稳定有效的策略优化。</div>
</details>
</div>
<div class="card">
<div class="title">Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters</div>
<div class="meta-line">Authors: Ailin Huang, Ang Li, Aobo Kong, Bin Wang, Binxing Jiao, Bo Dong, Bojun Wang, Boyu Chen, Brian Li, Buyun Ma, Chang Su, Changxin Miao, Changyi Wan, Chao Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengting Feng, Chengyuan Yao, Chunrui Han, Dan Ma, Dapeng Shi, Daxin Jiang, Dehua Ma, Deshan Sun, Di Qi, Enle Liu, Fajie Zhang, Fanqi Wan, Guanzhe Huang, Gulin Yan, Guoliang Cao, Guopeng Li, Han Cheng, Hangyu Guo, Hanshan Zhang, Hao Nie, Haonan Jia, Haoran Lv, Hebin Zhou, Hekun Lv, Heng Wang, Heung-Yeung Shum, Hongbo Huang, Hongbo Peng, Hongyu Zhou, Hongyuan Wang, Houyong Chen, Huangxi Zhu, Huimin Wu, Huiyong Guo, Jia Wang, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiashu Lv, Jiashuo Liu, Jiayi Fu, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yang, Jie Zhou, Jieyi Hou, Jing Bai, Jingcheng Hu, Jingjing Xie, Jingwei Wu, Jingyang Zhang, Jishi Zhou, Junfeng Liu, Junzhe Lin, Ka Man Lo, Kai Liang, Kaibo Liu, Kaijun Tan, Kaiwen Yan, Kaixiang Li, Kang An, Kangheng Lin, Lei Yang, Liang Lv, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lina Chen, Luck Ma, Mengqiang Ren, Michael Li, Ming Li, Mingliang Li, Mingming Zhang, Mingrui Chen, Mitt Huang, Na Wang, Peng Liu, Qi Han, Qian Zhao, Qinglin He, Qinxin Du, Qiuping Wu, Quan Sun, Rongqiu Yang, Ruihang Miao, Ruixin Han, Ruosi Wan, Ruyan Guo, Shan Wang, Shaoliang Pang, Shaowen Yang, Shengjie Fan, Shijie Shang, Shiliang Yang, Shiwei Li, Shuangshuang Tian, Siqi Liu, Siye Wu, Siyu Chen, Song Yuan, Tiancheng Cao, Tianchi Yue, Tianhao Cheng, Tianning Li, Tingdan Luo, Wang You, Wei Ji, Wei Yuan, Wei Zhang, Weibo Wu, Weihao Xie, Wen Sun, Wenjin Deng, Wenzhen Zheng, Wuxun Xie, Xiangfeng Wang, Xiangwen Kong, Xiangyu Liu, Xiangyu Zhang, Xiaobo Yang, Xiaojia Liu, Xiaolan Yuan, Xiaoran Jiao, Xiaoxiao Ren, Xiaoyun Zhang, Xin Li, Xin Liu, Xin Wu, Xing Chen, Xingping Yang, Xinran Wang, Xu Zhao, Xuan He, Xuanti Feng, Xuedan Cai, Xuqiang Zhou, Yanbo Yu, Yang Li, Yang Xu, Yanlin Lai, Yanming Xu, Yaoyu Wang, Yeqing Shen, Yibo Zhu, Yichen Lv, Yicheng Cao, Yifeng Gong, Yijing Yang, Yikun Yang, Yin Zhao, Yingxiu Zhao, Yinmin Zhang, Yitong Zhang, Yixuan Zhang, Yiyang Chen, Yongchi Zhao, Yongshen Long, Yongyao Wang, Yousong Guan, Yu Zhou, Yuang Peng, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yudi Zhao, Yue Peng, Yueqiang Lin, Yufan Lu, Yuling Zhao, Yunzhou Ju, Yurong Zhang, Yusheng Li, Yuxiang Yang, Yuyang Chen, Yuzhu Cai, Zejia Weng, Zetao Hong, Zexi Li, Zhe Xie, Zheng Ge, Zheng Gong, Zheng Zeng, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhiheng Hu, Zidong Yang, Zili Wang, Ziqi Ren, Zixin Zhang, Zixuan Wang</div>
<div class="meta-line">First: 2026-02-11T07:53:51+00:00 · Latest: 2026-02-11T07:53:51+00:00</div>
<div class="meta-line">Comments: Technical report for Step 3.5 Flash</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10604v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10604v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Step 3.5 Flash：以110亿活跃参数开启前沿级智能</div>
<div class="mono" style="margin-top:8px">我们推出Step 3.5 Flash，这是一个稀疏专家混合模型，旨在连接前沿级智能体智能与计算效率。我们聚焦于构建智能体最关键的要素：敏锐的推理能力与快速可靠的执行能力。该模型以1960亿参数为基础，通过110亿活跃参数实现高效推理，并采用3:1交错滑动窗口/全局注意力机制与多令牌预测技术，以降低多轮智能体交互的延迟与成本。为达到前沿智能水平，我们设计了可扩展的强化学习框架，将可验证信号与偏好反馈相结合，同时确保在大规模离线训练中保持稳定，实现在数学、代码和工具使用领域的持续自我改进。Step 3.5 Flash在智能体、编程和数学任务中表现优异：IMO-AnswerBench达85.4%、LiveCodeBench-v6（2024.08-2025.05）达86.4%、tau2-Bench达88.2%、BrowseComp（含上下文管理）达69.0%、Terminal-Bench 2.0达51.0%，性能媲美GPT-5.2 xHigh与Gemini 3.0 Pro等前沿模型。通过重新定义效率边界，Step 3.5 Flash为在真实工业环境中部署复杂智能体提供了高密度基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Step 3.5 Flash, a sparse Mixture-of-Experts model motivated by the need to achieve frontier-level agentic intelligence—characterized by sharp reasoning and fast, reliable execution—while maintaining computational efficiency. The method employs a 196B-parameter foundation with only 11B active parameters for inference, optimized through interleaved sliding-window/full attention and Multi-Token Prediction to reduce latency in multi-round interactions, and uses a scalable reinforcement learning framework that combines verifiable signals with preference feedback for stable, self-improving training. Experimental results show strong performance across agent, coding, and math benchmarks, including 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6, and 88.2% on tau2-Bench, achieving parity with frontier models like GPT-5.2 xHigh and Gemini 3.0 Pro.</div>
<div class="mono" style="margin-top:8px">本文介绍了Step 3.5 Flash，一个稀疏专家混合模型，其动机是在实现前沿水平的智能体智能（以敏锐推理和快速可靠执行为特征）的同时保持计算效率。该方法采用了一个1960亿参数的基础模型，但推理时仅激活110亿参数，并通过交错滑动窗口/全注意力机制与多令牌预测进行优化以降低多轮交互延迟，同时利用一个结合可验证信号与偏好反馈的可扩展强化学习框架进行稳定、自我改进的训练。实验结果表明，该模型在智能体、编码和数学任务上表现强劲，如在IMO-AnswerBench上达到85.4%，在LiveCodeBench-v6上达到86.4%，在tau2-Bench上达到88.2%，性能与GPT-5.2 xHigh和Gemini 3.0 Pro等前沿模型相当。</div>
</details>
</div>
<div class="card">
<div class="title">Neuro-symbolic Action Masking for Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Shuai Han, Mehdi Dastani, Shihan Wang</div>
<div class="meta-line">First: 2026-02-11T07:42:53+00:00 · Latest: 2026-02-11T07:42:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10598v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10598v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经符号动作掩码在深度强化学习中的应用</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）在训练和执行过程中可能探索不可行的动作。现有方法假设存在一个符号接地函数，将高维状态映射到一致的符号表示，并采用手动指定的动作掩码技术来约束动作。本文提出神经符号动作掩码（NSAM），这是一种新颖的框架，能够在DRL过程中以最小监督的方式自动学习符号模型，这些模型与高维状态的给定领域约束保持一致。基于学习到的状态符号模型，NSAM学习动作掩码以排除不可行动作。NSAM实现了符号推理与深度策略优化的端到端集成，其中符号接地和策略学习的改进相互促进。我们在多个约束领域评估了NSAM，实验结果表明，NSAM显著提高了DRL智能体的样本效率，同时大幅减少了约束违反。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the problem that deep reinforcement learning agents often explore infeasible actions, which existing methods address by relying on manually specified symbolic models and action masks. To overcome this limitation, the authors propose Neuro-symbolic Action Masking (NSAM), a framework that automatically learns symbolic models consistent with domain constraints from high-dimensional states in a minimally supervised way during training, and uses these models to generate action masks that eliminate infeasible actions. Experimental results across multiple constrained domains show that NSAM significantly enhances the sample efficiency of DRL agents while drastically reducing constraint violations.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决深度强化学习智能体在训练和执行中常探索不可行动作的问题，现有方法依赖于手动指定的符号模型和动作掩码。为此，作者提出了神经符号动作掩码（NSAM）框架，该框架能在训练过程中以最小监督方式自动从高维状态中学习符合领域约束的符号模型，并利用这些模型生成排除不可行动作的动作掩码。在多个带约束的领域上的实验结果表明，NSAM显著提高了深度强化学习智能体的样本效率，同时大幅减少了约束违反。</div>
</details>
</div>
<div class="card">
<div class="title">Fake-HR1: Rethinking Reasoning of Vision Language Model for Synthetic Image Detection</div>
<div class="meta-line">Authors: Changjiang Jiang, Xinkuan Sha, Fengchang Yu, Jingjing Liu, Jian Liu, Mingqi Fang, Chenfeng Zhang, Wei Lu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-10T18:10:08+00:00 · Latest: 2026-02-11T07:32:53+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10042v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10042v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model&#x27;s ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fake-HR1：面向合成图像检测的视觉语言模型推理机制重构</div>
<div class="mono" style="margin-top:8px">近期研究表明，在检测过程中引入思维链推理可提升模型识别合成图像的能力。然而，过长的推理过程会带来显著的资源开销（包括令牌消耗与延迟），在处理明显伪造图像时尤为冗余。为此，我们提出Fake-HR1——据我们所知，这是首个能根据生成式检测任务特性自适应判断是否需要推理的大规模混合推理模型。为实现该目标，我们设计了两阶段训练框架：首先通过混合微调进行冷启动初始化，随后采用混合推理分组策略优化的在线强化学习，隐式学习何时选择适宜的推理模式。实验表明，Fake-HR1能针对不同类型查询自适应执行推理，在推理能力与生成式检测性能上均超越现有大语言模型，同时显著提升响应效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve the efficiency of synthetic image detection by vision-language models, as existing methods using Chain-of-Thought reasoning incur high resource costs, especially for obvious forgeries. The proposed method, Fake-HR1, introduces a hybrid-reasoning approach that adaptively decides when reasoning is needed, trained via a two-stage framework combining Hybrid Fine-Tuning and online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization. Experimental results demonstrate that Fake-HR1 outperforms existing large language models in both reasoning capability and detection performance while significantly enhancing response efficiency across diverse query types.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升视觉语言模型在合成图像检测中的效率，因为现有基于思维链推理的方法资源开销大，尤其对明显伪造图像冗余。提出的Fake-HR1方法采用混合推理机制，能根据生成检测任务特性自适应决定是否进行推理，通过混合微调初始化和在线强化学习的两阶段训练框架实现。实验结果表明，Fake-HR1在不同查询类型中自适应执行推理，在推理能力和生成检测性能上均超越现有大语言模型，同时显著提高了响应效率。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization</div>
<div class="meta-line">Authors: Boxiao Wang, Kai Li, Tianyi Liu, Chen Li, Junzhe Wang, Yifan Zhang, Jian Cheng</div>
<div class="meta-line">First: 2026-02-11T07:02:23+00:00 · Latest: 2026-02-11T07:02:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10576v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10576v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model&#x27;s internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的科学方程发现：物理信息令牌正则化策略优化</div>
<div class="mono" style="margin-top:8px">符号回归旨在从观测数据中提炼数学方程。近期研究成功利用大语言模型（LLMs）生成方程假设，充分发挥其预训练科学先验知识。然而，现有框架主要将LLM视为静态生成器，依赖提示级引导进行探索。这种范式无法根据搜索反馈更新模型内部表示，常产生物理不一致或数学冗余的表达式。本文提出PiT-PO（物理信息令牌正则化策略优化），这是一个通过强化学习将LLM演化为自适应生成器的统一框架。PiT-PO的核心是双重约束机制：严格保障层次化物理有效性，同时施加细粒度令牌级惩罚以抑制冗余结构。因此，PiT-PO使LLM生成的方程兼具科学一致性与结构简洁性。实验表明，PiT-PO在标准基准测试中达到最先进性能，并成功为复杂流体动力学问题发现新型湍流模型。我们还证明PiT-PO能使小规模模型超越闭源大型模型，推动高性能科学发现的普惠化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing LLM-based symbolic regression methods, which treat the model as a static generator and often produce physically inconsistent or redundant equations, this paper introduces PiT-PO, a reinforcement learning framework that adaptively updates the LLM. The method employs a dual-constraint mechanism to enforce hierarchical physical validity and impose token-level penalties for structural parsimony. Experimental results show that PiT-PO achieves state-of-the-art performance on benchmarks, discovers novel turbulence models in fluid dynamics, and enables smaller models to outperform larger closed-source counterparts.</div>
<div class="mono" style="margin-top:8px">针对现有基于大语言模型的符号回归方法将模型视为静态生成器、常产生物理不一致或冗余方程的问题，本文提出了PiT-PO，一个通过强化学习自适应更新大语言模型的框架。该方法采用双重约束机制，在强制层次化物理有效性的同时施加细粒度词元级惩罚以确保结构简洁性。实验结果表明，PiT-PO在标准基准测试中取得了最先进的性能，成功发现了流体动力学中具有挑战性的湍流新模型，并使小规模模型能够超越大型闭源模型。</div>
</details>
</div>
<div class="card">
<div class="title">MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning</div>
<div class="meta-line">Authors: Chenhao Zhang, Yazhe Niu, Hongsheng Li</div>
<div class="meta-line">First: 2026-02-11T06:59:36+00:00 · Latest: 2026-02-11T06:59:36+00:00</div>
<div class="meta-line">Comments: 14 pages, 4 figures, 11 tables; Code: https://github.com/MING-ZCH/MetaphorStar, Model &amp; Dataset: https://huggingface.co/collections/MING-ZCH/metaphorstar</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10575v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10575v1">PDF</a> · <a href="https://github.com/MING-ZCH/MetaphorStar">Code1</a> · <a href="https://huggingface.co/collections/MING-ZCH/metaphorstar">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a> · <a href="https://metaphorstar.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task&#x27;s demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.
  Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MetaphorStar：基于端到端视觉强化学习的图像隐喻理解与推理</div>
<div class="mono" style="margin-top:8px">图像隐喻理解是当前人工智能系统面临的关键挑战。尽管多模态大语言模型在基础视觉问答任务中表现出色，却始终难以把握视觉内容中蕴含的微妙文化、情感与语境内涵。这一困难源于任务对复杂多跳推理、文化背景与心理理论能力的高要求，而现有模型尚不具备这些能力。为填补这一空白，我们提出了首个面向图像内涵任务的端到端视觉强化学习框架MetaphorStar。该框架包含三个核心组件：细粒度数据集TFQ-Data、视觉强化学习方法TFQ-GRPO以及结构化基准测试TFQ-Bench。
通过TFQ-GRPO方法在TFQ-Data上训练的全开源MetaphorStar系列模型，在图像内涵基准测试中平均性能提升82.6%。与20余个主流多模态大语言模型相比，MetaphorStar-32B在选择题与开放式问题上达到最先进水平，在判断题上显著超越顶级闭源模型Gemini-3.0-pro。关键的是，实验表明学习图像内涵任务能提升模型的通用理解能力，特别是复杂视觉推理能力。我们进一步系统分析了模型参数规模、训练数据规模以及不同模型架构与训练策略的影响，证明了方法的广泛适用性。所有模型权重、数据集与方法代码均已开源：https://metaphorstar.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inability of current Multimodal Large Language Models (MLLMs) to grasp the nuanced cultural, emotional, and contextual implications in images, which requires sophisticated multi-hop reasoning and Theory of Mind, this paper proposes MetaphorStar, an end-to-end visual reinforcement learning framework. The method introduces three core components: the TFQ-Data dataset, the TFQ-GRPO visual RL training method, and the TFQ-Bench benchmark. The main experimental results show that the fully open-source MetaphorStar models, trained with this framework, achieve an average performance improvement of 82.6% on image implication benchmarks, outperform over 20 mainstream MLLMs, and surpass the top closed-source model Gemini-3.0-pro on specific tasks, while also demonstrating enhanced general visual reasoning ability.</div>
<div class="mono" style="margin-top:8px">本文的动机源于当前多模态大语言模型难以理解图像中蕴含的微妙文化、情感和语境隐喻，这需要复杂的多步推理和心理理论能力。为此，论文提出了MetaphorStar，一个端到端的视觉强化学习框架，其方法核心包括TFQ-Data数据集、TFQ-GRPO视觉RL训练方法和TFQ-Bench基准。主要实验结果表明，基于此框架训练的全开源MetaphorStar模型在图像隐喻理解基准上的平均性能提升了82.6%，超越了20多个主流多模态大模型，并在特定任务上超过了顶尖闭源模型Gemini-3.0-pro，同时证明该方法能有效提升模型的通用视觉推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization</div>
<div class="meta-line">Authors: Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</div>
<div class="meta-line">First: 2025-10-09T17:58:07+00:00 · Latest: 2026-02-11T06:01:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08554v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.08554v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce Group Diffusion Policy Optimization (GDPO), a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过群组扩散策略优化提升扩散语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）通过迭代优化实现并行、顺序无关的文本生成，为自回归大语言模型（LLMs）提供了灵活的替代方案。然而，由于似然函数难以处理，将强化学习（RL）微调应用于DLMs仍是一个开放挑战。开创性工作如diffu-GRPO通过单步解掩码估计词元级似然，虽计算高效但存在严重偏差。更理论完备的基础在于序列级似然，其中证据下界（ELBO）可作为替代目标。尽管存在清晰的数学关联，但由于似然评估成本过高，基于ELBO的方法应用有限。本研究重新审视ELBO估计，解析其方差来源。该分解启发了通过沿关键维度进行快速确定性积分近似来降低方差。基于此，我们提出专为DLMs设计的强化学习算法——群组扩散策略优化（GDPO）。GDPO采用简洁高效的半确定性蒙特卡洛方案，缓解原始双蒙特卡洛采样下ELBO估计器的方差爆炸问题，在严格评估预算下构建了可证明更低方差的估计器。实验表明，GDPO在多数数学、推理和代码基准测试中，相比预训练检查点持续提升性能，并优于当前先进基线方法diffu-GRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fine-tuning diffusion language models (DLMs) with reinforcement learning, as traditional methods like diffu-GRPO suffer from bias or high computational cost when estimating likelihoods. The authors propose Group Diffusion Policy Optimization (GDPO), a new RL algorithm that reduces variance in evidence lower bound (ELBO) estimators by using semi-deterministic Monte Carlo schemes for fast, deterministic approximations along key dimensions. Experimental results show that GDPO consistently improves over pretrained checkpoints and outperforms diffu-GRPO on most math, reasoning, and coding benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对扩散语言模型（DLMs）的强化学习微调难题展开研究，传统方法如diffu-GRPO在似然估计上存在偏差或计算成本过高的问题。作者提出了群组扩散策略优化（GDPO），这是一种新的RL算法，通过半确定性蒙特卡洛方案在关键维度上进行快速确定性近似，以降低证据下界（ELBO）估计器的方差。实验结果表明，GDPO在多数数学、推理和编码基准测试中优于diffu-GRPO，并持续提升了预训练模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">StatLLaMA: Multi-Stage training for domain-optimized statistical large language models</div>
<div class="meta-line">Authors: Jing-Yi Zeng, Guan-Hua Huang</div>
<div class="meta-line">First: 2025-12-26T05:16:07+00:00 · Latest: 2026-02-11T05:48:53+00:00</div>
<div class="meta-line">Comments: 31 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09718v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09718v2">PDF</a> · <a href="https://github.com/HuangDLab/StatLLaMA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines--starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities--across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task fine-tuning (DTFT). Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that DTFT must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StatLLaMA：面向领域优化的统计大语言模型多阶段训练研究</div>
<div class="mono" style="margin-top:8px">本研究探讨如何以轻量级LLaMA-3.2-3B系列为基础模型，高效构建面向统计学的领域专用大语言模型。我们系统比较了三种多阶段训练流程：从无指令跟随能力的基础模型开始、从经过后置指令调优的基础模型开始、以及从具备强通用推理能力的指令调优模型开始，涵盖持续预训练、监督微调、基于人类反馈的强化学习偏好对齐及下游任务微调。结果表明，从基础模型开始的流程即使经过大量指令调优、监督微调或强化学习对齐，仍无法形成有效的统计推理能力；而从LLaMA-3.2-3B-Instruct出发则能实现有效的领域专业化。对监督微调变体的综合评估揭示了领域专业知识与通用推理能力间的明确权衡。我们进一步证明直接偏好优化能提供稳定有效的强化学习偏好对齐。最后发现，在高度优化的模型中执行下游任务微调需极低强度以避免灾难性遗忘。最终模型StatLLaMA在数学推理、常识推理和统计专业能力的基准测试中均表现出色且均衡，为开发资源高效的统计大语言模型提供了实用蓝图。代码发布于https://github.com/HuangDLab/StatLLaMA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to efficiently create a domain-specialized large language model for statistics, motivated by the need for resource-efficient models in this field. The method systematically compares three multi-stage training pipelines using the lightweight LLaMA-3.2-3B family, involving continual pretraining, supervised fine-tuning, reinforcement learning from human feedback, and downstream task fine-tuning. The main experimental results show that starting from an instruction-tuned foundation model is crucial for developing statistical reasoning, while pipelines beginning with a base model fail; direct preference optimization provides stable alignment, and downstream fine-tuning requires low intensity to prevent catastrophic forgetting, with the final StatLLaMA model achieving balanced performance across mathematical, common-sense, and statistical benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究旨在高效构建一个面向统计学的领域专用大语言模型，其动机在于该领域对资源高效模型的需求。方法上，系统比较了三种基于轻量级LLaMA-3.2-3B系列的多阶段训练流程，涉及持续预训练、监督微调、基于人类反馈的强化学习以及下游任务微调。主要实验结果表明，从指令微调的基础模型出发对于发展统计推理能力至关重要，而从基础模型开始的流程则失败；直接偏好优化能提供稳定的对齐效果，下游微调需极低强度以避免灾难性遗忘，最终模型StatLLaMA在数学推理、常识推理和统计专业基准测试中取得了均衡且强劲的性能。</div>
</details>
</div>
<div class="card">
<div class="title">What Makes Value Learning Efficient in Residual Reinforcement Learning?</div>
<div class="meta-line">Authors: Guozheng Ma, Lu Li, Haoyu Wang, Zixuan Liu, Pierre-Luc Bacon, Dacheng Tao</div>
<div class="meta-line">First: 2026-02-11T05:25:39+00:00 · Latest: 2026-02-11T05:25:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10539v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10539v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>残差强化学习中价值学习高效性的关键因素探析</div>
<div class="mono" style="margin-top:8px">残差强化学习通过冻结基础策略并仅学习有界修正，实现了表达能力强的预训练策略的稳定在线优化。然而，其价值学习面临独特挑战：冷启动困境（评论家缺乏基础策略周边价值景观知识）与结构尺度失配（残差贡献被基础动作量级淹没）。本研究揭示这两个瓶颈的机制，并提出简洁有效的解决方案：基础策略转移为隐式预热提供关键价值锚点，评论家归一化有效恢复表征敏感性以辨识价值差异。基于此，我们提出DAWN（数据锚定预热与归一化）——一种面向残差强化学习高效价值学习的极简方法。DAWN通过突破这些瓶颈，在多样化基准测试、策略架构与观测模态中均展现出显著的效率提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the challenges of value learning in residual reinforcement learning, where a pretrained base policy is frozen and only residual corrections are learned. The authors identify two key bottlenecks: cold start pathology, where the critic lacks initial knowledge of the value landscape, and structural scale mismatch, where residual actions are overshadowed by the base policy&#x27;s scale. To address these, they propose DAWN, a method that uses base-policy transitions for implicit warmup and critic normalization to restore sensitivity to value differences. Experiments across various benchmarks show that DAWN substantially improves learning efficiency in residual RL settings.</div>
<div class="mono" style="margin-top:8px">本文研究了残差强化学习中价值学习面临的挑战，即冻结预训练的基础策略并仅学习残差修正。作者指出了两个关键瓶颈：冷启动问题，即评论家缺乏对基础策略周围价值景观的初始了解；以及结构尺度不匹配，即残差动作被基础策略的尺度所掩盖。为解决这些问题，他们提出了DAWN方法，该方法利用基础策略转移进行隐式预热，并通过评论家归一化恢复对价值差异的敏感性。在不同基准测试中的实验表明，DAWN显著提高了残差强化学习的学习效率。</div>
</details>
</div>
<div class="card">
<div class="title">Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models</div>
<div class="meta-line">Authors: Williams Jonathan, Tureci Esin</div>
<div class="meta-line">First: 2026-02-11T04:39:42+00:00 · Latest: 2026-02-11T04:39:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10520v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10520v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model&#x27;s internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重视过程而非仅结果：奖励潜在思维轨迹提升循环语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">循环语言模型（LoopLMs）在生成词元前执行多步潜在推理，在较小参数量下于推理基准测试中超越传统大语言模型。然而，通过强化学习进一步改进LoopLM推理的尝试均告失败——如组相对策略优化（GRPO）等标准目标仅对最终潜在状态分配奖励，与模型内部计算存在根本性错配。为此，我们提出RLTT（奖励潜在思维轨迹）强化学习框架，将奖励分配至完整的潜在推理轨迹。RLTT无需依赖外部验证器即可实现密集的轨迹级奖励分配，并能以可忽略的开销直接替代GRPO。在相同训练与推理条件下对Ouro-2.6B-Thinking模型进行大量实验表明：RLTT在数学推理基准测试中显著优于GRPO，在MATH-500、AIME24和BeyondAIME的准确率分别提升14.4%、16.6%和10.0%。尽管仅针对数学任务训练，RLTT还能有效迁移至非数学推理基准，证明了轨迹级奖励分配对LoopLMs强化学习的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the failure of standard reinforcement learning objectives like GRPO to improve reasoning in Looped Language Models (LoopLMs), as these objectives only reward the final latent state, misaligning with the model&#x27;s multi-step internal reasoning process. The method introduces RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework that distributes reward across the entire latent reasoning trajectory, enabling dense credit assignment without external verifiers and replacing GRPO with minimal overhead. Main experimental results show that RLTT substantially outperforms GRPO on mathematical reasoning benchmarks using Ouro-2.6B-Thinking, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME, and it also transfers effectively to non-mathematical reasoning tasks, demonstrating the broad efficacy of trajectory-level credit assignment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决标准强化学习目标（如GRPO）在提升循环语言模型推理能力上的失败，因为这些目标仅奖励最终潜在状态，与模型的多步内部推理过程不匹配。方法上提出了RLTT（奖励潜在思维轨迹），这是一个强化学习框架，将奖励分布在整个潜在推理轨迹上，实现了无需外部验证器的密集信用分配，并以最小开销替代GRPO。主要实验结果表明，在使用Ouro-2.6B-Thinking的数学推理基准测试中，RLTT显著优于GRPO，在MATH-500上准确率提升14.4%，在AIME24上提升16.6%，在BeyondAIME上提升10.0%，并且能有效迁移到非数学推理任务，证明了轨迹级信用分配在循环语言模型强化学习中的广泛有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Shihao Dong, Yeke Chen, Zeren Luo, Jiahui Zhang, Bowen Xu, Jinghan Lin, Yimin Han, Ji Ma, Zhiyou Yu, Yudong Zhao, Peng Lu</div>
<div class="meta-line">First: 2026-02-11T04:28:04+00:00 · Latest: 2026-02-11T04:28:04+00:00</div>
<div class="meta-line">Comments: 14 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10514v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their solo capabilities. We tackle the high-impulse contact dynamics of this task under a decentralized setting, achieving synchronization without explicit communication or pre-specified motion primitives. Our framework leverages Multi-Agent Proximal Policy Optimization (MAPPO) enhanced by a progressive curriculum strategy, which effectively overcomes the sparse-reward exploration challenges inherent in mechanically coupled systems. We demonstrate robust performance in simulation and successful transfer to physical hardware, executing multi-directional jumps onto platforms up to 1.5 m in height. Specifically, one of the robots achieves a foot-end elevation of 1.1 m, which represents a 144% improvement over the 0.45 m jump height of a standalone quadrupedal robot, demonstrating superior vertical performance. Notably, this precise coordination is achieved solely through proprioceptive feedback, establishing a foundation for communication-free collaborative locomotion in constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协同跳跃：基于多智能体强化学习的四足机器人协作跳跃</div>
<div class="mono" style="margin-top:8px">尽管单智能体腿式运动已取得显著进展，但单个机器人仍受限于物理驱动能力。为突破此限制，我们提出协同跳跃任务，使两台四足机器人通过同步协作实现远超单体能力的跳跃。我们在去中心化场景下处理该任务的高冲量接触动力学，无需显式通信或预设动作基元即可实现同步。该框架采用渐进课程策略增强的多智能体近端策略优化算法，有效克服机械耦合系统固有的稀疏奖励探索难题。我们在仿真中验证了鲁棒性能，并成功迁移至实体硬件，实现了最高1.5米平台的多向跳跃。其中一台机器人足端抬升高度达1.1米，较单体四足机器人0.45米的跳跃高度提升144%，垂直性能表现卓越。值得注意的是，这种精确协调仅通过本体感知实现，为受限环境下无需通信的协作运动奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the physical limitations of individual legged robots and introduces Co-jump, a cooperative task where two quadrupedal robots synchronize to perform jumps exceeding their solo capabilities. The method employs Multi-Agent Proximal Policy Optimization (MAPPO) with a progressive curriculum to address the high-impulse contact dynamics and sparse-reward challenges in a decentralized setting, achieving synchronization without explicit communication. Experimental results show robust simulation performance and successful transfer to physical hardware, enabling multi-directional jumps onto platforms up to 1.5 m high, with one robot achieving a 1.1 m foot-end elevation—a 144% improvement over a solo robot&#x27;s 0.45 m jump height—using only proprioceptive feedback.</div>
<div class="mono" style="margin-top:8px">本文的动机源于单个腿式机器人的物理局限性，提出了Co-jump这一协作任务，使两个四足机器人通过同步实现超越其单独能力的跳跃。该方法采用多智能体近端策略优化（MAPPO）并结合渐进式课程学习，以解决去中心化设置下的高冲量接触动力学和稀疏奖励探索挑战，实现了无需显式通信的同步。实验结果表明，在仿真中性能稳健并成功迁移到实体硬件，实现了向高达1.5米平台的多方向跳跃，其中一个机器人的足端抬升高度达到1.1米，相比单机0.45米的跳跃高度提升了144%，且仅依靠本体感知反馈完成精确协调。</div>
</details>
</div>
<div class="card">
<div class="title">SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning</div>
<div class="meta-line">Authors: Furong Jia, Ling Dai, Wenjin Deng, Fan Zhang, Chen Hu, Daxin Jiang, Yu Liu</div>
<div class="meta-line">First: 2026-02-10T06:57:12+00:00 · Latest: 2026-02-11T03:34:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09463v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09463v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model&#x27;s reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpotAgent：通过智能体推理将视觉地理定位任务融入大型视觉语言模型</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型在视觉地理定位任务中展现出强大的推理能力，但在视觉线索稀疏、长尾分布且高度模糊的真实场景中仍面临挑战。现有方法受限于内部知识，常无法提供可验证的结果，面对复杂证据时易产生自信但无依据的预测。为此，我们提出SpotAgent框架，将地理定位形式化为智能体推理过程，通过专家级推理实现视觉解读与工具辅助验证的协同。该框架借助ReAct流程图，主动利用外部工具（如网络搜索、地图）探索并验证视觉线索。我们设计了包含三阶段的后训练流程：首先通过监督微调实现基础对齐，随后利用多智能体框架合成的高质量轨迹进行智能体冷启动，以培养工具调用能力，最后通过强化学习优化模型推理。我们提出空间感知动态过滤策略，依据空间难度对可学习样本进行优先级排序，从而提升强化学习阶段的效率。在标准基准测试上的大量实验表明，SpotAgent实现了最先进的性能，在提供精确且可验证的地理定位结果的同时，有效缓解了幻觉问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for SpotAgent stems from the limitations of Large Vision-Language Models (LVLMs) in real-world geo-localization, where they often produce ungrounded predictions due to sparse and ambiguous visual cues. The method formalizes geo-localization as an agentic reasoning process, employing a three-stage post-training pipeline that includes supervised fine-tuning, an agentic cold start phase using multi-agent synthesized trajectories to learn tool usage, and reinforcement learning refined with a Spatially-Aware Dynamic Filtering strategy to prioritize difficult samples. Experimental results on standard benchmarks show that SpotAgent achieves state-of-the-art performance by effectively reducing hallucinations and delivering precise, verifiable location predictions.</div>
<div class="mono" style="margin-top:8px">SpotAgent的提出动机源于大型视觉语言模型在现实世界地理定位中的局限性，这些模型常因视觉线索稀疏、模糊而产生不可靠的预测。该方法将地理定位形式化为一个智能体推理过程，采用三阶段后训练流程，包括监督微调、利用多智能体合成轨迹进行工具调用的智能体冷启动阶段，以及通过空间感知动态过滤策略优化难样本学习的强化学习阶段。在标准基准测试上的实验结果表明，SpotAgent实现了最先进的性能，有效减少了幻觉，并提供了精确且可验证的地理定位结果。</div>
</details>
</div>
<div class="card">
<div class="title">Hypercube Policy Regularization Framework for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Yi Shen, Hanyan Huang</div>
<div class="meta-line">First: 2024-11-07T08:48:32+00:00 · Latest: 2026-02-11T03:26:14+00:00</div>
<div class="meta-line">Comments: Revised version accepted at MLMI 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.04534v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.04534v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning has received extensive attention from scholars because it avoids the interaction between the agent and the environment by learning a policy through a static dataset. However, general reinforcement learning methods cannot get satisfactory results in offline reinforcement learning due to the out-of-distribution state actions that the dataset cannot cover during training. To solve this problem, the policy regularization method that tries to directly clone policies used in static datasets has received numerous studies due to its simplicity and effectiveness. However, policy constraint methods make the agent choose the corresponding actions in the static dataset. This type of constraint is usually over-conservative, which results in suboptimal policies, especially in low-quality static datasets. In this paper, a hypercube policy regularization framework is proposed, this method alleviates the constraints of policy constraint methods by allowing the agent to explore the actions corresponding to similar states in the static dataset, which increases the effectiveness of algorithms in low-quality datasets. It was also theoretically demonstrated that the hypercube policy regularization framework can effectively improve the performance of original algorithms. In addition, the hypercube policy regularization framework is combined with TD3-BC and Diffusion-QL for experiments on D4RL datasets which are called TD3-BC-C and Diffusion-QL-C. The experimental results of the score demonstrate that TD3-BC-C and Diffusion-QL-C perform better than state-of-the-art algorithms like IQL, CQL, TD3-BC and Diffusion-QL in most D4RL environments in approximate time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线强化学习的超立方体策略正则化框架</div>
<div class="mono" style="margin-top:8px">离线强化学习通过静态数据集学习策略，避免了智能体与环境的交互，因此受到学者广泛关注。然而，由于数据集中未覆盖的分布外状态动作，通用强化学习方法在离线强化学习中难以取得满意效果。为解决此问题，直接克隆静态数据集中策略的策略正则化方法因其简单有效而得到大量研究。但策略约束方法强制智能体选择静态数据集中的对应动作，这类约束通常过于保守，导致策略次优，尤其在低质量静态数据集中。本文提出超立方体策略正则化框架，该方法允许智能体探索静态数据集中相似状态对应的动作，从而缓解策略约束方法的限制，提升了算法在低质量数据集中的有效性。理论分析证明该框架能有效提升原始算法性能。此外，将超立方体策略正则化框架与TD3-BC和Diffusion-QL结合，在D4RL数据集上进行实验，分别命名为TD3-BC-C和Diffusion-QL-C。评分实验结果表明，在近似时间内，TD3-BC-C和Diffusion-QL-C在多数D4RL环境中优于IQL、CQL、TD3-BC和Diffusion-QL等前沿算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the over-conservatism of existing policy regularization methods in offline reinforcement learning, which often leads to suboptimal policies, especially with low-quality static datasets. The proposed method introduces a hypercube policy regularization framework that alleviates constraints by allowing the agent to explore actions corresponding to similar states in the dataset, thereby increasing algorithm effectiveness. Experimental results on D4RL datasets, where the framework is combined with TD3-BC and Diffusion-QL to create TD3-BC-C and Diffusion-QL-C, show that these variants outperform state-of-the-art algorithms like IQL, CQL, TD3-BC, and Diffusion-QL in most environments, with theoretical analysis supporting the performance improvement.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线强化学习中现有策略正则化方法过于保守的问题，这类方法常导致策略次优，尤其在低质量静态数据集中。提出的方法是一种超立方体策略正则化框架，通过允许智能体探索数据集中相似状态对应的动作来缓解约束，从而提升算法在低质量数据集上的有效性。在D4RL数据集上的实验将框架与TD3-BC和Diffusion-QL结合，形成TD3-BC-C和Diffusion-QL-C，结果显示这些变体在多数环境中优于IQL、CQL、TD3-BC和Diffusion-QL等先进算法，理论分析也证实了其性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Found-RL: foundation model-enhanced reinforcement learning for autonomous driving</div>
<div class="meta-line">Authors: Yansong Qu, Zihao Sheng, Zilin Huang, Jiancong Chen, Yuhao Luo, Tianyi Wang, Yiheng Feng, Samuel Labi, Sikai Chen</div>
<div class="meta-line">First: 2026-02-11T02:56:04+00:00 · Latest: 2026-02-11T02:56:04+00:00</div>
<div class="meta-line">Comments: 39 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10458v1">PDF</a> · <a href="https://github.com/ys-qu/found-rl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP&#x27;s dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Found-RL：基于基础模型增强的自动驾驶强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为端到端自动驾驶的主导范式，但其存在样本效率低、复杂场景语义可解释性不足的问题。基础模型（尤其是视觉语言模型）能通过提供丰富的上下文感知知识缓解此问题，但其高推理延迟阻碍了在高频强化学习训练循环中的部署。为此，我们提出Found-RL平台，专门利用基础模型高效增强自动驾驶强化学习。其核心创新是异步批量推理框架，将繁重的视觉语言模型推理与仿真循环解耦，有效解决延迟瓶颈以支持实时学习。我们引入多种监督机制：价值边际正则化与优势加权动作引导，将类专家的视觉语言模型动作建议有效蒸馏至强化学习策略中。此外，采用高吞吐量CLIP模型进行密集奖励塑形，并通过条件对比动作对齐方法解决CLIP的动态盲区问题——该方法基于离散化速度/指令条件化提示，通过特定上下文动作锚点评分生成归一化的边际奖励。Found-RL提供端到端的微调视觉语言模型集成流程，实验表明轻量级强化学习模型在保持实时推理（约500 FPS）的同时，能达到接近百亿参数视觉语言模型的性能。代码、数据与模型将在https://github.com/ys-qu/found-rl公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the sample inefficiency and lack of semantic interpretability in reinforcement learning (RL) for autonomous driving by integrating the rich contextual knowledge of foundation models, specifically Vision-Language Models (VLMs), into the RL training loop. The proposed Found-RL platform introduces an asynchronous batch inference framework to decouple heavy VLM reasoning from the simulation, overcoming latency bottlenecks, and employs supervision mechanisms like Value-Margin Regularization and Advantage-Weighted Action Guidance to distill VLM action suggestions into the RL policy. Experimental results demonstrate that Found-RL enables a lightweight RL model to achieve performance comparable to billion-parameter VLMs while maintaining real-time inference speeds of approximately 500 FPS.</div>
<div class="mono" style="margin-top:8px">本文针对自动驾驶中强化学习样本效率低和语义可解释性不足的问题，提出通过整合基础模型（特别是视觉语言模型）的丰富上下文知识来增强强化学习训练。所提出的Found-RL平台采用异步批量推理框架，将繁重的视觉语言模型推理与仿真循环解耦以解决延迟瓶颈，并利用价值边际正则化和优势加权动作引导等监督机制，将视觉语言模型的专家动作建议提炼到强化学习策略中。实验结果表明，Found-RL能使轻量级强化学习模型达到与数十亿参数视觉语言模型相近的性能，同时保持约500 FPS的实时推理速度。</div>
</details>
</div>
<div class="card">
<div class="title">Belief-Based Offline Reinforcement Learning for Delay-Robust Policy Optimization</div>
<div class="meta-line">Authors: Simon Sinong Zhan, Qingyuan Wu, Philip Wang, Frank Yang, Xiangyu Shi, Chao Huang, Qi Zhu</div>
<div class="meta-line">First: 2025-05-30T18:09:29+00:00 · Latest: 2026-02-11T02:52:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00131v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.00131v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline-to-online deployment of reinforcement-learning (RL) agents must bridge two gaps: (1) the sim-to-real gap, where real systems add latency and other imperfections not present in simulation, and (2) the interaction gap, where policies trained purely offline face out-of-distribution states during online execution because gathering new interaction data is costly or risky. Agents therefore have to generalize from static, delay-free datasets to dynamic, delay-prone environments. Standard offline RL learns from delay-free logs yet must act under delays that break the Markov assumption and hurt performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained Offline RL), an offline-RL framework built to cope with delayed dynamics at deployment. DT-CORL (i) produces delay-robust actions with a transformer-based belief predictor even though it never sees delayed observations during training, and (ii) is markedly more sample-efficient than naïve history-augmentation baselines. Experiments on D4RL benchmarks with several delay settings show that DT-CORL consistently outperforms both history-augmentation and vanilla belief-based methods, narrowing the sim-to-real latency gap while preserving data efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信念的离线强化学习用于延迟鲁棒策略优化</div>
<div class="mono" style="margin-top:8px">强化学习（RL）智能体从离线到在线部署需弥合两个差距：（1）仿真到现实的差距，即真实系统存在仿真中未出现的延迟等缺陷；（2）交互差距，即纯离线训练的策略在在线执行时会因新交互数据获取成本高或风险大而面临分布外状态。因此，智能体必须从静态、无延迟数据集泛化到动态、易延迟的环境。标准离线RL从无延迟日志中学习，却必须在破坏马尔可夫假设并损害性能的延迟下行动。我们提出DT-CORL（延迟变换器信念策略约束离线RL），这是一个专为应对部署时延迟动态而构建的离线RL框架。DT-CORL（i）通过基于变换器的信念预测器生成延迟鲁棒动作，尽管训练期间从未观测到延迟数据；（ii）相比简单历史增强基线方法显著提升样本效率。在多种延迟设置的D4RL基准测试中，DT-CORL始终优于历史增强和原始信念方法，在保持数据效率的同时缩小了仿真到现实的延迟差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of deploying offline reinforcement learning (RL) agents in real-world settings where delays and latency break the Markov assumption, causing performance degradation. To bridge this sim-to-real gap, the authors propose DT-CORL, a framework that uses a transformer-based belief predictor to generate delay-robust actions from static, delay-free datasets, without requiring delayed observations during training. Experimental results on D4RL benchmarks demonstrate that DT-CORL consistently outperforms history-augmentation and vanilla belief-based methods, offering improved sample efficiency and robustness across various delay settings.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习智能体在现实部署中因延迟破坏马尔可夫假设而导致性能下降的问题，提出了一种解决方案。为弥合仿真与现实的差距，作者设计了DT-CORL框架，该框架基于Transformer的信念预测器，仅利用静态无延迟数据集生成抗延迟动作，且训练时无需延迟观测。在D4RL基准测试中，DT-CORL在多种延迟设置下均优于历史增强和基础信念方法，显著提升了样本效率与策略鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning</div>
<div class="meta-line">Authors: Liyang Chen, Hongkai Chen, Yujun Cai, Sifan Li, Qingwen Ye, Yiwei Wang</div>
<div class="meta-line">First: 2026-02-11T02:30:48+00:00 · Latest: 2026-02-11T02:30:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10439v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Audio Language Models (LALMs) have demonstrated strong capabilities in audio understanding and reasoning. However, their performance on fine grained auditory perception remains unreliable, and existing approaches largely rely on data intensive training to internalize perceptual abilities. We propose AudioRouter, a reinforcement learning framework that enables LALMs to improve audio understanding by learning when and how to use external audio tools. Rather than tightly coupling tool usage with audio reasoning, AudioRouter formulates tool use as an explicit decision making problem and optimizes a lightweight routing policy while keeping the underlying reasoning model frozen. Experimental results show that AudioRouter achieves substantial improvements on standard audio understanding benchmarks while requiring up to 600x less training data to learn tool usage compared with conventional training paradigms. These findings suggest that learning effective tool usage offers a data efficient and scalable alternative to internalizing perceptual abilities in LALMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AudioRouter：基于强化学习双推理机制的数据高效音频理解方法</div>
<div class="mono" style="margin-top:8px">大型音频语言模型在音频理解与推理方面展现出强大能力，但其细粒度听觉感知性能仍不稳定，现有方法主要依赖数据密集型训练来内化感知能力。本文提出AudioRouter——一种强化学习框架，通过让大型音频语言模型学习何时及如何使用外部音频工具来提升音频理解能力。该框架将工具使用建模为显式决策问题，在保持底层推理模型冻结的同时优化轻量级路由策略，而非将工具使用与音频推理紧密耦合。实验表明，AudioRouter在标准音频理解基准上取得显著提升，且学习工具使用所需的训练数据量比传统训练范式减少高达600倍。这些发现表明，学习有效的工具使用为大型音频语言模型内化感知能力提供了一条数据高效且可扩展的替代路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for AudioRouter stems from the limitations of Large Audio Language Models (LALMs) in fine-grained auditory perception, which typically require data-intensive training to improve. The method introduces a reinforcement learning framework that enables LALMs to learn a lightweight routing policy for deciding when and how to utilize external audio tools, all while keeping the core reasoning model frozen. The main experimental results demonstrate that AudioRouter achieves significant performance gains on standard audio understanding benchmarks while requiring up to 600 times less training data to learn effective tool usage compared to conventional training approaches.</div>
<div class="mono" style="margin-top:8px">AudioRouter的提出动机源于大型音频语言模型在细粒度听觉感知上的不可靠性，以及现有方法通常依赖数据密集型训练来内化感知能力。该方法采用强化学习框架，使模型能够学习何时及如何使用外部音频工具的轻量级路由策略，同时保持底层推理模型固定不变。主要实验结果表明，AudioRouter在标准音频理解基准上取得了显著性能提升，且学习工具使用所需的训练数据比传统训练范式少高达600倍。</div>
</details>
</div>
<div class="card">
<div class="title">Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering</div>
<div class="meta-line">Authors: Seonglae Cho, Zekun Wu, Adriano Koshiyama</div>
<div class="meta-line">First: 2026-02-11T02:28:49+00:00 · Latest: 2026-02-11T02:28:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10437v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10437v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature discovery while preserving singlefeature interpretability. The framework yields new analysis capabilities: branch point tracking locates tokens where feature choice determines output correctness; critic trajectory analysis separates policy limitations from value estimation errors; layer-wise comparison reveals syntactic features in early layers and semantic features in later layers. On Gemma-2 2B across MMLU, BBQ, GSM8K, HarmBench, and XSTest, CRL achieves improvements while providing per-token intervention logs. These results establish learned feature steering as a mechanistic interpretability tool that complements static feature analysis with dynamic intervention probes</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>控制强化学习：基于学习型SAE特征导向的令牌级机制分析</div>
<div class="mono" style="margin-top:8px">稀疏自编码器（SAEs）可将语言模型激活分解为可解释特征，但现有方法仅能揭示哪些特征被激活，无法说明放大哪些特征会改变模型输出。我们提出控制强化学习（CRL），该方法训练策略以在每令牌处选择SAE特征进行导向，生成可解释的干预日志：学习到的策略能识别放大时会改变模型输出的特征。自适应特征掩码在保持单特征可解释性的同时促进多样化特征发现。该框架提供新的分析能力：分支点追踪定位特征选择决定输出正确性的令牌位置；评判轨迹分析区分策略局限与价值估计误差；分层比较揭示早期层的句法特征与后期层的语义特征。在Gemma-2 2B模型上，通过MMLU、BBQ、GSM8K、HarmBench和XSTest测试，CRL在提供每令牌干预日志的同时实现了性能提升。这些结果表明，学习型特征导向作为一种机制可解释性工具，可通过动态干预探针补充静态特征分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of existing sparse autoencoder (SAE) methods that only identify which features activate without revealing which ones actually change model outputs when amplified, this paper introduces Control Reinforcement Learning (CRL) to train a policy for selecting SAE features to steer at each token, producing interpretable intervention logs. The method employs Adaptive Feature Masking to encourage diverse feature discovery while preserving single-feature interpretability. Main experimental results on Gemma-2 2B across benchmarks including MMLU, BBQ, GSM8K, HarmBench, and XSTest show that CRL achieves performance improvements while enabling new analysis capabilities such as branch point tracking, critic trajectory analysis, and layer-wise comparisons revealing syntactic features in early layers and semantic features in later layers.</div>
<div class="mono" style="margin-top:8px">现有稀疏自编码器（SAE）方法仅能识别哪些特征被激活，而无法揭示哪些特征在放大时实际改变模型输出，为此本文引入了控制强化学习（CRL），通过训练策略在每一令牌处选择SAE特征进行引导，并生成可解释的干预日志。该方法采用自适应特征掩码，以鼓励多样化的特征发现，同时保持单个特征的可解释性。在Gemma-2 2B模型上，于MMLU、BBQ、GSM8K、HarmBench和XSTest等基准测试中的主要实验结果表明，CRL实现了性能提升，并提供了新的分析能力，如分支点跟踪、评论家轨迹分析以及层间比较，揭示了早期层的句法特征和后期层的语义特征。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation</div>
<div class="meta-line">Authors: Jie Jiang, Yusen Huo, Xiangxin Zhan, Changping Wang, Jun Zhang</div>
<div class="meta-line">First: 2026-02-11T02:18:27+00:00 · Latest: 2026-02-11T02:18:27+00:00</div>
<div class="meta-line">Comments: 21 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10430v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10430v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>打破排斥诅咒：面向离策略生成式推荐的乐观分布鲁棒策略优化</div>
<div class="mono" style="margin-top:8px">基于策略的强化学习已成为优化序列用户交互的生成式推荐主流范式。然而，当应用于离线历史日志时，这些方法存在严重缺陷：低质量数据的主导地位导致严重的模型崩溃。我们首先提出排斥优化的散度理论，揭示负梯度更新在离策略训练中会固有地引发指数级强度爆炸。该理论阐明了现有方法的内在困境，暴露了其在方差缩减与噪声模仿之间无法调和的矛盾。为打破这一诅咒，我们认为解决方案在于严格识别噪声行为策略中纠缠的潜在高质量分布。据此，我们将目标重构为乐观分布鲁棒优化问题。基于此框架，我们提出分布鲁棒策略优化方法。我们证明硬过滤是该分布鲁棒优化目标的精确解，使DRPO能够最优恢复高质量行为，同时严格剔除导致散度的噪声。大量实验表明，DRPO在混合质量推荐基准上实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the critical failure of policy-based reinforcement learning in generative recommendation when trained on offline logs, where low-quality data leads to model collapse. The method introduces the Divergence Theory of Repulsive Optimization to explain this issue and proposes Distributionally Robust Policy Optimization (DRPO), which reformulates the objective as an optimistic distributionally robust optimization problem, using hard filtering to recover high-quality behaviors. Experimental results show that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks, effectively mitigating divergence and improving recommendation quality.</div>
<div class="mono" style="margin-top:8px">本文的动机在于基于策略的强化学习在生成式推荐中应用离线日志训练时，因低质量数据导致模型崩溃的关键失败。方法提出了排斥优化的散度理论来解释此问题，并提出了分布鲁棒策略优化（DRPO），将目标重构为乐观的分布鲁棒优化问题，通过硬过滤恢复高质量行为。实验结果表明，DRPO在混合质量推荐基准上实现了最先进的性能，有效缓解了散度问题并提升了推荐质量。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-based agricultural management in partially observable environments subject to climate variability</div>
<div class="meta-line">Authors: Zhaoan Wang, Shaoping Xiao, Junchao Li, Jun Wang</div>
<div class="meta-line">First: 2024-01-02T16:18:53+00:00 · Latest: 2026-02-10T23:46:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.01273v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.01273v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agricultural management, with a particular focus on fertilization strategies, holds a central role in shaping crop yield, economic profitability, and environmental sustainability. While conventional guidelines offer valuable insights, their efficacy diminishes when confronted with extreme weather conditions, such as heatwaves and droughts. In this study, we introduce an innovative framework that integrates Deep Reinforcement Learning (DRL) with Recurrent Neural Networks (RNNs). Leveraging the Gym-DSSAT simulator, we train an intelligent agent to master optimal nitrogen fertilization management. Through a series of simulation experiments conducted on corn crops in Iowa, we compare Partially Observable Markov Decision Process (POMDP) models with Markov Decision Process (MDP) models. Our research underscores the advantages of utilizing sequential observations in developing more efficient nitrogen input policies. Additionally, we explore the impact of climate variability, particularly during extreme weather events, on agricultural outcomes and management. Our findings demonstrate the adaptability of fertilization policies to varying climate conditions. Notably, a fixed policy exhibits resilience in the face of minor climate fluctuations, leading to commendable corn yields, cost-effectiveness, and environmental conservation. However, our study illuminates the need for agent retraining to acquire new optimal policies under extreme weather events. This research charts a promising course toward adaptable fertilization strategies that can seamlessly align with dynamic climate scenarios, ultimately contributing to the optimization of crop management practices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的农业管理在气候变异性下的部分可观测环境中的应用</div>
<div class="mono" style="margin-top:8px">农业管理，特别是施肥策略，对作物产量、经济效益和环境可持续性具有核心影响。传统指导原则虽具参考价值，但在面对热浪和干旱等极端天气时效果有限。本研究提出一种创新框架，将深度强化学习与循环神经网络相结合，利用Gym-DSSAT模拟器训练智能体掌握最优氮肥管理策略。通过在爱荷华州玉米作物上进行系列模拟实验，对比了部分可观测马尔可夫决策过程模型与马尔可夫决策过程模型。研究凸显了利用序列观测数据制定更高效氮肥施用策略的优势，并探讨了气候变异性（尤其是极端天气事件）对农业产出与管理的影响。结果表明施肥策略能适应不同气候条件：固定策略在轻微气候波动下表现出韧性，可实现可观的玉米产量、成本效益和环境保护；但研究同时揭示在极端天气下需通过智能体重训练获取新的最优策略。本研究为开发能动态适应气候变化的施肥策略开辟了前景，最终助力作物管理实践的优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of optimizing agricultural fertilization under climate variability, where traditional guidelines often fail during extreme weather. The authors propose a framework combining Deep Reinforcement Learning (DRL) with Recurrent Neural Networks (RNNs) to train an agent using the Gym-DSSAT simulator for nitrogen management in corn crops in Iowa. Experimental results show that Partially Observable Markov Decision Process (POMDP) models, utilizing sequential observations, outperform Markov Decision Process (MDP) models, leading to more efficient fertilization policies. The findings indicate that while fixed policies are resilient to minor climate fluctuations, achieving good yields and sustainability, retraining is necessary under extreme events to adapt policies, highlighting a path toward climate-adaptive agricultural management.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决气候变化下农业施肥优化问题，传统方法在极端天气条件下往往失效。作者提出了一种结合深度强化学习（DRL）和循环神经网络（RNN）的框架，利用Gym-DSSAT模拟器训练智能体，用于爱荷华州玉米作物的氮肥管理。实验结果表明，采用序列观测的部分可观测马尔可夫决策过程（POMDP）模型优于马尔可夫决策过程（MDP）模型，能制定更高效的施肥策略。研究发现，固定策略对小范围气候波动具有韧性，可实现良好产量和可持续性，但在极端天气下需重新训练以调整策略，这为发展适应动态气候的农业管理提供了方向。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Inverse Reinforcement Learning through Encoding Dynamic Information in Reward Shaping</div>
<div class="meta-line">Authors: Simon Sinong Zhan, Philip Wang, Qingyuan Wu, Yixuan Wang, Ruochen Jiao, Chao Huang, Qi Zhu</div>
<div class="meta-line">First: 2024-10-04T18:27:37+00:00 · Latest: 2026-02-10T22:43:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.03847v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.03847v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we aim to tackle the limitation of the Adversarial Inverse Reinforcement Learning (AIRL) method in stochastic environments where theoretical results cannot hold and performance is degraded. To address this issue, we propose a novel method which infuses the dynamics information into the reward shaping with the theoretical guarantee for the induced optimal policy in the stochastic environments. Incorporating our novel model-enhanced rewards, we present a novel Model-Enhanced AIRL framework, which integrates transition model estimation directly into reward shaping. Furthermore, we provide a comprehensive theoretical analysis of the reward error bound and performance difference bound for our method. The experimental results in MuJoCo benchmarks show that our method can achieve superior performance in stochastic environments and competitive performance in deterministic environments, with significant improvement in sample efficiency, compared to existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过奖励塑形中编码动态信息增强逆向强化学习</div>
<div class="mono" style="margin-top:8px">本文旨在解决对抗性逆向强化学习（AIRL）方法在随机环境中理论结果不成立且性能下降的局限性。为此，我们提出一种新方法，将动态信息融入奖励塑形，并为随机环境中的诱导最优策略提供理论保证。结合我们提出的模型增强奖励，我们构建了一个新颖的模型增强AIRL框架，直接将转移模型估计整合到奖励塑形中。此外，我们对该方法的奖励误差界和性能差异界进行了全面的理论分析。在MuJoCo基准测试中的实验结果表明，与现有基线相比，我们的方法在随机环境中能实现更优性能，在确定性环境中具有竞争力，且样本效率显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the performance degradation of Adversarial Inverse Reinforcement Learning (AIRL) in stochastic environments, where its theoretical guarantees fail. The proposed method enhances AIRL by explicitly encoding environment dynamics into the reward shaping process, creating a Model-Enhanced AIRL framework that integrates transition model estimation to provide theoretical guarantees for the optimal policy in stochastic settings. Experiments on MuJoCo benchmarks demonstrate that the method achieves superior performance in stochastic environments and competitive results in deterministic ones, while significantly improving sample efficiency over existing baselines.</div>
<div class="mono" style="margin-top:8px">本文旨在解决对抗性逆强化学习（AIRL）在随机环境中理论保证失效且性能下降的问题。所提出的方法通过将环境动态信息显式编码到奖励塑形中，构建了一个模型增强的AIRL框架，该框架集成了转移模型估计，从而为随机环境中的最优策略提供了理论保证。在MuJoCo基准测试中的实验结果表明，该方法在随机环境中实现了优越的性能，在确定性环境中也具有竞争力，同时相比现有基线显著提高了样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality</div>
<div class="meta-line">Authors: Zhimin Hu, Riya Roshan, Sashank Varma</div>
<div class="meta-line">First: 2026-02-10T22:07:05+00:00 · Latest: 2026-02-10T22:07:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10329v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10329v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) models explicitly generate long reasoning steps during inference, whereas Large Reasoning Models (LRMs) are trained by reinforcement learning to discover reasoning paths that maximize accuracy. However, it remains unclear whether resource-rationality can emerge from such scaling without explicit reward related to computational costs. We introduce a Variable Attribution Task in which models infer which variables determine outcomes given candidate variables, input-output trials, and predefined logical functions. By varying the number of candidate variables and trials, we systematically manipulate task complexity. Both models exhibit a transition from brute-force to analytic strategies as complexity increases. IT models degrade on XOR and XNOR functions, whereas LRMs remain robust. These findings suggest that models can adjust their reasoning behavior in response to task complexity, even without explicit cost-based reward. It provides compelling evidence that resource rationality is an emergent property of inference-time scaling itself.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>更多推理步数是否合理？语言模型中的推理时扩展作为自适应资源理性</div>
<div class="mono" style="margin-top:8px">人类推理受资源理性影响——即在约束条件下优化性能。近期，推理时扩展已成为通过增加测试时计算来提升大语言模型推理能力的有效范式。具体而言，指令微调模型在推理时显式生成长推理链，而大型推理模型则通过强化学习训练以寻找最大化准确率的推理路径。然而，这类扩展能否在不涉及显式计算成本奖励的情况下自发产生资源理性仍不明确。我们引入变量归因任务，要求模型根据候选变量、输入输出样本及预定义逻辑函数推断决定结果的变量。通过调整候选变量数量和样本数，我们系统操控任务复杂度。两种模型均随复杂度增加表现出从暴力搜索到分析策略的转变：指令微调模型在异或和同或函数上性能下降，而大型推理模型保持稳健。这些发现表明，即使没有基于成本的显式奖励，模型仍能根据任务复杂度调整推理行为，为资源理性是推理时扩展本身涌现属性的观点提供了有力证据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether resource rationality, akin to human reasoning under constraints, emerges in language models through inference-time scaling without explicit computational cost rewards. The authors introduce a Variable Attribution Task to systematically vary complexity by adjusting the number of candidate variables and input-output trials, testing both instruction-tuned models and Large Reasoning Models trained with reinforcement learning. Experimental results show that both model types transition from brute-force to analytic strategies as complexity increases, with instruction-tuned models degrading on XOR and XNOR functions while Large Reasoning Models remain robust, suggesting that adaptive resource rationality can arise from scaling alone.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在无显式计算成本奖励的情况下，语言模型是否通过推理时扩展而涌现出类似人类在约束下进行推理的资源理性。作者引入了一个变量归因任务，通过调整候选变量数量和输入输出试验来系统改变任务复杂度，测试了指令调优模型和通过强化学习训练的大型推理模型。实验结果表明，随着复杂度增加，两类模型都从暴力策略转向分析策略，其中指令调优模型在XOR和XNOR函数上性能下降，而大型推理模型保持稳健，这表明自适应资源理性可能仅通过扩展本身即可产生。</div>
</details>
</div>
<div class="card">
<div class="title">The Specification Trap: Why Content-Based AI Value Alignment Cannot Produce Robust Alignment</div>
<div class="meta-line">Authors: Austin Spizzirri</div>
<div class="meta-line">First: 2025-11-19T23:31:29+00:00 · Latest: 2026-02-10T22:06:48+00:00</div>
<div class="meta-line">Comments: ~6,000 words, 19 pages. First in a five-paper research program on AI alignment. Establishes structural ceiling on content-based approaches (RLHF, Constitutional AI, IRL, assistance games); does not claim alignment is impossible, claims robust alignment under scaling/shift/autonomy requires process-based alternatives</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03048v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.03048v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">I argue that content-based AI value alignment--any approach that treats alignment as optimizing toward a formal value-object (reward function, utility function, constitutional principles, or learned preference representation)--cannot, by itself, produce robust alignment under capability scaling, distributional shift, and increasing autonomy. This limitation arises from three philosophical results: Hume&#x27;s is-ought gap (behavioral data cannot entail normative conclusions), Berlin&#x27;s value pluralism (human values are irreducibly plural and incommensurable), and the extended frame problem (any value encoding will misfit future contexts that advanced AI creates). I show that RLHF, Constitutional AI, inverse reinforcement learning, and cooperative assistance games each instantiate this specification trap, and that their failure modes are structural, not engineering limitations. Proposed escape routes--continual updating, meta-preferences, moral realism--relocate the trap rather than exit it. Drawing on Fischer and Ravizza&#x27;s compatibilist theory, I argue that behavioral compliance does not constitute alignment: there is a principled distinction between simulated value-following and genuine reasons-responsiveness, and specification-based methods cannot produce the latter. The specification trap establishes a ceiling on content-based approaches, not their uselessness--but this ceiling becomes safety-critical at the capability frontier. The alignment problem must be reframed from value specification to value emergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>规范陷阱：为何基于内容的AI价值对齐无法产生稳健对齐</div>
<div class="mono" style="margin-top:8px">本文论证基于内容的AI价值对齐——任何将对齐视为向形式价值目标（奖励函数、效用函数、宪法原则或习得偏好表征）优化的方法——在能力扩展、分布偏移和自主性增强的条件下，本身无法产生稳健对齐。这一局限源于三个哲学结论：休谟的实然-应然鸿沟（行为数据无法推导出规范性结论）、柏林的多元价值论（人类价值具有不可简化的多元性与不可通约性），以及扩展框架问题（任何价值编码都将无法适应先进AI创造的未来情境）。研究表明，RLHF、宪法AI、逆强化学习和协作辅助博弈均落入此规范陷阱，其失效模式是结构性的而非工程局限。提出的解决路径——持续更新、元偏好、道德实在论——仅转移而非跳出陷阱。借鉴费舍尔与拉维扎的相容论理论，本文主张行为合规不构成对齐：模拟价值遵循与真实理由响应存在原则性区别，基于规范的方法无法产生后者。规范陷阱为基于内容的方法设定了能力上限（非否定其效用），该上限在能力边界处成为安全关键。对齐问题必须从价值规范重构为价值涌现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper argues that content-based AI value alignment, which aims to optimize AI behavior toward a formal value specification such as a reward function or constitutional principles, is fundamentally limited in achieving robust alignment as AI capabilities scale, face distributional shifts, and gain autonomy. The author grounds this claim in three philosophical challenges: Hume&#x27;s is-ought gap, Berlin&#x27;s value pluralism, and the extended frame problem, demonstrating that methods like RLHF and Constitutional AI inherently fall into this &#x27;specification trap&#x27; due to structural, not merely technical, flaws. The analysis concludes that while such approaches have utility, they hit a safety-critical ceiling, necessitating a shift in the alignment problem from value specification to value emergence through process-based alternatives.</div>
<div class="mono" style="margin-top:8px">本文论证了基于内容的AI价值对齐方法——即旨在通过优化形式化的价值目标（如奖励函数、宪法原则）来对齐AI——在AI能力扩展、分布变化和自主性增强的背景下，无法从根本上实现稳健的对齐。作者基于休谟的实然应然鸿沟、柏林的多元价值论以及扩展框架问题这三个哲学论点，指出诸如RLHF和宪法AI等方法均陷入了这种结构性的&#x27;规范陷阱&#x27;，其失败模式是固有的而非工程局限。分析结论认为，虽然这些方法有一定效用，但其存在安全关键性的能力上限，因此必须将对齐问题的框架从价值规范转向通过基于过程的替代方案实现价值涌现。</div>
</details>
</div>
<div class="card">
<div class="title">Confounding Robust Continuous Control via Automatic Reward Shaping</div>
<div class="meta-line">Authors: Mateo Juliani, Mingxuan Li, Elias Bareinboim</div>
<div class="meta-line">First: 2026-02-10T21:23:12+00:00 · Latest: 2026-02-10T21:23:12+00:00</div>
<div class="meta-line">Comments: Mateo Juliani and Mingxuan Li contributed equally to this work; accepted in AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10305v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10305v1">PDF</a> · <a href="https://github.com/mateojuliani/confounding_robust_cont_control">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents&#x27; training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自动奖励塑形实现混淆鲁棒的连续控制</div>
<div class="mono" style="margin-top:8px">奖励塑形已被广泛应用于加速强化学习（RL）智能体的训练。然而，设计有效的奖励塑形函数（特别是针对复杂连续控制问题）的原则性方法仍缺乏充分解释。本研究提出从可能受未观测混淆变量污染的离线数据集中，自动学习连续控制问题的奖励塑形函数。具体而言，我们的方法基于近期提出的因果贝尔曼方程，学习最优状态值的紧致上界，并将其用作基于势的奖励塑形（PBRS）框架中的势函数。所提出的奖励塑形算法在多个常用连续控制基准测试中与软演员-评论家（SAC）结合验证，并在未观测混淆变量下展现出强性能保证。更广泛而言，本研究标志着从因果视角实现混淆鲁棒连续控制迈出了坚实的第一步。训练奖励塑形函数的代码可见于 https://github.com/mateojuliani/confounding_robust_cont_control。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of designing effective reward shaping functions for Reinforcement Learning (RL) in continuous control, particularly when offline datasets may contain unobserved confounding variables. The method leverages the causal Bellman equation to automatically learn a tight upper bound on optimal state values from offline data, which is then used as potentials within the Potential-Based Reward Shaping (PBRS) framework. Experimental results on multiple continuous control benchmarks using Soft-Actor-Critic (SAC) demonstrate that the proposed approach provides strong performance guarantees under unobserved confounders, marking a significant step towards confounding-robust continuous control from a causal perspective.</div>
<div class="mono" style="margin-top:8px">本文针对连续控制中强化学习奖励塑形函数的设计挑战，特别是在离线数据集可能包含未观测混杂变量的情况下，提出了一种解决方案。该方法基于因果贝尔曼方程，从离线数据中自动学习最优状态值的紧上界，并将其用作基于势的奖励塑形框架中的势函数。在多个常用连续控制基准上使用软演员-评论家算法的实验结果表明，所提出的方法在未观测混杂变量下具有强大的性能保证，标志着从因果视角实现混杂鲁棒的连续控制迈出了坚实的第一步。</div>
</details>
</div>
<div class="card">
<div class="title">HypeRL: Hypernetwork-Based Reinforcement Learning for Control of Parametrized Dynamical Systems</div>
<div class="meta-line">Authors: Nicolò Botteghi, Stefania Fresca, Mengwu Guo, Andrea Manzoni</div>
<div class="meta-line">First: 2025-01-08T14:38:03+00:00 · Latest: 2026-02-10T21:05:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.04538v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.04538v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we devise a new, general-purpose reinforcement learning strategy for the optimal control of parametric dynamical systems. Such problems frequently arise in applied sciences and engineering and entail a significant complexity when control and/or state variables are distributed in high-dimensional space or depend on varying parameters. Traditional numerical methods, relying on either iterative minimization algorithms -- exploiting, e.g., the solution of the adjoint problem -- or dynamic programming -- also involving the solution of the Hamilton-Jacobi-Bellman (HJB) equation -- while reliable, often become computationally infeasible. In this paper, we propose HypeRL a deep reinforcement learning (DRL) framework to overcome the limitations shown by traditional methods. HypeRL aims at approximating the optimal control policy directly. Specifically, we employ an actor-critic DRL approach to learn an optimal feedback control strategy that can generalize across the range of variation of the parameters. To effectively learn such optimal control laws for different instances of the parameters, encoding the parameter information into the DRL policy and value function neural networks (NNs) is essential. HypeRL uses two additional NNs, called hypernetworks, to learn the weights and biases of the value function and the policy NNs. In this way, HypeRL effectively embeds the parametric information into the value function and policy. We validate the proposed approach on two parametric control problems, namely (I) a 1D parametric Kuramoto-Sivashinsky equation with in-domain control, and (ii) a navigation problem of particle dynamics in a parametric 2D gyre flow. We show that the knowledge of physical and task-dependent information and the encoding of this information via a hypernetwork, are essential ingredients for learning parameter-dependent control policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HypeRL：基于超网络的强化学习用于参数化动态系统控制</div>
<div class="mono" style="margin-top:8px">本研究提出一种新的通用强化学习策略，用于参数化动态系统的最优控制。此类问题常见于应用科学与工程领域，当控制变量和/或状态变量分布于高维空间或依赖变化参数时，计算复杂度显著增加。传统数值方法（如依赖伴随问题求解的迭代最小化算法，或涉及哈密顿-雅可比-贝尔曼方程求解的动态规划）虽可靠，但常面临计算不可行的困境。本文提出HypeRL——一种深度强化学习框架以突破传统方法的局限。该框架旨在直接逼近最优控制策略：采用演员-评论家架构学习可泛化于参数变化范围的最优反馈控制策略。为有效学习不同参数实例下的最优控制律，将参数信息编码至DRL策略网络与价值函数神经网络至关重要。HypeRL引入两个称为超网络的附加神经网络，分别学习价值函数网络与策略网络的权重与偏置，从而实现参数信息在价值函数与策略中的有效嵌入。我们在两个参数控制问题上验证了该方法的有效性：（I）具有域内控制的一维参数化Kuramoto-Sivashinsky方程；（II）参数化二维环流场中的粒子动力学导航问题。实验表明，物理与任务相关信息的认知及其通过超网络的编码机制，是学习参数依赖型控制策略的关键要素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces HypeRL, a deep reinforcement learning framework designed to address the computational challenges of optimal control for parametric dynamical systems, which are common in engineering and applied sciences. The method employs an actor-critic approach enhanced with hypernetworks, which are additional neural networks that generate the weights and biases for the policy and value function networks, thereby embedding parameter information directly into the control strategy. Experimental validation on a 1D Kuramoto-Sivashinsky equation and a 2D gyre flow navigation problem demonstrates that this hypernetwork-based encoding is crucial for learning effective control policies that generalize across varying parameters.</div>
<div class="mono" style="margin-top:8px">本文提出了HypeRL，这是一种深度强化学习框架，旨在解决参数化动力系统最优控制中的计算难题，这类问题在工程和应用科学中常见。该方法采用演员-评论家架构，并结合超网络——即额外的神经网络来生成策略网络和价值函数网络的权重与偏置，从而将参数信息直接嵌入控制策略中。在1D Kuramoto-Sivashinsky方程和2D环流导航问题上的实验验证表明，这种基于超网络的编码对于学习能够泛化到不同参数的有效控制策略至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Evict from Key-Value Cache</div>
<div class="meta-line">Authors: Luca Moschella, Laura Manduchi, Ozan Sener</div>
<div class="meta-line">First: 2026-02-10T19:34:15+00:00 · Latest: 2026-02-10T19:34:15+00:00</div>
<div class="meta-line">Comments: 23 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10238v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10238v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token&#x27;s future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习从键值缓存中淘汰数据</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）规模的增长使高效推理面临挑战，主要源于自回归键值（KV）缓存的内存需求。现有淘汰或压缩方法虽能降低成本，但依赖启发式策略（如时效性或历史注意力分数），这些仅作为标记未来效用的间接代理并引入计算开销。我们将KV缓存淘汰重构为强化学习（RL）问题：通过学习预测标记对未来解码的效用进行排序。为此，我们提出KV策略（KVP）框架——一种轻量级逐头RL代理，仅使用键值向量在预计算生成轨迹上训练。每个代理学习以未来效用为指导的专用淘汰策略，可评估所有缓存预算下的排序质量，且无需修改底层LLM或增加推理开销。在长上下文基准RULER和多轮对话基准OASST2-4k上对两类模型族的评估表明，KVP显著优于基线方法。此外，在标准下游任务（如LongBench、BOOLQ、ARC）上的零样本测试显示，KVP能良好泛化至训练分布之外及更长上下文场景。这些结果证明，学习预测标记未来效用是实现自适应KV缓存管理的强大且可扩展范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to address the high memory cost of the Key-Value (KV) cache in Large Language Model inference by moving beyond heuristic eviction methods. The method reframes cache eviction as a reinforcement learning problem, introducing KV Policy (KVP), a framework of lightweight per-head agents trained on pre-computed traces to rank tokens by predicted future utility without modifying the base model. Main experimental results show that KVP significantly outperforms baselines on long-context and multi-turn dialogue benchmarks, and demonstrates strong zero-shot generalization to downstream tasks and longer contexts.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大语言模型推理中键值缓存内存消耗高的问题，其动机是超越依赖近期性或注意力分数等启发式的缓存淘汰方法。该方法将缓存淘汰重构为强化学习问题，提出了KV Policy（KVP）框架，通过轻量级的每头智能体在预计算轨迹上训练，以根据预测的未来效用对令牌进行排序，且无需修改基础模型。主要实验结果表明，KVP在长上下文和多轮对话基准测试上显著优于基线方法，并在下游任务和更长上下文上展现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Bayesian Optimization for Networked Black-Box Systems: A Path to Greener Profits and Smarter Designs</div>
<div class="meta-line">Authors: Akshay Kudva, Wei-Ting Tang, Joel A. Paulson</div>
<div class="meta-line">First: 2025-02-19T21:49:05+00:00 · Latest: 2026-02-10T19:20:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.14121v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.14121v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing modern industrial systems requires balancing several competing objectives, such as profitability, resilience, and sustainability, while accounting for complex interactions between technological, economic, and environmental factors. Multi-objective optimization (MOO) methods are commonly used to navigate these tradeoffs, but selecting the appropriate algorithm to tackle these problems is often unclear, particularly when system representations vary from fully equation-based (white-box) to entirely data-driven (black-box) models. While grey-box MOO methods attempt to bridge this gap, they typically impose rigid assumptions on system structure, requiring models to conform to the underlying structural assumptions of the solver rather than the solver adapting to the natural representation of the system of interest. In this chapter, we introduce a unifying approach to grey-box MOO by leveraging network representations, which provide a general and flexible framework for modeling interconnected systems as a series of function nodes that share various inputs and outputs. Specifically, we propose MOBONS, a novel Bayesian optimization-inspired algorithm that can efficiently optimize general function networks, including those with cyclic dependencies, enabling the modeling of feedback loops, recycle streams, and multi-scale simulations - features that existing methods fail to capture. Furthermore, MOBONS incorporates constraints, supports parallel evaluations, and preserves the sample efficiency of Bayesian optimization while leveraging network structure for improved scalability. We demonstrate the effectiveness of MOBONS through two case studies, including one related to sustainable process design. By enabling efficient MOO under general graph representations, MOBONS has the potential to significantly enhance the design of more profitable, resilient, and sustainable engineering systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向网络化黑箱系统的多目标贝叶斯优化：通往绿色利润与智能设计的路径</div>
<div class="mono" style="margin-top:8px">设计现代工业系统需在盈利性、韧性与可持续性等多个竞争目标间取得平衡，同时兼顾技术、经济和环境因素间的复杂交互。多目标优化方法常用于权衡这些目标，但针对系统模型从完全基于方程的白箱到完全数据驱动的黑箱等不同表征形式，如何选择合适的算法往往并不明确。现有灰箱多目标优化方法试图弥合这一差距，但通常对系统结构施加刚性假设，要求模型适应求解器的底层结构假设，而非求解器适配系统的自然表征。本章提出一种基于网络表征的统一化灰箱多目标优化方法，通过将互联系统建模为共享多输入输出的函数节点网络，提供通用灵活的框架。我们特别提出MOBONS算法——一种受贝叶斯优化启发的新方法，能高效优化包含循环依赖的通用函数网络，从而建模反馈回路、循环流与多尺度仿真等现有方法无法捕捉的特征。MOBONS还支持约束处理、并行评估，在保持贝叶斯优化样本效率的同时利用网络结构提升可扩展性。通过可持续过程设计等两个案例研究验证了MOBONS的有效性。该算法在通用图表征下实现高效多目标优化，有望显著提升工程系统在盈利性、韧性与可持续性方面的设计水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The chapter addresses the challenge of balancing competing objectives like profitability and sustainability in complex industrial systems, where existing multi-objective optimization methods often impose rigid structural assumptions. It introduces MOBONS, a Bayesian optimization-inspired algorithm that uses flexible network representations to model interconnected systems, including those with cyclic dependencies, while incorporating constraints and supporting parallel evaluations. Experimental results from case studies, including sustainable process design, demonstrate that MOBONS efficiently optimizes such systems, enhancing scalability and enabling more profitable and sustainable engineering designs.</div>
<div class="mono" style="margin-top:8px">本章针对复杂工业系统中平衡盈利性与可持续性等多目标优化的挑战，现有方法常强加僵化的结构假设。提出了MOBONS算法，这是一种基于贝叶斯优化的新方法，利用灵活的网络表示来建模互联系统，包括具有循环依赖性的情况，同时纳入约束并支持并行评估。通过可持续过程设计等案例研究，实验结果表明MOBONS能高效优化此类系统，提升可扩展性，助力实现更盈利、更具韧性的工程系统设计。</div>
</details>
</div>
<div class="card">
<div class="title">Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models</div>
<div class="meta-line">Authors: Shiting Huang, Zecheng Li, Yu Zeng, Qingnan Ren, Zhen Fang, Qisheng Su, Kou Shi, Lin Chen, Zehui Chen, Feng Zhao</div>
<div class="meta-line">First: 2026-02-10T19:16:09+00:00 · Latest: 2026-02-10T19:16:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10224v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model&#x27;s parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM&#x27;s self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM&#x27;s parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将元经验内化至记忆以指导大语言模型的强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为提升大语言模型（LLM）推理能力的有效方法。尽管效果显著，RLVR仍面临元学习瓶颈：其缺乏人类学习循环中除练习与验证外固有的错误归因和经验内化机制，从而限制了细粒度信用分配与可复用知识构建。我们将这种从过往错误中衍生的可复用知识表征称为元经验。基于此洞见，我们提出元经验学习（MEL）框架，将自蒸馏的元经验融入模型的参数化记忆。在标准RLVR基础上，我们引入一项额外设计：利用LLM的自验证能力对正确与错误推理轨迹进行对比分析，定位推理错误产生的精确分岔点，并将其归纳为可泛化的元经验。通过最小化负对数似然，元经验被进一步内化至LLM的参数化记忆，由此产生语言建模的奖励信号，桥接正确与错误推理轨迹，促进有效的知识复用。实验结果表明，MEL在基准测试中取得稳定提升，在不同规模模型上实现3.92%--4.73%的Pass@1增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the meta-learning bottleneck in Reinforcement Learning with Verifiable Rewards (RLVR), which lacks mechanisms for error attribution and reusable knowledge formation, this paper proposes the Meta-Experience Learning (MEL) framework to internalize distilled meta-experience into an LLM&#x27;s parametric memory. The method extends RLVR by using the LLM&#x27;s self-verification to conduct contrastive analysis on correct and incorrect reasoning trajectories, pinpoint error bifurcation points, and summarize them into generalizable meta-experience, which is then internalized via negative log-likelihood minimization to create a language-modeled reward signal. Experimental results show that MEL consistently improves performance, achieving Pass@1 gains of 3.92% to 4.73% across benchmarks with varying model sizes.</div>
<div class="mono" style="margin-top:8px">本文的动机是针对可验证奖励的强化学习（RLVR）存在的元学习瓶颈，即缺乏错误归因和可重用知识形成的机制，因此提出了元经验学习（MEL）框架，将提炼的元经验内化到大语言模型的参数记忆中。该方法在RLVR基础上，利用大语言模型的自我验证能力对正确和错误推理轨迹进行对比分析，定位错误分叉点，并将其总结为可泛化的元经验，然后通过最小化负对数似然将其内化，以产生语言建模的奖励信号。实验结果表明，MEL在不同模型规模的基准测试中均取得了性能提升，Pass@1指标获得了3.92%至4.73%的增长。</div>
</details>
</div>
<div class="card">
<div class="title">Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Nicolò Dal Fabbro, Milad Mesbahi, Renato Mendes, João Borges de Sousa, George J. Pappas</div>
<div class="meta-line">First: 2025-10-03T22:08:08+00:00 · Latest: 2026-02-10T19:00:17+00:00</div>
<div class="meta-line">Comments: Accepted at the 2026 IEEE International Conference on Robotics and Automation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03534v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.03534v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多智能体强化学习的杜罗河羽流长期测绘研究</div>
<div class="mono" style="margin-top:8px">本研究针对杜罗河典型应用场景，探讨利用多台自主水下航行器（AUV）对河流羽流进行长期（多日）测绘的问题。我们提出一种节能且通信高效的多智能体强化学习方法，其中中央协调器间歇性与AUV通信，收集测量数据并下达指令。该方法将时空高斯过程回归（GPR）与多头Q网络控制器相结合，用于调控每台AUV的航向与航速。基于Delft3D海洋模型的仿真实验表明，本方法在均方误差（MSE）和作业续航力方面均持续优于单智能体及多智能体基准方案。增加智能体数量可同时提升MSE精度与续航性能，在某些案例中，AUV数量翻倍可使续航力提升超一倍，同时保持或提高测绘精度，凸显了多智能体协同的优势。所学习的策略能够泛化至不同年份、月份中未见的季节性水文模式，为未来数据驱动的动态羽流环境长期监测技术发展提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient long-term monitoring of dynamic river plumes like the Douro River, this paper proposes a multi-agent reinforcement learning method where a central coordinator intermittently communicates with autonomous underwater vehicles (AUVs) to collect data and issue commands. The method integrates spatiotemporal Gaussian process regression with a multi-head Q-network controller to regulate each AUV&#x27;s direction and speed, aiming for energy and communication efficiency. Experimental simulations using the Delft3D ocean model show that this approach consistently outperforms single- and multi-agent benchmarks, improving mean squared error and operational endurance; notably, doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, and the learned policies generalize effectively across unseen seasonal conditions.</div>
<div class="mono" style="margin-top:8px">针对杜罗河等动态河流羽流长期高效监测的需求，本文提出了一种多智能体强化学习方法，其中中央协调器间歇性地与自主水下航行器（AUV）通信以收集数据和发出指令。该方法将时空高斯过程回归与多头Q网络控制器相结合，以调节每个AUV的方向和速度，旨在实现能源和通信效率。基于Delft3D海洋模型的仿真实验表明，该方法在均方误差和操作续航方面持续优于单智能体和多智能体基准；值得注意的是，将AUV数量加倍可使续航时间增加一倍以上，同时保持或提高精度，且学习到的策略能有效泛化到未见过的季节性条件中。</div>
</details>
</div>
<div class="card">
<div class="title">Anagent For Enhancing Scientific Table &amp; Figure Analysis</div>
<div class="meta-line">Authors: Xuehang Guo, Zhiyong Lu, Tom Hope, Qingyun Wang</div>
<div class="meta-line">First: 2026-02-10T18:46:28+00:00 · Latest: 2026-02-10T18:46:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10081v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10081v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xhguo7.github.io/Anagent/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \&amp; figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \&amp; figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \&amp; figure analysis. Our project page: https://xhguo7.github.io/Anagent/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于增强科学图表分析能力的Anagent框架</div>
<div class="mono" style="margin-top:8px">在科学研究中，分析工作需要准确解读复杂的多模态知识，整合不同来源的证据，并基于领域专业知识进行推理。然而，当前人工智能系统难以稳定展现此类能力。科学图表在复杂性和多样性方面的挑战，加之异构结构和长上下文需求，构成了科学图表分析的根本障碍。为量化这些挑战，我们提出了AnaBench大规模基准数据集，涵盖九个科学领域的63,178个实例，并沿七个复杂度维度进行系统分类。为应对这些挑战，我们提出Anagent多智能体框架，通过四个专项智能体增强科学图表分析能力：规划器将任务分解为可执行的子任务，专家通过定向工具执行检索任务相关信息，求解器综合信息生成连贯分析，评审器通过五维质量评估进行迭代优化。我们进一步开发了模块化训练策略，利用监督微调和专项强化学习优化个体能力，同时保持高效协作。在170个子领域的综合评估表明，Anagent实现了显著提升——在免训练设置中最高提升13.43%，经微调后最高提升42.12%，同时揭示面向任务的推理和上下文感知的问题解决能力对高质量科学图表分析至关重要。项目页面：https://xhguo7.github.io/Anagent/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for AI systems that can accurately interpret complex multimodal scientific knowledge from tables and figures, which current models struggle with due to their variability and long-context demands, this paper introduces Anagent, a multi-agent framework designed to enhance analysis. The method employs four specialized agents—Planner, Expert, Solver, and Critic—that decompose tasks, retrieve information, synthesize analyses, and iteratively refine outputs, supported by modular training strategies including supervised finetuning and reinforcement learning. Experimental results on the AnaBench benchmark, comprising 63,178 instances across nine domains, show that Anagent achieves significant improvements, with gains up to 13.43% in training-free settings and 42.12% with finetuning, demonstrating the importance of task-oriented reasoning and context-aware problem-solving for high-quality scientific analysis.</div>
<div class="mono" style="margin-top:8px">本文的动机在于当前人工智能系统难以准确解释科学图表中复杂的多模态知识，这些图表因异构结构和长上下文需求而构成分析障碍。为此，研究提出了Anagent，一个多智能体框架，通过规划者、专家、求解者和批评者四个专门代理来分解任务、检索信息、合成分析并进行迭代优化，并采用监督微调和强化学习的模块化训练策略以优化协作能力。在包含九个科学领域63,178个实例的AnaBench基准测试中，实验结果表明Anagent取得了显著提升，在无需训练的情况下性能提高达13.43%，经过微调后提升达42.12%，揭示了面向任务的推理和上下文感知的问题解决对于高质量科学图表分析至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability</div>
<div class="meta-line">Authors: Aaditya Vikram Prasad, Connor Watts, Jack Merullo, Dhruvil Gala, Owen Lewis, Thomas McGrath, Ekdeep Singh Lubana</div>
<div class="meta-line">First: 2026-02-10T18:33:45+00:00 · Latest: 2026-02-10T18:33:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10067v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>特征即奖励：通过可解释性为开放式任务提供可扩展的监督</div>
<div class="mono" style="margin-top:8px">在大规模数据集上训练的语言模型已被证明能学习编码抽象概念（如事实性或意图）的特征。这些特征传统上用于测试时监控或引导。我们提出一种替代性功能：将特征作为开放式任务的可扩展监督。我们以减少幻觉这一理想但开放的行为为例，设计了一个强化学习（RL）流程，称为RLFR（基于特征奖励的强化学习），将特征用作奖励函数。基于一个识别候选幻觉主张的新型探测框架，该流程教导模型在不确定其事实性时进行干预并修正其生成结果。此外，该流程通过奖励特征引导，实现了可扩展的测试时计算。在Gemma-3-12B-IT模型上实施的这一端到端流程，产生了一个策略，其幻觉概率比原始模型降低58%，同时保持标准基准测试的性能。综上所述，通过将监督建立在特征语言的基础上，本文引入了一种利用可解释性学习开放式任务的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of providing scalable supervision for open-ended tasks like reducing hallucinations in language models, where traditional reward specification is difficult. The method, Reinforcement Learning from Feature Rewards (RLFR), repurposes interpretable features—such as those encoding factuality—as intrinsic reward functions within a reinforcement learning pipeline; this includes a probing framework to identify potential hallucinations and guide model intervention. Experimental results on Gemma-3-12B-IT show the trained policy reduces hallucination likelihood by 58% compared to the base model while maintaining performance on standard benchmarks, demonstrating a novel paradigm for using interpretability to learn open-ended behaviors.</div>
<div class="mono" style="margin-top:8px">本文的动机是为减少语言模型幻觉等开放任务提供可扩展的监督，因为传统奖励设定在此类任务中较为困难。方法上，提出了基于特征奖励的强化学习（RLFR），将可解释性特征（如事实性特征）重新用作强化学习流程中的内在奖励函数，并通过探测框架识别潜在幻觉以指导模型干预。在Gemma-3-12B-IT上的实验结果表明，训练后的策略相比原始模型将幻觉可能性降低了58%，同时在标准基准测试上保持了性能，这为利用可解释性学习开放任务引入了一种新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction</div>
<div class="meta-line">Authors: Abdul Matin, Rupasree Dey, Tanjim Bin Faruk, Shrideep Pallickara, Sangmi Lee Pallickara</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-13T19:59:04+00:00 · Latest: 2026-02-10T18:18:19+00:00</div>
<div class="meta-line">Comments: Accepted to the KGML Bridge at AAAI 2026 (non-archival)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12445v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12445v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识引导的掩码自编码器：线性光谱混合与光谱角感知重建</div>
<div class="mono" style="margin-top:8px">将领域知识融入深度学习已成为提升模型可解释性、泛化能力和数据效率的重要方向。本研究提出一种基于视觉Transformer的知识引导掩码自编码器，将科学领域知识嵌入自监督重建过程。该方法不仅依赖数据驱动优化，还引入线性光谱混合模型作为物理约束，并结合基于物理的光谱角制图器，确保学习到的表征符合观测信号与潜在成分间的已知结构关系。该框架将线性光谱混合损失和光谱角损失与传统Huber损失目标联合优化，在特征空间中同时促进数值精度与几何一致性。这种知识引导设计增强了重建保真度，在有限监督下稳定训练过程，并产生基于物理原理的可解释潜在表征。实验结果表明，所提模型显著提升重建质量并改善下游任务性能，彰显了在基于Transformer的自监督学习中嵌入物理归纳偏置的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to integrate domain knowledge into deep learning for improved interpretability, generalization, and data efficiency, this paper introduces a knowledge-guided Vision Transformer (ViT) Masked Autoencoder. The method incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and uses a Spectral Angle Mapper (SAM) loss alongside a Huber loss to ensure representations adhere to known structural relationships between signals and latent components, promoting both numerical accuracy and geometric consistency. Experimental results demonstrate that the model significantly enhances reconstruction quality and downstream task performance, showing the value of embedding physics-informed biases in self-supervised learning.</div>
<div class="mono" style="margin-top:8px">本研究旨在将领域知识融入深度学习，以提升模型的可解释性、泛化能力和数据效率，提出了一种基于知识引导的视觉Transformer掩码自编码器。该方法将线性光谱混合模型作为物理约束，并结合基于光谱角制图器的损失与Huber损失进行联合优化，确保学习到的表征符合观测信号与潜在成分之间的已知结构关系，从而促进特征空间的数值准确性和几何一致性。实验结果表明，该模型显著提高了重建质量和下游任务性能，凸显了在基于Transformer的自监督学习中嵌入物理信息归纳偏置的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization</div>
<div class="meta-line">Authors: Xinchen Han, Hossam Afifi, Michel Marot, Xilu Wang, Lu Yin</div>
<div class="meta-line">Venue: ICASSP</div>
<div class="meta-line">First: 2026-02-10T18:15:58+00:00 · Latest: 2026-02-10T18:15:58+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10048v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10048v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \textbf{F}ine-grained \textbf{G}roup policy \textbf{O}ptimization (\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于细粒度分组策略优化的长链思维压缩</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）常生成不必要的冗长链式思维（CoT）推理，这会增加计算成本和延迟，却未带来相应的性能提升。本文提出细粒度分组策略优化（FGO），一种强化学习（RL）算法，通过将分组响应细分为子组，并根据长度和熵分配适当权重，从而实现有效的CoT压缩。同时，作为分组相对策略优化（GRPO）的增强变体，FGO成功解决了GRPO的两个主要局限：数据利用率低和熵崩溃。我们在多个推理LLM和基准测试（包括MATH500、AIME24、AMC23和Minerva）上评估FGO。实验结果表明，FGO在不降低性能的情况下实现了高效的CoT压缩，并同时解决了GRPO的关键局限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that large language models often produce excessively verbose chain-of-thought reasoning, which increases computational cost and latency without proportional performance benefits. The method introduces Fine-grained Group policy Optimization (FGO), a reinforcement learning algorithm that compresses reasoning chains by subdividing group responses and weighting them based on length and entropy, while also addressing inefficiencies and entropy collapse issues in the prior Group Relative Policy Optimization. Main experimental results on benchmarks including MATH500, AIME24, AMC23, and Minerva demonstrate that FGO achieves efficient compression of reasoning steps without degrading model performance and successfully resolves the key limitations of its predecessor.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于大型语言模型常生成不必要的冗长思维链推理，这会增加计算成本和延迟，却未带来相应的性能提升。方法上提出了细粒度分组策略优化，这是一种强化学习算法，通过细分组响应并依据长度和熵分配适当权重来实现有效的思维链压缩，同时解决了先前分组相对策略优化中数据利用效率低和熵崩溃两大局限。在MATH500、AIME24、AMC23和Minerva等多个推理基准上的实验结果表明，该方法能在不降低性能的前提下实现高效的思维链压缩，并成功克服了前代方法的关键缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Akshay Mete, Shahid Aamir Sheikh, Tzu-Hsiang Lin, Dileep Kalathil, P. R. Kumar</div>
<div class="meta-line">First: 2026-02-10T18:11:00+00:00 · Latest: 2026-02-10T18:11:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10044v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观世界模型：基于模型的深度强化学习中的高效探索</div>
<div class="mono" style="margin-top:8px">高效探索仍是强化学习（RL）的核心挑战，尤其在稀疏奖励环境中。我们提出乐观世界模型（OWMs），这是一种原则性且可扩展的乐观探索框架，将自适应控制中的经典奖励偏置最大似然估计（RBMLE）引入深度RL。与上置信界（UCB）类探索方法不同，OWMs通过添加乐观动力学损失直接融入模型学习，该损失使想象的状态转移偏向更高奖励结果。这种完全基于梯度的损失既无需不确定性估计，也无需约束优化。我们的方法可与现有世界模型框架即插即用，在保持可扩展性的同时仅需对标准训练流程进行最小改动。我们在两种先进世界模型架构中实例化了OWMs，构建出乐观DreamerV3和乐观STORM，相比基线模型在样本效率和累积回报上均展现出显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of efficient exploration in sparse-reward reinforcement learning by introducing Optimistic World Models (OWMs), a framework that integrates the classical reward-biased maximum likelihood estimation principle into deep RL. The method incorporates optimism directly into model learning through an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes, eliminating the need for uncertainty estimates or constrained optimization while remaining plug-and-play with existing world model architectures. Experimental results show that instantiations like Optimistic DreamerV3 and Optimistic STORM achieve significant improvements in sample efficiency and cumulative return compared to baseline models.</div>
<div class="mono" style="margin-top:8px">本文针对稀疏奖励强化学习中的高效探索挑战，提出了乐观世界模型框架，将经典的自适应控制中奖励偏置最大似然估计原理引入深度强化学习。该方法通过一个乐观动力学损失将乐观性直接融入模型学习，使想象的状态转移偏向更高奖励结果，无需不确定性估计或约束优化，并能即插即用地与现有世界模型架构兼容。实验结果表明，基于该框架实现的乐观DreamerV3和乐观STORM模型相比基线版本，在样本效率和累积回报上均取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Effectiveness of Binary Autoencoders for QUBO-Based Optimization Problems</div>
<div class="meta-line">Authors: Tetsuro Abe, Masashi Yamashita, Shu Tanaka</div>
<div class="meta-line">First: 2026-02-10T17:59:29+00:00 · Latest: 2026-02-10T17:59:29+00:00</div>
<div class="meta-line">Comments: 14 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10037v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In black-box combinatorial optimization, objective evaluations are often expensive, so high quality solutions must be found under a limited budget. Factorization machine with quantum annealing (FMQA) builds a quadratic surrogate model from evaluated samples and optimizes it on an Ising machine. However, FMQA requires binary decision variables, and for nonbinary structures such as integer permutations, the choice of binary encoding strongly affects search efficiency. If the encoding fails to reflect the original neighborhood structure, small Hamming moves may not correspond to meaningful modifications in the original solution space, and constrained problems can yield many infeasible candidates that waste evaluations. Recent work combines FMQA with a binary autoencoder (bAE) that learns a compact binary latent code from feasible solutions, yet the mechanism behind its performance gains is unclear. Using a small traveling salesman problem as an interpretable testbed, we show that the bAE reconstructs feasible tours accurately and, compared with manually designed encodings at similar compression, better aligns tour distances with latent Hamming distances, yields smoother neighborhoods under small bit flips, and produces fewer local optima. These geometric properties explain why bAE+FMQA improves the approximation ratio faster while maintaining feasibility throughout optimization, and they provide guidance for designing latent representations for black-box optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>二元自编码器在基于QUBO的优化问题中的有效性研究</div>
<div class="mono" style="margin-top:8px">在黑箱组合优化中，目标函数评估通常代价高昂，因此必须在有限预算内找到高质量解。基于量子退火的因子分解机（FMQA）通过评估样本构建二次代理模型，并在伊辛机上优化。然而FMQA要求二元决策变量，对于整数排列等非二元结构，二元编码的选择会显著影响搜索效率。若编码未能反映原始邻域结构，微小的汉明移动可能无法对应原始解空间的有效修改，且约束问题可能产生大量不可行候选解，造成评估资源浪费。近期研究将FMQA与二元自编码器（bAE）结合，后者从可行解中学习紧凑的二元潜码，但其性能提升机制尚不明确。本文以小型旅行商问题为可解释测试平台，证明bAE能准确重构可行路径，且在相同压缩率下相比人工设计的编码：能更好对齐路径距离与潜空间汉明距离、在小规模比特翻转下产生更平滑的邻域结构、生成更少的局部最优解。这些几何特性解释了为何bAE+FMQA能在保持全程可行性的同时更快提升近似比，并为黑箱优化的潜表示设计提供了指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the effectiveness of binary autoencoders (bAE) in enhancing quantum annealing-based optimization for black-box combinatorial problems, motivated by the challenge that manual binary encodings for non-binary structures like permutations can misrepresent neighborhood relationships and generate many infeasible solutions, wasting expensive evaluations. The method integrates a bAE with Factorization Machine using Quantum Annealing (FMQA), where the bAE learns a compact binary latent representation directly from feasible solutions to better preserve the original problem&#x27;s structure. Experimental results on a small traveling salesman problem demonstrate that the bAE-based encoding, compared to manual encodings at similar compression, more accurately reconstructs feasible tours, aligns tour distances with latent Hamming distances, creates smoother neighborhoods under small bit flips, and reduces local optima, leading to faster improvement in approximation ratio while maintaining feasibility throughout optimization.</div>
<div class="mono" style="margin-top:8px">本文研究了二元自编码器在增强基于量子退火的组合优化问题中的有效性，其动机在于，对于如排列等非二元结构，手动设计的二元编码可能无法反映原始邻域结构并产生大量不可行解，从而浪费昂贵的评估成本。该方法将二元自编码器与基于量子退火的因子分解机相结合，其中二元自编码器直接从可行解中学习紧凑的二元潜在表示，以更好地保留原始问题的结构。在小型旅行商问题上的实验结果表明，与压缩率相似的手动编码相比，基于二元自编码器的编码能更准确地重建可行路径，使路径距离与潜在汉明距离更一致，在小比特翻转下产生更平滑的邻域，并减少局部最优解，从而在优化过程中保持可行性的同时更快地提高近似比。</div>
</details>
</div>
<div class="card">
<div class="title">ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning</div>
<div class="meta-line">Authors: Qingnan Ren, Shiting Huang, Zhen Fang, Zehui Chen, Lin Chen, Lijun Li, Feng Zhao</div>
<div class="meta-line">First: 2026-02-10T17:40:39+00:00 · Latest: 2026-02-10T17:40:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce \textbf{ADORA} (\textbf{A}dvantage \textbf{D}ynamics via \textbf{O}nline \textbf{R}ollout \textbf{A}daptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function&#x27;s weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ADORA：基于动态优势估计的强化学习推理模型训练</div>
<div class="mono" style="margin-top:8px">强化学习已成为在数学解题到想象推理等复杂任务中开发推理模型的核心技术。这类模型的优化通常依赖策略梯度方法，其效果关键在于优势函数的准确估计。然而，现有方法多采用静态优势估计，这种做法因忽略训练样本随时间变化的动态效用，导致信用分配效率低下。该局限引发次优策略更新，进而表现为收敛速度减缓与学习不稳定性增加，因模型难以有效适应样本效用的动态演变。为解决此问题，我们提出\textbf{ADORA}（基于在线推演自适应的动态优势估计框架），一种创新的策略优化框架。ADORA通过在线模型推演过程中对训练样本动态效用的实时评估，自适应地将数据划分为暂时优势与劣势样本，从而动态调整优势函数的权重。这种定制化数据区分策略使ADORA能无缝集成到现有策略优化算法中，无需显著改动架构，使策略能优先从信息量更大的经验中学习，实现更高效的策略更新。跨多种模型系列与不同数据规模的广泛实验表明，ADORA是一个鲁棒且高效的框架。它在几何与数学任务中显著提升长链推理能力，无需敏感超参数调优即可持续取得显著性能增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the inefficiency of static advantage estimation in reinforcement learning for training reasoning models, which leads to suboptimal policy updates and slow convergence. The proposed method, ADORA, dynamically adjusts advantage function weighting by adaptively categorizing training samples based on their evolving utility during online rollouts, enabling seamless integration into existing policy optimization algorithms. Experimental results show that ADORA significantly enhances performance in geometric and mathematical reasoning tasks across diverse models and data scales, achieving robust gains without sensitive hyperparameter tuning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于强化学习中用于训练推理模型的静态优势估计效率低下，导致策略更新次优和收敛缓慢。提出的方法ADORA通过在线推演中根据样本效用动态分类训练数据，自适应调整优势函数权重，可无缝集成到现有策略优化算法中。实验结果表明，ADORA在几何和数学推理任务中显著提升了不同模型和数据规模下的性能，无需敏感超参数调优即实现了稳健的性能增益。</div>
</details>
</div>
<div class="card">
<div class="title">A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging</div>
<div class="meta-line">Authors: Bharathkumar Hegde, Melanie Bouroche</div>
<div class="meta-line">First: 2026-02-10T17:30:09+00:00 · Latest: 2026-02-10T17:30:09+00:00</div>
<div class="meta-line">Comments: Accepted in IEEE IV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10007v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10007v1">PDF</a> · <a href="https://github.com/hkbharath/MARL-MASS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拥堵匝道汇入场景下协同安全屏障保障CAV安全高效换道</div>
<div class="mono" style="margin-top:8px">密集交通中的换道是网联自动驾驶车辆面临的重要挑战。现有换道控制器主要侧重安全保障或协同提升交通效率，但未同时兼顾这两个冲突目标。为此，我们提出基于控制屏障函数设计的多智能体安全屏障，通过简单算法构建交互拓扑图来捕捉CAV间的多智能体交互，实现安全协同换道。进一步，通过集成MASS保障安全性并定义定制奖励函数以优先提升效率，扩展了先进的多智能体强化学习换道控制器。由此提出的MARL-MASS控制器在拥堵匝道汇入仿真中验证表明：MASS能通过严格遵循安全约束实现具有安全保障的协同换道；定制奖励函数提升了带安全屏障的MARL策略稳定性。总体而言，MARL-MASS在遵守安全约束的同时鼓励协同换道策略探索，有效平衡了拥堵交通中安全保障与效率提升的权衡。相关代码已在https://github.com/hkbharath/MARL-MASS开源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge for Connected and Autonomous Vehicles (CAVs) to perform lane changes in dense traffic by balancing the often conflicting objectives of safety and traffic efficiency. The method introduces the Multi-Agent Safety Shield (MASS), which uses Control Barrier Functions to enforce safety constraints through a graph-based interaction topology, and integrates it with a Multi-Agent Reinforcement Learning (MARL) controller enhanced with a custom reward function to prioritize efficiency. Experimental results from a congested on-ramp merging simulation show that the proposed MARL-MASS controller successfully guarantees safety by strictly adhering to constraints while also improving traffic efficiency and policy stability through collaborative maneuvers.</div>
<div class="mono" style="margin-top:8px">本文针对网联自动驾驶车辆在密集交通中变道时需平衡安全与通行效率这一冲突目标的挑战展开研究。方法上提出了多智能体安全屏障，利用控制屏障函数通过基于图的交互拓扑来保障安全，并将其与一个定制了奖励函数以优先提升效率的多智能体强化学习控制器相结合。在拥堵匝道汇入模拟中的实验结果表明，所提出的控制器能严格遵循安全约束保证安全，同时通过协同变道有效提高了交通效率并增强了策略的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning</div>
<div class="meta-line">Authors: Shijie Zhang, Xiang Guo, Rujun Guo, Shaoyu Liu, Xiaozhao Wang, Guanjun Jiang, Kevin Zhang</div>
<div class="meta-line">First: 2026-02-10T17:28:12+00:00 · Latest: 2026-02-10T17:28:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10006v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10006v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a &quot;Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)&quot; pipeline to achieve AFRL. However, directly applying existing RL training often leads to \textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards. From an information theory perspective: RL inherently minimizes the \textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to &quot;reward hacking.&quot; On the other hand, SFT minimizes the \textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules. Based on this insight, we propose a \textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance. Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>先答后析：通过模式平衡强化学习对齐搜索相关性</div>
<div class="mono" style="margin-top:8px">构建兼具低延迟与高性能的搜索相关性模型是搜索行业的长期挑战。为满足在线系统毫秒级响应需求，同时保留大语言模型（LLM）可解释的推理轨迹，我们提出创新的**先答后析（AFRL）**范式，要求模型在首个标记即输出确定性相关性分数，随后提供结构化逻辑解释。受推理模型成功启发，我们采用“监督微调（SFT）+强化学习（RL）”流程实现AFRL。然而，直接应用现有RL训练常导致搜索相关性任务中的**模式崩溃**——模型为追求高奖励而遗忘复杂长尾规则。从信息论视角看：RL本质最小化**反向KL散度**，倾向于寻找概率峰值（模式寻求），易引发“奖励破解”；而SFT最小化**前向KL散度**，迫使模型覆盖数据分布（模式覆盖），有效锚定专家规则。基于此洞见，我们提出**模式平衡优化**策略，在Stepwise-GRPO训练中引入SFT辅助损失以平衡两种特性。进一步构建自动化指令进化系统与多阶段课程学习机制保障专家级数据质量。大量实验表明，我们的320亿参数教师模型达到业界最优性能。AFRL架构还支持高效知识蒸馏，成功将专家级逻辑迁移至6亿参数模型，从而在推理深度与部署延迟间实现平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to balance low latency and high performance in search relevance systems, this paper introduces the Answer-First, Reason Later (AFRL) paradigm, which requires a model to output a relevance score immediately followed by a structured explanation. The method employs a Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) pipeline, enhanced with a Mode-Balanced Optimization strategy that combines an SFT auxiliary loss to prevent mode collapse—a common issue where RL overly focuses on high-reward patterns and neglects complex rules. Experimental results show that the proposed 32B teacher model achieves state-of-the-art performance, and through efficient knowledge distillation, the AFRL architecture successfully transfers expert-level reasoning to a compact 0.6B model, effectively reconciling reasoning depth with deployment speed.</div>
<div class="mono" style="margin-top:8px">本文旨在解决搜索相关性系统中低延迟与高性能难以兼顾的长期挑战，提出了“先回答，后推理”（AFRL）的新范式，要求模型首先生成相关性分数，随后提供结构化解释。方法上采用监督微调（SFT）与强化学习（RL）结合的流程，并引入一种模式平衡优化策略，通过加入SFT辅助损失来避免模式崩溃问题——即RL过度追求高奖励而忽略复杂长尾规则。实验结果表明，所提出的320亿参数教师模型实现了最先进的性能，且AFRL架构通过高效的知识蒸馏，成功将专家级逻辑迁移至一个6亿参数的小模型中，从而在推理深度与部署延迟之间取得了良好平衡。</div>
</details>
</div>
<div class="card">
<div class="title">ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference</div>
<div class="meta-line">Authors: Junda Wang, Zhichao Yang, Dongxu Zhang, Sanjit Singh Batra, Robert E. Tillman</div>
<div class="meta-line">First: 2026-02-10T17:27:26+00:00 · Latest: 2026-02-10T17:27:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10004v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10004v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated &lt;stop&gt; signals, and (iii) &lt;stop&gt;-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESTAR：面向高效推理的早期停止令牌感知推理方法</div>
<div class="mono" style="margin-top:8px">大型推理模型通过生成长链思维实现最优性能，但常在得出正确答案后继续冗余推理，造成计算浪费。本文提出早期停止令牌感知推理方法，通过检测并减少此类冗余推理，在不牺牲准确性的前提下提升效率。该方法结合：（1）基于轨迹的分类器，识别可安全停止推理的时机；（2）监督微调，指导模型生成自发的&lt;停止&gt;信号；（3）具备停止感知的强化学习，在自生成停止点截断推演过程，并引入计算感知奖励机制。在四个推理数据集上的实验表明，ESTAR将推理长度缩短约3.7倍（从4799步降至1290步），同时保持准确性（74.9%对比74.2%），并展现出强大的跨领域泛化能力。这些结果证明早期停止是提升大型推理模型效率的简洁而有效的机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational inefficiency of large reasoning models that generate lengthy chains-of-thought even after reaching correct answers, this paper introduces ESTAR, a method for early-stopping token-aware reasoning. The approach combines a trajectory-based classifier to detect when reasoning can be safely halted, supervised fine-tuning to teach models to propose self-generated stop signals, and stop-aware reinforcement learning with compute-aware rewards to truncate rollouts. Experimental results on four reasoning datasets demonstrate that ESTAR reduces average reasoning length by approximately 3.7 times (from 4,799 to 1,290 tokens) while maintaining comparable accuracy (74.9% vs. 74.2%), and shows strong cross-domain generalization, highlighting early stopping as an effective mechanism for improving inference efficiency.</div>
<div class="mono" style="margin-top:8px">针对大型推理模型在得出正确答案后仍生成冗长思维链导致计算效率低下的问题，本文提出了ESTAR方法，旨在实现基于令牌感知的早期停止推理。该方法融合了三种技术：基于轨迹的分类器用于识别可安全停止推理的时机，监督微调训练模型自主生成停止信号，以及结合计算感知奖励的停止感知强化学习以在停止点截断推理过程。在四个推理数据集上的实验结果表明，ESTAR将平均推理长度减少了约3.7倍（从4799个令牌降至1290个），同时保持了相当的准确率（74.9%对比74.2%），并展现出强大的跨领域泛化能力，证明了早期停止是提升推理效率的有效机制。</div>
</details>
</div>
<div class="card">
<div class="title">Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</div>
<div class="meta-line">Authors: Brennen A. Hill</div>
<div class="meta-line">First: 2025-11-04T04:07:16+00:00 · Latest: 2026-02-10T16:34:37+00:00</div>
<div class="meta-line">Comments: National Science Foundation (NSF) workshop on Brain-Inspired Dynamics for Engineering Energy-Efficient Circuits and Artificial Intelligence</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02241v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.02241v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell&#x27;s actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network&#x27;s intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network&#x27;s parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构可塑性作为主动推理：一种受生物学启发的稳态控制架构</div>
<div class="mono" style="margin-top:8px">传统神经网络虽功能强大，却依赖全局反向传播等生物学上难以实现的机制。本文提出结构自适应预测推理网络（SAPIN），该计算模型受主动推理原理及生物神经培养物形态可塑性的启发。SAPIN在二维网格上运行，其处理单元（细胞）通过最小化局部预测误差进行学习。模型包含两个并行的核心学习机制：一是基于细胞实际激活与学习期望间时序差异的局部类赫布突触可塑性规则；二是细胞在网格中物理迁移以优化信息接收场的结构可塑性机制。这种双重方法使网络既能学习信息处理方式（突触权重），也能优化计算资源布局（网络拓扑）。我们在经典Cart Pole强化学习基准上验证了SAPIN模型。结果表明该架构能成功解决CartPole任务，获得稳健性能。网络最小化预测误差与维持稳态的内在驱动力足以发现稳定的平衡策略。研究发现：持续学习会导致不稳定，但在成功后锁定网络参数可获得稳定策略。对锁定后的网络进行100轮测试（重复100个成功智能体），锁定网络平均保持82%的成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the biological implausibility of global backpropagation in traditional neural networks, this paper proposes the Structurally Adaptive Predictive Inference Network (SAPIN), a model inspired by active inference and morphological plasticity. The method employs a 2D grid where cells learn via local prediction error minimization, combining a Hebbian-like synaptic plasticity rule with a structural mechanism allowing cells to migrate to optimize their receptive fields, thus learning both synaptic weights and network topology. Experimental results on the CartPole benchmark show that SAPIN successfully solves the task through homeostatic error minimization, achieving robust performance; after locking parameters post-success, networks maintained an average 82% success rate over 100 episodes across 100 agents.</div>
<div class="mono" style="margin-top:8px">本文的动机在于传统神经网络依赖全局反向传播这一生物学上不合理的机制，因此提出了结构自适应预测推理网络（SAPIN），该模型受主动推理和生物神经形态可塑性启发。方法采用二维网格，其中细胞通过最小化局部预测误差来学习，结合了类赫布突触可塑性规则和允许细胞迁移以优化感受野的结构机制，从而同时学习突触权重和网络拓扑。在CartPole基准测试中的实验结果表明，SAPIN通过稳态误差最小化成功解决了任务，实现了稳健性能；在成功锁定参数后，网络在100个代理上经过100次测试平均保持了82%的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Jie Xiao, Meng Chen, Qingnan Ren, Jingwei Song, Jiaqi Huang, Yangshen Deng, Chris Tong, Wanyi Chen, Suli Wang, Ziqian Bi, Shuo Lu, Yiqun Duan, Xu Wang, Rymon Yu, Ween Yang, Lynn Ai, Eric Yang, Bill Shi</div>
<div class="meta-line">First: 2026-02-02T14:57:53+00:00 · Latest: 2026-02-10T15:56:18+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02192v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.02192v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ECHO-2：一种用于高效成本强化学习的大规模分布式推演框架</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是大语言模型（LLM）后训练的关键阶段，涉及推演生成、奖励评估与集中式学习的反复交互。分布式推演执行提供了利用更具成本效益的推理资源的机会，但也带来了广域协调与策略传播的挑战。本文提出ECHO-2，一种面向后训练的分布式RL框架，支持远程推理工作节点并处理不可忽略的传播延迟。ECHO-2结合集中式学习与分布式推演，将有限策略陈旧性作为用户可控参数，使推演生成、传播与训练能够重叠执行。我们提出一种基于重叠的容量模型，关联训练时间、传播延迟与推演吞吐量，为维持学习器利用率提供了实用的资源配置规则。为缓解传播瓶颈并降低成本，ECHO-2采用对等辅助流水线广播及异构工作节点的成本感知激活机制。在真实广域带宽环境下对4B和8B模型进行GRPO后训练的实验中，ECHO-2在保持与强基线相当的RL奖励的同时，显著提升了成本效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for ECHO-2 stems from the need to reduce the high costs of reinforcement learning (RL) for large language models (LLMs) by distributing the computationally intensive rollout generation to more cost-efficient, remote inference resources, despite the challenges of coordination and policy update delays in wide-area networks. The method introduces a framework that decouples centralized learning from distributed rollouts, explicitly models and manages policy staleness as a tunable parameter to overlap rollout, dissemination, and training phases, and employs an overlap-based capacity model for system provisioning alongside peer-assisted pipelined broadcast and cost-aware worker activation to optimize performance. The main experimental results, from GRPO post-training of 4B and 8B parameter models under realistic wide-area bandwidth conditions, demonstrate that ECHO-2 achieves significantly improved cost efficiency while maintaining RL reward outcomes comparable to strong baseline systems.</div>
<div class="mono" style="margin-top:8px">ECHO-2的研发动机是为了降低大语言模型强化学习阶段的高昂成本，其核心思路是将耗时的策略推演生成分布到更具成本效益的远程推理资源上执行，但这在广域网中引入了协调与策略分发延迟的挑战。该方法提出了一个框架，将集中式学习与分布式推演解耦，将有限的政策陈旧性作为用户可控参数进行建模和管理，从而实现推演、分发和训练的重叠执行；该框架还引入了基于重叠的容量模型进行资源供给规划，并采用对等辅助的流水线广播和成本感知的异构工作节点激活来优化系统。在真实广域网带宽环境下对40亿和80亿参数模型进行的GRPO后训练实验结果表明，ECHO-2在保持与强基线模型相当的强化学习奖励性能的同时，显著提升了成本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis</div>
<div class="meta-line">Authors: James Rudd-Jones, Mirco Musolesi, María Pérez-Ortiz</div>
<div class="meta-line">First: 2025-04-17T09:18:04+00:00 · Latest: 2026-02-10T15:30:59+00:00</div>
<div class="meta-line">Comments: Published in AAMAS&#x27;25 Blue Sky Ideas Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.12777v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.12777v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate policy development faces significant challenges due to deep uncertainty, complex system dynamics, and competing stakeholder interests. Climate simulation methods, such as Earth System Models, have become valuable tools for policy exploration. However, their typical use is for evaluating potential polices, rather than directly synthesizing them. The problem can be inverted to optimize for policy pathways, but the traditional optimization approaches often struggle with non-linear dynamics, heterogeneous agents, and comprehensive uncertainty quantification. We propose a framework for augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to address these limitations. We identify key challenges at the interface between climate simulations and the application of MARL in the context of policy synthesis, including reward definition, scalability with increasing agents and state spaces, uncertainty propagation across linked systems, and solution validation. Additionally, we discuss challenges in making MARL-derived solutions interpretable and useful for policy-makers. Our framework provides a foundation for more sophisticated climate policy exploration while acknowledging important limitations and areas for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向环境政策合成的多智能体强化学习仿真框架</div>
<div class="mono" style="margin-top:8px">气候政策制定因深度不确定性、复杂系统动态及利益相关方诉求冲突而面临重大挑战。以地球系统模型为代表的气候仿真方法已成为政策探索的重要工具，但其通常用于评估潜在政策，而非直接合成政策。虽然可通过逆向优化获取政策路径，但传统优化方法常受限于非线性动态、异质智能体及综合不确定性量化。本研究提出通过多智能体强化学习增强气候仿真的框架以突破这些局限。我们识别了气候仿真与MARL在政策合成应用中的关键挑战，包括奖励定义、智能体与状态空间扩展的可扩展性、关联系统间的不确定性传递及解决方案验证。同时探讨了使MARL衍生方案对决策者具备可解释性与实用性的挑战。本框架为深化气候政策探索奠定基础，并明确了当前局限与未来研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of synthesizing effective climate policies amidst deep uncertainty and complex stakeholder dynamics by proposing a framework that integrates Multi-Agent Reinforcement Learning (MARL) with climate simulations. The motivation stems from the limitations of traditional climate models, which are typically used for policy evaluation rather than synthesis, and the difficulty of optimization methods in handling non-linear dynamics and heterogeneous agents. The method involves augmenting existing climate simulations with MARL to directly optimize policy pathways, while tackling key challenges such as reward definition, scalability, uncertainty propagation, and solution interpretability for policymakers. The main experimental results are not detailed as this is a conceptual framework paper, but it establishes a foundation for advanced policy exploration and highlights critical areas for future research, including validation and making MARL-derived solutions actionable.</div>
<div class="mono" style="margin-top:8px">本文针对深度不确定性和复杂利益相关者动态下制定有效气候政策的挑战，提出了一个将多智能体强化学习（MARL）与气候模拟相结合的框架。其动机源于传统气候模型通常仅用于政策评估而非合成，以及优化方法在处理非线性动态和异质智能体方面的困难。该方法通过将MARL增强现有气候模拟，以直接优化政策路径，同时应对奖励定义、可扩展性、不确定性传播以及政策制定者可解释性等关键挑战。由于这是一篇概念性框架论文，未详述具体实验结果，但它为高级政策探索奠定了基础，并强调了未来研究的关键领域，包括验证和使MARL衍生解决方案可操作化。</div>
</details>
</div>
<div class="card">
<div class="title">SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs</div>
<div class="meta-line">Authors: Niccolo Avogaro, Nayanika Debnath, Li Mi, Thomas Frick, Junling Wang, Zexue He, Hang Hua, Konrad Schindler, Mattia Rigotti</div>
<div class="meta-line">First: 2026-02-06T10:05:25+00:00 · Latest: 2026-02-10T15:30:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06566v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06566v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses &quot;thinking with images&quot; by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARC：分离感知与推理电路以实现视觉语言模型的测试时扩展</div>
<div class="mono" style="margin-top:8px">尽管近期取得进展，测试时扩展（即在推理过程中按需动态增加令牌预算）对视觉语言模型（VLM）仍显脆弱：关于图像的非结构化思维链会混淆感知与推理，导致生成冗长混乱的上下文，使细微的感知错误可能引发完全错误的答案。此外，需依赖人工设计奖励的昂贵强化学习才能获得良好性能。本文提出SPARC（分离感知与推理电路），这是一种显式解耦视觉感知与推理的模块化框架。受大脑中序列化感觉-认知处理机制启发，SPARC采用两阶段流程：模型先执行显式视觉搜索以定位问题相关区域，再基于这些区域进行推理生成最终答案。这种分离支持非对称计算分配的独立测试时扩展（例如在分布偏移时优先处理感知阶段），允许选择性优化（例如当感知阶段成为端到端性能瓶颈时单独改进该阶段），并能通过低分辨率全局搜索与高分辨率局部处理相结合压缩上下文，从而减少视觉令牌总数与计算量。在多项视觉推理基准测试中，SPARC优于单体基线模型与强视觉 grounding 方法。例如，SPARC将Qwen3VL-4B在$V^*$ VQA基准上的准确率提升6.7个百分点，在挑战性OOD任务上以200倍更低的令牌预算超越“图像思维”方法4.6个百分点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SPARC is to address the brittleness of test-time scaling in vision-language models (VLMs), where entangled perception and reasoning in unstructured chains-of-thought can lead to error cascades and require costly reinforcement learning. The method introduces a modular framework that explicitly decouples visual perception from reasoning via a two-stage pipeline: first performing explicit visual search to localize question-relevant regions, then conditioning reasoning on those regions to produce final answers. Experimental results show that SPARC outperforms monolithic baselines and visual-grounding approaches on challenging benchmarks, improving Qwen3VL-4B accuracy on the V* VQA benchmark by 6.7 percentage points and surpassing &quot;thinking with images&quot; by 4.6 points on an OOD task while using 200× fewer tokens.</div>
<div class="mono" style="margin-top:8px">SPARC的提出动机是解决视觉语言模型在测试时扩展中的脆弱性问题，即感知与推理在非结构化思维链中纠缠，易导致错误级联，且需要昂贵的强化学习。该方法引入了一个模块化框架，通过两阶段流程显式解耦视觉感知与推理：先执行显式视觉搜索以定位问题相关区域，再基于这些区域进行推理生成最终答案。实验结果表明，在具有挑战性的基准测试中，SPARC优于整体基线和视觉定位方法，将Qwen3VL-4B在V* VQA基准上的准确率提高了6.7个百分点，并在OOD任务上以200倍更少的令牌使用量超越了“图像思维”方法4.6个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation</div>
<div class="meta-line">Authors: Zezhou Wang, Ziyun Zhang, Xiaoyi Zhang, Zhuzhong Qian, Yan Lu</div>
<div class="meta-line">First: 2026-01-09T13:26:38+00:00 · Latest: 2026-02-10T15:12:17+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05787v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05787v2">PDF</a> · <a href="https://github.com/LEON-gittech/Verl_GUI.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从离线策略到在线策略：通过双层专家-策略同化增强图形用户界面智能体</div>
<div class="mono" style="margin-top:8px">视觉语言模型正越来越多地被部署为操作桌面和浏览器的计算机使用智能体。性能最佳的此类智能体是基于框架的系统，将规划与执行解耦；而端到端的屏幕截图到动作策略虽更易部署，但在OSWorld-Verified等基准测试中表现落后。OSWorld等图形用户界面数据集存在两个瓶颈：仅提供数百个可交互、可验证的任务与环境，且专家轨迹必须通过与这些环境交互收集，导致数据难以扩展。因此，我们研究如何利用强化学习从可验证奖励中，最优地利用少量现有专家轨迹来训练端到端策略。简单地将这些离线策略轨迹混入在线策略强化学习是脆弱的：即使经过格式转换，专家轨迹仍存在与学习器的结构不匹配和分布偏移。我们提出BEPA（双层专家-策略同化），通过基础策略下自生成的可达轨迹将静态专家轨迹转化为策略对齐的指导，并构建用于强化学习的按任务动态更新缓存。在OSWorld-Verified上，BEPA将UITARS1.5-7B的成功率从22.87%提升至32.13%，并将预留测试集的结果从5.74%提高至10.30%，在MMBench-GUI和Online-Mind2Web上也取得一致增益。代码与数据公开于：https://github.com/LEON-gittech/Verl_GUI.git</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of training end-to-end GUI agents using limited expert trajectories, where naive off-policy reinforcement learning from verifiable rewards (RLVR) suffers from distribution shift. The authors propose BEPA, a bi-level method that first generates policy-aligned trajectories by rolling out the base policy to reach expert states, then uses a dynamically updated cache to integrate these adapted traces into on-policy RLVR. Experiments on OSWorld-Verified show that BEPA significantly improves success rates for a 7B parameter model from 22.87% to 32.13%, with consistent gains on other GUI benchmarks, demonstrating effective assimilation of scarce expert data.</div>
<div class="mono" style="margin-top:8px">本研究针对利用有限专家轨迹训练端到端GUI智能体的挑战，其中简单的离策略可验证奖励强化学习（RLVR）存在分布偏移问题。作者提出了BEPA这一双层方法，首先通过展开基础策略到达专家状态来生成策略对齐的轨迹，然后利用动态更新的缓存将这些适配后的轨迹整合到在策略RLVR中。在OSWorld-Verified上的实验表明，BEPA将7B参数模型的成功率从22.87%显著提升至32.13%，并在其他GUI基准测试上取得一致改进，证明了其对稀缺专家数据的有效同化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Code2World: A GUI World Model via Renderable Code Generation</div>
<div class="meta-line">Authors: Yuhao Zheng, Li&#x27;an Zhong, Yi Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu, Linyuan Lv, Philip Torr, Kevin Qinghong Lin</div>
<div class="meta-line">First: 2026-02-10T14:56:19+00:00 · Latest: 2026-02-10T14:56:19+00:00</div>
<div class="meta-line">Comments: github: https://github.com/AMAP-ML/Code2World project page: https://amap-ml.github.io/Code2World/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09856v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09856v1">PDF</a> · <a href="https://github.com/AMAP-ML/Code2World">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://amap-ml.github.io/Code2World/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Code2World：通过可渲染代码生成的GUI世界模型</div>
<div class="mono" style="margin-top:8px">自主GUI代理通过感知界面和执行操作与环境交互。作为虚拟沙箱，GUI世界模型通过支持动作条件预测，赋予代理类人的预见能力。然而，现有基于文本和像素的方法难以同时实现高视觉保真度和细粒度结构可控性。为此，我们提出Code2World——一种通过可渲染代码生成模拟下一视觉状态的视觉语言编码器。具体而言，为解决数据稀缺问题，我们构建AndroidCode数据集：将GUI轨迹转换为高保真HTML代码，并通过视觉反馈修正机制优化合成代码，最终获得包含超8万组高质量屏幕-动作对的数据集。为使现有视觉语言模型适应代码预测任务，我们首先采用监督微调作为格式布局跟随的冷启动，进而应用渲染感知强化学习——以渲染结果为奖励信号，强化视觉语义保真度与动作一致性。大量实验表明，Code2World-8B在下一界面预测任务中达到最优性能，媲美GPT-5与Gemini-3-Pro-Image。值得注意的是，Code2World以灵活方式显著提升下游导航成功率，在AndroidWorld导航任务中将Gemini-2.5-Flash性能提升9.5%。代码已开源：https://github.com/AMAP-ML/Code2World。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for GUI world models that combine high visual fidelity with fine-grained structural control for autonomous agents, this paper introduces Code2World, a vision-language model that predicts next visual states by generating renderable HTML code. The method addresses data scarcity by constructing AndroidCode, a dataset of over 80K screen-action pairs via translation and visual-feedback refinement, and adapts VLMs through supervised fine-tuning followed by render-aware reinforcement learning that uses rendered outcomes as rewards. Experimental results show Code2World-8B achieves top performance in next UI prediction, rivaling models like GPT-5 and Gemini-3-Pro-Image, and boosts downstream navigation success rates, such as improving Gemini-2.5-Flash by +9.5% on AndroidWorld navigation.</div>
<div class="mono" style="margin-top:8px">本文的动机是为自主智能体开发兼具高视觉保真度和细粒度结构可控性的GUI世界模型，为此提出了Code2World，这是一种通过生成可渲染HTML代码来预测下一视觉状态的视觉语言模型。方法上，通过翻译和视觉反馈修正构建了包含超过8万屏幕-动作对的高质量数据集AndroidCode以解决数据稀缺问题，并采用监督微调和以渲染结果为奖励的渲染感知强化学习来适配现有视觉语言模型。主要实验结果表明，Code2World-8B在下一UI预测任务中达到顶尖性能，可与GPT-5和Gemini-3-Pro-Image相媲美，并能显著提升下游导航成功率，例如在AndroidWorld导航任务中将Gemini-2.5-Flash的性能提高了9.5%。</div>
</details>
</div>
<div class="card">
<div class="title">CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization</div>
<div class="meta-line">Authors: Beicheng Xu, Keyao Ding, Wei Liu, Yupeng Lu, Bin Cui</div>
<div class="meta-line">First: 2026-02-10T14:54:17+00:00 · Latest: 2026-02-10T14:54:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09851v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy &quot;FE-then-HPO&quot; workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (ToT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoFEH：基于协作贝叶斯超参数优化的大语言模型驱动特征工程</div>
<div class="mono" style="margin-top:8px">特征工程（FE）在自动化机器学习（AutoML）中至关重要，但传统方法将其视为黑盒搜索，在僵化的预定义搜索空间内操作且缺乏领域感知，成为瓶颈。大语言模型（LLMs）通过语义推理生成无界算子提供了有前景的替代方案，但现有方法未能构建自由形式的FE流程，仍局限于特征生成等孤立子任务。最关键的是，它们很少与机器学习模型的超参数优化（HPO）联合优化，导致贪婪的“先FE后HPO”工作流无法捕捉强FE-HPO交互。本文提出CoFEH，一个将基于LLM的FE与贝叶斯HPO交织协作的稳健端到端AutoML框架。CoFEH采用思维树（ToT）驱动的LLM-FE优化器探索灵活FE流程，贝叶斯优化（BO）模块解决HPO，以及动态优化器选择器通过自适应调度FE和HPO步骤实现交织优化。关键的是，我们引入了LLM与BO间共享上下文的互条件机制，使决策能相互参考。实验表明，CoFEH不仅优于传统和基于LLM的FE基线，在联合优化下也实现了更优的端到端性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in automated feature engineering (FE) where traditional methods operate as black-box searches within rigid spaces and existing LLM-based approaches are confined to isolated subtasks without joint optimization with hyperparameter optimization (HPO). To overcome this, the authors propose CoFEH, a collaborative framework that interleaves LLM-driven FE using Tree of Thought reasoning with Bayesian HPO, facilitated by a dynamic optimizer selector and a mutual conditioning mechanism for shared context between components. Experimental results demonstrate that CoFEH outperforms both traditional and LLM-based FE baselines, achieving superior end-to-end performance through effective joint optimization of FE and HPO.</div>
<div class="mono" style="margin-top:8px">该论文针对自动化特征工程（FE）中的局限性提出改进，传统方法在固定搜索空间中作为黑盒搜索运行，而现有基于大语言模型（LLM）的方法局限于孤立子任务，且未与超参数优化（HPO）联合优化。为此，作者提出了CoFEH框架，它通过动态优化器选择器和互条件机制，将基于思维树的LLM驱动FE与贝叶斯HPO交错协同，实现组件间的上下文共享。实验结果表明，CoFEH在传统和基于LLM的FE基线方法上均表现更优，通过FE与HPO的有效联合优化获得了卓越的端到端性能。</div>
</details>
</div>
<div class="card">
<div class="title">AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</div>
<div class="meta-line">Authors: Valter Schütz, Han Wu, Reza Rezvan, Linus Aronsson, Morteza Haghir Chehreghani</div>
<div class="meta-line">First: 2025-08-20T14:29:16+00:00 · Latest: 2026-02-10T14:21:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14734v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.14734v2">PDF</a> · <a href="https://github.com/Linusaronsson/AFA-Benchmark">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world scenarios, acquiring all features of a data instance can be expensive or impractical due to monetary cost, latency, or privacy concerns. Active Feature Acquisition (AFA) addresses this challenge by dynamically selecting a subset of informative features for each data instance, trading predictive performance against acquisition cost. While numerous methods have been proposed for AFA, ranging from myopic information-theoretic strategies to non-myopic reinforcement learning approaches, fair and systematic evaluation of these methods has been hindered by a lack of standardized benchmarks. In this paper, we introduce AFABench, the first benchmark framework for AFA. Our benchmark includes a diverse set of synthetic and real-world datasets, supports a wide range of acquisition policies, and provides a modular design that enables easy integration of new methods and tasks. We implement and evaluate representative algorithms from all major categories, including static, myopic, and reinforcement learning-based approaches. To test the lookahead capabilities of AFA policies, we introduce a novel synthetic dataset, CUBE-NM, designed to expose the limitations of myopic selection. Our results highlight key trade-offs between different AFA strategies and provide actionable insights for future research. The benchmark code is available at: https://github.com/Linusaronsson/AFA-Benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AFABench：主动特征获取基准测试通用框架</div>
<div class="mono" style="margin-top:8px">在许多现实场景中，由于成本、延迟或隐私问题，获取数据实例的所有特征可能代价高昂或不切实际。主动特征获取通过为每个数据实例动态选择信息丰富的特征子集，在预测性能与获取成本之间进行权衡。尽管已提出从短视信息论策略到非短视强化学习方法等多种AFA方法，但缺乏标准化基准阻碍了对这些方法的公平系统评估。本文提出首个AFA基准框架AFABench，包含多样化的合成与真实数据集，支持广泛的获取策略，并采用模块化设计便于新方法与任务的集成。我们实现并评估了静态、短视及强化学习等主要类别的代表性算法。为测试AFA策略的前瞻能力，设计了新型合成数据集CUBE-NM以揭示短视选择的局限性。实验结果明确了不同AFA策略的关键权衡，为未来研究提供实用见解。基准代码已开源：https://github.com/Linusaronsson/AFA-Benchmark。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the practical challenge that acquiring all data features is often costly or infeasible, and while Active Feature Acquisition (AFA) methods aim to selectively gather informative features to balance prediction accuracy with acquisition cost, their evaluation has been inconsistent due to a lack of standardized benchmarks. To address this, the authors introduce AFABench, a generic benchmark framework that includes diverse datasets, supports various acquisition policies, and offers a modular design for easy integration of new methods. They experimentally evaluate representative algorithms, including static, myopic, and reinforcement learning-based approaches, using a novel synthetic dataset, CUBE-NM, designed to test lookahead capabilities, and their results reveal key trade-offs between different AFA strategies, providing insights for future research.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现实场景中获取数据实例的所有特征通常成本高昂或不切实际，而主动特征获取方法旨在动态选择信息丰富的特征以平衡预测性能与获取成本，但由于缺乏标准化基准，这些方法的评估一直不一致。为此，作者提出了AFABench，一个通用基准框架，包含多样化的数据集，支持多种获取策略，并采用模块化设计以便轻松集成新方法。他们通过实验评估了代表性算法，包括静态、短视和基于强化学习的方法，使用了一个新颖的合成数据集CUBE-NM来测试前瞻能力，结果揭示了不同AFA策略之间的关键权衡，为未来研究提供了实用见解。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning</div>
<div class="meta-line">Authors: Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchun Shi</div>
<div class="meta-line">First: 2025-04-03T16:16:35+00:00 · Latest: 2026-02-10T13:57:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.03784v6">Abs</a> · <a href="https://arxiv.org/pdf/2504.03784v6">PDF</a> · <a href="https://github.com/VRPO/VRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. The code is available at https://github.com/VRPO/VRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型微调的鲁棒人类反馈强化学习</div>
<div class="mono" style="margin-top:8px">人类反馈强化学习已成为将大语言模型输出与人类偏好对齐的关键技术。现有RLHF算法大多采用Bradley-Terry模型学习奖励函数，但其依赖的人类偏好假设可能无法反映现实判断的复杂性和多变性。本文提出一种鲁棒算法，以增强现有方法在奖励模型设定错误下的性能。理论上，该算法降低了奖励与策略估计量的方差，从而改进了遗憾界。在大语言模型基准数据集上的实证评估表明，所提算法持续优于现有方法，在Anthropic Helpful and Harmless数据集中77-81%的响应优于基线。代码发布于https://github.com/VRPO/VRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that standard RLHF methods for fine-tuning LLMs rely on the Bradley-Terry model, which makes simplifying assumptions about human preferences that may not capture real-world complexity, leading to potential reward model misspecification. To address this, the authors propose a robust algorithm designed to enhance existing RLHF approaches by reducing the variance of reward and policy estimators, which is supported by improved theoretical regret bounds. The main experimental results, using LLM benchmark datasets, show that this algorithm consistently outperforms baseline methods, with 77-81% of its generated responses being preferred on the Anthropic Helpful and Harmless dataset.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，用于大语言模型微调的标准人类反馈强化学习方法依赖于Bradley-Terry模型，该模型对人类偏好做了简化假设，可能无法反映现实判断的复杂性，从而导致奖励模型设定错误。为解决此问题，作者提出了一种鲁棒算法，旨在通过降低奖励和策略估计器的方差来增强现有方法，这得到了改进的理论遗憾上界的支持。在大型语言模型基准数据集上的主要实验结果表明，该算法 consistently 优于基线方法，在Anthropic Helpful and Harmless数据集上，其77-81%的生成响应更受青睐。</div>
</details>
</div>
<div class="card">
<div class="title">Flexible Entropy Control in RLVR with Gradient-Preserving Perspective</div>
<div class="meta-line">Authors: Kun Chen, Peng Shi, Fanfan Liu, Haibo Qiu, Zhixiong Zeng, Siqi Yang, Wenji Mao</div>
<div class="meta-line">First: 2026-02-10T13:42:12+00:00 · Latest: 2026-02-10T13:42:12+00:00</div>
<div class="meta-line">Comments: https://github.com/Kwen-Chen/Flexible-Entropy-Control</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09782v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09782v1">PDF</a> · <a href="https://github.com/Kwen-Chen/Flexible-Entropy-Control">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于梯度保持视角的RLVR灵活熵控制</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为提升大语言模型推理能力的关键方法。然而，持续训练常导致策略熵崩溃，表现为熵值快速衰减，引发过早过度自信、输出多样性降低及梯度范数消失，从而抑制学习。梯度保持裁剪是影响该动态的主要因素，但现有缓解策略多为静态，且缺乏将裁剪机制与精确熵控制相连接的框架。本文提出从梯度保持裁剪视角重塑强化学习中的熵控制。我们首先从理论与实证层面验证了特定重要性采样比率区域对熵增长与削减的贡献。基于这些发现，我们引入一种采用动态裁剪阈值的新型调控机制，以实现对熵的精确管理。此外，我们设计并评估了动态熵控制策略，包括“先增后减”“减-增-减”及振荡衰减模式。实验结果表明，这些策略能有效缓解熵崩溃，并在多个基准测试中取得更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of policy entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models, where continuous training leads to premature overconfidence, reduced output diversity, and vanishing gradients that hinder learning. The method reinterprets entropy control from the perspective of Gradient-Preserving Clipping, first analyzing the role of importance sampling ratio regions in entropy dynamics and then introducing a novel regulation mechanism using dynamic clipping thresholds to manage entropy precisely. Experimental results show that dynamic strategies like increase-then-decrease and oscillatory decay effectively mitigate entropy collapse and achieve superior performance across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型的可验证奖励强化学习中策略熵崩溃的问题展开研究，该问题会导致模型过早过度自信、输出多样性降低以及梯度消失从而阻碍学习。方法从梯度保留裁剪的角度重塑熵控制，首先理论并实证分析了重要性采样比率区域对熵增减的贡献，进而引入一种使用动态裁剪阈值的新型调节机制来精确管理熵。实验结果表明，诸如先增后减、振荡衰减等动态策略有效缓解了熵崩溃，并在多个基准测试中取得了更优的性能。</div>
</details>
</div>
<div class="card">
<div class="title">PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning</div>
<div class="meta-line">Authors: Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei</div>
<div class="meta-line">First: 2026-01-13T16:02:35+00:00 · Latest: 2026-02-10T13:41:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08679v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08679v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PersonaDual：通过自适应推理平衡个性化与客观性</div>
<div class="mono" style="margin-top:8px">随着用户日益期望大语言模型与其偏好保持一致，个性化信息变得愈发重要。然而，个性化信息可能是一把双刃剑：它虽能提升交互体验，却可能损害客观性与事实准确性，尤其当其与问题意图不符时。为缓解此问题，我们提出PersonaDual框架，该框架在单一模型中同时支持通用客观推理与个性化推理，并能根据上下文自适应切换模式。PersonaDual首先通过监督微调学习两种推理模式，再借助我们提出的DualGRPO强化学习方法优化模式选择机制。在客观性与个性化基准测试上的实验表明，PersonaDual在保留个性化优势的同时有效降低了干扰，实现了近乎无干扰的性能表现，并能更好地利用有益的个性化信号提升客观问题解决能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of balancing personalization with objectivity in large language models, as user-aligned responses can enhance interaction but risk factual compromise when personal preferences conflict with queries. To mitigate this, the authors introduce PersonaDual, a framework that integrates both general objective reasoning and personalized reasoning within a single model, employing adaptive mode switching based on context. The method involves supervised fine-tuning to learn dual reasoning patterns, followed by reinforcement learning optimization via DualGRPO to refine mode selection. Experimental results on objective and personalized benchmarks demonstrate that PersonaDual effectively reduces interference from unhelpful personalization while retaining its benefits, achieving near interference-free performance and leveraging helpful personalized signals to enhance objective problem-solving.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型中个性化与客观性之间的平衡问题展开研究，因为适应用户偏好的回答虽能提升交互体验，但当个人偏好与查询冲突时可能损害事实准确性。为缓解此问题，作者提出了PersonaDual框架，该框架在单一模型中整合了通用客观推理和个性化推理，并能根据上下文自适应切换模式。方法上，首先通过监督微调学习两种推理模式，然后利用提出的DualGRPO进行强化学习优化以改进模式选择。在客观性和个性化基准测试上的实验结果表明，PersonaDual有效减少了无益个性化信息的干扰，同时保留了其优势，实现了近乎无干扰的性能，并能利用有益的个性化信号来提升客观问题解决能力。</div>
</details>
</div>
<div class="card">
<div class="title">Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO</div>
<div class="meta-line">Authors: Kun Peng, Conghui Tan, Yu Liu, Guohua Tang, Zhongqian Sun, Wei Yang, Zining Zhu, Lei Jiang, Yanbing Liu, Hao Peng</div>
<div class="meta-line">First: 2026-02-09T11:32:02+00:00 · Latest: 2026-02-10T13:34:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08533v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08533v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users&#x27; traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework&#x27;s superior performance, sample efficiency, and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于智能体博弈与自适应树状GRPO的对话模型优化</div>
<div class="mono" style="margin-top:8px">开放域对话智能体旨在通过适应用户特质实现个性化交互，但现有方法存在关键局限：过度依赖预收集用户数据，以及强化学习中忽视长期对话价值的短视偏差。为此，我们提出一种融合在线个性化与自适应树状群组相对策略优化（AT-GRPO）的新型长程强化学习框架。采用双智能体博弈范式，用户智能体通过风格模仿（学习用户特定对话特质）和主动终止（预测轮次终止概率作为即时奖励）构建动态环境，形成驱动对话智能体深化兴趣探索的迭代循环。AT-GRPO将对话轨迹重构为树状结构并引入自适应观测范围：不同于指数级开销的全树扩展，该方法将各节点奖励聚合限制在阶段感知范围内——较大范围支持早期话题探索，较小范围促进后期对话维护。该设计将对话长度对应的计算开销从指数级降至多项式级，同时保持长期奖励捕获能力。大量实验验证了本框架在性能、样本效率与鲁棒性方面的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in open-ended dialogue agents, specifically their reliance on pre-collected user data and short-horizon biases in reinforcement learning that neglect long-term engagement. The proposed method introduces a long-horizon RL framework featuring a two-agent game: a user agent dynamically constructs environments through style mimicry and active termination to provide immediate rewards, while a dialogue agent is optimized via Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). AT-GRPO models dialogue trajectories as trees with adaptive observation ranges, reducing computational overhead from exponential to polynomial in dialogue length while effectively capturing long-term rewards. Experimental results demonstrate the framework&#x27;s superior performance, sample efficiency, and robustness compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">本文针对开放域对话智能体存在的依赖预收集用户数据、以及强化学习中忽视长期对话价值的短视偏差等关键局限，提出了一种新颖的长视界强化学习框架。该方法采用双智能体博弈范式：用户智能体通过风格模仿和主动终止来动态构建环境并提供即时奖励，驱动对话智能体进行兴趣探索；对话智能体则通过自适应树形分组相对策略优化（AT-GRPO）进行优化，将对话轨迹建模为具有自适应观察范围的树结构，从而将计算开销从对话长度的指数级降低至多项式级，同时有效捕获长期奖励。大量实验表明，该框架在性能、样本效率和鲁棒性方面均优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
