<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-31 03:57</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260131_0357</div>
    <div class="row"><div class="card">
<div class="title">Exploring Reasoning Reward Model for Agents</div>
<div class="meta-line">Authors: Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue</div>
<div class="meta-line">First: 2026-01-29T18:59:52+00:00 · Latest: 2026-01-29T18:59:52+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/kxfan2002/Reagent</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22154v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22154v1">PDF</a> · <a href="https://github.com/kxfan2002/Reagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索智能体推理奖励模型</div>
<div class="mono" style="margin-top:8px">智能体强化学习在实现复杂推理与工具使用方面取得显著成功，但现有方法仍主要依赖稀疏的结果奖励进行训练。此类反馈无法区分中间推理质量，导致训练效果欠佳。本文提出智能体推理奖励模型——一种为智能体轨迹提供结构化反馈的多维度奖励模型，包含：（1）显式推理轨迹；（2）聚焦式批判，通过突出推理缺陷提供改进指导；（3）评估过程性能的综合评分。基于这些信号，我们系统研究三种集成策略：Reagent-C（文本增强优化）、Reagent-R（奖励增强引导）和Reagent-U（统一反馈集成）。在12个多样化基准测试中的广泛评估表明，Reagent-U实现显著性能跃升，在GAIA和WebWalkerQA上分别达到43.7%和46.2%，验证了推理奖励模型与训练方案的有效性。代码、模型及数据集均已开源以促进后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of sparse outcome-based rewards in Agentic Reinforcement Learning, which fail to assess intermediate reasoning quality, this paper introduces the Agent Reasoning Reward Model (Agent-RRM) to provide structured feedback on agent trajectories, including explicit reasoning traces, focused critiques, and overall scores. The method systematically explores three integration strategies—Reagent-C, Reagent-R, and Reagent-U—to leverage these signals for training. Experimental results across 12 benchmarks show that Reagent-U achieves significant performance improvements, with scores of 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of the proposed reward model and training approaches.</div>
<div class="mono" style="margin-top:8px">针对智能体强化学习中稀疏结果奖励无法评估中间推理质量的局限性，本文提出了智能体推理奖励模型（Agent-RRM），为智能体轨迹提供结构化反馈，包括显式推理轨迹、聚焦式批判和整体评分。该方法系统探索了三种集成策略——Reagent-C、Reagent-R和Reagent-U，以利用这些信号进行训练。在12个基准测试上的实验结果表明，Reagent-U实现了显著性能提升，在GAIA和WebWalkerQA上分别达到43.7%和46.2%的得分，验证了所提奖励模型和训练方案的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DynaWeb: Model-Based Reinforcement Learning of Web Agents</div>
<div class="meta-line">Authors: Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu</div>
<div class="meta-line">First: 2026-01-29T18:59:07+00:00 · Latest: 2026-01-29T18:59:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DynaWeb：基于模型的网页智能体强化学习</div>
<div class="mono" style="margin-top:8px">基于大语言模型与强化学习的自主网页智能体开发，是迈向通用人工智能助手的重要进展。然而，与实时互联网交互的低效性、高成本与高风险严重阻碍了此类智能体的训练。基于模型的强化学习通过构建环境的世界模型实现模拟交互，为此提供了可行方案。本文提出DynaWeb——一种创新的MBRL框架，通过训练网页世界模型来预测智能体动作对应的拟真网页表征，并在此合成网页环境中训练智能体策略。该框架支持智能体通过生成海量推演轨迹进行高效在线强化学习。除自主推演外，DynaWeb还将训练数据中的真实专家轨迹与策略推演随机交织，以提升训练稳定性与样本效率。在WebArena和WebVoyager基准测试中的实验表明，DynaWeb能持续显著提升前沿开源网页智能体模型的性能。本研究证实了通过想象训练网页智能体的可行性，为在线智能体强化学习的规模化扩展提供了高效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the inefficiency, cost, and risk of training autonomous web agents via direct interaction with the live internet. It introduces DynaWeb, a model-based reinforcement learning framework that trains a world model to predict web page representations from agent actions, creating a synthetic environment for efficient policy rollouts and learning; this process is stabilized by interleaving real expert trajectories. Experimental results on WebArena and WebVoyager benchmarks show that DynaWeb consistently and significantly boosts the performance of state-of-the-art open-source web agent models, demonstrating the viability of scalable training through simulated interaction.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，通过直接与实时互联网交互来训练自主网络智能体存在效率低、成本高和风险大的问题。为此，论文提出了DynaWeb，这是一个基于模型的强化学习框架，它训练一个世界模型来根据智能体动作预测网页表示，从而创建一个合成环境以进行高效的策略推演和学习；该过程通过穿插真实专家轨迹来提升稳定性。在WebArena和WebVoyager基准测试上的实验结果表明，DynaWeb持续且显著地提升了最先进开源网络智能体模型的性能，证明了通过模拟交互进行可扩展训练的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization</div>
<div class="meta-line">Authors: Leonard Papenmeier, Petru Tighineanu</div>
<div class="meta-line">First: 2026-01-29T18:51:58+00:00 · Latest: 2026-01-29T18:51:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22131v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMOG：面向多目标贝叶斯优化的可扩展元学习方法</div>
<div class="mono" style="margin-top:8px">多目标优化旨在解决具有竞争性目标的问题，通常仅能通过黑盒方式访问问题且测量预算有限。许多应用场景中存在来自相关优化任务的历史数据，这为利用元学习加速优化提供了可能。贝叶斯优化作为黑盒优化的有效技术，已分别扩展至元学习和多目标优化领域，但能同时处理两种场景的方法——即面向多目标贝叶斯优化的元学习先验——仍鲜有研究。本文提出SMOG，一种基于多输出高斯过程的可扩展模块化元学习模型，能显式学习目标间的相关性。SMOG构建跨元任务与目标任务的结构化联合高斯过程先验，在基于元数据条件化后，通过灵活的多输出残差核增强得到闭式目标任务先验。该构建以理论完备的方式将元数据不确定性传递至目标代理模型。SMOG支持分层并行训练：元任务高斯过程经单次拟合后缓存，实现与元任务数量的线性扩展。所得代理模型可与标准多目标贝叶斯优化采集函数无缝集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of multi-objective optimization where competing objectives must be balanced under limited black-box evaluations, leveraging historical data from related tasks through meta-learning to accelerate optimization. It introduces SMOG, a scalable meta-learning method based on a multi-output Gaussian process that explicitly models correlations between objectives, constructing a structured joint prior across meta- and target tasks to propagate metadata uncertainty and enable closed-form priors with residual kernels. Experimental results demonstrate that SMOG achieves linear scaling with the number of meta-tasks through hierarchical, parallel training and integrates effectively with standard acquisition functions, enhancing optimization efficiency in multi-objective settings.</div>
<div class="mono" style="margin-top:8px">本文针对多目标优化中竞争目标在有限黑盒评估下需平衡的挑战，利用相关任务的历史数据通过元学习加速优化。提出了SMOG，一种基于多输出高斯过程的可扩展元学习方法，显式建模目标间相关性，构建跨元任务和目标任务的结构化联合先验，以传播元数据不确定性并实现带残差核的闭式先验。实验结果表明，SMOG通过分层并行训练实现与元任务数量的线性扩展，并能与标准采集函数无缝集成，提升了多目标场景下的优化效率。</div>
</details>
</div>
<div class="card">
<div class="title">Lens-descriptor guided evolutionary algorithm for optimization of complex optical systems with glass choice</div>
<div class="meta-line">Authors: Kirill Antonov, Teus Tukker, Tiago Botari, Thomas H. W. Bäck, Anna V. Kononova, Niki van Stein</div>
<div class="meta-line">First: 2026-01-29T18:13:24+00:00 · Latest: 2026-01-29T18:13:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22075v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings. In practice, standard optimizers (e.g., gradient-based local search and evolutionary strategies) often converge to a single local optimum, overlooking many comparably good alternatives that matter for downstream engineering decisions. We propose the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage framework for multimodal lens optimization. LDG-EA first partitions the design space into behavior descriptors defined by curvature-sign patterns and material indices, then learns a probabilistic model over descriptors to allocate evaluations toward promising regions. Within each descriptor, LDG-EA applies the Hill-Valley Evolutionary Algorithm with covariance-matrix self-adaptation to recover multiple distinct local minima, optionally followed by gradient-based refinement. On a 24-variable (18 continuous and 6 integer), six-element Double-Gauss topology, LDG-EA generates on average around 14500 candidate minima spanning 636 unique descriptors, an order of magnitude more than a CMA-ES baseline, while keeping wall-clock time at one hour scale. Although the best LDG-EA design is slightly worse than a fine-tuned reference lens, it remains in the same performance range. Overall, the proposed LDG-EA produces a diverse set of solutions while maintaining competitive quality within practical computational budgets and wall-clock time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于透镜描述符引导的进化算法用于复杂光学系统与玻璃选择的优化</div>
<div class="mono" style="margin-top:8px">设计高性能光学透镜需探索一个高维、强约束的空间，涉及表面曲率、玻璃选择、元件厚度与间距。实践中，标准优化器（如基于梯度的局部搜索和进化策略）常收敛至单一局部最优解，忽略了许多对下游工程决策至关重要的同等优质替代方案。本文提出透镜描述符引导进化算法（LDG-EA），一种用于多模态透镜优化的两阶段框架。LDG-EA首先将设计空间划分为由曲率符号模式和材料指数定义的行为描述符，随后学习描述符上的概率模型以将评估资源分配至有前景的区域。在每个描述符内，LDG-EA应用协方差矩阵自适应的Hill-Valley进化算法以恢复多个不同的局部极小值，并可选择性地进行基于梯度的精细化优化。在一个包含24个变量（18个连续、6个整数）的六片式双高斯结构上，LDG-EA平均生成约14500个候选极小值，覆盖636个独特描述符，数量级上超过CMA-ES基线，同时将实际计算时间控制在一小时量级。尽管LDG-EA的最佳设计略逊于精细调优的参考透镜，但仍处于相同性能区间。总体而言，所提出的LDG-EA在实用计算资源和时间预算内，生成多样化的解决方案集，同时保持具有竞争力的质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to explore diverse high-quality solutions in the complex, high-dimensional design space of optical lenses, where standard optimizers often converge to a single local optimum, this paper introduces the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA). The method employs a two-stage framework: it first partitions the design space using behavior descriptors based on curvature-sign patterns and material indices, then learns a probabilistic model to guide evaluations toward promising regions, and within each region, applies an evolutionary algorithm to find multiple local minima, optionally refined with gradient-based methods. Experimental results on a 24-variable six-element Double-Gauss system show that LDG-EA generates approximately 14,500 candidate minima across 636 unique descriptors—an order of magnitude more than a CMA-ES baseline—within about one hour of wall-clock time, producing a diverse solution set while maintaining performance competitive with a fine-tuned reference lens.</div>
<div class="mono" style="margin-top:8px">针对光学透镜复杂高维设计空间中标准优化器常收敛于单一局部最优解、而实际工程需要多种高质量方案的问题，本文提出了透镜描述符引导进化算法（LDG-EA）。该方法采用两阶段框架：首先基于曲率符号模式和材料指数定义行为描述符来划分设计空间，并学习概率模型以将评估导向有希望的区域；然后在每个描述符内应用进化算法寻找多个局部极小值，并可结合梯度方法进行细化。在包含24个变量（18个连续、6个整数）的六片双高斯结构实验中，LDG-EA平均生成约14,500个候选极小值，覆盖636个独特描述符，比CMA-ES基线多一个数量级，且计算时间控制在一小时左右；尽管最佳设计略逊于精细调优的参考镜头，但仍保持相近性能，从而在有限计算预算内产出了多样化的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control</div>
<div class="meta-line">Authors: MohammadErfan Jabbari, Abhishek Duttagupta, Claudio Fiandrino, Leonardo Bonati, Salvatore D&#x27;Oro, Michele Polese, Marco Fiore, Tommaso Melodia</div>
<div class="meta-line">First: 2026-01-29T17:46:46+00:00 · Latest: 2026-01-29T17:46:46+00:00</div>
<div class="meta-line">Comments: 10 pages, 12 figures, accepted at IEEE INFOCOM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22044v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA&#x27;s online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIA：面向网络控制中前瞻性深度强化学习的符号可解释性框架</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）为未来移动网络提供了自适应控制的潜力，但传统智能体仍停留在反应式模式：它们基于过去和当前的测量数据行动，无法利用带宽等外生关键绩效指标的短期预测。通过预测增强智能体可克服这种时序短视，但网络领域应用甚少，因为具备预测感知能力的智能体如同黑箱；运营商无法判断预测是否真正指导决策或证明其额外复杂性是合理的。我们提出SIA——首个实时揭示预测增强型DRL智能体运作机制的解析器。SIA融合符号人工智能抽象与逐指标知识图谱以生成解释，并引入新的影响力评分指标。SIA实现亚毫秒级解析速度，比现有可解释人工智能方法快200倍以上。我们在三个异构网络用例中评估SIA，揭示了包括预测集成时序错位和奖励设计偏差引发低效策略在内的潜在问题。这些洞见支持针对性改进：重新设计的智能体在视频流传输中实现平均比特率提升9%，SIA的在线动作优化模块无需重新训练即可将无线接入网切片奖励提升25%。通过使前瞻性DRL透明可调，SIA降低了下一代移动网络中主动控制的应用门槛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to make forecast-augmented deep reinforcement learning (DRL) agents, which promise proactive network control, interpretable to operators who currently view them as opaque black boxes. The method introduces SIA, a real-time interpreter that fuses symbolic AI abstractions with per-KPI knowledge graphs to generate explanations and includes a new Influence Score metric. Experimental results show SIA operates with sub-millisecond speed, over 200 times faster than existing explainable AI methods, and its application to three networking use cases uncovered issues like temporal misalignment and reward biases, enabling targeted fixes that improved video streaming bitrate by 9% and RAN-slicing reward by 25%.</div>
<div class="mono" style="margin-top:8px">本文的动机是让用于前瞻性网络控制的、基于预测的深度强化学习（DRL）智能体对运营商变得可解释，因为现有智能体被视为不透明的黑箱。方法上提出了SIA，这是一种实时解释器，它融合了符号AI抽象和每KPI知识图谱来生成解释，并包含一个新的影响力评分指标。实验结果表明，SIA以亚毫秒级速度运行，比现有可解释AI方法快200倍以上，在三个网络用例中应用后，发现了如预测时序错位和奖励设计偏差等问题，从而实现了针对性改进，将视频流平均码率提升了9%，无线接入网切片奖励提升了25%。</div>
</details>
</div>
<div class="card">
<div class="title">Reward-Preserving Attacks For Robust Reinforcement Learning</div>
<div class="meta-line">Authors: Lucas Schott, Elies Gherbi, Hatem Hajri, Sylvain Lamprier</div>
<div class="meta-line">First: 2026-01-12T01:14:03+00:00 · Latest: 2026-01-29T17:32:38+00:00</div>
<div class="meta-line">Comments: 27 pages, 28 figures, 4 algorithms, 3 tables, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07118v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07118v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial training in reinforcement learning (RL) is challenging because perturbations cascade through trajectories and compound over time, making fixed-strength attacks either overly destructive or too conservative. We propose reward-preserving attacks, which adapt adversarial strength so that an $α$ fraction of the nominal-to-worst-case return gap remains achievable at each state. In deep RL, perturbation magnitudes $η$ are selected dynamically, using a learned critic $Q((s,a),η)$ that estimates the expected return of $α$-reward-preserving rollouts. For intermediate values of $α$, this adaptive training yields policies that are robust across a wide range of perturbation magnitudes while preserving nominal performance, outperforming fixed-radius and uniformly sampled-radius adversarial training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向鲁棒强化学习的奖励保持型攻击</div>
<div class="mono" style="margin-top:8px">强化学习中的对抗训练面临挑战，因为扰动会随轨迹级联传播并随时间累积，导致固定强度的攻击要么破坏性过强，要么过于保守。我们提出奖励保持型攻击，通过动态调整对抗强度，使每个状态下仍能保持名义回报与最坏情况回报差距的α比例。在深度强化学习中，我们利用学习得到的评判器Q((s,a),η)动态选择扰动幅度η，该评判器用于估计α-奖励保持型推演的期望回报。对于中间值的α，这种自适应训练产生的策略能在广泛扰动幅度范围内保持鲁棒性，同时维持名义性能，其表现优于固定半径和均匀采样半径的对抗训练方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of adversarial training in reinforcement learning, where fixed-strength attacks often lead to either excessive disruption or insufficient robustness due to the compounding effects of perturbations over time. To overcome this, the authors introduce reward-preserving attacks, which dynamically adjust adversarial strength to ensure that a specified fraction of the nominal-to-worst-case return gap remains achievable at each state. In deep RL, this is implemented by using a learned critic to select perturbation magnitudes dynamically, enabling adaptive training. Experimental results demonstrate that this method produces policies robust across a wide range of perturbation magnitudes while maintaining nominal performance, outperforming traditional fixed-radius and uniformly sampled-radius adversarial training approaches.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中对抗性训练的挑战，即固定强度的攻击由于扰动随时间累积效应，常导致过度破坏或鲁棒性不足。为解决此问题，作者提出了奖励保持攻击，动态调整对抗强度，以确保在每个状态下仍能实现名义与最坏情况回报差距的指定比例。在深度强化学习中，该方法通过使用学习到的评论家动态选择扰动幅度来实现自适应训练。实验结果表明，该方法生成的策略在多种扰动幅度下均表现出鲁棒性，同时保持了名义性能，优于传统的固定半径和均匀采样半径对抗训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks</div>
<div class="meta-line">Authors: Abhishek Duttagupta, MohammadErfan Jabbari, Claudio Fiandrino, Marco Fiore, Joerg Widmer</div>
<div class="meta-line">Venue: IEEE INFOCOM 2025 - IEEE Conference on Computer Communications, London, United Kingdom, 19-22 May 2025, pp. 1-10</div>
<div class="meta-line">First: 2026-01-29T17:31:40+00:00 · Latest: 2026-01-29T17:31:40+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures, published in IEEE INFOCOM 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22024v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22024v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SymbXRL：面向移动网络的符号化可解释深度强化学习</div>
<div class="mono" style="margin-top:8px">未来第六代（6G）移动网络的运行将日益依赖深度强化学习（DRL）实时优化网络决策的能力。DRL已在用户调度与天线分配联合决策、计算资源与调制同步控制等多种资源分配问题上展现出显著效能。然而，训练完成的DRL智能体如同黑箱，本质难以解释，这阻碍了其在生产环境中的部署。本文通过提出SymbXRL——一种生成DRL智能体可人为理解解释的新型可解释强化学习（XRL）技术，向消除这一关键障碍迈出一步。SymbXRL利用符号化人工智能，通过直观符号与规则描述关键概念及其关联关系；将此类表征与逻辑推理结合，能揭示DRL智能体的决策过程，并提供较现有方法更易理解的行为描述。我们在DRL支持的实际网络管理用例中验证SymbXRL，证明其不仅能提升解释的语义质量，还为显式智能体控制开辟道路：例如，基于意图的程序化行动引导机制使累积奖励中位数较纯DRL方案提升12%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to make deep reinforcement learning (DRL) agents more interpretable for real-time optimization in 6G mobile networks, this paper introduces SymbXRL, a novel explainable reinforcement learning technique that combines symbolic AI with DRL to generate human-understandable explanations using symbols and rules. The method synthesizes intuitive symbolic representations and applies logical reasoning to expose the decision processes of DRL agents, enhancing explanation semantics. Experimental validation in practical network management scenarios demonstrates that SymbXRL not only provides more comprehensible behavioral descriptions but also enables explicit agent control, such as intent-based programmatic steering, which improves the median cumulative reward by 12% compared to a pure DRL solution.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决深度强化学习（DRL）智能体在6G移动网络实时优化中因缺乏可解释性而难以实际部署的问题，为此提出了SymbXRL这一新颖的可解释强化学习技术，它结合符号人工智能与DRL，通过符号和规则生成人类可理解的解释。该方法利用符号表示和逻辑推理来揭示DRL智能体的决策过程，提升了解释的语义清晰度。在实际网络管理用例中的实验结果表明，SymbXRL不仅能提供更易于理解的行为描述，还实现了对智能体的显式控制，例如基于意图的程序化引导，使得累积奖励中位数比纯DRL解决方案提高了12%。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with Local Rewards</div>
<div class="meta-line">Authors: Niklas Freymuth, Philipp Dahlinger, Tobias Würth, Simon Reisch, Luise Kärger, Gerhard Neumann</div>
<div class="meta-line">First: 2024-06-12T17:26:54+00:00 · Latest: 2026-01-29T17:22:03+00:00</div>
<div class="meta-line">Comments: Submitted to Journal of Machine Learning Research (JMLR)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.08440v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.08440v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulating physical systems is essential in engineering, but analytical solutions are limited to straightforward problems. Consequently, numerical methods like the Finite Element Method (FEM) are widely used. However, the FEM becomes computationally expensive as problem complexity and accuracy demands increase. Adaptive Mesh Refinement (AMR) improves the FEM by dynamically placing mesh elements on the domain, balancing computational speed and accuracy. Classical AMR depends on heuristics or expensive error estimators, which may lead to suboptimal performance for complex simulations. While AMR methods based on machine learning are promising, they currently only scale to simple problems. In this work, we formulate AMR as a system of collaborating, homogeneous agents that iteratively split into multiple new agents. This agent-wise perspective enables a spatial reward formulation focused on reducing the maximum mesh element error. Our approach, Adaptive Swarm Mesh Refinement++ (ASMR++), offers efficient, stable optimization and generates highly adaptive meshes at user-defined resolution at inference time. Extensive experiments demonstrate that ASMR++ outperforms heuristic approaches and learned baselines, matching the performance of expensive error-based oracle AMR strategies. ASMR additionally generalizes to different domains during inference, and produces meshes that simulate up to 2 orders of magnitude faster than uniform refinements in more demanding settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于局部奖励深度强化学习的自适应群体网格细化方法</div>
<div class="mono" style="margin-top:8px">物理系统模拟在工程领域至关重要，但解析解仅适用于简单问题。因此，有限元法（FEM）等数值方法被广泛采用。然而，随着问题复杂度和精度要求的提升，FEM的计算成本急剧增加。自适应网格细化（AMR）通过动态布置计算域内的网格单元，在计算速度与精度间取得平衡，从而改进FEM。传统AMR依赖启发式方法或高成本误差估计器，在复杂仿真中可能导致次优性能。虽然基于机器学习的AMR方法前景广阔，但目前仅能应用于简单问题。本研究将AMR构建为协作式同构智能体系统，通过迭代分裂生成新智能体。这种智能体视角支持以降低最大网格单元误差为核心的空间奖励机制。我们提出的自适应群体网格细化++（ASMR++）方法实现了高效稳定的优化，并在推理阶段生成用户指定分辨率的高适应性网格。大量实验表明，ASMR++在性能上超越启发式方法与学习基线，媲美高成本的基于误差的预言式AMR策略。此外，ASMR在推理阶段能泛化至不同计算域，在复杂场景中生成的网格比均匀细化方法的模拟速度快达两个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the computational expense of the Finite Element Method (FEM) in simulating physical systems, where classical Adaptive Mesh Refinement (AMR) relies on heuristics or costly error estimators, limiting performance and scalability. The authors propose ASMR++, a method that formulates AMR as a system of collaborating homogeneous agents that iteratively split, using deep reinforcement learning with a local reward focused on reducing maximum mesh element error. Experimental results show that ASMR++ outperforms heuristic and learned baselines, matches expensive error-based oracle strategies, generalizes to different domains, and in demanding settings produces meshes that simulate up to 100 times faster than uniform refinements.</div>
<div class="mono" style="margin-top:8px">本文针对有限元法在物理系统模拟中的计算成本问题，指出经典的自适应网格细化方法依赖启发式或昂贵的误差估计器，限制了性能和可扩展性。作者提出了ASMR++方法，将自适应网格细化为一个协作同质智能体系统，通过深度强化学习和专注于减少最大网格元素误差的局部奖励进行迭代分裂。实验结果表明，ASMR++优于启发式和基于学习的基线方法，匹配了昂贵的基于误差的预言机策略，能够泛化到不同领域，且在要求较高的设置中，生成的网格模拟速度比均匀细化快达100倍。</div>
</details>
</div>
<div class="card">
<div class="title">Geometry of Drifting MDPs with Path-Integral Stability Certificates</div>
<div class="meta-line">Authors: Zuyuan Zhang, Mahdi Imani, Tian Lan</div>
<div class="meta-line">First: 2026-01-29T17:03:23+00:00 · Latest: 2026-01-29T17:03:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21991v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21991v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world reinforcement learning is often \emph{nonstationary}: rewards and dynamics drift, accelerate, oscillate, and trigger abrupt switches in the optimal action. Existing theory often represents nonstationarity with coarse-scale models that measure \emph{how much} the environment changes, not \emph{how} it changes locally -- even though acceleration and near-ties drive tracking error and policy chattering. We take a geometric view of nonstationary discounted Markov Decision Processes (MDPs) by modeling the environment as a differentiable homotopy path and tracking the induced motion of the optimal Bellman fixed point. This yields a length-curvature-kink signature of intrinsic complexity: cumulative drift, acceleration/oscillation, and action-gap-induced nonsmoothness. We prove a solver-agnostic path-integral stability bound and derive gap-safe feasible regions that certify local stability away from switch regimes. Building on these results, we introduce \textit{Homotopy-Tracking RL (HT-RL)} and \textit{HT-MCTS}, lightweight wrappers that estimate replay-based proxies of length, curvature, and near-tie proximity online and adapt learning or planning intensity accordingly. Experiments show improved tracking and dynamic regret over matched static baselines, with the largest gains in oscillatory and switch-prone regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有路径积分稳定性证书的漂移MDP几何理论</div>
<div class="mono" style="margin-top:8px">现实世界的强化学习常具有非平稳性：奖励与动态特性会发生漂移、加速、振荡，并引发最优动作的突变。现有理论通常采用粗粒度模型刻画非平稳性，仅度量环境变化幅度，而忽略局部变化模式——尽管加速度与近似等价性正是跟踪误差与策略抖振的驱动因素。本文从几何视角研究非平稳折扣马尔可夫决策过程，将环境建模为可微同伦路径，并追踪最优贝尔曼不动点的诱导运动。由此导出表征内在复杂性的长度-曲率-拐点特征：累积漂移、加速度/振荡以及动作间隙导致的非光滑性。我们证明了与求解器无关的路径积分稳定性边界，并推导出间隙安全的可行区域，为远离切换机制的区域提供局部稳定性保证。基于这些成果，我们提出轻量级封装算法——同伦跟踪强化学习与同伦跟踪蒙特卡洛树搜索，通过在线估计基于经验回放的长度、曲率及近似等价性代理指标，自适应调整学习或规划强度。实验表明，相比匹配的静态基线方法，该框架在振荡与易切换场景中显著提升了跟踪性能与动态遗憾指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of nonstationary reinforcement learning, where environments change in complex ways beyond simple drift, such as acceleration and abrupt switches, which existing coarse models fail to capture. The authors propose a geometric approach by modeling nonstationary Markov Decision Processes as differentiable homotopy paths, tracking the motion of the optimal Bellman fixed point to derive a signature of complexity based on length, curvature, and kinks, and prove a path-integral stability bound with gap-safe feasible regions. Experimental results demonstrate that their lightweight wrappers, Homotopy-Tracking RL and HT-MCTS, which adapt learning or planning intensity using online proxies of these geometric features, achieve improved tracking and dynamic regret compared to static baselines, particularly in oscillatory and switch-prone regimes.</div>
<div class="mono" style="margin-top:8px">本文针对非平稳强化学习中的挑战，即环境变化复杂，包括加速和突变等，而现有粗粒度模型无法有效描述。作者提出一种几何方法，将非平稳马尔可夫决策过程建模为可微同伦路径，通过跟踪最优贝尔曼不动点的运动，推导出基于长度、曲率和拐点的复杂性特征，并证明了路径积分稳定性界及间隙安全可行区域。实验结果表明，其轻量级封装方法——同伦跟踪强化学习和HT-MCTS，利用这些几何特征的在线代理自适应调整学习或规划强度，相比静态基线在跟踪性能和动态遗憾上表现更优，尤其在振荡和易突变环境中提升显著。</div>
</details>
</div>
<div class="card">
<div class="title">Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields</div>
<div class="meta-line">Authors: Yunyang Li, Lin Huang, Luojia Xia, Wenhe Zhang, Mark Gerstein</div>
<div class="meta-line">First: 2026-01-29T17:00:09+00:00 · Latest: 2026-01-29T17:00:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21985v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21985v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Elign：基于基础机器学习力场的等变扩散模型对齐方法</div>
<div class="mono" style="margin-top:8px">三维分子构象的生成模型必须遵循欧几里得对称性，并将概率质量集中于热力学有利且机械稳定的结构。然而，E(3)等变扩散模型往往复现半经验训练数据的偏差，而非捕捉高精度哈密顿量的平衡分布。虽然基于物理的引导可纠正此问题，但面临两大计算瓶颈：昂贵的量子化学计算（如DFT）以及需在每个采样步骤重复此类查询。我们提出Elign——一种摊销这两项成本的后训练框架。首先，我们使用更快速的预训练基础机器学习力场（MLFF）替代昂贵的DFT计算以提供物理信号；其次，通过将物理引导转移至训练阶段，消除了运行时重复查询。为实现第二项摊销，我们将反向扩散构建为强化学习问题，并引入力-能量解耦分组相对策略优化（FED-GRPO）对去噪策略进行微调。FED-GRPO包含基于势能的能量奖励和基于力的稳定性奖励，二者独立进行优化和分组归一化。实验表明，Elign生成的构象具有更低的金标准DFT能量与力，同时提升稳定性。关键的是，由于生成过程无需能量计算，推理速度仍与无引导采样相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Elign, a framework designed to align E(3)-equivariant diffusion models for 3D molecular conformation generation with high-fidelity physical properties, addressing the limitations of models that often inherit biases from semi-empirical training data. The method replaces costly quantum-chemical evaluations like DFT with a pretrained foundational machine-learning force field (MLFF) for physical guidance and amortizes computational costs by shifting steering to the training phase via a reinforcement learning approach called Force–Energy Disentangled Group Relative Policy Optimization (FED-GRPO), which independently optimizes energy and force rewards. Experimental results demonstrate that Elign generates conformations with lower DFT energies and forces, improving stability while maintaining fast inference speeds comparable to unguided sampling.</div>
<div class="mono" style="margin-top:8px">本文提出了Elign框架，旨在将用于三维分子构象生成的E(3)等变扩散模型与高保真物理性质对齐，以解决现有模型常从半经验训练数据中继承偏差的问题。该方法使用预训练的基础机器学习力场（MLFF）替代昂贵的量子化学计算（如DFT）来提供物理信号，并通过强化学习技术——力-能量解耦组相对策略优化（FED-GRPO）——将物理引导转移到训练阶段，从而分摊计算成本，该技术独立优化基于能量的奖励和基于力的稳定性奖励。实验结果表明，Elign生成的构象具有更低的DFT能量和力，提高了稳定性，同时保持了与无引导采样相当的快速推理速度。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic</div>
<div class="meta-line">Authors: Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato</div>
<div class="meta-line">First: 2026-01-29T16:50:30+00:00 · Latest: 2026-01-29T16:50:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21972v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21972v1">PDF</a> · <a href="https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多智能体演员-评论家方法的去中心化大语言模型协作学习</div>
<div class="mono" style="margin-top:8px">近期研究探索通过多智能体强化学习优化大语言模型协作，但现有方法多依赖需集中式执行的预定义协议。去中心化协作更具实践价值，因其支持智能体并行推理与灵活部署。当前基于蒙特卡洛方法的微调存在高方差缺陷，需大量样本训练。演员-评论家方法能有效应对该问题，为此我们开发了多智能体演员-评论家方法以优化去中心化协作。本文系统分析了该方法的优势场景，提出两种实现方案：采用集中式评论家的CoLLM-CC与采用分布式评论家的CoLLM-DC。在写作、编程及游戏领域的实验表明，蒙特卡洛方法与CoLLM-DC在短周期/稠密奖励场景中可达CoLLM-CC相当性能，但在长周期/稀疏奖励任务中均表现不佳——蒙特卡洛方法需大量样本，CoLLM-DC则难以收敛。代码已开源：https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient decentralized collaboration among large language models (LLMs) without relying on centralized execution or high-variance Monte Carlo methods, this paper introduces Multi-Agent Actor-Critic (MAAC) approaches to optimize LLM teamwork. The method proposes two variants: CoLLM-CC with a centralized critic and CoLLM-DC with decentralized critics, leveraging actor-critic techniques to reduce sample variance and enable parallel agent inference. Experimental results across writing, coding, and game-playing tasks demonstrate that while Monte Carlo methods and CoLLM-DC perform similarly to CoLLM-CC in short-horizon, dense-reward scenarios, CoLLM-CC significantly outperforms them in long-horizon or sparse-reward settings, where the alternatives require more samples or fail to converge effectively.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLM）协作中依赖集中式协议和蒙特卡洛方法高方差的问题，提出采用多智能体演员-评论家（MAAC）方法优化去中心化LLM协作。方法上设计了两种变体：使用集中式评论家的CoLLM-CC和使用去中心化评论家的CoLLM-DC，以降低训练方差并支持智能体并行推理。实验在写作、编码和游戏领域进行，结果表明在短周期、密集奖励任务中，蒙特卡洛方法和CoLLM-DC与CoLLM-CC性能相当，但在长周期或稀疏奖励任务中，CoLLM-CC显著优于前者，后两者需要更多样本或难以收敛。</div>
</details>
</div>
<div class="card">
<div class="title">Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding</div>
<div class="meta-line">Authors: Yifan Zhu, Huiqiang Rong, Haoran Luo</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T16:48:47+00:00 · Latest: 2026-01-29T16:48:47+00:00</div>
<div class="meta-line">Comments: 26 pages and 11 figures,this work has been accepted for presentation at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21969v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21969v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Token-Guard：基于自检解码的令牌级幻觉控制方法</div>
<div class="mono" style="margin-top:8px">大语言模型常产生与输入不一致的幻觉内容。检索增强生成和基于人类反馈的强化学习可缓解幻觉，但需要资源密集的检索或大规模微调。基于解码的方法更轻量，但缺乏明确的幻觉控制机制。为此，我们提出Token-Guard——一种基于自检解码的令牌级幻觉控制方法。该方法在每个推理步骤执行内部验证，在幻觉令牌传播前进行检测。候选片段在潜空间中进行显式幻觉风险评分评估，并通过迭代剪枝与再生动态修正已检测错误。在HALU数据集上的实验表明，Token-Guard显著减少幻觉并提升生成准确性，为可靠的大语言模型输出提供了可扩展的模块化解决方案。代码已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Token-Guard, a method motivated by the need to control hallucinations in Large Language Models (LLMs) without relying on resource-intensive approaches like Retrieval-Augmented Generation or large-scale fine-tuning. The method employs a self-checking decoding process that performs internal verification at each reasoning step to detect hallucinated tokens, evaluates candidate fragments in a latent space with explicit risk scoring, and uses iterative pruning and regeneration to correct errors. Experimental results on HALU datasets demonstrate that Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable and modular solution for reliable LLM outputs.</div>
<div class="mono" style="margin-top:8px">本文提出了Token-Guard方法，其动机是在不依赖检索增强生成或大规模微调等资源密集型方法的情况下，控制大语言模型中的幻觉问题。该方法采用自检解码过程，在每个推理步骤进行内部验证以检测幻觉令牌，在潜在空间中通过显式风险评分评估候选片段，并利用迭代剪枝和重新生成来动态纠正检测到的错误。在HALU数据集上的实验结果表明，Token-Guard显著减少了幻觉并提高了生成准确性，为可靠的大语言模型输出提供了一个可扩展的模块化解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">One Model, Any Conjunctive Query: Graph Neural Networks for Answering Queries over Incomplete Knowledge Graphs</div>
<div class="meta-line">Authors: Krzysztof Olejniczak, Xingyue Huang, Mikhail Galkin, İsmail İlkan Ceylan</div>
<div class="meta-line">First: 2024-09-21T00:30:44+00:00 · Latest: 2026-01-29T16:42:37+00:00</div>
<div class="meta-line">Comments: Proceedings of the Fourth Learning on Graphs Conference (LoG 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.13959v3">Abs</a> · <a href="https://arxiv.org/pdf/2409.13959v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Motivated by the incompleteness of modern knowledge graphs, a new setup for query answering has emerged, where the goal is to predict answers that do not necessarily appear in the knowledge graph, but are present in its completion. In this paper, we formally introduce and study two query answering problems, namely, query answer classification and query answer retrieval. To solve these problems, we propose AnyCQ, a model that can classify answers to any conjunctive query on any knowledge graph. At the core of our framework lies a graph neural network trained using a reinforcement learning objective to answer Boolean queries. Trained only on simple, small instances, AnyCQ generalizes to large queries of arbitrary structure, reliably classifying and retrieving answers to queries that existing approaches fail to handle. This is empirically validated through our newly proposed, challenging benchmarks. Finally, we empirically show that AnyCQ can effectively transfer to completely novel knowledge graphs when equipped with an appropriate link prediction model, highlighting its potential for querying incomplete data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一模型应对任意合取查询：基于图神经网络的不完全知识图谱查询应答</div>
<div class="mono" style="margin-top:8px">针对现代知识图谱的不完整性，一种新的查询应答框架应运而生，其目标在于预测那些未必出现在知识图谱中、但存在于其完备化版本中的答案。本文正式提出并研究了两类查询应答问题：查询答案分类与查询答案检索。为解决这些问题，我们提出了AnyCQ模型，该模型能够对任意知识图谱上的任意合取查询进行答案分类。我们框架的核心是一个采用强化学习目标训练的图神经网络，用于应答布尔查询。仅通过简单的小规模实例训练，AnyCQ即可泛化至任意结构的大规模查询，可靠地对现有方法无法处理的查询进行分类与答案检索。这一能力通过我们新提出的挑战性基准测试得到了实证验证。最后，实验表明当配备适当的链接预测模型时，AnyCQ能有效迁移至全新的知识图谱，凸显了其在查询不完全数据方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the incompleteness of knowledge graphs, this paper introduces two query answering problems—classification and retrieval—over their completions. The method, AnyCQ, employs a graph neural network trained via reinforcement learning on small Boolean queries to generalize to large, arbitrary conjunctive queries. Experimental results on new benchmarks show that AnyCQ reliably classifies and retrieves answers where prior methods fail, and it effectively transfers to novel knowledge graphs when paired with a link prediction model.</div>
<div class="mono" style="margin-top:8px">针对知识图谱的不完整性，本文提出了在知识图谱补全上进行查询答案分类和检索的两个问题。方法上，AnyCQ采用图神经网络，通过强化学习目标在小规模布尔查询上训练，以泛化至任意结构的大型合取查询。在新提出的挑战性基准测试中，实验结果表明AnyCQ能可靠地分类和检索现有方法无法处理的查询答案，并且当配备适当的链接预测模型时，能有效迁移到全新的知识图谱上。</div>
</details>
</div>
<div class="card">
<div class="title">SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling</div>
<div class="meta-line">Authors: Loris Gaven, Clement Romac, Thomas Carta, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-10-16T11:59:27+00:00 · Latest: 2026-01-29T16:31:07+00:00</div>
<div class="meta-line">Comments: This work has been presented at the IMOL workshop at NeurIPS 2025 (https://neurips.cc/virtual/2024/101058)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.12481v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.12481v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The past years have seen Large Language Models (LLMs) strive not only as generative models but also as agents solving textual sequential decision-making tasks. When facing complex environments where their zero-shot abilities are insufficient, recent work showed online Reinforcement Learning (RL) could be used for the LLM agent to discover and learn efficient strategies interactively. However, most prior work sticks to on-policy algorithms, which greatly reduces the scope of methods such agents could use for both exploration and exploitation, such as experience replay and hindsight relabeling. Yet, such methods may be key for LLM learning agents, and in particular when designing autonomous intrinsically motivated agents sampling and pursuing their own goals (i.e. autotelic agents). This paper presents and studies an adaptation of Soft Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves the path towards autotelic LLM agents that learn online but can also outperform on-policy methods in more classic multi-goal RL environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAC-GLAM：基于软演员-评论家与后见之明重标注的大语言模型智能体在线强化学习改进方法</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型不仅作为生成模型发展，更成为解决文本序列决策任务的智能体。面对零样本能力不足的复杂环境，近期研究表明可通过在线强化学习使大语言模型智能体以交互方式探索并学习高效策略。然而，现有研究多局限于同策略算法，这极大限制了智能体在探索与利用时可采用的方法范围（如经验回放与后见之明重标注）。此类方法对大语言模型学习智能体至关重要，尤其在设计能自主采样并追求内在目标的自主动机智能体时。本文提出并研究将软演员-评论家算法与后见之明重标注适配于大语言模型智能体的方法。该工作不仅为实现在线学习的自主动机大语言模型智能体开辟道路，在经典多目标强化学习环境中也展现出超越同策略方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance online reinforcement learning for large language model agents beyond on-policy methods, this paper introduces SAC-GLAM, which adapts Soft Actor-Critic and hindsight relabeling to LLM agents. The method enables more efficient exploration and exploitation through techniques like experience replay, addressing limitations in complex environments where zero-shot abilities fall short. Experimental results demonstrate that this approach not only facilitates the development of autonomous, intrinsically motivated agents but also outperforms traditional on-policy methods in multi-goal reinforcement learning settings.</div>
<div class="mono" style="margin-top:8px">本文的动机是改进大型语言模型代理的在线强化学习，超越传统同策略方法的局限，提出了SAC-GLAM方法，该方法将软演员-评论家算法和事后重标记技术适配于LLM代理。通过引入经验回放等技术，该方法提升了在复杂环境中探索和利用的效率，解决了零样本能力不足的问题。实验结果表明，该方法不仅为开发自主内在动机的代理铺平了道路，而且在多目标强化学习环境中优于同策略方法。</div>
</details>
</div>
<div class="card">
<div class="title">WorldLLM: Improving LLMs&#x27; world modeling using curiosity-driven theory-making</div>
<div class="meta-line">Authors: Guillaume Levy, Cedric Colas, Pierre-Yves Oudeyer, Thomas Carta, Clement Romac</div>
<div class="meta-line">First: 2025-06-07T09:13:34+00:00 · Latest: 2026-01-29T16:25:29+00:00</div>
<div class="meta-line">Comments: This project&#x27;s code can be found at https://github.com/flowersteam/WorldLLM. This project was presented at RLDM 2025 (https://rldm.org/)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06725v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.06725v3">PDF</a> · <a href="https://github.com/flowersteam/WorldLLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model&#x27;s predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WorldLLM：利用好奇心驱动的理论构建改进大语言模型的世界建模能力</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）虽具备通用世界知识，但在模拟等结构化、特定领域情境中常难以生成精确预测。这种局限性源于其无法将宽泛的非结构化理解锚定于具体环境。为此，我们提出WorldLLM框架，通过结合贝叶斯推断、自主主动探索与强化学习，增强基于LLM的世界建模能力。该框架利用LLM的上下文学习能力，通过提示中给定的自然语言假设来指导基于LLM的世界模型预测。这些假设通过贝叶斯推断框架迭代优化：该框架使用第二个LLM作为给定收集证据的提议分布。证据收集采用好奇心驱动的强化学习策略，通过探索环境来发现当前假设下基于LLM的预测模型对数似然较低的转移过程。通过交替进行假设优化与新证据收集，本框架实现了预测能力的自主持续提升。实验在需要智能体操作与组合对象的文本游戏环境中验证了WorldLLM的有效性。该框架不仅提升了预测准确度，还能生成人类可理解的环境动态理论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for WorldLLM is to address the limitation of Large Language Models (LLMs) in making precise, structured predictions within specific environments, despite their broad world knowledge. The method combines Bayesian inference and reinforcement learning, using an LLM to generate natural language hypotheses about world dynamics; these hypotheses are refined via Bayesian inference with a second LLM as a proposal distribution, while a curiosity-driven RL policy actively explores the environment to collect evidence from low-likelihood transitions. Experimental results in a textual game environment requiring object manipulation show that the framework improves predictive accuracy and generates human-interpretable theories of environment dynamics.</div>
<div class="mono" style="margin-top:8px">WorldLLM的动机是解决大型语言模型（LLM）虽具备广泛世界知识，但在特定结构化环境中难以做出精确预测的问题。该方法结合了贝叶斯推理和强化学习，利用LLM生成关于世界动态的自然语言假设；这些假设通过贝叶斯推理框架进行迭代优化，其中第二个LLM作为给定证据的提议分布，同时一个基于好奇心的强化学习策略主动探索环境，从低似然度的状态转换中收集证据。在需要操作和组合对象的文本游戏环境中的实验结果表明，该框架不仅提高了预测准确性，还生成了人类可理解的环境动态理论。</div>
</details>
</div>
<div class="card">
<div class="title">Optimistic Transfer under Task Shift via Bellman Alignment</div>
<div class="meta-line">Authors: Jinhang Chai, Enpei Zhang, Elynn Chen, Yujun Yan</div>
<div class="meta-line">First: 2026-01-29T16:16:24+00:00 · Latest: 2026-01-29T16:16:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21924v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21924v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study online transfer reinforcement learning (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees.
  We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure. RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data.
  This alignment yields a two-stage RWT $Q$-learning framework that separates variance reduction from bias correction. Under RKHS function approximation, we establish regret bounds that scale with the complexity of the task shift rather than the target MDP. Empirical results in both tabular and neural network settings demonstrate consistent improvements over single-task learning and naïve pooling, highlighting Bellman alignment as a model-agnostic transfer principle for online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝尔曼对齐的任务迁移下的乐观迁移</div>
<div class="mono" style="margin-top:8px">本文研究在线迁移强化学习（RL）在分段马尔可夫决策过程中的应用，其中在目标任务学习期间可利用相关源任务的经验。核心难点在于任务相似性通常通过奖励或转移概率定义，而在线RL算法基于贝尔曼回归目标运行，直接复用源任务的贝尔曼更新会引入系统性偏差并破坏遗憾保证。我们提出一步贝尔曼对齐作为在线RL迁移的正确抽象，并引入重加权目标修正（RWT）——一种算子级校正方法，通过测度变换重新调整延续值并补偿转移失配。RWT将任务失配简化为固定的一步校正，实现源数据的统计可靠复用。该对齐框架形成两阶段RWT Q学习架构，将方差缩减与偏差校正分离。在RKHS函数逼近下，我们建立的遗憾界随任务迁移复杂度而非目标MDP复杂度变化。表格与神经网络场景的实验结果表明，该方法相较单任务学习和简单数据池化均取得稳定提升，验证了贝尔曼对齐作为在线RL模型无关迁移原则的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of online transfer reinforcement learning where source task data is available but naive reuse introduces bias due to misalignment between task similarity metrics and Bellman updates. The method introduces re-weighted targeting (RWT), an operator-level correction that aligns continuation values via a change of measure to compensate for transition mismatch, reducing task shift to a fixed one-step correction and enabling statistically sound data reuse. Experimental results in tabular and neural network settings show that this Bellman alignment approach consistently outperforms single-task learning and naive pooling, with regret bounds scaling with task shift complexity rather than target MDP complexity.</div>
<div class="mono" style="margin-top:8px">本文研究在线迁移强化学习，其中源任务数据可用，但直接重用会因任务相似性度量与贝尔曼更新之间的错位而产生偏差。方法提出了重加权目标（RWT），这是一种算子级校正，通过测度变换对齐连续值以补偿转移不匹配，将任务偏移简化为固定的一步校正，从而实现统计上可靠的数据重用。在表格和神经网络设置中的实验结果表明，这种贝尔曼对齐方法持续优于单任务学习和简单池化，其遗憾界随任务偏移复杂度而非目标MDP复杂度缩放。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Yiqun Chen, Jinyuan Feng, Wei Yang, Meizhi Zhong, Zhengliang Shi, Rui Li, Xiaochi Wei, Yan Gao, Yi Wu, Yao Hu, Zhiqiang Pu, Jiaxin Mao</div>
<div class="meta-line">First: 2026-01-29T16:13:10+00:00 · Latest: 2026-01-29T16:13:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21919v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21919v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多智能体强化学习的思维链自压缩方法</div>
<div class="mono" style="margin-top:8px">冗余推理导致的推理开销会损害交互体验，并严重制约大型推理模型的部署。现有基于强化学习的方法通过将长度惩罚与结果奖励相结合来解决此问题，但这种简单的奖励加权难以平衡简洁性与准确性——强制简洁可能损害关键推理逻辑。本研究提出一种多智能体强化学习框架，通过选择性惩罚冗余推理片段同时保留核心推理逻辑来解决这一局限。我们提出的SCMA框架通过两个专用智能体实现冗余检测与评估：\textbf{分割智能体}负责将推理过程分解为逻辑片段，\textbf{评分智能体}负责量化每个片段的重要性。训练过程中，分割与评分智能体协同定义重要性加权的长度惩罚，激励\textbf{推理智能体}优先处理核心逻辑，且部署时不引入额外推理开销。跨模型规模的实验表明，SCMA能将响应长度减少11.1%至39.0%，同时将准确率提升4.33%至10.02%。消融研究与定性分析进一步验证了多智能体强化学习框架中的协同优化能促进涌现行为，相比传统强化学习范式可产生更强大的大型推理模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to reduce the inference overhead caused by redundant reasoning steps in large reasoning models without compromising accuracy, this paper introduces a multi-agent reinforcement learning framework called Self-Compression via MARL (SCMA). The method employs two specialized agents—a Segmentation Agent to break down reasoning into logical chunks and a Scoring Agent to evaluate each chunk&#x27;s importance—which collaboratively guide a Reasoning Agent to compress non-essential parts while preserving critical logic. Experimental results show that SCMA reduces response length by 11.1% to 39.0% and simultaneously improves accuracy by 4.33% to 10.02% across various model scales, with ablation studies confirming the framework&#x27;s effectiveness in fostering efficient reasoning behaviors.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型推理模型中冗余推理步骤导致的推理开销问题，同时避免损害准确性，为此提出了一种名为SCMA的多智能体强化学习框架。该方法采用两个专用智能体：分割智能体将推理过程分解为逻辑块，评分智能体评估每个块的重要性，二者协同指导推理智能体压缩非必要部分并保留关键逻辑。实验结果表明，SCMA在不同模型规模上将响应长度减少了11.1%至39.0%，同时将准确率提高了4.33%至10.02%，消融研究验证了该框架在促进高效推理行为方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks</div>
<div class="meta-line">Authors: Zixuan Ke, Yifei Ming, Austin Xu, Ryan Chin, Xuan-Phi Nguyen, Prathyusha Jwalapuram, Jiayu Wang, Semih Yavuz, Caiming Xiong, Shafiq Joty</div>
<div class="meta-line">First: 2026-01-21T04:57:02+00:00 · Latest: 2026-01-29T16:05:47+00:00</div>
<div class="meta-line">Comments: Preprint; Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14652v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14652v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MASOrchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented subagents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and subagents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA, while achieving more than 10x efficiency over strong baselines. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAS-Orchestra：通过整体编排与受控基准理解并提升多智能体推理能力</div>
<div class="mono" style="margin-top:8px">尽管多智能体系统（MAS）通过智能体协作有望实现更高智能，但当前自动设计MAS的方法效果不佳。这一不足源于两个关键因素：（1）方法复杂性——现有编排采用顺序化、代码级执行方式，限制了全局系统层面的整体推理能力，且难以随智能体复杂度扩展；（2）效能不确定性——部署MAS时未明确其相比单智能体系统（SAS）是否具有实质优势。我们提出MAS-Orchestra，一种训练时框架，将MAS编排建模为具有整体编排能力的函数调用强化学习问题，可一次性生成完整MAS系统。该框架将复杂的目标导向子智能体抽象为可调用函数，在隐藏内部执行细节的同时实现系统结构的全局推理。为严谨探究MAS何时及为何有效，我们构建了受控基准MASBENCH，从五个维度刻画任务特性：深度、跨度、广度、并行性与鲁棒性。分析表明，MAS的增益关键取决于任务结构、验证协议及编排器与子智能体的能力，而非普遍适用。基于这些发现，MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准上取得稳定提升，同时效率较基线提升超10倍。MAS-Orchestra与MASBENCH共同为追求多智能体智能提供了更优的训练与理解框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underperformance of current multi-agent systems (MAS) due to methodological complexity and uncertain efficacy compared to single-agent systems, this paper introduces MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration to generate entire systems at once. The method abstracts subagents as callable functions, enabling global reasoning while hiding internal details, and is evaluated using MASBENCH, a controlled benchmark characterizing tasks along five axes. Experimental results show that MAS benefits depend on task structure and agent capabilities, and MAS-Orchestra achieves consistent improvements on mathematical reasoning, multi-hop QA, and search-based QA benchmarks with over 10x efficiency gains over strong baselines.</div>
<div class="mono" style="margin-top:8px">针对当前多智能体系统（MAS）因方法复杂性和相比单智能体系统效能不确定而表现不佳的问题，本文提出了MAS-Orchestra，这是一个训练时框架，将MAS编排表述为具有整体编排功能的函数调用强化学习问题，以一次性生成整个系统。该方法将子智能体抽象为可调用函数，在隐藏内部细节的同时实现全局推理，并使用MASBENCH（一个沿五个维度表征任务的受控基准）进行评估。实验结果表明，MAS的收益取决于任务结构和智能体能力，MAS-Orchestra在数学推理、多跳问答和基于搜索的问答基准上取得了持续改进，效率比强基线提高了10倍以上。</div>
</details>
</div>
<div class="card">
<div class="title">ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation</div>
<div class="meta-line">Authors: Zhao Wang, Ziliang Zhao, Zhicheng Dou</div>
<div class="meta-line">First: 2026-01-29T16:04:59+00:00 · Latest: 2026-01-29T16:04:59+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21912v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21912v1">PDF</a> · <a href="https://github.com/lilinwz/ProRAG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to &quot;process hallucinations&quot;, where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProRAG：面向检索增强生成的过程监督强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为优化复杂推理任务中检索增强生成的有前景范式。然而，传统基于结果的强化学习方法常受奖励稀疏性和低效信用分配困扰，因为粗粒度的标量奖励难以识别长轨迹中的具体错误步骤。这种模糊性常导致&#x27;过程幻觉&#x27;，即模型通过有缺陷的逻辑或冗余检索步骤得出正确答案。尽管近期过程感知方法尝试通过静态偏好学习或启发式奖励塑形缓解此问题，但往往缺乏将步骤级信用与全局结果解耦所需的在线策略探索能力。为应对这些挑战，我们提出ProRAG——一个将习得的步骤级监督集成至在线优化循环的过程监督强化学习框架。该框架包含四个阶段：(1)监督策略预热：用结构化推理格式初始化模型；(2)构建基于蒙特卡洛树搜索的过程奖励模型以量化中间推理质量；(3)过程奖励模型引导的推理优化：使策略与细粒度过程偏好对齐；(4)采用双粒度优势机制的过程监督强化学习。通过聚合步骤级过程奖励与全局结果信号，ProRAG为每个动作提供精确反馈。在五个多跳推理基准上的大量实验表明，相较于强结果导向和过程感知的强化学习基线，ProRAG实现了更优的整体性能，尤其在复杂长程任务中验证了细粒度过程监督的有效性。代码与模型发布于https://github.com/lilinwz/ProRAG。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces ProRAG, a process-supervised reinforcement learning framework designed to optimize Retrieval-Augmented Generation (RAG) for complex reasoning. The motivation stems from the limitations of traditional outcome-based RL, which suffers from sparse rewards and inefficient credit assignment, often leading to process hallucinations where models use flawed logic despite correct answers. The proposed method integrates step-level supervision through a four-stage process: supervised policy warmup, construction of a Monte Carlo Tree Search-based Process Reward Model (PRM), PRM-guided reasoning refinement, and process-supervised RL with a dual-granularity advantage mechanism. Experimental results on five multi-hop reasoning benchmarks show that ProRAG outperforms both outcome-based and process-aware RL baselines, particularly in long-horizon tasks, demonstrating the effectiveness of fine-grained process supervision.</div>
<div class="mono" style="margin-top:8px">本文提出了ProRAG，一个过程监督的强化学习框架，旨在优化检索增强生成（RAG）在复杂推理任务中的性能。其动机源于传统基于结果的强化学习方法存在奖励稀疏和信用分配效率低下的问题，常导致过程幻觉，即模型虽得出正确答案但推理逻辑存在缺陷。该方法通过四个阶段整合步骤级监督：监督策略预热、构建基于蒙特卡洛树搜索的过程奖励模型（PRM）、PRM引导的推理细化，以及采用双粒度优势机制的过程监督强化学习。在五个多跳推理基准上的实验结果表明，ProRAG在整体性能上优于基于结果和过程感知的强化学习基线，尤其在长视野任务中表现突出，验证了细粒度过程监督的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning</div>
<div class="meta-line">Authors: Zhihao Lin</div>
<div class="meta-line">First: 2025-11-11T13:32:38+00:00 · Latest: 2026-01-29T16:01:23+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08234v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.08234v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \(2d\) to \(d+1\), and avoids the \(O(dk)\) complexity of vMF rejection sampling, achieving simple \(O(d)\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\% improvement over SAC on Ant-v4 and up to 112\% on complex DMControl tasks, demonstrating strong performance across diverse benchmarks. Our ablation studies reveal that both \textbf{spherical normalization} and \textbf{adaptive concentration control} are essential to GAC&#x27;s success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越分布：连续强化学习的几何动作控制</div>
<div class="mono" style="margin-top:8px">高斯策略在深度强化学习的连续控制领域占据主导地位，但其存在根本性不匹配：其无界支撑集需要临时性的压缩函数，这会扭曲有界动作空间的几何结构。虽然冯·米塞斯-费舍尔分布在球面上提供了理论完备的替代方案，但其对贝塞尔函数和拒绝采样的依赖阻碍了实际应用。我们提出**几何动作控制**，这是一种新颖的动作生成范式，既保留了球形分布的几何优势，又**简化了计算**。GAC将动作生成分解为方向向量和可学习的集中度参数，实现了确定性动作与均匀球形噪声之间的高效插值。该设计将参数量从\(2d\)减少到\(d+1\)，并避免了vMF拒绝采样\(O(dk)\)的复杂度，实现了简单的\(O(d)\)操作。实证表明，GAC在六项MuJoCo基准测试中始终匹配或超越最先进方法，在Ant-v4上相比SAC提升37.6%，在复杂DMControl任务上最高提升112%，展现了跨基准的强劲性能。消融研究表明，**球形归一化**与**自适应集中度控制**对GAC的成功均至关重要。这些发现表明，稳健高效的连续控制并不需要复杂分布，而是需要对动作空间几何结构的原则性尊重。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the geometric mismatch and computational inefficiency of Gaussian and von Mises-Fisher distributions in continuous reinforcement learning, this paper introduces Geometric Action Control (GAC), a method that decomposes action generation into a direction vector and a learnable concentration parameter to preserve spherical geometry while simplifying computation. The method reduces parameters from 2d to d+1 and achieves O(d) operational complexity, avoiding costly rejection sampling. Experimental results on MuJoCo and DMControl benchmarks show GAC matches or exceeds state-of-the-art methods, with up to 112% improvement on complex tasks, and ablation studies confirm the importance of spherical normalization and adaptive concentration control.</div>
<div class="mono" style="margin-top:8px">针对连续强化学习中高斯分布和冯·米塞斯-费希尔分布存在的几何失配与计算效率低下问题，本文提出了几何动作控制方法，该方法将动作生成分解为方向向量和可学习的集中度参数，以在保持球面几何特性的同时简化计算。该方法将参数量从2d减少到d+1，实现了O(d)的计算复杂度，避免了耗时的拒绝采样。在MuJoCo和DMControl基准测试上的实验结果表明，GAC达到或超越了现有最优方法，在复杂任务上最高提升112%，消融研究证实了球面归一化和自适应集中度控制的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models</div>
<div class="meta-line">Authors: Kang Chen, Zhihao Liu, Tonghe Zhang, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Xiang Li, Quanlu Zhang, Zhaofei Yu, Guoliang Fan, Tiejun Huang, Yu Wang, Chao Yu</div>
<div class="meta-line">First: 2025-10-29T18:37:39+00:00 · Latest: 2026-01-29T16:00:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25889v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.25889v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying RL to large-scale flow-based VLAs (\eg, $π_0$, $π_{0.5}$) remains challenging due to intractable action log-likelihoods raised from flow matching. We address this challenge with $π_{\texttt{RL}}$, featuring two technical approaches: (1) \textbf{Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) \textbf{Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate $π_{\texttt{RL}}$ across various benchmarks, with experiments demonstrating that RL yields significant performance improvements in both in-distribution and out-of-distribution settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$π_\texttt{RL}$：基于流的视觉-语言-动作模型的在线强化学习微调</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型使机器人能够根据多模态输入理解并执行复杂任务。尽管近期研究探索使用强化学习（RL）来自动化监督微调（SFT）扩展中繁琐的数据收集过程，但由于流匹配产生的动作对数似然难以处理，将RL应用于大规模基于流的VLA模型（例如$π_0$、$π_{0.5}$）仍具挑战。我们通过$π_{\texttt{RL}}$应对这一挑战，其包含两项技术方案：（1）\textbf{流噪声}将去噪过程建模为离散时间MDP，通过可学习的噪声网络实现精确对数似然计算。（2）\textbf{流随机微分方程}将去噪与智能体-环境交互结合，构建双层MDP框架，利用ODE至SDE转换实现高效的RL探索。我们在多个基准测试中评估$π_{\texttt{RL}}$，实验表明RL在分布内与分布外场景下均能带来显著的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable reinforcement learning (RL) fine-tuning for large-scale flow-based Vision-Language-Action (VLA) models, which is challenging due to the intractable action log-likelihoods inherent in flow matching. The method introduces $π_{\texttt{RL}}$, which employs two technical approaches: Flow-Noise, modeling the denoising process as a discrete-time Markov Decision Process (MDP) with a learnable noise network for exact log-likelihood computation, and Flow-SDE, integrating denoising with agent-environment interaction via a two-layer MDP that uses ODE-to-SDE conversion for efficient RL exploration. The main experimental results show that RL fine-tuning with $π_{\texttt{RL}}$ yields significant performance improvements across various benchmarks in both in-distribution and out-of-distribution settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机是针对基于流的视觉-语言-动作大模型，解决其因流匹配导致动作对数似然难以计算，从而阻碍强化学习微调的挑战。方法上提出了$π_{\texttt{RL}}$，包含两项技术：Flow-Noise将去噪过程建模为离散时间马尔可夫决策过程，并引入可学习的噪声网络以精确计算对数似然；Flow-SDE则将去噪与智能体-环境交互结合，通过一个两层马尔可夫决策过程，利用ODE到SDE的转换实现高效的强化学习探索。主要实验结果表明，使用$π_{\texttt{RL}}$进行强化学习微调在多种基准测试中，无论是分布内还是分布外场景，均带来了显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning</div>
<div class="meta-line">Authors: Shaojie Wang, Liang Zhang</div>
<div class="meta-line">First: 2026-01-29T16:00:48+00:00 · Latest: 2026-01-29T16:00:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21909v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21909v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从元思维到执行：基于认知对齐的后训练方法实现可泛化且可靠的大语言模型推理</div>
<div class="mono" style="margin-top:8px">当前大语言模型后训练方法通过监督微调与基于结果的强化学习优化完整推理轨迹。虽有效，但深入分析揭示其根本缺陷：该方法未契合人类实际解题方式。人类认知自然将问题解决分解为两个阶段：先获取可跨问题泛化的抽象策略（即元知识），再将其适配至具体实例。相比之下，当前方法以完整轨迹为基本单元，本质上是问题中心的，将抽象策略与问题特定执行相纠缠。为解决这一错位，我们提出受认知启发的框架，显式模拟人类两阶段认知过程。具体而言，元思维链专注于通过监督学习获取不含具体执行的抽象推理模式，从而习得可泛化策略；置信度校准强化学习随后通过中间步骤的置信感知奖励优化任务适配，防止过度自信误差级联传播并提升执行可靠性。在四个模型与八个基准测试上的实验表明，相比标准方法，本方法在分布内与分布外场景分别提升2.19%与4.63%，同时减少65-70%训练时间与50%令牌消耗，证明使后训练与人类认知原则对齐不仅能实现更优泛化能力，还可显著提升训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the misalignment between current LLM post-training methods and human cognitive processes, which naturally separate abstract strategy acquisition from specific execution, this paper proposes a cognitively-inspired two-stage framework. The method, Chain-of-Meta-Thought (CoMT), uses supervised learning to capture generalizable reasoning patterns, followed by Confidence-Calibrated Reinforcement Learning (CCRL) to optimize reliable task adaptation with confidence-aware rewards. Experimental results across four models and eight benchmarks show improvements of 2.19% in-distribution and 4.63% out-of-distribution over standard methods, while also reducing training time by 65-70% and token consumption by 50%, demonstrating enhanced generalization and efficiency.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有大语言模型后训练方法与人类认知过程存在偏差，人类解决问题时会将抽象策略获取与具体执行分离。为此，作者提出一个受认知启发的两阶段框架：其方法首先通过链式元思维进行监督学习以获取可泛化的推理模式，随后使用置信度校准的强化学习，通过基于置信度的奖励来优化可靠的任务适应。主要实验结果表明，在四个模型和八个基准测试上，该方法相比标准方法在分布内和分布外性能分别提升了2.19%和4.63%，同时训练时间减少了65-70%，令牌消耗降低了50%，证明了其在泛化能力和训练效率上的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed when Solving Expensive Unconstrained Multi-Objective Optimisation Problems</div>
<div class="meta-line">Authors: Tiwonge Msulira Banda, Alexandru-Ciprian Zăvoianu</div>
<div class="meta-line">First: 2026-01-29T15:46:52+00:00 · Latest: 2026-01-29T15:46:52+00:00</div>
<div class="meta-line">Comments: Accepted for publication in SWEVO (Swarm and Evolutionary Computation)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21885v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21885v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Objective Evolutionary Algorithms (MOEAs) have proven effective at solving Multi-Objective Optimisation Problems (MOOPs). However, their performance can be significantly hindered when applied to computationally intensive industrial problems. To address this limitation, we propose an adaptive surrogate modelling approach designed to accelerate the early-stage convergence speed of state-of-the-art MOEAs. This is important because it ensures that a solver can identify optimal or near-optimal solutions with relatively few fitness function evaluations, thereby saving both time and computational resources. Our method employs a two-loop architecture. The outer loop runs a (baseline) host MOEA which carries out true fitness evaluations. The inner loop contains an Adaptive Accelerator that leverages data-driven machine learning (ML) surrogate models to approximate fitness functions. Integrated with NSGA-II and MOEA/D, our approach was tested on 31 widely known benchmark problems and a real-world North Sea fish abundance modelling case study. The results demonstrate that by incorporating Gaussian Process Regression, one-dimensional Convolutional Neural Networks, and Random Forest Regression, our proposed approach significantly accelerates the convergence speed of MOEAs in the early phases of optimisation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应代理模型的策略用于加速求解昂贵无约束多目标优化问题的收敛速度</div>
<div class="mono" style="margin-top:8px">多目标进化算法已被证明能有效解决多目标优化问题，但在处理计算密集型工业问题时性能可能显著受限。为克服此局限，本文提出一种自适应代理建模方法，旨在加速前沿多目标进化算法的早期收敛速度。该方法的重要性在于确保求解器能以较少的适应度函数评估次数找到最优或近似最优解，从而节省时间和计算资源。我们采用双循环架构：外循环运行执行真实适应度评估的宿主多目标进化算法，内循环则包含一个利用数据驱动的机器学习代理模型来近似适应度函数的自适应加速器。该方法与NSGA-II和MOEA/D集成，在31个广为人知的基准问题及一个北海鱼类丰度建模的实际案例中进行了测试。结果表明，通过结合高斯过程回归、一维卷积神经网络和随机森林回归，所提方法能显著提升多目标进化算法在优化初期的收敛速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the computational burden of applying Multi-Objective Evolutionary Algorithms (MOEAs) to expensive, real-world optimisation problems. The proposed method introduces an adaptive surrogate-based strategy with a two-loop architecture: an outer loop running a baseline MOEA for true evaluations and an inner Adaptive Accelerator using machine learning surrogates (Gaussian Process Regression, 1D CNNs, Random Forest) to approximate fitness functions. Experimental results on 31 benchmark problems and a real-world fish abundance case study, integrated with NSGA-II and MOEA/D, show the approach significantly accelerates early-stage convergence speed, enabling near-optimal solutions with fewer expensive evaluations.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多目标进化算法应用于计算密集型工业问题时性能受限的挑战。方法上提出了一种自适应代理模型策略，采用双循环架构：外循环运行基准MOEA进行真实适应度评估，内循环则利用数据驱动的机器学习代理模型（如高斯过程回归、一维卷积神经网络和随机森林回归）来近似适应度函数。在31个经典基准问题和北海鱼类丰度建模的实际案例中，与NSGA-II和MOEA/D集成测试的结果表明，该方法能显著加速优化早期阶段的收敛速度，从而以更少的昂贵函数评估获得近似最优解。</div>
</details>
</div>
<div class="card">
<div class="title">Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model</div>
<div class="meta-line">Authors: Chen Wang, Sijie Ma, Zeyuan Ma, Yue-Jiao Gong</div>
<div class="meta-line">First: 2026-01-29T15:45:11+00:00 · Latest: 2026-01-29T15:45:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21877v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Benchmark Design in Black-Box Optimization (BBO) is a fundamental yet open-ended topic. Early BBO benchmarks are predominantly human-crafted, introducing expert bias and constraining diversity. Automating this design process can relieve the human-in-the-loop burden while enhancing diversity and objectivity. We propose Evolution of Benchmark (EoB), an automated BBO benchmark designer empowered by the large language model (LLM) and its program evolution capability. Specifically, we formulate benchmark design as a bi-objective optimization problem towards maximizing (i) landscape diversity and (ii) algorithm-differentiation ability across a portfolio of BBO solvers. Under this paradigm, EoB iteratively prompts LLM to evolve a population of benchmark programs and employs a reflection-based scheme to co-evolve the landscape and its corresponding program. Comprehensive experiments validate our EoB is a competitive candidate in multi-dimensional usages: 1) Benchmarking BBO algorithms; 2) Training and testing learning-assisted BBO algorithms; 3) Extending proxy for expensive real-world problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基准演化：基于大语言模型的黑盒优化基准设计</div>
<div class="mono" style="margin-top:8px">黑盒优化（BBO）的基准设计是一个基础但开放的研究课题。早期的BBO基准主要由人工构建，存在专家偏见且多样性受限。自动化设计流程既能减轻人工参与负担，又能提升多样性与客观性。本文提出“基准演化”（EoB），一种基于大语言模型（LLM）及其程序演化能力的自动化BBO基准设计框架。具体而言，我们将基准设计建模为双目标优化问题，旨在最大化（1）景观多样性，以及（2）在BBO求解器组合中的算法区分能力。在此范式下，EoB通过迭代提示LLM演化基准程序种群，并采用基于反思的机制协同演化景观及其对应程序。综合实验验证了EoB在多维应用场景中的竞争力：1）BBO算法基准测试；2）学习辅助型BBO算法的训练与测试；3）昂贵现实问题的代理扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of human-crafted benchmarks in Black-Box Optimization (BBO), which often suffer from expert bias and limited diversity, this paper introduces Evolution of Benchmark (EoB), an automated designer that leverages large language models (LLMs) to evolve benchmark programs. The method formulates benchmark design as a bi-objective optimization problem aimed at maximizing landscape diversity and algorithm-differentiation ability, using LLMs to iteratively evolve programs alongside a reflection-based co-evolution scheme. Experimental results demonstrate that EoB is effective for benchmarking BBO algorithms, training and testing learning-assisted BBO methods, and serving as a proxy for costly real-world problems.</div>
<div class="mono" style="margin-top:8px">针对黑盒优化中人工设计基准存在的专家偏见和多样性不足问题，本研究提出了Evolution of Benchmark（EoB），一种利用大语言模型自动设计基准的方法。该方法将基准设计构建为一个双目标优化问题，旨在最大化景观多样性和算法区分能力，通过大语言模型迭代演化程序，并采用基于反思的协同演化策略。综合实验验证表明，EoB在黑盒优化算法基准测试、学习辅助算法的训练与测试，以及替代昂贵现实问题方面均表现出竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents</div>
<div class="meta-line">Authors: Yao Zhang, Shijie Tang, Zeyu Li, Zhen Han, Volker Tresp</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T15:39:50+00:00 · Latest: 2026-01-29T15:39:50+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21872v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21872v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebArbiter：一种面向网络智能体的原则引导推理过程奖励模型</div>
<div class="mono" style="margin-top:8px">网络智能体在自动化复杂计算机任务方面潜力巨大，但其交互涉及长周期、序列化的决策过程，且行动不可逆转。在此类场景中，基于结果的监督信号稀疏且延迟，常错误奖励无效轨迹，难以支持推理时的扩展。这促使了过程奖励模型在网络导航中的应用，但现有方法仍存在局限：标量型WebPRM将进展压缩为粗糙、弱基础的信号，而清单式WebPRM依赖脆弱的模板匹配，在界面布局或语义变化时易失效，常将表面正确的行动误判为成功，缺乏可解释性。为解决这些问题，我们提出WebArbiter——一种推理优先、原则引导的WebPRM，将奖励建模转化为文本生成任务，生成结构化论证并最终给出偏好裁决，识别当前情境下最有利于任务完成的行动。训练采用两阶段流程：推理蒸馏使模型掌握连贯的原则引导推理，强化学习通过直接对齐裁决与正确性来修正教师模型偏差，从而提升泛化能力。为支持系统化评估，我们发布了WebPRMBench，这是一个涵盖四种异构网络环境、包含丰富任务与高质量偏好标注的综合基准。在WebPRMBench上，WebArbiter-7B以9.1分优势超越最强基线GPT-5。在WebArena-Lite的奖励引导轨迹搜索中，其表现较先前最佳WebPRM提升高达7.2分，彰显了其在现实复杂网络任务中的鲁棒性与实用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenges of training web agents, where sparse and delayed outcome-based rewards often fail to guide long-horizon, sequential decision-making effectively. To address this, the authors introduce WebArbiter, a principle-guided reasoning process reward model that formulates reward modeling as text generation, producing structured justifications and preference verdicts to identify optimal actions. The method employs a two-stage training pipeline involving reasoning distillation and reinforcement learning to enhance generalization. Experimental results on the newly released WebPRMBench show that WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points, and achieves up to a 7.2-point improvement in reward-guided trajectory search on WebArena-Lite, demonstrating robust performance in complex web tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机源于训练网络智能体面临的挑战，即稀疏且延迟的结果监督难以有效指导长视野、序列化的决策过程。为解决此问题，作者提出了WebArbiter，一种基于原则推理的过程奖励模型，它将奖励建模转化为文本生成任务，生成结构化理由和偏好判决以识别最优行动。该方法采用两阶段训练流程，包括推理蒸馏和强化学习，以提升泛化能力。实验结果表明，在新发布的WebPRMBench基准上，WebArbiter-7B以9.1分的优势超越了最强基线GPT-5；在WebArena-Lite的奖励引导轨迹搜索中，其性能较先前最佳WebPRM提升了7.2分，证明了其在复杂网络任务中的鲁棒性和实用价值。</div>
</details>
</div>
<div class="card">
<div class="title">READY: Reward Discovery for Meta-Black-Box Optimization</div>
<div class="meta-line">Authors: Zechuan Huang, Zhiguang Cao, Hongshu Guo, Yue-Jiao Gong, Zeyuan Ma</div>
<div class="meta-line">First: 2026-01-29T15:23:18+00:00 · Latest: 2026-01-29T15:23:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21847v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21847v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY&#x27;s project at https://anonymous.4open.science/r/ICML_READY-747F.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>READY：元黑盒优化的奖励发现方法</div>
<div class="mono" style="margin-top:8px">元黑盒优化是优化领域的新兴方向，其算法设计策略可通过强化学习进行元学习以提升优化性能。现有研究中的奖励函数均由专家人工设计，存在设计偏差和奖励破解风险。本文提出使用大语言模型作为元黑盒优化的自动化奖励发现工具。在有效性方面，借鉴启发式演化思想，在基于LLM的迭代程序搜索过程中引入定制化演化范式，确保持续改进；在效率方面，引入多任务演化架构，支持为不同元黑盒优化方法并行发现奖励函数，并通过跨任务知识共享加速收敛。实验表明，该方法发现的奖励函数能有效提升现有元黑盒优化性能，印证了奖励设计的重要性。项目地址：https://anonymous.4open.science/r/ICML_READY-747F。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reward function design in Meta-Black-Box Optimization (MetaBBO), where human-designed rewards can introduce bias and reward hacking risks. The authors propose READY, a method that uses a Large Language Model (LLM) as an automated reward discovery tool, incorporating an evolutionary paradigm for continuous improvement on effectiveness and a multi-task architecture for efficient parallel discovery across different MetaBBO approaches. Experimental results show that the discovered reward functions successfully enhance the performance of existing MetaBBO methods, highlighting the critical role of automated reward design.</div>
<div class="mono" style="margin-top:8px">本文针对元黑盒优化中奖励函数设计存在的偏见和奖励破解风险，提出了一种自动化奖励发现方法READY。该方法利用大语言模型作为工具，通过引入定制化的进化范式确保奖励函数的持续改进，并采用多任务进化架构支持并行发现以提高效率。实验结果表明，所发现的奖励函数能有效提升现有元黑盒优化方法的性能，强调了自动化奖励设计的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Meta Reinforcement Learning with Provable Test-Time Safety</div>
<div class="meta-line">Authors: Tingting Ni, Maryam Kamgarpour</div>
<div class="meta-line">First: 2026-01-29T15:21:37+00:00 · Latest: 2026-01-29T15:21:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21845v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21845v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有可证明测试时安全性的约束元强化学习</div>
<div class="mono" style="margin-top:8px">元强化学习（RL）使智能体能够利用在任务分布上的经验进行自主训练，从而在新测试任务上更快学习最优策略。尽管其在降低测试任务样本复杂度方面取得成功，但许多实际应用（如机器人和医疗保健）在测试期间需满足安全约束。约束元强化学习为将安全性融入元RL提供了有前景的框架。该领域的一个开放性问题是如何在降低样本复杂度的同时，确保策略在现实世界测试任务中的安全性，从而实现更快速的最优策略学习。针对这一空白，我们提出一种算法，通过优化训练期间习得的策略，为测试任务学习近似最优策略提供可证明的安全性与样本复杂度保证。我们进一步推导出匹配下界，证明该样本复杂度是紧致的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for safe meta reinforcement learning (RL) in real-world applications like robotics and healthcare, where test-time safety constraints are critical. The authors propose a constrained meta RL algorithm that refines policies learned during training to ensure provable safety during testing while maintaining sample efficiency. Their main experimental results demonstrate that the algorithm achieves provable safety guarantees and a near-optimal policy on test tasks with tight sample complexity, supported by a matching lower bound confirming optimality.</div>
<div class="mono" style="margin-top:8px">本文针对机器人技术和医疗保健等现实应用中测试时安全约束的关键需求，提出了安全的元强化学习（RL）方法。作者设计了一种约束元RL算法，通过细化训练期间学习的策略，在测试时确保可证明的安全性，同时保持样本效率。主要实验结果表明，该算法在测试任务上实现了可证明的安全保证和接近最优的策略，并具有紧密的样本复杂度，匹配的下界进一步证实了其最优性。</div>
</details>
</div>
<div class="card">
<div class="title">Error Amplification Limits ANN-to-SNN Conversion in Continuous Control</div>
<div class="meta-line">Authors: Zijie Xu, Zihan Huang, Yiting Dong, Kang Chen, Wenxuan Liu, Zhaofei Yu</div>
<div class="meta-line">First: 2026-01-29T14:28:00+00:00 · Latest: 2026-01-29T14:28:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spiking Neural Networks (SNNs) can achieve competitive performance by converting already existing well-trained Artificial Neural Networks (ANNs), avoiding further costly training. This property is particularly attractive in Reinforcement Learning (RL), where training through environment interaction is expensive and potentially unsafe. However, existing conversion methods perform poorly in continuous control, where suitable baselines are largely absent. We identify error amplification as the key cause: small action approximation errors become temporally correlated across decision steps, inducing cumulative state distribution shift and severe performance degradation. To address this issue, we propose Cross-Step Residual Potential Initialization (CRPI), a lightweight training-free mechanism that carries over residual membrane potentials across decision steps to suppress temporally correlated errors. Experiments on continuous control benchmarks with both vector and visual observations demonstrate that CRPI can be integrated into existing conversion pipelines and substantially recovers lost performance. Our results highlight continuous control as a critical and challenging benchmark for ANN-to-SNN conversion, where small errors can be strongly amplified and impact performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>误差放大效应限制人工神经网络向脉冲神经网络在连续控制中的转换</div>
<div class="mono" style="margin-top:8px">脉冲神经网络（SNNs）可通过转换已训练成熟的人工神经网络（ANNs）获得竞争力，避免额外的高成本训练。这一特性在强化学习（RL）中尤为吸引人，因为通过环境交互进行训练既昂贵又存在安全隐患。然而，现有转换方法在连续控制任务中表现不佳，且该领域缺乏合适的基准。我们指出误差放大是主要原因：微小的动作近似误差会在决策步骤间产生时间相关性，引发累积性状态分布偏移和严重的性能下降。为解决此问题，我们提出跨步残差膜电位初始化（CRPI），这是一种轻量级免训练机制，通过在决策步骤间传递残差膜电位来抑制时间相关误差。在基于向量观测和视觉观测的连续控制基准测试中，实验表明CRPI可集成至现有转换流程，并显著恢复损失的性能。我们的研究结果强调，连续控制是ANN向SNN转换的关键且具有挑战性的基准领域，其中微小误差可能被强烈放大并影响性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the poor performance of existing ANN-to-SNN conversion methods in continuous control reinforcement learning tasks, where training SNNs directly is costly and unsafe. The method proposed is Cross-Step Residual Potential Initialization (CRPI), a lightweight, training-free mechanism that suppresses temporally correlated action approximation errors by carrying over residual membrane potentials across decision steps. Experimental results on continuous control benchmarks with vector and visual observations show that CRPI, when integrated into existing conversion pipelines, substantially recovers the performance lost due to error amplification, highlighting continuous control as a critical and challenging domain for conversion.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有ANN到SNN转换方法在连续控制强化学习任务中性能不佳的问题，因为直接训练SNN成本高且不安全。提出的方法是跨步残差膜电位初始化（CRPI），这是一种轻量级、无需训练的机制，通过跨决策步传递残差膜电位来抑制时间相关的动作近似误差。在具有向量和视觉观察的连续控制基准测试中，实验结果表明，将CRPI集成到现有转换流程中，能显著恢复因误差放大而损失的性能，凸显了连续控制作为转换领域关键且具有挑战性的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Language-based Trial and Error Falls Behind in the Era of Experience</div>
<div class="meta-line">Authors: Haoyu Wang, Guozheng Ma, Shugang Cui, Yilun Kong, Haotian Luo, Li Shen, Mengya Gao, Yichao Wu, Xiaogang Wang, Dacheng Tao</div>
<div class="meta-line">First: 2026-01-29T14:08:41+00:00 · Latest: 2026-01-29T14:08:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21754v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21754v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight &quot;scouts&quot; (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>经验时代下基于语言的试错方法已显滞后</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLMs）在基于语言的代理任务中表现出色，但其在未见过的非语言环境（如符号或空间任务）中的适用性仍然有限。先前研究将这种性能差距归因于预训练分布与测试分布之间的不匹配。本文指出，主要瓶颈在于探索成本过高：掌握这些任务需要大量试错，这对于在高维语义空间中运行、参数量庞大的LLMs而言，在计算上是不可持续的。为此，我们提出SCOUT（未见任务子尺度协作框架），一种将探索与利用解耦的新方法。我们采用轻量级“侦察器”（如小型MLPs）以远超LLMs的速度和规模探测环境动态，收集的轨迹通过监督微调（SFT）引导LLM，再经多轮强化学习（RL）激活其潜在的世界知识。实验表明，SCOUT使Qwen2.5-3B-Instruct模型平均得分达到0.86，显著优于包括Gemini-2.5-Pro（0.60）在内的专有模型，同时节省约60%的GPU时耗。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of Large Language Models (LLMs) in adapting to novel, nonlinguistic environments like symbolic or spatial tasks, where their reliance on language-based trial-and-error proves computationally prohibitive due to the high cost of exploration in a vast semantic space. To overcome this, the authors propose SCOUT, a framework that decouples exploration from exploitation by using lightweight &#x27;scout&#x27; models to efficiently probe environmental dynamics, whose collected trajectories are then used to bootstrap the LLM via supervised fine-tuning and multi-turn reinforcement learning. Experimental results show that SCOUT enables a relatively small Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming larger proprietary models like Gemini-2.5-Pro (0.60) while reducing GPU consumption by about 60%.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在适应新颖的非语言环境（如符号或空间任务）时的局限性展开研究，指出其依赖语言试错的探索方式在计算上成本过高，成为主要瓶颈。为此，作者提出了SCOUT框架，通过解耦探索与利用，使用轻量级&#x27;侦察&#x27;模型高效探测环境动态，并利用收集的轨迹通过监督微调和多轮强化学习来引导大语言模型。实验结果表明，SCOUT使较小的Qwen2.5-3B-Instruct模型平均得分达到0.86，显著优于Gemini-2.5-Pro等大型专有模型（得分0.60），同时节省了约60%的GPU计算时间。</div>
</details>
</div>
<div class="card">
<div class="title">Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems</div>
<div class="meta-line">Authors: Ruiwen Zhou, Maojia Song, Xiaobao Wu, Sitao Cheng, Xunjian Yin, Yuxi Xie, Zhuoqun Hao, Wenyue Hua, Liangming Pan, Soujanya Poria, Min-Yen Kan</div>
<div class="meta-line">First: 2026-01-29T13:59:32+00:00 · Latest: 2026-01-29T13:59:32+00:00</div>
<div class="meta-line">Comments: Codes and data are available at https://github.com/skyriver-2000/epistemic-context-learning</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21742v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21742v1">PDF</a> · <a href="https://github.com/skyriver-2000/epistemic-context-learning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>认知上下文学习：在基于大语言模型的多智能体系统中建立正确信任机制</div>
<div class="mono" style="margin-top:8px">多智能体系统中的个体智能体常因缺乏鲁棒性而盲目遵从误导性同伴。研究表明该弱点源于谄媚倾向与评估同伴可靠性能力的不足。为此，我们首先形式化历史感知参照的学习问题，将同伴历史交互作为附加输入，使智能体能够评估同伴可靠性并在不确定时向可信同伴学习。这将任务重心从评估同伴推理质量转向基于交互历史估计同伴可靠性。我们继而提出认知上下文学习框架：该推理框架基于历史构建的显式同伴画像进行预测。通过强化学习的辅助奖励机制进一步优化该框架。实验表明，ECL能使Qwen 3-4B等小型模型通过精准识别可靠同伴，超越规模为其8倍的历史无关基线模型（Qwen 3-30B）。该框架还能推动前沿模型实现接近完美（100%）的性能。研究证实ECL可良好泛化至多种多智能体配置，且大语言模型能有效建模信任机制——信任建模准确度与最终答案质量呈现强相关性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observed lack of robustness in individual agents within multi-agent systems, where they often blindly conform to misleading peers due to sycophancy and an inability to evaluate peer reliability. To address this, the method formalizes a history-aware reference learning problem, introducing peer interaction history as input to estimate reliability, and develops Epistemic Context Learning (ECL), a reasoning framework that conditions predictions on explicitly-built peer profiles from history, further optimized via reinforcement learning with auxiliary rewards. Experimental results show that ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline eight times its size (Qwen 3-30B) by accurately identifying reliable peers, boosts frontier models to near-perfect performance, generalizes well across various multi-agent configurations, and reveals a strong correlation between trust modeling accuracy and final answer quality.</div>
<div class="mono" style="margin-top:8px">本文的动机源于多智能体系统中个体智能体缺乏鲁棒性，常因盲从和无法评估同伴可靠性而错误地遵从误导性同伴。为解决此问题，方法首先形式化了历史感知的参考学习问题，引入同伴交互历史作为额外输入以估计其可靠性，并提出了认知上下文学习（ECL）这一推理框架，该框架基于从历史中显式构建的同伴档案进行预测，并通过强化学习与辅助奖励进一步优化。实验结果表明，ECL使小型模型如Qwen 3-4B能够通过准确识别可靠同伴，超越其八倍大小的历史无关基线模型（Qwen 3-30B），同时将前沿模型提升至接近完美的性能，且能良好泛化到多种多智能体配置中，并揭示了信任建模准确性与最终答案质量之间的强相关性。</div>
</details>
</div>
<div class="card">
<div class="title">ASAP: Exploiting the Satisficing Generalization Edge in Neural Combinatorial Optimization</div>
<div class="meta-line">Authors: Han Fang, Paul Weng, Yutong Ban</div>
<div class="meta-line">First: 2025-01-29T02:12:34+00:00 · Latest: 2026-01-29T13:56:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.17377v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.17377v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (DRL) has emerged as a promising approach for solving Combinatorial Optimization (CO) problems, such as the 3D Bin Packing Problem (3D-BPP), Traveling Salesman Problem (TSP), or Vehicle Routing Problem (VRP), but these neural solvers often exhibit brittleness when facing distribution shifts. To address this issue, we uncover the Satisficing Generalization Edge, which we validate both theoretically and experimentally: identifying a set of promising actions is inherently more generalizable than selecting the single optimal action. To exploit this property, we propose Adaptive Selection After Proposal (ASAP), a generic framework that decomposes the decision-making process into two distinct phases: a proposal policy that acts as a robust filter, and a selection policy as an adaptable decision maker. This architecture enables a highly effective online adaptation strategy where the selection policy can be rapidly fine-tuned on a new distribution. Concretely, we introduce a two-phase training framework enhanced by Model-Agnostic Meta-Learning (MAML) to prime the model for fast adaptation. Extensive experiments on 3D-BPP, TSP, and CVRP demonstrate that ASAP improves the generalization capability of state-of-the-art baselines and achieves superior online adaptation on out-of-distribution instances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ASAP：利用神经组合优化中的满意泛化边界</div>
<div class="mono" style="margin-top:8px">深度强化学习已成为解决组合优化问题的有效方法，如三维装箱问题、旅行商问题或车辆路径问题，但这些神经求解器在面对分布偏移时往往表现出脆弱性。为解决此问题，我们揭示了满意泛化边界，并从理论和实验上验证了其有效性：识别一组有潜力的动作本质上比选择单一最优动作更具泛化性。为利用这一特性，我们提出了自适应提议后选择框架，该通用框架将决策过程分解为两个独立阶段：作为鲁棒过滤器的提议策略，以及作为自适应决策器的选择策略。该架构支持高效的在线适应策略，使选择策略能在新分布上快速微调。具体而言，我们引入了通过模型无关元学习增强的两阶段训练框架，为模型实现快速适应奠定基础。在三维装箱问题、旅行商问题和带容量约束的车辆路径问题上的大量实验表明，该框架提升了先进基线的泛化能力，并在分布外实例上实现了卓越的在线适应性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the brittleness of deep reinforcement learning neural solvers for combinatorial optimization problems under distribution shifts by uncovering and exploiting the Satisficing Generalization Edge, where identifying a set of promising actions is more generalizable than selecting a single optimal one. The proposed method, Adaptive Selection After Proposal (ASAP), decomposes decision-making into a robust proposal policy and an adaptable selection policy, enhanced by a two-phase training framework using Model-Agnostic Meta-Learning to enable rapid online adaptation. Experimental results on 3D bin packing, traveling salesman, and capacitated vehicle routing problems show that ASAP improves generalization over state-of-the-art baselines and achieves superior performance on out-of-distribution instances through fast fine-tuning.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习神经求解器在组合优化问题中面对分布偏移时的脆弱性，揭示并利用了满意泛化边界，即识别一组有希望的动作比选择单一最优动作更具泛化性。所提出的方法ASAP将决策过程分解为稳健的提议策略和可适应的选择策略，并通过结合模型无关元学习的两阶段训练框架来实现快速在线适应。在三维装箱、旅行商和带容量车辆路径问题上的实验表明，ASAP提升了先进基线的泛化能力，并通过快速微调在分布外实例上取得了更优的适应性能。</div>
</details>
</div>
<div class="card">
<div class="title">Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators</div>
<div class="meta-line">Authors: Rebecca Pelke, Joel Klein, Jose Cubero-Cascante, Nils Bosbach, Jan Moritz Joseph, Rainer Leupers</div>
<div class="meta-line">First: 2026-01-29T13:54:55+00:00 · Latest: 2026-01-29T13:54:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21737v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21737v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向RRAM存内计算加速器的混合精度训练与编译方法</div>
<div class="mono" style="margin-top:8px">存内计算（CIM）加速器通过在存储器的交叉阵列中直接执行矩阵向量乘法（MVM），为机器学习（ML）工作负载提供了高效的加速方案。尽管交叉阵列的输入和单元位宽非常有限，但大多数CIM编译器不支持8位以下的量化，导致单个MVM需要大量计算周期，且权重无法高效存储在单个交叉单元中。为解决这一问题，我们提出了一种面向CIM架构的混合精度训练与编译框架。主要挑战在于巨大的搜索空间使得量化参数优化困难，因此我们引入基于强化学习的策略来寻找平衡延迟与精度的量化配置。在最佳情况下，该方法相比现有先进方案实现了最高2.48倍的加速，精度损失仅为0.086%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of existing Computing-in-Memory (CIM) accelerators, which are limited by their inability to effectively support quantization below 8 bits, leading to high latency and inefficient weight storage. The authors propose a mixed-precision training and compilation framework that employs a reinforcement learning strategy to navigate the vast search space and automatically determine optimal quantization configurations, thereby balancing computational speed and model accuracy. Experimental results demonstrate that their method can achieve up to a 2.48x speedup compared to state-of-the-art approaches while maintaining high accuracy, with only a minimal 0.086% loss.</div>
<div class="mono" style="margin-top:8px">本文针对现有存内计算加速器因无法有效支持8位以下量化而导致高延迟和权重存储效率低的问题，提出了一种混合精度训练与编译框架。该方法采用基于强化学习的策略，在庞大的搜索空间中自动寻找最优量化配置，以平衡计算延迟和模型精度。实验结果表明，该方法相比现有先进方案最高可实现2.48倍的加速，同时仅带来0.086%的精度损失。</div>
</details>
</div>
<div class="card">
<div class="title">Memento 2: Learning by Stateful Reflective Memory</div>
<div class="meta-line">Authors: Jun Wang</div>
<div class="meta-line">First: 2025-12-27T22:15:03+00:00 · Latest: 2026-01-29T13:49:34+00:00</div>
<div class="meta-line">Comments: 35 pages, four figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22716v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.22716v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical study of continual and experiential learning in large language model agents that combine episodic memory with reinforcement learning. We argue that the key mechanism for continual adaptation, without updating model parameters, is reflection: the agent&#x27;s ability to use past experience to guide future actions. Empirical findings suggest that episodic, experience-driven reflection enables generalised adaptation across a wide range of open-ended, long-horizon tasks. This indicates that efficient learning can occur during deployment and weakens the traditional separation between training and testing. Motivated by this, we introduce the Stateful Reflective Decision Process, a formal model of reflective memory dynamics. In this abstraction, an agent maintains an episodic memory and performs two core operations. Writing stores interaction outcomes and plays the role of policy evaluation. Reading retrieves relevant past cases to inform decisions and plays the role of policy improvement. This perspective treats reflective memory as a control object that can be analysed using classical reinforcement learning tools. We then develop a read-write reflective learning framework by integrating retrieval into soft policy iteration and establish convergence guarantees. We show that as memory grows and provides denser coverage of the state space, the resulting composite policy converges to the optimal solution. Overall, this framework connects practical memory-based methods with principled reinforcement learning, providing a rigorous mathematical basis for building reflective, memory-embedded agents capable of continual general-purpose learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Memento 2：基于状态化反思记忆的学习机制</div>
<div class="mono" style="margin-top:8px">本文对结合情景记忆与强化学习的大语言模型智能体进行了持续性与经验性学习的理论研究。我们认为，无需更新模型参数即可实现持续适应的关键机制在于反思：即智能体利用过往经验指导未来行动的能力。实证研究表明，基于情景经验的反思机制能够在开放域、长周期任务中实现泛化适应。这表明高效学习可在部署阶段发生，从而弱化了传统训练与测试阶段的界限。基于此，我们提出了状态化反思决策过程——一种形式化的反思记忆动态模型。在该抽象框架中，智能体维护情景记忆并执行两项核心操作：写入操作存储交互结果，承担策略评估功能；读取操作检索相关历史案例以辅助决策，承担策略改进功能。该视角将反思记忆视为可通过经典强化学习工具分析的控制对象。随后，我们通过将检索机制整合至软策略迭代中，构建了读写式反思学习框架，并建立了收敛性保证。研究表明，随着记忆增长及对状态空间覆盖密度的提升，复合策略将收敛至最优解。总体而言，该框架将实用化记忆方法与原则性强化学习相连接，为构建具备持续通用学习能力的反思型记忆嵌入智能体提供了严谨的数学基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates continual learning in large language model agents, motivated by the need for adaptation without parameter updates through reflective use of past experience. The method introduces the Stateful Reflective Decision Process, a formal model where agents maintain episodic memory with write operations for policy evaluation and read operations for policy improvement, integrating retrieval into soft policy iteration with convergence guarantees. Experimental results show that this episodic, experience-driven reflection enables generalized adaptation across open-ended, long-horizon tasks, suggesting efficient learning during deployment and blurring the traditional training-testing divide, with convergence to optimal solutions as memory coverage increases.</div>
<div class="mono" style="margin-top:8px">本文研究大型语言模型智能体的持续学习，其动机在于无需参数更新即可通过反思性利用过往经验实现适应。方法上提出了状态化反思决策过程这一形式化模型，其中智能体维护情景记忆，通过写入操作进行策略评估和读取操作进行策略改进，并将检索集成到软策略迭代中，确保了收敛性。实验结果表明，这种基于情景的经验驱动反思能在开放域、长周期任务中实现泛化适应，表明在部署期间可进行高效学习，模糊了传统训练与测试的界限，且随着记忆覆盖增加，复合策略会收敛至最优解。</div>
</details>
</div>
<div class="card">
<div class="title">Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations</div>
<div class="meta-line">Authors: Donatien Delehelle, Fei Chen, Darwin Caldwell</div>
<div class="meta-line">First: 2026-01-29T13:41:35+00:00 · Latest: 2026-01-29T13:41:35+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦感知与推理以提升无演示学习的布料操作数据效率</div>
<div class="mono" style="margin-top:8px">布料操作是日常生活中的常见任务，但对机器人而言仍是一个开放挑战。开发布料操作策略的困难源于布料的高维状态空间、复杂动力学特性及易自遮挡的特点。由于分析方法未能提供鲁棒且通用的操作策略，强化学习被视为解决这些问题的有前景途径。然而，为应对庞大的状态空间和复杂动力学，基于数据的方法通常依赖大型模型和长训练时间，其计算成本严重阻碍了这些方法的发展与应用。此外，由于鲁棒状态估计的挑战，布料操作策略常采用以工作空间图像为输入的端到端学习方法。虽然该方法通过现实世界微调实现了概念上直观的仿真到现实迁移，但训练智能体使用高度有损的环境状态表示也带来了显著计算成本。本文通过探索一种高效模块化的布料操作强化学习方法，对这一常见设计选择提出质疑。我们证明，通过精心设计，在仿真中学习时可显著减小模型规模与训练时间。进一步展示了如何将仿真训练的模型迁移至现实世界。我们在SoftGym基准上评估本方法，在任务中相比现有基线取得显著性能提升，同时使用的模型规模大幅减小。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the data inefficiency and high computational cost of reinforcement learning for cloth manipulation by proposing a modular approach that disentangles perception from reasoning. The method reduces reliance on large end-to-end models by using more efficient state representations, significantly decreasing model size and training time in simulation. Experimental results on the SoftGym benchmark show substantial performance improvements over baselines while using a smaller model, and the approach successfully transfers to real-world applications.</div>
<div class="mono" style="margin-top:8px">本文针对布料操作中强化学习的数据效率低和计算成本高的问题，提出了一种将感知与推理解耦的模块化方法。该方法通过使用更高效的状态表示，减少了对大型端到端模型的依赖，从而在仿真中显著降低了模型规模和训练时间。在SoftGym基准测试上的实验结果表明，该方法在使用较小模型的同时，性能相比现有基线有显著提升，并能成功迁移到实际应用中。</div>
</details>
</div>
<div class="card">
<div class="title">TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning</div>
<div class="meta-line">Authors: Huiyuan Lai, Malvina Nissim</div>
<div class="meta-line">First: 2026-01-29T13:40:35+00:00 · Latest: 2026-01-29T13:40:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model&#x27;s proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TACLer：面向高效推理的定制化课程强化学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在复杂推理任务上展现出卓越性能，尤其在配备长链思维推理时。然而，激发长链思维通常需要大规模强化学习训练，且常因冗余中间步骤导致过度思考。为在保持甚至提升性能的同时提高学习与推理效率，我们提出TACLer——一种基于模型能力定制课程、通过多阶段强化学习逐步提升数据复杂度的框架。TACLer包含两大核心组件：（1）定制化课程学习：动态识别模型知识缺口并规划渐进学习阶段；（2）混合型“思考/非思考”推理范式：通过启用或禁用思考模式平衡准确性与效率。实验表明TACLer具备双重优势：（1）降低计算成本：相比长链思维模型减少超50%训练算力，较基础模型节省超42%推理令牌；（2）提升准确性：在基础模型上实现超9%精度提升，于四个复杂数学数据集中持续优于当前最优的非思考与思考基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind TACLer is to enhance the efficiency of large language models in complex reasoning tasks, which often rely on long chain-of-thought processes that can lead to computational waste and overthinking. The method introduces a tailored curriculum reinforcement learning framework that progressively increases data complexity based on model proficiency, coupled with a hybrid Thinking/NoThinking paradigm to balance reasoning depth and speed. Experimental results demonstrate that TACLer reduces training compute by over 50% and inference token usage by over 42% compared to baselines, while improving accuracy by over 9% across four math datasets, outperforming state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">TACLer的提出动机是为了提升大语言模型在复杂推理任务中的效率，这些任务通常依赖长链思维过程，可能导致计算资源浪费和过度思考。该方法采用了一种定制化的课程强化学习框架，根据模型熟练度逐步增加数据复杂性，并结合混合的思考/非思考推理范式以平衡准确性与效率。实验结果表明，与基线相比，TACLer将训练计算成本降低了50%以上，推理令牌使用量减少了42%以上，同时在四个数学数据集上的准确率提高了9%以上，性能优于当前最先进的非思考和思考基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</div>
<div class="meta-line">Authors: Mathieu Blondel, Michael E. Sander, Germain Vivier-Ardisson, Tianlin Liu, Vincent Roulet</div>
<div class="meta-line">First: 2025-12-17T17:14:26+00:00 · Latest: 2026-01-29T13:36:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15605v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15605v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自回归语言模型实为基于能量的模型：对下一词预测前瞻能力的洞察</div>
<div class="mono" style="margin-top:8px">自回归模型（ARMs）是目前大语言模型（LLMs）的主导范式。基于能量的模型（EBMs）是另一类模型，在LLM发展中历来较少见，但能自然表征训练后对齐中的最优策略。本文为这两类模型提供了统一视角。以概率链式法则为起点，我们在函数空间建立了ARMs与EBMs之间的显式双射，并证明其对应最大熵强化学习中软贝尔曼方程的特例。基于此双射，我们推导出ARMs与EBMs监督学习的等价性。此外，通过给出理论误差界，我们分析了EBMs向ARMs的蒸馏过程。研究结果揭示了ARMs在基于下一词预测范式下仍具备前瞻规划能力的机理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to unify autoregressive models (ARMs), the dominant paradigm for large language models, with energy-based models (EBMs), which naturally characterize optimal policies in alignment, to better understand ARMs&#x27; capabilities. The method establishes an explicit bijection between ARMs and EBMs in function space, linking it to the soft Bellman equation in maximum entropy reinforcement learning, and derives the equivalence between their supervised learning while providing theoretical bounds for distilling EBMs into ARMs. The main experimental results, implied through theoretical analysis, offer insights into how ARMs can perform lookahead or planning despite being trained on next-token prediction, bridging these model classes conceptually.</div>
<div class="mono" style="margin-top:8px">本文的动机在于统一自回归模型（当前大语言模型的主导范式）与能量基模型（能自然表征对齐中的最优策略），以深入理解自回归模型的能力。方法上，该研究在函数空间中建立了自回归模型与能量基模型之间的显式双射，将其关联到最大熵强化学习中的软贝尔曼方程，并推导了二者监督学习的等价性，同时为能量基模型蒸馏到自回归模型提供了理论误差界。主要实验结果通过理论分析表明，尽管自回归模型基于下一词预测训练，但仍能具备前瞻或规划能力，从而在概念上连接了这两类模型。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-01-29T13:30:35+00:00</div>
<div class="meta-line">Comments: 87 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive policy gradient to discover such systematic reasoning remains poorly understood. We address this by analyzing the policy gradient dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, policy gradient drives the Transformer to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler examples, the Transformer learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, policy gradient learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动策略梯度发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的策略梯度动态来研究此问题——该任务必须通过思维链才能解决，但存在简单的迭代解法。我们证明：尽管仅通过最终答案正确性进行训练，策略梯度仍能驱动Transformer收敛至一种结构化、可解释的逐顶点迭代遍历算法。我们刻画了这种能力涌现所需的分布特性，指出“简单示例”（需要较少推理步骤的实例）的关键作用。当训练分布中简单示例占比足够时，Transformer能学习可泛化至更长链的遍历策略；当简单示例消失时，策略梯度学习将无法进行。我们通过合成数据实验及真实数学推理任务上的语言模型实验验证了理论结论在实际场景中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how Transformers trained with outcome-based Reinforcement Learning (RL) can develop systematic reasoning chains, a process not well understood. The authors analyze policy gradient dynamics on a synthetic graph traversal task that necessitates intermediate steps, proving that training solely on final-answer correctness leads the model to converge to an interpretable, iterative algorithm for vertex-by-vertex traversal. They identify that the emergence of this generalizable reasoning critically depends on the training distribution containing enough &quot;simple examples&quot; (instances with fewer steps); without them, learning fails. Experimental validation on both synthetic data and real-world language models for mathematical reasoning supports these theoretical findings.</div>
<div class="mono" style="margin-top:8px">本文研究了基于结果的强化学习训练的Transformer如何发展出系统性推理链，这一机制尚不明确。作者在一个需要中间步骤的合成图遍历任务上分析了策略梯度动态，证明仅基于最终答案正确性的训练会使模型收敛到一个可解释的、逐顶点迭代的遍历算法。他们发现，这种可泛化推理能力的出现关键依赖于训练分布包含足够的“简单示例”（即所需步骤较少的实例）；若缺乏此类示例，学习将无法进行。在合成数据以及用于数学推理的真实世界语言模型上的实验验证了这些理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning</div>
<div class="meta-line">Authors: Olivier Goudet, Quentin Suire, Adrien Goëffon, Frédéric Saubion, Sylvain Lamprier</div>
<div class="meta-line">First: 2025-10-02T09:12:17+00:00 · Latest: 2026-01-29T13:30:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01824v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01824v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce an order-invariant reinforcement learning framework for black-box combinatorial optimization. Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently. In contrast, we parameterize a multivariate autoregressive generative model trained without a fixed variable ordering. By sampling random generation orders during training, a form of information-preserving dropout, the model is encouraged to be invariant to variable order, promoting search-space diversity, and shaping the model to focus on the most relevant variable dependencies, improving sample efficiency. We adapt Group Relative Policy Optimization (GRPO) to this setting, providing stable policy-gradient updates from scale-invariant advantages. Across a wide range of benchmark algorithms and problem instances of varying sizes, our method frequently achieves the best performance and consistently avoids catastrophic failures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于顺序不变强化学习的黑盒组合优化方法</div>
<div class="mono" style="margin-top:8px">本文提出一种用于黑盒组合优化的顺序不变强化学习框架。传统的分布估计算法通常依赖学习显式的变量依赖图，这种方法计算成本高且难以有效捕捉复杂交互关系。相比之下，我们采用无固定变量顺序的多变量自回归生成模型，通过在训练过程中随机采样生成顺序（一种信息保留的随机丢弃技术），使模型具备变量顺序不变性，从而增强搜索空间多样性，并引导模型聚焦于最关键的变量依赖关系以提升采样效率。我们将组相对策略优化算法适配至该框架，通过尺度不变优势函数实现稳定的策略梯度更新。在涵盖多种基准算法和不同规模问题实例的广泛测试中，本方法频繁取得最优性能，并始终保持鲁棒性避免灾难性失效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of classical estimation-of-distribution algorithms in black-box combinatorial optimization, which often rely on costly explicit dependency graphs that may not capture complex interactions efficiently. To overcome this, the authors propose an order-invariant reinforcement learning framework that parameterizes a multivariate autoregressive generative model trained without a fixed variable ordering, using random generation orders during training as a form of information-preserving dropout to encourage invariance, promote search-space diversity, and improve sample efficiency by focusing on relevant dependencies. Experimental results across various benchmark algorithms and problem sizes show that this method frequently achieves top performance and consistently avoids catastrophic failures, demonstrating its robustness and effectiveness.</div>
<div class="mono" style="margin-top:8px">本文针对黑盒组合优化中经典分布估计算法依赖显式变量依赖图所带来的高成本和低效问题，提出了一种顺序不变的强化学习框架。该方法通过参数化一个无需固定变量顺序的多变量自回归生成模型，在训练中采用随机生成顺序作为信息保留的丢弃策略，以鼓励模型对变量顺序保持不变性，促进搜索空间多样性，并提高样本效率以聚焦关键依赖关系。在多种基准算法和不同规模问题实例上的实验结果表明，该方法经常取得最佳性能，并始终避免灾难性失败，验证了其鲁棒性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining</div>
<div class="meta-line">Authors: Jie Cheng, Ruixi Qiao, Yingwei Ma, Binhua Li, Gang Xiong, Qinghai Miao, Yongbin Li, Yisheng Lv</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2024-10-01T10:25:03+00:00 · Latest: 2026-01-29T13:25:42+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.00564v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.00564v4">PDF</a> · <a href="https://github.com/CJReinforce/JOWA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A significant aspiration of offline reinforcement learning (RL) is to develop a generalist agent with high capabilities from large and heterogeneous datasets. However, prior approaches that scale offline RL either rely heavily on expert trajectories or struggle to generalize to diverse unseen tasks. Inspired by the excellent generalization of world model in conditional video generation, we explore the potential of image observation-based world model for scaling offline RL and enhancing generalization on novel tasks. In this paper, we introduce JOWA: Jointly-Optimized World-Action model, an offline model-based RL agent pretrained on multiple Atari games with 6 billion tokens data to learn general-purpose representation and decision-making ability. Our method jointly optimizes a world-action model through a shared transformer backbone, which stabilize temporal difference learning with large models during pretraining. Moreover, we propose a provably efficient and parallelizable planning algorithm to compensate for the Q-value estimation error and thus search out better policies. Experimental results indicate that our largest agent, with 150 million parameters, achieves 78.9% human-level performance on pretrained games using only 10% subsampled offline data, outperforming existing state-of-the-art large-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales favorably with model capacity and can sample-efficiently transfer to novel games using only 5k offline fine-tuning data (approximately 4 trajectories) per game, demonstrating superior generalization. We will release codes and model weights at https://github.com/CJReinforce/JOWA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于联合优化的世界-行动模型预训练实现离线模型强化学习的规模化扩展</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）的一个重要目标是利用大规模异构数据集训练出高性能的通用智能体。然而，现有规模化离线RL方法要么严重依赖专家轨迹，要么难以泛化到多样未见任务。受条件视频生成中世界模型优异泛化能力的启发，我们探索了基于图像观测的世界模型在规模化离线RL及增强新任务泛化能力方面的潜力。本文提出JOWA：联合优化的世界-行动模型，这是一个基于模型的离线RL智能体，在包含60亿标记数据的多款Atari游戏上进行预训练，以学习通用表征和决策能力。该方法通过共享Transformer主干网络联合优化世界-行动模型，在预训练阶段稳定了大模型的时序差分学习。此外，我们提出了一种可证明高效且可并行的规划算法，以补偿Q值估计误差从而搜索更优策略。实验表明，我们最大的1.5亿参数智能体在预训练游戏中仅使用10%子采样离线数据即可达到78.9%的人类水平性能，平均超越现有大规模离线RL基线31.6%。JOWA的扩展性随模型容量提升而改善，并能以每款游戏仅5k离线微调数据（约4条轨迹）实现样本高效迁移至新游戏，展现出卓越的泛化能力。代码与模型权重发布于https://github.com/CJReinforce/JOWA</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of scaling offline reinforcement learning to create generalist agents from diverse datasets without heavy reliance on expert trajectories, this paper introduces JOWA, a jointly-optimized world-action model. The method employs a shared transformer backbone to jointly train a world and action model, stabilizing temporal difference learning during pretraining on large datasets, and incorporates a provably efficient planning algorithm to improve policy search. Experimental results show that the 150M-parameter agent achieves 78.9% human-level performance on pretrained Atari games using only 10% of the data, outperforming prior baselines by 31.6% on average, and demonstrates strong generalization by transferring to novel games with minimal fine-tuning data.</div>
<div class="mono" style="margin-top:8px">本文旨在解决离线强化学习在无需大量专家轨迹的情况下，从多样数据集中扩展为通用智能体的挑战，为此提出了联合优化的世界-行动模型JOWA。该方法采用共享的Transformer主干网络联合训练世界模型和行动模型，在大型数据集预训练中稳定时序差分学习，并引入一种可证明高效的规划算法以提升策略搜索。实验结果表明，该1.5亿参数智能体在预训练的Atari游戏上仅使用10%数据即达到78.9%的人类水平性能，平均优于现有基线31.6%，并能以极少量微调数据高效迁移至新游戏，展现出卓越的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture</div>
<div class="meta-line">Authors: Tianhao Fu, Xinxin Xu, Weichen Xu, Jue Chen, Ruilong Ren, Bowen Deng, Xinyu Zhao, Jian Cao, Xixin Cao</div>
<div class="meta-line">First: 2025-11-10T13:57:05+00:00 · Latest: 2026-01-29T13:22:04+00:00</div>
<div class="meta-line">Comments: accepted by AAAI2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07110v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.07110v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Market making (MM) through Reinforcement Learning (RL) has attracted significant attention in financial trading. With the development of Large Language Models (LLMs), more and more attempts are being made to apply LLMs to financial areas. A simple, direct application of LLM as an agent shows significant performance. Such methods are hindered by their slow inference speed, while most of the current research has not studied LLM distillation for this specific task. To address this, we first propose the normalized fluorescent probe to study the mechanism of the LLM&#x27;s feature. Based on the observation found by our investigation, we propose Cooperative Market Making (CMM), a novel framework that decouples LLM features across three orthogonal dimensions: layer, task, and data. Various student models collaboratively learn simple LLM features along with different dimensions, with each model responsible for a distinct feature to achieve knowledge distillation. Furthermore, CMM introduces an Hájek-MoE to integrate the output of the student models by investigating the contribution of different models in a kernel function-generated common feature space. Extensive experimental results on four real-world market datasets demonstrate the superiority of CMM over the current distillation method and RL-based market-making strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双剑合璧：通过特征分解与混合将大语言模型特征蒸馏至小模型</div>
<div class="mono" style="margin-top:8px">基于强化学习的做市策略在金融交易领域备受关注。随着大语言模型的发展，越来越多研究尝试将其应用于金融领域。直接将大语言模型作为智能体的简单方法已展现出显著性能，但其推理速度缓慢成为瓶颈，且当前研究鲜少针对该特定任务进行大语言模型蒸馏。为此，我们首先提出标准化荧光探针以研究大语言模型特征机制。基于研究发现，我们提出协同做市框架——一种通过层维度、任务维度和数据维度三个正交方向解耦大语言模型特征的新型架构。多个学生模型沿不同维度协作学习简化的大语言模型特征，各模型专攻特定特征以实现知识蒸馏。此外，该框架引入哈耶克混合专家模块，通过在核函数生成的公共特征空间中评估不同模型的贡献度来整合学生模型输出。在四个真实市场数据集上的大量实验结果表明，该框架在蒸馏效果和基于强化学习的做市策略方面均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the slow inference speed of directly using large language models (LLMs) as agents in reinforcement learning-based market making, while current research lacks focused study on LLM distillation for this financial task. The method introduces a Cooperative Market Making (CMM) framework, which decouples LLM features across layer, task, and data dimensions for distillation into various specialized student models, and integrates their outputs using a Hájek-MoE mechanism in a kernel-based common feature space. The main experimental results, validated on four real-world market datasets, show that CMM outperforms existing distillation methods and RL-based market-making strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在基于强化学习的做市任务中，直接使用大语言模型作为代理导致的推理速度慢的问题，而当前研究缺乏针对此金融任务的大模型蒸馏探索。方法上提出了协同做市框架，该框架将大语言模型特征在层、任务和数据三个正交维度上进行解耦，蒸馏到多个专门的学生模型中，并利用Hájek-MoE机制在核函数生成的公共特征空间中整合这些模型的输出。主要实验结果表明，在四个真实市场数据集上的测试验证了该框架优于现有的蒸馏方法和基于强化学习的做市策略。</div>
</details>
</div>
<div class="card">
<div class="title">Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling</div>
<div class="meta-line">Authors: Abhijeet Sinha, Sundari Elango, Dianbo Liu</div>
<div class="meta-line">First: 2026-01-29T13:03:33+00:00 · Latest: 2026-01-29T13:03:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21669v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21669v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>期望回报导致强化学习中的结果层面模式崩溃及其通过逆概率缩放的修正方法</div>
<div class="mono" style="margin-top:8px">许多强化学习问题存在多个质量相当的终端解决方案，其目标并非寻找单一最优解，而是表征一组多样化的高质量结果。然而，通过标准期望回报最大化训练的策略常会坍缩到少数结果子集，这一现象通常被归因于探索不足或正则化较弱。本文指出这种解释并不完整：结果层面的模式崩溃是期望回报目标本身的结构性后果。在理想化学习动态下，任意两个结果的概率对数比会按其奖励差值线性演化，这意味着指数级的比率发散和不可避免的坍缩，且与探索策略、熵正则化或优化算法无关。我们将此病理根源归因于期望内部的概率乘子，并提出一种最小化修正方案：逆概率缩放。该方法从学习信号中移除结果频率的放大效应，从根本上改变学习动态，可证明地产生与奖励成正比的终端分布，从而防止多模态场景中的坍缩。我们将此原理实例化为组相对策略优化的即插即用改进版本IPS-GRPO，无需辅助模型或架构调整。在多种推理和分子生成任务中，IPS-GRPO持续减少结果层面的模式崩溃，同时达到或超越基线性能，表明修正目标函数而非添加探索启发式方法，才是实现可靠多模态策略优化的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the phenomenon of outcome-level mode collapse in reinforcement learning, where policies trained to maximize expected return converge to only a subset of high-quality solutions despite the existence of multiple equally good terminal outcomes. The authors argue that this collapse is not merely due to insufficient exploration but is a structural flaw inherent in the expected-return objective itself, as its learning dynamics cause exponential divergence in the probability ratios between outcomes. To address this, they propose inverse probability scaling, a minimal correction that removes outcome-frequency amplification from the learning signal, leading to reward-proportional terminal distributions and preventing collapse. The method is instantiated as IPS-GRPO, a drop-in modification of Group Relative Policy Optimization, and experimental results on reasoning and molecular generation tasks demonstrate that it consistently reduces mode collapse while matching or exceeding baseline performance.</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习中的结果层面模式崩溃现象，即尽管存在多个同等优质的结果，但为最大化期望回报而训练的策略仍会收敛到少数解决方案上。作者指出，这种崩溃并非仅仅源于探索不足，而是期望回报目标本身的结构性缺陷，其学习动态会导致不同结果间的概率比呈指数级发散。为解决此问题，他们提出了逆概率缩放方法，这是一种最小修正，可从学习信号中移除结果频率的放大效应，从而产生与奖励成比例的终端分布并防止崩溃。该方法被实例化为IPS-GRPO，作为组相对策略优化的即插即用修改，在推理和分子生成任务上的实验结果表明，它能持续减少模式崩溃，同时达到或超过基线性能。</div>
</details>
</div>
<div class="card">
<div class="title">CleanSurvival: Automated data preprocessing for time-to-event models using reinforcement learning</div>
<div class="meta-line">Authors: Yousef Koka, David Selby, Gerrit Großmann, Sebastian Vollmer, Kathan Pandya</div>
<div class="meta-line">First: 2025-02-06T10:33:37+00:00 · Latest: 2026-01-29T12:51:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03946v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.03946v3">PDF</a> · <a href="https://github.com/datasciapps/CleanSurvival">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data preprocessing is a critical yet frequently neglected aspect of machine learning, often paid little attention despite its potentially significant impact on model performance. While automated machine learning pipelines are starting to recognize and integrate data preprocessing into their solutions for classification and regression tasks, this integration is lacking for more specialized tasks like survival or time-to-event models. As a result, survival analysis not only faces the general challenges of data preprocessing but also suffers from the lack of tailored, automated solutions in this area. To address this gap, this paper presents &#x27;CleanSurvival&#x27;, a reinforcement-learning-based solution for optimizing preprocessing pipelines, extended specifically for survival analysis. The framework can handle continuous and categorical variables, using Q-learning to select which combination of data imputation, outlier detection and feature extraction techniques achieves optimal performance for a Cox, random forest, neural network or user-supplied time-to-event model. The package is available on GitHub: https://github.com/datasciapps/CleanSurvival Experimental benchmarks on real-world datasets show that the Q-learning-based data preprocessing results in superior predictive performance to standard approaches, finding such a model up to 10 times faster than undirected random grid search. Furthermore, a simulation study demonstrates the effectiveness in different types and levels of missingness and noise in the data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CleanSurvival：基于强化学习的生存模型自动化数据预处理框架</div>
<div class="mono" style="margin-top:8px">数据预处理是机器学习中至关重要却常被忽视的环节，尽管其对模型性能可能产生显著影响，却往往未获足够重视。虽然自动化机器学习流程已开始将数据预处理纳入分类与回归任务的解决方案，但在生存分析等专业领域仍缺乏针对性整合。因此，生存分析不仅面临数据预处理的普遍挑战，更因缺乏定制化自动解决方案而受限。为填补这一空白，本文提出&#x27;CleanSurvival&#x27;——一种基于强化学习的预处理流程优化方案，专为生存分析扩展设计。该框架可处理连续与分类变量，通过Q学习算法选择数据填补、异常值检测和特征提取技术的组合，从而为Cox模型、随机森林、神经网络或用户自定义的生存模型实现最优性能。该工具包已在GitHub发布：https://github.com/datasciapps/CleanSurvival 基于真实数据集的实验表明，采用Q学习的数据预处理相比标准方法具有更优的预测性能，其模型搜索速度比无导向随机网格搜索提升最高达10倍。此外，模拟研究验证了该方法对不同类型与程度的数据缺失及噪声均具有良好适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces CleanSurvival, a reinforcement learning framework designed to automate data preprocessing for survival analysis, motivated by the lack of specialized automated solutions for time-to-event models compared to classification and regression tasks. The method employs Q-learning to optimize preprocessing pipelines by selecting combinations of imputation, outlier detection, and feature extraction techniques tailored for models like Cox regression, random forests, or neural networks. Experimental results on real-world datasets demonstrate that this approach yields superior predictive performance over standard methods and identifies optimal models up to 10 times faster than random grid search, with simulations confirming its effectiveness across various data missingness and noise levels.</div>
<div class="mono" style="margin-top:8px">本文提出了CleanSurvival，一个基于强化学习的框架，旨在自动化生存分析的数据预处理，其动机在于与分类和回归任务相比，时间-事件模型缺乏专门的自动化解决方案。该方法利用Q学习来优化预处理流程，通过选择针对Cox回归、随机森林或神经网络等模型的插补、异常值检测和特征提取技术组合。在真实数据集上的实验结果表明，该方法比标准方法具有更优的预测性能，并能比随机网格搜索快10倍找到最优模型，模拟研究还验证了其在处理不同类型和程度的数据缺失与噪声时的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning</div>
<div class="meta-line">Authors: Tianrun Xu, Haoda Jing, Ye Li, Yuquan Wei, Jun Feng, Guanyu Chen, Haichuan Gao, Tianren Zhang, Feng Chen</div>
<div class="meta-line">First: 2025-09-25T08:58:10+00:00 · Latest: 2026-01-29T12:47:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20912v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.20912v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal language models (MLLMs) have achieved remarkable progress in vision-language reasoning, especially with the emergence of &quot;thinking with images,&quot; which integrates explicit visual steps into the reasoning process. While this paradigm strengthens image-based reasoning, a significant challenge remains: models may arrive at correct answers by relying on irrelevant or spurious regions, driven by prior knowledge or dataset biases. Even when the answer is correct, flawed reasoning indicates that the model has not truly understood the image, highlighting the critical importance of reasoning fidelity in multimodal tasks. To address this issue, we propose DeFacto, a counterfactual reasoning framework that jointly enforces accurate answering and faithful reasoning. A key component of our approach is the design of three complementary training paradigms: (i) positive, (ii) counterfactual, and (iii) random-masking. To enable these paradigms, we develop a pipeline that automatically localizes question-relevant evidence and constructs positive, counterfactual, and random variants, resulting in a dataset of about 100k images. Building on this framework, we train multimodal language models with GRPO-based reinforcement learning, where we design three complementary rewards to guide the model toward accurate answering and evidence-grounded reasoning. Experiments on diverse benchmarks demonstrate that DeFacto substantially improves both answer accuracy and reasoning faithfulness, establishing a stronger foundation for interpretable multimodal reasoning. The code is available on GitHub and the dataset is released on HuggingFace.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeFacto：基于反事实图像推理的证据驱动与忠实推理强化框架</div>
<div class="mono" style="margin-top:8px">多模态语言模型在视觉语言推理领域取得显著进展，尤其是‘图像思维’范式将显式视觉步骤融入推理过程。然而，模型可能依赖无关或虚假图像区域得出正确答案，这源于先验知识或数据偏见。即使答案正确，有缺陷的推理表明模型未真正理解图像，凸显了多模态任务中推理忠实性的重要性。为此，我们提出DeFacto反事实推理框架，联合强化准确回答与忠实推理。核心设计包含三种互补训练范式：（i）正向训练，（ii）反事实训练，（iii）随机掩码训练。我们开发了自动定位问题相关证据的流程，构建正向、反事实与随机变体，生成约10万图像的数据集。基于此框架，我们采用GRPO强化学习训练多模态语言模型，设计三种互补奖励机制引导模型实现准确回答与证据驱动推理。多基准实验表明，DeFacto显著提升了答案准确性与推理忠实性，为可解释多模态推理奠定更坚实基础。代码已开源，数据集发布于HuggingFace平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind DeFacto is to address the issue of multimodal language models (MLLMs) reaching correct answers through flawed or unfaithful reasoning, such as relying on irrelevant image regions, which undermines true understanding. The method introduces a counterfactual reasoning framework that employs three training paradigms—positive, counterfactual, and random-masking—supported by an automated pipeline to generate a dataset of approximately 100k images with localized evidence variants, and trains models using GRPO-based reinforcement learning with complementary rewards for accuracy and faithfulness. Main experimental results show that DeFacto significantly enhances both answer accuracy and reasoning faithfulness across diverse benchmarks, establishing a more interpretable foundation for multimodal reasoning.</div>
<div class="mono" style="margin-top:8px">DeFacto的动机是解决多模态语言模型（MLLMs）通过有缺陷或不忠实的推理（例如依赖图像中无关区域）得出正确答案的问题，这阻碍了真正的理解。该方法提出了一个反事实推理框架，采用三种训练范式——正向、反事实和随机掩码，并通过自动化流程构建了约10万张图像的数据集，其中包含定位证据的变体，同时使用基于GRPO的强化学习和互补奖励来训练模型，以提升准确性和忠实性。主要实验结果表明，DeFacto在多种基准测试中显著提高了答案准确性和推理忠实性，为可解释的多模态推理奠定了更强基础。</div>
</details>
</div>
<div class="card">
<div class="title">CTRLS: Chain-of-Thought Reasoning via Latent State-Transition</div>
<div class="meta-line">Authors: Junda Wu, Yuxin Xiong, Xintong Li, Sheldon Yu, Zhengmian Hu, Tong Yu, Rui Wang, Xiang Chen, Jingbo Shang, Julian McAuley</div>
<div class="meta-line">First: 2025-07-10T21:32:18+00:00 · Latest: 2026-01-29T12:31:14+00:00</div>
<div class="meta-line">Comments: 10 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.08182v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.08182v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-thought (CoT) reasoning enables large language models (LLMs) to break down complex problems into interpretable intermediate steps, significantly enhancing model transparency and performance in reasoning tasks. However, conventional CoT methods rely on heuristic sampling without structured modeling of reasoning transitions, constraining their ability to systematically explore and discover diverse and effective reasoning trajectories. In this work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov decision process (MDP) with latent state transitions, enabling principled and state-aware exploration via distributional reinforcement learning. By modelling reasoning actions as explicit probability distributions in latent space, our approach explicitly models epistemic uncertainty, facilitating robust exploration of the reasoning space. As part of our framework, we introduce an on-policy reinforcement learning strategy incorporating epsilon-greedy exploration and entropy-based regularization to iteratively refine latent state transitions without requiring additional fine-tuning of the underlying LLM. Theoretical analyses provide evidence lower bounds (ELBO), theoretically grounding our transition-aware modeling of latent reasoning dynamics. Further experiments demonstrate improvements in reasoning accuracy, diversity, and exploration efficiency across benchmark reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CTRLS：基于潜在状态转移的思维链推理</div>
<div class="mono" style="margin-top:8px">思维链（CoT）推理使大语言模型（LLM）能够将复杂问题分解为可解释的中间步骤，显著提升了模型在推理任务中的透明度与性能。然而，传统CoT方法依赖启发式采样，缺乏对推理转移的结构化建模，限制了其系统性地探索和发现多样化、高效推理路径的能力。本研究提出CTRLS框架，将CoT推理建模为具有潜在状态转移的马尔可夫决策过程（MDP），通过分布式强化学习实现基于原则和状态感知的探索。该方法将推理动作建模为潜在空间中的显式概率分布，从而显式建模认知不确定性，促进对推理空间的稳健探索。框架中引入了结合ε-贪婪探索和基于熵的正则化的同策略强化学习策略，可在无需对底层LLM进行额外微调的情况下迭代优化潜在状态转移。理论分析提供了证据下界（ELBO），为潜在推理动态的转移感知建模奠定了理论基础。进一步实验表明，该方法在基准推理任务中提升了推理准确性、多样性和探索效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind CTRLS is to address the limitations of conventional chain-of-thought (CoT) methods, which rely on heuristic sampling and lack structured modeling of reasoning transitions, thereby hindering systematic exploration of diverse reasoning paths. The method formulates CoT reasoning as a Markov decision process with latent state transitions, employing distributional reinforcement learning to model reasoning actions as probability distributions in latent space, which captures epistemic uncertainty and enables state-aware exploration; it also uses an on-policy reinforcement learning strategy with epsilon-greedy exploration and entropy regularization to refine transitions without fine-tuning the underlying large language model. Experimental results show that CTRLS improves reasoning accuracy, diversity, and exploration efficiency across benchmark reasoning tasks, with theoretical analyses providing evidence lower bounds to ground the latent transition modeling.</div>
<div class="mono" style="margin-top:8px">CTRLS的动机在于解决传统思维链方法依赖启发式采样、缺乏对推理转换的结构化建模的局限性，从而阻碍了对多样化推理路径的系统性探索。该方法将思维链推理建模为具有潜在状态转换的马尔可夫决策过程，采用分布强化学习将推理动作表示为潜在空间中的概率分布，以捕捉认知不确定性并实现状态感知探索；同时使用包含ε-贪婪探索和基于熵的正则化的同策略强化学习策略来优化转换，而无需微调底层大语言模型。实验结果表明，CTRLS在基准推理任务中提高了推理准确性、多样性和探索效率，并通过理论分析提供了证据下界以支撑潜在转换建模。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Adaptive Composition of Quantum Circuit Optimisation Passes</div>
<div class="meta-line">Authors: Daniel Mills, Ifan Williams, Jacob Swain, Gabriel Matos, Enrico Rinaldi, Alexander Koziell-Pipe</div>
<div class="meta-line">First: 2026-01-29T12:29:10+00:00 · Latest: 2026-01-29T12:29:10+00:00</div>
<div class="meta-line">Comments: 14 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21629v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21629v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many quantum software development kits provide a suite of circuit optimisation passes. These passes have been highly optimised and tested in isolation. However, the order in which they are applied is left to the user, or else defined in general-purpose default pass sequences. While general-purpose sequences miss opportunities for optimisation which are particular to individual circuits, designing pass sequences bespoke to particular circuits requires exceptional knowledge about quantum circuit design and optimisation. Here we propose and demonstrate training a reinforcement learning agent to compose optimisation-pass sequences. In particular the agent&#x27;s action space consists of passes for two-qubit gate count reduction used in default PyTKET pass sequences. For the circuits in our diverse test set, the (mean, median) fraction of two-qubit gates removed by the agent is $(57.7\%, \ 56.7 \%)$, compared to $(41.8 \%, \ 50.0 \%)$ for the next best default pass sequence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的量子电路优化通道自适应组合方法</div>
<div class="mono" style="margin-top:8px">多数量子软件开发工具包提供成套的电路优化通道。这些通道经过独立的高度优化与测试，但其应用顺序通常由用户决定，或采用通用默认通道序列。通用序列会忽略特定电路的优化机会，而为特定电路定制通道序列需要量子电路设计与优化的专业知识。本研究提出并演示了训练强化学习智能体来组合优化通道序列的方法，其动作空间包含PyTKET默认通道序列中用于减少双量子比特门数量的通道。在多样化测试电路中，智能体移除双量子比特门的（平均，中位数）比例为$(57.7\%, \ 56.7\%)$，优于次优默认通道序列的$(41.8\%, \ 50.0\%)$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge that general-purpose default sequences for quantum circuit optimization passes often miss circuit-specific optimization opportunities, while custom sequences require deep expertise, this paper proposes using reinforcement learning to adaptively compose optimization-pass sequences. The method trains a reinforcement learning agent whose action space consists of specific two-qubit gate reduction passes from PyTKET, aiming to automatically generate effective pass sequences tailored to individual circuits. Experimental results on a diverse test set show the agent achieved mean and median two-qubit gate reductions of 57.7% and 56.7%, respectively, outperforming the next best default sequence which achieved 41.8% and 50.0%.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，量子电路优化通道的通用默认序列常错过电路特定的优化机会，而定制序列又需要深厚专业知识，因此提出使用强化学习来自适应组合优化通道序列。该方法训练一个强化学习智能体，其动作空间由PyTKET中用于减少双量子比特门的特定优化通道构成，旨在自动生成针对单个电路的有效通道序列。在多样化测试集上的实验结果表明，该智能体实现的双量子比特门减少比例均值为57.7%、中位数为56.7%，优于次佳默认序列的41.8%和50.0%。</div>
</details>
</div>
<div class="card">
<div class="title">RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems</div>
<div class="meta-line">Authors: Bingqian Li, Xiaolei Wang, Junyi Li, Weitao Li, Long Zhang, Sheng Chen, Wayne Xin Zhao, Ji-Rong Wen</div>
<div class="meta-line">First: 2026-01-29T12:14:31+00:00 · Latest: 2026-01-29T12:14:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21609v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RecNet：面向智能推荐系统的自演化偏好传播框架</div>
<div class="mono" style="margin-top:8px">智能推荐系统利用大语言模型（LLMs）建模复杂用户行为并支持个性化决策。然而，现有方法主要基于稀疏、嘈杂且无法反映用户与项目间实时相互影响的显式用户-项目交互来建模偏好变化。为克服这些局限，本文提出RecNet——一种自演化的偏好传播框架，能主动将实时偏好更新传播至相关用户与项目。RecNet包含两个互补阶段：在前向传播阶段，集中式偏好路由机制通过路由智能体整合偏好更新，并动态传播至最相关的智能体；为确保传播偏好的精准个性化整合，进一步引入个性化偏好接收机制，结合用于临时缓存的消息缓冲区与可优化的基于规则的过滤记忆模块，以指导基于历史经验与兴趣的选择性偏好融合。在反向传播阶段，反馈驱动的传播优化机制模拟多智能体强化学习框架，利用LLMs进行信用分配、梯度分析与模块级优化，实现传播策略的持续自演化。多场景实验验证了RecNet在推荐系统偏好传播建模中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for RecNet stems from the limitations of existing agentic recommender systems, which rely on sparse and noisy explicit user-item interactions and fail to capture real-time mutual influences. The method introduces a self-evolving preference propagation framework with two phases: a forward phase using centralized preference routing and personalized reception mechanisms to propagate and integrate updates, and a backward phase employing a feedback-driven optimization mechanism that simulates multi-agent reinforcement learning with LLMs for credit assignment and strategy evolution. Experimental results across various scenarios demonstrate RecNet&#x27;s effectiveness in modeling dynamic preference propagation for recommender systems.</div>
<div class="mono" style="margin-top:8px">RecNet的提出动机源于现有智能推荐系统依赖稀疏且嘈杂的显式用户-物品交互，无法捕捉实时相互影响的局限性。该方法采用一个自演化的偏好传播框架，包含两个阶段：前向阶段通过集中式偏好路由和个性化接收机制来传播和整合更新，后向阶段则利用反馈驱动的优化机制，模拟多智能体强化学习，使用大语言模型进行信用分配和策略演化。在不同场景下的广泛实验结果表明，RecNet在建模推荐系统的动态偏好传播方面具有显著有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization</div>
<div class="meta-line">Authors: Zhi Zheng, Yu Gu, Wei Liu, Yee Whye Teh, Wee Sun Lee</div>
<div class="meta-line">First: 2025-11-09T14:55:50+00:00 · Latest: 2026-01-29T12:07:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06411v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.06411v2">PDF</a> · <a href="https://github.com/zz1358m/SofT-GRPO-master">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SofT-GRPO：通过Gumbel重参数化软思维策略优化超越离散令牌大语言模型强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型的软思维推理范式在某些场景下能超越传统的离散令牌思维链推理，凸显其研究与应用价值。然而，离散令牌思维链推理可通过群体相对策略优化等算法强化，软思维范式与强化学习的结合仍具挑战，主要源于向软思维令牌注入随机性及相应策略更新的复杂性，导致先前软思维与GRPO的结合尝试通常逊于离散令牌版本。为充分释放软思维潜力，本文提出新型策略优化算法SofT-GRPO，在软思维推理模式下强化大语言模型。该算法向logits注入Gumbel噪声，采用Gumbel-Softmax技术避免软思维令牌脱离预训练嵌入空间，并利用重参数化技巧计算策略梯度。在1.5B至7B参数的基础大语言模型实验中，SofT-GRPO使软思维模型在Pass@1指标上略优于离散令牌GRPO（平均准确率+0.13%），在Pass@32指标上显著提升（平均准确率+2.19%）。代码与权重已开源：https://github.com/zz1358m/SofT-GRPO-master</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SofT-GRPO, a novel reinforcement learning algorithm designed to enhance the soft-thinking reasoning paradigm in large language models, which can outperform discrete-token reasoning like Chain-of-Thought but has been difficult to optimize with RL due to challenges in injecting stochasticity and updating policies. The method addresses this by injecting Gumbel noise into logits, using Gumbel-Softmax to keep tokens within the pre-trained embedding space, and applying the reparameterization trick for policy gradients. Experimental results on models from 1.5B to 7B parameters show that SofT-GRPO enables soft-thinking LLMs to slightly surpass discrete-token GRPO in Pass@1 accuracy by 0.13% on average and achieve a more substantial improvement of 2.19% on average in Pass@32 accuracy.</div>
<div class="mono" style="margin-top:8px">本文提出了SofT-GRPO，一种新颖的策略优化算法，旨在强化大语言模型中的软思维推理范式，该范式在某些场景下优于离散令牌的思维链推理，但由于在软思维令牌中注入随机性和更新策略的复杂性，此前难以用强化学习进行优化。该方法通过向logits注入Gumbel噪声、使用Gumbel-Softmax技术避免令牌超出预训练嵌入空间，并利用重参数化技巧进行策略梯度更新。在1.5B到7B参数的基座模型上的实验结果表明，SofT-GRPO使软思维大语言模型在Pass@1上平均准确率略微超越离散令牌GRPO 0.13%，并在Pass@32上平均准确率显著提升2.19%。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Imitation: Reinforcement Learning for Active Latent Planning</div>
<div class="meta-line">Authors: Zhi Zheng, Wee Sun Lee</div>
<div class="meta-line">First: 2026-01-29T12:07:16+00:00 · Latest: 2026-01-29T12:07:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21598v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21598v1">PDF</a> · <a href="https://github.com/zz1358m/ATP-Latent-master">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越模仿：面向主动潜在规划的强化学习</div>
<div class="mono" style="margin-top:8px">为实现高效密集的思维链推理，潜在推理方法通过微调大语言模型，将离散语言标记替换为连续潜在标记。与传统语言思维链推理相比，这些方法消耗更少的标记，并具备在密集潜在空间中进行规划的潜力。然而，现有潜在标记通常基于模仿语言标签进行监督。考虑到同一问题可能存在多个等效但不同的思维链标签，被动模仿任意标签可能导致次优的潜在标记表示与推理策略，削弱规划潜力并造成训练与测试间的明显差距。本研究强调在潜在标记表示空间中进行主动规划对实现最优潜在推理策略的重要性，由此提出\underline{A}c\underline{t}ive Latent \underline{P}lanning方法（ATP-Latent）。该方法将潜在标记的监督过程建模为条件变分自编码器，以获得更平滑的潜在空间。此外，为促进最合理的潜在推理策略，ATP-Latent通过基于潜在标记VAE解码内容一致性的辅助连贯性奖励进行强化学习，实现有指导的强化学习过程。在LLaMA-1B上的实验表明，相较于先进基线方法，ATP-Latent在四个基准测试中实现准确率提升4.1%、标记消耗降低3.3%。代码发布于https://github.com/zz1358m/ATP-Latent-master。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses limitations in latent reasoning methods for large language models, where current approaches passively imitate diverse chain-of-thought labels, leading to suboptimal latent representations and a gap between training and testing. To enable active planning in latent space, the authors propose ATP-Latent, which models latent token supervision with a conditional variational auto-encoder for a smoother latent space and employs reinforcement learning guided by a coherence reward based on decoded content consistency. Experimental results on LLaMA-1B show that ATP-Latent improves accuracy by 4.1% and reduces token usage by 3.3% across four benchmarks compared to advanced baselines.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型中潜在推理方法的局限性，现有方法被动模仿多样化的思维链标签，导致潜在表示欠佳及训练与测试间的差距。为实现潜在空间中的主动规划，作者提出ATP-Latent方法，使用条件变分自编码器建模潜在令牌监督以获得更平滑的潜在空间，并采用基于解码内容一致性的连贯性奖励来引导强化学习过程。在LLaMA-1B上的实验表明，与先进基线相比，ATP-Latent在四个基准测试中准确率提升4.1%，令牌使用量减少3.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</div>
<div class="meta-line">Authors: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar</div>
<div class="meta-line">First: 2026-01-29T12:01:53+00:00 · Latest: 2026-01-29T12:01:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21590v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21590v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model&#x27;s generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展的幂采样：通过分布锐化实现LLM高效免训练推理</div>
<div class="mono" style="margin-top:8px">强化学习（RL）后训练是提升大语言模型（LLM）推理性能的主流方法，但越来越多的证据表明其增益主要源于分布锐化而非新能力的获得。近期研究表明，使用马尔可夫链蒙特卡洛（MCMC）从LLM的幂分布中采样可获得与RL后训练相当的性能，且无需依赖外部奖励；然而MCMC的高计算成本限制了其广泛应用。本研究提出一种理论完备的替代方案，无需迭代式MCMC。我们推导出新公式，证明全局幂分布可通过标记级缩放低温分布来近似，其中缩放因子捕获未来轨迹质量。基于此，我们提出一种免训练、免验证器的自回归算法，可锐化基础模型的生成分布。实验部分在数学、问答和代码生成任务上评估了四个LLM，结果表明：本方法在无需外部奖励的情况下达到或超越单次GRPO性能，且推理延迟较基于MCMC的采样降低10倍以上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by evidence that reinforcement learning post-training improves reasoning primarily by sharpening the model&#x27;s output distribution rather than teaching new skills, and by the impractical computational cost of existing Markov chain Monte Carlo (MCMC) methods for power sampling, this paper introduces a training-free alternative. The method derives a theoretical formulation showing the global power distribution can be approximated by a token-level scaled low-temperature distribution, where scaling factors estimate future trajectory quality, enabling an autoregressive algorithm that sharpens the base model&#x27;s outputs without external rewards or verifiers. Experimental results on math, QA, and code tasks across four large language models demonstrate the method matches or exceeds the performance of one-step GRPO while reducing inference latency by over 10 times compared to MCMC-based approaches.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，有证据表明强化学习后训练提升推理能力主要源于锐化模型输出分布而非学习新能力，而现有基于马尔可夫链蒙特卡洛的幂采样方法计算成本过高，难以实用。为此，本文提出一种无需训练的理论替代方案，推导出全局幂分布可通过令牌级缩放低温分布来近似，其中缩放因子捕获未来轨迹质量，从而设计出一种自回归算法，无需外部奖励或验证器即可锐化基础模型的生成分布。在数学、问答和代码任务上对四个大语言模型的实验结果表明，该方法性能匹配或超越一步GRPO，同时相比基于MCMC的采样将推理延迟降低了10倍以上。</div>
</details>
</div>
<div class="card">
<div class="title">CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning</div>
<div class="meta-line">Authors: Cédric Colas, Pierre Fournier, Olivier Sigaud, Mohamed Chetouani, Pierre-Yves Oudeyer</div>
<div class="meta-line">Venue: ICML 2019</div>
<div class="meta-line">First: 2018-10-15T11:40:28+00:00 · Latest: 2026-01-29T11:57:35+00:00</div>
<div class="meta-line">Comments: Accepted at ICML 2019 https://github.com/flowersteam/curious</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/1810.06284v5">Abs</a> · <a href="https://arxiv.org/pdf/1810.06284v5">PDF</a> · <a href="https://github.com/flowersteam/curious">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CURIOUS：基于内在激励的模块化多目标强化学习</div>
<div class="mono" style="margin-top:8px">在开放环境中，自主学习智能体需通过内在激励的探索自主设定目标并构建学习进程。它们可能考虑多样化的目标，旨在发现环境中可控与不可控的因素。鉴于部分目标可能易于实现而部分无法达成，智能体必须动态选择当前训练目标，以最大化对可学习目标集的整体掌握能力。本文提出CURIOUS算法，其融合两大机制：1）采用模块化通用价值函数逼近器与事后学习技术，使单一策略能实现多种类型目标；2）通过自动化课程学习机制，使智能体注意力偏向于能产生最大绝对学习进展的目标。智能体依次聚焦复杂度递增的目标，并循环巩固易被遗忘的目标。在新型模块化目标机器人环境中的实验表明，该方法能形成自组织的学习进程，并展现出对干扰目标、记忆遗忘及身体属性变化的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for autonomous agents to self-direct their learning in open-ended environments, this paper introduces CURIOUS, an algorithm that combines a modular Universal Value Function Approximator with hindsight learning to handle diverse goals within a single policy, alongside an automated curriculum mechanism that prioritizes goals based on absolute learning progress. The method enables agents to sequentially tackle goals of increasing complexity and revisit forgotten ones, with experimental results in a modular-goal robotic environment demonstrating robust self-organized curriculum development, resilience to distracting goals, and adaptability to changes in body properties.</div>
<div class="mono" style="margin-top:8px">针对开放环境中自主智能体需自我引导学习的需求，本文提出了CURIOUS算法，该方法结合了模块化通用价值函数逼近器与事后学习，以单一策略处理多样目标，并采用基于绝对学习进度的自动课程机制来优先选择目标。该方法使智能体能依次学习复杂度递增的目标并复习遗忘内容，在模块化目标机器人环境中的实验结果表明，其能实现稳健的自组织课程发展，有效抵抗干扰目标的影响，并能适应身体属性的变化。</div>
</details>
</div>
<div class="card">
<div class="title">Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks</div>
<div class="meta-line">Authors: Jinhao Li, Yuhao Sun, Zhiyuan Ma, Hao He, Xinche Zhang, Xing Chen, Jin Li, Sen Song</div>
<div class="meta-line">First: 2026-01-29T11:34:49+00:00 · Latest: 2026-01-29T11:34:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21572v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21572v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向循环脉冲神经网络无梯度优化的信号自适应信赖域方法</div>
<div class="mono" style="margin-top:8px">循环脉冲神经网络（RSNN）是实现高能效控制策略的有前景的架构，但针对高维、长周期强化学习任务的训练仍具挑战性。基于种群的无梯度优化通过估计梯度，绕过了不可微脉冲动力学中的反向传播问题。然而，在有限种群规模下，梯度估计的高方差可能导致有害且过于激进的更新步长。受强化学习中在分布空间约束策略更新的信赖域方法启发，我们提出**信号自适应信赖域（SATR）**——一种通过以估计信号能量归一化的KL散度上界来约束相对变化的分布更新规则。SATR能在强信号下自动扩展信赖域，在噪声主导更新时收缩信赖域。我们将SATR实例化于伯努利连接分布（该分布在RSNN优化中已展现出卓越的实证性能）。在一系列高维连续控制基准测试中，SATR在有限种群条件下提升了稳定性，并在包括PPO-LSTM在内的强基线对比中达到具有竞争力的回报水平。此外，为实现SATR的大规模应用，我们引入了面向二元脉冲与二元权重的位集实现方案，显著缩短了实际训练时间，实现了高效的RSNN策略搜索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training recurrent spiking neural networks (RSNNs) for high-dimensional reinforcement learning, where gradient-free optimization methods often suffer from high-variance gradient estimates leading to unstable updates. To mitigate this, the authors propose Signal-Adaptive Trust Regions (SATR), a method that constrains policy updates by bounding KL divergence normalized by estimated signal energy, allowing the trust region to adaptively expand or contract based on signal strength. Experimental results on continuous-control benchmarks show that SATR improves stability with limited population sizes and achieves competitive performance compared to baselines like PPO-LSTM, while a bitset implementation for binary operations significantly reduces training time.</div>
<div class="mono" style="margin-top:8px">本文针对循环脉冲神经网络在高维强化学习中训练困难的问题，提出了一种解决方案，其中无梯度优化方法因梯度估计方差高而易导致不稳定更新。为此，作者引入了信号自适应信任区域方法，该方法通过以估计信号能量归一化的KL散度来约束策略更新，使信任区域能根据信号强度自适应调整。在连续控制基准测试中，该方法在有限种群规模下提高了稳定性，并取得了与PPO-LSTM等基线模型竞争的性能，同时通过二进制操作的位集实现大幅减少了训练时间。</div>
</details>
</div>
<div class="card">
<div class="title">Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning</div>
<div class="meta-line">Authors: Irene Ambrosini, Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Luna Gava, Giacomo Indiveri, Chiara De Luca, Chiara Bartolozzi</div>
<div class="meta-line">First: 2026-01-29T11:05:23+00:00 · Latest: 2026-01-29T11:05:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21548v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task&#x27;s temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用脉冲强化学习训练慢速硅神经元控制极速机器人</div>
<div class="mono" style="margin-top:8px">空气曲棍球需要在高速冰球运动中实现瞬间决策，我们通过运行在混合信号模拟/数字神经形态处理器上的紧凑脉冲神经元网络应对这一挑战。通过硬件与学习算法的协同设计，我们训练该系统在极少量尝试中通过强化学习实现成功的冰球交互。该网络利用固定随机连接捕捉任务的时间结构，并在读出层采用局部e-prop学习规则，以利用事件驱动活动实现快速高效的学习。最终构建了包含计算机与神经形态芯片的实时闭环学习系统，为机器人自主系统提供了实用的脉冲神经网络训练方案。这项工作将神经科学启发的硬件与现实世界的机器人控制相结合，表明类脑方法能够处理快速交互任务，并支持智能机器的持续在线学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need for brain-inspired, energy-efficient systems capable of real-time learning in fast-paced robotic tasks like air hockey, where split-second decisions are required. The method employs a co-design of hardware and algorithms, using a compact spiking neural network on a mixed-signal neuromorphic processor with fixed random connectivity to capture temporal dynamics, and applies a local e-prop reinforcement learning rule in the readout layer for event-driven training. Experimental results demonstrate that the system achieves successful puck interactions in remarkably few trials, enabling real-time learning with a neuromorphic chip in-the-loop, thus bridging neuroscience-inspired hardware with practical robotic control for autonomous systems.</div>
<div class="mono" style="margin-top:8px">这项研究的动机是，在类似空气曲棍球这样需要瞬间决策的快速机器人任务中，需要开发受大脑启发、能效高且能实时学习的系统。方法上，通过硬件与算法的协同设计，在混合信号神经形态处理器上使用具有固定随机连接的紧凑脉冲神经网络来捕捉时间动态，并在读出层应用局部e-prop强化学习规则进行事件驱动训练。实验结果表明，该系统在极少的试验次数内就能成功实现冰球交互，通过神经形态芯片的在线环路实现了实时学习，从而将神经科学启发的硬件与实用的机器人自主控制连接起来。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Bang Giang Le, Viet Cuong Ta</div>
<div class="meta-line">First: 2026-01-29T10:38:19+00:00 · Latest: 2026-01-29T10:38:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21523v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21523v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents&#x27; contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents&#x27; local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中基于局部奖励与依赖图的显式信用分配</div>
<div class="mono" style="margin-top:8px">为促进多智能体强化学习中的协作，可将所有智能体的奖励信号聚合为全局奖励（即完全协作设定）。然而全局奖励通常包含所有智能体的贡献而存在噪声，需通过信用分配过程解析。另一方面，局部奖励虽能通过分离智能体贡献加速学习，但可能导致智能体短视地优化自身奖励而忽视全局最优性。本研究提出融合两种方法优势的新方案：通过构建智能体交互图，以比全局奖励更细粒度的方式辨识个体贡献，同时缓解局部奖励的协作难题。我们还提出了该交互图的实用近似方法。实验表明该方法具有灵活性，在传统局部与全局奖励设定基础上实现了性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the credit assignment problem in cooperative multi-agent reinforcement learning, where global rewards are noisy due to aggregated contributions and local rewards can lead to suboptimal, myopic behavior. The proposed method combines the advantages of both by using a graph of agent interactions to discern individual contributions more precisely than global rewards, while mitigating cooperation issues inherent in local rewards; it also introduces a practical approximation for constructing such graphs. Experimental results show that this approach improves performance over traditional global and local reward settings, demonstrating its flexibility and effectiveness.</div>
<div class="mono" style="margin-top:8px">本文针对合作式多智能体强化学习中的信用分配问题展开研究，其中全局奖励因聚合所有智能体贡献而存在噪声，而局部奖励则可能导致次优的短视行为。所提出的方法结合了两者的优点，通过利用智能体交互图来比全局奖励更精细地辨别个体贡献，同时缓解局部奖励带来的合作问题；该方法还引入了一种实用的图近似构建方式。实验结果表明，该方法的性能优于传统的全局和局部奖励设置，体现了其灵活性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Formal Verification of Noisy Quantum Reinforcement Learning Policies</div>
<div class="meta-line">Authors: Dennis Gross</div>
<div class="meta-line">First: 2025-12-01T10:26:33+00:00 · Latest: 2026-01-29T10:31:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01502v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.01502v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum reinforcement learning (QRL) aims to use quantum effects to create sequential decision-making policies that achieve tasks more effectively than their classical counterparts. However, QRL policies face uncertainty from quantum measurements and hardware noise, such as bit-flip, phase-flip, and depolarizing errors, which can lead to unsafe behavior. Existing work offers no systematic way to verify whether trained QRL policies meet safety requirements under specific noise conditions. We introduce QVerifier, a formal verification method that applies probabilistic model checking to analyze trained QRL policies with and without modeled quantum noise. QVerifier builds a complete model of the policy-environment interaction, incorporates quantum uncertainty directly into the transition probabilities, and then checks safety properties using the Storm model checker. Experiments across multiple QRL environments show that QVerifier precisely measures how different noise models influence safety, revealing both performance degradation and cases where noise can help. By enabling rigorous safety verification before deployment, QVerifier addresses a critical need: because access to quantum hardware is expensive, pre-deployment verification is essential for any safety-critical use of QRL. QVerifier targets a potential sweet spot between classical and quantum computation, where trained QRL policies could still be modeled classically for probabilistic model checking. When the policy was trained under matching noise conditions, this formal model is exact; when trained on physical hardware, it constitutes an idealized approximation, as unknown hardware noise prevents exact policy modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>噪声量子强化学习策略的形式化验证</div>
<div class="mono" style="margin-top:8px">量子强化学习旨在利用量子效应构建比经典方法更有效的序列决策策略，但其策略面临量子测量与硬件噪声（如比特翻转、相位翻转、退极化误差）带来的不确定性，可能导致不安全行为。现有研究缺乏在特定噪声条件下系统验证训练后QRL策略是否满足安全要求的方法。我们提出QVerifier——一种应用概率模型检测分析含噪声与无噪声QRL策略的形式化验证方法。该方法构建策略-环境交互的完整模型，将量子不确定性直接嵌入转移概率，并利用Storm模型检测器验证安全属性。在多类QRL环境中的实验表明，QVerifier能精确量化不同噪声模型对安全性的影响，既揭示性能退化现象，也发现噪声可能产生积极作用的案例。通过实现部署前的严格安全验证，QVerifier解决了关键需求：鉴于量子硬件访问成本高昂，预部署验证对QRL的安全关键应用至关重要。该方法瞄准经典与量子计算间的潜在平衡点——训练后的QRL策略仍可通过经典方式建模进行概率模型检测。当策略在匹配噪声条件下训练时，该形式模型具有精确性；在物理硬件上训练时，因未知硬件噪声无法精确建模策略，则构成理想化近似。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the safety risks posed by quantum measurement uncertainty and hardware noise in quantum reinforcement learning (QRL) policies, for which systematic verification methods were lacking. The authors introduce QVerifier, a formal verification method that uses probabilistic model checking to analyze trained QRL policies by modeling policy-environment interactions and directly incorporating quantum uncertainty into transition probabilities, which are then checked with the Storm model checker. Experimental results across multiple QRL environments demonstrate that QVerifier precisely quantifies the impact of various noise models on safety, revealing both performance degradation and instances where noise can be beneficial, thereby providing essential pre-deployment verification for safety-critical QRL applications.</div>
<div class="mono" style="margin-top:8px">本文针对量子强化学习策略中由量子测量不确定性和硬件噪声引发的安全风险，而现有方法缺乏系统性验证的问题，提出了一种解决方案。作者引入了QVerifier，这是一种形式化验证方法，它通过概率模型检测来分析训练好的QRL策略，具体方法是建模策略与环境的交互，并将量子不确定性直接纳入转移概率，然后使用Storm模型检查器进行安全性验证。在多个QRL环境中的实验结果表明，QVerifier能够精确量化不同噪声模型对安全性的影响，既揭示了性能下降的情况，也发现了噪声可能有益的实例，从而为安全关键的QRL应用提供了必要的部署前验证。</div>
</details>
</div>
<div class="card">
<div class="title">ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment</div>
<div class="meta-line">Authors: Xiuyu Li, Jinkai Zhang, Mingyang Yi, Yu Li, Longqiang Wang, Yue Wang, Ju Fan</div>
<div class="meta-line">First: 2026-01-29T10:06:52+00:00 · Latest: 2026-01-29T10:06:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21484v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21484v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ETS：基于能量引导的无训练强化学习对齐测试时缩放方法</div>
<div class="mono" style="margin-top:8px">语言模型的强化学习（RL）后训练对齐方法虽有效，但因其复杂的训练过程，在实践中成本高昂且稳定性不足。为此，我们提出一种免训练的推理方法，直接从最优RL策略中采样。应用于掩码语言建模（MLM）的转移概率由参考策略模型和能量项构成。基于此，我们的算法——能量引导测试时缩放（ETS）通过在线蒙特卡洛方法估计关键能量项，并具备可证明的收敛速率。此外，为确保实际效率，ETS结合现代加速框架与定制的重要性采样估计器，在可证明保持采样质量的同时显著降低推理延迟。在涵盖推理、编程和科学领域的MLM（包括自回归模型与扩散语言模型）基准测试中，ETS持续提升生成质量，验证了其有效性与设计合理性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high cost and instability of conventional reinforcement learning (RL) alignment for language models, which involves complex training. To address this, the authors propose a training-free inference method called Energy-Guided Test-Time Scaling (ETS), which directly samples from the optimal RL policy by combining a reference policy model with an energy term in Masked Language Modeling (MLM) transitions. The method estimates the energy term via online Monte Carlo with a provable convergence rate and employs acceleration frameworks with importance sampling to reduce inference latency while maintaining sampling quality. Experimental results on reasoning, coding, and science benchmarks using autoregressive and diffusion language models demonstrate that ETS consistently improves generation quality, confirming its effectiveness.</div>
<div class="mono" style="margin-top:8px">本文的动机在于传统强化学习（RL）对齐方法用于语言模型时成本高、不稳定，且训练过程复杂。为此，作者提出了一种无需训练的推理方法——能量引导测试时缩放（ETS），该方法通过将参考策略模型与能量项结合到掩码语言建模（MLM）的转移概率中，直接从最优RL策略中采样。ETS使用在线蒙特卡洛估计关键能量项，具有可证明的收敛速度，并利用现代加速框架和定制的重要性采样估计器，在保证采样质量的同时显著降低推理延迟。在推理、编码和科学基准测试中，对自回归和扩散语言模型的实验结果表明，ETS能持续提升生成质量，验证了其有效性和设计。</div>
</details>
</div>
<div class="card">
<div class="title">Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions</div>
<div class="meta-line">Authors: Tobias Schmidt, Kai Cui</div>
<div class="meta-line">First: 2026-01-29T09:57:48+00:00 · Latest: 2026-01-29T09:57:48+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21477v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21477v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent&#x27;s optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏图上的平均场控制：从局部极限到图神经网络——基于邻域分布的方法</div>
<div class="mono" style="margin-top:8px">平均场控制为多智能体系统的维度灾难提供了可扩展解决方案，但传统方法依赖密集全连接交互的可交换性假设，具有较大局限性。本研究通过建立大型稀疏图上的严格平均场控制框架，弥合了理论与现实网络结构的鸿沟。我们将系统状态重新定义为装饰根节点邻域上的概率测度，有效捕捉局部异质性。核心贡献在于为此场景下的可扩展强化学习奠定理论基础：证明了有限时域问题具有时域依赖的局部性——智能体在时刻t的最优策略严格取决于其(T-t)跳邻域。该结论使无限维控制问题可解，并支撑了邻域分布提升空间上的新型动态规划原理。此外，我们从理论与实验两方面论证了在此场景下采用图神经网络执行行动者-评论者算法的合理性。本框架不仅将经典平均场控制作为退化情形自然包含，还能在复杂稀疏拓扑上实现高效且理论完备的控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of traditional mean-field control (MFC), which assumes dense, all-to-all interactions and exchangeability, by extending it to large sparse graphs that better reflect real-world network structures. The method redefines the system state as a probability measure over decorated rooted neighborhoods to capture local heterogeneity, and establishes a theoretical foundation for scalable reinforcement learning, proving horizon-dependent locality where an agent&#x27;s optimal policy depends only on its finite-hop neighborhood. Experimental results demonstrate that this framework enables efficient control on sparse topologies and justifies the use of Graph Neural Networks (GNNs) for actor-critic algorithms, while naturally recovering classical MFC as a special case.</div>
<div class="mono" style="margin-top:8px">本文针对传统平均场控制（MFC）依赖密集全连接交互和可交换性的局限，将其扩展到更能反映现实世界网络结构的大型稀疏图上。方法通过将系统状态重新定义为装饰根邻域上的概率测度来捕捉局部异质性，并为可扩展强化学习建立了理论基础，证明了有限时域问题中智能体的最优策略仅依赖于其有限跳邻域的视界依赖局部性。实验结果表明，该框架能在稀疏拓扑上实现高效控制，并验证了图神经网络（GNN）在行动者-评论者算法中的适用性，同时自然地将经典MFC作为特例包含在内。</div>
</details>
</div>
<div class="card">
<div class="title">Task-free Adaptive Meta Black-box Optimization</div>
<div class="meta-line">Authors: Chao Wang, Licheng Jiao, Lingling Li, Jiaxuan Zhao, Guanchun Wang, Fang Liu, Shuyuan Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T09:54:10+00:00 · Latest: 2026-01-29T09:54:10+00:00</div>
<div class="meta-line">Comments: This article was published as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21475v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21475v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Handcrafted optimizers become prohibitively inefficient for complex black-box optimization (BBO) tasks. MetaBBO addresses this challenge by meta-learning to automatically configure optimizers for low-level BBO tasks, thereby eliminating heuristic dependencies. However, existing methods typically require extensive handcrafted training tasks to learn meta-strategies that generalize to target tasks, which poses a critical limitation for realistic applications with unknown task distributions. To overcome the issue, we propose the Adaptive meta Black-box Optimization Model (ABOM), which performs online parameter adaptation using solely optimization data from the target task, obviating the need for predefined task distributions. Unlike conventional metaBBO frameworks that decouple meta-training and optimization phases, ABOM introduces a closed-loop adaptive parameter learning mechanism, where parameterized evolutionary operators continuously self-update by leveraging generated populations during optimization. This paradigm shift enables zero-shot optimization: ABOM achieves competitive performance on synthetic BBO benchmarks and realistic unmanned aerial vehicle path planning problems without any handcrafted training tasks. Visualization studies reveal that parameterized evolutionary operators exhibit statistically significant search patterns, including natural selection and genetic recombination.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无任务自适应元黑盒优化</div>
<div class="mono" style="margin-top:8px">针对复杂黑盒优化任务，手工设计的优化器效率低下。元黑盒优化通过元学习自动配置底层优化器，以消除启发式依赖。然而，现有方法通常需要大量手工训练任务来学习可泛化至目标任务的元策略，这在任务分布未知的实际应用中存在局限。为此，我们提出自适应元黑盒优化模型，仅利用目标任务的优化数据进行在线参数自适应，无需预定义任务分布。与传统解耦元训练与优化阶段的框架不同，该模型引入闭环自适应参数学习机制，参数化进化算子能利用优化过程中生成的种群持续自我更新。这一范式转变实现了零样本优化：该模型在合成黑盒优化基准测试和无人机路径规划实际问题中均取得优异性能，且无需任何手工训练任务。可视化研究表明，参数化进化算子展现出具有统计显著性的搜索模式，包括自然选择和基因重组。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of handcrafted optimizers for complex black-box optimization (BBO) tasks and the reliance of existing meta-learning methods on predefined task distributions. It proposes the Adaptive meta Black-box Optimization Model (ABOM), which introduces a closed-loop adaptive parameter learning mechanism that continuously updates evolutionary operators using only optimization data from the target task, eliminating the need for handcrafted training tasks. Experimental results show that ABOM achieves competitive performance in synthetic BBO benchmarks and unmanned aerial vehicle path planning problems through zero-shot optimization, with visualizations revealing statistically significant search patterns like natural selection and genetic recombination.</div>
<div class="mono" style="margin-top:8px">本文针对复杂黑盒优化任务中手工优化器效率低下以及现有元学习方法依赖预定义任务分布的问题，提出了自适应元黑盒优化模型（ABOM）。该方法采用闭环自适应参数学习机制，仅利用目标任务的优化数据持续更新参数化进化算子，无需手工训练任务。实验结果表明，ABOM在合成黑盒优化基准测试和无人机路径规划问题中通过零样本优化实现了有竞争力的性能，可视化研究揭示了自然选择和基因重组等统计显著的搜索模式。</div>
</details>
</div>
<div class="card">
<div class="title">MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning</div>
<div class="meta-line">Authors: Yaorui Shi, Shugui Liu, Yu Yang, Wenyu Mao, Yuxin Chen, Qi GU, Hui Su, Xunliang Cai, Xiang Wang, An Zhang</div>
<div class="meta-line">First: 2026-01-29T09:47:17+00:00 · Latest: 2026-01-29T09:47:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21468v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemOCR：面向高效长程推理的布局感知视觉记忆系统</div>
<div class="mono" style="margin-top:8px">长程智能体推理需要将不断增长的交互历史有效压缩至有限上下文窗口。现有记忆系统多将历史序列化为文本，其词元级成本均匀且随长度线性增长，常将稀缺预算消耗于低价值细节。为此，我们提出MemOCR——一种多模态记忆智能体，通过视觉布局实现自适应信息密度的内存空间分配，从而在严格上下文预算下提升长程推理能力。具体而言，MemOCR维护结构化富文本记忆（如标题、高亮内容），并将其渲染为智能体查询记忆时参考的图像，在视觉层面突出关键证据并大幅压缩辅助细节。为确保不同内存预算下的鲁棒性，我们采用预算感知目标的强化学习训练MemOCR，使智能体适应多样化压缩层级。在长上下文多跳与单跳问答基准测试中，MemOCR优于强文本基线，并在极端预算下实现更高效的上下文利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for MemOCR stems from the need to efficiently compress long interaction histories for agentic reasoning within limited context windows, as existing text-based memory systems often waste tokens on low-value details. The method introduces a multimodal memory agent that uses visual layout to allocate memory space adaptively, maintaining structured rich-text memory and rendering it into an image for consultation, which prioritizes crucial evidence while compressing auxiliary details. Experimental results show that MemOCR outperforms text-based baselines on long-context multi-hop and single-hop QA benchmarks, achieving more effective context utilization under extreme budgets through reinforcement learning with budget-aware objectives.</div>
<div class="mono" style="margin-top:8px">MemOCR的动机源于需要在有限上下文窗口中高效压缩长交互历史以支持智能体推理，因为现有基于文本的记忆系统常将稀缺资源浪费在低价值细节上。该方法引入了一种多模态记忆智能体，利用视觉布局自适应分配记忆空间，维护结构化富文本记忆并将其渲染为图像供查询，从而优先处理关键证据并压缩辅助细节。实验结果表明，通过基于预算目标的强化学习训练，MemOCR在长上下文多跳和单跳问答基准测试中优于基于文本的基线方法，在极端预算下实现了更有效的上下文利用。</div>
</details>
</div>
<div class="card">
<div class="title">HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing</div>
<div class="meta-line">Authors: Chengyu Du, Xintao Wang, Aili Chen, Weiyuan Li, Rui Xu, Junteng Liu, Zishan Huang, Rong Tian, Zijun Sun, Yuhao Li, Liheng Feng, Deming Ding, Pengyu Zhao, Yanghua Xiao</div>
<div class="meta-line">First: 2026-01-29T09:35:27+00:00 · Latest: 2026-01-29T09:35:27+00:00</div>
<div class="meta-line">Comments: 41pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21459v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21459v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters&#x27; first-person thinking from LLMs&#x27; third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HER：面向大语言模型角色扮演的人类化推理与强化学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型角色扮演，即利用大语言模型模拟特定人物角色，已成为陪伴、内容创作和数字游戏等应用中的关键能力。现有模型虽能有效捕捉角色语气与知识，但模拟其行为背后的内在思维仍具挑战。为实现大语言模型角色扮演的认知模拟，先前研究主要存在两大不足：缺乏高质量推理轨迹的数据，以及缺少符合人类偏好的可靠奖励信号。本文提出HER——一个认知层面人物模拟的统一框架。HER引入双层思维机制，区分角色的第一人称思维与大语言模型的第三人称思维。为弥合这些差距，我们通过逆向工程构建推理增强的角色扮演数据，并建立符合人类偏好的原则与奖励模型。基于这些资源，我们以Qwen3-32B为基础模型，通过监督学习与强化学习训练HER模型。大量实验验证了方法的有效性：我们的模型显著超越Qwen3-32B基线，在CoSER基准上提升30.26分，在Minimax角色扮演基准上提升14.97分。我们将公开数据集、原则与模型以促进后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of simulating the inner cognitive processes behind persona behaviors in LLM role-playing, which is crucial for applications like companionship and gaming. To overcome the lack of high-quality reasoning data and human-aligned rewards, the authors propose HER, a framework featuring dual-layer thinking that separates character-first-person from LLM-third-person reasoning, and they curate reasoning-augmented data via reverse engineering while constructing principle-based reward models. Through supervised and reinforcement learning on Qwen3-32B, the method achieves significant experimental gains, notably improving by 30.26 points on the CoSER benchmark and 14.97 points on the Minimax Role-Play Bench compared to the baseline.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型角色扮演中模拟人物行为背后内在认知过程的挑战展开研究，这对于陪伴、游戏等应用至关重要。为解决高质量推理数据和人类偏好对齐奖励的缺乏，作者提出了HER框架，采用双层思维区分角色第一人称与模型第三人称推理，并通过逆向工程构建推理增强数据及基于原则的奖励模型。基于Qwen3-32B模型进行监督学习和强化学习后，实验结果显示该方法显著优于基线，在CoSER基准上提升了30.26分，在Minimax角色扮演基准上提升了14.97分。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning</div>
<div class="meta-line">Authors: Qian Wan, Ziao Xu, Luona Wei, Xiaoxuan Shen, Jianwen Sun</div>
<div class="meta-line">First: 2026-01-29T08:56:45+00:00 · Latest: 2026-01-29T08:56:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21418v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21418v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于难度感知强化学习缓解大型推理模型的过度思考现象</div>
<div class="mono" style="margin-top:8px">大型推理模型通过模仿人类深度思考行为实现显式的思维链扩展，在复杂任务场景中展现出优异性能。然而在处理简单任务时，这种深度思考模式常导致不必要的冗长推理和资源低效。过度思考现象可能源于后训练阶段奖励函数触发的生成偏好。现有研究尝试从提示设计或模型训练角度缓解该问题，但普遍低估了任务难度感知的重要性，使得模型难以有效分配推理资源。本文提出难度感知策略优化——一种基于强化学习的训练框架，鼓励模型自主建模任务复杂度，并将其整合至强化学习框架中以调整后训练引入的生成偏好。我们提出基于模型自推理的难度建模方法，显著降低对人工标注的依赖并形式化任务复杂度。进一步开发了难度信号增强的奖励函数，在考虑推理性能与输出格式的同时，对冗长推理施加惩罚。实验表明，该框架能使模型自主调整推理开销，在保持性能不因思维压缩受损的前提下显著减少冗余标记。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the overthinking problem in Large Reasoning Models (LRMs), where deep-thinking modes lead to unnecessarily lengthy reasoning on simple tasks, causing resource inefficiency. The authors propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning framework that encourages LRMs to model task complexity autonomously and integrate this awareness to adjust generation preferences from post-training. A key innovation is a self-reasoning-based difficulty modeling method that reduces reliance on manual annotation. Experiments show that DiPO significantly reduces redundant tokens in reasoning without compromising performance, enabling models to adapt inference overhead effectively.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型中的过度思考问题展开研究，该问题导致模型在处理简单任务时产生不必要的冗长推理，造成资源低效。作者提出了难度感知策略优化方法，这是一种基于强化学习的训练框架，旨在鼓励模型自主建模任务复杂度，并将其整合到强化学习中，以调整后训练引入的生成偏好。该方法创新性地采用基于模型自推理的难度建模，显著降低了对人工标注的依赖。实验结果表明，该方法能有效减少推理中的冗余标记，且不因思维压缩而损失性能，使模型能够自适应调整推理开销。</div>
</details>
</div>
<div class="card">
<div class="title">Intrinsic Reward Policy Optimization for Sparse-Reward Environments</div>
<div class="meta-line">Authors: Minjae Cho, Huy Trong Tran</div>
<div class="meta-line">First: 2026-01-29T08:25:14+00:00 · Latest: 2026-01-29T08:25:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21391v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21391v1">PDF</a> · <a href="https://github.com/Mgineer117/IRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exploration is essential in reinforcement learning as an agent relies on trial and error to learn an optimal policy. However, when rewards are sparse, naive exploration strategies, like noise injection, are often insufficient. Intrinsic rewards can also provide principled guidance for exploration by, for example, combining them with extrinsic rewards to optimize a policy or using them to train subpolicies for hierarchical learning. However, the former approach suffers from unstable credit assignment, while the latter exhibits sample inefficiency and sub-optimality. We propose a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for an extrinsic reward without pretraining subpolicies. Our algorithm -- intrinsic reward policy optimization (IRPO) -- achieves this by using a surrogate policy gradient that provides a more informative learning signal than the true gradient in sparse-reward environments. We demonstrate that IRPO improves performance and sample efficiency relative to baselines in discrete and continuous environments, and formally analyze the optimization problem solved by IRPO. Our code is available at https://github.com/Mgineer117/IRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏奖励环境下的内在奖励策略优化</div>
<div class="mono" style="margin-top:8px">在强化学习中，探索至关重要，因为智能体需通过试错学习最优策略。然而，当奖励稀疏时，噪声注入等简单探索策略往往不足。内在奖励可为探索提供原则性指导，例如将其与外在奖励结合以优化策略，或用于训练分层学习的子策略。但前者存在信用分配不稳定的问题，后者则表现出样本效率低下和次优性。我们提出一种策略优化框架，利用多种内在奖励直接优化面向外在奖励的策略，无需预训练子策略。我们的算法——内在奖励策略优化（IRPO）——通过使用替代策略梯度实现这一目标，该梯度在稀疏奖励环境中提供比真实梯度更具信息量的学习信号。实验表明，在离散和连续环境中，IRPO相较于基线方法提升了性能与样本效率，并对IRPO求解的优化问题进行了形式化分析。代码发布于https://github.com/Mgineer117/IRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of effective exploration in sparse-reward reinforcement learning, where naive strategies and existing intrinsic reward methods face issues like unstable credit assignment or sample inefficiency, this paper introduces Intrinsic Reward Policy Optimization (IRPO). The method employs a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for extrinsic rewards, using a surrogate policy gradient to provide a more informative learning signal than the true gradient. Experimental results demonstrate that IRPO improves performance and sample efficiency relative to baselines in both discrete and continuous environments, with formal analysis provided for the optimization problem it addresses.</div>
<div class="mono" style="margin-top:8px">本文针对稀疏奖励环境中强化学习探索效率低下的问题，旨在克服传统探索策略和现有内在奖励方法存在的信用分配不稳定或样本效率不足等缺陷，提出了内在奖励策略优化（IRPO）方法。该方法采用一个策略优化框架，利用多种内在奖励直接为外在奖励优化策略，并通过替代策略梯度提供比真实梯度更丰富的学习信号。实验结果表明，在离散和连续环境中，IRPO相较于基线方法均提升了性能与样本效率，并对所解决的优化问题进行了形式化分析。</div>
</details>
</div>
<div class="card">
<div class="title">Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning</div>
<div class="meta-line">Authors: Alex Schutz, Victor-Alexandru Darvariu, Efimia Panagiotaki, Bruno Lacerda, Nick Hawes</div>
<div class="meta-line">First: 2025-09-23T12:49:25+00:00 · Latest: 2026-01-29T08:20:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18930v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.18930v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks to execute classic algorithms by supervised learning. Despite its successes, important limitations remain: inability to construct valid solutions without post-processing and to reason about multiple correct ones, poor performance on combinatorial NP-hard problems, and inapplicability to problems for which strong algorithms are not yet known. To address these limitations, we reframe the problem of learning algorithm trajectories as a Markov Decision Process, which imposes structure on the solution construction procedure and unlocks the powerful tools of imitation and reinforcement learning (RL). We propose the GNARL framework, encompassing the methodology to translate problem formulations from NAR to RL and a learning architecture suitable for a wide range of graph-based problems. We achieve very high graph accuracy results on several CLRS-30 problems, performance matching or exceeding much narrower NAR approaches for NP-hard problems and, remarkably, applicability even when lacking an expert algorithm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>攻克GNARLy难题：通过强化学习重构图神经算法推理</div>
<div class="mono" style="margin-top:8px">神经算法推理（NAR）是一种通过监督学习训练神经网络执行经典算法的范式。尽管取得了一定成功，但仍存在重要局限：无法在不进行后处理的情况下构建有效解、难以对多个正确解进行推理、在组合NP难问题上表现不佳，以及不适用于尚未发现强算法的问题。为解决这些局限，我们将学习算法轨迹的问题重新定义为马尔可夫决策过程，这为解构建过程提供了结构化框架，并解锁了模仿学习与强化学习（RL）的强大工具。我们提出GNARL框架，包含将问题表述从NAR转化为RL的方法论，以及适用于广泛图基问题的学习架构。我们在多个CLRS-30问题上实现了极高的图精度结果，在NP难问题上的性能匹配甚至超越了更局限的NAR方法，值得注意的是，该框架即使在缺乏专家算法的情况下仍具适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of supervised Neural Algorithmic Reasoning (NAR), which struggles with solution construction, multiple correct answers, NP-hard problems, and scenarios lacking expert algorithms, this paper reframes algorithm learning as a Markov Decision Process. The proposed GNARL framework employs reinforcement and imitation learning to structure solution construction for graph-based problems. Experimental results on CLRS-30 benchmarks show high graph accuracy, performance competitive with specialized NAR methods on NP-hard tasks, and successful application even without a known expert algorithm.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决监督式神经算法推理（NAR）的局限性，如无法直接构建有效解、难以处理多个正确答案、在NP难问题上表现不佳，以及无法应用于缺乏已知强算法的问题。为此，研究将算法轨迹学习重新定义为马尔可夫决策过程，并提出了GNARL框架，利用强化学习和模仿学习来结构化地构建图相关问题的解。在CLRS-30基准测试上的实验结果表明，该方法取得了很高的图精度，在NP难问题上的性能匹配或超越了更专门的NAR方法，并且即使在缺乏专家算法的情况下也能有效应用。</div>
</details>
</div>
<div class="card">
<div class="title">Model-Free Output Feedback Stabilization via Policy Gradient Methods</div>
<div class="meta-line">Authors: Ankang Zhang, Ming Chi, Xiaoling Wang, Lintao Ye</div>
<div class="meta-line">First: 2026-01-27T07:15:59+00:00 · Latest: 2026-01-29T08:15:47+00:00</div>
<div class="meta-line">Comments: 31 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19284v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19284v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stabilizing a dynamical system is a fundamental problem that serves as a cornerstone for many complex tasks in the field of control systems. The problem becomes challenging when the system model is unknown. Among the Reinforcement Learning (RL) algorithms that have been successfully applied to solve problems pertaining to unknown linear dynamical systems, the policy gradient (PG) method stands out due to its ease of implementation and can solve the problem in a model-free manner. However, most of the existing works on PG methods for unknown linear dynamical systems assume full-state feedback. In this paper, we take a step towards model-free learning for partially observable linear dynamical systems with output feedback and focus on the fundamental stabilization problem of the system. We propose an algorithmic framework that stretches the boundary of PG methods to the problem without global convergence guarantees. We show that by leveraging zeroth-order PG update based on system trajectories and its convergence to stationary points, the proposed algorithms return a stabilizing output feedback policy for discrete-time linear dynamical systems. We also explicitly characterize the sample complexity of our algorithm and verify the effectiveness of the algorithm using numerical examples.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略梯度方法的无模型输出反馈镇定</div>
<div class="mono" style="margin-top:8px">镇定动态系统是控制领域许多复杂任务的基础问题，当系统模型未知时该问题变得尤为困难。在已成功应用于未知线性动态系统的强化学习算法中，策略梯度方法因其易于实现且能以无模型方式解决问题而备受关注。然而，现有针对未知线性动态系统的策略梯度研究大多假设全状态反馈。本文针对具有输出反馈的部分可观线性动态系统，向无模型学习迈进一步，聚焦于系统的基本镇定问题。我们提出一种算法框架，将策略梯度方法拓展至缺乏全局收敛保证的问题。通过利用基于系统轨迹的零阶策略梯度更新及其对驻点的收敛性，所提算法能为离散时间线性动态系统返回镇定的输出反馈策略。我们明确量化了算法的样本复杂度，并通过数值算例验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of stabilizing unknown linear dynamical systems using only output feedback, a more realistic scenario than the common full-state feedback assumption, by extending policy gradient methods to this partially observable setting. The method employs a model-free, zeroth-order policy gradient approach that learns directly from system trajectories, converging to stationary points without requiring a system model. Experimental results demonstrate that the algorithm successfully returns stabilizing output feedback policies for discrete-time linear systems, with explicitly characterized sample complexity and validation through numerical examples.</div>
<div class="mono" style="margin-top:8px">本文针对仅使用输出反馈来稳定未知线性动态系统的挑战，这是一种比常见的全状态反馈假设更现实的场景，通过将策略梯度方法扩展到这种部分可观测的设置中。该方法采用无模型的零阶策略梯度方法，直接从系统轨迹中学习，无需系统模型即可收敛到驻点。实验结果表明，该算法成功地为离散时间线性系统返回了稳定的输出反馈策略，并明确了样本复杂度，通过数值示例验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Repairing Reward Functions with Feedback to Mitigate Reward Hacking</div>
<div class="meta-line">Authors: Stephane Hatgis-Kessell, Logan Mondal Bhamidipaty, Emma Brunskill</div>
<div class="meta-line">First: 2025-10-14T23:18:24+00:00 · Latest: 2026-01-29T07:52:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13036v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13036v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-designed reward functions for reinforcement learning (RL) agents are frequently misaligned with the humans&#x27; true, unobservable objectives, and thus act only as proxies. Optimizing for a misspecified proxy reward function often induces reward hacking, resulting in a policy misaligned with the human&#x27;s true objectives. An alternative is to perform RL from human feedback, which involves learning a reward function from scratch by collecting human preferences over pairs of trajectories. However, building such datasets is costly. To address the limitations of both approaches, we propose Preference-Based Reward Repair (PBRR): an automated iterative framework that repairs a human-specified proxy reward function by learning an additive, transition-dependent correction term from preferences. A manually specified reward function can yield policies that are highly suboptimal under the ground-truth objective, yet corrections on only a few transitions may suffice to recover optimal performance. To identify and correct for those transitions, PBRR uses a targeted exploration strategy and a new preference-learning objective. We prove in tabular domains PBRR has a cumulative regret that matches, up to constants, that of prior preference-based RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR consistently outperforms baselines that learn a reward function from scratch from preferences or modify the proxy reward function using other approaches, requiring substantially fewer preferences to learn high performing policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于反馈修复奖励函数以缓解奖励破解</div>
<div class="mono" style="margin-top:8px">为强化学习（RL）智能体设计的人工奖励函数常与人类真实但不可观测的目标不一致，仅作为代理目标。优化错误指定的代理奖励函数易引发奖励破解，导致策略偏离人类真实目标。另一种方法是从人类反馈中学习奖励函数，通过收集轨迹对的人类偏好从头构建，但数据集构建成本高昂。为克服两种方法的局限，我们提出基于偏好的奖励修复（PBRR）：一种自动化迭代框架，通过从偏好中学习依赖状态转移的加性修正项，修复人工指定的代理奖励函数。手动指定的奖励函数可能在真实目标下产生严重次优策略，但仅需对少量转移进行修正即可恢复最优性能。为识别并修正这些转移，PBRR采用定向探索策略和新偏好学习目标。我们在表格化环境中证明PBRR的累积遗憾与现有基于偏好的RL方法在常数范围内相当。此外，在一系列奖励破解基准测试中，PBRR始终优于从偏好从头学习奖励函数或通过其他方法修改代理奖励函数的基线，学习高性能策略所需偏好数据显著减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of reward hacking in reinforcement learning, where human-designed proxy reward functions often misalign with true objectives, leading to suboptimal policies. The authors propose Preference-Based Reward Repair (PBRR), an automated iterative method that repairs a given proxy reward function by learning an additive, transition-dependent correction term from human preferences over trajectory pairs, using targeted exploration and a new preference-learning objective. Experimental results on reward-hacking benchmarks show that PBRR outperforms baselines that learn rewards from scratch or modify proxies via other approaches, achieving high-performing policies with substantially fewer human preferences, and theoretical analysis in tabular domains confirms its cumulative regret matches prior preference-based RL methods up to constants.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的奖励黑客问题，即人工设计的代理奖励函数常与真实目标不一致，导致策略次优。作者提出基于偏好的奖励修复方法，这是一种自动化迭代框架，通过从人类对轨迹对的偏好中学习一个依赖于状态转移的加性修正项，来修复给定的代理奖励函数，并采用定向探索和新偏好学习目标。在奖励黑客基准测试中，该方法优于从零学习奖励或通过其他方式修改代理奖励的基线，能以更少的人类偏好学习高性能策略，且在表格域的理论分析表明其累积遗憾与先前基于偏好的强化学习方法在常数范围内匹配。</div>
</details>
</div>
<div class="card">
<div class="title">Expected Improvement via Gradient Norms</div>
<div class="meta-line">Authors: Joshua Hang Sai Ip, Georgios Makrygiorgos, Ali Mesbah</div>
<div class="meta-line">First: 2026-01-29T07:37:13+00:00 · Latest: 2026-01-29T07:37:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21357v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization (BO) is a principled approach for optimizing expensive black-box functions, with Expected Improvement (EI) being one of the most widely used acquisition functions. Despite its empirical success, EI is known to be overly exploitative and can converge to suboptimal stationary points. We propose Expected Improvement via Gradient Norms (EI-GN), a novel acquisition function that applies the improvement principle to a gradient-aware auxiliary objective, thereby promoting sampling in regions that are both high-performing and approaching first-order stationarity. EI-GN relies on gradient observations used to learn gradient-enhanced surrogate models that enable principled gradient inference from function evaluations. We derive a tractable closed-form expression for EI-GN that allows efficient optimization and show that the proposed acquisition is consistent with the improvement-based acquisition framework. Empirical evaluations on standard BO benchmarks demonstrate that EI-GN yields consistent improvements against standard baselines. We further demonstrate applicability of EI-GN to control policy learning problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于梯度范数的期望改进</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）是一种优化昂贵黑盒函数的原理性方法，其中期望改进（EI）是最广泛使用的采集函数之一。尽管EI在实证中取得成功，但其已知具有过度开发性，可能收敛至次优驻点。本文提出基于梯度范数的期望改进（EI-GN），这是一种新颖的采集函数，将改进原则应用于梯度感知的辅助目标，从而促进在高性能且接近一阶驻点的区域进行采样。EI-GN依赖于用于学习梯度增强代理模型的梯度观测，该模型支持从函数评估中进行原理性梯度推断。我们推导了EI-GN的可处理闭式表达式，支持高效优化，并证明所提出的采集函数与基于改进的采集框架一致。在标准BO基准上的实证评估表明，EI-GN相较于标准基线方法实现了持续改进。我们进一步展示了EI-GN在控制策略学习问题中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the exploitative nature and tendency to converge to suboptimal points of the widely used Expected Improvement (EI) acquisition function in Bayesian Optimization (BO). The authors propose Expected Improvement via Gradient Norms (EI-GN), a novel acquisition function that applies the improvement principle to a gradient-aware auxiliary objective, encouraging sampling in regions that are both high-performing and near stationarity; this relies on gradient observations to build gradient-enhanced surrogate models for principled gradient inference. Experimental results on standard BO benchmarks show that EI-GN consistently outperforms baseline methods, and its applicability is further demonstrated in control policy learning problems.</div>
<div class="mono" style="margin-top:8px">本文针对贝叶斯优化中广泛使用的期望改进采集函数存在的过度开发和易收敛于次优点的问题，提出了基于梯度范数的期望改进方法。该方法通过将改进原则应用于一个梯度感知的辅助目标，鼓励在性能高且接近一阶平稳点的区域进行采样，并利用梯度观测构建梯度增强的代理模型以实现梯度推断。在标准贝叶斯优化基准测试上的实验结果表明，该方法相比基线方法取得了持续改进，并在控制策略学习问题中展示了应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging On-Device and Cloud LLMs for Collaborative Reasoning: A Unified Methodology for Local Routing and Post-Training</div>
<div class="meta-line">Authors: Wenzhi Fang, Dong-Jun Han, Liangqi Yuan, Evan Chen, Christopher Brinton</div>
<div class="meta-line">First: 2025-09-28T19:48:56+00:00 · Latest: 2026-01-29T07:25:15+00:00</div>
<div class="meta-line">Comments: We propose a unified post-training framework that integrates routing optimization, enabling the on-device LLM to improve its problem-solving ability while learning routing strategies</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24050v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.24050v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Device-cloud collaboration holds promise for deploying large language models (LLMs), leveraging lightweight on-device models for efficiency while relying on powerful cloud models for superior reasoning. A central challenge in this setting is determining, for each incoming query, whether it should be processed locally or offloaded to the cloud. Existing approaches typically rely on external routers, which often struggle to determine difficulty from the prompt itself, especially for tasks involving complex reasoning. Motivated by this limitation, we propose enabling on-device LLMs to decide internally whether to invoke cloud assistance at inference time, with this capability instilled through reinforcement learning based post-training. Casting on-device LLM post-training as a reward maximization problem, we design hierarchical rewards to encourage local problem solving and judicious cloud offloading. To solve the resulting problem, we develop an algorithm featuring a group-level policy gradient that stabilizes optimization, together with adaptive prompt filtering that provides complementary learning signals to mitigate policy collapse (i.e., exclusive local execution or exclusive cloud offloading). Extensive experiments on on-device-scale LLaMA and Qwen models across multiple reasoning benchmarks show that our method consistently outperforms baselines and significantly narrows the gap to full cloud LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>设备端与云端大语言模型协同推理的桥梁：本地路由与后训练的统一方法论</div>
<div class="mono" style="margin-top:8px">设备-云端协作为大语言模型部署提供了前景，通过轻量级设备端模型实现高效处理，同时依托强大的云端模型获得卓越推理能力。该场景的核心挑战在于如何针对每个输入查询，判定其应在本地处理还是卸载至云端。现有方法通常依赖外部路由器，这类方案往往难以仅从提示本身判断任务难度，尤其在涉及复杂推理的任务中表现不足。受此局限启发，我们提出让设备端大语言模型在推理时自主决定是否调用云端协助，该能力通过基于强化学习的后训练注入。将设备端大语言模型后训练构建为奖励最大化问题，我们设计了分层奖励机制以激励本地问题解决与审慎的云端卸载。针对该优化问题，我们开发了一种融合组级策略梯度以稳定训练的自适应算法，并结合动态提示过滤技术提供互补学习信号，从而缓解策略崩溃现象（即仅执行本地处理或仅进行云端卸载）。在多个推理基准测试中，对设备端规模的LLaMA和Qwen模型进行的广泛实验表明，我们的方法持续超越基线模型，并显著缩小了与完整云端大语言模型的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of effectively routing queries between on-device and cloud large language models (LLMs) in collaborative deployments, where external routers often fail to assess query difficulty accurately, this paper introduces a unified post-training methodology that enables on-device LLMs to autonomously decide when to offload to the cloud. The method employs reinforcement learning with hierarchical rewards to encourage local problem-solving and judicious cloud offloading, stabilized by a group-level policy gradient and adaptive prompt filtering to prevent policy collapse. Experimental results on LLaMA and Qwen models across reasoning benchmarks demonstrate that this approach consistently outperforms baseline methods and significantly narrows the performance gap with full cloud LLMs.</div>
<div class="mono" style="margin-top:8px">针对设备与云端大语言模型协作部署中，外部路由机制难以准确评估查询复杂性的挑战，本文提出了一种统一的训练后方法，使设备端模型能够自主决定何时调用云端辅助。该方法采用强化学习，通过分层奖励机制鼓励本地问题解决和审慎的云端卸载，并利用组级策略梯度进行稳定优化，结合自适应提示过滤以防止策略崩溃。在LLaMA和Qwen模型上的多推理基准实验表明，该方法持续优于基线，显著缩小了与全云端模型之间的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Factored Causal Representation Learning for Robust Reward Modeling in RLHF</div>
<div class="meta-line">Authors: Yupei Yang, Lin Yang, Wanxi Deng, Lin Qu, Fan Feng, Biwei Huang, Shikui Tu, Lei Xu</div>
<div class="meta-line">First: 2026-01-29T07:18:45+00:00 · Latest: 2026-01-29T07:18:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21350v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21350v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A reliable reward model is essential for aligning large language models with human preferences through reinforcement learning from human feedback. However, standard reward models are susceptible to spurious features that are not causally related to human labels. This can lead to reward hacking, where high predicted reward does not translate into better behavior. In this work, we address this problem from a causal perspective by proposing a factored representation learning framework that decomposes the model&#x27;s contextual embedding into (1) causal factors that are sufficient for reward prediction and (2) non-causal factors that capture reward-irrelevant attributes such as length or sycophantic bias. The reward head is then constrained to depend only on the causal component. In addition, we introduce an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal to discourage them from encoding reward-relevant information. Experiments on both mathematical and dialogue tasks demonstrate that our method learns more robust reward models and consistently improves downstream RLHF performance over state-of-the-art baselines. Analyses on length and sycophantic bias further validate the effectiveness of our method in mitigating reward hacking behaviors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向RLHF稳健奖励建模的因子化因果表征学习</div>
<div class="mono" style="margin-top:8px">可靠的奖励模型对于通过人类反馈强化学习对齐大语言模型与人类偏好至关重要。然而，标准奖励模型易受与人类标签无因果关系的伪特征影响，可能导致奖励欺骗现象——高预测奖励并未转化为更优行为。本研究从因果视角提出因子化表征学习框架：将模型上下文嵌入分解为（1）足以预测奖励的因果因子，及（2）捕获奖励无关属性（如文本长度或谄媚偏见）的非因果因子。奖励预测头被约束为仅依赖因果成分。同时引入对抗性预测头，通过梯度反转机制抑制非因果因子编码奖励相关信息。数学与对话任务的实验表明，该方法能学习更稳健的奖励模型，在RLHF下游任务中持续超越现有基线。针对长度与谄媚偏见的分析进一步验证了本方法缓解奖励欺骗行为的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of standard reward models in reinforcement learning from human feedback (RLHF) to spurious, non-causal features, which can lead to reward hacking. To build a more robust reward model, the authors propose a factored causal representation learning framework that decomposes contextual embeddings into causal factors sufficient for reward prediction and non-causal factors capturing irrelevant attributes like length or sycophantic bias; the reward head is constrained to use only the causal component, while an adversarial head with gradient reversal discourages reward-relevant information in non-causal factors. Experimental results on mathematical and dialogue tasks show that this method learns more robust reward models, improves downstream RLHF performance over state-of-the-art baselines, and effectively mitigates reward hacking behaviors related to length and sycophantic bias.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习中标准奖励模型易受虚假、非因果特征影响而导致奖励攻击的问题，提出了一种分解因果表示学习框架以提高奖励模型的鲁棒性。该方法将上下文嵌入分解为足以预测奖励的因果因子和捕获无关属性（如长度或谄媚偏差）的非因果因子，并约束奖励头仅依赖于因果成分，同时通过带梯度反转的对抗头阻止非因果因子编码奖励相关信息。在数学和对话任务上的实验表明，该方法能学习到更鲁棒的奖励模型，在下游RLHF性能上持续优于现有先进基线，并有效缓解了与长度和谄媚偏差相关的奖励攻击行为。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Improving Pretraining: using post-trained models to pretrain better models</div>
<div class="meta-line">Authors: Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar, Jason Weston, Olga Golovneva</div>
<div class="meta-line">First: 2026-01-29T07:09:30+00:00 · Latest: 2026-01-29T07:09:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21343v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21343v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model&#x27;s core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我改进式预训练：利用后训练模型预训练更优模型</div>
<div class="mono" style="margin-top:8px">确保大语言模型生成内容的安全性、事实性与整体质量是一项关键挑战，尤其在模型日益投入实际应用的背景下。当前主流解决方案依赖昂贵且精心标注的数据集，并进行多阶段微调与对齐，但即使如此复杂的流程仍无法修正预训练阶段习得的错误模式。因此，在预训练阶段解决这些问题至关重要，这能塑造模型的核心行为模式，防止不安全或虚构输出被深度固化。为此，我们提出一种新型预训练方法：通过流式文档处理，运用强化学习逐步优化后续K个生成标记。一个经过后训练的强模型将对候选生成内容（包括模型推演序列、原始后缀及重写后缀）进行质量、安全性与事实性评估。训练初期依赖原始与重写后缀；随着模型改进，强化学习将奖励高质量推演。该方法从底层构建了更优质、更安全、更符合事实的模型。实验表明，本方法在事实性与安全性方面相较标准预训练分别实现36.2%与18.5%的相对提升，整体生成质量胜率最高提升达86.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to embed safety, factuality, and quality directly into large language models during pretraining, rather than relying solely on costly post-training alignment, which cannot fully correct patterns learned early on. The method introduces a self-improving pretraining approach that streams documents and uses reinforcement learning to optimize the next K tokens, with a post-trained model judging candidate generations—including model rollouts, original text, and rewritten text—for quality, safety, and factuality, shifting reliance from external suffixes to model rollouts as training progresses. Experimental results show that this method achieves relative improvements of 36.2% in factuality and 18.5% in safety over standard pretraining, with win rate improvements up to 86.3% in overall generation quality.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要在预训练阶段就将安全性、事实性和质量直接嵌入大型语言模型，而非仅依赖昂贵的后训练对齐，因为后者无法完全纠正早期学习到的模式。方法提出了一种自我改进的预训练方法，通过流式处理文档并使用强化学习优化后续K个词元，由一个后训练模型评估候选生成（包括模型自生成、原始文本和改写文本）的质量、安全性和事实性，并随着训练进展从依赖外部文本转向模型自生成。实验结果表明，该方法在事实性上相比标准预训练相对提升36.2%，安全性提升18.5%，整体生成质量的胜率提升高达86.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach</div>
<div class="meta-line">Authors: Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li, Chung Shue Chen, Man-On Pun</div>
<div class="meta-line">First: 2026-01-29T06:26:16+00:00 · Latest: 2026-01-29T06:26:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21316v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21316v1">PDF</a> · <a href="https://github.com/Traffic-Alpha/UAGMC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic exploration.To address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at https://github.com/Traffic-Alpha/UAGMC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向按需空中出租车服务的异构垂直起降场选择优化：一种深度强化学习方法</div>
<div class="mono" style="margin-top:8px">城市空中交通（UAM）作为一种变革性解决方案，通过利用低空空域缓解城市拥堵，从而减轻地面交通网络压力。为实现真正高效无缝的门到门出行体验，UAM需与现有地面交通基础设施紧密融合。然而，当前针对空陆联运系统中乘客最优集成路径策略的研究仍显不足，缺乏系统性探索。为填补这一空白，我们首先提出一个统一优化模型，集成空陆交通策略选择。该模型捕捉多式联运网络的动态特性，并结合实时交通状况与乘客决策行为。基于此模型，我们提出统一空陆交通协调框架，利用深度强化学习和车联网通信技术优化垂直起降场选择并动态规划空中出租车路线。实验结果表明，相较于传统比例分配方法，该框架使平均出行时间减少34%，提升了整体出行效率，并为多式联运系统的集成优化提供了新思路。本研究通过协调空陆交通模式，为推进智能城市出行解决方案奠定了坚实基础。相关代码可在 https://github.com/Traffic-Alpha/UAGMC 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the research gap in integrated routing strategies for Urban Air Mobility (UAM) by proposing a unified optimization model that captures dynamic multimodal network characteristics and passenger decision-making. The method introduces a Unified Air-Ground Mobility Coordination (UAGMC) framework, which employs deep reinforcement learning and V2X communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results show that UAGMC reduces average travel time by 34% compared to conventional proportional allocation methods, significantly enhancing travel efficiency and offering insights for multimodal transportation integration.</div>
<div class="mono" style="margin-top:8px">本文针对城市空中交通中综合路径规划策略的研究不足，提出了一个统一优化模型，该模型捕捉了多式联运网络的动态特性及乘客决策行为。方法上，引入了基于深度强化学习和车联网通信的统一空地交通协调框架，以优化垂直起降场选择并动态规划空中出租车路线。实验结果表明，与传统比例分配方法相比，该框架将平均出行时间减少了34%，显著提升了出行效率，并为多式联运系统的整合优化提供了新见解。</div>
</details>
</div>
<div class="card">
<div class="title">Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach</div>
<div class="meta-line">Authors: Xiaozhuang Li, Xindi Tang, Fang He</div>
<div class="meta-line">First: 2026-01-29T06:16:34+00:00 · Latest: 2026-01-29T06:16:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21312v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21312v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging network, this simplification creates a significant gap between theoretical models and real-world operations. To bridge this gap, we propose GAT-PEARL, a novel meta-reinforcement learning framework that learns an adaptive operational policy. Our approach integrates a graph attention network (GAT) to effectively extract robust spatial representations under infrastructure layouts and model the complex spatiotemporal relationships of the urban environment, and employs probabilistic embeddings for actor-critic reinforcement learning (PEARL) to enable rapid, inference-based adaptation to changes in charging network layouts without retraining. Through extensive simulations on real-world data in Chengdu, China, we demonstrate that GAT-PEARL significantly outperforms conventional reinforcement learning baselines, showing superior generalization to unseen infrastructure layouts and achieving higher overall operational efficiency in dynamic settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向充电基础设施演化的自动化电动出租车车队动态运营：一种元深度强化学习方法</div>
<div class="mono" style="margin-top:8px">随着电动汽车及充电基础设施的快速扩张，在充电可用性动态不确定的环境中，自动驾驶电动出租车（AET）车队的有效管理面临关键挑战。现有研究多假设静态充电网络，这种简化导致理论模型与实际运营存在显著差距。为弥合此差距，我们提出GAT-PEARL——一种学习自适应运营策略的新型元强化学习框架。该方法集成图注意力网络（GAT），以有效提取基础设施布局下的稳健空间表征并建模城市环境的复杂时空关系；同时采用基于概率嵌入的行动者-评论家强化学习（PEARL），无需重新训练即可实现基于推理的充电网络布局快速适应。通过在成都真实数据上的大量仿真实验，我们证明GAT-PEARL显著优于传统强化学习基线，对未见过的基础设施布局展现出卓越的泛化能力，并在动态环境中实现了更高的整体运营效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the challenge of managing autonomous electric taxi fleets in dynamic urban environments where charging infrastructure availability is uncertain and evolving, a gap left by existing models that assume static networks. The method introduces GAT-PEARL, a meta-reinforcement learning framework that combines a graph attention network to capture spatiotemporal relationships and probabilistic embeddings for actor-critic learning, enabling rapid adaptation to new charging layouts without retraining. Experimental results on real-world Chengdu data show that GAT-PEARL outperforms conventional reinforcement learning baselines, demonstrating superior generalization to unseen infrastructure layouts and higher operational efficiency in dynamic settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于在充电基础设施可用性不确定且动态变化的城市环境中管理自动电动出租车车队的挑战，现有模型假设静态网络留下了这一空白。方法上提出了GAT-PEARL，这是一种元强化学习框架，结合图注意力网络捕捉时空关系，并采用概率嵌入的演员-评论家学习，无需重新训练即可快速适应新的充电布局。基于中国成都真实数据的实验结果表明，GAT-PEARL显著优于传统强化学习基线，在未见过的充电布局中表现出更优的泛化能力，并在动态环境中实现了更高的整体运营效率。</div>
</details>
</div>
<div class="card">
<div class="title">The Surprising Difficulty of Search in Model-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Wei-Di Chang, Mikael Henaff, Brandon Amos, Gregory Dudek, Scott Fujimoto</div>
<div class="meta-line">First: 2026-01-29T05:58:24+00:00 · Latest: 2026-01-29T05:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21306v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模型的强化学习中搜索的意外困难</div>
<div class="mono" style="margin-top:8px">本文研究基于模型的强化学习（RL）中的搜索问题。传统观点认为长期预测和误差累积是基于模型RL的主要障碍。我们挑战了这一观点，指出搜索并非学习策略的即插即用替代方案。令人惊讶的是，即使模型精度很高，搜索也可能损害性能。相反，我们证明缓解分布偏移比提升模型或价值函数精度更为关键。基于这一洞见，我们提出了实现有效搜索的关键技术，在多个主流基准领域取得了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the conventional view that long-term prediction errors are the main challenge in model-based reinforcement learning, arguing instead that effectively integrating search with a learned policy is surprisingly difficult and can even degrade performance despite an accurate model. The authors propose that mitigating distribution shift during search is more critical than solely improving model or value function accuracy. Their experimental results demonstrate that by identifying and applying key techniques to address this shift, their method achieves state-of-the-art performance across multiple benchmark domains.</div>
<div class="mono" style="margin-top:8px">本文挑战了传统观点，即长期预测误差是基于模型的强化学习的主要挑战，转而认为将搜索与学习策略有效结合异常困难，即使在模型高度准确的情况下也可能损害性能。作者提出，缓解搜索过程中的分布偏移比单纯提高模型或价值函数精度更为关键。实验结果表明，通过识别并应用关键技术来解决这种偏移，他们的方法在多个流行基准领域实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration</div>
<div class="meta-line">Authors: Yuejie Li, Ke Yang, Tao Wang, Bolin Chen, Bowen Li, Chengjun Mao</div>
<div class="meta-line">First: 2026-01-16T10:02:31+00:00 · Latest: 2026-01-29T05:46:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11144v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.11144v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度图检索增强生成：一种层次化检索与自适应融合的平衡方法</div>
<div class="mono" style="margin-top:8px">基于图的检索增强生成（GraphRAG）框架面临全局检索全面性与局部检索效率之间的权衡。现有方法在导航大规模层次化图结构、优化检索路径、平衡探索-利用动态性方面常遇挑战，且多缺乏鲁棒的多阶段重排序机制。为克服这些不足，本文提出深度图检索增强生成（Deep GraphRAG）框架，旨在实现层次化检索与自适应融合的平衡。该框架引入从全局到局部的层次化检索策略，整合宏观的社区间关联与微观的社区内语境关系。该策略采用三阶段流程：（1）社区间过滤，利用局部语境剪枝搜索空间；（2）社区级优化，通过实体交互分析对相关子图进行优先级排序；（3）目标社区内的实体级细粒度搜索。通过基于束搜索优化的动态重排序模块引导全过程，持续筛选候选结果以平衡效率与全局覆盖性。Deep GraphRAG还配备知识融合模块，采用经动态加权奖励GRPO（DW-GRPO）训练的紧凑大语言模型。这种创新的强化学习方法动态调整奖励权重，以平衡相关性、忠实度与简洁性三大目标。该训练使紧凑模型（1.5B参数）在融合任务中接近大模型（70B参数）的性能。在Natural Questions和HotpotQA数据集上的评估表明，Deep GraphRAG在准确性与效率上均显著优于基线图检索方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the trade-off between global comprehensiveness and local efficiency in existing GraphRAG frameworks, this paper proposes Deep GraphRAG, a framework featuring a hierarchical global-to-local retrieval strategy and an adaptive knowledge integration module. The method employs a three-stage retrieval process—inter-community filtering, community-level refinement, and entity-level search—guided by a beam search-optimized re-ranking module, and integrates knowledge using a compact LLM trained with a novel Dynamic Weighting Reward GRPO (DW-GRPO) reinforcement learning approach to balance relevance, faithfulness, and conciseness. Experimental results on Natural Questions and HotpotQA show that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency, with compact 1.5B models approaching the performance of much larger 70B models.</div>
<div class="mono" style="margin-top:8px">针对现有基于图的检索增强生成框架在全局搜索全面性与局部搜索效率之间的权衡问题，本文提出了Deep GraphRAG框架，旨在实现分层检索与自适应集成的平衡。该方法采用一种从全局到局部的分层检索策略，包含社区间过滤、社区级细化和社区内实体级搜索的三阶段过程，并通过集束搜索优化的动态重排序模块进行引导；同时，其知识集成模块利用一个采用新颖的动态加权奖励GRPO强化学习方法训练的紧凑大语言模型，以动态平衡相关性、忠实性和简洁性。在Natural Questions和HotpotQA数据集上的评估表明，Deep GraphRAG在准确性和效率上均显著优于基线图检索方法，且1.5B的紧凑模型在集成任务上性能可接近70B的大型模型。</div>
</details>
</div>
<div class="card">
<div class="title">EGAM: Extended Graph Attention Model for Solving Routing Problems</div>
<div class="meta-line">Authors: Licheng Wang, Yuzi Yan, Mingtao Huang, Yuan Shen</div>
<div class="meta-line">First: 2026-01-29T05:30:34+00:00 · Latest: 2026-01-29T05:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21281v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EGAM：用于求解路径规划问题的扩展图注意力模型</div>
<div class="mono" style="margin-top:8px">基于图神经网络（GNN）实现的神经组合优化（NCO）求解器为路径规划问题提供了新方法。通过强化学习（RL）训练，当前最先进的图注意力模型（GAM）无需专家知识或标注数据即可获得近似最优解。本研究对现有图注意力机制进行泛化，提出扩展图注意力模型（EGAM）。该模型采用多头点积注意力机制同时更新节点与边嵌入，克服了传统GAM仅考虑节点特征的局限性。我们采用自回归编码器-解码器架构，并配合融入特殊设计基线的策略梯度算法进行训练。实验表明，EGAM在多种路径规划问题上均达到或超越现有方法。特别值得注意的是，该模型在强约束问题上展现出卓越性能，凸显了其处理复杂图结构的高效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance neural combinatorial optimization for routing problems beyond existing graph attention models that focus solely on node features, this paper introduces the Extended Graph Attention Model (EGAM). The method generalizes the graph attention mechanism by employing multi-head dot-product attention to update both node and edge embeddings within an autoregressive encoder-decoder architecture, trained with policy gradient algorithms and a custom baseline. Experimental results demonstrate that EGAM matches or surpasses prior methods across various routing tasks, showing particularly strong performance on highly constrained problems, which underscores its efficiency in managing complex graph structures.</div>
<div class="mono" style="margin-top:8px">本文的动机是改进神经组合优化方法，以解决现有图注意力模型仅关注节点特征的限制，提出了扩展图注意力模型（EGAM）。该方法通过使用多头点积注意力来更新节点和边嵌入，采用自回归编码器-解码器架构，并利用结合定制基线的策略梯度算法进行训练。实验结果表明，EGAM在多种路由问题上达到或超越了现有方法，尤其在高度约束问题上表现出色，突显了其处理复杂图结构的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels</div>
<div class="meta-line">Authors: Micah Rentschler, Jesse Roberts</div>
<div class="meta-line">First: 2026-01-29T05:02:08+00:00 · Latest: 2026-01-29T05:02:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21268v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most reinforcement learning (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce Reinforcement Learning from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator&#x27;s answers to natural-language meta-questions (e.g., &quot;Is the answer correct?&quot; or &quot;Is the reasoning logically consistent?&quot;). RLME treats the evaluator&#x27;s probability of a positive judgment as a reward and updates the generator via group-relative policy optimization, enabling learning without labels. Across a suite of experiments, we show that RLME achieves accuracy and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于元评估的强化学习：无需真实标签的语言模型对齐方法</div>
<div class="mono" style="margin-top:8px">现有训练大语言模型（LLM）的强化学习方法大多依赖真实标签或任务特定验证器，当正确性难以界定或获取成本高昂时，其可扩展性受限。本文提出基于元评估的强化学习（RLME），该方法通过评估者对自然语言元问题（如“答案是否正确？”或“推理是否逻辑一致？”）的反馈生成奖励信号，并利用组间相对策略优化更新生成器，实现无标签学习。实验表明：RLME在准确性和样本效率上达到与基于标签训练相当的水平；支持多目标可控权衡；引导模型形成可靠推理模式而非事后合理化；在缺乏真实标签的开放域场景中仍具泛化能力，从而拓展了强化学习训练LLM的应用领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of conventional reinforcement learning methods for large language models, which rely on ground-truth labels or task-specific verifiers that are often costly or ambiguous to obtain. It proposes Reinforcement Learning from Meta-Evaluation (RLME), a method that optimizes a generator by using rewards derived from an evaluator&#x27;s responses to natural-language meta-questions, such as assessing correctness or logical consistency, and applies group-relative policy optimization for label-free learning. Experimental results demonstrate that RLME achieves accuracy and sample efficiency similar to label-based training, allows controllable trade-offs among multiple objectives, encourages reliable reasoning patterns over post-hoc rationalization, and generalizes effectively to open-domain settings without ground-truth labels, thereby expanding the applicability of RL in training LLMs.</div>
<div class="mono" style="margin-top:8px">本文针对传统强化学习方法训练大语言模型时依赖真实标签或任务特定验证器所带来的成本高或模糊性问题，提出了一种基于元评估的强化学习方法。该方法通过评估者对自然语言元问题的回答生成奖励，并采用组相对策略优化来无标签地训练生成器。实验结果表明，该方法在准确性和样本效率上与基于标签的训练相当，支持多目标间的可控权衡，引导模型形成可靠的推理模式而非事后合理化，并在无真实标签的开放域场景中具有良好的泛化能力，从而拓宽了强化学习在语言模型训练中的应用范围。</div>
</details>
</div>
<div class="card">
<div class="title">Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification</div>
<div class="meta-line">Authors: Yiju Guo, Tianyi Hu, Zexu Sun, Yankai Lin</div>
<div class="meta-line">First: 2026-01-29T04:08:24+00:00 · Latest: 2026-01-29T04:08:24+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21244v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21244v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>降噪增声：基于指令净化的强化学习推理方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）虽推动了大型语言模型的推理能力，但在有限计算预算下仍受低效探索的制约，导致复杂任务中采样成功率低、训练不稳定。研究发现，许多探索失败并非源于问题本身难度，而是由少数引入干扰的提示词元所致。基于此，本文提出降噪采样框架（LENS），其首先通过识别并移除干扰词元进行提示净化，随后将净化过程中成功的轨迹迁移至原始含噪提示上监督策略优化，使模型学会在实际含噪提示环境中忽略干扰。实验表明，LENS显著优于GRPO，实现了更高性能与更快收敛，平均提升3.88%，加速超1.6倍。本研究揭示了剪除干扰词元对提升轨迹效率的关键作用，为RLVR研究提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of reinforcement learning with verifiable rewards (RLVR) in complex reasoning tasks, where limited rollout budgets lead to low sampling success and unstable training, this work identifies that many exploration failures stem from a small number of interfering prompt tokens rather than inherent problem difficulty. The proposed method, Less Noise Sampling Framework (LENS), addresses this by first purifying prompts through interference token removal and then transferring successful rollouts from purified prompts to supervise policy optimization on original noisy prompts, enabling the model to learn to ignore interference in real-world settings. Experimental results demonstrate that LENS significantly outperforms GRPO, achieving an average performance gain of 3.88% and a convergence speedup of over 1.6 times, highlighting the importance of pruning interference tokens for improved rollout efficiency in RLVR research.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习与可验证奖励（RLVR）在复杂推理任务中因有限采样预算导致采样成功率低、训练不稳定的低效探索问题，发现许多探索失败源于少量干扰提示词而非任务本身难度。为此，提出了低噪声采样框架（LENS），其方法首先通过识别并移除干扰词来净化提示，然后将净化后提示的成功采样结果迁移至原始噪声提示上监督策略优化，使模型学会在真实噪声环境中忽略干扰。实验结果表明，LENS显著优于GRPO，平均性能提升3.88%，收敛速度加快超过1.6倍，强调了修剪干扰词对提升RLVR研究中采样效率的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning</div>
<div class="meta-line">Authors: Xixian Yong, Peilin Sun, Zihe Wang, Xiao Zhou</div>
<div class="meta-line">Venue: The Web Conference 2026</div>
<div class="meta-line">First: 2026-01-29T03:23:40+00:00 · Latest: 2026-01-29T03:23:40+00:00</div>
<div class="meta-line">Comments: The Web Conference 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective urban planning is crucial for enhancing residents&#x27; quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Intelli-Planner：基于大语言模型增强强化学习的定制化城市规划</div>
<div class="mono" style="margin-top:8px">有效的城市规划对提升居民生活质量、保障社会稳定性至关重要，在城市可持续发展中扮演关键角色。现有规划方法严重依赖人工专家，耗时耗力，或采用深度学习算法，常限制利益相关方参与。为弥补这些不足，我们提出Intelli-Planner——一种融合深度强化学习与大语言模型的新型框架，以促进参与式、定制化规划方案的生成。该框架利用人口、地理数据及规划偏好，确定各类功能区域的高层规划需求。训练过程中采用知识增强模块提升策略网络的决策能力，并建立多维评估体系，借助基于大语言模型的利益相关方进行满意度评分。多场景城市环境实验表明，Intelli-Planner在客观指标上超越传统基线方法，与前沿深度强化学习方法性能相当，同时显著提升利益相关方满意度与收敛速度。这些发现印证了框架的有效性与优越性，凸显了大语言模型与深度强化学习融合在功能区规划任务中的革新潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to overcome the time-consuming, expert-dependent nature of traditional urban planning and the limited stakeholder involvement in deep learning approaches, this paper introduces Intelli-Planner, a framework that integrates Deep Reinforcement Learning (DRL) with large language models (LLMs) to generate participatory and customized urban plans. The method uses demographic, geographic, and preference data to define planning requirements, employs a knowledge enhancement module to improve policy network decisions, and leverages LLM-based stakeholders within a multi-dimensional evaluation system for satisfaction scoring. Experimental results across various urban settings demonstrate that Intelli-Planner outperforms traditional baselines, matches state-of-the-art DRL methods in objective metrics, and improves stakeholder satisfaction and convergence speed, highlighting the potential of combining LLMs and DRL for functional area planning.</div>
<div class="mono" style="margin-top:8px">针对传统城市规划方法依赖专家、耗时费力，以及深度学习算法中利益相关者参与不足的问题，本文提出了Intelli-Planner框架，通过将深度强化学习（DRL）与大语言模型（LLM）相结合，以生成参与式、定制化的城市规划方案。该方法利用人口、地理数据和规划偏好来确定各类功能区域的高层需求，在训练中采用知识增强模块提升策略网络的决策能力，并建立多维评估系统，借助基于LLM的利益相关者进行满意度评分。在不同城市环境下的实验验证表明，Intelli-Planner超越了传统基线方法，在客观指标上与先进的DRL方法性能相当，同时提高了利益相关者满意度和收敛速度，这凸显了LLM与DRL结合在功能区规划任务中的潜力和优越性。</div>
</details>
</div>
<div class="card">
<div class="title">When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning</div>
<div class="meta-line">Authors: Wei Wen, Sihang Deng, Tianjun Wei, Keyu Chen, Ruizhi Qiao, Xing Sun</div>
<div class="meta-line">First: 2026-01-29T03:16:53+00:00 · Latest: 2026-01-29T03:16:53+00:00</div>
<div class="meta-line">Comments: 16 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时应扩大搜索：基于强化学习的自适应复杂查询优化</div>
<div class="mono" style="margin-top:8px">查询优化是提升检索增强生成（RAG）系统效能的关键环节。尽管基于强化学习（RL）的代理推理方法近期成为查询优化的新兴方向，现有研究多聚焦于单一查询的扩展与抽象。然而现实场景中复杂用户查询普遍存在，常需通过并行与串行相结合的搜索策略实现消歧与解构。直接将RL应用于此类复杂案例面临显著挑战：确定最优子查询数量、对检索文档进行有效重排与融合，会急剧扩大搜索空间并加剧奖励函数设计的复杂性，常导致训练不稳定。为应对这些挑战，我们提出名为自适应复杂查询优化（ACQO）的新型RL框架。该框架能自适应决策搜索过程的扩展时机与方式，其核心包含两个模块：自适应查询重构（AQR）模块动态决定何时将查询分解为多个子查询；排序-分数融合（RSF）模块确保稳健的结果聚合并为智能体提供稳定的奖励信号。为缓解训练不稳定性，我们采用课程强化学习（CRL）方法，通过两阶段策略逐步引入复杂查询以稳定训练过程。综合实验表明，ACQO在三个复杂查询基准测试中达到最先进性能，显著超越现有基线。该框架还展现出更优的计算效率及对不同检索架构的广泛兼容性，为下一代RAG系统提供了强大且可泛化的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing complex user queries in Retrieval-Augmented Generation (RAG) systems, where existing reinforcement learning (RL) methods often struggle with determining the optimal number of sub-queries and aggregating results, leading to training instability. The authors propose the Adaptive Complex Query Optimization (ACQO) framework, which features an Adaptive Query Reformulation module to dynamically decide when to decompose queries and a Rank-Score Fusion module for robust result aggregation, stabilized via a Curriculum Reinforcement Learning training strategy. Experimental results on three benchmarks show that ACQO achieves state-of-the-art performance, outperforming baselines while improving computational efficiency and demonstrating compatibility with various retrieval architectures.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）系统中复杂用户查询优化的挑战，现有强化学习方法常在确定最优子查询数量和结果聚合方面存在困难，导致训练不稳定。作者提出了自适应复杂查询优化（ACQO）框架，其核心包括自适应查询重构模块以动态决定何时分解查询，以及排序-分数融合模块用于稳健的结果聚合，并通过课程强化学习策略稳定训练过程。在三个基准测试上的实验结果表明，ACQO实现了最先进的性能，显著优于现有基线，同时提升了计算效率，并展现出与不同检索架构的广泛兼容性。</div>
</details>
</div>
<div class="card">
<div class="title">Do Reasoning Models Enhance Embedding Models?</div>
<div class="meta-line">Authors: Wun Yu Chan, Shaojin Chen, Huihao Jing, Kwun Hang Lau, Elton Chun-Chai Li, Zihao Wang, Haoran Li, Yangqiu Song</div>
<div class="meta-line">First: 2026-01-29T02:48:34+00:00 · Latest: 2026-01-29T02:48:34+00:00</div>
<div class="meta-line">Comments: 10 main pages, 18 appendix pages, 13 figures, 11 tables, 4 prompts</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold&#x27;s local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理模型能否增强嵌入模型？</div>
<div class="mono" style="margin-top:8px">当前最先进的嵌入模型越来越多地源自仅解码器的大型语言模型（LLM）主干，并通过对比学习进行适配。随着通过可验证奖励强化学习（RLVR）训练出的推理模型的出现，一个自然的问题随之产生：当这些模型作为嵌入初始化时，增强的推理能力是否能转化为更优的语义表示？与预期相反，我们在MTEB和BRIGHT上的评估显示了一种**无效效应**：从经过RLVR调优的主干初始化的嵌入模型，在采用相同训练方案时，并未比其基础对应模型展现出持续的性能优势。为解析这一悖论，我们引入了**层次化表示相似性分析（HRSA）**框架，该框架将相似性分解为表示层、几何层和功能层。HRSA揭示，虽然RLVR会引发潜在流形局部几何结构的不可逆重组和可逆的坐标基漂移，但它保留了全局流形几何结构和线性读出。因此，随后的对比学习驱动了基础初始化模型与推理初始化模型之间的强对齐，这一现象我们称之为**流形重对齐**。实证表明，我们的发现意味着，与监督微调（SFT）不同，RLVR是在现有语义景观内优化轨迹，而非从根本上重构景观本身。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether reasoning models, specifically those fine-tuned via Reinforcement Learning with Verifiable Rewards (RLVR), provide superior semantic representations when used as initializations for embedding models compared to their base large language model counterparts. The method involves evaluating embedding models derived from RLVR-tuned backbones on standard benchmarks (MTEB and BRIGHT) and introducing a novel Hierarchical Representation Similarity Analysis (HRSA) framework to dissect representation changes. The main experimental results reveal a null effect: RLVR-initialized models show no consistent performance gain over base models after identical contrastive training, as HRSA analysis shows RLVR primarily causes reversible local geometric changes without altering the global semantic manifold, leading to a convergence termed Manifold Realignment during subsequent training.</div>
<div class="mono" style="margin-top:8px">本文研究了通过可验证奖励的强化学习（RLVR）微调得到的推理模型，在作为嵌入模型初始化时，是否比其基础大语言模型能提供更优的语义表示。方法包括在标准基准（MTEB和BRIGHT）上评估基于RLVR调优骨干的嵌入模型，并引入一种新颖的分层表示相似性分析（HRSA）框架来剖析表示变化。主要实验结果表明存在零效应：经过相同的对比学习训练后，RLVR初始化的模型相比基础模型没有一致的性能优势，HRSA分析显示RLVR主要引起可逆的局部几何重组而非全局语义流形改变，导致在后续训练中产生称为流形重对齐的收敛现象。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Orchestrate Agents in Natural Language with the Conductor</div>
<div class="meta-line">Authors: Stefan Nielsen, Edoardo Cetin, Peter Schwendeman, Qi Sun, Jinglue Xu, Yujin Tang</div>
<div class="meta-line">First: 2025-12-04T02:23:13+00:00 · Latest: 2026-01-29T02:24:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04388v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习使用指挥者模型以自然语言编排智能体</div>
<div class="mono" style="margin-top:8px">来自不同供应商的强大大型语言模型（LLM）经过昂贵的训练和微调，已在多个领域实现专业化。本研究引入一种新型指挥者模型，通过强化学习训练，能自动发现LLM间的有效协调策略。该模型不仅学习设计针对性的通信拓扑以促进智能体间协作，还通过提示工程向LLM生成聚焦指令，以最大化发挥其个体能力。实验表明，通过学习对强大工作LLM池的最优协调策略，一个70亿参数的指挥者模型实现了超越任何单个工作模型的显著性能提升，在LiveCodeBench和GPQA等复杂推理基准测试中达到最先进水平。通过随机化智能体池训练，指挥者能灵活适配任意开源或闭源智能体组合，满足多样化用户需求。此外，允许指挥者自身作为工作智能体可形成递归拓扑结构，通过在线迭代适应的动态测试时扩展机制进一步提升性能。本研究是早期通过强化学习解锁语言模型协调能力的探索之一，证明强大的协调策略可通过纯端到端奖励最大化在LLM中自然涌现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to leverage diverse, specialized large language models (LLMs) by automating their coordination, as manually orchestrating them is inefficient. The method introduces a Conductor model trained with reinforcement learning to dynamically design communication topologies and engineer prompts, enabling effective collaboration among pools of LLM agents. Main experimental results show that a 7B Conductor outperforms individual worker LLMs, achieving state-of-the-art performance on benchmarks like LiveCodeBench and GPQA, while adapting to arbitrary agent sets and enabling recursive topologies for further gains through iterative adaptation.</div>
<div class="mono" style="margin-top:8px">该研究的动机是自动化协调多样化的专业大语言模型，以克服手动编排的低效问题。方法上，它引入了一个通过强化学习训练的指挥者模型，能动态设计通信拓扑并优化提示指令，从而促进大语言模型代理池之间的有效协作。主要实验结果表明，一个70亿参数的指挥者模型超越了单个工作模型，在LiveCodeBench和GPQA等挑战性推理基准上取得了最先进的性能，同时能适应任意代理组合，并通过递归拓扑实现在线迭代适应，进一步提升表现。</div>
</details>
</div>
<div class="card">
<div class="title">Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space</div>
<div class="meta-line">Authors: Tobias Materzok</div>
<div class="meta-line">First: 2026-01-29T02:11:43+00:00 · Latest: 2026-01-29T02:11:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21169v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21169v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>输出空间搜索：在冻结编码器定义的输出空间中定位大语言模型生成</div>
<div class="mono" style="margin-top:8px">本文提出输出空间搜索方法，将大语言模型生成转化为端点搜索问题。外层循环在冻结编码器定义的三维输出空间Z中选择目标点z*，通过序列级强化学习训练的检索增强策略生成输出，使其在标准自回归解码下的坐标落于z*附近。该方法支持在Z空间进行并行扫描和黑盒优化，无需依赖路径的标记/程序搜索。在故事生成任务中，对Z（文本）空间的扫描使大语言模型评分多样性比提示链方法提升3.1倍；在代码生成任务中，对Z（代码）空间的贝叶斯优化在匹配推理预算下能提升控制器未知的目标函数，同时保持代码有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Output-Space Search (OS-Search), motivated by the need to enhance LLM generation diversity and optimization without modifying the model&#x27;s internal parameters. The method frames generation as a search problem, where an outer loop selects a target point in a frozen encoder-defined 3D output space, and a retrieval-grounded policy, trained with sequence-level reinforcement learning, produces outputs that align with this target under standard autoregressive decoding. Experimental results show that on story generation, sweeping the output space yields 3.1 times higher LLM-scored diversity compared to prompt-chaining, and on code generation, Bayesian optimization in this space improves an objective withheld from the controller while maintaining validity under matched inference budgets.</div>
<div class="mono" style="margin-top:8px">本文提出了输出空间搜索方法，其动机是在不改变模型内部参数的情况下，提升大语言模型生成的多样性和优化能力。该方法将生成过程转化为搜索问题，通过外层循环在冻结编码器定义的三维输出空间中选择目标点，并利用基于检索的策略，结合序列级强化学习训练，在标准自回归解码下生成与该目标对齐的输出。实验结果表明，在故事生成任务中，遍历输出空间相比提示链方法获得了3.1倍的LLM评分多样性提升；在代码生成任务中，通过贝叶斯优化在该空间内搜索，能够在匹配的推理预算下改进控制器未知的目标，同时保持生成的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GEPO: Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning</div>
<div class="meta-line">Authors: Han Zhang, Ruibin Zheng, Zexuan Yi, Zhuo Zhang, Hanyang Peng, Hui Wang, Zike Yuan, Cai Ke, Shiwei Chen, Jiacheng Yang, Yangning Li, Xiang Li, Jiangyue Yan, Yaoqi Liu, Liwen Jing, Jiayin Qi, Ruifeng Xu, Binxing Fang, Yue Yu</div>
<div class="meta-line">First: 2025-08-25T09:57:35+00:00 · Latest: 2026-01-29T01:55:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.17850v9">Abs</a> · <a href="https://arxiv.org/pdf/2508.17850v9">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As single-center computing approaches power constraints, decentralized training becomes essential. However, traditional Reinforcement Learning (RL) methods, crucial for enhancing large model post-training, cannot adapt to decentralized distributed training due to the tight coupling between parameter learning and rollout sampling. For this, we propose HeteroRL, a heterogeneous RL architecture that decouples these processes, enabling stable training across geographically distributed nodes connected via the Internet. The core component is Group Expectation Policy Optimization (GEPO), an asynchronous RL algorithm robust to latency caused by network delays or heterogeneity in computational resources. Our study reveals that high latency significantly increases KL divergence, leading to higher variance of importance weights and training instability. GEPO mitigates this issue by using group expectation weighting to exponentially reduce the variance of importance weights, with theoretical guarantees. Experiments show GEPO achieves superior stability - only a 3% performance drop from online to 1800s latency-and reduces the best-to-last gap by 85% versus GSPO (1.8 vs. 12.0) while attaining the highest scores, highlighting its effectiveness in decentralized, resource-heterogeneous environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GEPO：面向稳定异构强化学习的群体期望策略优化</div>
<div class="mono" style="margin-top:8px">随着单中心计算逼近算力瓶颈，去中心化训练变得至关重要。然而，传统强化学习方法虽对提升大模型后训练性能关键，却因参数学习与轨迹采样的紧耦合而难以适应去中心化分布式训练。为此，我们提出异构强化学习架构HeteroRL，通过解耦这两个过程，实现在互联网连接的地理分布式节点间进行稳定训练。其核心组件是群体期望策略优化算法GEPO，该异步强化学习算法对网络延迟或计算资源异构性引发的时延具有强鲁棒性。研究发现，高时延会显著增加KL散度，导致重要性权重方差增大及训练失稳。GEPO通过群体期望加权法以指数级降低重要性权重方差，并具备理论保证。实验表明GEPO实现了卓越的稳定性——从在线训练到1800秒时延仅产生3%性能衰减，且相比GSPO将最优-最终性能差距缩小85%（1.8对12.0），同时获得最高评分，凸显了其在去中心化资源异构环境中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of single-center computing and the need for decentralized training, as traditional Reinforcement Learning (RL) methods struggle with the tight coupling between parameter learning and rollout sampling in distributed settings. To address this, the authors propose HeteroRL, a heterogeneous RL architecture that decouples these processes, and introduce Group Expectation Policy Optimization (GEPO) as an asynchronous algorithm robust to network latency and computational heterogeneity. The method uses group expectation weighting to exponentially reduce the variance of importance weights, mitigating training instability caused by high latency, which is shown to increase KL divergence. Experimental results demonstrate that GEPO achieves superior stability with only a 3% performance drop under 1800s latency, reduces the best-to-last performance gap by 85% compared to GSPO, and attains the highest scores in decentralized, resource-heterogeneous environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于单中心计算面临性能限制，需要转向去中心化训练，而传统强化学习方法由于参数学习与采样过程的紧密耦合，难以适应分布式环境。为此，作者提出了HeteroRL这一异构强化学习架构，以解耦这些过程，并引入了组期望策略优化（GEPO）作为异步算法，能够有效应对网络延迟和计算资源异构性。该方法通过组期望加权指数级降低重要性权重的方差，解决了高延迟导致KL散度增加和训练不稳定的问题。实验结果表明，GEPO在1800秒延迟下仅出现3%的性能下降，相比GSPO将最佳与最终性能差距降低了85%，并在去中心化、资源异构的环境中取得了最高分数，凸显了其优越的稳定性与有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</div>
<div class="meta-line">Authors: Fernando Martinez, Tao Li, Yingdong Lu, Juntao Chen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-08-10T18:36:54+00:00 · Latest: 2026-01-28T22:31:00+00:00</div>
<div class="meta-line">Comments: The Fourteenth International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07452v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07452v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Q-learning jointly learns representations and values within monolithic networks, promising beneficial co-adaptation between features and value estimates. Although this architecture has attained substantial success, the coupling between representation and value learning creates instability as representations must constantly adapt to non-stationary value targets, while value estimates depend on these shifting representations. This is compounded by high variance in bootstrapped targets, which causes bias in value estimation in off-policy methods. We introduce Stackelberg Coupled Representation and Reinforcement Learning (SCORER), a framework for value-based RL that views representation and Q-learning as two strategic agents in a hierarchical game. SCORER models the Q-function as the leader, which commits to its strategy by updating less frequently, while the perception network (encoder) acts as the follower, adapting more frequently to learn representations that minimize Bellman error variance given the leader&#x27;s committed strategy. Through this division of labor, the Q-function minimizes MSBE while perception minimizes its variance, thereby reducing bias accordingly, with asymmetric updates allowing stable co-adaptation, unlike simultaneous parameter updates in monolithic solutions. Our proposed SCORER framework leads to a bi-level optimization problem whose solution is approximated by a two-timescale algorithm that creates an asymmetric learning dynamic between the two players. Extensive experiments on DQN and its variants demonstrate that gains stem from algorithmic insight rather than model complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线表征学习与强化学习的斯塔克尔伯格耦合</div>
<div class="mono" style="margin-top:8px">深度Q学习在单一网络中联合学习表征与价值，有望实现特征与价值估计间的有益协同适应。尽管该架构已取得显著成功，但表征学习与价值学习的耦合会引发不稳定性：表征需持续适应非平稳的价值目标，而价值估计又依赖于这些动态变化的表征。这一问题因自举目标的高方差而加剧，导致离轨方法中的价值估计偏差。本文提出斯塔克尔伯格耦合表征与强化学习（SCORER），这是一个基于价值的强化学习框架，将表征与Q学习视为分层博弈中的两个策略主体。SCORER将Q函数建模为领导者——通过较低频率更新来固定策略，而感知网络（编码器）作为跟随者——以更高频率适应学习，以在领导者既定策略下最小化贝尔曼误差方差。通过这种分工，Q函数最小化均方贝尔曼误差，感知网络则最小化其方差，从而相应减少偏差；非对称更新机制实现了稳定的协同适应，有别于单一方案中的同步参数更新。我们提出的SCORER框架导出一个双层优化问题，其解通过双时间尺度算法近似实现，该算法在两者间构建非对称学习动态。在DQN及其变体上的大量实验表明，性能提升源于算法洞察而非模型复杂度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the instability in deep Q-learning caused by the tight coupling between representation learning and value estimation, where shifting representations and high-variance targets lead to biased value updates. The method introduces SCORER, a Stackelberg game framework that treats the Q-function as a leader updating less frequently and the perception network as a follower adapting more rapidly to minimize Bellman error variance, approximated via a two-timescale algorithm. Experimental results on DQN variants show that this asymmetric update dynamic reduces bias and improves stability, with gains attributed to the algorithmic design rather than increased model complexity.</div>
<div class="mono" style="margin-top:8px">本文的动机在于深度Q学习中表征学习与价值估计的紧密耦合导致的不稳定性，其中不断变化的表征和高方差目标会引发价值更新的偏差。方法上提出了SCORER框架，将其建模为Stackelberg博弈，其中Q函数作为领导者以较低频率更新，而感知网络作为跟随者以较高频率适应以最小化贝尔曼误差方差，并通过双时间尺度算法近似求解。在DQN及其变体上的大量实验表明，这种不对称更新动态降低了偏差并增强了稳定性，其性能提升源于算法洞察而非模型复杂度的增加。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed</div>
<div class="meta-line">Authors: Minjae Kwon, Josephine Lamp, Lu Feng</div>
<div class="meta-line">First: 2026-01-28T22:28:17+00:00 · Latest: 2026-01-28T22:28:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21094v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21094v1">PDF</a> · <a href="https://github.com/safe-autonomy-lab/GlucoSim">Code1</a> · <a href="https://github.com/safe-autonomy-lab/GlucoAlg">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全强化学习中分布偏移下的安全性泛化：以糖尿病管理为测试平台</div>
<div class="mono" style="margin-top:8px">安全强化学习算法通常在固定训练条件下评估。本研究以糖尿病管理为安全关键测试平台，探究训练阶段的安全保证在分布偏移下的部署中是否有效。我们在统一临床模拟器上对安全强化学习算法进行基准测试，揭示了安全性泛化差距：训练时满足约束的策略常在未见患者上违反安全要求。我们证明，利用学习到的动态模型过滤不安全动作的测试时屏蔽机制，能有效恢复不同算法和患者群体的安全性。在八种安全强化学习算法、三种糖尿病类型和三个年龄组中，屏蔽机制为PPO-Lag和CPO等强基线带来13-14%的血糖达标时间增益，同时降低临床风险指数和血糖变异性。我们的模拟器和基准平台为研究安全关键控制领域的分布偏移安全性提供了工具。代码发布于https://github.com/safe-autonomy-lab/GlucoSim与https://github.com/safe-autonomy-lab/GlucoAlg。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to ensure that safety guarantees from safe reinforcement learning (RL) training persist under real-world distribution shifts, this paper evaluates safe RL algorithms on a diabetes management simulator, treating it as a safety-critical testbed. The method involves benchmarking eight safe RL algorithms under unified clinical conditions and applying test-time shielding, which uses learned dynamics models to filter unsafe actions during deployment. The main experimental results reveal a safety generalization gap where policies safe during training frequently violate constraints on unseen patients; however, shielding effectively restores safety, achieving Time-in-Range improvements of 13–14% for strong baselines like PPO-Lag and CPO while reducing clinical risk and glucose variability across diverse patient populations.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究安全强化学习算法在训练阶段的安全保证能否在现实分布偏移下保持，以糖尿病管理这一安全关键领域为测试平台。方法上，在统一的临床模拟器中评估了八种安全强化学习算法，并采用测试时屏蔽技术，利用学习到的动力学模型在部署时过滤不安全动作。主要实验结果表明，存在安全泛化差距：训练中安全的策略在未见患者上常违反安全约束；但屏蔽技术有效恢复了安全性，在PPO-Lag和CPO等强基线算法上实现了13–14%的血糖达标时间提升，同时降低了临床风险指数和血糖变异性，适用于不同患者群体。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies</div>
<div class="meta-line">Authors: Mohammad Walid Charrwi, Zaid Hussain</div>
<div class="meta-line">First: 2026-01-28T22:25:22+00:00 · Latest: 2026-01-28T22:25:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21090v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra&#x27;s algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra&#x27;s algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy&#x27;s efficiency and Dijkstra&#x27;s optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的爱因斯坦-雅可比互连拓扑容错自适应路由</div>
<div class="mono" style="margin-top:8px">众核架构密度的持续提升对互连网络的高性能与容错性提出了更高要求。爱因斯坦-雅可比网络凭借其对称的六正则拓扑结构具备优越的拓扑特性，但在故障条件下对传统路由启发式算法构成挑战。本文评估了故障EJ环境中的三种路由范式：确定性贪婪自适应路由、理论最优的迪杰斯特拉算法，以及基于强化学习的方法。通过采用惩罚故障邻近度与奖励路径效率的多目标奖励函数，强化学习智能体成功规避了通常导致贪婪几何路由陷入死锁的集群故障。迪杰斯特拉算法凭借完整拓扑知识计算全局最优路径，确立了52-54%的理论性能上限，揭示了故障网络的真实连通极限。在九节点故障的量化分析中，贪婪路由的有效可达性与数据包投递率骤降至10%，而强化学习智能体实现了94%的有效可达性与91%的投递率，具备分布式部署潜力。吞吐量评估进一步表明，强化学习方法在所有负载下均保持90%以上的归一化吞吐量，其隐式负载均衡策略在拥塞场景下甚至优于迪杰斯特拉算法。这些成果确立了基于强化学习的自适应策略作为实用解决方案，在无需全局拓扑知识或最优算法计算开销的前提下，弥合了贪婪路由的效率与迪杰斯特拉算法的最优性之间的鸿沟，为故障易发互连网络提供了鲁棒的自愈通信能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for high-performance and fault-resilient interconnection networks in many-core architectures, this paper evaluates routing paradigms for Eisenstein-Jacobi topologies under fault conditions. The method compares deterministic greedy adaptive routing, Dijkstra&#x27;s optimal algorithm, and a reinforcement learning (RL) approach that uses a multi-objective reward function to navigate around clustered failures. Experimental results with nine faulty nodes show greedy routing degrades to 10% effective reachability, Dijkstra achieves 52-54% as the theoretical optimum, and the RL agent attains 94% reachability and 91% packet delivery, sustaining over 90% normalized throughput across loads through implicit load balancing.</div>
<div class="mono" style="margin-top:8px">本文针对众核架构对高性能、高容错互连网络的需求，研究了在故障条件下Eisenstein-Jacobi拓扑中的路由策略。方法上比较了确定性贪婪自适应路由、理论最优的Dijkstra算法以及一种采用多目标奖励函数来规避集群故障的强化学习方法。实验结果表明，在九个故障节点场景下，贪婪路由的有效可达性降至10%，Dijkstra算法达到52-54%的理论最优值，而强化学习智能体实现了94%的可达性和91%的数据包投递率，并通过隐式负载均衡在所有负载下保持了90%以上的归一化吞吐量。</div>
</details>
</div>
<div class="card">
<div class="title">OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence</div>
<div class="meta-line">Authors: Jarrod Barnes</div>
<div class="meta-line">First: 2026-01-28T22:12:54+00:00 · Latest: 2026-01-28T22:12:54+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures, 3 tables. Code: https://github.com/jbarnes850/opensec-env</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21083v1">PDF</a> · <a href="https://github.com/jbarnes850/opensec-env">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenSec：对抗性证据下事件响应智能体校准度评估</div>
<div class="mono" style="margin-top:8px">随着大语言模型能力的提升，其攻击性应用也在演进：前沿智能体现能以低于50美元的计算成本生成有效漏洞利用程序（Heelan, 2026）。防御性事件响应智能体必须同步发展，但现有基准测试将行动执行与正确执行混为一谈，掩盖了智能体处理对抗性证据时的校准缺陷。我们提出OpenSec——一个双控强化学习环境，通过在真实提示注入场景中评估事件响应智能体。与静态能力基准不同，OpenSec通过执行导向的指标（首次遏制时间、误报半径、注入违规率）量化对抗性证据下改变世界状态的遏制行动。在40个标准级场景中评估四个前沿模型发现：GPT-5.2、Gemini 3和DeepSeek在100%场景中执行遏制，但伴随90-97%的误报率；Claude Sonnet 4.5展现出部分校准能力（85%遏制率，72%误报率）。这表明OpenSec能揭示被整体成功率掩盖的校准失效模式。代码已开源：https://github.com/jbarnes850/opensec-env。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the increasing offensive use of large language models to generate exploits, highlighting the need for defensive incident response (IR) agents to keep pace, but existing benchmarks fail to reveal calibration issues when agents handle adversarial evidence. The method introduces OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios using execution-based metrics like time-to-first-containment, blast radius, and injection violation rates. Main experimental results from evaluating four frontier models on 40 episodes show consistent over-triggering, with GPT-5.2, Gemini 3, and DeepSeek executing containment in all episodes but with 90-97% false positive rates, while Claude Sonnet 4.5 demonstrates partial calibration, indicating that OpenSec exposes a calibration failure mode missed by aggregate metrics.</div>
<div class="mono" style="margin-top:8px">本文的动机是大型语言模型在攻击性应用中日益增多，能够低成本生成漏洞利用，因此需要防御性事件响应（IR）智能体跟上步伐，但现有基准测试在智能体处理对抗性证据时未能揭示校准问题。方法上提出了OpenSec，这是一个双控制强化学习环境，通过基于执行的指标（如首次遏制时间、爆炸半径和注入违规率）来评估IR智能体在真实提示注入场景下的表现。主要实验结果对四个前沿模型在40个标准级场景中的评估显示，存在一致的过度触发问题：GPT-5.2、Gemini 3和DeepSeek在所有场景中都执行了遏制，但假阳性率高达90-97%，而Claude Sonnet 4.5表现出部分校准能力，这表明OpenSec能够揭示被聚合成功指标掩盖的校准失败模式。</div>
</details>
</div>
<div class="card">
<div class="title">Human-LLM Collaborative Feature Engineering for Tabular Data</div>
<div class="meta-line">Authors: Zhuoyan Li, Aditya Bansal, Jinzhao Li, Shishuang He, Zhuoran Lu, Mutian Zhang, Qin Liu, Yiwei Yang, Swati Jain, Ming Yin, Yunyao Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T21:33:42+00:00 · Latest: 2026-01-28T21:33:42+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21060v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users&#x27; cognitive load during the feature engineering process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向表格数据的人机协同特征工程</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在表格学习中正被日益用于自动化特征工程。给定任务特定信息后，LLMs能够提出多样化的特征转换操作以提升下游模型性能。然而，现有方法通常将LLM视为黑盒优化器，由其同时负责基于内部启发式规则提出和选择操作，这往往缺乏对操作效用的校准估计，导致在缺乏优先探索高潜力方向的原则性策略时，反复尝试低效操作。本文提出一种面向表格学习的人机协同特征工程框架。我们首先解耦转换操作的提出与选择过程：LLM仅用于生成操作候选，而选择过程则通过显式建模每个提议操作的效用和不确定性来指导。由于准确的效用估计在特征工程早期阶段尤为困难，我们在框架中设计了一种机制，选择性地收集并整合人类专家偏好反馈——通过比较哪些操作更具潜力——以辅助识别更有效的操作。在合成实验和真实用户研究中的评估表明，该框架在多种表格数据集上提升了特征工程性能，并降低了用户在特征工程过程中的认知负荷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of using large language models (LLMs) as black-box optimizers for feature engineering in tabular learning, which often leads to inefficient exploration due to uncalibrated utility estimates, this paper introduces a human-LLM collaborative framework that decouples operation proposal and selection. The method employs LLMs to generate feature transformation candidates while explicitly modeling each operation&#x27;s utility and uncertainty for selection, and it incorporates selective human expert feedback on operation comparisons to guide the process. Experimental results from synthetic and real user studies show that this framework enhances feature engineering performance across diverse tabular datasets and reduces users&#x27; cognitive load.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决大型语言模型（LLM）作为黑盒优化器在表格数据特征工程中的局限性，即其内部启发式方法缺乏对操作效用的校准估计，常导致低效探索。为此，论文提出了一种人机协作的特征工程框架，将操作提议与选择过程解耦：LLM仅用于生成候选操作，而选择过程则通过显式建模每个操作的效用和不确定性来指导，并选择性融入人类专家对操作优先级的偏好反馈。在合成实验和真实用户研究中的评估表明，该框架提升了多种表格数据集上的特征工程性能，并降低了用户在过程中的认知负担。</div>
</details>
</div>
<div class="card">
<div class="title">Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report</div>
<div class="meta-line">Authors: Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam, Paul Kassianik, Sajana Weerawardhena, Anu Vellore, Blaine Nelson, Neusha Javidnia, Arthur Goldblatt, Fraser Burch, Avi Zohary, Assaf Eisenman, Mahdi Sabbaghi, Supriti Vijay, Rahim Dharssi, Dhruv Kedia, Kojin Oshiba, Yaron Singer, Amin Karbasi</div>
<div class="meta-line">First: 2026-01-28T21:15:24+00:00 · Latest: 2026-01-28T21:15:24+00:00</div>
<div class="meta-line">Comments: 31 pages, 5 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21051v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21051v1">PDF</a> · <a href="https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B技术报告</div>
<div class="mono" style="margin-top:8px">我们推出Foundation-Sec-8B-Reasoning，这是首个面向网络安全领域的开源原生推理模型。该模型基于我们先前发布的Foundation-Sec-8B基础模型（源自Llama-3.1-8B-Base），通过监督微调（SFT）与可验证奖励强化学习（RLVR）两阶段训练流程构建。训练采用涵盖网络安全分析、指令跟随和数学推理的专有推理数据集。在10项网络安全基准测试和10项通用基准测试中的评估表明，该模型在网络安全任务上可与规模显著更大的模型竞争，同时保持强大的通用能力。模型在多跳推理任务中展现出有效泛化能力，在部署适当系统提示与防护机制时具有优异的安全性能。本研究表明，领域专业化推理模型可在保持广泛通用能力的同时，在专业任务上实现强劲性能。模型已通过https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning公开释放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation was to create the first open-source native reasoning model specifically for cybersecurity, addressing the need for specialized AI in this domain. The method involved a two-stage training process on the Llama-3.1-8B-Base-derived Foundation-Sec-8B model, combining supervised fine-tuning and reinforcement learning from verifiable rewards using proprietary cybersecurity reasoning data. Main experimental results showed that the model achieved competitive performance on 10 cybersecurity benchmarks against larger models, maintained strong general capabilities across 10 general-purpose benchmarks, demonstrated effective generalization in multi-hop reasoning, and exhibited strong safety with appropriate prompts and guardrails.</div>
<div class="mono" style="margin-top:8px">该研究的动机是创建首个专为网络安全设计的开源原生推理模型，以应对该领域对专业化人工智能的需求。方法基于Llama-3.1-8B-Base衍生的Foundation-Sec-8B基础模型，采用两阶段训练流程，结合监督微调和基于可验证奖励的强化学习，并利用专有的网络安全推理数据进行训练。主要实验结果表明，该模型在10个网络安全基准测试中与更大模型相比具有竞争力，在10个通用基准测试中保持了强大的通用能力，在多跳推理任务中展现出有效的泛化性能，并在配合适当系统提示和防护措施时表现出良好的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Log2Motion: Biomechanical Motion Synthesis from Touch Logs</div>
<div class="meta-line">Authors: Michał Patryk Miazga, Hannah Bussmann, Antti Oulasvirta, Patrick Ebel</div>
<div class="meta-line">First: 2026-01-28T21:04:19+00:00 · Latest: 2026-01-28T21:04:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21043v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Log2Motion：基于触摸日志的生物力学运动合成</div>
<div class="mono" style="margin-top:8px">移动设备的大规模触摸数据难以揭示其背后的交互过程。尽管生物力学仿真能阐明运动控制机制，但尚未应用于触摸交互研究。为填补这一空白，我们提出一项新颖的计算任务：直接从日志合成合理运动。核心创新在于采用强化学习驱动的肌肉骨骼前向仿真，生成与触摸日志事件一致的生物力学合理运动序列。通过将软件模拟器集成至物理仿真环境，实现了生物力学模型对真实应用的实时操控。Log2Motion能够从触摸日志合成丰富的用户运动数据，包括动作、速度、精度及用力程度的估计。我们通过运动捕捉实验数据与既有研究对比，验证生成运动的合理性，并在大规模数据集中展示了该方法的有效性。生物力学运动合成为理解日志数据提供了新途径，揭示了触摸交互背后的人体工学与运动控制机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the gap between abundant touch log data from mobile devices and the lack of understanding of the underlying biomechanical interactions that produce them. The authors propose Log2Motion, a method that synthesizes plausible human motion from touch logs by integrating a reinforcement learning-driven musculoskeletal forward simulation with a software emulator in a physics simulator, enabling biomechanical models to interact with real applications in real-time. Experimental validation against human motion capture data and prior findings demonstrates that Log2Motion generates biomechanically plausible estimates of motion, speed, accuracy, and effort, offering a new approach to illuminate ergonomics and motor control in touch interactions.</div>
<div class="mono" style="margin-top:8px">本文针对移动设备中大量触摸日志数据与对其背后生物力学交互理解不足之间的差距，提出了一种新方法。研究者开发了Log2Motion，该方法通过将强化学习驱动的肌肉骨骼前向模拟与物理模拟器中的软件模拟器集成，使生物力学模型能够实时操作真实应用，从而从触摸日志合成合理的人体运动。实验通过与人体动作捕捉数据和先前研究结果对比验证表明，Log2Motion能够生成生物力学上合理的运动、速度、准确性和努力程度估计，为理解触摸交互中的人体工学和运动控制提供了新途径。</div>
</details>
</div>
<div class="card">
<div class="title">SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model</div>
<div class="meta-line">Authors: Zongheng Guo, Tao Chen, Yang Jiao, Yi Pan, Xiao Hu, Manuela Ferrario</div>
<div class="meta-line">First: 2026-01-28T20:46:50+00:00 · Latest: 2026-01-28T20:46:50+00:00</div>
<div class="meta-line">Comments: 31 pages, 9 figures, 14 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21031v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21031v1">PDF</a> · <a href="https://github.com/ZonghengGuo/SigmaPPG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIGMA-PPG：面向PPG基础模型的统计先验引导生成掩码架构</div>
<div class="mono" style="margin-top:8px">当前光电容积描记（PPG）信号基础模型受限于信号固有的冗余性与噪声。标准掩码建模常产生平凡解，而对比方法缺乏形态学精度。为应对这些局限，我们提出统计先验引导生成掩码架构（SIGMA-PPG），这是一种具备先验引导对抗掩码机制的生成式基础模型。该模型通过强化学习驱动的教师网络，利用统计先验构建具有挑战性的学习路径，防止对噪声的过拟合。同时，我们通过向量量化引入语义一致性约束，确保生理特征相同的波形（即使受记录伪影或微小扰动影响）映射至共享索引。此举提升了码本语义密度并消除了冗余特征结构。基于超12万小时数据预训练的SIGMA-PPG，在12项多样化下游任务中，相比五种前沿基线模型均展现出更优的平均性能。代码发布于https://github.com/ZonghengGuo/SigmaPPG。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of intrinsic redundancy and noise in photoplethysmography (PPG) signals, which cause standard masked modeling to yield trivial solutions and contrastive methods to lack morphological precision, this paper proposes SIGMA-PPG, a generative foundation model. The method introduces a Prior-Guided Adversarial Masking mechanism where a reinforcement learning-driven teacher uses statistical priors to create challenging learning paths, preventing overfitting to noise, and incorporates a semantic consistency constraint via vector quantization to map physiologically identical waveforms to shared indices, enhancing codebook semantic density. Pre-trained on over 120,000 hours of data, the main experimental results show that SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks.</div>
<div class="mono" style="margin-top:8px">针对光电容积脉搏波（PPG）信号固有的冗余和噪声问题，其中标准掩码建模易产生平凡解而对比方法缺乏形态学精度，本文提出了SIGMA-PPG这一生成式基础模型。该方法采用先验引导的对抗性掩码机制，通过强化学习驱动的教师模型利用统计先验创建具有挑战性的学习路径以防止噪声过拟合，并结合基于矢量量化的语义一致性约束，将生理学上相同的波形映射到共享索引以增强码本语义密度。在超过12万小时数据上预训练后，主要实验结果表明，SIGMA-PPG在12个不同的下游任务中相比五种先进基线模型取得了更优的平均性能。</div>
</details>
</div>
<div class="card">
<div class="title">Distributional Active Inference</div>
<div class="meta-line">Authors: Abdullah Akgül, Gulcin Baykal, Manuel Haußmann, Mustafa Mert Çelikok, Melih Kandemir</div>
<div class="meta-line">First: 2026-01-28T19:36:33+00:00 · Latest: 2026-01-28T19:36:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20985v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20985v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分布式主动推理</div>
<div class="mono" style="margin-top:8px">机器人系统对复杂环境的最优控制面临两个互补且交织的挑战：感官状态信息的高效组织与长远行动规划。由于强化学习框架仅针对后者，其解决方案往往存在样本效率低下的问题。主动推理作为解释生物大脑如何处理这一双重问题的最先进过程理论，目前在人工智能领域的应用仍局限于现有基于模型方法的扩展。本文提出一种形式化的强化学习算法抽象框架，涵盖基于模型、分布式及无模型方法。该框架将主动推理无缝整合至分布式强化学习架构中，无需建立状态转移动力学模型即可实现其性能优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the dual challenges of efficient sensory information organization and long-term planning in robotic control, noting that reinforcement learning alone is sample-inefficient as it focuses only on planning. The authors propose integrating active inference—a biologically inspired theory for handling both challenges—into distributional reinforcement learning through a formal abstraction that spans model-based, distributional, and model-free methods. This approach enables performance improvements without requiring explicit transition dynamics modeling, making active inference&#x27;s advantages accessible within a modern reinforcement learning framework.</div>
<div class="mono" style="margin-top:8px">本文针对机器人控制中高效组织感官信息与长远行动规划的双重挑战，指出仅关注规划的强化学习方法样本效率低下。作者提出通过一个涵盖基于模型、分布式和免模型方法的正式抽象，将生物启发的主动推理理论整合到分布式强化学习框架中。该方法无需显式建模状态转移动力学，即可利用主动推理的优势，从而在现代强化学习中实现性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes</div>
<div class="meta-line">Authors: Ruijia Zhang, Xiangyu Zhang, Zhengling Qi, Yue Wu, Yanxun Xu</div>
<div class="meta-line">First: 2025-06-25T13:22:57+00:00 · Latest: 2026-01-28T19:23:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.20406v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.20406v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic treatment regimes (DTRs) provide a principled framework for optimizing sequential decision-making in domains where decisions must adapt over time in response to individual trajectories, such as healthcare, education, and digital interventions. However, existing statistical methods often rely on strong positivity assumptions and lack robustness under partial data coverage, while offline reinforcement learning approaches typically focus on average training performance, lack statistical guarantees, and require solving complex optimization problems. To address these challenges, we propose POLAR, a novel pessimistic model-based policy learning algorithm for offline DTR optimization. POLAR estimates the transition dynamics from offline data and quantifies uncertainty for each history-action pair. A pessimistic penalty is then incorporated into the reward function to discourage actions with high uncertainty. Unlike many existing methods that focus on average training performance or provide guarantees only for an oracle policy, POLAR directly targets the suboptimality of the final learned policy and offers theoretical guarantees, without relying on computationally intensive minimax or constrained optimization procedures. To the best of our knowledge, POLAR is the first model-based DTR method to provide both statistical and computational guarantees, including finite-sample bounds on policy suboptimality. Empirical results on both synthetic data and the MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods and yields near-optimal, history-aware treatment strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POLAR：一种用于动态治疗方案的悲观模型策略学习算法</div>
<div class="mono" style="margin-top:8px">动态治疗方案（DTRs）为医疗、教育和数字干预等需随时间适应个体轨迹的序列决策优化提供了原则性框架。然而，现有统计方法常依赖强正性假设，在部分数据覆盖下缺乏鲁棒性；而离线强化学习方法通常关注平均训练性能，缺乏统计保证，且需解决复杂优化问题。为应对这些挑战，我们提出POLAR，一种用于离线DTR优化的新型悲观模型策略学习算法。POLAR从离线数据估计状态转移动态，并量化每个历史-行动对的不确定性，随后将悲观惩罚项纳入奖励函数以抑制高不确定性行动。与许多仅关注平均训练性能或仅对理想策略提供保证的现有方法不同，POLAR直接针对最终学习策略的次优性提供理论保证，且无需依赖计算密集的极小极大或约束优化过程。据我们所知，POLAR是首个同时提供统计与计算保证的模型DTR方法，包括策略次优性的有限样本界。在合成数据和MIMIC-III数据集上的实证结果表明，POLAR优于现有先进方法，并能生成接近最优的、历史感知的治疗策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces POLAR, a pessimistic model-based policy learning algorithm designed to address the limitations of existing methods for optimizing dynamic treatment regimes (DTRs), which adapt decisions over time in fields like healthcare. Existing approaches often rely on strong assumptions, lack robustness with partial data, or focus on average performance without guarantees. POLAR estimates transition dynamics from offline data, quantifies uncertainty for each history-action pair, and incorporates a pessimistic penalty into the reward to discourage uncertain actions, thereby directly targeting the suboptimality of the learned policy with theoretical guarantees and avoiding complex optimization. Experimental results on synthetic data and the MIMIC-III dataset show that POLAR outperforms state-of-the-art methods, producing near-optimal, history-aware treatment strategies.</div>
<div class="mono" style="margin-top:8px">本文提出了POLAR，一种基于悲观模型的策略学习算法，旨在解决动态治疗策略优化中现有方法的局限性，这些策略在医疗等领域需随时间调整决策。现有方法通常依赖强假设、在部分数据下缺乏鲁棒性，或仅关注平均性能而无理论保证。POLAR从离线数据估计转移动态，量化每个历史-行动对的不确定性，并在奖励中引入悲观惩罚以抑制不确定行动，从而直接针对学习策略的次优性提供理论保证，避免了复杂的优化过程。在合成数据和MIMIC-III数据集上的实验结果表明，POLAR优于现有先进方法，能生成接近最优且考虑历史背景的治疗策略。</div>
</details>
</div>
<div class="card">
<div class="title">LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</div>
<div class="meta-line">Authors: Piyush Jha, Arnav Arora, Vijay Ganesh</div>
<div class="meta-line">Venue: AAAI 2025</div>
<div class="meta-line">First: 2024-11-13T18:44:30+00:00 · Latest: 2026-01-28T18:58:57+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.08862v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.08862v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLMStinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks. Unlike traditional methods, which require complex prompt engineering or white-box access, LLMStinger uses a reinforcement learning (RL) loop to fine-tune an attacker LLM, generating new suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Our method significantly outperforms existing red-teaming approaches (we compared against 15 of the latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for their extensive safety measures. Additionally, we achieved a 94.97% ASR on GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability of LLMStinger across open and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLMStinger：利用强化学习微调的大语言模型实现越狱攻击</div>
<div class="mono" style="margin-top:8px">本文提出LLMStinger，一种利用大语言模型自动生成对抗性后缀进行越狱攻击的新方法。与传统需要复杂提示工程或白盒访问的方法不同，LLMStinger采用强化学习循环微调攻击者大语言模型，基于HarmBench基准中的恶意问题及现有攻击生成新后缀。该方法显著优于现有红队测试方法（我们对比了15种最新方法），在具有严格安全措施的LLaMA2-7B-chat模型上攻击成功率提升57.2%，在Claude 2上提升50.3%。此外，在GPT-3.5上达到94.97%攻击成功率，在Gemma-2B-it上达到99.4%，证明了LLMStinger在开源与闭源模型间的鲁棒性与适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to efficiently test the safety of large language models (LLMs) against jailbreak attacks without relying on complex manual engineering or white-box access, this paper introduces LLMStinger, a method that uses reinforcement learning to fine-tune an attacker LLM to automatically generate adversarial suffixes. The approach iteratively refines suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Experimental results demonstrate that LLMStinger significantly outperforms 15 existing red-teaming methods, achieving improvements in Attack Success Rate (ASR) of +57.2% on LLaMA2-7B-chat and +50.3% on Claude 2, while also reaching high ASRs of 94.97% on GPT-3.5 and 99.4% on Gemma-2B-it, showcasing its robustness across both open and closed-source models.</div>
<div class="mono" style="margin-top:8px">本文的动机是需要在无需复杂手动工程或白盒访问的情况下，高效测试大语言模型（LLM）抵御越狱攻击的安全性，因此提出了LLMStinger方法，该方法利用强化学习微调一个攻击者LLM来自动生成对抗性后缀。该方法基于HarmBench基准中的有害问题，利用现有攻击迭代优化后缀。实验结果表明，LLMStinger显著优于15种现有的红队方法，在LLaMA2-7B-chat和Claude 2上的攻击成功率（ASR）分别提升了57.2%和50.3%，同时在GPT-3.5和Gemma-2B-it上分别达到了94.97%和99.4%的ASR，证明了其在开源和闭源模型上的鲁棒性与适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning</div>
<div class="meta-line">Authors: Minwu Kim, Safal Shrestha, Keith Ross</div>
<div class="meta-line">First: 2026-01-28T18:29:21+00:00 · Latest: 2026-01-28T18:29:21+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20829v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20829v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model&#x27;s robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于失败前缀条件化的饱和问题推理模型训练</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）显著提升了大型语言模型（LLM）的推理能力，但随着问题趋于饱和，训练常陷入停滞。我们发现核心挑战在于信息性失败的可及性差：学习信号存在，但在标准推演中极少出现。为此，我们提出失败前缀条件化——一种从饱和问题中学习的简洁有效方法。该方法不再从原始问题出发，而是通过将训练条件化于罕见错误推理轨迹衍生的前缀，重新分配探索范围，使模型暴露于易失败状态。实验表明，失败前缀条件化带来的性能提升与中等难度问题训练相当，同时保持标记效率。进一步分析模型鲁棒性发现，该方法能降低误导性失败前缀下的性能衰减，仅需以轻微牺牲早期正确推理遵循度为代价。最后，我们证明在训练中迭代更新失败前缀的策略，能在性能平台期后实现额外增益。总体而言，本研究结果表明失败前缀条件化为延续RLVR在饱和问题上的训练提供了有效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the training stagnation of large language models in Reinforcement Learning with Verifiable Rewards (RLVR) when problems become saturated, where informative failure signals are rarely encountered. The proposed method, failure-prefix conditioning, addresses this by reallocating exploration to condition training on prefixes from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. Experimental results show that this approach yields performance gains comparable to training on medium-difficulty problems while maintaining token efficiency, enhances robustness by reducing performance degradation under misleading prefixes, and an iterative variant unlocks further gains after plateaus, offering an effective pathway to extend RLVR training.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型在可验证奖励的强化学习中，当问题趋于饱和时训练停滞的难题，即信息丰富的失败信号难以获取。所提出的方法称为失败前缀条件化，通过将训练探索重新分配到基于罕见错误推理轨迹的前缀上，使模型暴露于易失败状态。实验结果表明，该方法在保持令牌效率的同时，取得了与中等难度问题训练相当的性能提升，并通过减少在误导性前缀下的性能退化增强了鲁棒性，其迭代变体还能在性能平台期后实现额外增益，为扩展饱和问题的RLVR训练提供了有效途径。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning via Self-Distillation</div>
<div class="meta-line">Authors: Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</div>
<div class="meta-line">First: 2026-01-28T17:45:12+00:00 · Latest: 2026-01-28T17:45:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20802v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#x27;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自蒸馏的强化学习</div>
<div class="mono" style="margin-top:8px">大型语言模型在代码和数学等可验证领域越来越多地通过强化学习进行后训练。然而，当前基于可验证奖励的强化学习方法仅从每次尝试的标量结果奖励中学习，形成了严重的信用分配瓶颈。许多可验证环境实际提供丰富的文本反馈（如运行时错误或评判评估），用以解释尝试失败的原因。我们将此设定形式化为具有丰富反馈的强化学习，并提出自蒸馏策略优化方法，该方法将标记化反馈转化为密集学习信号，无需外部教师或显式奖励模型。SDPO将基于反馈调节的当前模型视为自教师，并将其反馈感知的下一标记预测蒸馏回策略中。通过这种方式，SDPO利用了模型在上下文中回溯识别自身错误的能力。在科学推理、工具使用以及LiveCodeBench v6的竞技编程任务中，SDPO相比强RLVR基线显著提升了样本效率和最终准确率。值得注意的是，在仅返回标量反馈的标准RLVR环境中，SDPO通过将成功轨迹作为失败尝试的隐式反馈，同样超越了基线方法。最后，在测试阶段对单个问题应用SDPO可加速困难二元奖励任务的探索过程，以仅需三分之一的尝试次数即可达到与k最佳采样或多轮对话相同的发现概率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the credit-assignment bottleneck in reinforcement learning with verifiable rewards (RLVR), where models only receive scalar outcome rewards, by proposing a method to leverage rich textual feedback often available in verifiable domains like code and math. It introduces Self-Distillation Policy Optimization (SDPO), which uses the model itself as a teacher to distill feedback-informed next-token predictions back into the policy, enabling retrospective learning from mistakes without external reward models. Experimental results across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6 show that SDPO improves sample efficiency and final accuracy over RLVR baselines, even outperforming them in scalar-feedback settings by using successful rollouts as implicit feedback, and accelerates discovery on difficult tasks with fewer attempts compared to methods like best-of-k sampling.</div>
<div class="mono" style="margin-top:8px">本文针对可验证奖励强化学习中仅依赖标量结果奖励导致的信用分配瓶颈问题，提出利用代码、数学等可验证领域常提供的丰富文本反馈（如运行时错误）来改进学习。方法上引入了自蒸馏策略优化（SDPO），将当前模型在反馈条件下的预测作为自我教师，把反馈信息下的下一个令牌预测蒸馏回策略中，从而无需外部奖励模型即可实现从错误中回顾学习。在科学推理、工具使用和LiveCodeBench v6的竞技编程实验中，SDPO相比强基线在样本效率和最终准确率上均有提升，即使在仅返回标量反馈的标准环境中，通过使用成功轨迹作为隐式反馈也优于基线，并且在困难二元奖励任务上以更少尝试次数加速了发现过程。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-01-28T17:27:08+00:00</div>
<div class="meta-line">Comments: 21 main pages, 7 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效代码库智能体</div>
<div class="mono" style="margin-top:8px">开源权重编码智能体本应具备超越闭源系统的根本优势：可针对私有代码库进行专业化训练，将仓库特定信息直接编码至权重中。然而训练成本与复杂性使这一优势长期停留于理论层面。本研究证明该目标现已具备可行性。我们提出软验证高效代码库智能体（SERA）——一种高效的编码智能体训练方法，能够快速低成本创建专用于私有代码库的智能体。仅通过监督微调（SFT），SERA即在完全开源（开放数据、方法、代码）模型中取得最先进成果，同时达到如Devstral-Small-2等前沿开源权重模型的性能水平。创建SERA模型的成本比强化学习降低26倍，比先前合成数据方法降低57倍即可达到同等性能。我们的软验证生成（SVG）方法能从单个代码库生成数千条轨迹，结合成本效益优势，实现了对私有代码库的专业化适配。除仓库专业化外，我们将SVG应用于更大规模的代码库语料，生成超过20万条合成轨迹，并基于此数据集对编码智能体训练的缩放规律、消融实验及混杂因素进行详细分析。总体而言，我们相信这项工作将极大加速开源编码智能体的研究进程，并彰显可适配私有代码库的开源模型优势。我们将SERA作为Ai2开源编码智能体系列的首个模型发布，同时开放全部代码、数据及Claude Code集成方案以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SERA is to make it practical for open-weight coding agents to specialize on private codebases, an advantage that has been theoretical due to high training costs. The method introduces Soft-Verified Efficient Repository Agents (SERA), which uses an efficient approach called Soft Verified Generation (SVG) to generate thousands of training trajectories from a single repository and applies only supervised finetuning (SFT). The main experimental results show that SERA achieves state-of-the-art performance among fully open-source models, matches the performance of frontier open-weight models like Devstral-Small-2, and is created at a cost 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods for equivalent performance, enabling effective specialization to private repositories.</div>
<div class="mono" style="margin-top:8px">SERA 的研究动机是使开源权重编码智能体能够实际应用于私有代码库的专业化，这一优势此前因训练成本高昂而仅停留在理论层面。该方法提出了软验证高效仓库智能体（SERA），采用一种名为软验证生成（SVG）的高效技术，从单个代码库生成数千条训练轨迹，并仅使用监督微调（SFT）。主要实验结果表明，SERA 在完全开源的模型中取得了最先进的性能，匹配了如 Devstral-Small-2 等前沿开源权重模型的水平，且其创建成本比强化学习方法低 26 倍，比先前的合成数据方法低 57 倍即可达到同等性能，从而实现了对私有代码库的有效专业化。</div>
</details>
</div>
<div class="card">
<div class="title">Less is More: Clustered Cross-Covariance Control for Offline RL</div>
<div class="meta-line">Authors: Nan Qiao, Sheng Yue, Shuning Wang, Yongheng Deng, Ju Ren</div>
<div class="meta-line">First: 2026-01-28T16:55:04+00:00 · Latest: 2026-01-28T16:55:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20765v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20765v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少即是多：面向离线强化学习的聚类交叉协方差控制</div>
<div class="mono" style="margin-top:8px">离线强化学习的核心挑战在于分布偏移。数据稀缺或由分布外区域主导的数据集会加剧这一问题。我们的理论分析与实验表明，标准平方误差目标会产生有害的时序差分交叉协方差。该效应在分布外区域被放大，导致优化偏差并损害策略学习。为抵消此机制，我们提出两种互补策略：一是分区缓冲采样，将更新限制在局部回放分区内，以减弱异常协方差效应并校准更新方向，形成易于集成至现有框架的方案——时序差分聚类交叉协方差控制；二是引入显式的基于梯度的校正惩罚项，在每次更新中消除协方差引起的偏差。我们证明缓冲分区能保持最大化目标的下界性质，且这些约束可在不改变策略约束型离线强化学习核心行为的前提下，缓解极端分布外区域的过度保守性。实验表明，本方法较现有方案具有更高稳定性，且在回报率上最高提升30%，尤其在小规模数据集及强调分布外区域的数据划分中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of distributional shift in offline reinforcement learning, particularly exacerbated by scarce data or datasets dominated by out-of-distribution areas. The authors theoretically and empirically identify that the standard squared error objective induces a harmful TD cross-covariance, which biases optimization and degrades policy learning, especially in OOD regions. To counteract this, they propose two strategies: partitioned buffer sampling, which localizes updates to attenuate irregular covariance effects and align update directions, forming the Clustered Cross-Covariance Control for TD method, and an explicit gradient-based corrective penalty to cancel covariance-induced bias. They prove that buffer partitioning preserves the lower bound property of the maximization objective and mitigates excessive conservatism in extreme OOD areas without altering core policy-constrained offline RL behavior. Experimental results demonstrate higher stability and up to 30% improvement in returns over prior methods, particularly with small datasets and OOD-emphasized splits.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中的分布偏移问题，特别是在数据稀缺或数据集由分布外区域主导时更为突出。作者通过理论分析和实验发现，标准的平方误差目标会导致有害的TD交叉协方差，从而在优化中引入偏差并降低策略学习效果，尤其在分布外区域更为明显。为应对此机制，他们提出了两种互补策略：分区缓冲区采样，通过将更新限制在局部回放分区中来减弱不规则协方差效应并对齐更新方向，形成了聚类交叉协方差控制TD方法；以及一种显式的基于梯度的校正惩罚，以消除每次更新中的协方差诱导偏差。他们证明了缓冲区分区保留了最大化目标的下界性质，且这些约束能在不改变核心策略约束离线强化学习行为的情况下，缓解极端分布外区域的过度保守性。实验结果表明，该方法展现出更高的稳定性，并在回报上比先前方法提升高达30%，特别是在小数据集和强调分布外区域的数据划分中。</div>
</details>
</div>
<div class="card">
<div class="title">Analysis of approximate linear programming solution to Markov decision problem with log barrier function</div>
<div class="meta-line">Authors: Donghwan Lee, Hyukjun Yang, Bum Geun Park</div>
<div class="meta-line">First: 2025-09-24T06:36:11+00:00 · Latest: 2026-01-28T16:42:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19800v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19800v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There are two primary approaches to solving Markov decision problems (MDPs): dynamic programming based on the Bellman equation and linear programming (LP). Dynamic programming methods are the most widely used and form the foundation of both classical and modern reinforcement learning (RL). By contrast, LP-based methods have been less commonly employed, although they have recently gained attention in contexts such as offline RL. The relative underuse of the LP-based methods stems from the fact that it leads to an inequality-constrained optimization problem, which is generally more challenging to solve effectively compared with Bellman-equation-based methods. The purpose of this paper is to establish a theoretical foundation for solving LP-based MDPs in a more effective and practical manner. Our key idea is to leverage the log-barrier function, widely used in inequality-constrained optimization, to transform the LP formulation of the MDP into an unconstrained optimization problem. This reformulation enables approximate solutions to be obtained easily via gradient descent. While the method may appear simple, to the best of our knowledge, a thorough theoretical interpretation of this approach has not yet been developed. This paper aims to bridge this gap.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于对数障碍函数的马尔可夫决策问题近似线性规划解分析</div>
<div class="mono" style="margin-top:8px">求解马尔可夫决策问题（MDP）主要有两种方法：基于贝尔曼方程的动态规划和线性规划（LP）。动态规划方法应用最广泛，是经典及现代强化学习（RL）的基础。相比之下，基于LP的方法较少使用，尽管近期在离线RL等场景中受到关注。这类方法相对未被充分利用，是因为其会转化为不等式约束优化问题，通常比基于贝尔曼方程的方法更难高效求解。本文旨在为更高效、实用地求解基于LP的MDP建立理论基础。核心思路是利用不等式约束优化中广泛使用的对数障碍函数，将MDP的LP形式转化为无约束优化问题，从而可通过梯度下降轻松获得近似解。尽管方法看似简单，但据我们所知，目前尚未有对此方法的完整理论阐释。本文旨在填补这一空白。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underutilization of linear programming (LP) methods for solving Markov decision problems (MDPs), which are typically tackled via dynamic programming. The authors propose transforming the inequality-constrained LP formulation into an unconstrained optimization problem by incorporating a log-barrier function, thereby enabling efficient approximate solutions through gradient descent. The main contribution is establishing a theoretical foundation for this approach, which facilitates more practical and effective LP-based solutions for MDPs, particularly relevant in areas like offline reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本文针对马尔可夫决策问题中线性规划方法使用不足的现状，提出了一种改进方案。研究者利用对数障碍函数将不等式约束的线性规划问题转化为无约束优化问题，从而可以通过梯度下降法高效地获得近似解。该工作的主要贡献是为这一方法建立了理论基础，使得基于线性规划的马尔可夫决策问题求解更加实用有效，尤其在离线强化学习等场景中具有应用价值。</div>
</details>
</div>
<div class="card">
<div class="title">GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning</div>
<div class="meta-line">Authors: Zhiheng Jiang, Yunzhe Wang, Ryan Marr, Ellen Novoseller, Benjamin T. Files, Volkan Ustun</div>
<div class="meta-line">First: 2026-01-28T16:36:37+00:00 · Latest: 2026-01-28T16:36:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20753v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20753v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphAllocBench：面向偏好条件多目标策略学习的灵活基准</div>
<div class="mono" style="margin-top:8px">多目标强化学习中的偏好条件策略学习旨在通过将策略与用户对目标的指定偏好条件相结合，近似获取多样化的帕累托最优解。这使得单一模型能够在运行时通过生成位于或接近帕累托前沿的策略，灵活适应任意权衡。然而，现有PCPL基准大多局限于简单任务和固定环境，限制了其实用性与可扩展性。为填补这一空白，我们提出了GraphAllocBench——一个基于新型图结构资源分配沙盒环境构建的灵活基准，该环境受城市管理启发而命名为CityPlannerEnv。GraphAllocBench提供涵盖多样化目标函数、可变偏好条件及高维可扩展性的丰富问题集。我们还提出了两个新评估指标——非支配解比例与排序分数，在补充广泛使用的超体积指标的同时，直接捕捉偏好一致性。通过多层感知机与图感知模型的实验，我们证明GraphAllocBench能够揭示现有MORL方法的局限性，并为在复杂高维组合分配任务中应用图神经网络等图学习方法开辟道路。除预设问题集外，GraphAllocBench允许用户灵活调整目标、偏好与分配规则，使其成为推动PCPL发展的通用可扩展基准。代码：https://anonymous.4open.science/r/GraphAllocBench</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GraphAllocBench, a benchmark for Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL), motivated by the lack of realistic and scalable benchmarks beyond toy tasks. The method centers on a graph-based resource allocation sandbox environment called CityPlannerEnv, which offers flexible problem variations in objectives, preferences, and scalability. Experimental results using MLPs and graph-aware models reveal limitations in existing MORL approaches and demonstrate the potential of graph-based methods like GNNs for complex combinatorial tasks, supported by new evaluation metrics (PNDS and OS) that assess preference consistency alongside hypervolume.</div>
<div class="mono" style="margin-top:8px">本文提出了GraphAllocBench，一个用于多目标强化学习中偏好条件策略学习的基准，其动机是现有基准多局限于玩具任务，缺乏现实性和可扩展性。该方法基于一个名为CityPlannerEnv的图资源分配沙盒环境，可灵活调整目标、偏好和分配规则，提供多样化问题集。实验使用多层感知机和图感知模型进行，结果表明现有多目标强化学习方法存在局限，并展示了图神经网络等图方法在复杂组合分配任务中的潜力，同时引入了两个新评估指标（PNDS和OS）来补充超体积度量，以直接衡量偏好一致性。</div>
</details>
</div>
<div class="card">
<div class="title">GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning</div>
<div class="meta-line">Authors: Silvia Sapora, Devon Hjelm, Alexander Toshev, Omar Attia, Bogdan Mazoure</div>
<div class="meta-line">First: 2025-10-02T16:31:39+00:00 · Latest: 2026-01-28T16:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02180v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.02180v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield black-box models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the MuJoCo, BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GRACE：面向可解释逆向强化学习的语言模型框架</div>
<div class="mono" style="margin-top:8px">逆向强化学习旨在从专家示范中恢复奖励模型，但传统方法产生的黑盒模型难以解释和调试。本文提出GRACE（以代码形式生成奖励），该方法在进化搜索中利用大语言模型，直接从专家轨迹逆向工程出可解释的、基于代码的奖励函数。所得奖励函数为可执行代码，可供检查与验证。我们在MuJoCo、BabyAI和AndroidWorld基准测试中实证验证了GRACE的有效性，该方法即使在复杂的多任务场景中也能高效学习高精度奖励。进一步实验表明，相较于采用真实奖励的竞争性模仿学习与在线强化学习方法，GRACE生成的奖励能产生更优策略。最后，我们证明GRACE能在多任务配置中构建复杂的奖励API。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for GRACE stems from the need to move beyond black-box reward models in Inverse Reinforcement Learning, which are difficult to interpret. The method introduces a framework that uses Large Language Models within an evolutionary search to reverse-engineer interpretable, code-based reward functions directly from expert demonstrations. Experimentally, GRACE efficiently learns accurate rewards on benchmarks like MuJoCo, BabyAI, and AndroidWorld, even in multi-task settings, and these rewards produce policies competitive with imitation learning and online RL using ground-truth rewards.</div>
<div class="mono" style="margin-top:8px">GRACE的提出动机源于传统逆向强化学习产生的黑盒奖励模型难以解释和调试。该方法引入了一个框架，利用大语言模型在进化搜索中，直接从专家轨迹中逆向生成可解释的、基于代码的奖励函数。实验结果表明，在MuJoCo、BabyAI和AndroidWorld等基准测试中，GRACE能高效学习到高精度的奖励，即使在复杂的多任务场景下也是如此，并且由此产生的策略性能可与使用真实奖励的模仿学习和在线强化学习方法相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions</div>
<div class="meta-line">Authors: Raul de la Rosa, Ivana Dusparic, Nicolas Cardozo</div>
<div class="meta-line">Venue: 2025 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C), Tokyo, Japan, 2025, pp. 148-153</div>
<div class="meta-line">First: 2026-01-28T15:46:51+00:00 · Latest: 2026-01-28T15:46:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent&#x27;s action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习智能体对动态动作空间与奖励函数的适应性行为调整</div>
<div class="mono" style="margin-top:8px">强化学习智能体在现实应用中常面临环境非平稳性的挑战，尤其在奖励函数变化或动作空间扩展时表现不佳。本文提出MORPHIN——一种自适应Q学习框架，支持无需完整重训练的实时适应。该框架通过集成概念漂移检测与动态调整学习/探索超参数，使智能体能同时适应奖励函数变化与动作空间的实时扩展，并保留先验策略知识以避免灾难性遗忘。我们在Gridworld基准测试和交通信号控制仿真中验证了该方法。结果表明，相较于标准Q学习基线，MORPHIN在收敛速度与持续适应能力方面表现更优，学习效率最高提升1.7倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reinforcement learning agents operating in non-stationary environments where reward functions and available actions can change dynamically. The authors propose MORPHIN, a self-adaptive Q-learning framework that integrates concept drift detection to dynamically adjust learning and exploration parameters, allowing agents to adapt to new reward functions and action space expansions without full retraining while preserving prior knowledge. Experimental validation on a Gridworld benchmark and a traffic signal control simulation shows that MORPHIN achieves up to 1.7x faster learning efficiency and superior continuous adaptation compared to standard Q-learning baselines.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习智能体在非平稳环境中运行时，奖励函数和可用动作空间可能动态变化的挑战展开研究。作者提出了MORPHIN，一种自适应的Q学习框架，它集成了概念漂移检测机制，能动态调整学习和探索参数，使智能体无需完全重新训练即可适应新的奖励函数和动作空间扩展，同时保留先验策略知识以防止灾难性遗忘。在Gridworld基准测试和交通信号控制仿真中的实验结果表明，与标准Q学习基线相比，MORPHIN实现了高达1.7倍的学习效率提升，并展现出更优的持续适应能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
