<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-01 03:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260101_0321</div>
    <div class="row"><div class="card">
<div class="title">Training AI Co-Scientists Using Rubric Rewards</div>
<div class="meta-line">Authors: Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse</div>
<div class="meta-line">First: 2025-12-29T18:59:33+00:00 · Latest: 2025-12-29T18:59:33+00:00</div>
<div class="meta-line">Comments: 11 pages in the main paper, total 119 including sample outputs in the Appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23707v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于评分准则奖励训练AI科研助手</div>
<div class="mono" style="margin-top:8px">AI科研助手正逐渐成为协助人类研究者实现科研目标的工具。这类AI助手的核心能力在于根据既定目标与约束条件生成研究方案。研究者可将方案用于头脑风暴，或经进一步优化后实施。然而，当前语言模型在生成满足所有约束与隐含要求的研究方案方面仍存在困难。本研究探索如何利用海量现有科研文献训练语言模型以生成更优质的研究方案。我们通过跨领域论文自动提取研究目标及目标专用评分准则，构建了可扩展的多样化训练语料库。随后采用带自评分的强化学习方法训练研究方案生成模型：训练期间由初始策略的冻结副本担任评分器，评分准则形成的生成器-验证器差距使模型无需外部人工监督即可持续优化。为验证该方法，我们组织机器学习领域专家开展了225小时的人工评估。专家对70%的研究目标更偏好经微调的Qwen3-30B-A3B模型生成的方案，并认可84%的自动提取目标专用评分准则。为评估普适性，我们将该方法扩展至医学论文及arXiv预印本的研究目标，并采用前沿模型陪审团进行评估。微调带来12-22%的相对性能提升，并展现出显著的跨领域泛化能力，即使在医学研究等难以获取执行反馈的问题场景中同样有效。这些发现共同证明，这种可扩展的自动化训练方法具备提升通用AI科研助手能力的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for AI co-scientists that can generate high-quality research plans adhering to constraints, this work develops a method to train language models using rubric rewards derived from existing research papers. The approach automatically extracts research goals and grading rubrics from papers across domains, then employs reinforcement learning with self-grading, where a frozen initial policy acts as a grader to create a generator-verifier gap for unsupervised improvement. Experimental results from a human expert study on machine learning goals show that the finetuned Qwen3-30B-A3B model&#x27;s plans are preferred 70% of the time over the initial model, with 84% of extracted rubrics approved, and the method generalizes to medical and arXiv domains, yielding 12-22% relative improvements and effective cross-domain performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升AI科研助手生成符合约束的研究计划的能力，其动机源于当前语言模型在此任务上的不足。方法上，通过自动从多领域论文中提取研究目标和评分标准构建训练语料，并采用基于自我评分的强化学习进行训练，其中冻结的初始策略作为评分者，形成生成器-验证器差距以实现无监督改进。实验结果表明，在机器学习目标的人类专家评估中，微调后的Qwen3-30B-A3B模型生成的研究计划在70%的情况下优于初始模型，84%的自动提取评分标准获认可，且该方法可推广至医学和arXiv领域，相对性能提升12-22%，展现出有效的跨领域泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization</div>
<div class="meta-line">Authors: Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</div>
<div class="meta-line">First: 2025-10-09T17:58:07+00:00 · Latest: 2025-12-29T18:55:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08554v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08554v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce Group Diffusion Policy Optimization (GDPO), a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过群组扩散策略优化提升扩散语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）通过迭代优化实现并行且顺序无关的文本生成，为自回归大语言模型（LLMs）提供了灵活的替代方案。然而，由于似然函数难以处理，将强化学习（RL）微调应用于DLMs仍是一个开放挑战。开创性工作如diffu-GRPO通过单步解掩码估计词元级似然，虽计算高效但存在严重偏差。更理论完备的方法基于序列级似然，其中证据下界（ELBO）可作为替代目标。尽管数学关联清晰，但ELBO方法因似然评估成本过高而应用受限。本研究重新审视ELBO估计，解析其方差来源。该分解启发了通过沿关键维度进行快速确定性积分近似来降低方差。基于此，我们提出专为DLMs设计的强化学习算法——群组扩散策略优化（GDPO）。GDPO采用简洁有效的半确定性蒙特卡洛方案，缓解原始双蒙特卡洛采样下ELBO估计器的方差爆炸问题，在严格评估预算下构建了可证明的低方差估计器。实验表明，GDPO在多数数学、推理和代码基准测试中，持续超越预训练检查点并优于当前先进基线方法diffu-GRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying reinforcement learning fine-tuning to diffusion language models, which lack tractable likelihoods, by introducing Group Diffusion Policy Optimization (GDPO). The method reduces variance in evidence lower bound estimators through semi-deterministic Monte Carlo sampling, enabling efficient sequence-level likelihood estimation. Experimental results show that GDPO consistently improves pretrained checkpoints and outperforms the baseline diffu-GRPO on most math, reasoning, and coding benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对扩散语言模型因似然函数难以处理而难以进行强化学习微调的问题，提出了分组扩散策略优化方法。该方法通过半确定性蒙特卡洛采样降低证据下界估计的方差，实现了高效的序列级似然估计。实验结果表明，该方法在多数数学、推理和代码基准测试中优于现有基线，并持续提升了预训练模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Bellman Calibration for V-Learning in Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Lars van der Laan, Nathan Kallus</div>
<div class="meta-line">First: 2025-12-29T18:52:18+00:00 · Latest: 2025-12-29T18:52:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23694v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23694v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model&#x27;s predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线强化学习中V学习的贝尔曼校准</div>
<div class="mono" style="margin-top:8px">我们提出了迭代贝尔曼校准——一种简单、模型无关、后处理的校准方法，用于在无限时域马尔可夫决策过程中校准离策略价值预测。贝尔曼校准要求具有相似预测长期回报的状态，在目标策略下展现出符合贝尔曼方程的单步回报。我们通过将拟合的贝尔曼目标重复回归到模型预测上，采用双重稳健伪结果处理离策略数据，从而将经典直方图校准和等渗校准方法适配到动态反事实场景。这产生了一种可应用于任意价值估计器的一维拟合价值迭代方案。我们的分析在弱假设条件下为校准和预测提供了有限样本保证，且关键地无需贝尔曼完备性或可实现性假设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for reliable off-policy value prediction in infinite-horizon Markov decision processes, where existing methods often lack calibration guarantees. The method introduces Iterated Bellman Calibration, a model-agnostic post-hoc procedure that enforces consistency with the Bellman equation by repeatedly regressing doubly robust pseudo-outcomes onto a model&#x27;s predictions, adapting histogram and isotonic calibration to dynamic settings. Experimental results, supported by finite-sample guarantees, show that this approach improves calibration and prediction accuracy without requiring strong assumptions like Bellman completeness or realizability.</div>
<div class="mono" style="margin-top:8px">本文的动机是在无限时域马尔可夫决策过程中，现有方法常缺乏校准保证，需要可靠的离策略价值预测。方法上提出了迭代贝尔曼校准，这是一种模型无关的后处理程序，通过使用双重稳健伪结果反复回归到模型预测上，将直方图和等渗校准适配到动态环境中，以强制满足贝尔曼方程的一致性。实验结果表明，该方法在无需贝尔曼完备性或可实现性等强假设下，通过有限样本保证，提高了校准和预测的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</div>
<div class="meta-line">Authors: Deniz Akdemir</div>
<div class="meta-line">First: 2025-12-29T17:21:44+00:00 · Latest: 2025-12-29T17:21:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23617v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23617v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing &quot;negative transfer&quot; that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam&#x27;s theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Le Cam畸变：一种用于鲁棒迁移学习的决策理论框架</div>
<div class="mono" style="margin-top:8px">分布偏移是现实世界机器学习中的核心挑战。主流范式——无监督域适应（UDA）——通过对称散度最小化强制特征不变性以对齐源域和目标域表示[Ganin等人，2016]。我们证明该方法存在根本缺陷：当域信息量不均等时（例如高质量传感器与低质量传感器），严格不变性必然导致信息破坏，引发“负迁移”，在安全关键应用中可能造成灾难性后果[Wang等人，2019]。
我们提出基于Le Cam统计实验理论[Le Cam, 1986]的决策理论框架，通过构造性近似以定向可模拟性替代对称不变性。我们引入Le Cam畸变（以缺陷距离$δ(E_1, E_2)$量化）作为可模拟性条件下迁移风险的严格上界。该框架通过学习从源域模拟目标域的内核，实现无需源域性能退化的迁移。在五项实验（基因组学、视觉、强化学习）中，Le Cam畸变实现：（1）HLA基因组学中接近完美的频率估计（相关性$r=0.999$，媲美经典方法），（2）CIFAR-10图像分类中零源域效用损失（保持81.2%准确率，而CycleGAN下降34.7%），（3）RL控制中的安全策略迁移（基于不变性的方法出现灾难性崩溃）。Le Cam畸变为医疗影像、自主系统和精准医疗等无法承受负迁移的领域，提供了首个风险可控的迁移学习原理性框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of strict feature invariance in unsupervised domain adaptation, which can cause catastrophic negative transfer when domains are unequally informative, this paper proposes a decision-theoretic framework based on Le Cam&#x27;s theory. The method replaces symmetric invariance with directional simulability, introducing Le Cam Distortion quantified by the Deficiency Distance as a rigorous bound for transfer risk, and learns a kernel to simulate the target from the source without degrading source information. Experimental results across genomics, vision, and reinforcement learning show near-perfect frequency estimation in HLA genomics, zero source utility loss in CIFAR-10 classification compared to significant drops in invariance-based methods, and safe policy transfer in RL where other methods fail catastrophically.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决无监督域适应中严格特征对齐的缺陷，该缺陷在域信息不均等时会导致灾难性的负迁移。方法基于Le Cam统计实验理论，用方向可模拟性替代对称不变性，引入以缺陷距离量化的Le Cam畸变作为迁移风险的严格上界，并学习从源到目标的模拟核而不损害源信息。在基因组学、视觉和强化学习的实验中，该方法在HLA基因组学中实现了近乎完美的频率估计，在CIFAR-10图像分类中保持了源效用零损失，而在基于不变性的方法中则出现显著下降，并在强化学习控制中实现了安全策略迁移，避免了其他方法的灾难性崩溃。</div>
</details>
</div>
<div class="card">
<div class="title">PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</div>
<div class="meta-line">Authors: Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang</div>
<div class="meta-line">First: 2025-12-29T15:34:27+00:00 · Latest: 2025-12-29T15:34:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23545v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23545v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PathFound：一种激活循证病理诊断的智能体多模态模型</div>
<div class="mono" style="margin-top:8px">当前病理基础模型在视觉表征学习和多模态交互方面取得显著进展，但多数模型仍采用静态推理范式——全切片图像经单次处理即生成预测，缺乏在诊断模糊时的重新评估或针对性证据获取能力。这与临床通过重复切片观察和追加检查来优化假设的诊断流程形成鲜明对比。本研究提出PathFound，一种支持循证推理的智能体多模态病理诊断模型。该模型整合病理视觉基础模型、视觉语言模型及强化学习训练推理模型的能力，通过初始诊断、证据搜寻和最终决策三阶段推进，实现主动信息获取与诊断优化。实验表明，该策略在多种大型多模态模型中均能持续提升诊断准确率，验证了循证工作流在计算病理学中的有效性。其中PathFound在多样化临床场景中达到最优诊断性能，并展现出发现细胞核特征与局部浸润等细微病变的突出潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the gap between static AI inference and the iterative, evidence-seeking nature of clinical pathology workflows, this paper introduces PathFound, an agentic multimodal model for pathological diagnosis. The method integrates pathological visual foundation models, vision-language models, and reinforcement learning-trained reasoning models to actively progress through initial diagnosis, evidence-seeking, and final decision stages. Experimental results show that this evidence-seeking strategy consistently boosts diagnostic accuracy across several large multimodal models, with PathFound achieving state-of-the-art performance in diverse clinical scenarios and demonstrating strong capability in identifying subtle pathological details.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有病理AI模型的静态推理范式与临床动态寻求证据的诊断流程存在差距，因此提出了PathFound，一种用于病理诊断的智能体多模态模型。该方法整合了病理视觉基础模型、视觉语言模型和基于强化学习的推理模型，通过主动执行初始诊断、证据寻求和最终决策三阶段流程来优化诊断。实验结果表明，这种证据寻求策略在多个大型多模态模型中持续提高了诊断准确性，其中PathFound在多样临床场景中取得了最先进的诊断性能，并展现出识别细胞核特征、局部浸润等细微病理细节的强大潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Expressive Temporal Specifications for Reward Monitoring</div>
<div class="meta-line">Authors: Omar Adalat, Francesco Belardinelli</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-16T22:28:30+00:00 · Latest: 2025-12-29T15:04:16+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12808v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.12808v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于奖励监控的表达性时序规约</div>
<div class="mono" style="margin-top:8px">在强化学习中，如何设计信息丰富且密集的奖励函数仍是核心挑战，因其直接影响智能体训练效率。本研究利用有限迹上定量线性时序逻辑（$\text{LTL}_f[\mathcal{F}]$）的表达能力，合成为运行时可观测状态轨迹生成密集奖励流的监控器。通过在训练期间提供细粒度反馈，这些监控器能引导智能体实现最优行为，并缓解当前主流布尔语义下长时序决策中常见的奖励稀疏问题。本框架与算法无关，仅依赖状态标注函数，天然支持非马尔可夫性质的规约。实验结果表明：在最大化任务完成度的定量度量与缩短收敛时间方面，定量监控器始终优于布尔监控器，且其优势随环境不同而进一步凸显。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of sparse rewards in Reinforcement Learning by proposing a framework that uses quantitative Linear Temporal Logic on finite traces (LTL_f[F]) to synthesize expressive reward monitors. The method generates dense reward signals from state trajectories, providing nuanced feedback to guide agents, and is algorithm-agnostic, relying only on a state labelling function to accommodate non-Markovian properties. Experimental results demonstrate that these quantitative monitors consistently outperform or subsume Boolean monitors in maximizing task completion and reducing convergence time across various environments.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中奖励稀疏的挑战，提出了一种使用有限轨迹上的定量线性时序逻辑（LTL_f[F]）来合成表达性奖励监控器的框架。该方法从状态轨迹生成密集的奖励信号，提供细致的反馈以指导智能体，且与算法无关，仅依赖于状态标记函数来适应非马尔可夫性质。实验结果表明，这些定量监控器在最大化任务完成度和减少收敛时间方面，持续优于或包含布尔监控器，并在不同环境中表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua</div>
<div class="meta-line">First: 2025-12-29T14:50:23+00:00 · Latest: 2025-12-29T14:50:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23515v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23515v1">PDF</a> · <a href="https://github.com/FinStep-AI/Alpha-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alpha-R1：基于强化学习与大语言模型推理的阿尔法因子筛选框架</div>
<div class="mono" style="margin-top:8px">在非平稳市场中，信号衰减与机制转换对数据驱动的投资策略构成持续性挑战。传统时序方法与机器学习模型主要依赖历史相关性，当经济环境变化时往往难以保持泛化能力。尽管大语言模型在处理非结构化信息方面展现出强大潜力，但其通过显式经济推理支持量化因子筛选的能力尚未得到充分探索。现有因子方法通常将阿尔法因子简化为数值时间序列，忽略了决定因子经济相关性的语义逻辑。本文提出Alpha-R1——一个通过强化学习训练的80亿参数推理模型，用于情境感知的阿尔法因子筛选。该模型通过分析因子逻辑与实时新闻，评估动态市场条件下的阿尔法相关性，并依据情境一致性选择性地激活或停用因子。跨多资产池的实证结果表明，Alpha-R1持续超越基准策略，并对阿尔法衰减表现出更强的鲁棒性。完整实现与资源详见：https://github.com/FinStep-AI/Alpha-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of alpha decay and regime shifts in non-stationary financial markets, where traditional data-driven strategies relying on historical correlations often fail to generalize. To overcome this, the authors propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning that explicitly reasons over factor logic and real-time news to assess alpha relevance under changing conditions, selectively activating or deactivating factors based on contextual consistency. Experimental results across multiple asset pools demonstrate that Alpha-R1 consistently outperforms benchmark strategies and shows improved robustness to alpha decay.</div>
<div class="mono" style="margin-top:8px">本文针对非平稳金融市场中阿尔法衰减和机制转换的挑战，传统依赖历史相关性的数据驱动策略往往难以泛化。为此，作者提出了Alpha-R1，这是一个通过强化学习训练的80亿参数推理模型，能够基于因子逻辑和实时新闻进行显式推理，以评估变化市场条件下的阿尔法相关性，并根据上下文一致性选择性地激活或停用因子。在多个资产池上的实验结果表明，Alpha-R1持续优于基准策略，并展现出对阿尔法衰减更强的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization</div>
<div class="meta-line">Authors: Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink</div>
<div class="meta-line">First: 2025-12-29T14:32:34+00:00 · Latest: 2025-12-29T14:32:34+00:00</div>
<div class="meta-line">Comments: 16 page,10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23493v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23493v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this article, we consider an industrial internet of things (IIoT) network supporting multi-device dynamic ultra-reliable low-latency communication (URLLC) while the channel state information (CSI) is imperfect. A joint link adaptation (LA) and device scheduling (including the order) design is provided, aiming at maximizing the total transmission rate under strict block error rate (BLER) constraints. In particular, a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method is proposed, which determines the device served order sequence and the corresponding modulation and coding scheme (MCS) adaptively based on the imperfect CSI. Note that the imperfection of CSI, error sample imbalance in URLLC networks, as well as the parameter sensitivity nature of the TD3 algorithm likely diminish the algorithm&#x27;s convergence speed and reliability. To address such an issue, we proposed a BO based training mechanism for the convergence speed improvement, which provides a more reliable learning direction and sample selection method to track the imbalance sample problem. Via extensive simulations, we show that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向URLLC工业物联网网络的联合链路自适应与设备调度方法：一种基于深度强化学习及贝叶斯优化的策略</div>
<div class="mono" style="margin-top:8px">本文研究支持多设备动态超可靠低时延通信的工业物联网网络，在信道状态信息不完美的条件下，提出一种联合链路自适应与设备调度（含服务顺序）的设计方案，旨在严格误块率约束下最大化总传输速率。特别地，提出一种基于贝叶斯优化的双延迟深度确定性策略梯度方法，该方法依据不完美的信道状态信息自适应确定设备服务顺序序列及相应的调制编码方案。针对信道状态信息不完美、URLLC网络中误差样本不平衡以及TD3算法参数敏感特性可能降低算法收敛速度与可靠性的问题，设计了一种基于贝叶斯优化的训练机制以提升收敛速度，通过提供更可靠的学习方向与样本选择方法来追踪不平衡样本问题。大量仿真表明，所提算法相比现有方案具有更快的收敛速度与更高的总速率性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing multi-device dynamic Ultra-Reliable Low-Latency Communication (URLLC) in Industrial Internet of Things (IIoT) networks under imperfect Channel State Information (CSI). The authors propose a joint link adaptation and device scheduling design, employing a Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method to adaptively determine device serving order and modulation and coding schemes, aiming to maximize total transmission rate under strict block error rate constraints. To overcome issues like CSI imperfection, error sample imbalance, and TD3&#x27;s parameter sensitivity that hinder convergence, they introduce a BO-based training mechanism to improve convergence speed and reliability via better learning direction and sample selection. Experimental simulations demonstrate that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.</div>
<div class="mono" style="margin-top:8px">本文针对工业物联网网络中信道状态信息不完善条件下的多设备动态超可靠低时延通信优化问题展开研究。作者提出了一种联合链路自适应和设备调度的设计方案，采用基于贝叶斯优化的双延迟深度确定性策略梯度方法，自适应地确定设备服务顺序及调制编码方案，以在严格误块率约束下最大化总传输速率。为解决信道状态信息不完善、误差样本不平衡及算法参数敏感度导致的收敛速度慢和可靠性降低等问题，他们引入了一种基于贝叶斯优化的训练机制，通过改进学习方向和样本选择来提升收敛速度和可靠性。仿真实验表明，与现有方案相比，所提算法实现了更快的收敛速度和更高的总速率性能。</div>
</details>
</div>
<div class="card">
<div class="title">RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation</div>
<div class="meta-line">Authors: Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Jiakun Du, Xiangyuan Wang, Xu Fu, Letong Shi, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang</div>
<div class="meta-line">First: 2025-09-19T13:24:17+00:00 · Latest: 2025-12-29T14:13:21+00:00</div>
<div class="meta-line">Comments: GitHub Repo: https://github.com/RLinf/RLinf</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15965v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.15965v2">PDF</a> · <a href="https://github.com/RLinf/RLinf">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker&#x27;s adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving $1.07\times-2.43\times$ speedup in end-to-end training throughput.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLinf：通过宏微观流转换实现灵活高效的大规模强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在推动通用人工智能、智能体智能与具身智能方面展现出巨大潜力。然而，RL工作流固有的异构性与动态性常导致现有系统硬件利用率低、训练速度慢。本文提出RLinf，一个高性能RL训练系统，其核心洞察在于：高效RL训练的主要瓶颈在于系统灵活性。为最大化灵活性与效率，RLinf基于一种名为“宏微观流转换”（M2Flow）的新型RL系统设计范式构建，该范式能在时空维度自动分解高层易组合的RL工作流，并将其重组为优化的执行流。依托RLinf工作节点的自适应通信能力，我们设计了上下文切换与弹性流水线技术以实现M2Flow转换，并采用性能分析驱动的调度策略生成最优执行计划。在推理RL与具身RL任务上的广泛实验表明，RLinf持续优于现有先进系统，端到端训练吞吐量提升达1.07倍至2.43倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for RLinf stems from the inefficiencies in reinforcement learning (RL) training caused by heterogeneous and dynamic workflows, which lead to low hardware utilization and slow training speeds. The method introduces a novel system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically decomposes high-level RL workflows temporally and spatially and recomposes them into optimized execution flows, enhanced by adaptive communication, context switching, elastic pipelining, and a profiling-guided scheduling policy. Experimental results on reasoning and embodied RL tasks show that RLinf consistently outperforms state-of-the-art systems, achieving a 1.07× to 2.43× speedup in end-to-end training throughput.</div>
<div class="mono" style="margin-top:8px">RLinf的动机源于强化学习（RL）训练中因工作流异构和动态性导致的硬件利用率低和训练速度慢的问题。该方法提出了一种名为宏到微流转换（M2Flow）的新系统设计范式，能自动在时间和空间维度上分解高级RL工作流并重组为优化执行流，辅以自适应通信、上下文切换、弹性流水线和基于性能分析的调度策略。在推理和具身RL任务上的实验结果表明，RLinf持续优于现有先进系统，端到端训练吞吐量提升了1.07倍至2.43倍。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation</div>
<div class="meta-line">Authors: Toqeer Ali Syed, Mohammad Riyaz Belgaum, Salman Jan, Asadullah Abdullah Khan, Saad Said Alqahtani</div>
<div class="meta-line">First: 2025-12-29T14:06:09+00:00 · Latest: 2025-12-29T14:06:09+00:00</div>
<div class="meta-line">Comments: Conference paper, accept in ACCA IEEE Bahrain</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23480v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23480v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, reinforcement learning (RL), and multi-agent coordination. The suggested system utilizes specialized security agents coordinated with the help of LangChain and LangGraph, communicates with actual CI/CD environments with the Model Context Protocol (MCP), and documents all the observations and actions in a blockchain security ledger to ensure integrity and auditing. Reinforcement learning can be used to achieve adaptive mitigation strategies that consider the balance between security effectiveness and the operational overhead, and LLMs can be used to achieve semantic vulnerability analysis, as well as explainable decisions. This framework is tested based on simulated pipelines, as well as, actual world CI/CD integrations on GitHub Actions and Jenkins, including injection attacks, insecure deserialization, access control violations, and configuration errors. Experimental outcomes indicate better detection accuracy, shorter mitigation latency and reasonable build-time overhead than rule-based, provenance only and RL only baselines. These results show that agentic AI can facilitate the transition to self defending, proactive software supply chains rather than reactive verification ones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向软件供应链安全自主防御的智能体人工智能：从溯源到漏洞缓解的超越</div>
<div class="mono" style="margin-top:8px">软件供应链攻击日益聚焦可信开发与交付流程，传统构建后完整性机制已不适用。现有框架如SLSA、SBOM和in-toto主要提供溯源与可追溯性，但缺乏主动识别和消除软件生产漏洞的能力。本文提出一种基于自主软件供应链安全的智能体人工智能框架，融合大型语言模型推理、强化学习与多智能体协同。该系统通过LangChain和LangGraph协调专业化安全智能体，借助模型上下文协议与实际CI/CD环境交互，并将所有观测与行动记录于区块链安全账本以确保完整性与可审计性。强化学习用于实现兼顾安全效能与运维开销的自适应缓解策略，大型语言模型则支持语义化漏洞分析与可解释决策。该框架在模拟流水线及GitHub Actions与Jenkins的真实CI/CD环境中进行测试，涵盖注入攻击、不安全反序列化、访问控制违规及配置错误等场景。实验结果表明，相较于基于规则、纯溯源及纯强化学习的基线方法，本框架在检测精度、缓解延迟和构建时开销方面均表现更优。这些发现证明智能体人工智能可推动软件供应链从被动验证向自主防御、主动防护模式演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing provenance-focused frameworks like SLSA and SBOM in actively mitigating vulnerabilities within software supply chains, this paper proposes an agentic AI system for autonomous defense. The method integrates LLM-based reasoning for semantic vulnerability analysis, reinforcement learning for adaptive mitigation strategies, and multi-agent coordination via LangChain/LangGraph, with actions logged on a blockchain ledger and integrated into CI/CD environments using the Model Context Protocol. Experimental results from simulated and real-world CI/CD pipelines, including tests against injection attacks and configuration errors, demonstrate that the system achieves higher detection accuracy, lower mitigation latency, and acceptable build-time overhead compared to rule-based, provenance-only, and RL-only baselines.</div>
<div class="mono" style="margin-top:8px">针对现有如SLSA和SBOM等侧重于溯源的可信框架在主动缓解软件供应链漏洞方面的不足，本文提出了一种用于自主防御的智能体AI系统。该方法结合了基于大语言模型的语义漏洞分析与可解释决策、强化学习的自适应缓解策略，以及通过LangChain/LangGraph实现的多智能体协调，并利用模型上下文协议与CI/CD环境集成，所有观察和行动均记录在区块链安全账本中。在模拟及真实CI/CD管道（如GitHub Actions和Jenkins）上针对注入攻击、不安全反序列化等场景的实验结果表明，该系统相比基于规则、仅溯源和仅强化学习的基线方法，具有更高的检测精度、更低的缓解延迟以及可接受的构建时间开销。</div>
</details>
</div>
<div class="card">
<div class="title">HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation</div>
<div class="meta-line">Authors: Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao</div>
<div class="meta-line">First: 2025-12-29T13:46:24+00:00 · Latest: 2025-12-29T13:46:24+00:00</div>
<div class="meta-line">Comments: Github: see https://github.com/Tencent-Hunyuan/HY-Motion-1.0</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23464v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23464v1">PDF</a> · <a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HY-Motion 1.0：面向文本到动作生成的流匹配模型规模化研究</div>
<div class="mono" style="margin-top:8px">我们推出HY-Motion 1.0系列模型，这是一套能够根据文本描述生成3D人体动作的先进大规模动作生成模型。该研究首次成功将基于扩散Transformer（DiT）的流匹配模型扩展至十亿参数规模，在动作生成领域实现了显著超越当前开源基准的指令跟随能力。我们创新性地采用全阶段训练范式——包括基于3000多小时动作数据的大规模预训练、400小时精选数据的高质量微调，以及结合人类反馈与奖励模型的强化学习——确保模型精准对齐文本指令并保持高动作质量。该框架依托我们精心设计的数据处理流程，执行严格的动作清洗与标注。最终，我们的模型实现了最广泛的动作覆盖，涵盖6大类超过200种动作类别。我们将HY-Motion 1.0开源发布，旨在推动未来研究并加速3D人体动作生成模型向商业化成熟阶段迈进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for high-quality, text-aligned 3D human motion generation, this paper introduces HY-Motion 1.0, a series of large-scale flow matching models based on Diffusion Transformers (DiT) scaled to billions of parameters. The method employs a comprehensive training paradigm involving large-scale pretraining on over 3,000 hours of motion data, fine-tuning on 400 hours of curated data, and reinforcement learning from human feedback and reward models to ensure precise text instruction following and motion quality. The main experimental results demonstrate that the model significantly outperforms existing open-source benchmarks, achieving extensive coverage of over 200 motion categories across six major classes, and is released open-source to advance research and commercial applications.</div>
<div class="mono" style="margin-top:8px">本文旨在生成高质量且与文本描述对齐的3D人体运动，为此提出了HY-Motion 1.0系列模型，这是首个在运动生成领域将基于扩散变换器（DiT）的流匹配模型扩展到十亿参数规模的尝试。方法采用全面的全阶段训练范式，包括在超过3000小时运动数据上的大规模预训练、在400小时精选数据上的高质量微调，以及基于人类反馈和奖励模型的强化学习，以确保运动与文本指令的精确对齐和高品质。主要实验结果表明，该模型在性能上显著超越了当前开源基准，实现了覆盖六大类超过200种运动类别的广泛生成能力，并已开源以促进未来研究和加速商业应用成熟。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs</div>
<div class="meta-line">Authors: Daniel Furelos-Blanco, Charles Pert, Frederik Kelbel, Alex F. Spies, Alessandra Russo, Michael Dennis</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-16T17:48:09+00:00 · Latest: 2025-12-29T13:42:46+00:00</div>
<div class="meta-line">Comments: Extended version of paper accepted for publication at the 40th AAAI Conference on Artificial Intelligence (AAAI)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12706v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12706v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越固定任务：面向任务-关卡组合的无监督环境设计</div>
<div class="mono" style="margin-top:8px">在复杂环境（关卡）中训练通用智能体执行复杂指令（任务）仍是强化学习的核心挑战。随机采样任务-关卡组合常产生无解配对，凸显了任务与关卡协同设计的必要性。虽然无监督环境设计（UED）已被证明能有效自动生成关卡课程，但现有研究仅针对固定任务。本文提出ATLAS（面向规范自课程的任務与关卡对齐方法），这是一种在任务与关卡上生成联合自课程的新方法。该方法基于UED框架，能自动生成可解且具挑战性的任务-关卡组合用于策略训练。为评估ATLAS并推动领域发展，我们构建了在Minigrid关卡中以奖励机建模任务的评估套件。实验表明ATLAS显著优于随机采样方法，尤其在可解组合采样概率较低时优势更明显。研究进一步证明，利用任务与关卡结构的变异操作能加速高性能策略的收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training general reinforcement learning agents to follow complex instructions in intricate environments, where random sampling of task-level pairs often yields unsolvable combinations. To overcome this, the authors propose ATLAS, a novel unsupervised environment design method that generates joint autocurricula over both tasks and levels, automatically producing solvable yet challenging pairs for policy training. Experimental results on a suite using reward machines in Minigrid levels show that ATLAS significantly outperforms random sampling, especially when solvable pairs are rare, and that mutations leveraging task and level structure accelerate policy convergence.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中训练通用智能体在复杂环境中执行复杂指令的挑战，指出随机采样任务-关卡组合常产生不可解配对的问题。为此，作者提出了ATLAS方法，这是一种无监督环境设计方法，能生成任务和关卡的联合自动课程，自动产生可解且具挑战性的配对用于策略训练。在基于Minigrid关卡和奖励机器的评估套件上的实验表明，ATLAS显著优于随机采样方法，尤其在可解配对稀少时效果更明显，并且利用任务和关卡结构的突变能加速策略收敛。</div>
</details>
</div>
<div class="card">
<div class="title">Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance</div>
<div class="meta-line">Authors: Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang</div>
<div class="meta-line">First: 2025-12-29T13:39:41+00:00 · Latest: 2025-12-29T13:39:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23461v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23461v1">PDF</a> · <a href="https://github.com/Qwen-Applications/DIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用信息论指导消除奖励模型中的归纳偏差</div>
<div class="mono" style="margin-top:8px">奖励模型在基于人类反馈的强化学习中至关重要，用于使大语言模型与人类价值观对齐。然而，奖励模型训练数据通常质量较低，包含易导致过拟合和奖励攻击的归纳偏差。例如，更详细全面的回答通常更受人类青睐，但字数也更多，导致回答长度成为不可避免的偏差之一。现有少数去偏差方法要么针对单一特定偏差，要么仅用简单线性相关性建模问题。为缓解奖励建模中更复杂多样的归纳偏差，我们提出一种新颖的信息论去偏差方法DIR。受信息瓶颈启发，我们最大化奖励模型分数与人类偏好对之间的互信息，同时最小化奖励模型输出与偏好输入偏差属性间的互信息。基于信息论的理论证明，DIR能处理具有非线性关联的更复杂偏差类型，大幅扩展了奖励模型去偏差方法的实际应用场景。实验中，我们通过三种归纳偏差验证DIR的有效性：回答长度、迎合性及格式偏差。研究发现DIR不仅能有效缓解目标偏差，还能提升多类基准测试中的RLHF性能，获得更好的泛化能力。代码与训练方案已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of inductive biases in reward models (RMs) used for reinforcement learning from human feedback, where biases like a preference for longer responses can lead to overfitting and reward hacking. The authors propose DIR, an information-theoretic debiasing method that maximizes mutual information between RM scores and human preferences while minimizing it between RM outputs and biased input attributes, thereby handling complex, non-linear biases. Experimental results demonstrate that DIR effectively mitigates biases such as response length, sycophancy, and format, and improves RLHF performance and generalization across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习中奖励模型存在的归纳偏差问题展开研究，这类偏差（如对较长回复的偏好）容易导致过拟合和奖励攻击。作者提出了DIR这一信息论去偏方法，通过最大化奖励模型分数与人类偏好之间的互信息，同时最小化其输出与带偏输入属性之间的互信息，从而处理复杂的非线性偏差。实验结果表明，DIR能有效减轻响应长度、迎合性及格式等偏差，并在多种基准测试中提升了强化学习的人类反馈性能与泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following</div>
<div class="meta-line">Authors: Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song</div>
<div class="meta-line">First: 2025-12-29T13:31:08+00:00 · Latest: 2025-12-29T13:31:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23457v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23457v1">PDF</a> · <a href="https://github.com/sastpg/HIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将失败回放为成功：面向指令跟随任务的样本高效强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在使大语言模型（LLM）遵循多样化约束的指令方面展现出潜力。尽管成果鼓舞人心，但RL的改进必然依赖于采样成功的高质量响应；然而，初始模型常因能力有限而难以生成满足所有约束的响应，导致奖励稀疏或难以区分，从而阻碍学习。本研究提出后见指令回放（HiR），一种面向复杂指令跟随任务的新型样本高效RL框架，采用“选择-重写”策略，基于事后已满足的约束将失败尝试回放为成功样本。我们对这些回放样本及原始样本进行RL训练，从理论上将目标构建为指令级和响应级的双重偏好学习，从而仅用二元奖励信号即可实现高效优化。大量实验表明，所提HiR在不同指令跟随任务中均取得显著效果，同时所需计算资源更少。代码与数据集已开源：https://github.com/sastpg/HIR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the challenge that reinforcement learning for aligning large language models to follow complex instructions often suffers from sparse rewards, as initial models struggle to generate responses meeting all constraints. The method introduces Hindsight Instruction Replay (HiR), a sample-efficient framework that replays failed attempts as successes by selecting and rewriting responses based on partially satisfied constraints, framing the objective as dual-preference learning for efficient optimization with binary rewards. Experimental results show that HiR achieves promising performance across various instruction-following tasks while reducing computational costs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，使用强化学习对齐大语言模型以遵循复杂指令时，常因初始模型难以生成满足所有约束的响应而导致奖励稀疏，阻碍学习。方法上提出了后见指令回放（HiR），这是一种样本高效的框架，通过基于事后已满足的约束选择和重写失败尝试，将其回放为成功样本，并将目标构建为指令和响应层面的双重偏好学习，仅用二元奖励实现高效优化。实验结果表明，HiR在不同指令遵循任务中取得了良好性能，同时降低了计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis</div>
<div class="meta-line">Authors: Alex Lewandowski, Adtiya A. Ramesh, Edan Meyer, Dale Schuurmans, Marlos C. Machado</div>
<div class="meta-line">Venue: NeurIPS 2025 spotlight</div>
<div class="meta-line">First: 2025-12-29T12:31:46+00:00 · Latest: 2025-12-29T12:31:46+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 (spotlight)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23419v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning is often motivated by the idea, known as the big world hypothesis, that &quot;the world is bigger&quot; than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent&#x27;s capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent&#x27;s ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界更大！基于计算嵌入视角的大世界假说探讨</div>
<div class="mono" style="margin-top:8px">持续学习常受“大世界假说”驱动，即“世界比智能体更大”。现有问题建模通过显式约束智能体与环境的关系来体现这一思想，这类约束促使智能体持续调整以最优利用有限能力，而非收敛至固定解。然而，显式约束往往具有随意性、难以整合，且可能限制智能体能力扩展的有效性。本文提出一种问题设定：无论智能体能力如何，其始终受嵌入环境的约束。具体而言，我们引入计算嵌入视角，将嵌入智能体表征为在通用（形式化）计算机中模拟的自动机。此类自动机始终受约束；我们证明其等价于在可数无限状态空间上与部分可观测马尔可夫决策过程交互的智能体。针对该设定，我们提出称为“交互性”的目标度量，用于评估智能体通过学习新预测持续调整行为的能力。随后开发了基于模型的强化学习算法以寻求交互性，并构建合成问题评估持续学习能力。实验表明，深度非线性网络难以维持交互性，而深度线性网络随容量增加能保持更高交互性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges conventional continual learning formulations by proposing a computationally-embedded perspective, motivated by the need to naturally constrain an agent within an environment rather than through explicit, ad hoc constraints. The method formalizes an embedded agent as an automaton simulated in a universal computer, proving its equivalence to an agent interacting with a partially observable Markov decision process over an infinite state-space, and introduces an objective called interactivity to measure continual adaptation. Experimental results on a synthetic problem reveal that deep nonlinear networks struggle to maintain interactivity, whereas deep linear networks achieve higher interactivity as their capacity increases, highlighting a key limitation in current models for sustained learning.</div>
<div class="mono" style="margin-top:8px">本文通过提出一种计算嵌入的视角来挑战传统的持续学习框架，其动机在于需要自然地约束智能体于环境中，而非依赖人为的显式约束。该方法将嵌入智能体形式化为在通用计算机中模拟的自动机，证明其等价于在无限状态空间的部分可观测马尔可夫决策过程中交互的智能体，并引入称为交互性的目标来衡量持续适应能力。在合成问题上的实验结果表明，深度非线性网络难以维持交互性，而深度线性网络随着容量增加能实现更高的交互性，这揭示了当前模型在持续学习能力上的关键局限。</div>
</details>
</div>
<div class="card">
<div class="title">OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</div>
<div class="meta-line">Authors: Zhenguo Zhang, Haohan Zheng, Yishen Wang, Le Xu, Tianchen Deng, Xuefeng Chen, Qu Chen, Bo Zhang, Wuxiong Huang</div>
<div class="meta-line">First: 2025-12-16T03:19:28+00:00 · Latest: 2025-12-29T12:27:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14044v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.14044v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning. While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels. Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and &quot;zoom in&quot; on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model&#x27;s significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniDrive-R1：基于强化驱动的交错式多模态思维链实现可信视觉语言自动驾驶</div>
<div class="mono" style="margin-top:8px">视觉语言模型在自动驾驶等安全关键领域的应用，因可靠性缺陷（尤其是物体幻觉问题）而严重受限。该缺陷源于模型依赖未接地的文本思维链推理。现有多模态思维链方法虽尝试缓解，但存在两个根本缺陷：（1）解耦的感知与推理阶段阻碍端到端联合优化；（2）依赖昂贵密集的定位标注。为此，我们提出OmniDrive-R1——专为自动驾驶设计的端到端视觉语言模型框架，通过交错式多模态思维链机制统一感知与推理。其核心创新是强化驱动的视觉接地能力，使模型能自主聚焦关键区域进行细粒度分析。该能力由纯两阶段强化学习训练流程与Clip-GRPO算法实现。关键突破在于Clip-GRPO引入了无需标注、基于过程的接地奖励机制，不仅消除了密集标注需求，还通过强制视觉焦点与文本推理的实时跨模态一致性，规避了外部工具调用的不稳定性。在DriveLMM-o1数据集上的大量实验表明，相比基线模型Qwen2.5VL-7B，OmniDrive-R1将整体推理分数从51.77%提升至80.35%，最终答案准确率从37.81%提升至73.62%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the reliability issues, particularly object hallucination, that hinder Vision-Language Models (VLMs) in autonomous driving, which stem from ungrounded text-based reasoning. The authors propose OmniDrive-R1, an end-to-end framework that unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism, enhanced by a reinforcement-driven visual grounding capability that allows the model to autonomously focus on critical regions. Using a two-stage reinforcement learning pipeline with the novel Clip-GRPO algorithm and an annotation-free grounding reward, the model achieves significant improvements on the DriveLMM-o1 benchmark, raising the overall reasoning score from 51.77% to 80.35% and final answer accuracy from 37.81% to 73.62% compared to the Qwen2.5VL-7B baseline.</div>
<div class="mono" style="margin-top:8px">本文针对视觉语言模型在自动驾驶等安全关键领域因依赖无根据的文本推理而产生的可靠性问题，特别是物体幻觉问题，提出了解决方案。作者引入了OmniDrive-R1，这是一个端到端的框架，通过交错多模态思维链机制统一感知与推理，并利用强化学习驱动的视觉定位能力使模型能自主聚焦于关键区域进行细粒度分析。该方法采用两阶段强化学习流程和Clip-GRPO算法，无需密集标注，通过过程性定位奖励实现跨模态一致性。在DriveLMM-o1基准上的实验表明，相比基线模型Qwen2.5VL-7B，该模型将整体推理分数从51.77%提升至80.35%，最终答案准确率从37.81%提高至73.62%。</div>
</details>
</div>
<div class="card">
<div class="title">AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis</div>
<div class="meta-line">Authors: Cehua Yang, Dongyu Xiao, Junming Lin, Yuyang Song, Hanxu Yan, Shawn Guo, Wei Zhang, Jian Yang, Mingjie Tang, Bryan Dai</div>
<div class="meta-line">First: 2025-12-29T10:49:35+00:00 · Latest: 2025-12-29T10:49:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23366v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23366v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advancement of Text-to-SQL systems is currently hindered by the scarcity of high-quality training data and the limited reasoning capabilities of models in complex scenarios. In this paper, we propose a holistic framework that addresses these issues through a dual-centric approach. From a Data-Centric perspective, we construct an iterative data factory that synthesizes RL-ready data characterized by high correctness and precise semantic-logic alignment, ensured by strict verification. From a Model-Centric perspective, we introduce a novel Agentic Reinforcement Learning framework. This framework employs a Diversity-Aware Cold Start stage to initialize a robust policy, followed by Group Relative Policy Optimization (GRPO) to refine the agent&#x27;s reasoning via environmental feedback. Extensive experiments on BIRD and Spider benchmarks demonstrate that our synergistic approach achieves state-of-the-art performance among single-model methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AGRO-SQL：基于高保真数据合成的智能体群组相对优化方法</div>
<div class="mono" style="margin-top:8px">当前，文本到SQL系统的发展受限于高质量训练数据的稀缺以及模型在复杂场景下的推理能力不足。本文提出一种整体性框架，通过双中心路径解决这些问题。在数据中心层面，我们构建了迭代式数据工厂，合成具有高正确性和精确语义逻辑对齐的强化学习就绪数据，并通过严格验证确保质量。在模型中心层面，我们引入了新型智能体强化学习框架。该框架采用多样性感知冷启动阶段初始化稳健策略，随后通过群组相对策略优化（GRPO）利用环境反馈优化智能体推理能力。在BIRD和Spider基准上的大量实验表明，我们的协同方法在单模型方法中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the scarcity of high-quality training data and limited reasoning capabilities hindering Text-to-SQL systems. The proposed method is a holistic framework with a data-centric component that synthesizes verified, high-fidelity data, and a model-centric component featuring an agentic reinforcement learning framework using Diversity-Aware Cold Start and Group Relative Policy Optimization (GRPO) to refine reasoning. The main experimental results show that this synergistic approach achieves state-of-the-art performance on the BIRD and Spider benchmarks among single-model methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是高质量训练数据的稀缺以及模型在复杂场景下推理能力的不足，阻碍了文本到SQL系统的发展。其方法是一个整体框架，包含数据中心视角，通过迭代数据工厂合成经过严格验证的高保真数据；以及模型中心视角，引入一种新型智能体强化学习框架，采用多样性感知冷启动和组相对策略优化（GRPO）来优化推理。主要实验结果表明，该协同方法在BIRD和Spider基准测试中取得了单模型方法的最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning</div>
<div class="meta-line">Authors: Yilun Liu, Ziang Chen, Song Xu, Minggui He, Shimin Tao, Weibin Meng, Yuming Xie, Tao Han, Chunguang Zhao, Jingzhou Du, Daimeng Wei, Shenglin Zhang, Yongqian Sun</div>
<div class="meta-line">First: 2025-09-30T09:19:31+00:00 · Latest: 2025-12-29T10:05:56+00:00</div>
<div class="meta-line">Comments: Accepted by ICSE 2026 (SEIP Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25987v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25987v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing complexity of log data in modern software systems has prompted the use of Large Language Models (LLMs) for automated log analysis. Current approaches typically rely on direct supervised fine-tuning (SFT) on log-label pairs. However, this exacerbates the domain discrepancy between general-purpose LLMs and specialized log data, causing overfitting. Furthermore, SFT&#x27;s imbalanced loss computation often allows lengthy contexts to overwhelm critical, concise details in model answers, leading to hallucinations. To address these limitations, we propose R-Log, a novel reasoning-based paradigm that mirrors the structured, step-by-step analytical process of human engineers. This approach enhances generalizability by learning the underlying rules behind conclusions. We further employ Reinforcement Learning (RL) to optimize the model within a simulated O&amp;M environment, thereby reducing hallucinations by directly rewarding correct outcomes. R-Log is first cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13 strategies from manual O&amp;M practices, to establish an initial reasoning capability. This ability is then refined via RL using a joint reward function. Empirical evaluations on real-world logs show that R-Log outperforms existing methods across five log analysis tasks, particularly in unseen scenarios (by 228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the efficacy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>R-Log：基于推理的强化学习激励大语言模型的日志分析能力</div>
<div class="mono" style="margin-top:8px">现代软件系统中日志数据日益复杂，促使大语言模型（LLMs）被用于自动化日志分析。现有方法通常依赖于对日志-标签对的直接监督微调（SFT），但这加剧了通用LLMs与专业日志数据之间的领域差异，导致过拟合。此外，SFT的不平衡损失计算常使冗长上下文淹没模型答案中的关键简洁细节，引发幻觉。为应对这些局限，我们提出R-Log，一种新颖的基于推理的范式，模拟人类工程师结构化、逐步的分析过程。该方法通过学习结论背后的潜在规则增强泛化能力。我们进一步采用强化学习（RL）在模拟运维环境中优化模型，通过直接奖励正确结果减少幻觉。R-Log首先在包含2000多条推理轨迹的精选数据集上冷启动，借鉴13种人工运维策略以建立初始推理能力，随后通过联合奖励函数利用RL进行精炼。真实日志的实证评估表明，R-Log在五项日志分析任务中优于现有方法，尤其在未见场景下（提升228.05%）。我们还设计了提速5倍且保持93%效能的R-Log-fast版本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of supervised fine-tuning for log analysis with LLMs, which suffers from domain discrepancy and hallucinations due to imbalanced loss, this paper introduces R-Log, a reasoning-based reinforcement learning method that mimics human step-by-step analysis to learn underlying rules and improve generalizability. The method first cold-starts on a dataset of reasoning trajectories guided by operational strategies, then refines via RL with a joint reward function in a simulated environment to reduce hallucinations. Experimental results on real-world logs demonstrate that R-Log outperforms existing methods across five tasks, especially in unseen scenarios by 228.05%, and a faster variant, R-Log-fast, achieves a 5x speedup while retaining 93% efficacy.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在日志分析中直接监督微调导致的领域差异和幻觉问题，提出R-Log方法，该方法基于推理的强化学习，模拟人类工程师的逐步分析过程以学习底层规则并提升泛化能力。该方法首先在基于运维策略构建的推理轨迹数据集上进行冷启动，然后通过强化学习在模拟环境中使用联合奖励函数进行优化以减少幻觉。实验结果表明，R-Log在五个日志分析任务上优于现有方法，尤其在未见场景中性能提升228.05%，其快速变体R-Log-fast在保持93%效能的同时实现了5倍加速。</div>
</details>
</div>
<div class="card">
<div class="title">Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL</div>
<div class="meta-line">Authors: Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer</div>
<div class="meta-line">First: 2025-12-29T08:57:58+00:00 · Latest: 2025-12-29T08:57:58+00:00</div>
<div class="meta-line">Comments: 11 pages, 9 figures. Accepted by ACM for presentation at UCC &#x27;25 (18th International Conference on Utility and Cloud Computing), December 1-4, 2025, France. Proceedings publication pending</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23310v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23310v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying large language models (LLMs) on edge devices is challenging due to their limited memory and power resources. Cloud-only inference reduces device burden but introduces high latency and cost. Static edge-cloud partitions optimize a single metric and struggle when bandwidth fluctuates. We propose Splitwise, a novel Lyapunov-assisted deep reinforcement learning (DRL) framework for fine-grained, adaptive partitioning of LLMs across edge and cloud environments. Splitwise decomposes transformer layers into attention heads and feed-forward sub-blocks, exposing more partition choices than layer-wise schemes. A hierarchical DRL policy, guided by Lyapunov optimization, jointly minimizes latency, energy consumption, and accuracy degradation while guaranteeing queue stability under stochastic workloads and variable network bandwidth. Splitwise also guarantees robustness via partition checkpoints with exponential backoff recovery in case of communication failures. Experiments on Jetson Orin NX, Galaxy S23, and Raspberry Pi 5 with GPT-2 (1.5B), LLaMA-7B, and LLaMA-13B show that Splitwise reduces end-to-end latency by 1.4x-2.8x and cuts energy consumption by up to 41% compared with existing partitioners. It lowers the 95th-percentile latency by 53-61% relative to cloud-only execution, while maintaining accuracy and modest memory requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Splitwise：基于李雅普诺夫辅助深度强化学习的LLM协同边缘-云端推理框架</div>
<div class="mono" style="margin-top:8px">在边缘设备上部署大型语言模型（LLM）面临内存和功耗资源有限的挑战。纯云端推理虽减轻设备负担，但会引入高延迟与成本。静态边缘-云端划分方案仅优化单一指标，难以适应带宽波动。本文提出Splitwise——一种基于李雅普诺夫辅助深度强化学习（DRL）的新型框架，可实现LLM在边缘与云端环境的细粒度自适应划分。该框架将Transformer层解耦为注意力头与前馈子模块，提供比分层方案更丰富的划分选择。通过李雅普诺夫优化指导的分层DRL策略，在随机工作负载和动态网络带宽下联合优化延迟、能耗与精度损失，并保障队列稳定性。Splitwise还通过划分检查点与指数退避恢复机制保障通信故障时的鲁棒性。在Jetson Orin NX、Galaxy S23和树莓派5设备上，使用GPT-2（1.5B）、LLaMA-7B和LLaMA-13B的测试表明：相比现有划分器，Splitwise将端到端延迟降低1.4-2.8倍，能耗最高减少41%；相较于纯云端执行，其第95百分位延迟降低53-61%，同时保持精度并仅需适度内存开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of deploying large language models (LLMs) on resource-constrained edge devices and the high latency of cloud-only inference, this paper introduces Splitwise, a framework for adaptive edge-cloud model partitioning. The method employs a Lyapunov-assisted deep reinforcement learning (DRL) approach to perform fine-grained partitioning at the level of attention heads and feed-forward sub-blocks within transformer layers, jointly optimizing for latency, energy, and accuracy while ensuring queue stability under variable network conditions. Experimental results on devices including Jetson Orin NX and Raspberry Pi 5 with models like GPT-2 and LLaMA show that Splitwise reduces end-to-end latency by 1.4x-2.8x and energy consumption by up to 41% compared to existing partitioners, while also significantly lowering tail latency compared to cloud-only execution.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决大型语言模型在内存和功耗受限的边缘设备上部署的挑战，以及纯云推理的高延迟问题。方法上提出了Splitwise，一个基于李雅普诺夫辅助深度强化学习的框架，用于在边缘和云之间对Transformer层的注意力头和前馈子块进行细粒度自适应划分，在随机工作负载和可变带宽下联合优化延迟、能耗和精度，并保证队列稳定性。主要实验结果表明，在Jetson Orin NX和树莓派5等设备上使用GPT-2和LLaMA模型时，与现有划分方案相比，Splitwise将端到端延迟降低了1.4至2.8倍，能耗削减了高达41%，同时相比纯云执行显著降低了尾部延迟。</div>
</details>
</div>
<div class="card">
<div class="title">ZIA: A Theoretical Framework for Zero-Input AI</div>
<div class="meta-line">Authors: Aditi De</div>
<div class="meta-line">First: 2025-02-22T07:42:05+00:00 · Latest: 2025-12-29T07:12:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.16124v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.16124v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Input AI (ZIA) introduces a novel framework for human-computer interaction by enabling proactive intent prediction without explicit user commands. It integrates gaze tracking, bio-signals (EEG, heart rate), and contextual data (time, location, usage history) into a multi-modal model for real-time inference, targeting &lt;100 ms latency. The proposed architecture employs a transformer-based model with cross-modal attention, variational Bayesian inference for uncertainty estimation, and reinforcement learning for adaptive optimization. To support deployment on edge devices (CPUs, TPUs, NPUs), ZIA utilizes quantization, weight pruning, and linear attention to reduce complexity from quadratic to linear with sequence length. Theoretical analysis establishes an information-theoretic bound on prediction error and demonstrates how multi-modal fusion improves accuracy over single-modal approaches. Expected performance suggests 85-90% accuracy with EEG integration and 60-100 ms inference latency. ZIA provides a scalable, privacy-preserving framework for accessibility, healthcare, and consumer applications, advancing AI toward anticipatory intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ZIA：零输入人工智能的理论框架</div>
<div class="mono" style="margin-top:8px">零输入人工智能（ZIA）通过实现无需显式用户指令的主动意图预测，提出了一种新颖的人机交互框架。该框架整合视线追踪、生物信号（脑电图、心率）和情境数据（时间、位置、使用历史）至多模态模型中，以进行实时推理，目标延迟低于100毫秒。所提出的架构采用基于Transformer的跨模态注意力模型、用于不确定性估计的变分贝叶斯推理，以及用于自适应优化的强化学习。为支持在边缘设备（CPU、TPU、NPU）上部署，ZIA利用量化、权重剪枝和线性注意力机制，将复杂度从序列长度的二次方降至线性。理论分析建立了预测误差的信息论界限，并论证了多模态融合相较于单模态方法如何提升准确性。预期性能显示，整合脑电图数据时准确率达85-90%，推理延迟为60-100毫秒。ZIA为无障碍服务、医疗健康和消费应用提供了一个可扩展且保护隐私的框架，推动人工智能向预见性智能发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by advancing AI toward anticipatory intelligence without explicit user commands, this paper proposes the Zero-Input AI (ZIA) framework, which integrates gaze tracking, bio-signals like EEG and heart rate, and contextual data via a transformer-based model with cross-modal attention, variational Bayesian inference, and reinforcement learning for adaptive optimization. The method employs quantization, pruning, and linear attention to enable efficient edge deployment by reducing computational complexity from quadratic to linear. Experimental results indicate that multi-modal fusion improves prediction accuracy, achieving 85-90% accuracy with EEG integration while maintaining 60-100 ms inference latency, alongside theoretical bounds on prediction error, supporting applications in accessibility and healthcare.</div>
<div class="mono" style="margin-top:8px">本文旨在推动人工智能向无需显式用户指令的预知智能发展，提出了零输入人工智能（ZIA）框架，通过整合眼动追踪、脑电图和心率等生物信号以及上下文数据，采用基于Transformer的跨模态注意力模型、变分贝叶斯推理和强化学习进行自适应优化。该方法利用量化、权重剪枝和线性注意力技术降低计算复杂度，从二次降至线性，以实现高效的边缘设备部署。实验结果表明，多模态融合提高了预测准确性，结合脑电图时准确率达到85-90%，推理延迟保持在60-100毫秒，同时理论分析确立了预测误差的信息论界限，为无障碍和医疗等应用提供了可扩展且保护隐私的框架。</div>
</details>
</div>
<div class="card">
<div class="title">ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing</div>
<div class="meta-line">Authors: Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang</div>
<div class="meta-line">First: 2025-12-29T06:58:46+00:00 · Latest: 2025-12-29T06:58:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23244v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23244v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViLaCD-R1：面向遥感语义变化检测的视觉-语言框架</div>
<div class="mono" style="margin-top:8px">遥感变化检测（RSCD）是一项复杂的多图像推理任务，传统方法采用基于像素的算子或编码器-解码器网络，这些方法难以充分捕捉高层语义信息，且易受非语义干扰影响。尽管近期基于多模态和视觉语言模型（VLM）的方法通过引入文本描述增强了对变化区域的语义理解，但仍面临空间定位不准确、像素级边界划分不精确、可解释性有限等挑战。为解决这些问题，我们提出ViLaCD-R1——一个包含多图像推理器（MIR）与掩码引导解码器（MGD）的两阶段框架。具体而言，该框架通过监督微调（SFT）和强化学习（RL）在区块级双时相推理任务上训练VLM，以双时相图像块作为输入，输出粗粒度变化掩码。随后，解码器融合双时相图像特征与该粗掩码，生成精确的二元变化图。在多个RSCD基准测试上的综合评估表明，ViLaCD-R1显著提升了真实语义变化的识别与定位能力，有效抑制非语义变化，并在复杂现实场景中实现了最先进的检测精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional remote sensing change detection methods, which struggle with high-level semantics and non-semantic noise, and the shortcomings of recent vision-language models in spatial localization and boundary precision, this paper introduces ViLaCD-R1, a two-stage vision-language framework. The method employs a Multi-Image Reasoner, fine-tuned with supervised and reinforcement learning on block-level tasks to generate a coarse change mask from dual-temporal image patches, followed by a Mask-Guided Decoder that refines this mask with image features to produce a precise binary change map. Experimental results on multiple benchmarks show that the framework significantly enhances the recognition and localization of true semantic changes, effectively suppresses irrelevant variations, and achieves state-of-the-art accuracy in complex real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本文的动机在于传统遥感变化检测方法难以捕捉高层语义且易受非语义干扰，而近期基于视觉语言模型的方法又存在空间定位不准和边界模糊等问题。为此，论文提出了ViLaCD-R1这一两阶段视觉语言框架，其方法包含一个多图像推理器和一个掩码引导解码器：推理器通过监督微调和强化学习在块级任务上进行训练，以双时相图像块为输入生成粗变化掩码；解码器则融合双时相图像特征与该掩码来预测精确的二进制变化图。在多个基准测试上的实验结果表明，该框架显著提升了真实语义变化的识别与定位能力，鲁棒地抑制了非语义变化，并在复杂现实场景中取得了最先进的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search</div>
<div class="meta-line">Authors: Ziyang Zeng, Heming Jing, Jindong Chen, Xiangli Li, Hongyu Liu, Yixuan He, Zhengyu Li, Yige Sun, Zheyong Xie, Yuqing Yang, Shaosheng Cao, Jun Fan, Yi Wu, Yao Hu</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-11-30T16:31:16+00:00 · Latest: 2025-12-29T06:38:38+00:00</div>
<div class="meta-line">Comments: Accepted to the ADS Track at KDD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00968v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive offline evaluations and online A/B tests demonstrate that our approach consistently delivers significant improvements across key relevance and business metrics, validating its effectiveness, robustness, and practicality for large-scale industrial search systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小红书搜索中基于强化学习的生成式排序相关性优化</div>
<div class="mono" style="margin-top:8px">排序相关性是搜索引擎的核心任务，旨在识别与用户查询最相关的条目。传统相关性模型通常输出标量分数或直接预测相关性标签，限制了可解释性及复杂相关性信号的建模能力。受近期思维链推理在复杂任务中进展的启发，本研究探讨显式推理能否提升相关性建模的可解释性与性能。然而，现有基于推理的生成式相关性模型主要依赖大量人工标注或合成思维链数据的监督微调，其泛化能力有限。此外，领域无关的自由形式推理往往过于通用且缺乏事实依据，难以处理开放域搜索中普遍存在的多样性与模糊性案例。本研究将小红书搜索中的相关性建模构建为推理任务，并提出基于强化学习的训练框架以增强生成式相关性模型的依据性推理能力。具体而言，我们将实际业务特定的相关性标准融入多步推理提示设计，并提出轻量级过程监督策略——逐步优势掩码，通过改进信用分配机制促进对这些标准的高效学习。为实现工业部署，我们将大规模强化学习调优模型蒸馏为适用于实际搜索系统的轻量版本。大量离线评估与在线A/B测试表明，该方法在关键相关性指标和业务指标上均取得显著提升，验证了其在大规模工业搜索系统中的有效性、鲁棒性与实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional scalar-based relevance models and the poor generalization of supervised generative relevance models, this work introduces a reinforcement learning framework to enhance grounded reasoning for search relevance in Xiaohongshu. The method formulates relevance as a reasoning task, incorporating business-specific criteria into multi-step prompts and employing a Stepwise Advantage Masking strategy for improved process supervision and credit assignment, followed by model distillation for deployment. Experimental results from offline evaluations and online A/B tests show significant improvements in key relevance and business metrics, demonstrating the approach&#x27;s effectiveness and practicality for industrial systems.</div>
<div class="mono" style="margin-top:8px">本研究针对传统相关性模型可解释性差、以及基于监督学习的生成式相关性模型泛化能力不足的问题，提出了一种强化学习框架来增强小红书搜索中相关性建模的 grounded 推理能力。该方法将相关性建模构建为推理任务，将业务特定的相关性标准融入多步推理提示设计，并提出了逐步优势掩码这一轻量级过程监督策略以优化信用分配，随后通过模型蒸馏实现工业部署。离线和在线A/B测试结果表明，该方法在关键的相关性和业务指标上均取得了显著提升，验证了其在大规模工业搜索系统中的有效性、鲁棒性和实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Parameter Efficient Methods for RLVR</div>
<div class="meta-line">Authors: Qingyu Yin, Yulun Wu, Zhennan Shen, Sunbowen Li, Zhilin Wang, Yanshu Li, Chak Tou Leong, Jiale Kang, Jinjin Gu</div>
<div class="meta-line">First: 2025-12-29T03:13:08+00:00 · Latest: 2025-12-29T03:13:08+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23165v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估RLVR的参数高效方法</div>
<div class="mono" style="margin-top:8px">我们在可验证奖励强化学习（RLVR）范式下，系统评估了参数高效微调（PEFT）方法。RLVR通过可验证反馈激励语言模型提升推理能力；然而，尽管LoRA等方法被广泛使用，但适用于RLVR的最佳PEFT架构仍未明确。本研究首次在数学推理基准上，对DeepSeek-R1-Distill系列模型进行了超过12种PEFT方法的全面评估。实证结果对默认采用标准LoRA的做法提出挑战，主要发现有三点：首先，结构变体（如DoRA、AdaLoRA、MiSS）持续优于LoRA；其次，我们揭示了基于SVD的初始化策略（如PiSSA、MiLoRA）存在谱崩溃现象，其失败归因于主成分更新与RL优化的根本性错配；此外，消融实验表明极端参数削减（如VeRA、Rank-1）会严重制约推理能力。我们通过消融研究与规模实验进一步验证了结论。本研究为倡导参数高效RL方法的深入探索提供了权威指南。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to identify optimal Parameter-Efficient Fine-Tuning (PEFT) methods for Reinforcement Learning with Verifiable Rewards (RLVR), a paradigm that uses verifiable feedback to enhance language model reasoning, as the default choice of LoRA remains unverified. The method involves a systematic evaluation of over 12 PEFT techniques, including structural variants like DoRA and SVD-based methods like PiSSA, applied to the DeepSeek-R1-Distill model family on mathematical reasoning benchmarks. The main experimental results reveal that structural variants such as DoRA, AdaLoRA, and MiSS consistently outperform standard LoRA, while SVD-informed methods suffer from spectral collapse due to misalignment with RL optimization, and extreme parameter reduction methods like VeRA severely impair reasoning capacity, challenging the default use of LoRA and advocating for further exploration in parameter-efficient RL.</div>
<div class="mono" style="margin-top:8px">本文的研究动机源于需要为基于可验证奖励的强化学习（RLVR）这一利用可验证反馈增强语言模型推理能力的范式，确定最优的参数高效微调方法，因为默认使用的LoRA方法尚未得到验证。研究方法是对超过12种参数高效微调技术（包括DoRA等结构变体和PiSSA等基于奇异值分解的方法）在DeepSeek-R1-Distill模型系列上，针对数学推理基准进行系统评估。主要实验结果表明，DoRA、AdaLoRA和MiSS等结构变体持续优于标准LoRA，而基于奇异值分解的方法因与强化学习优化的根本性不匹配而出现谱崩溃现象，VeRA等极端参数削减方法则严重制约推理能力，这些发现挑战了LoRA的默认地位，并呼吁对参数高效的强化学习方法进行更多探索。</div>
</details>
</div>
<div class="card">
<div class="title">Scaled-Dot-Product Attention as One-Sided Entropic Optimal Transport</div>
<div class="meta-line">Authors: Elon Litman</div>
<div class="meta-line">First: 2025-08-11T18:00:17+00:00 · Latest: 2025-12-29T01:41:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08369v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.08369v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The scaled-dot-product attention (SDPA) mechanism is a core component of modern deep learning, but its mathematical form is often motivated by heuristics. This work provides a first-principles justification for SDPA. We first show that the attention forward pass is the exact solution to a degenerate, one-sided Entropic Optimal Transport (EOT) problem, which seeks a distribution that maximizes similarity while being maximally entropic. This optimization perspective has a direct consequence for the backward pass. We prove that the standard gradient computed via backpropagation is mathematically identical to an advantage-based policy gradient, a variance-reduced update rule from reinforcement learning. Crucially, we demonstrate that the EOT formulation of the forward pass induces a specific information geometry on the space of attention distributions. It is this geometry, characterized by the Fisher Information Matrix, that dictates the precise form of the learning gradient, revealing the advantage-based update as a natural consequence of the optimization problem being solved. This unified view reveals SDPA as a principled mechanism where the forward pass performs optimal inference and the backward pass implements a rational, manifold-aware learning update.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缩放点积注意力作为单侧熵最优传输</div>
<div class="mono" style="margin-top:8px">缩放点积注意力机制是现代深度学习的核心组件，但其数学形式常基于启发式推导。本研究为SDPA提供了第一性原理的论证。我们首先证明注意力前向传播是一个退化单侧熵最优运输问题的精确解，该问题寻求在最大化熵的同时最大化相似性的分布。这一优化视角对反向传播具有直接影响：我们证明通过反向传播计算的标准梯度在数学上等同于基于优势的策略梯度——一种强化学习中方差缩减的更新规则。关键在于，我们证明了前向传播的EOT公式在注意力分布空间上诱导出特定的信息几何结构。正是这种以费舍尔信息矩阵为特征的几何结构决定了学习梯度的精确形式，揭示出基于优势的更新是所求解优化问题的自然结果。这一统一视角表明SDPA是一种原理性机制：前向传播执行最优推断，反向传播则实现理性的流形感知学习更新。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work provides a first-principles justification for the scaled-dot-product attention (SDPA) mechanism by showing its forward pass is the exact solution to a degenerate, one-sided entropic optimal transport problem, which maximizes similarity and entropy. The derived optimization perspective reveals that the standard backpropagation gradient is mathematically identical to an advantage-based policy gradient from reinforcement learning, and the induced information geometry, characterized by the Fisher Information Matrix, dictates this specific learning update. Consequently, SDPA is unified as a principled mechanism where the forward pass performs optimal inference and the backward pass implements a manifold-aware learning update.</div>
<div class="mono" style="margin-top:8px">本研究为缩放点积注意力机制提供了第一性原理的证明，表明其前向传播是退化单侧熵最优传输问题的精确解，该问题旨在最大化相似性和熵。由此优化视角揭示，标准的反向传播梯度在数学上等同于强化学习中基于优势的策略梯度，且由费舍尔信息矩阵刻画的诱导信息几何决定了这一特定学习更新形式。因此，缩放点积注意力被统一为一个原则性机制，其中前向传播执行最优推断，而后向传播实现一种流形感知的学习更新。</div>
</details>
</div>
<div class="card">
<div class="title">A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms</div>
<div class="meta-line">Authors: Yingru Li, Ziniu Li, Jiacai Liu</div>
<div class="meta-line">First: 2025-12-28T22:25:27+00:00 · Latest: 2025-12-28T22:25:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23097v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23097v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于大语言模型混合在线强化与模仿学习的注记：形式化与算法</div>
<div class="mono" style="margin-top:8px">本文提出一个统一的大语言模型微调框架，整合了模仿学习与强化学习。通过分析结合轨迹级KL散度与任务奖励的复合目标梯度，我们推导出两个自然分解的组成部分：(1) 用于词元级模仿的解析可计算稠密梯度，(2) 用于长程奖励优化的蒙特卡洛估计稀疏梯度。该稠密梯度具有闭式的逻辑值层面计算公式，可实现高效的GPU部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient fine-tuning of Large Language Models (LLMs) that balances behavior cloning with reward-driven optimization, this paper presents a unified framework integrating Imitation Learning and Reinforcement Learning. The method formulates a composite objective combining trajectory-level KL divergence and task rewards, whose gradient decomposes analytically into a dense component for token-level imitation and a sparse component for long-horizon reward optimization, with the dense gradient enabling a closed-form, GPU-efficient implementation. The main experimental results, implied by the analytical derivation, demonstrate that this decomposition allows for stable and computationally efficient fine-tuning by separately handling immediate imitation and sparse reward signals.</div>
<div class="mono" style="margin-top:8px">本文的动机是寻求一种高效的大型语言模型（LLM）微调方法，以平衡行为克隆与奖励驱动的优化，为此提出了一个统一模仿学习与强化学习的框架。该方法构建了一个结合轨迹级KL散度与任务奖励的复合目标函数，通过对其梯度进行分析，自然地分解为两个部分：用于词元级模仿的、可解析计算的稠密梯度，以及用于长程奖励优化的、通过蒙特卡洛估计的稀疏梯度，其中稠密梯度具有闭式的逻辑值级公式，支持高效的GPU实现。主要的实验结果，基于理论推导所暗示，表明这种分解能够通过分别处理即时模仿信号与稀疏奖励信号，实现稳定且计算高效的微调。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</div>
<div class="meta-line">Authors: Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa</div>
<div class="meta-line">First: 2025-12-28T21:57:42+00:00 · Latest: 2025-12-28T21:57:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23090v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基准测试成功，临床应用失败：当强化学习为基准而非患者优化时</div>
<div class="mono" style="margin-top:8px">近期针对大语言模型的强化学习进展提升了推理任务性能，但其在资源受限的医学影像领域的应用仍待探索。我们提出ChexReason——一个通过R1风格方法（先监督微调后GRPO）训练的视觉语言模型，仅使用2000个SFT样本、1000个RL样本和单张A100 GPU。在CheXpert和NIH基准上的评估揭示了根本性矛盾：GRPO恢复了分布内性能（CheXpert提升23%，宏观F1=0.346），但削弱了跨数据集迁移能力（NIH下降19%）。这与NV-Reason-CXR-3B等高资源模型表现一致，表明问题源于RL范式而非模型规模。我们发现泛化悖论：SFT检查点在优化前能独特提升NIH性能，说明教师引导的推理能捕获更多机构无关特征。跨模型比较显示，结构化推理框架对通用视觉语言模型有益，但对医学预训练模型增益有限。因此，对于需要跨人群鲁棒性的临床部署，精心设计的监督微调可能优于激进的强化学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the tension between benchmark performance and clinical robustness when applying reinforcement learning (RL) to medical vision-language models. The authors introduce ChexReason, a model trained with a resource-efficient R1-style method (supervised fine-tuning followed by GRPO) using minimal data and compute. Their experiments reveal a generalization paradox: while RL optimization significantly improves in-distribution performance on the CheXpert benchmark (23% improvement), it degrades cross-dataset transferability to the NIH dataset (19% drop), a pattern also observed in larger models. The findings suggest that aggressive RL may overfit to benchmark specifics, whereas the initial supervised fine-tuning checkpoint captures more institution-agnostic features, indicating that curated supervised approaches might be preferable for clinical deployment requiring robustness across diverse populations.</div>
<div class="mono" style="margin-top:8px">本文探讨了将强化学习应用于医学视觉语言模型时，基准测试性能与临床鲁棒性之间的冲突。作者提出了ChexReason模型，采用资源高效的R1风格方法（监督微调后接GRPO），使用极少的数据和算力进行训练。实验揭示了一个泛化悖论：虽然强化学习优化显著提升了在CheXpert基准上的分布内性能（改进23%），却损害了向NIH数据集的跨数据集可迁移性（下降19%），这一模式在更大模型中也存在。研究结果表明，激进的强化学习可能导致对基准特定细节的过拟合，而初始的监督微调检查点能捕捉更多与机构无关的特征，这意味着对于需要跨不同人群鲁棒性的临床部署，精心设计的监督方法可能优于激进的强化学习。</div>
</details>
</div>
<div class="card">
<div class="title">Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning</div>
<div class="meta-line">Authors: Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang</div>
<div class="meta-line">First: 2025-12-28T21:44:07+00:00 · Latest: 2025-12-28T21:44:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23087v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23087v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe&#x27;&#x27; vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>驯服长尾分布：通过动态词汇剪枝实现稳定的大语言模型强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的强化学习面临一个根本性矛盾：高吞吐量推理引擎与数值精确的训练系统会从相同参数产生不同的概率分布，造成训练-推理失配。我们证明这种失配具有不对称效应：对数概率失配的边界按$(1-p)$缩放（其中$p$为词元概率）。对于高概率词元，该边界趋近于零，对序列级失配的贡献可忽略；而对于长尾分布中的低概率词元，边界保持较大值，且当这些词元被采样时，会呈现系统性偏差失配，在序列中不断累积，导致梯度估计失稳。我们提出不依赖事后修正，而是将强化学习目标约束在动态剪枝的“安全”词汇表内（排除极端长尾词元）。通过剪除此类词元，我们将大规模系统性偏差失配转化为有界的小幅优化偏差。实验表明，该方法能实现稳定训练；理论上，我们界定了词汇剪枝引入的优化偏差范围。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the training-inference mismatch in reinforcement learning for large language models, where differences in probability distributions between training and inference systems destabilize learning, particularly due to systematic biases from low-probability tail tokens. The method introduces dynamic vocabulary pruning to exclude extreme tail tokens from the RL objective, trading large biased mismatches for a small bounded optimization bias. Experimental results show that this approach achieves stable training, with theoretical bounds provided on the introduced optimization bias.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习中的训练-推理不匹配问题，该问题源于训练和推理系统产生的概率分布差异，尤其由低概率尾部令牌的系统性偏差引起，导致学习不稳定。方法采用动态词汇剪枝，将极端尾部令牌从强化学习目标中排除，以小的有界优化偏差替代大的系统性偏差。实验结果表明，该方法实现了稳定的训练，并从理论上对引入的优化偏差进行了界定。</div>
</details>
</div>
<div class="card">
<div class="title">Trust Region Masking for Long-Horizon LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang</div>
<div class="meta-line">First: 2025-12-28T20:41:59+00:00 · Latest: 2025-12-28T20:41:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23075v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长视野大语言模型强化学习的信任区域掩码方法</div>
<div class="mono" style="margin-top:8px">针对大语言模型的策略梯度方法通过采样滚动策略$π_{\text{roll}}$计算代理目标进行优化。当$π_{\text{roll}} \ne π_θ$时，代理目标与真实目标间存在近似误差。先前研究表明，由于实现差异、专家混合路由不连续性及分布式训练滞后性，这种离策略失配在现代LLM-RL中不可避免。经典信任区域误差界随序列长度$T$呈$O(T^2)$增长，导致其在长视野任务中失效。本文推导出两个更紧致的误差界：按$O(T^{3/2})$缩放的Pinsker-边际界和按$O(T)$缩放的混合界。关键的是，这两个误差界均依赖于$D_{kl}^{tok,max}$——序列中所有位置的最大词元级KL散度。这是本质上的序列级量值：需检查完整轨迹才能计算，因此无法通过PPO裁剪等词元无关方法控制。我们提出信任区域掩码（TRM）方法，若序列中任意词元违反信任区域则将该序列完全排除在梯度计算之外，从而为长视野LLM-RL首次提供非空泛的单调改进保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the unavoidable off-policy mismatch in large language model reinforcement learning (LLM-RL), caused by factors like implementation divergence and distributed training staleness, where classical trust region bounds become vacuous for long-horizon tasks due to their quadratic scaling with sequence length. The method introduces two tighter theoretical bounds—a Pinsker-Marginal bound scaling as O(T^{3/2}) and a Mixed bound scaling as O(T)—both dependent on the maximum token-level KL divergence across sequences, and proposes Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region. The main experimental results demonstrate that TRM provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL, effectively controlling sequence-level divergence where token-independent methods like PPO clipping fail.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大语言模型强化学习（LLM-RL）中不可避免的离策略不匹配问题，该问题由实现差异和分布式训练滞后等因素引起，导致传统信任域界限因随序列长度呈二次方增长而在长视野任务中失效。方法上，本文推导出两个更紧的理论界限——一个按O(T^{3/2})缩放的Pinsker-Marginal界限和一个按O(T)缩放的混合界限，两者均依赖于序列中所有位置的最大令牌级KL散度，并提出了信任域掩码（TRM）方法，该方法在任一令牌违反信任域时排除整个序列的梯度计算。主要实验结果表明，TRM为长视野LLM-RL提供了首个非空单调改进保证，有效控制了序列级散度，而像PPO裁剪这样的令牌无关方法则无法做到这一点。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
