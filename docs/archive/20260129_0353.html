<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-29 03:53</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260129_0353</div>
    <div class="row"><div class="card">
<div class="title">Self-Distillation Enables Continual Learning</div>
<div class="meta-line">Authors: Idan Shenfeld, Mehul Damani, Jonas Hübotter, Pulkit Agrawal</div>
<div class="meta-line">First: 2026-01-27T18:59:08+00:00 · Latest: 2026-01-27T18:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19897v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19897v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自蒸馏实现持续学习</div>
<div class="mono" style="margin-top:8px">持续学习旨在使模型能够获取新技能与知识而不损害现有能力，这始终是基础模型面临的核心挑战。虽然在线策略强化学习可缓解遗忘问题，但其依赖的显式奖励函数往往难以获取。基于专家示范的主流替代方案——监督微调（SFT）本质上属于离线策略。本文提出自蒸馏微调（SDFT），这是一种可直接从示范中进行在线策略学习的简洁方法。SDFT通过将示范条件化模型作为自身的教师，利用上下文学习生成在线策略训练信号，从而在掌握新技能的同时保持原有能力。在技能学习与知识获取任务中，SDFT持续优于SFT，在显著降低灾难性遗忘的同时获得更高的新任务准确率。序列学习实验表明，SDFT能使单一模型随时间累积多项技能且无性能衰退，为基于示范的持续学习确立了在线策略蒸馏的实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of continual learning in foundation models, where acquiring new skills often leads to catastrophic forgetting of prior knowledge. The proposed method, Self-Distillation Fine-Tuning (SDFT), enables on-policy learning from demonstrations by using a demonstration-conditioned model as its own teacher to generate training signals, thus avoiding the need for explicit rewards. Experimental results across skill learning and knowledge acquisition tasks show that SDFT consistently outperforms supervised fine-tuning, achieving higher new-task accuracy while significantly reducing forgetting, and allows a single model to accumulate multiple skills over time without performance regression.</div>
<div class="mono" style="margin-top:8px">本文针对基础模型持续学习中的挑战，即学习新技能时常导致对先前知识的灾难性遗忘。提出的方法自蒸馏微调（SDFT）通过使用演示条件模型作为自身教师来生成训练信号，实现了直接从演示中进行策略上学习，从而避免了显式奖励需求。在技能学习和知识获取任务上的实验结果表明，SDFT持续优于监督微调，在获得更高新任务准确率的同时显著减少了遗忘，并使单一模型能够随时间积累多项技能而无需性能回退。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</div>
<div class="meta-line">Authors: Jinyeop Song, Song Wang, Julian Shun, Yada Zhu</div>
<div class="meta-line">First: 2025-09-30T15:14:24+00:00 · Latest: 2026-01-27T17:44:43+00:00</div>
<div class="meta-line">Comments: Wrong numbers are reported for main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26383v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.26383v4">PDF</a> · <a href="https://github.com/Jinyeop3110/KG-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的高效可迁移智能体知识图谱检索增强生成</div>
<div class="mono" style="margin-top:8px">知识图谱检索增强生成（KG-RAG）将大语言模型（LLMs）与结构化、可验证的知识图谱（KGs）结合，以减少幻觉并展示推理过程。然而，许多KG-RAG系统由多个LLM模块（如规划、推理和响应）组成，导致推理成本增加且行为受限于特定目标KG。为此，我们提出KG-R1，一种通过强化学习（RL）实现的智能体化KG检索增强生成框架。KG-R1采用单一智能体与KGs作为环境交互，学习在每一步进行检索，并将检索信息融入其推理和生成过程，通过端到端RL进行优化。在知识图谱问答（KGQA）基准的受控实验中，我们的方法展现出高效性和可迁移性：使用Qwen-2.5-3B模型时，KG-R1以更少的生成标记数提升了答案准确性，优于先前使用更大基础模型或微调模型的多模块工作流方法。此外，KG-R1支持即插即用：训练后无需修改即可在新KGs上保持高准确率。这些特性使KG-R1成为实际部署中极具前景的KG-RAG框架。代码已开源：https://github.com/Jinyeop3110/KG-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the high inference costs and lack of transferability in multi-module KG-RAG systems. The method introduces KG-R1, a framework that employs a single reinforcement learning agent to interact with knowledge graphs, learning to retrieve and integrate information for end-to-end optimized reasoning and generation. Main experimental results on KGQA benchmarks show that KG-R1, using a compact 3B model, achieves higher answer accuracy with fewer tokens than prior multi-module methods using larger models, and demonstrates strong transferability to new knowledge graphs without modification.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多模块知识图谱检索增强生成系统推理成本高和缺乏可迁移性的问题。方法上提出了KG-R1框架，它采用单一的强化学习智能体与知识图谱交互，学习检索并整合信息，以进行端到端优化的推理和生成。在知识图谱问答基准上的主要实验结果表明，使用紧凑的30亿参数模型的KG-R1，相比先前使用更大模型的多模块方法，以更少的生成标记实现了更高的答案准确率，并且无需修改即可在新知识图谱上保持强大的准确性，展现了良好的可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals</div>
<div class="meta-line">Authors: Octavio Pappalardo</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T17:10:29+00:00 · Latest: 2026-01-27T17:10:29+00:00</div>
<div class="meta-line">Comments: To appear at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19810v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19810v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent&#x27;s post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent&#x27;s capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效探索的无监督学习：通过自设目标预训练自适应策略</div>
<div class="mono" style="margin-top:8px">无监督预训练可为强化学习智能体提供先验知识，加速下游任务学习。基于人类发展机制的研究方向探索智能体通过设定并追求自设目标进行学习，其核心挑战在于如何有效生成、选择并从中学习。本文聚焦于下游任务分布广泛、无法零样本解决所有任务的情境，例如目标任务超出预训练分布或智能体未知任务具体信息。本研究（i）在元学习框架内优化多轮次探索与适应效率，（ii）通过动态评估智能体适应后表现来指导训练课程。我们提出ULEE方法，一种结合上下文学习器与对抗性目标生成策略的无监督元学习方法，使训练持续处于智能体能力前沿。在XLand-MiniGrid基准测试中，ULEE预训练展现出可泛化至新目标、环境动态及地图结构的探索与适应能力，所获策略在零样本与小样本场景中表现更优，并为长时微调提供优质初始化，其性能优于从头学习、DIAYN预训练及其他课程设置方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reinforcement learning agents to adapt efficiently to broad downstream task distributions where zero-shot solving is infeasible, this paper introduces ULEE, an unsupervised meta-learning method. The approach optimizes for multi-episode exploration and adaptation by combining an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent&#x27;s capabilities, guided by evolving performance estimates. Experimental results on XLand-MiniGrid benchmarks demonstrate that ULEE pre-training yields improved exploration and adaptation abilities, generalizing to novel objectives, dynamics, and structures, and outperforming learning from scratch, DIAYN, and alternative curricula in zero-shot, few-shot, and fine-tuning performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是强化学习智能体需要高效适应广泛的下游任务分布，其中零样本求解不可行，因此提出了无监督元学习方法ULEE。该方法通过结合上下文学习器和对抗性目标生成策略来优化多回合探索与适应，该策略依据性能估计引导训练保持在智能体能力前沿。在XLand-MiniGrid基准测试中，实验结果表明ULEE预训练提升了探索与适应能力，能泛化至新目标、环境动态和地图结构，并在零样本、少样本和微调性能上优于从头学习、DIAYN预训练及其他课程学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Reimagining Peer Review Process Through Multi-Agent Mechanism Design</div>
<div class="meta-line">Authors: Ahmad Farooq, Kamran Iqbal</div>
<div class="meta-line">First: 2026-01-27T16:43:11+00:00 · Latest: 2026-01-27T16:43:11+00:00</div>
<div class="meta-line">Comments: To appear in the Proceedings of the 2026 IEEE/ACM 48th International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE). 4 pages, 1 figure, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as &quot;broken.&quot; This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多智能体机制设计重构同行评审流程</div>
<div class="mono" style="margin-top:8px">软件工程研究界面临系统性危机：随着投稿量激增、激励错配及评审者疲劳，同行评审正逐渐失效。社区调查显示，研究人员普遍认为该流程已“崩溃”。本立场论文指出，这些功能障碍本质上是机制设计缺陷，可通过计算方案解决。我们提出将研究社区建模为随机多智能体系统，并应用多智能体强化学习来设计激励相容的协议。我们概述三项干预措施：基于信用的投稿经济体系、MARL优化的评审分配机制，以及评审一致性的混合验证方法。文中还提出了威胁模型、公平性考量及分阶段试点评估指标。这一愿景为构建可持续的同行评审体系规划了研究路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by a perceived systemic crisis in software engineering peer review, characterized by overwhelming submissions, misaligned incentives, and reviewer fatigue, this position paper proposes a computational solution. The method involves modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design new incentive-compatible protocols, including a credit-based submission economy and optimized reviewer assignment. The main experimental results are not empirical findings but rather the presentation of a structured research agenda, complete with threat models, equity considerations, and proposed pilot metrics, to guide future work toward a more sustainable review process.</div>
<div class="mono" style="margin-top:8px">本文的动机源于软件工程领域同行评审所面临的系统性危机，包括投稿量激增、激励错配和评审疲劳等问题。其方法是将研究社区建模为一个随机多智能体系统，并应用多智能体强化学习来设计激励相容的新协议，例如基于信用的投稿经济机制和优化评审分配。主要的实验成果并非实证数据，而是提出了一个结构化的研究议程，包括威胁模型、公平性考量以及分阶段试点指标，旨在为构建更可持续的评审体系规划未来研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Activation Function Design Sustains Plasticity in Continual Learning</div>
<div class="meta-line">Authors: Lute Lillo, Nick Cheney</div>
<div class="meta-line">First: 2025-09-26T16:41:47+00:00 · Latest: 2026-01-27T16:19:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22562v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22562v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In independent, identically distributed (i.i.d.) training regimes, activation functions have been benchmarked extensively, and their differences often shrink once model size and optimization are tuned. In continual learning, however, the picture is different: beyond catastrophic forgetting, models can progressively lose the ability to adapt (referred to as loss of plasticity) and the role of the non-linearity in this failure mode remains underexplored. We show that activation choice is a primary, architecture-agnostic lever for mitigating plasticity loss. Building on a property-level analysis of negative-branch shape and saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leaky and Randomized Smooth-Leaky) and evaluate them in two complementary settings: (i) supervised class-incremental benchmarks and (ii) reinforcement learning with non-stationary MuJoCo environments designed to induce controlled distribution and dynamics shifts. We also provide a simple stress protocol and diagnostics that link the shape of the activation to the adaptation under change. The takeaway is straightforward: thoughtful activation design offers a lightweight, domain-general way to sustain plasticity in continual learning without extra capacity or task-specific tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>激活函数设计维持持续学习中的可塑性</div>
<div class="mono" style="margin-top:8px">在独立同分布训练模式下，激活函数已得到广泛基准测试，一旦调整模型规模和优化策略，其差异往往缩小。然而在持续学习中情况不同：除了灾难性遗忘，模型可能逐渐丧失适应能力（称为可塑性丧失），非线性在此失效模式中的作用仍未充分探索。我们证明激活函数选择是缓解可塑性丧失的主要且与架构无关的调控手段。基于对负分支形态和饱和行为的特性分析，我们提出两种即插即用非线性函数（平滑泄漏型与随机平滑泄漏型），并在两个互补场景中评估：（1）监督式类增量基准测试；（2）采用非平稳MuJoCo环境的强化学习，该环境设计用于引发受控分布与动态偏移。我们还提供简易压力测试协议和诊断方法，将激活函数形态与变化适应能力相关联。核心结论明确：经过深思熟虑的激活函数设计，无需额外容量或任务特定调优，即可为持续学习提供轻量级、跨领域的可塑性维持方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of activation functions in mitigating the loss of plasticity—a progressive decline in a model&#x27;s ability to adapt—in continual learning, where standard i.i.d. benchmarks show minimal differences. The motivation stems from the underexplored impact of non-linearities on this failure mode beyond catastrophic forgetting. The method involves a property-level analysis of activation shapes, leading to the introduction of two novel drop-in nonlinearities, Smooth-Leaky and Randomized Smooth-Leaky, designed to sustain plasticity. Experimental results in supervised class-incremental benchmarks and reinforcement learning with non-stationary MuJoCo environments demonstrate that these activations effectively maintain adaptation under distribution and dynamics shifts, offering a lightweight, architecture-agnostic solution without requiring extra capacity or task-specific tuning.</div>
<div class="mono" style="margin-top:8px">本文研究了激活函数在持续学习中缓解可塑性丧失的作用，即模型适应能力逐渐下降的问题，而在标准的独立同分布基准测试中激活函数差异很小。其动机在于非线性特性对这一失败模式的影响尚未被充分探索，超越了灾难性遗忘。方法包括对激活函数形状进行属性级分析，从而引入了两种新颖的即插即用非线性函数：平滑泄漏和随机平滑泄漏，旨在维持可塑性。在监督式类增量基准测试以及具有非平稳MuJoCo环境的强化学习中的实验结果表明，这些激活函数在分布和动态变化下能有效保持适应能力，提供了一种轻量级、与架构无关的解决方案，无需额外容量或任务特定调整。</div>
</details>
</div>
<div class="card">
<div class="title">Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning</div>
<div class="meta-line">Authors: David Bani-Harouni, Chantal Pellegrini, Ege Özsoy, Matthias Keicher, Nassir Navab</div>
<div class="meta-line">First: 2025-06-16T13:32:01+00:00 · Latest: 2026-01-27T16:05:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.13474v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.13474v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited &quot;out-of-the-box&quot; capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的假设驱动临床决策语言智能体</div>
<div class="mono" style="margin-top:8px">临床决策是一个动态、交互且循环的过程，医生需反复决定执行何种临床操作，并依据新发现的信息进行诊断与治疗。大语言模型（LLMs）具备辅助临床医生完成这一过程的潜力，但当前LLMs在临床决策支持中的应用大多存在以下局限：要么假设所有患者信息可即时获取（这一场景不切实际），未对交互式迭代调查过程进行建模；要么仅依赖大型预训练模型的有限“开箱即用”能力，未进行任务特异性训练。为此，我们提出一种假设驱动、具备不确定性感知的语言智能体LA-CDM，通过循环请求并解读相关检测，逐步收敛至最终诊断，以此建模临床诊断决策过程。采用监督学习与强化学习结合的混合训练范式，我们围绕临床决策的三个关键目标训练LA-CDM：准确生成假设、评估假设不确定性、实现高效决策。我们在真实世界数据集MIMIC-CDM（涵盖四种腹部疾病及多样临床检测）上评估该方法，结果表明显式训练临床决策能有效提升诊断性能与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current Large Language Model (LLM) applications in clinical decision support, which either assume all patient data is immediately available or rely solely on pre-trained models without task-specific training. The authors propose LA-CDM, a hypothesis-driven and uncertainty-aware language agent that models the iterative diagnostic process by repeatedly requesting and interpreting relevant clinical tests. Using a hybrid training approach combining supervised and reinforcement learning with objectives for hypothesis generation, uncertainty estimation, and decision efficiency, the method is evaluated on the real-world MIMIC-CDM dataset for abdominal diseases. Experimental results demonstrate that explicitly training for clinical decision-making improves both diagnostic accuracy and efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型语言模型在临床决策支持中的应用局限性，即要么假设所有患者信息立即可用，要么仅依赖预训练模型而缺乏任务特定训练。作者提出了LA-CDM，一种假设驱动且具有不确定性感知的语言智能体，通过反复请求和解读相关临床检验来模拟迭代诊断过程。该方法采用结合监督学习和强化学习的混合训练范式，针对假设生成、不确定性估计和决策效率三个目标进行训练，并在涵盖四种腹部疾病的真实世界数据集MIMIC-CDM上进行评估。实验结果表明，针对临床决策过程进行显式训练能有效提高诊断性能和效率。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models</div>
<div class="meta-line">Authors: David Bani-Harouni, Chantal Pellegrini, Paul Stangel, Ege Özsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab</div>
<div class="meta-line">First: 2025-03-04T13:48:50+00:00 · Latest: 2026-01-27T16:03:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.02623v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.02623v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We propose a novel Reinforcement Learning approach that allows to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to factual questions. Our method optimizes a reward based on the logarithmic scoring rule, explicitly penalizing both over- and under-confidence. This encourages the model to align its confidence estimates with the actual predictive accuracy. The optimal policy under our reward design would result in perfectly calibrated confidence expressions. Unlike prior approaches that decouple confidence estimation from response generation, our method integrates confidence calibration seamlessly into the generative process of the LLM. Empirically, we demonstrate that models trained with our approach exhibit substantially improved calibration and generalize to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励质疑：基于强化学习的大语言模型校准置信度表达方法</div>
<div class="mono" style="margin-top:8px">安全可信地使用大语言模型（LLMs）需要对其答案的置信度进行准确表达。我们提出一种新颖的强化学习方法，可直接微调LLMs使其在回答事实性问题时同步输出经过校准的置信度估计。该方法通过基于对数评分规则的奖励函数进行优化，明确惩罚过度自信与自信不足两种偏差，促使模型将置信度估计与实际预测准确率对齐。在我们的奖励设计下，最优策略将产生完全校准的置信度表达。与以往将置信度估计与回答生成相分离的方法不同，本方法将置信度校准无缝集成至LLM的生成过程中。实证研究表明，采用本方法训练的模型显著提升了校准性能，且无需额外微调即可泛化至未见任务，表明其形成了通用的置信度感知能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for large language models (LLMs) to express calibrated confidence in their answers to enhance safety and trustworthiness. The authors propose a novel reinforcement learning method that directly fine-tunes LLMs to output confidence estimates alongside factual responses, optimizing a reward based on the logarithmic scoring rule to penalize both over- and under-confidence, thereby aligning confidence with predictive accuracy. Experimental results show that models trained with this approach achieve substantially improved calibration and generalize to unseen tasks without additional fine-tuning, indicating the emergence of general confidence awareness.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLMs）需要表达校准后的置信度以增强安全性和可信度的问题，提出了一种新颖的强化学习方法。该方法直接微调LLMs，使其在回答事实性问题时输出置信度估计，并通过基于对数评分规则的奖励来优化，明确惩罚过度自信和自信不足，从而将置信度与实际预测准确性对齐。实验结果表明，采用此方法训练的模型在校准方面显著改进，且无需进一步微调即可泛化到未见任务，这表明模型出现了普遍的置信度意识。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action</div>
<div class="meta-line">Authors: Gong Gao, Weidong Zhao, Xianhui Liu, Ning Jia</div>
<div class="meta-line">First: 2026-01-27T15:43:02+00:00 · Latest: 2026-01-27T15:43:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19720v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19720v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing value-based online reinforcement learning (RL) algorithms suffer from slow policy exploitation due to ineffective exploration and delayed policy updates. To address these challenges, we propose an algorithm called Instant Retrospect Action (IRA). Specifically, we propose Q-Representation Discrepancy Evolution (RDE) to facilitate Q-network representation learning, enabling discriminative representations for neighboring state-action pairs. In addition, we adopt an explicit method to policy constraints by enabling Greedy Action Guidance (GAG). This is achieved through backtracking historical actions, which effectively enhances the policy update process. Our proposed method relies on providing the learning algorithm with accurate $k$-nearest-neighbor action value estimates and learning to design a fast-adaptable policy through policy constraints. We further propose the Instant Policy Update (IPU) mechanism, which enhances policy exploitation by systematically increasing the frequency of policy updates. We further discover that the early-stage training conservatism of the IRA method can alleviate the overestimation bias problem in value-based RL. Experimental results show that IRA can significantly improve the learning efficiency and final performance of online RL algorithms on eight MuJoCo continuous control tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于即时回溯动作改进在线强化学习的策略利用效率</div>
<div class="mono" style="margin-top:8px">现有基于价值的在线强化学习算法因探索效率低下与策略更新延迟导致策略利用缓慢。为解决这些问题，本文提出即时回溯动作算法。具体而言，我们提出Q表示差异演化方法以促进Q网络表示学习，实现对相邻状态-动作对的判别性表示。同时，通过启用贪婪动作引导的显式策略约束方法，借助历史动作回溯有效增强策略更新过程。该方法依赖于为学习算法提供精确的k近邻动作价值估计，并通过策略约束学习设计快速适应策略。进一步提出的即时策略更新机制通过系统增加策略更新频率来增强策略利用。研究还发现该算法的早期训练保守性可缓解基于价值强化学习中的过高估计偏差问题。实验结果表明，在八项MuJoCo连续控制任务中，该算法能显著提升在线强化学习算法的学习效率与最终性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the slow policy exploitation in value-based online reinforcement learning, which stems from ineffective exploration and delayed policy updates. To overcome these issues, the authors propose the Instant Retrospect Action (IRA) algorithm, which incorporates Q-Representation Discrepancy Evolution (RDE) to learn discriminative representations for state-action pairs and Greedy Action Guidance (GAG) via historical action backtracking to impose policy constraints, alongside an Instant Policy Update (IPU) mechanism to increase policy update frequency. Experiments on eight MuJoCo continuous control tasks demonstrate that IRA significantly enhances learning efficiency and final performance, while its early-stage training conservatism also helps mitigate overestimation bias.</div>
<div class="mono" style="margin-top:8px">本文针对基于价值的在线强化学习中策略利用缓慢的问题，该问题源于无效探索和策略更新延迟。为解决这些挑战，作者提出了即时回溯行动（IRA）算法，该方法通过Q表示差异演化（RDE）来学习状态-动作对的区分性表示，并利用贪婪行动指导（GAG）通过回溯历史动作实施策略约束，同时引入即时策略更新（IPU）机制以提高策略更新频率。在八个MuJoCo连续控制任务上的实验结果表明，IRA能显著提升学习效率和最终性能，且其早期训练保守性有助于缓解价值估计过高偏差问题。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow</div>
<div class="meta-line">Authors: Yunyue Wei, Chenhui Zuo, Yanan Sui</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T15:30:10+00:00 · Latest: 2026-01-27T15:30:10+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于价值引导流的高维连续控制可扩展探索方法</div>
<div class="mono" style="margin-top:8px">在生物与机器人应用中，高维系统的控制因庞大的状态-动作空间而极具挑战，其中高效探索至关重要。强化学习中常用的探索策略大多缺乏方向性，且随动作维度增加性能急剧下降。现有方法多采用降维技术，这会限制策略表达能力并牺牲系统灵活性。本文提出Q引导流探索（Qflex），这是一种可直接在原生高维动作空间进行探索的可扩展强化学习方法。训练过程中，Qflex沿着价值函数诱导的概率流，从可学习的源分布中遍历动作，使探索与任务相关梯度而非各向同性噪声对齐。该方法在多种高维连续控制基准测试中显著优于代表性在线强化学习基线。Qflex还成功控制了全身人体肌肉骨骼模型执行敏捷复杂动作，在超高维场景中展现出卓越的可扩展性与样本效率。结果表明，价值引导流为大规模探索提供了原则性且实用的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of effective exploration in high-dimensional continuous control tasks, where traditional reinforcement learning methods suffer from undirected exploration and performance degradation as action dimensionality increases. The authors propose Q-guided Flow Exploration (Qflex), a method that performs exploration directly in the high-dimensional action space by leveraging a learnable source distribution and a probability flow guided by the learned value function, thereby aligning exploration with task-relevant gradients instead of isotropic noise. Experimental results show that Qflex significantly outperforms online reinforcement learning baselines across various high-dimensional benchmarks and successfully controls a full-body human musculoskeletal model to perform agile movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings.</div>
<div class="mono" style="margin-top:8px">本文针对高维连续控制任务中探索效率低下的问题展开研究，传统强化学习方法在高维动作空间中往往存在探索方向不明确、性能随维度增加而急剧下降的局限。作者提出了Q引导流探索方法（Qflex），该方法通过可学习的源分布和基于价值函数引导的概率流，直接在高维动作空间中进行探索，使探索过程与任务相关的梯度对齐，而非各向同性噪声。实验结果表明，Qflex在多种高维连续控制基准测试中显著优于代表性的在线强化学习基线，并成功控制了一个全身人体肌肉骨骼模型执行灵活复杂的运动，证明了其在极高维场景下具有卓越的可扩展性和样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion</div>
<div class="meta-line">Authors: Tianyue Jiang, Yanli Wang, Yanlin Wang, Daya Guo, Ensheng Shi, Yuchi Ma, Jiachi Chen, Zibin Zheng</div>
<div class="meta-line">First: 2026-01-27T15:23:14+00:00 · Latest: 2026-01-27T15:23:14+00:00</div>
<div class="meta-line">Comments: To appear at ASE&#x27;25</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19697v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlignCoder：面向仓库级代码补全的检索与目标意图对齐框架</div>
<div class="mono" style="margin-top:8px">仓库级代码补全对现有代码大语言模型仍具挑战性，因其对仓库特定上下文和领域知识的理解有限。检索增强生成方法通过检索相关代码片段作为跨文件上下文展现出潜力，但存在两个根本问题：检索过程中查询与目标代码的错位，以及现有检索方法无法有效利用推理信息。为此，我们提出AlignCoder——一个引入查询增强机制和基于强化学习的检索器训练方法的仓库级代码补全框架。该方法通过生成多个候选补全构建增强查询，弥合初始查询与目标代码间的语义鸿沟。同时，我们采用强化学习训练AlignRetriever，使其学习利用增强查询中的推理信息实现更精准检索。我们在两个广泛使用的基准测试（CrossCodeEval和RepoEval）上对五种骨干代码大语言模型进行评估，结果显示在CrossCodeEval基准上EM分数较基线提升18.1%。实验表明该框架不仅性能优越，且在不同代码大语言模型和编程语言间具有高度泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing code large language models (code LLMs) in repository-level code completion, which struggle with repository-specific context and where retrieval-augmented generation (RAG) methods often suffer from misalignment between queries and target code and ineffective use of inference information. The method proposed, AlignCoder, introduces a query enhancement mechanism that generates multiple candidate completions to bridge the semantic gap and employs reinforcement learning to train an AlignRetriever that leverages inference information for more accurate retrieval. The main experimental results show that AlignCoder achieves an 18.1% improvement in EM score over baselines on the CrossCodeEval benchmark and demonstrates high generalizability across various code LLMs and programming languages in evaluations on CrossCodeEval and RepoEval.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有代码大语言模型在仓库级代码补全任务中的局限性，这些模型对仓库特定上下文理解有限，而检索增强生成方法常存在查询与目标代码不对齐以及无法有效利用推理信息的问题。提出的方法AlignCoder引入了查询增强机制，通过生成多个候选补全来弥合语义鸿沟，并采用强化学习训练AlignRetriever以利用推理信息进行更准确的检索。主要实验结果表明，在CrossCodeEval和RepoEval基准测试中，AlignCoder在CrossCodeEval上相比基线实现了18.1%的EM分数提升，并在多种代码大语言模型和编程语言上展现出高度的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Chuanyue Yu, Kuo Zhao, Yuhan Li, Heng Chang, Mingjian Feng, Xiangzhe Jiang, Yufei Sun, Jia Li, Yuzhi Zhang, Jianxin Li, Ziwei Zhang</div>
<div class="meta-line">Venue: the Web Conference 2026</div>
<div class="meta-line">First: 2025-07-31T14:11:16+00:00 · Latest: 2026-01-27T14:24:16+00:00</div>
<div class="meta-line">Comments: Accepted by the Web Conference 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.23581v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.23581v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness in enhancing the reasoning abilities of LLMs by leveraging graph structures for knowledge representation and modeling complex real-world relationships. However, existing GraphRAG methods still face significant bottlenecks when handling complex problems that require multi-hop reasoning, as their query and retrieval phases are largely based on pre-defined heuristics and do not fully utilize the reasoning potentials of LLMs. To address this problem, we propose GraphRAG-R1, an adaptive GraphRAG framework by training LLMs with process-constrained outcome-based reinforcement learning (RL) to enhance the multi-hop reasoning ability. Our method can decompose complex problems, autonomously invoke retrieval tools to acquire necessary information, and perform effective reasoning. Specifically, we utilize a modified version of Group Relative Policy Optimization (GRPO) that supports rollout-with-thinking capability. Next, we design two process-constrained reward functions. To handle the shallow retrieval problem, we design a Progressive Retrieval Attenuation (PRA) reward to encourage essential retrievals. Then, to handle the over-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the model performance with computational costs. We further design a phase-dependent training strategy, containing three training stages corresponding to cold start and these two rewards. Lastly, our method adopts a hybrid graph-textual retrieval to improve the reasoning capacity. Extensive experimental results demonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex reasoning problems compared to state-of-the-art GraphRAG methods on both in-domain and out-of-domain datasets. Furthermore, our framework can be flexibly integrated with various existing retrieval methods, consistently delivering performance improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphRAG-R1：基于过程约束强化学习的图检索增强生成</div>
<div class="mono" style="margin-top:8px">图检索增强生成（GraphRAG）通过图结构进行知识表示和复杂现实关系建模，显著提升了大型语言模型的推理能力。然而，现有GraphRAG方法在处理需要多跳推理的复杂问题时仍面临瓶颈，其查询和检索阶段主要依赖预定义启发式规则，未能充分利用LLM的推理潜力。为此，我们提出GraphRAG-R1——一种通过过程约束的基于结果强化学习训练LLM的自适应GraphRAG框架，以增强多跳推理能力。该方法能分解复杂问题、自主调用检索工具获取必要信息并进行有效推理。具体而言，我们采用支持“带思考的推演”能力的改进版组相对策略优化算法，并设计两种过程约束奖励函数：针对浅层检索问题设计渐进检索衰减奖励以激励必要检索；针对过度思考问题设计成本感知F1奖励以平衡模型性能与计算成本。进一步提出包含冷启动阶段和双奖励训练的三阶段训练策略，同时采用混合图-文本检索提升推理能力。大量实验表明，在领域内和跨领域数据集上，GraphRAG-R1相比前沿GraphRAG方法显著提升了LLM解决复杂推理问题的能力。该框架可灵活集成多种现有检索方法，持续带来性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GraphRAG-R1, a framework designed to overcome the limitations of existing Graph Retrieval-Augmented Generation methods in handling complex multi-hop reasoning tasks, which often rely on pre-defined heuristics and underutilize LLMs&#x27; reasoning potential. The method employs process-constrained outcome-based reinforcement learning, specifically a modified Group Relative Policy Optimization with rollout-with-thinking, to train LLMs to decompose problems, autonomously invoke retrieval tools, and reason effectively; it incorporates two novel reward functions—Progressive Retrieval Attenuation to encourage essential retrievals and Cost-Aware F1 to balance performance with computational costs—alongside a phase-dependent training strategy and hybrid graph-textual retrieval. Experimental results show that GraphRAG-R1 significantly enhances LLM performance on complex reasoning problems across both in-domain and out-of-domain datasets compared to state-of-the-art GraphRAG methods, while remaining flexible for integration with various retrieval approaches.</div>
<div class="mono" style="margin-top:8px">本文提出了GraphRAG-R1框架，旨在解决现有图检索增强生成方法在处理需要多跳推理的复杂问题时的瓶颈，这些方法通常依赖预定义启发式规则且未充分利用大语言模型的推理潜力。该方法采用过程约束的基于结果的强化学习，特别是改进的具有思维推演能力的组相对策略优化，来训练大语言模型分解问题、自主调用检索工具并进行有效推理；它引入了两种新颖的奖励函数——渐进检索衰减以鼓励必要检索，以及成本感知F1以平衡性能与计算成本——并结合了分阶段训练策略和混合图-文本检索。实验结果表明，与最先进的图检索增强生成方法相比，GraphRAG-R1在领域内和领域外数据集上均显著提升了大语言模型解决复杂推理问题的能力，同时保持了与多种现有检索方法灵活集成的特性。</div>
</details>
</div>
<div class="card">
<div class="title">Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement Learning</div>
<div class="meta-line">Authors: Tongxi Wang, Zhuoyang Xia, Xinran Chen, Shan Liu</div>
<div class="meta-line">First: 2026-01-27T13:58:11+00:00 · Latest: 2026-01-27T13:58:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19624v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19624v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world reinforcement learning often faces environment drift, but most existing methods rely on static entropy coefficients/target entropy, causing over-exploration during stable periods and under-exploration after drift (thus slow recovery), and leaving unanswered the principled question of how exploration intensity should scale with drift magnitude. We prove that entropy scheduling under non-stationarity can be reduced to a one-dimensional, round-by-round trade-off, faster tracking of the optimal solution after drift vs. avoiding gratuitous randomness when the environment is stable, so exploration strength can be driven by measurable online drift signals. Building on this, we propose AES (Adaptive Entropy Scheduling), which adaptively adjusts the entropy coefficient/temperature online using observable drift proxies during training, requiring almost no structural changes and incurring minimal overhead. Across 4 algorithm variants, 12 tasks, and 4 drift modes, AES significantly reduces the fraction of performance degradation caused by drift and accelerates recovery after abrupt changes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>追踪漂移：面向非平稳强化学习的变分感知熵调度方法</div>
<div class="mono" style="margin-top:8px">现实世界中的强化学习常面临环境漂移问题，但现有方法大多依赖静态熵系数/目标熵，导致稳定期过度探索、漂移后探索不足（恢复缓慢），且未能从原理上回答探索强度应如何随漂移幅度调整。我们证明非平稳性下的熵调度可简化为逐轮的一维权衡：在漂移后更快追踪最优解 vs 环境稳定时避免无谓随机性，从而使探索强度可由可测量的在线漂移信号驱动。基于此，我们提出AES（自适应熵调度），在训练期间利用可观测的漂移代理指标自适应调整在线熵系数/温度，几乎无需结构调整且开销极小。在4种算法变体、12项任务和4种漂移模式的实验中，AES显著降低了漂移导致的性能退化比例，并加速了突变后的恢复速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of environment drift in real-world reinforcement learning, where static entropy coefficients lead to inefficient exploration—either excessive during stable periods or insufficient after drift, hindering recovery. The authors theoretically reduce entropy scheduling to a one-dimensional trade-off between tracking speed and stability, enabling exploration strength to be guided by online drift signals. They propose AES (Adaptive Entropy Scheduling), a method that adaptively adjusts entropy coefficients using observable drift proxies with minimal overhead. Experiments across 12 tasks and 4 drift modes show AES significantly reduces performance degradation from drift and accelerates recovery after abrupt changes.</div>
<div class="mono" style="margin-top:8px">本文针对现实世界中强化学习面临的环境漂移问题，指出静态熵系数会导致探索效率低下：稳定期过度探索，漂移后探索不足，从而延缓恢复。作者从理论上将熵调度简化为跟踪速度与稳定性之间的一维权衡，使得探索强度可由在线漂移信号驱动。他们提出了AES（自适应熵调度）方法，利用可观测的漂移代理自适应调整熵系数，几乎无需结构改动且开销极小。在12个任务和4种漂移模式上的实验表明，AES显著降低了漂移引起的性能下降，并加速了突变后的恢复速度。</div>
</details>
</div>
<div class="card">
<div class="title">R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Zhizheng Jiang, Kang Zhao, Weikai Xu, Xinkui Lin, Wei Liu, Jian Luan, Shuo Shang, Peng Han</div>
<div class="meta-line">First: 2026-01-27T13:55:34+00:00 · Latest: 2026-01-27T13:55:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19620v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19620v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>R^3：面向大语言模型强化学习的回放、反思与排序奖励机制</div>
<div class="mono" style="margin-top:8px">大型推理模型旨在通过结构化推理解决多样复杂问题。基于分组的策略优化方法近期取得进展，能在不依赖过程级标注的情况下实现稳定的优势估计。然而，这些方法依赖同批次内高质量样本产生的优势差异，当组内优势在复杂任务中坍缩时，会导致训练过程脆弱且低效。为解决这些问题，我们提出名为R^3的强化学习机制，包含三个方向：（1）跨上下文回放策略：通过回溯同一查询的历史轨迹中有价值的示例，维持组内优势；（2）上下文内自反思机制：使模型能借助过往失败经验优化输出；（3）结构熵排序奖励：通过基于词元级熵模式对响应排序，为截断或失败样本分配相对奖励，兼顾局部探索与全局稳定性。我们在Deepseek-R1-Distill-Qwen-1.5B上实现该方法，并使用数学领域的DeepscaleR-40k数据集进行训练。实验表明，该方法在多个数学基准测试中达到最先进性能，相比基线模型实现了显著提升并减少了推理词元消耗。代码与模型将开源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces R^3, a reinforcement learning mechanism designed to enhance the training stability and efficiency of large reasoning models on complex tasks, addressing the fragility of existing group-based methods that rely on intra-batch advantage gaps. The method combines three components: cross-context replay to preserve advantage by reusing historical trajectories, in-context self-reflection for iterative output refinement using past failures, and a structural entropy ranking reward that assigns relative rewards based on token-level entropy patterns to handle truncated or failed samples. Experimental results on math benchmarks using Deepseek-R1-Distill-Qwen-1.5B trained with DeepscaleR-40k show state-of-the-art performance with significant improvements and reduced reasoning tokens compared to base models.</div>
<div class="mono" style="margin-top:8px">本文提出了R^3强化学习机制，旨在提升大型推理模型在复杂任务上的训练稳定性和效率，以解决现有基于群体的方法依赖批次内优势差距而导致的脆弱性问题。该方法整合了三个方向：通过跨上下文回放重用历史轨迹以保持优势，利用上下文内自反思机制基于过往失败迭代优化输出，以及采用结构熵排序奖励根据词元级熵模式为截断或失败样本分配相对奖励。在数学领域使用Deepseek-R1-Distill-Qwen-1.5B模型和DeepscaleR-40k数据进行的实验表明，该方法在多个数学基准测试中取得了最先进的性能，相比基础模型有显著提升并减少了推理词元数量。</div>
</details>
</div>
<div class="card">
<div class="title">Model-free policy gradient for discrete-time mean-field control</div>
<div class="meta-line">Authors: Matthieu Meunier, Huyên Pham, Christoph Reisinger</div>
<div class="meta-line">First: 2026-01-16T11:49:25+00:00 · Latest: 2026-01-27T13:47:58+00:00</div>
<div class="meta-line">Comments: 42 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11217v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11217v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study model-free policy learning for discrete-time mean-field control (MFC) problems with finite state space and compact action space. In contrast to the extensive literature on value-based methods for MFC, policy-based approaches remain largely unexplored due to the intrinsic dependence of transition kernels and rewards on the evolving population state distribution, which prevents the direct use of likelihood-ratio estimators of policy gradients from classical single-agent reinforcement learning. We introduce a novel perturbation scheme on the state-distribution flow and prove that the gradient of the resulting perturbed value function converges to the true policy gradient as the perturbation magnitude vanishes. This construction yields a fully model-free estimator based solely on simulated trajectories and an auxiliary estimate of the sensitivity of the state distribution. Building on this framework, we develop MF-REINFORCE, a model-free policy gradient algorithm for MFC, and establish explicit quantitative bounds on its bias and mean-squared error. Numerical experiments on representative mean-field control tasks demonstrate the effectiveness of the proposed approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离散时间平均场控制的无模型策略梯度方法</div>
<div class="mono" style="margin-top:8px">本文研究具有有限状态空间和紧致动作空间的离散时间平均场控制问题的无模型策略学习方法。与平均场控制领域大量基于价值函数的方法相比，策略梯度方法因转移核与奖励函数对演化群体状态分布的内在依赖性而鲜有探索——这种依赖性阻碍了经典单智能体强化学习中似然比策略梯度估计量的直接应用。我们提出一种针对状态分布流的新型扰动方案，并证明当扰动幅度趋近于零时，所得扰动值函数的梯度收敛至真实策略梯度。该构造产生了一个完全无模型的估计量，仅依赖于模拟轨迹和状态分布敏感度的辅助估计。基于此框架，我们开发了MF-REINFORCE算法——一种适用于平均场控制的无模型策略梯度算法，并建立了其偏差与均方误差的显式定量界。在典型平均场控制任务上的数值实验验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of model-free policy learning in discrete-time mean-field control (MFC) problems, where traditional policy gradient methods are hindered by the dependence of system dynamics on the evolving population distribution. The authors propose a novel perturbation scheme on the state-distribution flow, proving that the gradient of the perturbed value function converges to the true policy gradient, which enables a fully model-free estimator using only simulated trajectories and an auxiliary sensitivity estimate. They develop the MF-REINFORCE algorithm based on this framework, providing explicit bounds on its bias and mean-squared error, and demonstrate its effectiveness through numerical experiments on representative MFC tasks.</div>
<div class="mono" style="margin-top:8px">本文研究了离散时间平均场控制中的无模型策略学习问题，传统策略梯度方法因系统动态依赖于演化中的群体分布而难以直接应用。作者提出了一种针对状态分布流的新颖扰动方案，证明了扰动值函数的梯度收敛于真实策略梯度，从而构建了一个仅基于模拟轨迹和辅助敏感性估计的无模型估计器。基于此框架，他们开发了MF-REINFORCE算法，给出了其偏差和均方误差的显式定量界限，并通过代表性平均场控制任务的数值实验验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation</div>
<div class="meta-line">Authors: Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin</div>
<div class="meta-line">First: 2026-01-13T08:22:28+00:00 · Latest: 2026-01-27T13:47:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08323v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08323v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AtomMem：基于原子内存操作的可学习动态智能体记忆</div>
<div class="mono" style="margin-top:8px">为智能体配备记忆对于解决现实世界中的长程问题至关重要。然而，现有的大多数智能体记忆机制依赖于静态且手工设计的工作流程，这限制了其性能与泛化能力，凸显了对更灵活、基于学习的记忆框架的需求。本文提出AtomMem，将记忆管理重构为一个动态决策问题：通过将高层记忆过程解构为基本的原子CRUD（创建、读取、更新、删除）操作，将记忆工作流转化为可学习的决策过程。结合监督微调与强化学习，AtomMem能够学习一种自主的、任务对齐的策略，以编排适应特定任务需求的记忆行为。在三个长上下文基准测试上的实验结果表明，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法。对训练动态的进一步分析表明，这种基于学习的框架使智能体能够发现结构化、任务对齐的记忆管理策略，凸显了其相对于预定义流程的关键优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static, hand-crafted memory workflows in agents for long-horizon tasks, this paper introduces AtomMem, a learnable memory framework that reframes memory management as a dynamic decision-making problem. The method deconstructs memory processes into fundamental atomic CRUD operations, enabling a learnable workflow, and trains an autonomous policy via supervised fine-tuning combined with reinforcement learning to orchestrate memory behaviors aligned with task demands. Experimental results on three long-context benchmarks show that the trained AtomMem-8B model consistently outperforms prior static memory methods, with further analysis revealing that the learning-based approach allows the agent to discover structured, task-aligned memory management strategies, surpassing predefined routines.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有智能体用于长周期任务的记忆机制多依赖静态、手工设计的工作流，这限制了其性能与泛化能力，因此需要更灵活、基于学习的记忆框架。方法上提出了AtomMem，将记忆管理重构为一个动态决策问题，将高层记忆过程解构为基本的原子CRUD操作，从而将记忆工作流转化为可学习的决策过程，并通过监督微调与强化学习相结合，训练出能自主协调、与任务需求对齐的记忆行为策略。在三个长上下文基准上的实验结果表明，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法，进一步对训练动态的分析显示，这种基于学习的框架能使智能体发现结构化、与任务对齐的记忆管理策略，凸显了其相对于预定义流程的关键优势。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Exploration via Policy Priors</div>
<div class="meta-line">Authors: Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros, Andreas Krause</div>
<div class="meta-line">First: 2026-01-27T13:45:28+00:00 · Latest: 2026-01-27T13:45:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19612v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19612v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略先验的安全探索</div>
<div class="mono" style="margin-top:8px">安全探索是强化学习智能体在受控环境（如仿真环境）之外进行在线学习与适应的关键需求。本研究通过利用次优但保守的策略（例如从离线数据或模拟器中获得）作为先验来应对这一挑战。我们提出的SOOPER方法采用概率动力学模型进行乐观探索，同时在必要时悲观地回退至保守策略先验。我们证明SOOPER能保证整个学习过程的安全性，并通过界定其累积遗憾确立了向最优策略的收敛性。在关键安全强化学习基准和实际硬件上的大量实验表明，SOOPER具备可扩展性，性能优于现有最优方法，并在实践中验证了我们的理论保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable safe online reinforcement learning beyond simulated environments by leveraging suboptimal but conservative policies as priors. The method, named SOOPER, employs probabilistic dynamics models to explore optimistically while pessimistically reverting to the conservative policy prior when necessary to ensure safety. Experimental results on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms state-of-the-art methods, and validates its theoretical safety and convergence guarantees in practice.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过利用次优但保守的策略作为先验，实现模拟环境之外的安全在线强化学习。所提出的SOOPER方法采用概率动力学模型进行乐观探索，同时在必要时悲观地回退到保守策略先验以确保安全。在关键安全强化学习基准和真实硬件上的实验结果表明，SOOPER具有可扩展性，性能优于现有最优方法，并在实践中验证了其理论安全性和收敛性保证。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation</div>
<div class="meta-line">Authors: Chongjun Xia, Yanchun Peng, Xianzhi Wang</div>
<div class="meta-line">First: 2026-01-27T13:22:30+00:00 · Latest: 2026-01-27T13:22:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19585v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19585v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive recommender systems can dynamically adapt to user feedback, but often suffer from content homogeneity and filter bubble effects due to overfitting short-term user preferences. While recent efforts aim to improve content diversity, they predominantly operate in static or one-shot settings, neglecting the long-term evolution of user interests. Reinforcement learning provides a principled framework for optimizing long-term user satisfaction by modeling sequential decision-making processes. However, its application in recommendation is hindered by sparse, long-tailed user-item interactions and limited semantic planning capabilities. In this work, we propose LLM-Enhanced Reinforcement Learning (LERL), a novel hierarchical recommendation framework that integrates the semantic planning power of LLM with the fine-grained adaptability of RL. LERL consists of a high-level LLM-based planner that selects semantically diverse content categories, and a low-level RL policy that recommends personalized items within the selected semantic space. This hierarchical design narrows the action space, enhances planning efficiency, and mitigates overexposure to redundant content. Extensive experiments on real-world datasets demonstrate that LERL significantly improves long-term user satisfaction when compared with state-of-the-art baselines. The implementation of LERL is available at https://anonymous.4open.science/r/code3-18D3/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM增强的强化学习在交互式推荐中提升长期用户满意度</div>
<div class="mono" style="margin-top:8px">交互式推荐系统能动态适应用户反馈，但常因过度拟合短期偏好而面临内容同质化和信息茧房效应。现有研究虽致力于提升内容多样性，但多基于静态或单次交互场景，忽略了用户兴趣的长期演化。强化学习通过建模序列决策过程，为优化长期用户满意度提供了理论框架，但其在推荐领域的应用受限于稀疏、长尾的用户-物品交互及语义规划能力不足。本文提出LLM增强的强化学习框架，融合LLM的语义规划能力与RL的细粒度适应性。该框架包含高层LLM规划器（选择语义多样的内容类别）和底层RL策略（在选定语义空间内推荐个性化物品），通过分层设计压缩动作空间、提升规划效率并缓解冗余内容过度曝光。基于真实数据集的实验表明，该框架显著优于现有基线方法。代码已开源：https://anonymous.4open.science/r/code3-18D3/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of interactive recommender systems that overfit short-term preferences and cause content homogeneity, this paper introduces LLM-Enhanced Reinforcement Learning (LERL) to optimize long-term user satisfaction. The method employs a hierarchical framework where a high-level LLM planner selects diverse content categories, and a low-level RL policy recommends personalized items within those categories, thereby narrowing the action space and enhancing semantic planning. Experimental results on real-world datasets show that LERL significantly outperforms state-of-the-art baselines in improving long-term user satisfaction.</div>
<div class="mono" style="margin-top:8px">本文针对交互式推荐系统因过度拟合短期偏好而导致内容同质化和过滤气泡的问题，提出了LLM增强的强化学习（LERL）方法以优化长期用户满意度。该方法采用分层框架，其中高层LLM规划器选择语义多样的内容类别，而低层RL策略在选定类别内推荐个性化项目，从而缩小动作空间并提升规划效率。在真实数据集上的实验结果表明，LERL在提升长期用户满意度方面显著优于现有先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning</div>
<div class="meta-line">Authors: Jiayu Chen, Le Xu, Wentse Chen, Jeff Schneider</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2024-10-15T03:36:43+00:00 · Latest: 2026-01-27T12:54:32+00:00</div>
<div class="meta-line">Comments: This paper is accepted in ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.11234v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.11234v4">PDF</a> · <a href="https://github.com/LucasCJYSDL/Offline-RL-Kit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) is a powerful approach for data-driven decision-making and control. Compared to model-free methods, offline model-based reinforcement learning (MBRL) explicitly learns world models from a static dataset and uses them as surrogate simulators, improving the data efficiency and enabling the learned policy to potentially generalize beyond the dataset support. However, there could be various MDPs that behave identically on the offline dataset and dealing with the uncertainty about the true MDP can be challenging. In this paper, we propose modeling offline MBRL as a Bayes Adaptive Markov Decision Process (BAMDP), which is a principled framework for addressing model uncertainty. We further propose a novel Bayes Adaptive Monte-Carlo planning algorithm capable of solving BAMDPs in continuous state and action spaces with stochastic transitions. This planning process is based on Monte Carlo Tree Search and can be integrated into offline MBRL as a policy improvement operator in policy iteration. Our &quot;RL + Search&quot; framework follows in the footsteps of superhuman AIs like AlphaZero, improving on current offline MBRL methods by incorporating more computation input. The proposed algorithm significantly outperforms state-of-the-art offline RL methods on twelve D4RL MuJoCo tasks and three challenging, stochastic tokamak control tasks. The codebase is available at: https://github.com/LucasCJYSDL/Offline-RL-Kit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯自适应蒙特卡洛树搜索的离线模型强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习是一种数据驱动的决策与控制方法。相较于无模型方法，离线模型强化学习从静态数据集中显式学习世界模型，并将其作为替代模拟器，从而提升数据效率，并使学习策略可能泛化至数据集支持范围之外。然而，离线数据集中可能存在行为一致的不同马尔可夫决策过程，处理真实MDP的不确定性具有挑战性。本文提出将离线MBRL建模为贝叶斯自适应马尔可夫决策过程，这是一个解决模型不确定性的理论框架。我们进一步提出一种新颖的贝叶斯自适应蒙特卡洛规划算法，能够求解具有随机转移的连续状态与动作空间的BAMDP。该规划过程基于蒙特卡洛树搜索，可作为策略改进算子集成到离线MBRL的策略迭代中。我们的“强化学习+搜索”框架遵循AlphaZero等超人类AI的设计思路，通过引入更多计算输入改进现有离线MBRL方法。所提算法在十二项D4RL MuJoCo任务和三项具有挑战性的随机托卡马克控制任务上显著优于当前最先进的离线RL方法。代码库已开源：https://github.com/LucasCJYSDL/Offline-RL-Kit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of model uncertainty in offline model-based reinforcement learning (MBRL) by framing it as a Bayes Adaptive Markov Decision Process (BAMDP). The proposed method introduces a novel Bayes Adaptive Monte-Carlo planning algorithm that leverages Monte Carlo Tree Search to solve BAMDPs in continuous state-action spaces with stochastic transitions, integrating this planner as a policy improvement operator. Experimental results demonstrate that the algorithm significantly outperforms state-of-the-art offline RL methods on twelve D4RL MuJoCo benchmarks and three stochastic tokamak control tasks, showcasing improved performance through increased computational search.</div>
<div class="mono" style="margin-top:8px">本文针对离线基于模型的强化学习中的模型不确定性挑战，将其建模为贝叶斯自适应马尔可夫决策过程。该方法提出了一种新颖的贝叶斯自适应蒙特卡洛规划算法，利用蒙特卡洛树搜索在连续状态-动作空间和随机转移中求解该决策过程，并将此规划器作为策略改进算子集成到学习框架中。实验结果表明，该算法在十二个D4RL MuJoCo基准任务和三个具有挑战性的随机托卡马克控制任务上显著优于现有最先进的离线强化学习方法，通过增加计算搜索提升了性能。</div>
</details>
</div>
<div class="card">
<div class="title">daVinci-Dev: Agent-native Mid-training for Software Engineering</div>
<div class="meta-line">Authors: Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-26T12:20:18+00:00 · Latest: 2026-01-27T12:16:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18418v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18418v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model&#x27;s agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>daVinci-Dev：面向软件工程的智能体原生中期训练</div>
<div class="mono" style="margin-top:8px">近期，大语言模型（LLM）能力前沿已从单轮代码生成转向智能体化软件工程——即模型能自主导航、编辑和测试复杂代码库的范式。尽管后训练方法已成为代码智能体的主流方案，但**智能体中期训练**——在模拟真实智能体工作流程的大规模数据上进行中期训练（MT）——虽比单纯依赖昂贵的强化学习更能规模化地培养基础智能体行为，却因资源需求巨大而尚未得到充分探索。实现有效智能体中期训练的核心挑战在于静态训练数据与真实开发中动态、反馈丰富的环境之间的分布不匹配。为此，我们系统研究了智能体中期训练，建立了大规模智能体开发的数据合成原则与训练方法。我们方法的核心是**智能体原生数据**——包含两种互补轨迹的监督数据：**上下文原生轨迹**完整保留智能体经历的信息流，提供广泛覆盖与多样性；**环境原生轨迹**从可执行代码库收集，其观测源自实际工具调用与测试执行，提供深度与交互真实性。我们在`SWE-Bench Verified`上验证了模型的智能体能力。实验表明，在采用对齐基座模型与智能体框架的两种后训练设置下，我们的方法以不足半数中期训练词元（731亿）超越了先前开源软件工程中期训练方案`Kimi-Dev`。除相对优势外，我们表现最佳的320亿与720亿参数模型分别达到**56.1%**与**58.5%**的问题解决率……</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the shift from single-turn code generation to agentic software engineering, where LLMs autonomously handle complex repositories, and identifies a gap in scalable training methods; it proposes agentic mid-training using agent-native data, which combines contextually-native trajectories for broad coverage and environmentally-native trajectories from executable repositories for authentic interaction. The method involves synthesizing this data to mirror real agentic workflows and training models at scale, with experimental results on SWE-Bench Verified showing that their 32B and 72B models achieve resolution rates of 56.1% and 58.5%, respectively, outperforming the prior Kimi-Dev approach while using less than half the mid-training tokens.</div>
<div class="mono" style="margin-top:8px">本文的动机在于大型语言模型能力前沿已从单轮代码生成转向代理式软件工程，即模型自主处理复杂代码库，但现有训练方法在可扩展性上存在不足；为此提出代理式中期训练，采用代理原生数据，包括保留完整信息流的上下文原生轨迹和来自可执行仓库的环境原生轨迹，以模拟真实工作流程。该方法通过合成数据并进行大规模训练，在SWE-Bench Verified上的实验结果表明，其32B和72B模型分别达到56.1%和58.5%的解决率，优于之前的Kimi-Dev方法，且中期训练令牌使用量减少一半以上。</div>
</details>
</div>
<div class="card">
<div class="title">Coupled Variational Reinforcement Learning for Language Model General Reasoning</div>
<div class="meta-line">Authors: Xueru Wen, Jie Lou, Yanjiang Liu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Yaojie Lu, Debing Zhang</div>
<div class="meta-line">First: 2025-12-14T07:03:51+00:00 · Latest: 2026-01-27T10:47:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12576v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12576v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While reinforcement learning has achieved impressive progress in language model reasoning, it is constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the probabilities that LLMs generate reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>耦合变分强化学习用于语言模型通用推理</div>
<div class="mono" style="margin-top:8px">尽管强化学习在语言模型推理领域取得了显著进展，但其发展受限于对可验证奖励的需求。近期无需验证器的强化学习方法通过利用大语言模型生成参考答案的概率作为奖励信号，解决了这一局限。然而，这些方法通常仅基于问题条件采样推理轨迹。这种设计使推理轨迹采样与答案信息解耦，导致探索效率低下以及轨迹与最终答案间的不连贯性。本文提出耦合变分强化学习方法，通过混合采样策略耦合先验分布与后验分布，搭建变分推断与强化学习的桥梁。通过构建并优化融合这两种分布的复合分布，该方法在保持强思维-答案连贯性的同时实现高效探索。在数学与通用推理基准上的大量实验表明，该方法较基础模型性能提升12.4%，较当前最优的无验证器强化学习基线额外提升2.3%，为增强语言模型通用推理能力提供了理论框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitation of existing verifier-free reinforcement learning methods for language model reasoning, which sample reasoning traces based only on the question, leading to inefficient exploration and poor coherence between traces and answers. The proposed method, CoVRL, integrates variational inference with reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy, optimizing a composite distribution to enhance exploration and thought-answer coherence. Experimental results on mathematical and general reasoning benchmarks demonstrate that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% gain over state-of-the-art verifier-free RL baselines, offering a principled framework for advancing language model reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有无需验证器的语言模型强化学习方法仅基于问题采样推理轨迹，导致探索效率低下且轨迹与答案间缺乏连贯性。提出的方法CoVRL通过混合采样策略耦合先验和后验分布，将变分推断与强化学习相结合，并优化复合分布以提升探索能力和思维-答案一致性。在数学和通用推理基准上的广泛实验表明，CoVRL相比基础模型性能提升12.4%，并在当前最先进的无需验证器强化学习基线基础上额外提升2.3%，为增强语言模型的通用推理能力提供了一个原则性框架。</div>
</details>
</div>
<div class="card">
<div class="title">APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition</div>
<div class="meta-line">Authors: Finn Rietz, Pedro Zuidberg dos Martires, Johannes Andreas Stork</div>
<div class="meta-line">First: 2026-01-27T10:38:32+00:00 · Latest: 2026-01-27T10:38:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19452v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19452v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating demonstration data into reinforcement learning (RL) can greatly accelerate learning, but existing approaches often assume demonstrations are optimal and fully aligned with the target task. In practice, demonstrations are frequently sparse, suboptimal, or misaligned, which can degrade performance when these demonstrations are integrated into RL. We propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow (NF) priors. Instead of enforcing strict adherence to the priors, APC estimates each prior&#x27;s applicability to the target task while leveraging them for exploration. Moreover, APC either refines useful priors, or sidesteps misaligned ones when necessary to optimize downstream reward. Across diverse benchmarks, APC accelerates learning when demonstrations are aligned, remains robust under severe misalignment, and leverages suboptimal demonstrations to bootstrap exploration while avoiding performance degradation caused by overly strict adherence to suboptimal demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APC-RL：通过自适应策略组合超越数据驱动的行为先验</div>
<div class="mono" style="margin-top:8px">将演示数据融入强化学习（RL）能显著加速学习过程，但现有方法通常假设演示数据是最优且与目标任务完全对齐的。实践中，演示数据往往稀疏、次优或存在偏差，直接整合到RL中可能导致性能下降。我们提出自适应策略组合（APC）——一种分层模型，能自适应地组合多个数据驱动的标准化流（NF）先验。APC不强制严格遵循先验，而是评估每个先验对目标任务的适用性，同时利用它们进行探索。此外，APC会优化有用先验，或在必要时规避偏差先验以最大化下游奖励。在多样化基准测试中，APC在演示数据对齐时加速学习，在严重偏差下保持鲁棒性，并能利用次优演示数据引导探索，同时避免因过度遵循次优演示导致的性能下降。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of integrating suboptimal or misaligned demonstration data into reinforcement learning, which can degrade performance if treated as optimal. It introduces Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple Normalizing Flow priors by estimating each prior&#x27;s applicability to the target task, refining useful ones and sidesteps misaligned ones to optimize reward. Experimental results show APC accelerates learning with aligned demonstrations, remains robust under severe misalignment, and leverages suboptimal data for exploration without performance degradation.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中整合次优或未对齐示范数据可能导致性能下降的问题，提出自适应策略组合（APC）这一分层模型。该方法通过自适应组合多个归一化流先验，评估每个先验对目标任务的适用性，优化有用先验并规避未对齐先验以最大化奖励。实验表明，APC在示范对齐时加速学习，在严重未对齐情况下保持鲁棒性，并能利用次优示范引导探索而不造成性能损失。</div>
</details>
</div>
<div class="card">
<div class="title">OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation</div>
<div class="meta-line">Authors: Giuseppe Chiari, Michele Piccoli, Davide Zoni</div>
<div class="meta-line">First: 2026-01-27T10:18:46+00:00 · Latest: 2026-01-27T10:18:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19439v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automation of analog integrated circuit (IC) design remains a longstanding challenge, primarily due to the intricate interdependencies among physical layout, parasitic effects, and circuit-level performance. These interactions impose complex constraints that are difficult to accurately capture and optimize using conventional design methodologies. Although recent advances in machine learning (ML) have shown promise in automating specific stages of the analog design flow, the development of holistic, end-to-end frameworks that integrate these stages and iteratively refine layouts using post-layout, parasitic-aware performance feedback is still in its early stages. Furthermore, progress in this direction is hindered by the limited availability of open, high-quality datasets tailored to the analog domain, restricting both the benchmarking and the generalizability of ML-based techniques. To address these limitations, we present OSIRIS, a scalable dataset generation pipeline for analog IC design. OSIRIS systematically explores the design space of analog circuits while producing comprehensive performance metrics and metadata, thereby enabling ML-driven research in electronic design automation (EDA). In addition, we release a dataset consisting of 87,100 circuit variations generated with OSIRIS, accompanied by a reinforcement learning (RL)-based baseline method that exploits OSIRIS for analog design optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OSIRIS：通过可扩展数据集生成桥接模拟电路设计与机器学习</div>
<div class="mono" style="margin-top:8px">模拟集成电路设计的自动化仍是一项长期挑战，主要源于物理版图、寄生效应与电路性能之间复杂的相互依赖关系。这些相互作用施加了传统设计方法难以精确捕获和优化的复杂约束。尽管机器学习的最新进展在自动化模拟设计流程的特定阶段展现出潜力，但整合这些阶段、并利用后版图寄生效应感知的性能反馈迭代优化版图的端到端整体框架仍处于早期发展阶段。此外，该方向的进展受限于面向模拟领域开放高质量数据集的稀缺，制约了基于机器学习技术的基准测试与泛化能力。为应对这些局限，我们提出了OSIRIS——一个用于模拟集成电路设计的可扩展数据集生成流程。OSIRIS系统性地探索模拟电路设计空间，同时生成全面的性能指标与元数据，从而推动电子设计自动化领域的机器学习驱动研究。此外，我们发布了包含87,100个由OSIRIS生成电路变体的数据集，并配套提供了基于强化学习的基准方法，该方法利用OSIRIS实现模拟设计优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the longstanding challenge of automating analog integrated circuit design, which is hindered by complex interdependencies between layout, parasitics, and performance, as well as a lack of open, high-quality datasets for machine learning, this paper introduces OSIRIS, a scalable dataset generation pipeline. The method systematically explores the analog circuit design space to produce comprehensive performance metrics and metadata, facilitating ML-driven electronic design automation research. The main experimental results include the release of a dataset of 87,100 circuit variations generated using OSIRIS, along with a reinforcement learning-based baseline method that demonstrates its utility for analog design optimization.</div>
<div class="mono" style="margin-top:8px">本文的动机在于模拟集成电路设计自动化长期面临的挑战，其受限于版图、寄生效应与性能间的复杂依赖关系，以及缺乏适用于机器学习的高质量开源数据集。为此，论文提出了OSIRIS，一种可扩展的数据集生成流程，该方法系统探索模拟电路设计空间，生成全面的性能指标与元数据，以支持电子设计自动化中的机器学习研究。主要实验结果包括发布了使用OSIRIS生成的87,100个电路变体数据集，并提供了一个基于强化学习的基准方法，展示了OSIRIS在模拟设计优化中的应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Who Gets Cited Most? Benchmarking Long-Context Reasoning on Scientific Articles</div>
<div class="meta-line">Authors: Miao Li, Alexander Gurung, Irina Saparina, Mirella Lapata</div>
<div class="meta-line">First: 2025-09-25T11:36:09+00:00 · Latest: 2026-01-27T10:01:24+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21028v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21028v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce SciTrek, a novel question-answering benchmark designed to evaluate long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by creating benchmark questions that require information aggregation and synthesis across multiple full-text scientific articles. The questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (i.e., titles, authors, and references). These SQL queries provide explicit, verifiable reasoning processes that enable fine-grained error analysis on model answers, and the data construction scales to contexts of up to 1M tokens with minimal supervision. Experiments on open-weight and proprietary LLMs show that SciTrek poses significant challenges as the context length increases, with supervised fine-tuning and reinforcement learning offering only limited gains. Our analysis reveals systematic shortcomings of frontier LLMs&#x27; ability to effectively perform numerical operations and accurately locate information in long contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>谁被引用最多？基于科学文献的长上下文推理能力基准测试</div>
<div class="mono" style="margin-top:8px">我们提出了SciTrek——一个新颖的问答基准测试集，旨在利用科学文献评估大语言模型（LLMs）的长上下文推理能力。当前的长上下文基准测试多聚焦于简单信息检索任务或采用人工构造的上下文。SciTrek通过创建需要跨多篇全文科学文献进行信息整合与综合的基准问题，解决了这些局限性。问题及其标准答案通过将文献元数据（即标题、作者和参考文献）构建的数据库进行SQL查询自动生成。这些SQL查询提供了明确、可验证的推理过程，支持对模型答案进行细粒度错误分析，且数据构建可扩展至百万级token的上下文规模而仅需极少人工干预。对开源与专有大语言模型的实验表明，随着上下文长度增加，SciTrek会带来显著挑战，监督微调与强化学习仅能带来有限提升。我们的分析揭示了前沿大语言模型在有效执行数值运算和准确定位长上下文信息方面存在系统性缺陷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SciTrek, a new benchmark designed to address the limitations of existing long-context evaluations, which often rely on simple retrieval or artificial contexts. The method automatically generates questions and answers by formulating them as SQL queries over a database of scientific article metadata, enabling scalable creation of tasks requiring information synthesis across multiple full-text articles and providing verifiable reasoning traces for analysis. Experimental results on various large language models demonstrate that SciTrek presents significant challenges as context length grows, with fine-tuning offering limited improvement, and reveals systematic model weaknesses in numerical reasoning and information location within long documents.</div>
<div class="mono" style="margin-top:8px">本文提出了SciTrek这一新基准，旨在解决现有长上下文评测多依赖简单检索或人工语境的局限。其方法通过对科学文献元数据构建数据库，并将问题与答案自动生成为SQL查询，从而可扩展地创建需要跨多篇全文进行信息综合的任务，并提供可验证的推理路径以供分析。在各种大语言模型上的实验结果表明，随着上下文长度增加，SciTrek带来了显著挑战，微调提升有限，并揭示了前沿模型在长文档中进行数值运算和准确定位信息的系统性不足。</div>
</details>
</div>
<div class="card">
<div class="title">Task-Centric Policy Optimization from Misaligned Motion Priors</div>
<div class="meta-line">Authors: Ziang Zheng, Kai Feng, Yi Nie, Shentao Qin</div>
<div class="meta-line">First: 2026-01-27T09:46:34+00:00 · Latest: 2026-01-27T09:46:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19411v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19411v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing naïve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于错位运动先验的任务中心策略优化</div>
<div class="mono" style="margin-top:8px">人形机器人控制常利用人类演示的运动先验来促进行为自然性。然而，由于本体差异、重定向误差和任务无关变异，此类演示常存在次优性或与机器人任务错位，导致简单模仿会降低任务性能。反之，纯任务强化学习虽能产生多种任务最优解，却常导致不自然或不稳定运动。这揭示了对抗模仿学习中线性奖励混合的根本局限。我们提出\emph{任务中心运动先验}（TCMP），这是一个任务优先的对抗模仿框架，将模仿视为条件正则化器而非对等目标。TCMP在最大化任务改进的同时，仅当模仿信号与任务进展兼容时才纳入，产生自适应、几何感知的更新机制，既能保持任务可行下降方向，又能在错位时抑制有害模仿。我们提供了梯度冲突与任务优先稳定点的理论分析，并通过人形控制实验验证了所提方法能在噪声演示下实现稳健任务性能与一致运动风格。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of using human motion priors for humanoid control, where direct imitation often degrades task performance due to misalignment from embodiment differences or suboptimal demonstrations, while task-only reinforcement learning can produce unnatural motions. The authors propose Task-Centric Motion Priors (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer, adaptively incorporating imitation signals only when they align with task progress to avoid harmful updates. Experimental results on humanoid control show that TCMP achieves robust task performance while maintaining consistent motion style even with noisy demonstrations, effectively balancing task optimization and natural behavior.</div>
<div class="mono" style="margin-top:8px">本文针对人形机器人控制中使用人类运动先验的挑战，指出直接模仿常因本体差异或次优演示导致任务性能下降，而仅依赖任务的强化学习则可能产生不自然的运动。作者提出任务中心运动先验（TCMP），这是一个任务优先的对抗模仿框架，将模仿视为条件正则化器，仅在模仿信号与任务进展兼容时自适应地引入，以避免有害更新。在人形控制实验中，TCMP在噪声演示下实现了稳健的任务性能并保持了一致的运动风格，有效平衡了任务优化与自然行为。</div>
</details>
</div>
<div class="card">
<div class="title">SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling</div>
<div class="meta-line">Authors: Loris Gaven, Clement Romac, Thomas Carta, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer</div>
<div class="meta-line">First: 2024-10-16T11:59:27+00:00 · Latest: 2026-01-27T09:14:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.12481v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.12481v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The past years have seen Large Language Models (LLMs) strive not only as generative models but also as agents solving textual sequential decision-making tasks. When facing complex environments where their zero-shot abilities are insufficient, recent work showed online Reinforcement Learning (RL) could be used for the LLM agent to discover and learn efficient strategies interactively. However, most prior work sticks to on-policy algorithms, which greatly reduces the scope of methods such agents could use for both exploration and exploitation, such as experience replay and hindsight relabeling. Yet, such methods may be key for LLM learning agents, and in particular when designing autonomous intrinsically motivated agents sampling and pursuing their own goals (i.e. autotelic agents). This paper presents and studies an adaptation of Soft Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves the path towards autotelic LLM agents that learn online but can also outperform on-policy methods in more classic multi-goal RL environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAC-GLAM：基于软演员-评论家与后见之明重标注的大语言模型智能体在线强化学习改进方法</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型不仅作为生成模型发展，更成为解决文本序列决策任务的智能体。面对零样本能力不足的复杂环境，近期研究表明可通过在线强化学习使大语言模型智能体以交互方式探索并习得高效策略。然而，现有研究多局限于同策略算法，这限制了智能体在探索与利用时可采用的方法范围（如经验回放与后见之明重标注）。此类方法对大语言模型学习智能体至关重要，特别是在设计能自主采样并追求目标的具内在动机智能体（即自趋智能体）时。本文提出并研究了适用于大语言模型智能体的软演员-评论家与后见之明重标注适配方案。该方法不仅为在线学习的自趋大语言模型智能体开辟了道路，在经典多目标强化学习环境中也展现出超越同策略方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance online reinforcement learning for large language model agents beyond on-policy methods, this paper introduces SAC-GLAM, which adapts Soft Actor-Critic and hindsight relabeling to enable more efficient exploration and exploitation. The method leverages experience replay and hindsight relabeling to improve learning in textual sequential decision-making tasks, particularly for autonomous, goal-directed agents. Experimental results demonstrate that this approach outperforms traditional on-policy methods in multi-goal reinforcement learning environments, paving the way for more capable autotelic LLM agents.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进大型语言模型代理的在线强化学习，超越传统同策略方法的限制，提出了SAC-GLAM方法，该方法结合了软演员-评论家算法和事后重标记技术。通过利用经验回放和事后重标记，该方法增强了代理在文本序列决策任务中的探索和利用能力，特别适用于自主设定和追求目标的自驱动代理。实验结果表明，在多目标强化学习环境中，该方法优于同策略方法，为开发更强大的自驱动大型语言模型代理奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Behaviors of Evolutionary Algorithms on GPUs: When Does Parallelism Pay Off?</div>
<div class="meta-line">Authors: Xinmeng Yu, Tao Jiang, Ran Cheng, Yaochu Jin, Kay Chen Tan</div>
<div class="meta-line">First: 2026-01-26T12:55:21+00:00 · Latest: 2026-01-27T09:01:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18446v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18446v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evolutionary algorithms (EAs) are increasingly implemented on graphics processing units (GPUs) to leverage parallel processing capabilities for enhanced efficiency. However, existing studies largely emphasize the raw speedup obtained by porting individual algorithms from CPUs to GPUs. Consequently, these studies offer limited insight into when and why GPU parallelism fundamentally benefits EAs. To address this gap, we investigate how GPU parallelism alters the behavior of EAs beyond simple acceleration metrics. We conduct a systematic empirical study of 16 representative EAs on 30 benchmark problems. Specifically, we compare CPU and GPU executions across a wide range of problem dimensionalities and population sizes. Our results reveal that the impact of GPU acceleration is highly heterogeneous and depends strongly on algorithmic structure. We further demonstrate that conventional fixed-budget evaluation based on the number of function evaluations (FEs) is inadequate for GPU execution. In contrast, fixed-time evaluation uncovers performance characteristics that are unobservable under small or practically constrained FE budgets, particularly for adaptive and exploration-oriented algorithms. Moreover, we identify distinct scaling regimes in which GPU parallelism is beneficial, saturates, or degrades as problem dimensionality and population size increase. Crucially, we show that large populations enabled by GPUs not only improve hardware utilization but also reveal algorithm-specific convergence and diversity dynamics that are difficult to observe under CPU-constrained settings. Consequently, our findings indicate that GPU parallelism is not strictly an implementation detail, but a pivotal factor that influences how EAs should be evaluated, compared, and designed for modern computing platforms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化算法在GPU上的扩展行为：并行化何时产生效益？</div>
<div class="mono" style="margin-top:8px">进化算法（EAs）越来越多地在图形处理器（GPU）上实现，以利用并行处理能力提升效率。然而，现有研究主要关注将单个算法从CPU移植到GPU所获得的原始加速比，因此对GPU并行化何时及为何从根本上使EAs受益的见解有限。为填补这一空白，我们研究了GPU并行化如何改变EAs的行为，超越简单的加速指标。我们对30个基准问题上的16种代表性EAs进行了系统性实证研究，特别比较了CPU和GPU在不同问题维度和种群规模下的执行情况。结果表明，GPU加速的影响高度异质，且强烈依赖于算法结构。我们进一步证明，基于函数评估次数（FEs）的传统固定预算评估不适用于GPU执行；相比之下，固定时间评估揭示了在小规模或实际受限的FE预算下无法观察到的性能特征，尤其对于自适应和探索导向的算法。此外，我们识别了不同的扩展机制：随着问题维度和种群规模增加，GPU并行化可能带来效益、趋于饱和或性能下降。关键的是，我们发现GPU支持的大规模种群不仅提高了硬件利用率，还揭示了在CPU受限设置下难以观察到的算法特定收敛与多样性动态。因此，我们的研究结果表明，GPU并行化并非严格的实现细节，而是影响EAs在现代计算平台上如何评估、比较和设计的关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the fundamental impact of GPU parallelism on evolutionary algorithms (EAs), moving beyond simple speedup metrics to understand when and why GPUs benefit EA performance. The authors systematically compare 16 EAs on 30 benchmark problems across CPUs and GPUs, varying problem dimensionality and population size. Key results show that GPU acceleration&#x27;s effect is highly heterogeneous and algorithm-dependent, that fixed-time evaluation reveals performance traits missed by conventional fixed-budget (function evaluation) analysis, and that GPUs enable distinct scaling regimes and large-population dynamics which can alter algorithm convergence and diversity, influencing how EAs should be evaluated and designed.</div>
<div class="mono" style="margin-top:8px">本研究探讨了GPU并行性对进化算法的根本性影响，超越了简单的加速指标，旨在理解GPU何时以及为何能提升算法性能。作者在30个基准问题上系统比较了16种进化算法在CPU和GPU上的表现，并改变了问题维度和种群规模。主要实验结果表明：GPU加速的效果高度异质且依赖于算法结构；基于固定时间的评估能揭示传统固定评估次数（函数评估）分析所忽略的性能特征；GPU支持的不同规模扩展机制以及大种群运行动态可以改变算法的收敛性和多样性，从而影响进化算法在现代计算平台上的评估与设计方式。</div>
</details>
</div>
<div class="card">
<div class="title">CHEHAB RL: Learning to Optimize Fully Homomorphic Encryption Computations</div>
<div class="meta-line">Authors: Bilel Sefsaf, Abderraouf Dandani, Abdessamed Seddiki, Arab Mohammed, Eduardo Chielle, Michail Maniatakos, Riyadh Baghdadi</div>
<div class="meta-line">First: 2026-01-27T08:49:09+00:00 · Latest: 2026-01-27T08:49:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19367v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19367v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fully Homomorphic Encryption (FHE) enables computations directly on encrypted data, but its high computational cost remains a significant barrier. Writing efficient FHE code is a complex task requiring cryptographic expertise, and finding the optimal sequence of program transformations is often intractable. In this paper, we propose CHEHAB RL, a novel framework that leverages deep reinforcement learning (RL) to automate FHE code optimization. Instead of relying on predefined heuristics or combinatorial search, our method trains an RL agent to learn an effective policy for applying a sequence of rewriting rules to automatically vectorize scalar FHE code while reducing instruction latency and noise growth. The proposed approach supports the optimization of both structured and unstructured code. To train the agent, we synthesize a diverse dataset of computations using a large language model (LLM). We integrate our proposed approach into the CHEHAB FHE compiler and evaluate it on a suite of benchmarks, comparing its performance against Coyote, a state-of-the-art vectorizing FHE compiler. The results show that our approach generates code that is $5.3\times$ faster in execution, accumulates $2.54\times$ less noise, while the compilation process itself is $27.9\times$ faster than Coyote (geometric means).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHEHAB RL：学习优化全同态加密计算</div>
<div class="mono" style="margin-top:8px">全同态加密（FHE）支持直接在加密数据上进行计算，但其高昂的计算成本仍是主要障碍。编写高效的FHE代码是一项需要密码学专业知识的复杂任务，寻找最优的程序转换序列通常难以实现。本文提出CHEHAB RL，一种利用深度强化学习（RL）自动化FHE代码优化的新型框架。该方法不依赖预定义启发式规则或组合搜索，而是训练RL智能体学习有效策略，通过应用一系列重写规则自动向量化标量FHE代码，同时降低指令延迟和噪声增长。所提方法支持结构化和非结构化代码的优化。为训练智能体，我们使用大语言模型（LLM）合成了多样化的计算数据集。我们将该方法集成到CHEHAB FHE编译器中，并通过基准测试套件进行评估，与当前最先进的向量化FHE编译器Coyote进行性能对比。结果显示：本方法生成的代码执行速度提升5.3倍，噪声累积减少2.54倍，且编译过程本身比Coyote快27.9倍（几何平均值）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the high computational cost and expert-driven complexity of writing efficient code for Fully Homomorphic Encryption (FHE). The method introduces CHEHAB RL, a framework that uses deep reinforcement learning to automate FHE code optimization by training an agent to apply a sequence of rewriting rules for vectorizing scalar code, aiming to reduce instruction latency and noise growth. The main experimental results, integrated into the CHEHAB compiler and evaluated against the state-of-the-art Coyote compiler, show that the generated code runs 5.3 times faster, accumulates 2.54 times less noise, and compiles 27.9 times faster on geometric mean.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决全同态加密计算成本高、依赖专家经验编写高效代码的难题。方法上提出了CHEHAB RL框架，利用深度强化学习自动化优化FHE代码，通过训练智能体学习应用一系列重写规则来实现标量代码的向量化，以降低指令延迟和噪声增长。主要实验结果在CHEHAB编译器中实现，并与先进的Coyote编译器对比，表明所生成代码的执行速度提升5.3倍，噪声积累减少2.54倍，且编译过程本身快27.9倍（几何平均值）。</div>
</details>
</div>
<div class="card">
<div class="title">From Observations to Events: Event-Aware World Model for Reinforcement Learning</div>
<div class="meta-line">Authors: Zhao-Han Peng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu, You He</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T08:20:44+00:00 · Latest: 2026-01-27T08:20:44+00:00</div>
<div class="meta-line">Comments: 43 pages, accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19336v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19336v1">PDF</a> · <a href="https://github.com/MarquisDarwin/EAWM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从观察到事件：强化学习的事件感知世界模型</div>
<div class="mono" style="margin-top:8px">尽管基于模型的强化学习（MBRL）通过从原始观察中学习世界模型提高了样本效率，但现有方法难以在结构相似的场景间泛化，且易受纹理或颜色偏移等虚假变化的影响。从认知科学角度看，人类将连续感官流分割为离散事件，并依赖这些关键事件进行决策。受此原理启发，我们提出事件感知世界模型（EAWM），这是一个通用框架，通过学习事件感知表征来简化策略学习，无需人工标注。EAWM采用自动化事件生成器从原始观察中推导事件，并引入通用事件分割器（GES）识别事件边界（即事件片段的起止时间）。通过事件预测，表征空间被塑造以捕捉有意义的时空转换。此外，我们提出了看似不同的世界模型架构的统一形式化表述，并展示了方法的广泛适用性。在Atari 100K、Craftax 1M、DeepMind Control 500K和DMC-GB2 500K上的实验表明，EAWM持续将强MBRL基线的性能提升10%-45%，在各项基准测试中创造了新的最优结果。代码发布于https://github.com/MarquisDarwin/EAWM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the cognitive principle that humans segment continuous sensory input into discrete events for decision-making, this paper proposes the Event-Aware World Model (EAWM) to address the generalization limitations of model-based reinforcement learning (MBRL) in the face of spurious visual variations. The method introduces an automated event generator and a Generic Event Segmentor (GES) to derive event-aware representations from raw observations without manual labels, shaping the representation space through event prediction to capture meaningful spatio-temporal transitions. Experimental results on benchmarks including Atari 100K and DeepMind Control show that EAWM consistently enhances strong MBRL baselines by 10%-45%, achieving new state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">受人类将连续感官输入分割为离散事件以辅助决策的认知原理启发，本文提出了事件感知世界模型（EAWM），以解决基于模型的强化学习在面对纹理或颜色变化等虚假干扰时泛化能力不足的问题。该方法通过自动化事件生成器和通用事件分割器（GES）从原始观测中提取无需人工标注的事件感知表征，并通过事件预测塑造表征空间以捕获有意义的时空转换。在Atari 100K和DeepMind Control等基准测试上的实验结果表明，EAWM持续将强基线模型的性能提升10%-45%，实现了新的最先进水平。</div>
</details>
</div>
<div class="card">
<div class="title">Innovator-VL: A Multimodal Large Language Model for Scientific Discovery</div>
<div class="meta-line">Authors: Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han, Junlong Ke, Cong Wang, Yicheng Fu, Jiawang Zhao, Jiangchao Yao, Xi Fang, Zhen Wang, Henxing Cai, Lin Yao, Zhifeng Gao, Yanhui Hong, Nang Yuan, Yixuan Li, Guojiang Zhao, Haoyi Tao, Nan Wang, Han Lyu, Guolin Ke, Ning Liao, Xiaoxing Wang, Kai Chen, Zhiyu Li, Feiyu Xiong, Sihan Hu, Kun Chen, Yanfeng Wang, Weinan E, Linfeng Zhang, Linfeng Zhang</div>
<div class="meta-line">First: 2026-01-27T08:12:18+00:00 · Latest: 2026-01-27T08:12:18+00:00</div>
<div class="meta-line">Comments: Innovator-VL tech report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19325v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19325v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Innovator-VL：面向科学发现的多模态大语言模型</div>
<div class="mono" style="margin-top:8px">我们推出Innovator-VL，这是一个面向科学领域的多模态大语言模型，旨在提升跨学科科学领域的理解与推理能力，同时在通用视觉任务上保持优异性能。与依赖海量领域预训练和不透明流程的趋势不同，我们的研究表明：通过原则性训练设计和透明方法论，能以显著降低的数据需求实现强大的科学智能。(i) 首先，我们提供完全透明、端到端可复现的训练流程，涵盖数据收集、清洗、预处理、监督微调、强化学习及评估，并附详细优化方案，便于学界系统性扩展。(ii) 其次，Innovator-VL展现出卓越的数据效率，仅用不足五百万精选样本（无需大规模预训练）即在多项科学任务中取得竞争力表现，证明有效推理可通过原则性数据选择而非盲目扩规模实现。(iii) 再次，该模型具备强泛化能力，在通用视觉、多模态推理及科学基准测试中均表现优异，表明科学对齐能力可融入统一模型且不损害通用性能。我们的实践表明，无需大规模数据亦可构建高效、可复现、高性能的科学多模态模型，为未来研究提供实用基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind Innovator-VL is to develop a scientific multimodal large language model that advances understanding and reasoning across diverse scientific domains while maintaining strong performance on general vision tasks, challenging the trend of relying on massive domain-specific pretraining and opaque pipelines. The method involves a fully transparent, end-to-end reproducible training pipeline covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, emphasizing principled data selection over indiscriminate scaling. Main experimental results show that Innovator-VL achieves competitive performance on various scientific tasks with fewer than five million curated samples, demonstrating remarkable data efficiency, and it also exhibits strong generalization by performing well on general vision, multimodal reasoning, and scientific benchmarks, indicating that scientific alignment can be integrated without compromising general-purpose capabilities.</div>
<div class="mono" style="margin-top:8px">Innovator-VL的动机是开发一个科学多模态大语言模型，旨在提升跨多样科学领域的理解和推理能力，同时保持通用视觉任务的优异性能，挑战了依赖大规模领域特定预训练和不透明流程的趋势。方法采用完全透明、端到端可复现的训练流程，涵盖数据收集、清洗、预处理、监督微调、强化学习和评估，强调通过原则性数据选择而非无差别扩展。主要实验结果表明，Innovator-VL使用不到五百万精选样本即在多项科学任务上取得有竞争力的性能，展现出显著的数据效率，并在通用视觉、多模态推理和科学基准测试中表现强劲，表明科学对齐能力可融入统一模型而不损害通用功能。</div>
</details>
</div>
<div class="card">
<div class="title">Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs</div>
<div class="meta-line">Authors: Yanlin Song, Ben Liu, Víctor Gutiérrez-Basulto, Zhiwei Hu, Qianqian Xie, Min Peng, Sophia Ananiadou, Jeff Z. Pan</div>
<div class="meta-line">First: 2025-10-23T16:04:13+00:00 · Latest: 2026-01-27T07:40:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20691v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.20691v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge Graph Question Answering aims to answer natural language questions by reasoning over structured knowledge graphs. While large language models have advanced KGQA through their strong reasoning capabilities, existing methods continue to struggle to fully exploit both the rich knowledge encoded in KGs and the reasoning capabilities of LLMs, particularly in complex scenarios. They often assume complete KG coverage and lack mechanisms to judge when external information is needed, and their reasoning remains locally myopic, failing to maintain coherent multi-step planning, leading to reasoning failures even when relevant knowledge exists. We propose Graph-RFT, a novel two-stage reinforcement fine-tuning KGQA framework with a &#x27;plan-KGsearch-and-Websearch-during-think&#x27; paradigm, that enables LLMs to perform autonomous planning and adaptive retrieval scheduling across KG and web sources under incomplete knowledge conditions. Graph-RFT introduces a chain-of-thought fine-tuning method with a customized plan-retrieval dataset activates structured reasoning and resolves the GRPO cold-start problem. It then introduces a novel plan-retrieval guided reinforcement learning process integrates explicit planning and retrieval actions with a multi-reward design, enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired planning module to decompose complex questions into ordered subquestions, and logical expression to guide tool invocation for globally consistent multi-step reasoning. This reasoning retrieval process is optimized with a multi-reward combining outcome and retrieval specific signals, enabling the model to learn when and how to combine KG and web retrieval effectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>先规划后检索：基于强化学习的知识图谱复杂推理方法</div>
<div class="mono" style="margin-top:8px">知识图谱问答旨在通过对结构化知识图谱进行推理来回答自然语言问题。尽管大语言模型凭借其强大的推理能力推动了KGQA的发展，但现有方法仍难以充分利用知识图谱中编码的丰富知识和大语言模型的推理能力，尤其在复杂场景下。这些方法通常假设知识图谱覆盖完整，缺乏判断何时需要外部信息的机制，且其推理过程存在局部短视问题，无法保持连贯的多步规划，导致即使存在相关知识也会出现推理失败。我们提出了Graph-RFT，一种新颖的两阶段强化微调KGQA框架，采用&#x27;规划-知识图谱搜索与网络搜索-思考&#x27;范式，使大语言模型能在知识不完备条件下跨知识图谱和网络源进行自主规划与自适应检索调度。Graph-RFT引入了一种思维链微调方法，通过定制的规划-检索数据集激活结构化推理并解决GRPO冷启动问题。随后，该框架通过创新的规划-检索引导强化学习流程，将显式规划与检索动作结合多奖励设计，实现覆盖感知的检索调度。它采用笛卡尔启发的规划模块将复杂问题分解为有序子问题，并运用逻辑表达式指导工具调用，以实现全局一致的多步推理。该推理检索过程通过结合结果信号与检索特定信号的多重奖励进行优化，使模型能学习何时及如何有效结合知识图谱与网络检索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of existing Knowledge Graph Question Answering methods, which struggle to fully leverage both the structured knowledge in KGs and the reasoning power of large language models, especially under incomplete knowledge and complex multi-step reasoning scenarios. To address this, the authors propose Graph-RFT, a two-stage reinforcement fine-tuning framework that employs a &#x27;plan-KGsearch-and-Websearch-during-think&#x27; paradigm; the method first uses chain-of-thought fine-tuning on a custom dataset to activate structured reasoning, then applies a plan-retrieval guided reinforcement learning process with a multi-reward design and a Cartesian-inspired planning module to decompose questions and schedule adaptive retrievals from KG and web sources. The main experimental results demonstrate that this approach enables autonomous planning and coverage-aware retrieval scheduling, effectively improving complex reasoning performance by learning when and how to combine different information sources under incomplete knowledge conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于现有知识图谱问答方法在充分利用知识图谱的结构化知识和大型语言模型的推理能力方面存在不足，特别是在知识不完整和复杂多步推理场景下。为此，作者提出了Graph-RFT，一个两阶段的强化学习微调框架，采用“规划-知识图谱搜索-网络搜索-思考”范式；该方法首先使用基于定制数据集的思维链微调来激活结构化推理，然后通过一个结合多奖励设计和笛卡尔式规划模块的计划-检索引导强化学习过程，将问题分解并调度从知识图谱和网络来源的自适应检索。主要实验结果表明，该方法能够实现自主规划和覆盖感知的检索调度，通过在不完整知识条件下学习何时及如何结合不同信息源，有效提升了复杂推理性能。</div>
</details>
</div>
<div class="card">
<div class="title">Output Feedback Stabilization of Linear Systems via Policy Gradient Methods</div>
<div class="meta-line">Authors: Ankang Zhang, Ming Chi, Xiaoling Wang, Lintao Ye</div>
<div class="meta-line">First: 2026-01-27T07:15:59+00:00 · Latest: 2026-01-27T07:15:59+00:00</div>
<div class="meta-line">Comments: 31 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19284v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19284v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stabilizing a dynamical system is a fundamental problem that serves as a cornerstone for many complex tasks in the field of control systems. The problem becomes challenging when the system model is unknown. Among the Reinforcement Learning (RL) algorithms that have been successfully applied to solve problems pertaining to unknown linear dynamical systems, the policy gradient (PG) method stands out due to its ease of implementation and can solve the problem in a model-free manner. However, most of the existing works on PG methods for unknown linear dynamical systems assume full-state feedback. In this paper, we take a step towards model-free learning for partially observable linear dynamical systems with output feedback and focus on the fundamental stabilization problem of the system. We propose an algorithmic framework that stretches the boundary of PG methods to the problem without global convergence guarantees. We show that by leveraging zeroth-order PG update based on system trajectories and its convergence to stationary points, the proposed algorithms return a stabilizing output feedback policy for discrete-time linear dynamical systems. We also explicitly characterize the sample complexity of our algorithm and verify the effectiveness of the algorithm using numerical examples.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略梯度方法的线性系统输出反馈镇定</div>
<div class="mono" style="margin-top:8px">镇定动态系统是控制领域的基础性问题，为众多复杂任务提供支撑。当系统模型未知时，该问题变得极具挑战性。在已成功应用于未知线性动态系统的强化学习算法中，策略梯度方法因其易于实现且能以无模型方式解决问题而备受关注。然而，现有针对未知线性动态系统的策略梯度研究大多假设全状态反馈。本文针对具有输出反馈的部分可观线性动态系统，向无模型学习迈进一步，聚焦于系统的基础镇定问题。我们提出一种算法框架，将策略梯度方法拓展至缺乏全局收敛保证的问题场景。通过基于系统轨迹的零阶策略梯度更新及其向驻点的收敛性，所提算法能为离散时间线性动态系统返回镇定的输出反馈策略。我们明确量化了算法的样本复杂度，并通过数值算例验证了算法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the stabilization problem for partially observable linear dynamical systems with unknown models, where existing policy gradient methods typically assume full-state feedback. The method introduces a model-free algorithmic framework that extends policy gradient techniques to output feedback scenarios, utilizing zeroth-order updates based on system trajectories to converge to stationary points. The main experimental results demonstrate that the proposed algorithm successfully returns stabilizing output feedback policies for discrete-time linear systems, with explicit sample complexity characterization and numerical validation of its effectiveness.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决模型未知的部分可观测线性动态系统的镇定问题，现有策略梯度方法通常假设全状态反馈。该方法提出了一种无模型的算法框架，将策略梯度技术扩展到输出反馈场景，利用基于系统轨迹的零阶更新收敛至驻点。主要实验结果表明，所提算法成功为离散时间线性系统返回镇定的输出反馈策略，明确了样本复杂度并通过数值示例验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning</div>
<div class="meta-line">Authors: Kishan Panaganti, Zhenwen Liang, Wenhao Yu, Haitao Mi, Dong Yu</div>
<div class="meta-line">First: 2026-01-27T07:10:41+00:00 · Latest: 2026-01-27T07:10:41+00:00</div>
<div class="meta-line">Comments: Keywords: Large Language Models, Reasoning Models, Reinforcement Learning, Distributionally Robust Optimization, GRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19280v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19280v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.
  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model&#x27;s performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型推理的群组分布鲁棒优化驱动强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型推理能力的近期进展，日益依赖于训练后损失函数与对齐策略的优化。然而，以群组相对策略优化为代表的传统强化学习范式仍受限于静态均匀性：均匀的提示采样与每提示固定次数的推演。对于异构、重尾的推理数据，这会造成结构性低效——在已解决的模式上浪费算力，而对困难问题的长尾部分训练不足。为此，我们提出多对抗者群组分布鲁棒优化，这是一个以优化为先的框架，通过动态调整训练分布，超越了均匀推理模型。我们引入在线难度分类器，将提示动态划分为基于pass@k的难度群组，并设计了两种独立的训练后GDRO博弈：(1) 提示-GDRO，采用EMA去偏乘性权重赌博机采样器，聚焦于密集难度边界，对持续困难群组进行无频率偏见的加权提升；(2) 推演-GDRO，利用影子价格控制器在群组间重新分配推演次数，在固定平均预算下最大化困难任务的梯度方差缩减。我们为两种控制器提供了无悔保证，并对推演-GDRO进行了方差代理分析，推导出平方根最优的推演分配方案。在DAPO 14.1k数据集上使用Qwen3-Base模型的验证表明，相比GRPO基线，提示-GDRO与推演-GDRO在1.7B、4B和8B规模上的pass@8准确率平均相对提升分别达到+10.6%和+10.1%。定性分析揭示了自涌现的课程学习机制：对抗者将资源动态调配至演进的推理前沿，从而提升推理模型的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of standard RL methods like GRPO, which use uniform sampling and fixed rollouts for heterogeneous reasoning tasks, this paper proposes Multi-Adversary Group Distributionally Robust Optimization (GDRO) to dynamically adapt training. The method employs an Online Difficulty Classifier to group prompts by difficulty and introduces two GDRO games: Prompt-GDRO uses a bandit sampler to upweight hard groups, and Rollout-GDRO reallocates rollouts via a shadow-price controller to maximize gradient variance reduction under a fixed compute budget. Experimental results on the DAPO dataset with Qwen3-Base models show average relative gains of +10.6% and +10.1% in pass@8 accuracy over GRPO across model scales, with qualitative analysis revealing an emergent curriculum that shifts resources to challenging reasoning frontiers.</div>
<div class="mono" style="margin-top:8px">本文针对标准强化学习方法（如GRPO）在异构推理任务中因均匀采样和固定轮次导致的效率低下问题，提出了多对抗组分布鲁棒优化（GDRO）框架以动态调整训练分布。方法通过在线难度分类器将提示按难度动态分组，并引入两个GDRO博弈：Prompt-GDRO使用带偏置乘性权重的赌博机采样器来侧重困难组，而Rollout-GDRO通过影子价格控制器在固定计算预算下重新分配轮次以最大化困难任务的梯度方差减少。在DAPO数据集上使用Qwen3-Base模型的实验结果表明，相比GRPO基线，在1.7B、4B和8B规模上，两种方法在pass@8准确率上分别平均相对提升10.6%和10.1%，定性分析显示资源向不断演化的推理前沿转移，形成了涌现课程。</div>
</details>
</div>
<div class="card">
<div class="title">Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model</div>
<div class="meta-line">Authors: Qi Si, Xuyang Liu, Penglei Wang, Xin Guo, Yuan Qi, Yuan Cheng</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-27T06:04:02+00:00 · Latest: 2026-01-27T06:04:02+00:00</div>
<div class="meta-line">Comments: 20 pages (7 pages content + 2 pages references + 11 pages appendix), 11 figures, 8 tables. Source code available at https://github.com/darkflash03/SOLD Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19232v1">PDF</a> · <a href="https://github.com/darkflash03/SOLD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">RNA inverse folding, designing sequences to form specific 3D structures, is critical for therapeutics, gene regulation, and synthetic biology. Current methods, focused on sequence recovery, struggle to address structural objectives like secondary structure consistency (SS), minimum free energy (MFE), and local distance difference test (LDDT), leading to suboptimal structural accuracy. To tackle this, we propose a reinforcement learning (RL) framework integrated with a latent diffusion model (LDM). Drawing inspiration from the success of diffusion models in RNA inverse folding, which adeptly model complex sequence-structure interactions, we develop an LDM incorporating pre-trained RNA-FM embeddings from a large-scale RNA model. These embeddings capture co-evolutionary patterns, markedly improving sequence recovery accuracy. However, existing approaches, including diffusion-based methods, cannot effectively handle non-differentiable structural objectives. By contrast, RL excels in this task by using policy-driven reward optimization to navigate complex, non-gradient-based objectives, offering a significant advantage over traditional methods. In summary, we propose the Step-wise Optimization of Latent Diffusion Model (SOLD), a novel RL framework that optimizes single-step noise without sampling the full diffusion trajectory, achieving efficient refinement of multiple structural objectives. Experimental results demonstrate SOLD surpasses its LDM baseline and state-of-the-art methods across all metrics, establishing a robust framework for RNA inverse folding with profound implications for biotechnological and therapeutic applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结构的RNA设计：通过潜扩散模型的分步优化</div>
<div class="mono" style="margin-top:8px">RNA逆折叠旨在设计能形成特定三维结构的序列，对治疗学、基因调控和合成生物学至关重要。现有方法侧重于序列恢复，难以兼顾二级结构一致性（SS）、最小自由能（MFE）和局部距离差异测试（LDDT）等结构目标，导致结构准确性欠佳。为此，我们提出一种与潜扩散模型（LDM）结合的强化学习（RL）框架。受扩散模型在RNA逆折叠中成功建模复杂序列-结构相互作用的启发，我们开发了一种融合大规模RNA模型预训练RNA-FM嵌入的LDM。这些嵌入能捕捉协同进化模式，显著提升序列恢复精度。然而，现有方法（包括基于扩散的方法）难以有效处理不可微分的结构目标。相比之下，RL通过策略驱动的奖励优化处理复杂的非梯度目标，较传统方法具有显著优势。综上，我们提出潜扩散模型分步优化（SOLD）这一新型RL框架，通过优化单步噪声而非采样完整扩散轨迹，实现对多重结构目标的高效优化。实验表明，SOLD在所有指标上均超越其LDM基线和前沿方法，为RNA逆折叠建立了稳健框架，对生物技术和治疗应用具有深远意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing RNA inverse folding methods, which prioritize sequence recovery but often yield suboptimal structural accuracy regarding secondary structure consistency, minimum free energy, and local distance difference, this paper introduces SOLD, a reinforcement learning framework integrated with a latent diffusion model. The method leverages pre-trained RNA-FM embeddings to capture co-evolutionary patterns for improved sequence modeling and employs RL to optimize non-differentiable structural objectives through step-wise noise optimization without full diffusion trajectory sampling. Experimentally, SOLD outperforms its latent diffusion baseline and state-of-the-art methods across all structural metrics, establishing a robust framework with significant potential for biotechnological and therapeutic applications.</div>
<div class="mono" style="margin-top:8px">针对现有RNA逆折叠方法侧重于序列恢复但常导致二级结构一致性、最小自由能和局部距离差异等结构目标精度不足的问题，本文提出了SOLD，一种与潜在扩散模型集成的强化学习框架。该方法利用预训练的RNA-FM嵌入捕捉协同进化模式以改进序列建模，并通过强化学习以逐步噪声优化的方式处理不可微的结构目标，无需对整个扩散轨迹采样。实验结果表明，SOLD在所有结构指标上均超越了其潜在扩散模型基线和现有最先进方法，为生物技术和治疗应用建立了一个强大的框架。</div>
</details>
</div>
<div class="card">
<div class="title">Text2Grad: Reinforcement Learning from Natural Language Feedback</div>
<div class="meta-line">Authors: Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-28T13:23:49+00:00 · Latest: 2026-01-27T04:31:51+00:00</div>
<div class="meta-line">Comments: The code for our method is available at https://github.com/microsoft/Text2Grad</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22338v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.22338v2">PDF</a> · <a href="https://github.com/microsoft/Text2Grad">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model&#x27;s policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answers while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results suggest that natural-language feedback can serve not only as explanations, but also as actionable training signals for fine-grained alignment. The code for our method is available at https://github.com/microsoft/Text2Grad.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Text2Grad：基于自然语言反馈的强化学习</div>
<div class="mono" style="margin-top:8px">传统RLHF使用粗糙的标量奖励优化语言模型，掩盖了成功或失败的细粒度原因，导致学习过程缓慢且不透明。近期研究通过提示或反思用文本批评增强强化学习，提升了可解释性但未改变模型参数。我们提出Text2Grad，一种将自由形式文本反馈转化为片段级梯度的强化学习范式。给定人类（或程序化）批评后，Text2Grad将每个反馈短语与相关词元片段对齐，将这些对齐转化为可微奖励信号，并执行梯度更新以直接修正模型策略中的问题部分。这实现了精确的、基于反馈的调整而非全局微调。Text2Grad通过三个组件实现：(1) 将批评与词元片段配对的高质量反馈标注流程；(2) 预测答案片段级奖励并生成解释性批评的细粒度奖励模型；(3) 反向传播自然语言梯度的片段级策略优化器。在摘要生成、代码生成和问答任务中，Text2Grad持续超越标量奖励强化学习和纯提示基线，既提供更高的任务指标，也带来更丰富的可解释性。结果表明，自然语言反馈不仅能作为解释，还可作为细粒度对齐的可操作训练信号。方法代码发布于https://github.com/microsoft/Text2Grad。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional RLHF, which uses coarse scalar rewards that obscure fine-grained learning signals, this paper introduces Text2Grad, a reinforcement learning method that converts free-form textual feedback into span-level gradients. The method aligns critique phrases with relevant token spans, transforms these into differentiable rewards, and performs gradient updates to directly refine specific portions of the model&#x27;s policy, enabling precise, feedback-conditioned adjustments. Experimental results on summarization, code generation, and question answering show that Text2Grad consistently outperforms scalar-reward RL and prompt-only baselines in both task performance and interpretability, demonstrating that natural-language feedback can serve as actionable training signals for fine-grained alignment.</div>
<div class="mono" style="margin-top:8px">本文的动机是传统RLHF使用粗粒度的标量奖励，掩盖了细粒度的学习信号，导致学习缓慢且不透明。为此，作者提出了Text2Grad方法，将自由形式的文本反馈转化为词元级别的梯度：该方法将批评短语与相关词元对齐，将其转换为可微分的奖励信号，并通过梯度更新直接优化模型策略的特定部分，实现精确的反馈条件调整。在摘要、代码生成和问答任务上的实验结果表明，Text2Grad在任务指标和可解释性上均一致优于标量奖励强化学习和仅提示的基线方法，证明自然语言反馈可作为细粒度对齐的有效训练信号。</div>
</details>
</div>
<div class="card">
<div class="title">Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation</div>
<div class="meta-line">Authors: Xiao He, Huangxuan Zhao, Guojia Wan, Wei Zhou, Yanxing Liu, Juhua Liu, Yongchao Xu, Yong Luo, Dacheng Tao, Bo Du</div>
<div class="meta-line">First: 2025-10-14T19:57:03+00:00 · Latest: 2026-01-27T04:30:00+00:00</div>
<div class="meta-line">Comments: This paper contains fundamental errors and will not be replaced</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.12953v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.12953v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hexiao0275.github.io/FetalMind">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent medical vision-language models have shown promise on tasks such as VQA, report generation, and anomaly detection. However, most are adapted to structured adult imaging and underperform in fetal ultrasound, which poses challenges of multi-view image reasoning, numerous diseases, and image diversity. To bridge this gap, we introduce FetalMind, a medical AI system tailored to fetal ultrasound for both report generation and diagnosis. Guided by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which injects an expert-curated bipartite graph into the model to decouple view-disease associations and to steer preference selection along clinically faithful steps via reinforcement learning. This design mitigates variability across diseases and heterogeneity across views, reducing learning bottlenecks while aligning the model&#x27;s inference with obstetric practice. To train FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale fetal ultrasound report corpus, comprising 20K reports from twelve medical centers, addressing the scarcity of domain data. Extensive experiments show that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable. Project Page: https://hexiao0275.github.io/FetalMind.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向胎儿超声解读的认知感知视觉-语言基础模型</div>
<div class="mono" style="margin-top:8px">近期医学视觉-语言模型在视觉问答、报告生成和异常检测等任务中展现出潜力，但多数模型适配结构化成人影像，在胎儿超声领域表现欠佳。胎儿超声面临多视图图像推理、疾病种类繁多和图像多样性等挑战。为填补这一空白，我们推出专为胎儿超声设计的医学人工智能系统FetalMind，兼具报告生成与诊断功能。在临床工作流指导下，我们提出显著认知解耦方法，通过向模型注入专家构建的二部图，解耦视图与疾病的关联，并借助强化学习引导模型沿临床可信步骤进行偏好选择。该设计缓解了疾病间的变异性和视图间的异质性，减少学习瓶颈，同时使模型推理与产科实践保持一致。为大规模训练FetalMind，我们构建了首个大规模胎儿超声报告数据集FetalSigma-1M，包含来自12个医疗中心的2万份报告，缓解了领域数据稀缺问题。大量实验表明，FetalMind在所有孕周阶段均优于开源与闭源基线模型，平均性能提升14%，关键病症准确率提高61.2%，同时保持高效、稳定和可扩展性。项目页面：https://hexiao0275.github.io/FetalMind。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing medical vision-language models, which are primarily designed for structured adult imaging and perform poorly on fetal ultrasound due to challenges like multi-view reasoning, numerous diseases, and image diversity. The method introduces FetalMind, a system tailored for fetal ultrasound, which employs Salient Epistemic Disentanglement (SED) to decouple view-disease associations using an expert-curated bipartite graph and steers preference selection via reinforcement learning, aligning inference with clinical workflow. Main experimental results demonstrate that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving an average gain of +14% and a +61.2% higher accuracy on critical conditions, while remaining efficient and scalable, supported by the newly curated FetalSigma-1M dataset.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有医学视觉语言模型的局限性，这些模型主要针对结构化成人影像设计，在胎儿超声中表现不佳，原因包括多视图推理、疾病种类繁多和图像多样性等挑战。方法上提出了FetalMind系统，专为胎儿超声定制，采用显著性认知解耦技术，通过专家构建的二部图解耦视图与疾病关联，并利用强化学习引导偏好选择，使推理与临床工作流程对齐。主要实验结果表明，FetalMind在所有孕周阶段均优于开源和闭源基线模型，平均性能提升14%，在关键病症上的准确率提高61.2%，同时保持高效和可扩展性，并基于新构建的FetalSigma-1M数据集进行训练。</div>
</details>
</div>
<div class="card">
<div class="title">Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind</div>
<div class="meta-line">Authors: Zhitao He, Zongwei Lyu, Yi R Fung</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-22T07:36:48+00:00 · Latest: 2026-01-27T04:09:51+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026, 36 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15715v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15715v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author&#x27;s own critical analysis and response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>镣铐之舞：基于心智理论的学术反驳策略性说服</div>
<div class="mono" style="margin-top:8px">尽管人工智能已深度融入研究流程的各个阶段并取得显著进展，学术反驳仍是一个重要且尚未充分探索的挑战。这是因为反驳是在严重信息不对称下进行的复杂策略性沟通过程，而非简单的技术辩论。因此，当前方法大多仅模仿表层语言特征，难以奏效，缺失了有效说服所需的核心要素——观点采择。本文提出首个基于心智理论的学术反驳框架RebuttalAgent，通过心智理论-策略-响应的三阶段流程实现：建模审稿人心理状态、制定说服策略、生成策略驱动的回应。为训练智能体，我们构建了大规模数据集RebuttalBench，采用创新的批判-精炼方法合成数据。训练过程分为两阶段：首先通过监督微调赋予智能体基于心智理论的分析与策略规划能力，随后利用自奖励机制进行强化学习以实现可扩展的自我改进。为进行可靠高效的自动化评估，我们进一步开发了专用评估器Rebuttal-RM，基于超10万条多源反驳数据训练，其评分与人类偏好的一致性已超越GPT-4.1。大量实验表明，RebuttalAgent在自动化指标上平均超越基线模型18.3%，同时在自动化与人工评估中均优于先进的专有模型。免责声明：生成的反驳内容仅供作者启发思路与辅助草拟，不可替代作者自身的批判性分析与回应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complex strategic communication and information asymmetry inherent in academic rebuttal, which existing AI approaches fail to address by merely imitating surface-level language, this paper introduces RebuttalAgent, a framework grounded in Theory of Mind (ToM). The method employs a ToM-Strategy-Response pipeline to model reviewer mental states and formulate persuasive strategies, trained via supervised fine-tuning and reinforcement learning on a novel large-scale dataset, RebuttalBench, synthesized through a critique-and-refine approach. Main experimental results, evaluated by a specialized automated evaluator, Rebuttal-RM, show that RebuttalAgent outperforms the base model by an average of 18.3% on automated metrics and surpasses advanced proprietary models in both automated and human evaluations.</div>
<div class="mono" style="margin-top:8px">本文的动机在于学术反驳是一个存在严重信息不对称的复杂战略沟通过程，而现有人工智能方法仅模仿表面语言，未能解决这一挑战。为此，论文提出了首个基于心智理论（ToM）的框架RebuttalAgent，其方法通过ToM-策略-响应流程建模审稿人心理状态并制定说服策略，并利用基于批判-精炼方法合成的大规模数据集RebuttalBench，通过监督微调和强化学习进行两阶段训练。主要实验结果表明，在专门开发的自动评估器Rebuttal-RM的评测下，RebuttalAgent在自动指标上平均优于基线模型18.3%，并在自动和人工评估中均超越了先进的专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs</div>
<div class="meta-line">Authors: Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li</div>
<div class="meta-line">First: 2025-09-25T11:51:05+00:00 · Latest: 2026-01-27T03:49:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21044v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21044v2">PDF</a> · <a href="https://github.com/tsinghua-fib-lab/llm_rl_probing_analysis">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families and mathematical datasets shows two robust effects of online RL post-training: (i) an overall increase in average activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in mathematical generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://github.com/tsinghua-fib-lab/llm_rl_probing_analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习微调增强大语言模型内部回路的激活强度与多样性</div>
<div class="mono" style="margin-top:8px">大语言模型通过大规模预训练获得广泛先验知识，并可通过监督微调或基于强化学习的后训练进一步优化。现有研究表明，强化学习微调能带来超越单纯监督微调的性能提升，但其对不同内在特性模型的增强机制尚不明确。本研究受边缘归因分析方法的启发，探究强化学习微调前后模型内部结构的变化。通过对多模型系列与数学数据集的实验分析，发现在线强化学习后训练产生两种稳健效应：（1）平均激活强度整体提升，表明更多内部通路被激活且信号增强；（2）激活模式多样性增加，表现为更高熵值与更分散的边缘分布。这些变化显示强化学习通过重塑信息流路径，使其更具冗余性与灵活性，这或许能解释其在数学泛化任务中的优势。值得注意的是，采用直接偏好优化微调的模型偏离此趋势，其内部变化显著弱于基于近端策略优化与组相对策略优化的训练。本研究系统揭示了强化学习微调如何改变大语言模型内部回路，并凸显了在线强化学习与基于偏好方法之间的方法论差异。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the internal mechanisms by which reinforcement learning (RL) fine-tuning enhances large language model (LLM) capabilities beyond supervised fine-tuning, motivated by the need to understand why RL improves performance across diverse models. Using edge attribution patching (EAP) to analyze internal circuitry across multiple model families on mathematical datasets, the method reveals that online RL post-training consistently increases average activation intensity and diversifies activation patterns, indicating more engaged and flexible information flow. The main experimental results show these changes correlate with improved mathematical generalization, though models fine-tuned with Direct Preference Optimization (DPO) exhibit weaker or inconsistent internal alterations compared to PPO- and GRPO-based approaches, highlighting methodological distinctions.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究强化学习微调如何从内部机制上增强大语言模型的能力，其动机源于需要理解为何强化学习能超越监督微调，提升不同模型的性能。通过使用边缘归因修补技术分析多个模型系列在数学数据集上的内部电路，方法表明在线强化学习后训练能持续提高平均激活强度并增加激活模式的多样性，意味着信息流更活跃和灵活。主要实验结果显示这些变化与数学泛化能力提升相关，但使用直接偏好优化微调的模型相较于基于PPO和GRPO的方法表现出更弱或不一致的内部改变，从而凸显了方法学上的差异。</div>
</details>
</div>
<div class="card">
<div class="title">RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning</div>
<div class="meta-line">Authors: Qianyue Hao, Sibo Li, Jian Yuan, Yong Li</div>
<div class="meta-line">First: 2025-05-20T09:43:33+00:00 · Latest: 2026-01-27T03:39:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.14140v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.14140v3">PDF</a> · <a href="https://github.com/tsinghua-fib-lab/RL-LLM-Reasoning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs&#x27; parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://github.com/tsinghua-fib-lab/RL-LLM-Reasoning for reproducibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维强化学习：通过推理时强化学习引导大语言模型推理</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLM）发展迅速，但其基于词元的自回归特性限制了复杂推理能力。为提升LLM推理，推理时技术（如思维链/树/图）通过引导复杂逻辑结构而不修改模型参数，以较低成本有效提升了性能。然而，这些手动预定义、任务无关的框架在不同任务中统一应用，缺乏适应性。为此，我们提出思维强化学习（RLoT），利用强化学习训练轻量级导航器模型，在推理时自适应增强LLM推理。具体而言，我们从人类认知视角设计了五种基础逻辑模块。在推理过程中，训练后的RL导航器根据问题特征动态选择合适逻辑模块，组合成任务专属的逻辑结构。在多个推理基准（AIME、MATH、GPQA等）和多种LLM（GPT、Llama、Qwen、DeepSeek）上的实验表明，RLoT相比现有推理时技术最高提升13.4%。值得注意的是，仅用不足3K参数的RL导航器，可使百亿级以下LLM达到千亿级模型的性能。此外，该导航器展现出强泛化能力：基于特定LLM-任务对训练的模型能有效迁移至未见过的LLM和任务。代码已开源：https://github.com/tsinghua-fib-lab/RL-LLM-Reasoning。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of existing inference-time reasoning techniques for large language models (LLMs), which rely on manually predefined, task-agnostic logical structures that lack adaptability. The proposed method, RL-of-Thoughts (RLoT), addresses this by training a lightweight navigator model with reinforcement learning to dynamically select and combine five basic logic blocks, derived from human cognition, into task-specific reasoning structures during inference. Main experimental results across multiple reasoning benchmarks (e.g., AIME, MATH, GPQA) and LLMs (e.g., GPT, Llama) show that RLoT outperforms established techniques by up to 13.4%, enables sub-10B LLMs to match the performance of 100B-scale models, and demonstrates strong transferability to unseen LLMs and tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于现有大语言模型推理时技术（如思维链/树/图）的局限性，这些技术依赖手动预定义、与任务无关的逻辑框架，缺乏适应性。为此，本文提出了RL-of-Thoughts方法，通过强化学习训练一个轻量级导航器模型，在推理时根据问题特征，动态选择并组合五个基于人类认知的基本逻辑块，形成任务特定的推理结构。主要实验结果表明，在多个推理基准和不同大语言模型上，该方法比现有技术性能提升最高达13.4%，仅用不足3K参数的导航器就能使百亿参数以下的模型达到千亿参数模型的推理水平，并且展现出对未见过的模型和任务的强泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Endless Terminals: Scaling RL Environments for Terminal Agents</div>
<div class="meta-line">Authors: Kanishk Gandhi, Shivam Garg, Noah D. Goodman, Dimitris Papailiopoulos</div>
<div class="meta-line">First: 2026-01-23T04:39:55+00:00 · Latest: 2026-01-27T03:34:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16443v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16443v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无尽终端：面向终端智能体的强化学习环境规模化构建</div>
<div class="mono" style="margin-top:8px">环境是自进化智能体的发展瓶颈。现有终端基准仅为评估设计，而非训练用途；强化学习需要可扩展的流程，而非单纯数据集。我们提出&#x27;无尽终端&#x27;——无需人工标注、通过程序化生成终端任务的自主流程。该流程包含四个阶段：生成多样化任务描述、构建验证容器化环境、生成完成度测试、基于可解性筛选。由此获得3255项涵盖文件操作、日志管理、数据处理、脚本编写及数据库操作的任务。我们采用原始PPO算法配合二元回合奖励及极简交互循环（无检索/多智能体协调/专用工具）训练智能体。尽管设计简洁，在无尽终端上训练的模型仍取得显著提升：在预留开发集上，Llama-3.2-3B从4.0%提升至18.2%，Qwen2.5-7B从10.7%提升至53.3%，Qwen3-8B-openthinker-sft从42.6%提升至59.0%。该提升可迁移至人工标注基准：在TerminalBench 2.0中，Llama-3.2-3B从0.0%提升至2.2%，Qwen2.5-7B从2.2%提升至3.4%，Qwen3-8B-openthinker-sft从1.1%提升至6.7%，均优于包含复杂智能体框架在内的其他方案。结果表明：当环境实现规模化时，简易强化学习即可取得成效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the bottleneck of environment scarcity for training self-improving terminal agents, as existing benchmarks are designed for evaluation rather than scalable reinforcement learning. It introduces Endless Terminals, an autonomous pipeline that procedurally generates terminal-use tasks without human annotation through four stages: generating task descriptions, building containerized environments, producing completion tests, and filtering for solvability, yielding 3255 tasks across domains like file operations and scripting. Using vanilla PPO with binary rewards and a minimal interaction loop, models trained on this pipeline show substantial gains, with Llama-3.2-3B improving from 4.0% to 18.2% on a held-out dev set and transferring to human-curated benchmarks like TerminalBench 2.0, where Qwen3-8B-openthinker-sft rises from 1.1% to 6.7%, outperforming more complex agentic approaches and demonstrating that simple RL succeeds with scaled environments.</div>
<div class="mono" style="margin-top:8px">本文针对自改进终端智能体训练中环境稀缺的瓶颈问题，指出现有基准测试仅适用于评估而非可扩展的强化学习训练。为此，提出了Endless Terminals这一全自动流程，通过生成任务描述、构建容器化环境、生成完成测试及筛选可解性四个阶段，无需人工标注即可程序化生成终端使用任务，共获得3255个涵盖文件操作、日志管理等领域的任务。使用带二元奖励的普通PPO算法和最小交互循环进行训练后，模型性能显著提升：例如，Llama-3.2-3B在内部开发集上从4.0%提升至18.2%，并在人工标注的TerminalBench 2.0基准测试中实现迁移改进，如Qwen3-8B-openthinker-sft从1.1%提升至6.7%，优于更复杂的智能体框架，证明了环境规模化后简单强化学习的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Q-Probe: Scaling Image Quality Assessment to High Resolution via Context-Aware Agentic Probing</div>
<div class="meta-line">Authors: Xiang Li, XueHeng Li, Yu Wang, XuanHua He, ZhangChi Hu, WeiWei Yu, ChengJun Xie</div>
<div class="meta-line">First: 2026-01-21T08:02:32+00:00 · Latest: 2026-01-27T03:25:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15356v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15356v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has empowered Multimodal Large Language Models (MLLMs) to achieve superior human preference alignment in Image Quality Assessment (IQA). However, existing RL-based IQA models typically rely on coarse-grained global views, failing to capture subtle local degradations in high-resolution scenarios. While emerging &quot;Thinking with Images&quot; paradigms enable multi-scale visual perception via zoom-in mechanisms, their direct adaptation to IQA induces spurious &quot;cropping-implies-degradation&quot; biases and misinterprets natural depth-of-field as artifacts. To address these challenges, we propose Q-Probe, the first agentic IQA framework designed to scale IQA to high resolution via context-aware probing. First, we construct Vista-Bench, a pioneering benchmark tailored for fine-grained local degradation analysis in high-resolution IQA settings. Furthermore, we propose a three-stage training paradigm that progressively aligns the model with human preferences, while simultaneously eliminating causal bias through a novel context-aware cropping strategy. Extensive experiments demonstrate that Q-Probe achieves state-of-the-art performance in high-resolution settings while maintaining superior efficacy across resolution scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Q-Probe：通过上下文感知智能探测将图像质量评估扩展至高分辨率</div>
<div class="mono" style="margin-top:8px">强化学习（RL）赋能多模态大语言模型（MLLM）在图像质量评估（IQA）中实现了更优的人类偏好对齐。然而，现有基于RL的IQA模型通常依赖粗粒度的全局视图，难以捕捉高分辨率场景下细微的局部退化。尽管新兴的“图像思维”范式通过放大机制实现了多尺度视觉感知，但其直接应用于IQA会引发虚假的“裁剪即退化”偏见，并将自然景深误解为伪影。为解决这些挑战，我们提出Q-Probe——首个通过上下文感知探测将IQA扩展至高分辨率的智能代理框架。首先，我们构建了Vista-Bench，这是专为高分辨率IQA场景中细粒度局部退化分析设计的开创性基准。此外，我们提出一种三阶段训练范式，在逐步对齐模型与人类偏好的同时，通过新颖的上下文感知裁剪策略消除因果偏见。大量实验表明，Q-Probe在高分辨率场景下实现了最先进的性能，同时在不同分辨率尺度上保持卓越效能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing Reinforcement Learning (RL)-based Image Quality Assessment (IQA) models, which rely on coarse global views and fail to capture subtle local degradations in high-resolution images, while also avoiding the biases introduced by direct adaptation of multi-scale &#x27;Thinking with Images&#x27; paradigms. The method proposed, Q-Probe, is a novel agentic IQA framework that scales to high resolution via context-aware probing; it introduces a new benchmark called Vista-Bench for fine-grained local degradation analysis and employs a three-stage training paradigm with a context-aware cropping strategy to align the model with human preferences and eliminate causal bias. The main experimental results show that Q-Probe achieves state-of-the-art performance in high-resolution IQA settings while maintaining superior efficacy across different resolution scales.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有基于强化学习的图像质量评估模型在高分辨率场景下的局限性，这些模型依赖粗粒度的全局视图，无法捕捉细微的局部退化，同时避免直接采用多尺度“图像思维”范式所带来的偏差。所提出的方法Q-Probe是一种新型的智能体化图像质量评估框架，通过上下文感知的探测来适应高分辨率；它引入了一个名为Vista-Bench的新基准，用于细粒度局部退化分析，并采用三阶段训练范式及上下文感知裁剪策略，以对齐人类偏好并消除因果偏差。主要实验结果表明，Q-Probe在高分辨率图像质量评估中实现了最先进的性能，同时在不同分辨率尺度上均保持卓越的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand</div>
<div class="meta-line">Authors: Kiattikun Chobtham</div>
<div class="meta-line">First: 2026-01-15T08:40:01+00:00 · Latest: 2026-01-27T03:13:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10181v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.10181v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习发现东北季风指数以预测泰国月降雨量</div>
<div class="mono" style="margin-top:8px">气候预测因地球系统复杂的时空模式而面临挑战。全球气候指数（如厄尔尼诺-南方涛动）是长期降雨预测的标准输入特征。然而，在泰国特定区域能提升预测精度的局地尺度指数仍存在显著空白。本文提出一种基于海表温度计算的新型东北季风气候指数，以反映北半球冬季季风的气候特征。为优化该指数计算区域，深度Q网络强化学习智能体通过探索并选择与季节性降雨相关性最高的矩形区域来实现。降雨站点被划分为12个独立聚类，以区分泰国南部与北部地区的降雨模式。实验结果表明，将优化后的指数融入长短期记忆模型，能显著提升多数聚类区域的长期月降雨预测能力，有效降低12个月前瞻预测的均方根误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of improving long-term monthly rainfall prediction in Thailand by developing a localized climate index, as existing global indices like ENSO are insufficient for regional accuracy. The authors propose a novel NorthEast monsoon index derived from sea surface temperature and optimize its calculation areas using a Deep Q-Network reinforcement learning agent, which selects rectangular regions with the highest correlation to seasonal rainfall. Experiments across 12 clustered rainfall stations demonstrate that integrating this optimized index into Long Short-Term Memory models significantly enhances prediction skill, notably reducing Root Mean Square Error for 12-month-ahead forecasts in most areas.</div>
<div class="mono" style="margin-top:8px">本文针对泰国长期月降雨量预测的挑战，旨在开发一个本地化气候指数，因为如ENSO等全球指数难以满足区域预测精度需求。作者提出了一种基于海表温度的新型东北季风指数，并利用深度Q网络强化学习智能体优化其计算区域，选择与季节性降雨相关性最高的矩形区域。在12个聚类降雨站的实验中，将优化后的指数融入长短期记忆模型显著提升了预测能力，在大多数区域有效降低了12个月前瞻预测的均方根误差。</div>
</details>
</div>
<div class="card">
<div class="title">Risk-Sensitive Agent Compositions</div>
<div class="meta-line">Authors: Guruprerana Shabadi, Rajeev Alur</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-05T05:04:44+00:00 · Latest: 2026-01-27T03:02:04+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures. Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04632v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.04632v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">From software development to robot control, modern agentic systems decompose complex objectives into a sequence of subtasks and choose a set of specialized AI agents to complete them. We formalize agentic workflows as directed acyclic graphs, called agent graphs, where edges represent AI agents and paths correspond to feasible compositions of agents. Real-world deployment requires selecting agent compositions that not only maximize task success but also minimize violations of safety, fairness, and privacy requirements which demands a careful analysis of the low-probability (tail) behaviors of compositions of agents. In this work, we consider risk minimization over the set of feasible agent compositions and seek to minimize the value-at-risk and the conditional value-at-risk of the loss distribution of the agent composition where the loss quantifies violations of these requirements. We introduce an efficient algorithm which traverses the agent graph and finds a near-optimal composition of agents. It uses a dynamic programming approach to approximate the value-at-risk of agent compositions by exploiting a union bound. Furthermore, we prove that the approximation is near-optimal asymptotically for a broad class of practical loss functions. We also show how our algorithm can be used to approximate the conditional value-at-risk as a byproduct. To evaluate our framework, we consider a suite of video game-like control benchmarks that require composing several agents trained with reinforcement learning and demonstrate our algorithm&#x27;s effectiveness in approximating the value-at-risk and identifying the optimal agent composition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>风险敏感型智能体组合</div>
<div class="mono" style="margin-top:8px">从软件开发到机器人控制，现代智能体系统将复杂目标分解为一系列子任务，并选择一组专用人工智能智能体来完成它们。我们将智能体工作流形式化为有向无环图（称为智能体图），其中边代表人工智能智能体，路径对应可行的智能体组合。实际部署不仅需要最大化任务成功率，还需最小化对安全、公平和隐私要求的违反，这要求对智能体组合的低概率（尾部）行为进行细致分析。本文考虑在可行智能体组合集合上进行风险最小化，旨在最小化智能体组合损失分布的风险价值和条件风险价值，其中损失量化了对上述要求的违反程度。我们提出一种高效算法，通过遍历智能体图寻找近似最优的智能体组合。该算法采用动态规划方法，利用并集界近似计算智能体组合的风险价值，并证明对于广泛的实际损失函数类别，该近似具有渐近近优性。我们还展示了如何将算法扩展用于近似条件风险价值。为评估框架性能，我们构建了一套类视频游戏控制基准测试，需要组合多个通过强化学习训练的智能体，并验证了算法在近似风险价值及识别最优智能体组合方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for AI agent compositions in workflows to not only maximize task success but also minimize violations of safety, fairness, and privacy requirements, this paper formalizes agentic workflows as directed acyclic graphs and addresses risk minimization over feasible agent compositions. The method introduces an efficient dynamic programming algorithm that traverses the agent graph to approximate the value-at-risk and conditional value-at-risk of loss distributions, using a union bound and proving near-optimal asymptotic approximation for practical loss functions. Experimental results on video game-like control benchmarks with reinforcement learning agents demonstrate the algorithm&#x27;s effectiveness in approximating risk measures and identifying optimal agent compositions.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，现代智能体系统在分解复杂任务时，不仅需最大化任务成功率，还需最小化对安全、公平和隐私要求的违反，因此将智能体工作流形式化为有向无环图，并研究可行智能体组合的风险最小化问题。方法上提出一种高效的动态规划算法，通过遍历智能体图并利用联合界来近似损失分布的风险价值与条件风险价值，证明了该近似对于一类实用损失函数是渐近近优的，且能作为副产品近似条件风险价值。实验部分在一系列类似视频游戏的控制基准测试中，使用强化学习训练的智能体进行组合，结果表明该算法能有效近似风险度量并识别出最优的智能体组合。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach</div>
<div class="meta-line">Authors: Weiran Guo, Bing Bo, Shaoxiang Wu, Jingsheng Yang</div>
<div class="meta-line">First: 2026-01-27T02:49:07+00:00 · Latest: 2026-01-27T02:49:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19122v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19122v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的函数调用模型弱点探索：一种对抗性数据增强方法</div>
<div class="mono" style="margin-top:8px">函数调用能力已成为大语言模型（LLMs）与外部工具及API有效交互的关键。现有提升LLMs函数调用能力的方法依赖于人工标注或模型自动生成的数据，并利用这些数据对LLMs进行微调。然而，这些方法往往缺乏针对性设计，且受限于固定模式和数据分布，制约了函数调用LLMs泛化性与鲁棒性的提升。为突破此局限，本文提出一种新颖的对抗性数据增强方法，采用强化学习系统性地识别并针对函数调用LLMs的弱点。我们的训练框架引入经强化学习训练的查询模型，专门生成挑战函数调用模型的对抗性查询。该方法采用零和博弈框架，使查询模型与函数调用模型进行迭代交替训练。总体而言，本方法推动了更鲁棒函数调用模型的发展，并为系统化识别与修正LLMs外部工具交互能力的缺陷提供了新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of existing data-driven methods for improving the function call capabilities of large language models, which rely on manually annotated or automatically generated data with fixed patterns, thereby hindering generalization and robustness. To address this, the authors propose an adversarial data augmentation method that employs reinforcement learning, framing the training as a zero-sum game where a query model is trained via RL to generate challenging adversarial queries that target weaknesses in the function call model, which are then used for iterative alternating training. Experimental results demonstrate that this approach systematically identifies and corrects weaknesses, advancing the development of more robust function call models capable of better interaction with external tools.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于现有提升大语言模型函数调用能力的方法依赖于手动标注或自动生成的固定模式数据，这限制了模型的泛化性和鲁棒性。为此，作者提出了一种基于强化学习的对抗性数据增强方法，将训练构建为零和博弈，其中通过强化学习训练一个查询模型来生成针对函数调用模型弱点的对抗性查询，并用于迭代交替训练。实验结果表明，该方法能系统性地识别并修正模型弱点，推动了更鲁棒的函数调用模型的发展，使其能更有效地与外部工具交互。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning-Aware Proxy Reward Model using Process Mining</div>
<div class="meta-line">Authors: Yongjae Lee, Taekhyun Park, Sunghyun Sim, Hyerim Bae</div>
<div class="meta-line">First: 2025-10-29T01:07:45+00:00 · Latest: 2026-01-27T02:10:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25065v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25065v2">PDF</a> · <a href="https://github.com/Thrillcrazyer/TACReward}{GitHub">Code1</a> · <a href="https://huggingface.co/Thrillcrazyer/TACReward7B}{HuggingFace">Code2</a> · <a href="https://github.com/Thrillcrazyer/TACReward">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in sparse reward policy gradient methods have enabled effective reinforcement learning (RL) for language models post-training. However, for reasoning tasks such as mathematical problem solving, binarized outcome rewards provide limited feedback on intermediate reasoning steps. While some studies have attempted to address this issue by estimating overall reasoning quality, it remains unclear whether these rewards are reliable proxies for the quality of stepwise reasoning. In this study, we consider reasoning as a structured process and propose \textbf{TACReward}, the reward model that can be seamlessly integrated into sparse reward policy gradient methods without additional human annotation costs or architectural modifications. TACReward aggregates stepwise structural deviations between teacher and policy reasoning using process mining techniques, producing a scalar output reward range of $[0, 1]$ to indicate reasoning quality. Experiments on multiple mathematical reasoning benchmarks demonstrate that integrating the TACReward into sparse reward frameworks encourages the policy model to improve the structural quality of reasoning. Consequently, this leads to consistent performance improvements over existing sparse reward frameworks. Our code and model are publicly available at \href{https://github.com/Thrillcrazyer/TACReward}{GitHub} and \href{https://huggingface.co/Thrillcrazyer/TACReward7B}{HuggingFace}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于过程挖掘的推理感知代理奖励模型</div>
<div class="mono" style="margin-top:8px">稀疏奖励策略梯度方法的最新进展使得语言模型后训练中的强化学习（RL）更为高效。然而，对于数学解题等推理任务，二值化结果奖励对中间推理步骤的反馈有限。尽管已有研究尝试通过评估整体推理质量来解决这一问题，但这些奖励能否可靠地代表逐步推理的质量仍不明确。本研究将推理视为结构化过程，提出\textbf{TACReward}奖励模型，该模型可无缝集成到稀疏奖励策略梯度方法中，无需额外人工标注成本或架构修改。TACReward利用过程挖掘技术聚合教师推理与策略推理间的逐步结构偏差，生成标量输出奖励范围$[0, 1]$以指示推理质量。在多个数学推理基准上的实验表明，将TACReward集成到稀疏奖励框架中能有效提升策略模型的推理结构质量，从而在现有稀疏奖励框架基础上实现持续的性能改进。代码与模型已公开于\href{https://github.com/Thrillcrazyer/TACReward}{GitHub}和\href{https://huggingface.co/Thrillcrazyer/TACReward7B}{HuggingFace}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that binary outcome rewards in reinforcement learning for language models provide insufficient feedback on intermediate reasoning steps in tasks like math problem solving. The method introduces TACReward, a proxy reward model that uses process mining to quantify structural deviations between teacher and policy reasoning steps, producing a scalar reward between 0 and 1 without extra human annotation or architectural changes. Experimental results on multiple mathematical reasoning benchmarks show that integrating TACReward into sparse reward frameworks improves the structural quality of reasoning and consistently boosts performance over existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，针对语言模型的强化学习中，二元结果奖励在数学解题等推理任务上对中间推理步骤的反馈不足。方法上提出了TACReward，这是一个代理奖励模型，它利用过程挖掘技术量化教师模型与策略模型推理步骤之间的结构性偏差，生成0到1之间的标量奖励，无需额外人工标注或架构修改。在多个数学推理基准上的实验结果表明，将TACReward集成到稀疏奖励框架中能提升推理的结构质量，并持续优于现有稀疏奖励框架的性能。</div>
</details>
</div>
<div class="card">
<div class="title">m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</div>
<div class="meta-line">Authors: Yosub Shin, Michael Buriek, Igor Molybog</div>
<div class="meta-line">First: 2026-01-27T02:01:56+00:00 · Latest: 2026-01-27T02:01:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19099v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19099v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>m2sv：面向地图到街景空间推理的可扩展基准</div>
<div class="mono" style="margin-top:8px">视觉-语言模型在多模态基准测试中表现优异，但在需要将抽象俯视表征与第一人称视角对齐的空间推理任务中仍显脆弱。我们提出m2sv——一个可扩展的地图到街景空间推理基准，要求模型通过将正北朝向的俯视地图与同一真实路口采集的街景图像进行对齐，以推断相机拍摄方向。我们发布了m2sv-20k（一个具有地理多样性且控制模糊度的基准数据集）以及m2sv-sft-11k（用于监督微调的结构化推理轨迹精选集）。尽管现有最佳视觉-语言模型在多模态基准上表现强劲，但在m2sv上仅达到65.2%的准确率，远低于人类基准的95%。监督微调和强化学习虽能带来稳定提升，但跨基准评估显示其迁移能力有限。除总体准确率外，我们通过结构信号与人工标注系统分析了地图到街景推理的难度，并对适配开源模型进行了详尽的失败案例分析。研究结果揭示了在几何对齐、证据整合和推理一致性方面存在的持续缺陷，为跨视角的具身空间推理研究指明了方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces m2sv, a scalable benchmark designed to evaluate the spatial reasoning capabilities of vision-language models (VLMs) in aligning abstract overhead maps with egocentric street-view images, motivated by VLMs&#x27; brittleness in such tasks. The method involves creating m2sv-20k, a geographically diverse dataset with controlled ambiguity, and m2sv-sft-11k, a curated set for supervised fine-tuning, to test models on inferring camera viewing direction from map and image pairs. Experimental results show that the best VLM achieves only 65.2% accuracy, far below the human baseline of 95%, with fine-tuning and reinforcement learning yielding gains but limited cross-benchmark transfer, and analysis reveals persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency.</div>
<div class="mono" style="margin-top:8px">本文提出了m2sv，一个可扩展的基准测试，旨在评估视觉-语言模型（VLMs）在将抽象俯视地图与以自我为中心的街景图像对齐时的空间推理能力，其动机在于VLMs在此类任务中的脆弱性。方法包括创建m2sv-20k，一个具有受控模糊性的地理多样化数据集，以及m2sv-sft-11k，一个用于监督微调的精选集，以测试模型从地图和图像对中推断相机视角方向的能力。实验结果表明，最佳VLM的准确率仅为65.2%，远低于人类基线95%，微调和强化学习虽带来提升但跨基准迁移有限，分析揭示了在几何对齐、证据聚合和推理一致性方面存在持续差距。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Elicitation of Language Models</div>
<div class="meta-line">Authors: Jiaxin Wen, Zachary Ankner, Arushi Somani, Peter Hase, Samuel Marks, Jacob Goldman-Wetzler, Linda Petrini, Henry Sleight, Collin Burns, He He, Shi Feng, Ethan Perez, Jan Leike</div>
<div class="meta-line">First: 2025-06-11T19:40:08+00:00 · Latest: 2026-01-26T23:57:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10139v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.10139v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To steer pretrained language models for downstream tasks, today&#x27;s post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden labels and outperforms training on crowdsourced human supervision. On tasks where LMs&#x27; capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 4 Sonnet-based assistant. The resulting assistant matches its counterpart trained on production-grade human labels on average, with higher scores on chat and safety yet lower scores on math and coding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型的无监督引导</div>
<div class="mono" style="margin-top:8px">为引导预训练语言模型适应下游任务，当前的后训练范式依赖人类指定期望行为。然而，对于具备超人类能力的模型，获取高质量人工监督十分困难甚至不可能。为解决这一挑战，我们提出一种新的无监督算法——内部一致性最大化（ICM），该算法基于模型自身生成的标签对预训练语言模型进行微调，无需外部监督。在GSM8k验证、TruthfulQA和Alpaca奖励建模任务中，本方法达到使用黄金标签训练的性能水平，并优于基于众包人工监督的训练。在语言模型能力显著超越人类的任务上，本方法引导模型能力的表现远优于基于人工标签的训练。最后，我们证明该方法可提升前沿语言模型的训练效果：通过无监督训练的奖励模型，结合强化学习训练基于Claude 4 Sonnet的助手。最终获得的助手在整体表现上与基于生产级人工标签训练的版本相当，在对话和安全方面得分更高，但在数学和编程方面得分略低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of steering pretrained language models for downstream tasks when human supervision is difficult or impossible, particularly for models with superhuman capabilities. It introduces an unsupervised algorithm called Internal Coherence Maximization (ICM), which fine-tunes models using their own generated labels without external supervision. Experimental results on tasks like GSM8k-verification, TruthfulQA, and Alpaca reward modeling show that ICM matches performance with golden labels and outperforms crowdsourced human supervision, especially in superhuman capability scenarios; additionally, when applied to train a Claude 4 Sonnet-based assistant via reinforcement learning, it achieves comparable average performance to models trained on production-grade human labels, with improvements in chat and safety but slight declines in math and coding.</div>
<div class="mono" style="margin-top:8px">本文针对预训练语言模型在下游任务中难以获得高质量人类监督的问题，特别是在模型具备超人类能力时，提出了一种无监督算法——内部一致性最大化（ICM），该方法利用模型自身生成的标签进行微调，无需外部监督。在GSM8k验证、TruthfulQA和Alpaca奖励建模等任务上的实验结果表明，ICM的性能与使用黄金标签训练相当，且优于基于众包人类监督的训练，尤其在模型能力超人类的任务中表现更佳；此外，通过ICM训练无监督奖励模型并结合强化学习开发的Claude 4 Sonnet助手，在平均表现上与基于生产级人类标签训练的模型持平，在聊天和安全方面得分更高，但在数学和编码方面略有下降。</div>
</details>
</div>
<div class="card">
<div class="title">A Unifying View of Coverage in Linear Off-Policy Evaluation</div>
<div class="meta-line">Authors: Philip Amortila, Audrey Huang, Akshay Krishnamurthy, Nan Jiang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-26T23:30:24+00:00 · Latest: 2026-01-26T23:30:24+00:00</div>
<div class="meta-line">Comments: To appear at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19030v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19030v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-policy evaluation (OPE) is a fundamental task in reinforcement learning (RL). In the classic setting of linear OPE, finite-sample guarantees often take the form $$ \textrm{Evaluation error} \le \textrm{poly}(C^π, d, 1/n,\log(1/δ)), $$ where $d$ is the dimension of the features and $C^π$ is a coverage parameter that characterizes the degree to which the visited features lie in the span of the data distribution. While such guarantees are well-understood for several popular algorithms under stronger assumptions (e.g. Bellman completeness), the understanding is lacking and fragmented in the minimal setting where only the target value function is linearly realizable in the features. Despite recent interest in tight characterizations of the statistical rate in this setting, the right notion of coverage remains unclear, and candidate definitions from prior analyses have undesirable properties and are starkly disconnected from more standard definitions in the literature.
  We provide a novel finite-sample analysis of a canonical algorithm for this setting, LSTDQ. Inspired by an instrumental-variable view, we develop error bounds that depend on a novel coverage parameter, the feature-dynamics coverage, which can be interpreted as linear coverage in an induced dynamical system for feature evolution. With further assumptions -- such as Bellman-completeness -- our definition successfully recovers the coverage parameters specialized to those settings, finally yielding a unified understanding for coverage in linear OPE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>线性离策略评估中覆盖度的统一视角</div>
<div class="mono" style="margin-top:8px">离策略评估（OPE）是强化学习（RL）中的一项基础任务。在线性OPE的经典设定中，有限样本保证通常采用形式：$$ \textrm{评估误差} \le \textrm{多项式}(C^π, d, 1/n,\log(1/δ)), $$ 其中 $d$ 是特征维度，$C^π$ 是覆盖度参数，用于刻画访问特征位于数据分布张成空间的程度。尽管在更强假设（如贝尔曼完备性）下，多种流行算法的此类保证已得到较好理解，但在仅目标值函数在特征中线性可实现的极小设定下，相关理解仍显不足且零散。尽管近期对该设定下统计率的紧致刻画产生兴趣，但覆盖度的恰当定义仍不明确，先前分析中的候选定义存在不良特性，且与文献中更标准的定义明显脱节。
  我们针对该设定下的经典算法LSTDQ提出了一种新颖的有限样本分析。受工具变量视角启发，我们推导了依赖于新型覆盖度参数——特征动态覆盖度的误差界，该参数可解释为特征演化诱导动力系统中的线性覆盖度。在进一步假设（如贝尔曼完备性）下，我们的定义成功恢复了针对这些设定特化的覆盖度参数，最终为线性OPE中的覆盖度提供了统一理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the fragmented understanding of coverage in linear off-policy evaluation (OPE), where existing coverage parameters are often disconnected from standard definitions and lack desirable properties. The authors propose a novel finite-sample analysis of the LSTDQ algorithm, introducing a new coverage parameter called feature-dynamics coverage, which is derived from an instrumental-variable perspective and interpreted as linear coverage in an induced dynamical system for feature evolution. Their main experimental results show that this parameter unifies coverage notions across settings, recovering specialized parameters under stronger assumptions like Bellman-completeness, thereby providing a cohesive framework for statistical guarantees in linear OPE.</div>
<div class="mono" style="margin-top:8px">本文针对线性离策略评估中覆盖度概念理解碎片化的问题展开研究，现有覆盖度参数常与标准定义脱节且缺乏理想特性。作者提出对LSTDQ算法进行新颖的有限样本分析，引入了一个称为特征动态覆盖度的新参数，该参数基于工具变量视角推导，可解释为特征演化的诱导动力系统中的线性覆盖度。主要实验结果表明，该参数能统一不同设置下的覆盖度概念，在贝尔曼完备性等更强假设下可还原出特定参数，从而为线性离策略评估的统计保证提供了统一的理论框架。</div>
</details>
</div>
<div class="card">
<div class="title">Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning</div>
<div class="meta-line">Authors: Haolin Liu, Dian Yu, Sidi Lu, Yujun Zhou, Rui Liu, Zhenwen Liang, Haitao Mi, Chen-Yu Wei, Dong Yu</div>
<div class="meta-line">First: 2026-01-26T21:38:20+00:00 · Latest: 2026-01-26T21:38:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18984v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18984v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has emerged as a powerful framework for improving the reasoning capabilities of large language models (LLMs). However, most existing RL approaches rely on sparse outcome rewards, which fail to credit correct intermediate steps in partially successful solutions. Process reward models (PRMs) offer fine-grained step-level supervision, but their scores are often noisy and difficult to evaluate. As a result, recent PRM benchmarks focus on a more objective capability: detecting the first incorrect step in a reasoning path. However, this evaluation target is misaligned with how PRMs are typically used in RL, where their step-wise scores are treated as raw rewards to maximize. To bridge this gap, we propose Verifiable Prefix Policy Optimization (VPPO), which uses PRMs only to localize the first error during RL. Given an incorrect rollout, VPPO partitions the trajectory into a verified correct prefix and an erroneous suffix based on the first error, rewarding the former while applying targeted penalties only after the detected mistake. This design yields stable, interpretable learning signals and improves credit assignment. Across multiple reasoning benchmarks, VPPO consistently outperforms sparse-reward RL and prior PRM-guided baselines on both Pass@1 and Pass@K.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>保留正确前缀：通过过程监督强化学习实现精确错误惩罚以增强大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">强化学习已成为提升大语言模型推理能力的有效框架，但现有方法多依赖稀疏结果奖励，无法对部分成功解决方案中的正确中间步骤给予认可。过程奖励模型提供细粒度的步骤级监督，但其评分常存在噪声且难以评估。因此，近期PRM基准转向更客观的能力评估：检测推理路径中的首个错误步骤。然而，该评估目标与PRM在强化学习中的典型应用方式存在偏差——后者将其逐步评分视为待最大化的原始奖励。为弥合此差距，我们提出可验证前缀策略优化方法，该方法在强化学习中仅使用PRM定位首个错误。针对错误输出轨迹，VPPO根据首个错误将轨迹划分为已验证的正确前缀与错误后缀，对前者给予奖励，仅在检测到错误后施加针对性惩罚。该设计产生稳定、可解释的学习信号，并优化了信用分配。在多项推理基准测试中，VPPO在Pass@1和Pass@K指标上均持续优于稀疏奖励强化学习及现有PRM引导基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that while process reward models (PRMs) can provide step-level supervision for improving large language model reasoning, their scores are often noisy and their typical use in reinforcement learning (RL) is misaligned with the objective benchmark task of detecting the first incorrect step. To address this, the method introduces Verifiable Prefix Policy Optimization (VPPO), which uses a PRM solely to identify the first error in a reasoning trajectory, then partitions the rollout into a verified correct prefix and an erroneous suffix, rewarding the prefix while applying penalties only after the detected mistake. The main experimental results show that VPPO consistently outperforms sparse-reward RL and prior PRM-guided baselines on multiple reasoning benchmarks in terms of both Pass@1 and Pass@K metrics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管过程奖励模型能为提升大语言模型的推理能力提供细粒度监督，但其评分通常存在噪声，且其在强化学习中的典型使用方式与检测首个错误步骤这一客观基准任务不一致。为此，本文提出了可验证前缀策略优化方法，该方法仅使用过程奖励模型定位推理轨迹中的首个错误，随后将轨迹划分为已验证的正确前缀和错误后缀，对前缀给予奖励并仅在检测到的错误之后施加针对性惩罚。主要实验结果表明，在多个推理基准测试中，该方法在Pass@1和Pass@K指标上均稳定优于稀疏奖励强化学习及先前基于过程奖励模型的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Self-Optimizing Electron Microscope: Robust Tuning of Aberration Coefficients via Physics-Aware Multi-Objective Bayesian Optimization</div>
<div class="meta-line">Authors: Utkarsh Pratiush, Austin Houston, Richard Liu, Gerd Duscher, Sergei Kalinin</div>
<div class="meta-line">First: 2026-01-26T21:12:48+00:00 · Latest: 2026-01-26T21:12:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18972v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18972v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realizing high-throughput aberration-corrected Scanning Transmission Electron Microscopy (STEM) exploration of atomic structures requires rapid tuning of multipole probe correctors while compensating for the inevitable drift of the optical column. While automated alignment routines exist, conventional approaches rely on serial, gradient-free searches (e.g., Nelder-Mead) that are sample-inefficient and struggle to correct multiple interacting parameters simultaneously. Conversely, emerging deep learning methods offer speed but often lack the flexibility to adapt to varying sample conditions without extensive retraining. Here, we introduce a Multi-Objective Bayesian Optimization (MOBO) framework for rapid, data-efficient aberration correction. Importantly, this framework does not prescribe a single notion of image quality; instead, it enables user-defined, physically motivated reward formulations (e.g., symmetry-induced objectives) and uses Pareto fronts to expose the resulting trade-offs between competing experimental priorities. By using Gaussian Process regression to model the aberration landscape probabilistically, our workflow actively selects the most informative lens settings to evaluate next, rather than performing an exhaustive blind search. We demonstrate that this active learning loop is more robust than traditional optimization algorithms and effectively tunes focus, astigmatism, and higher-order aberrations. By balancing competing objectives, this approach enables &quot;self-optimizing&quot; microscopy by dynamically sustaining optimal performance during experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向自优化电子显微镜：基于物理感知多目标贝叶斯优化的像差系数鲁棒调谐</div>
<div class="mono" style="margin-top:8px">实现原子结构的高通量像差校正扫描透射电子显微镜（STEM）表征，需要在补偿光学柱不可避免漂移的同时快速调谐多极探针校正器。现有自动对准方案依赖串行无梯度搜索（如Nelder-Mead法），存在样本效率低且难以同步校正多参数交互作用的局限；新兴深度学习方法虽具速度优势，却缺乏适应多样本条件的灵活性而需大量重训练。本文提出用于快速数据高效像差校正的多目标贝叶斯优化（MOBO）框架，其创新在于：不预设单一图像质量标准，支持用户定义物理驱动的奖励函数（如对称性目标），通过帕累托前沿揭示竞争性实验目标间的权衡关系。采用高斯过程回归概率建模像差场，主动选择信息量最大的透镜参数进行评估，替代传统穷举式盲搜索。实验证明该主动学习循环较传统优化算法更具鲁棒性，可有效调谐聚焦、像散及高阶像差。通过平衡竞争目标，本方法能在实验过程中动态维持最优性能，实现显微镜的“自优化”运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for efficient and robust aberration correction in scanning transmission electron microscopy (STEM) to enable high-throughput atomic-scale imaging, overcoming the limitations of traditional serial optimization methods and inflexible deep learning approaches. The authors propose a multi-objective Bayesian optimization (MOBO) framework that uses Gaussian Process regression to probabilistically model the aberration landscape and actively select informative lens settings, allowing users to define physics-based reward functions and explore trade-offs via Pareto fronts. Experimental results demonstrate that this method robustly and data-efficiently tunes focus, astigmatism, and higher-order aberrations, outperforming conventional algorithms and enabling self-optimizing microscopy by dynamically maintaining optimal performance during experiments.</div>
<div class="mono" style="margin-top:8px">本文针对扫描透射电子显微镜（STEM）中实现高通量原子尺度成像时对像差校正的高效性和鲁棒性需求，旨在克服传统串行优化方法效率低下以及深度学习模型灵活性不足的问题。研究者提出了一种多目标贝叶斯优化（MOBO）框架，利用高斯过程回归对像差景观进行概率建模，并主动选择信息量最大的透镜设置进行评估，同时允许用户定义基于物理的奖励函数并通过帕累托前沿权衡不同实验目标。实验结果表明，该方法能够稳健且数据高效地调焦、校正像散及高阶像差，其性能优于传统优化算法，可在实验过程中动态维持最佳性能，从而实现显微镜的“自优化”。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Quantum Technology</div>
<div class="meta-line">Authors: Marin Bukov, Florian Marquardt</div>
<div class="meta-line">First: 2026-01-26T20:47:48+00:00 · Latest: 2026-01-26T20:47:48+00:00</div>
<div class="meta-line">Comments: review article; comments are welcome!</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18953v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many challenges arising in Quantum Technology can be successfully addressed using a set of machine learning algorithms collectively known as reinforcement learning (RL), based on adaptive decision-making through interaction with the quantum device. After a concise and intuitive introduction to RL aimed at a broad physics readership, we discuss the key ideas and core concepts in reinforcement learning with a particular focus on quantum systems. We then survey recent progress in RL in all relevant areas. We discuss state preparation in few- and many-body quantum systems, the design and optimization of high-fidelity quantum gates, and the automated construction of quantum circuits, including applications to variational quantum eigensolvers and architecture search. We further highlight the interactive capabilities of RL agents, emphasizing recent progress in quantum feedback control and quantum error correction, and briefly discuss quantum reinforcement learning as well as applications to quantum metrology. The review concludes with a discussion of open challenges -- such as scalability, interpretability, and integration with experimental platforms -- and outlines promising directions for future research. Throughout, we highlight experimental implementations that exemplify the increasing role of reinforcement learning in shaping the development of quantum technologies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习在量子技术中的应用</div>
<div class="mono" style="margin-top:8px">量子技术中的诸多挑战可通过一套统称为强化学习（RL）的机器学习算法成功应对，其核心在于通过与量子设备的交互实现自适应决策。本文首先面向广大物理学读者，对强化学习进行了简明直观的介绍，随后重点聚焦量子系统，探讨了强化学习的关键思想与核心概念。接着，我们综述了强化学习在所有相关领域的最新进展，涵盖少体和多体量子系统的态制备、高保真量子门的设计与优化，以及量子电路的自动化构建（包括在变分量子本征求解器和架构搜索中的应用）。我们进一步强调了强化学习智能体的交互能力，着重介绍了量子反馈控制和量子纠错领域的最新成果，并简要讨论了量子强化学习及其在量子计量学中的应用。最后，本文总结了当前面临的开放性挑战——如可扩展性、可解释性及与实验平台的集成——并展望了未来研究的潜在方向。全文通过突出实验案例，阐明了强化学习在推动量子技术发展中的日益重要的作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This review article is motivated by the potential of reinforcement learning (RL) to address adaptive control and optimization challenges in quantum technology. The method involves providing an intuitive introduction to RL for a physics audience and systematically surveying its core concepts and applications to quantum systems. The main experimental results and progress highlighted include successful RL demonstrations for quantum state preparation, high-fidelity gate design, automated quantum circuit construction, and applications in quantum feedback control, error correction, and metrology, though open challenges in scalability and experimental integration remain.</div>
<div class="mono" style="margin-top:8px">这篇综述文章的动机在于强化学习（RL）在解决量子技术中自适应控制与优化挑战方面的潜力。其方法是为物理学读者提供直观的RL介绍，并系统性地综述其核心概念及在量子系统中的应用。主要涵盖的实验进展包括RL在量子态制备、高保真量子门设计、自动量子电路构建方面的成功演示，以及在量子反馈控制、纠错和计量学中的应用，但可扩展性和实验平台集成等开放挑战依然存在。</div>
</details>
</div>
<div class="card">
<div class="title">Vector-Valued Distributional Reinforcement Learning Policy Evaluation: A Hilbert Space Embedding Approach</div>
<div class="meta-line">Authors: Mehrdad Mohammadi, Qi Zheng, Ruoqing Zhu</div>
<div class="meta-line">First: 2026-01-26T20:46:00+00:00 · Latest: 2026-01-26T20:46:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18952v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18952v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose an (offline) multi-dimensional distributional reinforcement learning framework (KE-DRL) that leverages Hilbert space mappings to estimate the kernel mean embedding of the multi-dimensional value distribution under a proposed target policy. In our setting, the state-action variables are multi-dimensional and continuous. By mapping probability measures into a reproducing kernel Hilbert space via kernel mean embeddings, our method replaces Wasserstein metrics with an integral probability metric. This enables efficient estimation in multi-dimensional state-action spaces and reward settings, where direct computation of Wasserstein distances is computationally challenging. Theoretically, we establish contraction properties of the distributional Bellman operator under our proposed metric involving the Matern family of kernels and provide uniform convergence guarantees. Simulations and empirical results demonstrate robust off-policy evaluation and recovery of the kernel mean embedding under mild assumptions, namely, Lipschitz continuity and boundedness of the kernels, highlighting the potential of embedding-based approaches in complex real-world decision-making scenarios and risk evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>向量值分布式强化学习策略评估：希尔伯特空间嵌入方法</div>
<div class="mono" style="margin-top:8px">我们提出了一种（离线）多维分布式强化学习框架（KE-DRL），利用希尔伯特空间映射来估计目标策略下多维价值分布的核均值嵌入。在我们的设定中，状态-动作变量是多维且连续的。通过核均值嵌入将概率测度映射到再生核希尔伯特空间，我们的方法用积分概率度量替代了Wasserstein度量。这使得在多维状态-动作空间和奖励设置中能够进行高效估计，而直接计算Wasserstein距离在计算上具有挑战性。理论上，我们建立了所提出度量下分布式贝尔曼算子的收缩性质（涉及Matern核族），并提供了均匀收敛保证。仿真和实证结果表明，在温和假设（即核的Lipschitz连续性和有界性）下，能够实现稳健的离策略评估和核均值嵌入恢复，凸显了基于嵌入的方法在复杂现实世界决策场景和风险评估中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational challenges of using Wasserstein metrics for multi-dimensional distributional reinforcement learning (DRL) in continuous state-action spaces. The method, KE-DRL, employs kernel mean embeddings to map value distributions into a reproducing kernel Hilbert space, replacing Wasserstein distances with an integral probability metric for more efficient estimation. Experimental results demonstrate robust off-policy evaluation and recovery of embeddings under assumptions like Lipschitz continuity, supported by theoretical guarantees on operator contraction and uniform convergence.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决在多维连续状态-动作空间中，使用Wasserstein度量进行分布强化学习时面临的计算挑战。所提出的KE-DRL方法利用核均值嵌入将价值分布映射到再生核希尔伯特空间，以积分概率度量替代Wasserstein距离，从而实现更高效的估计。实验结果表明，在Lipschitz连续性和有界性等温和假设下，该方法能实现稳健的离策略评估和嵌入恢复，并得到了算子收缩性和一致收敛性的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration</div>
<div class="meta-line">Authors: Yiwei Shi, Hongnan Ma, Mengyue Yang, Cunjia Liu, Weiru Liu</div>
<div class="meta-line">First: 2025-12-01T22:08:26+00:00 · Latest: 2026-01-26T20:38:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03102v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.03102v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散贝叶斯探索的错误状态估计动态校正</div>
<div class="mono" style="margin-top:8px">在应急响应等高风险社会应用中，早期状态估计对后续结果具有决定性影响。然而，这些基于有限或偏差信息的初始估计常与现实严重偏离，制约后续行动，可能导致灾难性延误、资源错配及人员伤害。在静态自助法基线（零转移且无更新）下，自助粒子滤波器表现出静态诱导后验支撑不变性（S-PSI），即初始先验排除的区域将永久无法探索，即使新证据与当前信念相悖也无法修正。经典扰动方法虽理论上可打破这种锁定，但其持续运行模式效率低下。为此，我们提出一种扩散驱动的贝叶斯探索框架，实现对早期状态估计误差的实时校正。该方法通过熵正则化采样与协方差缩放扩散扩展后验支撑，并采用Metropolis-Hastings检验验证提案，使推断能自适应意外证据。在有害气体定位任务中的实证评估表明：当先验正确时，本方法与强化学习和规划基线性能相当；在估计失准情况下，其显著优于经典序贯蒙特卡洛扰动与基于强化学习的方法。我们同时提供理论证明，表明DEPF在保持统计严谨性的同时能有效解决S-PSI问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the critical need to correct early, erroneous state estimates in high-stakes applications like emergency response, where initial biases can lead to catastrophic outcomes; existing methods like bootstrap particle filters suffer from Stationarity-Induced Posterior Support Invariance (S-PSI), permanently locking out corrections even with new evidence. To overcome this, the authors propose a diffusion-driven Bayesian exploration framework that expands posterior support through entropy-regularized sampling and covariance-scaled diffusion, with a Metropolis-Hastings check to adaptively validate proposals. Experimental results on hazardous-gas localization tasks show the method matches baseline performance when priors are correct and substantially outperforms classical sequential Monte Carlo perturbations and reinforcement learning methods under misalignment, while providing theoretical guarantees for resolving S-PSI.</div>
<div class="mono" style="margin-top:8px">本文的动机源于高风险应用（如应急响应）中纠正早期错误状态估计的迫切需求，因为初始偏差可能导致灾难性后果；现有方法如自举粒子滤波器存在“平稳性诱导的后验支持不变性”（S-PSI）问题，即使有新证据也无法修正估计。为解决此问题，作者提出了一种扩散驱动的贝叶斯探索框架，通过熵正则化采样和协方差缩放扩散来扩展后验支持，并利用Metropolis-Hastings检验自适应验证提案。在有害气体定位任务上的实验结果表明，该方法在先验正确时与基线性能相当，而在先验失准时显著优于经典序列蒙特卡洛扰动和强化学习方法，同时提供了解决S-PSI的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning</div>
<div class="meta-line">Authors: Ruiyang Zhou, Shuozhe Li, Amy Zhang, Liu Leqi</div>
<div class="meta-line">Venue: NeurIPS 2025 Poster</div>
<div class="meta-line">First: 2025-07-03T17:44:55+00:00 · Latest: 2026-01-26T19:36:48+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025 (Poster). Code available at https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02834v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.02834v3">PDF</a> · <a href="https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-improvement via RL often fails on complex reasoning tasks because GRPO-style post-training methods rely on the model&#x27;s initial ability to generate positive samples. Without guided exploration, these approaches merely reinforce what the model already knows (distribution-sharpening) rather than enabling the model to solve problems where it initially generates no correct solutions. To unlock reasoning ability in such settings, the model must explore new reasoning trajectories beyond its current output distribution. Such exploration requires access to sufficiently good positive samples to guide the learning. While expert demonstrations seem like a natural solution, we find that they are often ineffective in RL post-training. Instead, we identify two key properties of effective positive samples: they should (1) be likely under the current policy, and (2) increase the model&#x27;s likelihood of predicting the correct answer. Based on these insights, we propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and modular framework that generates such samples by conditioning on the ground-truth answer. It can be integrated with popular RL training methods like GRPO and DPO. ExPO enables efficient exploration and guides the model to produce reasoning trajectories more aligned with its policy than expert-written CoTs, while ensuring higher quality than its own (incorrect) samples. Experiments show that ExPO improves both learning efficiency and final performance on reasoning benchmarks, surpassing expert-demonstration-based methods in challenging settings such as MATH level-5, where the model initially struggles the most. Code is available at https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExPO：通过自解释引导的强化学习解锁复杂推理能力</div>
<div class="mono" style="margin-top:8px">基于强化学习的自我改进方法在复杂推理任务上常失效，因为GRPO式后训练方法依赖模型生成正样本的初始能力。若无引导性探索，这类方法仅强化模型已有知识（分布锐化），而无法解决初始无法生成正确解的问题。要解锁此类场景的推理能力，模型需探索超出当前输出分布的新推理轨迹，这需要足够优质的正样本引导学习。虽然专家演示看似是自然解决方案，但我们发现其在RL后训练中常无效。相反，我们识别出有效正样本的两个关键特性：（1）在当前策略下应具有高可能性；（2）能提升模型预测正确答案的概率。基于此，我们提出$\textbf{自解释策略优化（ExPO）}$——通过以真实答案为条件生成此类样本的简洁模块化框架，可与GRPO、DPO等主流RL训练方法集成。相比专家撰写的思维链，ExPO能实现高效探索并引导模型生成更符合其策略的推理轨迹，同时确保比自身错误样本更高的质量。实验表明，ExPO在推理基准上同时提升学习效率和最终性能，在模型初始表现最困难的MATH 5级等挑战性场景中超越基于专家演示的方法。代码发布于https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of self-improvement via reinforcement learning (RL) on complex reasoning tasks, where methods like GRPO often fail because they merely reinforce existing knowledge rather than enabling exploration when the model initially generates no correct solutions. To overcome this, the authors propose Self-Explanation Policy Optimization (ExPO), a modular framework that generates effective positive samples by conditioning on ground-truth answers, ensuring these samples are likely under the current policy and increase the model&#x27;s likelihood of predicting correct answers, thus guiding exploration beyond the initial output distribution. Experimental results demonstrate that ExPO improves learning efficiency and final performance on reasoning benchmarks, outperforming expert-demonstration-based methods in challenging settings like MATH level-5, where models initially struggle the most.</div>
<div class="mono" style="margin-top:8px">本文针对复杂推理任务中基于强化学习的自我改进方法的局限性展开研究，指出如GRPO等方法因仅强化模型已有知识而无法在初始无正确解时进行有效探索。为解决这一问题，作者提出了自解释策略优化（ExPO），这是一个模块化框架，通过基于真实答案生成有效的正样本，确保这些样本在当前策略下具有高可能性并能提高模型预测正确答案的似然，从而引导模型探索超出初始输出分布的推理轨迹。实验结果表明，ExPO在推理基准测试中提升了学习效率和最终性能，在如MATH level-5等模型初始表现最差的挑战性场景中超越了基于专家演示的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit Q-Learning and SARSA: Liberating Policy Control from Step-Size Calibration</div>
<div class="meta-line">Authors: Hwanwoo Kim, Eric Laber</div>
<div class="meta-line">First: 2026-01-26T19:17:48+00:00 · Latest: 2026-01-26T19:17:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18907v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18907v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Q-learning and SARSA are foundational reinforcement learning algorithms whose practical success depends critically on step-size calibration. Step-sizes that are too large can cause numerical instability, while step-sizes that are too small can lead to slow progress. We propose implicit variants of Q-learning and SARSA that reformulate their iterative updates as fixed-point equations. This yields an adaptive step-size adjustment that scales inversely with feature norms, providing automatic regularization without manual tuning. Our non-asymptotic analyses demonstrate that implicit methods maintain stability over significantly broader step-size ranges. Under favorable conditions, it permits arbitrarily large step-sizes while achieving comparable convergence rates. Empirical validation across benchmark environments spanning discrete and continuous state spaces shows that implicit Q-learning and SARSA exhibit substantially reduced sensitivity to step-size selection, achieving stable performance with step-sizes that would cause standard methods to fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>隐式Q学习与SARSA：从步长校准中解放策略控制</div>
<div class="mono" style="margin-top:8px">Q学习和SARSA是强化学习的基础算法，其实际成功关键依赖于步长校准。步长过大会导致数值不稳定，步长过小则收敛缓慢。我们提出Q学习和SARSA的隐式变体，将其迭代更新重构为不动点方程。该方法通过特征范数的倒数自适应调整步长，实现无需手动调参的自动正则化。非渐近分析表明，隐式方法能在更宽的步长范围内保持稳定性，在有利条件下甚至允许任意大步长同时保持相当的收敛速率。在涵盖离散与连续状态空间的基准环境中的实证验证显示，隐式Q学习和SARSA对步长选择的敏感性显著降低，能在导致传统方法失效的步长设置下保持稳定性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the critical yet delicate dependence of standard Q-learning and SARSA on step-size calibration, where improper values cause instability or slow convergence, this paper introduces implicit variants that reformulate the iterative updates as fixed-point equations. The method automatically adjusts the step-size inversely with feature norms, providing inherent regularization without manual tuning. Non-asymptotic analysis shows these implicit methods remain stable over much wider step-size ranges, even allowing arbitrarily large steps under favorable conditions while matching convergence rates. Experiments across discrete and continuous benchmark environments confirm substantially reduced sensitivity to step-size selection, with stable performance where standard methods would fail.</div>
<div class="mono" style="margin-top:8px">针对标准Q学习和SARSA算法对步长校准高度敏感、易导致数值不稳定或收敛缓慢的问题，本文提出了隐式变体，将迭代更新重新表述为不动点方程。该方法通过使步长与特征范数成反比来自适应调整，无需手动调参即可实现自动正则化。非渐近分析表明，隐式方法在更宽的步长范围内保持稳定性，在有利条件下甚至允许任意大步长同时保持相当的收敛速度。在离散和连续状态空间的基准环境中的实验验证表明，隐式Q学习和SARSA对步长选择的敏感性显著降低，能在导致标准方法失效的步长下保持稳定性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes</div>
<div class="meta-line">Authors: Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie</div>
<div class="meta-line">First: 2026-01-26T18:57:00+00:00 · Latest: 2026-01-26T18:57:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18795v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18795v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复用计算量：通过条件化极离策略前缀扩展强化学习在难题上的应用</div>
<div class="mono" style="margin-top:8px">传统用于大语言模型推理的强化学习方法在难题上浪费算力，因为正确的同策略轨迹稀少、策略梯度消失且学习停滞。为引导更高效的强化学习，我们考虑以离策略轨迹的形式复用旧采样计算量（来自先前的推理或强化学习训练）。标准离策略方法通过离策略数据进行监督，导致强化学习优化过程不稳定。我们提出PrefixRL方法：以成功离策略轨迹的前缀为条件，运行同策略强化学习来完成后续部分，从而规避离策略不稳定性。该方法通过调节离策略前缀长度来控制问题难度，从而增强难题上的学习信号。我们证明PrefixRL目标不仅与标准强化学习目标一致，且具有更高的样本效率。实验中发现反向泛化现象：仅在前缀化问题上训练可泛化至分布外无前缀任务，且学习策略常与前缀策略不同。实验中通过基础模型的拒绝采样获取离策略轨迹，形成自我改进循环。在复杂推理问题上，即使计入初始拒绝采样的算力消耗，PrefixRL达到相同训练奖励的速度仍比最强基线（离策略数据监督微调后强化学习）快2倍，最终奖励提升3倍。该优势可迁移至保留基准测试，且当离策略轨迹源自不同模型家族时仍保持有效，验证了其在实际场景中的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of standard reinforcement learning (RL) for large language models on hard reasoning problems, where on-policy learning stalls due to rare correct traces and vanishing gradients. To overcome this, the authors propose PrefixRL, a method that reuses off-policy traces from prior inference by conditioning on their prefixes and performing on-policy RL to complete them, thereby avoiding instabilities from direct off-policy supervision while modulating problem difficulty via prefix length. Experimental results show that PrefixRL achieves training rewards 2x faster than strong baselines and improves final rewards by 3x on hard problems, with gains transferring to held-out benchmarks and demonstrating flexibility across model families.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在困难推理问题上标准强化学习方法效率低下的问题，提出PrefixRL方法，通过重用先前推理产生的离策略轨迹前缀，并基于这些前缀进行在策略强化学习来完成后续部分，从而避免直接离策略监督的不稳定性，同时通过前缀长度调节问题难度。实验结果表明，PrefixRL在困难问题上比最强基线训练速度提升2倍，最终奖励提高3倍，且效果能迁移到未见基准测试中，在不同模型家族上也保持有效性，验证了其实际灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic</div>
<div class="meta-line">Authors: Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani</div>
<div class="meta-line">First: 2026-01-26T18:50:21+00:00 · Latest: 2026-01-26T18:50:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18783v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高速公路卡车高效战术决策的多目标强化学习</div>
<div class="mono" style="margin-top:8px">在高速公路驾驶中平衡安全性、效率与运营成本，对重型车辆构成具有挑战性的决策难题。核心难点在于，通过聚合这些竞争性目标获得的传统标量奖励函数，往往模糊了其权衡关系的结构。我们提出一种基于近端策略优化的多目标强化学习框架，该框架学习能明确表征这些权衡关系的连续策略集，并在可扩展的卡车战术决策仿真平台上进行评估。所提方法学习到一组连续的帕累托最优策略，捕捉了三个冲突目标间的权衡关系：安全性（通过碰撞次数与任务完成率量化）、能源效率与时间效率（分别通过能源成本与驾驶员成本量化）。生成的帕累托前沿平滑且可解释，支持沿不同冲突目标灵活选择驾驶行为。该框架无需重新训练即可实现不同驾驶策略间的无缝切换，为自动驾驶卡车应用提供了鲁棒且自适应的决策策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of balancing safety, efficiency, and cost in highway driving for heavy-duty trucks, where traditional scalar reward functions obscure critical trade-offs. The authors propose a multi-objective reinforcement learning framework based on Proximal Policy Optimization to learn a continuous set of Pareto-optimal policies that explicitly represent trade-offs among safety, energy efficiency, and time efficiency. Experimental evaluation on a scalable simulation platform demonstrates that the method produces a smooth and interpretable Pareto frontier, enabling flexible selection of driving behaviors without retraining, thus supporting robust and adaptive decision-making for autonomous trucking.</div>
<div class="mono" style="margin-top:8px">本文针对重型卡车在高速公路驾驶中平衡安全性、效率和成本的挑战，传统标量奖励函数常掩盖关键权衡。作者提出了一种基于近端策略优化的多目标强化学习框架，以学习一组连续的帕累托最优策略，明确表征安全性、能源效率和时间效率之间的权衡。在可扩展仿真平台上的实验评估表明，该方法能生成平滑且可解释的帕累托前沿，允许无需重新训练即可灵活选择驾驶行为，从而为自动驾驶卡车应用提供稳健且自适应的决策策略。</div>
</details>
</div>
<div class="card">
<div class="title">POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration</div>
<div class="meta-line">Authors: Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-26T18:47:21+00:00 · Latest: 2026-01-26T18:47:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POPE：通过特权在线策略探索学习解决难题的推理能力</div>
<div class="mono" style="margin-top:8px">强化学习（RL）提升了大型语言模型（LLM）的推理能力，但现有先进方法仍难以从许多训练问题中学习。在难题上，在线策略RL很少能探索到正确的轨迹，导致零奖励且缺乏驱动改进的学习信号。我们发现，经典RL中解决探索问题的自然方法（如熵奖励、重要性比率更宽松的裁剪或直接优化pass@k目标）均无法解决此问题，且常破坏优化稳定性而不提升可解性。一种自然替代方案是利用简单问题的迁移学习，但研究表明，在RL训练中混合简单与难题会因优化聚焦于已可解问题而阻碍难题进展，产生射线干扰效应。为此，我们提出特权在线策略探索（POPE），该方法利用人类或其他先知解决方案作为特权信息引导难题探索，而非将其作为训练目标（如离线策略RL或从监督微调预热）。POPE通过为难题添加先知解决方案前缀，使RL在引导轨迹中获得非零奖励。关键在于，通过指令遵循与推理的协同作用，习得的行为能迁移回原始无引导问题。实验表明，POPE显著扩展了可解问题集，并在挑战性推理基准上大幅提升性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the exploration challenge in reinforcement learning (RL) for large language models (LLMs) on hard reasoning problems, where on-policy RL often fails to generate correct rollouts, leading to zero reward and no learning signal. To overcome this, the authors propose Privileged On-Policy Exploration (POPE), a method that uses oracle solutions as privileged information to guide exploration by augmenting hard problems with prefixes of these solutions, enabling non-zero rewards during guided rollouts. Experimental results show that POPE significantly expands the set of solvable problems and improves performance on challenging reasoning benchmarks by facilitating transfer back to unguided problems through a synergy between instruction-following and reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在强化学习解决困难推理问题时的探索挑战，即在线策略强化学习常因无法生成正确轨迹而导致零奖励和缺乏学习信号。为解决此问题，作者提出了特权在线策略探索方法，该方法利用人类或其他预言机解决方案作为特权信息，通过为难题添加预言机解的前缀来引导探索，从而在引导过程中获得非零奖励。实验结果表明，该方法通过指令遵循与推理的协同作用，将学习行为迁移回原始未引导问题，显著扩大了可解问题集，并在具有挑战性的推理基准上大幅提升了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</div>
<div class="meta-line">Authors: Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier, Julia Kempe</div>
<div class="meta-line">First: 2026-01-26T18:46:56+00:00 · Latest: 2026-01-26T18:46:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>让模型学会自我教学：可学习性边缘的推理研究</div>
<div class="mono" style="margin-top:8px">模型能否突破自身的学习瓶颈？针对大型推理模型的强化学习微调方法在初始成功率较低的数据集上容易停滞，导致训练信号匮乏。本研究探讨一个核心问题：预训练大语言模型能否利用潜在知识，为自身无法解决的问题生成自动化课程？为此，我们设计了SOAR框架：一种通过元强化学习挖掘教学信号的自改进框架。模型的教师副本为学生副本生成合成问题，并根据其在少量难题子集上的进步获得奖励。关键创新在于，SOAR将课程设计锚定于可测量的学生进展，而非内在代理奖励。在数学基准最难子集（初始成功率0/128）上的实验揭示三项核心发现：首先，通过激发预训练模型生成有效阶梯问题的潜在能力，可实现双层元强化学习，在稀疏二元奖励下开启学习进程；其次，基于实际进展的奖励机制优于先前大语言模型自对弈中的内在奖励方案，能稳定避免其常见的失稳与多样性崩溃问题；第三，对生成问题的分析表明，结构质量与问题明确性比答案正确性更关键。这些发现表明，生成有效阶梯问题的能力并不以预先解决难题为前提，这为无需额外标注数据即可突破推理瓶颈提供了理论路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of reinforcement learning methods stalling when finetuning large reasoning models on datasets with very low initial success rates, which provide little training signal. To overcome this, the authors propose SOAR, a self-improvement framework using meta-reinforcement learning where a teacher model generates synthetic problems for a student model, with rewards based on the student&#x27;s measured progress on a small set of hard problems rather than intrinsic proxy rewards. Experimental results on the hardest subsets of mathematical benchmarks, starting from zero success, demonstrate that SOAR enables effective bi-level meta-RL under sparse rewards, outperforms prior intrinsic reward schemes by avoiding instability and diversity collapse, and reveals that the structural quality and well-posedness of generated questions are more critical for learning progress than solution correctness, suggesting a path to escape reasoning plateaus without additional curated data.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习方法在微调大型推理模型时，因初始成功率极低的数据集缺乏训练信号而停滞的问题展开研究。为此，作者提出了SOAR框架，采用元强化学习方法，让教师模型为学生模型生成合成问题，并根据学生在少量难题上的实测进步给予奖励，而非依赖内在代理奖励。在数学基准测试最难子集上的实验结果表明，SOAR能够在稀疏奖励下实现有效的双层元强化学习，优于先前基于内在奖励的方法，避免了不稳定性和多样性崩溃，并揭示出生成问题的结构质量和明确性比解答正确性对学习进展更为关键，这为无需额外标注数据即可突破推理瓶颈提供了可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory</div>
<div class="meta-line">Authors: Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang</div>
<div class="meta-line">First: 2026-01-26T18:42:33+00:00 · Latest: 2026-01-26T18:42:33+00:00</div>
<div class="meta-line">Comments: Dep-Search 1st version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18771v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18771v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#x27; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dep-Search：基于持久化记忆的依赖感知推理轨迹学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型在复杂推理任务中展现出卓越能力，尤其是在结合搜索机制系统探索外部知识库时。该领域已从传统检索增强生成框架发展为更复杂的搜索框架，通过显式搜索策略协调多步推理。然而现有搜索框架仍严重依赖隐式自然语言推理来确定搜索策略及跨推理步骤利用检索信息，这种依赖对管理子问题间依赖关系、高效复用历史检索知识、通过强化学习优化搜索策略造成根本性挑战。为突破这些局限，我们提出Dep-Search——一种依赖感知搜索框架，通过GRPO整合结构化推理、检索与持久化记忆，实现三大创新：1）引入显式控制机制，支持模型解析具有依赖关系的问题；2）按需检索信息并从记忆库调用历史知识；3）将长推理上下文压缩为可复用的记忆单元。在七个多样化问答数据集上的实验表明，Dep-Search显著提升大语言模型处理复杂多跳推理任务的能力，在不同模型规模上均大幅超越基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for Dep-Search arises from the limitations of existing search-augmented LLMs, which rely on implicit natural language reasoning to manage search strategies and information reuse, leading to challenges in handling dependencies between sub-questions and efficiently leveraging past knowledge. The method introduces a dependency-aware search framework that integrates structured reasoning, retrieval, and persistent memory via GRPO, featuring explicit control mechanisms for question decomposition with dependencies, on-demand retrieval, memory access, and summarization of reasoning contexts into reusable entries. Experimental results on seven diverse QA datasets show that Dep-Search significantly improves LLMs&#x27; performance on complex multi-hop reasoning tasks, achieving substantial gains over strong baselines across various model scales.</div>
<div class="mono" style="margin-top:8px">Dep-Search的提出动机源于现有搜索增强大语言模型依赖隐式自然语言推理来管理搜索策略和信息复用，导致在处理子问题依赖关系和高效利用历史知识方面存在挑战。该方法引入了一个依赖感知的搜索框架，通过GRPO整合结构化推理、检索和持久记忆，具备明确的控制机制，用于带依赖关系的问题分解、按需检索、记忆访问以及将长推理上下文总结为可重用条目。在七个多样化问答数据集上的实验结果表明，Dep-Search显著提升了大语言模型处理复杂多跳推理任务的能力，在不同模型规模上均实现了对强基线的实质性改进。</div>
</details>
</div>
<div class="card">
<div class="title">Trust, Don&#x27;t Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback</div>
<div class="meta-line">Authors: Seyed Amir Hosseini, Maryam Abdolali, Amirhosein Tavakkoli, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi</div>
<div class="meta-line">First: 2026-01-26T18:21:48+00:00 · Latest: 2026-01-26T18:21:48+00:00</div>
<div class="meta-line">Comments: Equal contribution: Seyed Amir Hosseini and Maryam Abdolali. Corresponding author: Maryam Abdolali (maryam.abdolali@kntu.ac.ir)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18751v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18751v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信任、不信任或翻转：基于多专家反馈的鲁棒偏好强化学习</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PBRL）通过从成对轨迹比较中学习，为显式奖励工程提供了有前景的替代方案。然而，现实世界的偏好数据常来自可靠性各异的异构标注者：部分准确、部分含噪声、部分系统性地具有对抗性。现有PBRL方法要么平等对待所有反馈，要么尝试过滤不可靠来源，但面对系统性提供错误偏好的对抗性标注者时均告失效。我们提出TriTrust-PBRL（TTP）——一个从多专家偏好反馈中联合学习共享奖励模型和专家特定信任参数的统一框架。其核心洞见在于：信任参数在基于梯度的优化过程中会自然演化为正值（信任）、接近零值（忽略）或负值（翻转），使模型能自动反转对抗性偏好并恢复有效信号，而非仅丢弃被污染的反馈。我们通过理论分析建立了可识别性保证，并通过梯度分析阐明专家分离如何在无显式监督的训练过程中自然涌现。在涵盖操作任务（MetaWorld）和运动控制（DM Control）的四个多样化领域及多种污染场景下的实证评估表明，TTP实现了最先进的鲁棒性——在对抗性污染下保持接近理论最优的性能，而标准PBRL方法则完全失效。值得注意的是，TTP无需除标识索引外的专家特征，且能与现有PBRL流程无缝集成，在包含可靠与对抗性标注者的混合专家池中成功学习，显著超越现有基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of learning from heterogeneous and potentially adversarial human preferences in preference-based reinforcement learning (PBRL), where existing methods fail to robustly handle systematically incorrect feedback. The authors propose TriTrust-PBRL (TTP), a framework that jointly learns a shared reward model and expert-specific trust parameters, which adaptively assign positive (trust), near-zero (ignore), or negative (flip) values to invert adversarial preferences and extract useful signal. Experimental results on manipulation and locomotion tasks demonstrate that TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption and outperforming baselines in mixed expert settings without requiring additional expert features.</div>
<div class="mono" style="margin-top:8px">该论文针对基于偏好的强化学习（PBRL）中从异构且可能具有对抗性的人类偏好中学习的挑战，现有方法无法鲁棒地处理系统性错误反馈。作者提出了TriTrust-PBRL（TTP）框架，该框架联合学习共享奖励模型和专家特定的信任参数，这些参数自适应地分配正值（信任）、接近零值（忽略）或负值（翻转）以反转对抗性偏好并提取有用信号。在操作和运动任务上的实验结果表明，TTP实现了最先进的鲁棒性，在对抗性干扰下保持接近最优性能，并在混合专家设置中优于基线方法，且无需额外的专家特征。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models</div>
<div class="meta-line">Authors: Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, Aditya Grover</div>
<div class="meta-line">First: 2026-01-26T17:56:50+00:00 · Latest: 2026-01-26T17:56:50+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18734v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18734v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student&#x27;s own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自蒸馏推理器：大语言模型的在线自蒸馏方法</div>
<div class="mono" style="margin-top:8px">知识蒸馏通过压缩教师大语言模型（LLM）的知识来训练更小的LLM，从而提升大语言模型的推理能力。在线蒸馏方法通过让学生模型采样自身轨迹，同时由教师LLM提供密集的词元级监督，解决了离线蒸馏方法中训练与推理分布不匹配的问题。然而，在线蒸馏通常需要独立且规模更大的教师LLM，且未显式利用推理数据集中可用的真实解。受“能力足够的LLM能够合理化外部特权推理轨迹并指导其较弱自身（即无法访问特权信息的版本）”这一直觉启发，我们提出了在线自蒸馏（OPSD）框架。该框架通过在不同上下文条件下切换角色，使单一模型同时担任教师和学生：教师策略基于特权信息（如已验证的推理轨迹），学生策略仅看到问题；训练过程通过最小化学生自身推演中两种策略分布的逐词元差异来实现。我们在多个数学推理基准测试中验证了该方法的有效性，相比GRPO等强化学习方法实现了4-8倍的词元效率提升，且性能优于离线蒸馏方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces On-Policy Self-Distillation (OPSD), a method motivated by the need to improve large language model reasoning without relying on a separate, larger teacher model, while also leveraging ground-truth solutions often available in reasoning datasets. The proposed framework enables a single model to act as both teacher and student: the teacher policy conditions on privileged information like verified reasoning traces, while the student policy sees only the question, and training minimizes token-level divergence between these distributions over the student&#x27;s own sampled trajectories. Experimental results on multiple mathematical reasoning benchmarks show that OPSD achieves 4-8x greater token efficiency compared to reinforcement learning methods like GRPO and outperforms off-policy distillation approaches.</div>
<div class="mono" style="margin-top:8px">本文提出了策略上自蒸馏（OPSD）方法，其动机是在不依赖单独、更大的教师模型的情况下提升大语言模型的推理能力，同时利用推理数据集中常可用的真实解。该方法框架使单一模型同时充当教师和学生：教师策略基于特权信息（如已验证的推理轨迹）进行条件生成，而学生策略仅看到问题，训练过程通过最小化学生自身采样轨迹上的词元级分布差异来实现。在多个数学推理基准上的实验结果表明，OPSD相比强化学习方法（如GRPO）实现了4-8倍的词元效率提升，且性能优于策略外蒸馏方法。</div>
</details>
</div>
<div class="card">
<div class="title">Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale</div>
<div class="meta-line">Authors: Henry Bell, Caroline Zhang, Mohammed Mobasserul Haque, Dhaval Potdar, Samia Zaman, Brandon Fain</div>
<div class="meta-line">First: 2026-01-26T17:54:54+00:00 · Latest: 2026-01-26T17:54:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18730v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18730v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}&#x27;s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model&#x27;s original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Reflect：基于透明原则指导的大规模宪法对齐推理框架</div>
<div class="mono" style="margin-top:8px">对齐的宪法框架旨在使大语言模型（LLM）与用自然语言编写的价值负载原则（例如避免使用偏见语言）保持一致。先前研究主要集中于参数微调技术，如基于人类反馈的强化学习（RLHF），以灌输这些原则。然而，这些方法计算需求高、需要精细的工程设计和调优，且通常依赖难以获取的人工标注数据。我们提出\textsc{reflect}，一种用于宪法对齐的推理时框架，无需任何训练或数据，为指令调优模型提供即插即用的原则对齐方案。\textsc{reflect}完全在上下文内运行，结合（i）宪法条件化基础响应与生成后的（ii）自我评估、（iii）（a）自我批判及（iii）（b）最终修订。\textsc{reflect}在生成后对原则进行显式上下文推理的技术，优于标准少样本提示法，并提供透明的推理轨迹。实验表明，\textsc{reflect}显著提升了LLM对多样复杂原则的遵循度，包括与模型原始参数微调重点迥异的原则，且不牺牲事实推理能力。\textsc{reflect}尤其能有效降低原则违反的罕见但严重事件发生率，从而提升生成分布尾端的安全性与鲁棒性。最后，我们证明\textsc{reflect}能自然生成适用于传统参数微调技术的训练数据，在长期部署场景中支持高效扩展并降低推理时计算开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Reflect, a framework designed to align large language models with constitutional principles without requiring additional training or annotated data, addressing the computational and engineering burdens of prior fine-tuning methods like RLHF. The method operates at inference time by generating a base response conditioned on the principles, followed by a self-evaluation, self-critique, and final revision, all performed in-context to ensure transparent reasoning. Experimental results show that Reflect significantly enhances model adherence to diverse and complex principles, including those not emphasized during original fine-tuning, while maintaining factual accuracy and notably reducing rare but severe principle violations, thereby improving safety and robustness; it also generates useful training data for potential fine-tuning to reduce long-term inference costs.</div>
<div class="mono" style="margin-top:8px">本文提出了Reflect框架，旨在无需额外训练或标注数据的情况下，使大语言模型与宪法原则对齐，以解决先前基于强化学习人类反馈等微调方法在计算和工程上的负担。该方法在推理时通过生成基于原则的初始响应，随后进行自我评估、自我批评和最终修订，全部在上下文中执行以确保推理透明性。实验结果表明，Reflect显著提升了模型对多样复杂原则的遵循能力，包括那些在原始微调中未强调的原则，同时保持了事实准确性，并有效减少了罕见但严重的原则违反情况，从而增强了安全性和鲁棒性；此外，它还能生成有用的训练数据，为潜在的微调提供支持以降低长期推理开销。</div>
</details>
</div>
<div class="card">
<div class="title">Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs</div>
<div class="meta-line">Authors: Zhichao Yang, Sepehr Janghorbani, Dongxu Zhang, Jun Han, Qian Qian, Andrew Ressler, Gregory D. Lyng, Sanjit Singh Batra, Robert E. Tillman</div>
<div class="meta-line">First: 2026-01-26T17:34:10+00:00 · Latest: 2026-01-26T17:34:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18706v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18706v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Health-SCORE：面向可扩展的评估准则以改进健康领域大语言模型</div>
<div class="mono" style="margin-top:8px">评估准则对于评估开放式大语言模型响应至关重要，尤其在医疗等安全关键领域。然而，创建高质量且领域特定的评估准则通常需要大量专家人力与开发成本，使得基于准则的评估与训练难以规模化。本研究提出Health-SCORE——一个可泛化、可扩展的基于准则的训练与评估框架，能在保持性能的同时大幅降低准则开发成本。我们证明Health-SCORE除独立评估外还具有两项实际优势：可作为结构化奖励信号引导具备安全监督的强化学习，并能直接嵌入提示词通过上下文学习提升响应质量。在开放式医疗任务中，Health-SCORE实现了与人工创建准则相当的评估质量，同时显著降低开发成本，使基于准则的评估与训练更具可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that creating high-quality, domain-specific rubrics for evaluating open-ended LLM responses in healthcare is costly and time-consuming, hindering scalable evaluation and training. The method introduces Health-SCORE, a scalable framework that reduces rubric development costs while maintaining performance, and demonstrates its utility not only for evaluation but also as a structured reward signal for reinforcement learning with safety-aware supervision and for improving responses via in-context learning. The main experimental results show that across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics with significantly lower development effort, thereby enhancing the scalability of rubric-based approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，为医疗保健领域开放式大语言模型回答创建高质量、领域特定的评估量规通常需要大量专业人力与开发成本，这使得基于量规的评估和训练难以规模化。方法上，研究提出了Health-SCORE这一可泛化、可扩展的量规训练与评估框架，旨在显著降低量规开发成本而不牺牲性能，并证明其除了独立评估外，还能作为结构化奖励信号用于具有安全监督的强化学习，以及通过上下文学习直接融入提示以提升回答质量。主要实验结果表明，在开放式医疗任务中，Health-SCORE实现了与人工创建量规相当的评估质量，同时大幅降低了开发工作量，从而提高了基于量规的评估与训练的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</div>
<div class="meta-line">Authors: Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Zihan Dong, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Linjun Zhang, Shujie Liu, Yan Lu, Huaxiu Yao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-31T13:22:55+00:00 · Latest: 2026-01-26T17:15:26+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00555v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00555v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy with dynamic entropy regulation, progressively teaching the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs. Notably, it achieves an average performance gain of 23.6% over strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMedAgent-RL：优化多智能体协作以实现多模态医学推理</div>
<div class="mono" style="margin-top:8px">医学大型视觉语言模型（Med-LVLMs）在多模态诊断任务中展现出强大潜力。然而，现有单智能体模型难以泛化至不同医学专科，限制了其性能。近期研究借鉴临床工作流程，引入了全科医生与专科医生按固定顺序交互的多智能体协作框架。尽管有所改进，这些静态流程在推理中缺乏灵活性与适应性。为此，我们提出MMedAgent-RL——一种基于强化学习（RL）的多智能体框架，可实现医疗智能体间的动态优化协作。具体而言，我们通过RL训练两个基于Qwen2.5-VL的全科医生智能体：分诊医生学习将患者分配至合适专科，而主治医生则整合多专科判断与自身知识做出最终决策。为解决专科医生输出的不一致性，我们引入课程学习（CL）引导的RL策略，通过动态熵调节逐步指导主治医生在模仿专科医生与纠正其错误间取得平衡。在五个医学VQA基准上的实验表明，MMedAgent-RL性能优于开源及专有Med-LVLMs，较基线模型平均提升23.6%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of single-agent medical vision-language models, which struggle to generalize across diverse medical specialties, and existing multi-agent frameworks that lack flexibility due to static collaboration pipelines. To address this, the authors propose MMedAgent-RL, a reinforcement learning-based multi-agent framework that enables dynamic, optimized collaboration among medical agents, featuring a triage doctor that learns to assign patients to specialists and an attending physician that integrates multi-specialist judgments. Key methodological innovations include a curriculum learning-guided RL strategy with dynamic entropy regulation to help the attending physician balance imitating specialists and correcting their inconsistencies. Experimental results on five medical VQA benchmarks show that MMedAgent-RL outperforms both open-source and proprietary models, achieving an average performance gain of 23.6% over strong baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于现有单智能体医疗视觉语言模型难以泛化到多样化的医学专科，而多智能体协作框架又因静态流程缺乏灵活性。为此，作者提出了MMedAgent-RL，这是一个基于强化学习的多智能体框架，实现了医疗智能体间的动态优化协作，包括学习分诊至专科的导诊医生和整合多专科判断的主治医生。方法上的核心创新是采用课程学习引导的强化学习策略，通过动态熵调节，逐步教导主治医生在模仿专科医生与纠正其错误之间取得平衡。在五个医疗视觉问答基准上的实验结果表明，MMedAgent-RL超越了开源和专有模型，平均性能较基线提升了23.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</div>
<div class="meta-line">Authors: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-18T18:59:27+00:00 · Latest: 2026-01-26T17:06:02+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16912v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.16912v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索与利用：通过裁剪、熵与伪奖励重思RLVR</div>
<div class="mono" style="margin-top:8px">本文研究了可验证奖励强化学习（RLVR）中的探索-利用权衡问题，该框架旨在提升大语言模型（LLM）的推理能力。近期研究表明，RLVR可通过两种看似矛盾的机制激发LLM的数学推理能力：伪奖励（通过奖励与真实答案无关的结果来抑制利用）和熵最小化（通过推动模型产生更自信、确定性的输出来抑制探索）。这揭示了一个令人困惑的动态：既抑制利用又抑制探索反而提升了推理性能，但其背后的协调机制尚不明确。我们聚焦两个核心问题：（i）策略熵如何影响性能；（ii）伪奖励是否通过裁剪偏差与模型污染的相互作用产生增益。实验表明，伪奖励下的裁剪偏差会降低策略熵，从而产生更自信、确定性的输出，而仅靠熵最小化不足以提升性能。我们进一步提出奖励错配模型，解释为何伪奖励能在污染环境之外提升性能。本研究阐明了伪奖励获益的机制，并为更有效的RLVR训练提供了原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the exploration-exploitation trade-off in Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing Large Language Model reasoning, motivated by the paradoxical observation that both spurious rewards (which discourage exploitation) and entropy minimization (which discourages exploration) can improve performance, yet their underlying mechanisms are unclear. The method involves analyzing how policy entropy relates to performance and whether spurious rewards yield gains through clipping bias and model contamination. Experimental results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident outputs, while entropy minimization alone is insufficient; a reward-misalignment model is proposed to explain how spurious rewards enhance performance beyond contaminated settings, clarifying mechanisms for more effective RLVR training.</div>
<div class="mono" style="margin-top:8px">本文研究了用于增强大语言模型推理能力的可验证奖励强化学习（RLVR）中的探索与利用权衡，其动机源于一个矛盾观察：虚假奖励（抑制利用）和熵最小化（抑制探索）均能提升性能，但其内在机制尚不明确。方法包括分析策略熵与性能的关系，以及虚假奖励是否通过裁剪偏差和模型污染产生增益。实验结果表明，在虚假奖励下，裁剪偏差降低了策略熵，从而产生更自信的输出，而仅靠熵最小化不足以改善性能；研究进一步提出了一个奖励错配模型来解释虚假奖励如何在污染设置之外提升性能，阐明了更有效RLVR训练的机制。</div>
</details>
</div>
<div class="card">
<div class="title">ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule</div>
<div class="meta-line">Authors: Yilie Huang, Wenpin Tang, Xunyu Zhou</div>
<div class="meta-line">First: 2026-01-26T16:56:40+00:00 · Latest: 2026-01-26T16:56:40+00:00</div>
<div class="meta-line">Comments: 17 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18681v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18681v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ART扩散采样：基于强化学习的时间步调度方法</div>
<div class="mono" style="margin-top:8px">本文研究基于分数的扩散模型在有限时间网格上生成样本时的时间离散化问题。在给定时间步数预算下，均匀或人工设计的网格可能并非最优。我们提出自适应重参数化时间（ART）方法，通过控制重参数化时间变量的时钟速率，在保持终止时间不变的前提下实现采样轨迹上的时间变换与非均匀时间步划分。目标是最小化离散化欧拉格式产生的累积误差。我们推导出随机控制对应框架ART-RL，将时间变换建模为具有高斯策略的连续时间强化学习问题，并证明求解ART-RL可恢复最优ART调度，进而通过数据驱动的行动者-评论家更新实现实际学习。基于官方EDM框架的实验表明，ART-RL在CIFAR-10上显著提升Fréchet起始距离指标，且无需重新训练即可迁移至AFHQv2、FFHQ和ImageNet数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the suboptimality of uniform or hand-crafted timestep schedules in score-based diffusion models, which can lead to inefficient sampling given a fixed computational budget. The authors propose Adaptive Reparameterized Time (ART), a method that adaptively controls the clock speed of a reparameterized time variable to create an uneven, optimized timestep schedule that minimizes the aggregate discretization error from the Euler scheme. They further develop ART-RL, a reinforcement learning formulation with Gaussian policies, proving it recovers the optimal ART schedule and enabling data-driven learning. Experimental results using the EDM pipeline show that ART-RL consistently improves Fréchet Inception Distance on CIFAR-10 across various budgets and transfers effectively to other datasets like AFHQv2, FFHQ, and ImageNet without retraining.</div>
<div class="mono" style="margin-top:8px">本文针对基于分数的扩散模型中均匀或手工设计的时序步长调度可能存在的次优问题进行研究，该问题在固定计算预算下会导致采样效率低下。作者提出了自适应重参数化时间（ART）方法，通过自适应控制重参数化时间变量的时钟速度，创建非均匀的、优化的时序步长调度，以最小化欧拉格式带来的累积离散化误差。他们进一步提出了ART-RL，一种采用高斯策略的强化学习框架，证明其能够恢复最优的ART调度，并实现数据驱动的学习。基于官方EDM流程的实验结果表明，ART-RL在CIFAR-10上显著提升了Fréchet Inception Distance，且在不同计算预算下均表现优异，并能无需重新训练即可有效迁移到AFHQv2、FFHQ和ImageNet等数据集。</div>
</details>
</div>
<div class="card">
<div class="title">MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models</div>
<div class="meta-line">Authors: Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller</div>
<div class="meta-line">First: 2025-12-24T15:15:18+00:00 · Latest: 2026-01-26T16:31:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21231v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.21231v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term &#x27;latent solvability&#x27;. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiST：理解中期科学训练在化学推理模型发展中的作用</div>
<div class="mono" style="margin-top:8px">大型语言模型可通过基于规则的奖励进行在线微调来发展推理能力。然而，近期研究揭示了一个关键限制：仅当基础模型已对正确答案分配了不可忽略的概率时，强化学习才能成功——这一特性我们称为“潜在可解性”。本研究探讨了化学推理能力的涌现及其先决条件对化学领域的意义。我们确定了基于强化学习的化学推理的两个必要条件：1）符号能力，2）潜在化学知识。我们提出了中期科学训练（MiST）：一套满足这些条件的中期训练技术，包括结合SMILES/CIF感知预处理的数据混合、对29亿词元的持续预训练，以及对10亿词元的有监督微调。这些步骤将30亿和70亿参数模型的潜在可解性分数提升至多1.8倍，并使强化学习在有机反应命名任务中的Top-1准确率从10.9%提升至63.9%，在无机材料生成任务中从40.6%提升至67.4%。其他具有挑战性的化学任务也观察到类似结果，同时生成可解释的推理轨迹。我们的研究明确了化学推理训练的清晰前提，并凸显了中期训练在解锁推理能力方面的更广泛作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the observation that reinforcement learning (RL) for chemical reasoning in large language models only succeeds if the base model already has a non-negligible probability of generating correct answers, a property termed &#x27;latent solvability&#x27;. To establish this prerequisite, the method introduces Mid-Stage Scientific Training (MiST), which combines data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens to build symbolic competence and latent chemical knowledge. The main experimental results show that MiST increases the latent-solvability score by up to 1.8x for 3B and 7B models, enabling RL to improve top-1 accuracy from 10.9% to 63.9% on organic reaction naming and from 40.6% to 67.4% on inorganic material generation, with similar gains on other challenging chemical tasks while producing interpretable reasoning traces.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于发现，大型语言模型通过基于规则的奖励进行在线微调以发展化学推理能力时，强化学习仅在基础模型已对正确答案具有不可忽略的生成概率（即“潜在可解性”）时才能成功。为此，方法提出了中期科学训练（MiST），通过混合SMILES/CIF感知预处理的数据、在29亿词元上继续预训练、以及在10亿词元上进行监督微调，来建立符号能力和潜在化学知识这两项必要前提。主要实验结果表明，MiST将30亿和70亿参数模型的潜在可解性分数提升了最高1.8倍，使得强化学习能够将有机反应命名的Top-1准确率从10.9%提升至63.9%，将无机材料生成的准确率从40.6%提升至67.4%，在其他挑战性化学任务上也观察到类似提升，同时产生了可解释的推理轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning</div>
<div class="meta-line">Authors: Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu, Ranjay Krishna, Yu Cheng</div>
<div class="meta-line">First: 2026-01-26T16:04:43+00:00 · Latest: 2026-01-26T16:04:43+00:00</div>
<div class="meta-line">Comments: 28 pages, 10 figures and 13 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18631v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18631v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaReasoner：面向迭代式视觉推理的动态工具编排框架</div>
<div class="mono" style="margin-top:8px">当人类面临超出即时能力的问题时，会借助工具解决，这为提升多模态大语言模型（MLLMs）的视觉推理能力提供了可行范式。有效的推理关键在于：即使面对新工具或新任务，也能在多个步骤中准确判断应使用何种工具、何时调用以及如何组合工具。本文提出\textbf{AdaReasoner}系列多模态模型，其将工具使用视为通用推理技能进行学习，而非针对特定工具或依赖显式监督的行为。AdaReasoner的实现基于三大核心机制：（一）可扩展的数据构建流程，使模型接触长跨度、多步骤的工具交互场景；（二）Tool-GRPO强化学习算法，根据终端任务成功率优化工具选择与序列编排；（三）自适应学习机制，动态调控工具使用策略。这些组件协同工作，使模型能够从任务上下文与中间结果推断工具效用，实现多工具协同调度及对未见工具的泛化能力。实验表明，AdaReasoner展现出强大的工具适应与泛化特性：尽管从未接受相关显式训练，它能自主采用有益工具、抑制无关工具，并根据任务需求动态调整工具使用频率。这些能力使其在多项挑战性基准测试中取得领先性能：7B基础模型平均提升24.9%，在VSP、Jigsaw等任务上超越GPT-5等强商业系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces AdaReasoner, a family of multimodal models designed to enhance visual reasoning by enabling dynamic tool orchestration, motivated by the need for models to autonomously select, sequence, and adapt tool usage in iterative reasoning tasks without explicit supervision. The method employs a scalable data curation pipeline for multi-step tool interactions, a reinforcement learning algorithm called Tool-GRPO to optimize tool selection based on end-task success, and an adaptive learning mechanism that regulates tool usage dynamically. Experimental results demonstrate that AdaReasoner achieves state-of-the-art performance, improving a 7B base model by an average of 24.9% and outperforming proprietary systems like GPT-5 on tasks such as VSP and Jigsaw, while showing strong generalization to unseen tools and adaptive tool usage behaviors.</div>
<div class="mono" style="margin-top:8px">本文提出了AdaReasoner，这是一个多模态模型系列，旨在通过动态工具编排来增强视觉推理能力，其动机是让模型能够在无需显式监督的情况下，自主选择、排序和适应迭代推理任务中的工具使用。该方法采用可扩展的数据处理流程来构建多步骤工具交互，使用名为Tool-GRPO的强化学习算法基于最终任务成功优化工具选择，并引入自适应学习机制动态调节工具使用。实验结果表明，AdaReasoner在多个基准测试中取得了最先进的性能，将7B基础模型的平均性能提升了24.9%，并在VSP和Jigsaw等任务上超越了GPT-5等强大专有系统，同时展现出对未见工具的强泛化能力和自适应工具使用行为。</div>
</details>
</div>
<div class="card">
<div class="title">Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Yingxiao Huo, Satya Prakash Dash, Radu Stoican, Samuel Kaski, Mingfei Sun</div>
<div class="meta-line">First: 2026-01-26T16:02:18+00:00 · Latest: 2026-01-26T16:02:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18626v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18626v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习中自然策略梯度的逆费舍尔矩阵秩-1近似方法</div>
<div class="mono" style="margin-top:8px">自然梯度因其快速收敛特性与协变权重更新机制，在深度强化学习领域长期受到关注。然而，计算自然梯度需在每次迭代中求取费舍尔信息矩阵的逆，这本质上是计算不可行的。本文提出一种高效可扩展的自然策略优化技术，利用秩-1近似替代完整的逆费舍尔矩阵。理论证明表明，在特定条件下，逆费舍尔矩阵的秩-1近似比策略梯度收敛更快，且在部分条件下具有与随机策略梯度方法相同的样本复杂度。通过在多样化环境中的基准测试，本方法在性能上超越了标准的演员-评论家框架与信赖域基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational challenge of inverting the Fisher Information Matrix (FIM) in natural policy gradient methods, which is essential for fast convergence but prohibitively expensive. The proposed method introduces a scalable natural policy optimization technique that uses a rank-1 approximation to the inverse-FIM, theoretically demonstrating faster convergence than policy gradients and comparable sample complexity to stochastic policy gradient methods under certain conditions. Experimentally, the method is benchmarked across diverse environments and shown to outperform standard actor-critic and trust-region baselines in performance.</div>
<div class="mono" style="margin-top:8px">本文的动机源于自然策略梯度方法中计算费舍尔信息矩阵逆矩阵的挑战，该计算对快速收敛至关重要但计算成本过高。所提出的方法采用了一种可扩展的自然策略优化技术，利用对逆费舍尔信息矩阵的秩-1近似，理论上证明了在某些条件下比策略梯度收敛更快，且与随机策略梯度方法具有相当的样本复杂度。实验结果表明，该方法在多种环境中进行基准测试，其性能优于标准的演员-评论家和信任域基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning</div>
<div class="meta-line">Authors: Miguel Costa, Arthur Vandervoort, Carolin Schmidt, Morten W. Petersen, Martin Drews, Karyn Morrissey, Francisco C. Pereira</div>
<div class="meta-line">First: 2026-01-26T15:32:40+00:00 · Latest: 2026-01-26T15:32:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18586v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework&#x27;s transferability to other hazards and cities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的气候韧性交通适应路径规划：应对直接与间接洪水影响的长期策略</div>
<div class="mono" style="margin-top:8px">气候变化预计将加剧降雨等灾害，导致城市交通系统中断频发。由于基础设施投资具有长期性、序列性特征，且面临深度不确定性与复杂的跨部门交互作用，设计有效的适应策略颇具挑战。本研究提出一个通用决策支持框架，将综合评估模型与强化学习相结合，用于在不确定性环境下探索适应性的数十年投资路径。该框架整合长期气候预测（如IPCC情景路径）与多级影响模型：首先将极端天气驱动因子（如降雨）映射为灾害概率（如洪水），进而推演至城市基础设施影响（如交通中断），最终量化服务性能与社会成本的直接与间接损失。通过强化学习闭环，该框架可习得权衡投资维护成本与灾害规避效益的气候适应策略。在与哥本哈根市政府合作中，我们以2024-2100年内城雨洪为例进行验证。相较于传统优化基准（无作为与随机行动），习得策略展现出协调的时空路径与更强的鲁棒性，证明了该框架向其他灾害类型与城市的可迁移性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to design long-term, resilient urban transport adaptation strategies under climate change uncertainties and complex cross-sector interactions, this paper proposes a decision-support framework that couples an integrated assessment model with reinforcement learning. The method integrates long-term climate projections, hazard mapping, and impact valuation to learn multi-decade investment pathways that balance costs against avoided direct and indirect flood impacts. Experimental application to pluvial flooding in Copenhagen from 2024 to 2100 demonstrates that the learned adaptive strategies yield coordinated spatio-temporal pathways and improved robustness compared to conventional baselines like inaction or random action, highlighting the framework&#x27;s potential transferability to other hazards and urban contexts.</div>
<div class="mono" style="margin-top:8px">本研究旨在应对气候变化下长期、序列性基础设施投资所面临的深度不确定性与跨部门复杂交互挑战，提出了一种将综合评估模型与强化学习耦合的决策支持框架。该方法整合长期气候预测、灾害概率映射及影响评估，以学习权衡投资维护成本与避免洪涝直接间接影响的数十年适应路径。在哥本哈根内城2024至2100年地表洪水案例中，实验表明所学策略能产生协调的时空路径，相较于无为或随机行动等传统基线具有更强鲁棒性，体现了该框架向其他灾害与城市迁移的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents</div>
<div class="meta-line">Authors: Vincenzo De Paola, Mirco Mutti, Riccardo Zamboni, Marcello Restelli</div>
<div class="meta-line">First: 2026-01-26T15:26:40+00:00 · Latest: 2026-01-26T15:26:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18580v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18580v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>K-Myriad：基于无监督并行智能体的强化学习快速启动方法</div>
<div class="mono" style="margin-top:8px">强化学习中的并行化通常用于加速单一策略的训练，即多个工作器从相同的采样分布中收集经验。这种常见设计因忽视多样化探索策略的优势而限制了并行化的潜力。我们提出K-Myriad——一种可扩展的无监督方法，通过并行策略群体最大化集体状态熵。通过培育专业化探索策略组合，K-Myriad为强化学习提供稳健的初始化方案，既能提升训练效率，又能发现异构解决方案。在高维连续控制任务中进行大规模并行实验表明，K-Myriad能够学习到大量差异化策略，凸显了其在集体探索方面的有效性，并为新型并行化策略开辟了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that typical parallel reinforcement learning setups, where multiple workers sample from an identical distribution, fail to leverage diverse exploration strategies, thus limiting training potential. The proposed method, K-Myriad, is an unsupervised, scalable approach that maximizes the collective state entropy across a population of parallel policies to cultivate a portfolio of specialized exploration strategies, thereby providing a robust initialization for reinforcement learning. Experimental results on high-dimensional continuous control tasks with large-scale parallelization show that K-Myriad successfully learns a broad set of distinct policies, enhancing training efficiency and enabling the discovery of heterogeneous solutions, which validates its effectiveness for collective exploration.</div>
<div class="mono" style="margin-top:8px">本文的动机在于观察到典型的并行强化学习设置中，多个工作器从相同分布中采样，未能利用多样化的探索策略，从而限制了训练潜力。所提出的方法K-Myriad是一种无监督、可扩展的方法，通过最大化并行策略群体诱导的集体状态熵，培养一系列专业化的探索策略，从而为强化学习提供稳健的初始化。在高维连续控制任务上进行的大规模并行实验结果表明，K-Myriad能够成功学习到一组广泛且不同的策略，提高了训练效率并促进了异构解决方案的发现，这验证了其在集体探索方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates</div>
<div class="meta-line">Authors: Yibo Li, Zijie Lin, Ailin Deng, Xuan Zhang, Yufei He, Shuo Ji, Tri Cao, Bryan Hooi</div>
<div class="meta-line">First: 2026-01-26T14:16:51+00:00 · Latest: 2026-01-26T14:16:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18510v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18510v1">PDF</a> · <a href="https://github.com/liushiliushi/JitRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM&#x27;s output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>即时强化学习：无需梯度更新的大语言模型智能体持续学习</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLM）智能体在通用任务上表现出色，但由于部署后权重固定，其持续适应能力存在固有局限。传统强化学习（RL）虽能提供解决方案，但计算成本过高且存在灾难性遗忘风险。本文提出即时强化学习（JitRL），一种无需训练即可在测试时进行策略优化的免梯度更新框架。JitRL通过动态非参数化记忆库存储经验，实时检索相关轨迹以估算动作优势值，并直接用于调节LLM的输出逻辑值。理论证明该加法更新规则是KL约束策略优化目标的精确闭式解。在WebArena和Jericho上的大量实验表明，JitRL在免训练方法中达到最新最优水平，其性能甚至超越计算密集型微调方法（如WebRL），同时将成本降低超过30倍，为持续学习智能体提供了可扩展路径。代码已开源：https://github.com/liushiliushi/JitRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling continual adaptation in deployed large language model (LLM) agents without costly fine-tuning, as frozen model weights hinder learning from new experiences. The authors propose Just-In-Time Reinforcement Learning (JitRL), a training-free framework that performs test-time policy optimization by maintaining a dynamic memory of past experiences, retrieving relevant trajectories to estimate action advantages on-the-fly, and using these estimates to directly modulate the LLM&#x27;s output logits without gradient updates. Experimental results on WebArena and Jericho benchmarks show that JitRL sets a new state-of-the-art among training-free methods, even outperforming computationally expensive fine-tuning approaches like WebRL while reducing monetary costs by over 30 times, demonstrating its efficacy for scalable continual learning.</div>
<div class="mono" style="margin-top:8px">本文针对已部署大语言模型（LLM）智能体因权重冻结而难以持续适应新任务的问题，提出了一种无需昂贵微调的解决方案。研究者引入了即时强化学习（JitRL），这是一个免训练框架，通过在测试时动态维护非参数化经验记忆、检索相关轨迹以实时估算动作优势，并直接利用这些估算调整LLM的输出逻辑，而无需梯度更新。在WebArena和Jericho基准上的大量实验表明，JitRL在免训练方法中达到了新的最优性能，甚至超越了计算成本高昂的微调方法（如WebRL），同时将经济成本降低了30倍以上，为持续学习智能体提供了一条可扩展的路径。</div>
</details>
</div>
<div class="card">
<div class="title">Pretrain Value, Not Reward: Decoupled Value Policy Optimization</div>
<div class="meta-line">Authors: Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</div>
<div class="meta-line">First: 2025-02-24T08:11:33+00:00 · Latest: 2026-01-26T14:09:10+00:00</div>
<div class="meta-line">Comments: 16 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.16944v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.16944v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore how directly pretraining a value model simplifies and stabilizes reinforcement learning from human feedback (RLHF). In reinforcement learning, value estimation is the key to policy optimization, distinct from reward supervision. The value function predicts the \emph{return-to-go} of a partial answer, that is, how promising the partial answer is if it were continued to completion. In RLHF, however, the standard pipeline first pretrains a reward model and then learns a value function online, even though no new reward signals are available once preference data is collected. This makes critic learning redundant, as the process of training a reward model and then deriving a value model is informationally equivalent to directly pretraining a value model. Importantly, this requires no additional supervision, and our value model is trained on exactly the same data used for reward modeling. Building on this insight, we introduce \emph{Decoupled Value Policy Optimization} (DVPO), a framework that pretrains a \emph{Global Value Model} (GVM) offline and freezes it as a universal critic for policy learning. The GVM provides stable, fine-grained credit assignment without critic drift or trajectory sampling. Experiments across MT-Bench, Alpaca-Eval, and Arena-Hard demonstrate that DVPO matches or surpasses state-of-the-art RLHF methods. These results highlight RLHF can be reframed as policy-only optimization guided by a single pretrained value model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预训练价值而非奖励：解耦价值策略优化</div>
<div class="mono" style="margin-top:8px">本文探讨了直接预训练价值模型如何简化和稳定基于人类反馈的强化学习（RLHF）。在强化学习中，价值估计是策略优化的关键，不同于奖励监督。价值函数预测部分答案的“剩余回报”，即该部分答案若延续至完成时的潜在价值。然而在RLHF标准流程中，通常先预训练奖励模型再在线学习价值函数，尽管收集偏好数据后并无新奖励信号。这使得评论者学习变得冗余，因为训练奖励模型再推导价值模型的过程，在信息层面等同于直接预训练价值模型。关键的是，这无需额外监督，我们的价值模型训练数据与奖励建模完全相同。基于此，我们提出《解耦价值策略优化》（DVPO）框架，该框架离线预训练《全局价值模型》（GVM）并将其冻结为策略学习的通用评论者。GVM提供稳定、细粒度的信用分配，避免评论者漂移或轨迹采样。在MT-Bench、Alpaca-Eval和Arena-Hard上的实验表明，DVPO达到或超越了当前最先进的RLHF方法。这些结果凸显了RLHF可重构为仅由单一预训练价值模型指导的纯策略优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the observation that in standard RLHF, pretraining a reward model and then learning a value function online is redundant, as both processes are informationally equivalent; thus, directly pretraining a value model could simplify and stabilize RLHF. The method introduces Decoupled Value Policy Optimization (DVPO), which pretrains a Global Value Model offline on the same preference data used for reward modeling and freezes it as a universal critic to guide policy optimization, eliminating the need for online critic learning and trajectory sampling. Experimental results on benchmarks like MT-Bench, Alpaca-Eval, and Arena-Hard show that DVPO matches or surpasses state-of-the-art RLHF methods, demonstrating that RLHF can be effectively reframed as policy-only optimization guided by a single pretrained value model.</div>
<div class="mono" style="margin-top:8px">本文的动机在于观察到标准RLHF中先预训练奖励模型再在线学习价值函数是冗余的，因为这两个过程在信息上是等价的；因此，直接预训练价值模型可以简化和稳定RLHF。方法上提出了解耦价值策略优化（DVPO），它在离线状态下使用与奖励建模相同的偏好数据预训练一个全局价值模型，并将其冻结为通用评论家来指导策略优化，从而避免了在线评论家学习和轨迹采样的需求。在MT-Bench、Alpaca-Eval和Arena-Hard等基准测试上的实验结果表明，DVPO达到或超越了最先进的RLHF方法，这证明RLHF可以有效地重构为仅由单个预训练价值模型指导的策略优化。</div>
</details>
</div>
<div class="card">
<div class="title">STaR: Towards Effective and Stable Table Reasoning via Slow-Thinking Large Language Models</div>
<div class="meta-line">Authors: Huajian Zhang, Mingyue Cheng, Yucong Luo, Xiaoyu Tao</div>
<div class="meta-line">First: 2025-11-14T12:34:17+00:00 · Latest: 2026-01-26T13:35:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11233v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11233v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Table reasoning with large language models (LLMs) plays a critical role in building intelligent systems capable of understanding and analyzing tabular data. Despite recent progress, existing methods still face key limitations: their reasoning processes lacks depth and explicit multi-step reasoning, often relying solely on implicit language model understanding. In addition, their reasoning processes suffer from instability, primarily caused by model uncertainty. In this work, we propose STaR, a novel slow-thinking model that can achieve effective and stable table reasoning. To enable effective multi-step reasoning, we design a two-stage training framework consisting of supervised fine-tuning (SFT) warm-up followed by reinforced fine-tuning (RFT). Specifically, in the SFT stage, we construct a high-quality dataset through automatic self-verification. In the RFT stage, we introduce a difficulty-aware reinforcement learning mechanism to further enhance reasoning capabilities. Furthermore, to improve reasoning stability, we introduce trajectory-level uncertainty quantification, which fuses token-level confidence with answer-level consistency, enabling the selection of better reasoning trajectories. Extensive experiments demonstrate that STaR-8B achieves state-of-the-art performance on in-domain benchmarks and exhibits strong generalization to out-of-domain datasets, highlighting its potential for enhancing both effectiveness and stability in table reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STaR：通过慢思考大语言模型实现高效稳定的表格推理</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLMs）的表格推理在构建能够理解和分析表格数据的智能系统中至关重要。尽管近期取得进展，现有方法仍面临关键局限：其推理过程缺乏深度和显式的多步推理，往往仅依赖语言模型的隐式理解；同时，推理过程存在不稳定性，主要由模型不确定性导致。本研究提出STaR——一种新型慢思考模型，能够实现高效且稳定的表格推理。为实现有效的多步推理，我们设计了包含监督微调（SFT）预热和强化微调（RFT）的两阶段训练框架：SFT阶段通过自动自验证构建高质量数据集；RFT阶段引入难度感知强化学习机制以增强推理能力。此外，为提升推理稳定性，我们提出轨迹级不确定性量化方法，融合词元级置信度与答案级一致性，从而筛选更优推理轨迹。大量实验表明，STaR-8B在领域内基准测试中达到最先进性能，并在领域外数据集上展现出强大泛化能力，凸显了其在提升表格推理效能与稳定性方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in table reasoning with large language models, where existing methods lack explicit multi-step reasoning and suffer from instability due to model uncertainty. To overcome this, the authors propose STaR, a slow-thinking model that employs a two-stage training framework: supervised fine-tuning with a self-verified dataset for warm-up, followed by reinforced fine-tuning with a difficulty-aware mechanism to enhance reasoning. Additionally, trajectory-level uncertainty quantification, combining token confidence and answer consistency, is introduced to improve stability. Experimental results show that STaR-8B achieves state-of-the-art performance on in-domain benchmarks and generalizes well to out-of-domain datasets, demonstrating improved effectiveness and stability in table reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在表格推理中的局限性展开研究，现有方法缺乏显式的多步推理且因模型不确定性导致稳定性不足。为此，作者提出了STaR这一慢思考模型，采用两阶段训练框架：首先通过自动自验证构建高质量数据集进行监督微调预热，随后引入难度感知的强化微调机制以增强推理能力。此外，通过融合词元级置信度和答案级一致性的轨迹级不确定性量化，提升了推理稳定性。实验结果表明，STaR-8B在领域内基准测试中取得了最先进的性能，并在领域外数据集上表现出强大的泛化能力，凸显了其在表格推理中提升有效性和稳定性的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States</div>
<div class="meta-line">Authors: Kyoleen Kwak, Hyoseok Hwang</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-01-26T13:34:34+00:00 · Latest: 2026-01-26T13:34:34+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26. 7 pages (excluding references), 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18479v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18479v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过对齐动作与前序状态预测增强控制策略平滑性</div>
<div class="mono" style="margin-top:8px">深度强化学习已被证明是解决控制任务的有效方法，但其固有的高频振荡特性使其难以应用于现实环境。现有方法虽通过架构或损失函数设计缓解动作振荡，但后者通常依赖启发式或人工定义的状态相似性来促进动作一致性，往往难以准确反映系统动态特性。本文提出一种基于损失函数的新方法，引入转移诱导相似状态——定义为从前一状态转移得到的下一状态分布。该方法仅利用环境反馈和实际采集数据，能更准确地捕捉系统动态。在此基础上，我们提出通过对齐动作与前序状态预测实现动作平滑的方法，该方法通过将当前动作与转移诱导相似状态中的动作对齐，并惩罚二阶差分以抑制高频振荡，从而有效缓解动作振荡。在Gymnasium和Isaac-Lab环境中的实验表明，该方法相比现有方法能产生更平滑的控制效果并提升策略性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the high-frequency action oscillations characteristic of deep reinforcement learning policies, which hinder real-world deployment, by moving beyond heuristic definitions of state similarity used in prior loss-based methods. The proposed method, ASAP, introduces a transition-induced similar state—defined as the distribution of next states from a previous state—which leverages environmental feedback to better capture system dynamics, and enforces smoothness by aligning actions with those for these states while penalizing second-order differences to suppress oscillations. Experimental results in Gymnasium and Isaac-Lab environments show that ASAP achieves smoother control and improved policy performance compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决深度强化学习中策略动作高频振荡的问题，该问题阻碍了其在现实世界的应用，为此需要改进以往基于损失的方法中使用的启发式状态相似性定义。所提出的ASAP方法引入了转移诱导相似状态——定义为从先前状态转移得到的下一状态分布，该方法利用环境反馈更好地捕捉系统动态，并通过使动作与这些状态下的动作对齐，同时惩罚二阶差分以抑制振荡，来强制实现动作平滑性。在Gymnasium和Isaac-Lab环境中的实验结果表明，与现有方法相比，ASAP能实现更平滑的控制和更优的策略性能。</div>
</details>
</div>
<div class="card">
<div class="title">Tandem Training for Language Models</div>
<div class="meta-line">Authors: Robert West, Ashton Anderson, Ece Kamar, Eric Horvitz</div>
<div class="meta-line">Venue: Proceedings of the 2026 Conference of the European Chapter of the Association for Computational Linguistics (EACL)</div>
<div class="meta-line">First: 2025-10-15T13:48:16+00:00 · Latest: 2026-01-26T13:19:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13551v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13551v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators. We formalize intelligibility as handoff robustness: a strong model&#x27;s solution is intelligible to a weaker model if randomly handing off control to the weaker model along the solution path does not cause failure. Building on this criterion, we introduce tandem training for language models, a reinforcement learning (RL) paradigm in which rollout tokens are intermittently and randomly sampled from a frozen weak model rather than the strong model being trained. Because rollouts succeed only when the strong model&#x27;s actions and reasoning process can be continued by the weak model -- when the two can co-construct a successful solution -- optimizing standard RL objectives with tandem training implicitly incentivizes both correctness and intelligibility. In the GSM8K math reasoning task, tandem training reliably teaches models to abandon jargon and adapt their language to weaker partners while keeping task accuracy high. Our results demonstrate a promising route to building AI systems that remain auditable by weaker agents, with implications for human--AI collaboration and multi-agent communication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型的串联训练</div>
<div class="mono" style="margin-top:8px">随着语言模型持续快速进步，其行为和推理可能变得难以被较弱智能体或人类理解，从而削弱可解释性与监督。着眼于长远发展，我们探索促使模型生成对较弱合作者保持可理解解决方案的方法。我们将可理解性形式化为交接鲁棒性：若在解决方案路径中随机将控制权移交给较弱模型不会导致失败，则强模型的解决方案对较弱模型是可理解的。基于此标准，我们提出语言模型的串联训练——一种强化学习范式，其中推演过程的令牌间歇性地从冻结的较弱模型随机采样，而非来自正在训练的强模型。由于推演仅在强模型的行为和推理过程能被较弱模型延续时（即两者能协同构建成功解决方案时）才成功，通过串联训练优化标准强化学习目标可隐式激励正确性与可理解性。在GSM8K数学推理任务中，串联训练能可靠地引导模型放弃专业术语、使语言适应较弱合作方，同时保持高任务准确率。我们的研究结果为构建能被较弱智能体审计的人工智能系统提供了可行路径，对人类-AI协作与多智能体通信具有启示意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to maintain interpretability and oversight as language models become more advanced, this paper introduces tandem training, a reinforcement learning method that encourages strong models to produce solutions intelligible to weaker agents. The method formalizes intelligibility as handoff robustness, where a solution is considered intelligible if randomly handing off control to a weaker model during its execution does not cause failure; tandem training implements this by intermittently sampling rollout tokens from a frozen weak model during the strong model&#x27;s training. Experimental results on the GSM8K math reasoning task show that tandem training successfully teaches models to adapt their language to be understandable by weaker partners while maintaining high task accuracy, demonstrating a promising approach for building auditable AI systems.</div>
<div class="mono" style="margin-top:8px">本文的动机是随着语言模型能力快速提升，其决策和推理过程可能超出较弱智能体或人类的理解范围，从而损害可解释性和监督，因此提出串联训练方法以鼓励强模型生成对弱协作方可理解的解决方案。该方法将可理解性形式化为交接鲁棒性，即强模型的解决方案如果在执行过程中随机交由弱模型接管而不会导致失败，则被视为可理解；串联训练通过强化学习实现，在训练强模型时随机从冻结的弱模型中采样生成令牌。在GSM8K数学推理任务上的实验结果表明，串联训练能有效引导模型放弃专业术语、适应弱伙伴的语言风格，同时保持高任务准确率，为构建可被较弱智能体审计的人工智能系统提供了可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents</div>
<div class="meta-line">Authors: Yuhang Zhou, Kai Zheng, Qiguang Chen, Mengkang Hu, Qingfeng Sun, Can Xu, Jingjing Chen</div>
<div class="meta-line">First: 2026-01-26T13:13:59+00:00 · Latest: 2026-01-26T13:13:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18467v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18467v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OffSeeker：深度研究智能体并非仅需在线强化学习</div>
<div class="mono" style="margin-top:8px">深度研究智能体在处理长周期任务方面展现出显著潜力，但当前最优性能通常依赖在线强化学习（RL），而频繁的API调用导致其经济成本高昂。虽然离线训练提供了更高效的替代方案，但高质量研究轨迹的稀缺性阻碍了其发展。本文论证了构建强大研究智能体并非必须依赖昂贵的在线强化学习。为填补这一空白，我们推出了一套专为高效离线训练设计的全开源工具集。核心贡献包括：DeepForge——一个无需繁重预处理即可生成大规模研究查询的即用型任务合成框架；以及精心整理的6.6万组问答对、3.3万条监督微调轨迹和2.1万组直接偏好优化数据对。基于这些资源，我们训练出完全离线开发的OffSeeker（80亿参数）模型。在六项基准测试中的广泛评估表明，OffSeeker不仅领先于同规模智能体，更能与通过大量在线RL训练的300亿参数系统保持竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high cost of online reinforcement learning for training deep research agents, which relies on expensive API calls, and the lack of quality offline data hinders efficient training. To address this, the method introduces an open-source suite featuring DeepForge for generating large-scale research queries without heavy preprocessing, along with curated datasets including QA pairs, SFT trajectories, and DPO pairs. The main experimental results show that OffSeeker, an 8B-parameter model trained entirely offline, outperforms similar-sized agents and competes with 30B-parameter systems trained via online RL across six benchmarks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于训练深度研究智能体时，在线强化学习因依赖大量API调用而成本高昂，且高质量离线数据的缺乏阻碍了高效训练。为解决此问题，方法提出一个开源套件，包括无需繁重预处理的DeepForge框架用于生成大规模研究查询，以及精心策划的数据集，涵盖问答对、监督微调轨迹和直接偏好优化对。主要实验结果表明，完全离线训练的8B参数模型OffSeeker在六个基准测试中领先于同类规模智能体，并与通过在线强化学习训练的30B参数系统保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication</div>
<div class="meta-line">Authors: Michael Kölle, Christian Reff, Leo Sünkel, Julian Hager, Gerhard Stenzel, Claudia Linnhoff-Popien</div>
<div class="meta-line">First: 2026-01-26T12:21:05+00:00 · Latest: 2026-01-26T12:21:05+00:00</div>
<div class="meta-line">Comments: Accepted at IEEE ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18419v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Emergent cooperation in classical Multi-Agent Reinforcement Learning has gained significant attention, particularly in the context of Sequential Social Dilemmas (SSDs). While classical reinforcement learning approaches have demonstrated capability for emergent cooperation, research on extending these methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication. In this paper, we apply communication approaches to quantum Q-Learning agents: the Mutual Acknowledgment Token Exchange (MATE) protocol, its extension Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange (MEDIATE), the peer rewarding mechanism Gifting, and Reinforced Inter-Agent Learning (RIAL). We evaluate these approaches in three SSDs: the Iterated Prisoner&#x27;s Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. Our experimental results show that approaches using MATE with temporal-difference measure (MATE\textsubscript{TD}), AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all dilemmas, demonstrating that communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于通信的量子多智能体强化学习中的涌现合作</div>
<div class="mono" style="margin-top:8px">经典多智能体强化学习中的涌现合作已受到广泛关注，尤其是在序列社会困境（SSDs）背景下。尽管经典强化学习方法已展现出实现涌现合作的能力，但将这些方法扩展到量子多智能体强化学习的研究仍显不足，特别是通过通信机制的研究。本文在量子Q学习智能体中应用了多种通信方法：互认令牌交换（MATE）协议、其扩展版本互认分布式激励令牌交换（MEDIATE）、同伴奖励机制Gifting以及强化智能体间学习（RIAL）。我们在三种SSD环境中评估了这些方法：迭代囚徒困境、迭代猎鹿博弈和迭代斗鸡博弈。实验结果表明，采用时差度量的MATE（MATE\textsubscript{TD}）、AutoMATE、MEDIATE-I和MEDIATE-S方法在所有困境中均实现了高合作水平，证明通信是促进量子多智能体强化学习中涌现合作的有效机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the success of emergent cooperation in classical multi-agent reinforcement learning for sequential social dilemmas, this paper investigates whether communication protocols can foster similar cooperation in a quantum multi-agent reinforcement learning setting. The method applies several communication-based approaches—including MATE, MEDIATE, Gifting, and RIAL—to quantum Q-learning agents and evaluates them in three classic social dilemma games: the Iterated Prisoner&#x27;s Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. The main experimental results demonstrate that specific protocols, namely MATE with temporal-difference measure, AutoMATE, and both MEDIATE variants, achieved high levels of cooperation across all tested dilemmas, confirming communication as an effective mechanism for promoting cooperation in quantum multi-agent systems.</div>
<div class="mono" style="margin-top:8px">受经典多智能体强化学习中涌现合作在序列社会困境方面成功的启发，本研究旨在探索通信协议能否在量子多智能体强化学习环境中促进类似的合作行为。方法上，该工作将多种基于通信的机制——包括MATE、MEDIATE、Gifting和RIAL——应用于量子Q学习智能体，并在三个经典社会困境博弈（重复囚徒困境、重复猎鹿博弈和重复懦夫博弈）中进行评估。主要实验结果表明，特定的协议，尤其是结合时序差分度量的MATE、AutoMATE以及两种MEDIATE变体，在所有测试困境中都实现了高度的合作水平，从而证实了通信是促进量子多智能体系统合作的有效机制。</div>
</details>
</div>
<div class="card">
<div class="title">Noise-based reward-modulated learning</div>
<div class="meta-line">Authors: Jesús García Fernández, Nasir Ahmad, Marcel van Gerven</div>
<div class="meta-line">First: 2025-03-31T11:35:23+00:00 · Latest: 2026-01-26T11:58:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.23972v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.23972v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The pursuit of energy-efficient and adaptive artificial intelligence (AI) has positioned neuromorphic computing as a promising alternative to conventional computing. However, achieving learning on these platforms requires techniques that prioritize local information while enabling effective credit assignment. Here, we propose noise-based reward-modulated learning (NRL), a novel synaptic plasticity rule that mathematically unifies reinforcement learning and gradient-based optimization with biologically-inspired local updates. NRL addresses the computational bottleneck of exact gradients by approximating them through stochastic neural activity, transforming the inherent noise of biological and neuromorphic substrates into a functional resource. Drawing inspiration from biological learning, our method uses reward prediction errors as its optimization target to generate increasingly advantageous behavior, and eligibility traces to facilitate retrospective credit assignment. Experimental validation on reinforcement tasks, featuring immediate and delayed rewards, shows that NRL achieves performance comparable to baselines optimized using backpropagation, although with slower convergence, while showing significantly superior performance and scalability in multi-layer networks compared to reward-modulated Hebbian learning (RMHL), the most prominent similar approach. While tested on simple architectures, the results highlight the potential of noise-driven, brain-inspired learning for low-power adaptive systems, particularly in computing substrates with locality constraints. NRL offers a theoretically grounded paradigm well-suited for the event-driven characteristics of next-generation neuromorphic AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于噪声的奖励调制学习</div>
<div class="mono" style="margin-top:8px">追求节能和自适应人工智能（AI）使神经形态计算成为传统计算的有前景替代方案。然而，在这些平台上实现学习需要优先考虑局部信息并支持有效信用分配的技术。本文提出基于噪声的奖励调制学习（NRL），这是一种新颖的突触可塑性规则，通过受生物启发的局部更新，在数学上统一了强化学习和基于梯度的优化。NRL通过随机神经活动近似精确梯度，将生物和神经形态基底固有的噪声转化为功能资源，从而解决精确梯度的计算瓶颈。受生物学习启发，该方法以奖励预测误差为优化目标生成逐渐有利的行为，并利用资格迹促进回溯信用分配。在包含即时和延迟奖励的强化任务上的实验验证表明，NRL虽收敛较慢，但性能可与基于反向传播优化的基线相媲美，同时在多层网络中相比最突出的类似方法——奖励调制赫布学习（RMHL）展现出显著优越的性能和可扩展性。尽管在简单架构上测试，结果凸显了噪声驱动、类脑学习在低功耗自适应系统中的潜力，尤其适用于具有局部性约束的计算基底。NRL为事件驱动的新一代神经形态AI提供了理论完备的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for energy-efficient and adaptive AI that aligns with neuromorphic computing&#x27;s local information constraints, this paper introduces Noise-based Reward-Modulated Learning (NRL), a synaptic plasticity rule that unifies reinforcement learning and gradient-based optimization through biologically-inspired local updates. The method leverages stochastic neural activity to approximate gradients, turning inherent noise into a functional resource, and employs reward prediction errors with eligibility traces for credit assignment. Experimental results on reinforcement tasks with immediate and delayed rewards show that NRL matches backpropagation-optimized baselines in performance, albeit with slower convergence, and significantly outperforms reward-modulated Hebbian learning in multi-layer networks, demonstrating superior scalability and potential for low-power adaptive systems.</div>
<div class="mono" style="margin-top:8px">本文受对符合神经形态计算局部信息约束的高能效自适应人工智能的需求驱动，提出了噪声驱动的奖励调制学习（NRL），这是一种通过生物启发的局部更新，将强化学习与基于梯度的优化统一起来的突触可塑性规则。该方法利用随机神经活动近似梯度，将固有噪声转化为功能资源，并采用奖励预测误差和资格迹进行信用分配。在包含即时和延迟奖励的强化任务上的实验结果表明，NRL在性能上可与基于反向传播的基线方法相媲美，尽管收敛较慢，并且在多层网络中显著优于奖励调制赫布学习，展现出更优的可扩展性及在低功耗自适应系统中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito</div>
<div class="meta-line">Authors: Yinghan Hou, Zongyou Yang</div>
<div class="meta-line">First: 2026-01-26T11:31:00+00:00 · Latest: 2026-01-26T11:31:00+00:00</div>
<div class="meta-line">Comments: 14 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18381v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18381v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system&#x27;s hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于逆向工程遗留有限差分代码并转换为Devito的AI智能体</div>
<div class="mono" style="margin-top:8px">为促进将遗留有限差分实现迁移至Devito环境，本研究开发了集成化AI智能体框架。系统通过混合LangGraph架构中的多阶段迭代工作流，结合检索增强生成（RAG）与开源大语言模型。智能体通过文档解析、结构感知分割、实体关系提取及基于Leiden的社区检测，构建了完整的Devito知识图谱。GraphRAG优化提升了涵盖地震波模拟、计算流体力学和性能调优库等语义社区的查询性能。逆向工程组件通过对Fortran源代码的静态分析，为RAG检索制定三级查询策略。多阶段检索流水线通过并行搜索、概念扩展、社区级检索和语义相似性分析，为语言模型提供精准的上下文引导。代码合成受基于Pydantic的约束机制管控，确保结构化输出与可靠性。验证框架整合传统静态分析与G-Eval方法，涵盖执行正确性、结构完整性、数学一致性和API合规性。整体工作流基于LangGraph框架实现，采用并发处理以支持基于质量的迭代优化和状态感知动态路由。核心创新在于引入受强化学习启发的反馈机制，实现了从静态代码翻译到动态自适应分析行为的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the need to modernize legacy finite-difference codes by translating them into the Devito framework. The method employs an AI agent built on a hybrid LangGraph architecture, integrating Retrieval-Augmented Generation (RAG) with open-source LLMs; it constructs a Devito knowledge graph via document parsing and community detection, and uses a reverse-engineering component to analyze Fortran code for multi-stage retrieval and constrained code synthesis. Experimental validation through static analysis and G-Eval confirms the agent&#x27;s effectiveness in producing correct, structurally sound, and API-compliant translations, with the key contribution being reinforcement learning-inspired feedback that enables adaptive, dynamic code analysis.</div>
<div class="mono" style="margin-top:8px">本研究旨在将传统的有限差分代码现代化，转换为Devito框架。方法上，它开发了一个基于混合LangGraph架构的AI智能体，结合了检索增强生成（RAG）与开源大语言模型；通过文档解析和社区检测构建Devito知识图谱，并利用逆向工程组件分析Fortran代码，实现多阶段检索和受约束的代码合成。实验通过静态分析和G-Eval进行验证，结果表明该智能体能生成正确、结构合理且符合API的翻译代码，其主要贡献在于引入了受强化学习启发的反馈机制，从而实现了从静态代码翻译向动态自适应分析行为的转变。</div>
</details>
</div>
<div class="card">
<div class="title">Shared Spatial Memory Through Predictive Coding</div>
<div class="meta-line">Authors: Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Wenbo Ding, Yuguang Fang</div>
<div class="meta-line">First: 2025-11-06T10:12:46+00:00 · Latest: 2026-01-26T11:24:30+00:00</div>
<div class="meta-line">Comments: We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04235v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04235v3">PDF</a> · <a href="http://github.com/fangzr/SSM-PC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners&#x27; locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于预测编码的共享空间记忆</div>
<div class="mono" style="margin-top:8px">在多智能体系统中，构建一致的共享空间记忆是关键挑战——局部可观测性与有限带宽常导致协同任务灾难性失败。本研究提出一种多智能体预测编码框架，将协同问题转化为智能体间互不确定性的最小化。通过信息瓶颈目标，该框架促使智能体不仅学习与谁沟通、传递何种信息，还掌握沟通时机。框架基础采用类网格细胞的度量作为自定位的内部空间编码，这些编码通过自监督运动预测自发涌现。基于此内部空间编码，智能体逐步发展出带宽高效的通信机制与编码同伴位置的特化神经集群——这相当于海马体社会位置细胞的人工模拟。分层强化学习策略进一步利用这些社会表征进行主动探索以降低联合不确定性。在Memory-Maze基准测试中，本方法展现出对带宽限制的卓越鲁棒性：当带宽从128比特/步缩减至4比特/步时，成功率从73.5%平缓下降至64.4%，而全广播基线则从67.6%骤降至28.6%。本研究为复杂社会表征如何从统一的预测驱动中涌现并形成集体智能，建立了理论严谨且生物学合理的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of building a consistent shared spatial memory in multi-agent systems under partial observability and limited communication bandwidth, this paper introduces a multi-agent predictive coding framework that formulates coordination as minimizing mutual uncertainty via an information bottleneck. The method enables agents to learn an internal grid-cell-like spatial code from self-supervised motion prediction, which spontaneously gives rise to bandwidth-efficient communication and specialized neural populations encoding partners&#x27; locations, analogous to social place cells. Experimental results on the Memory-Maze benchmark demonstrate exceptional resilience to bandwidth constraints, with success rates degrading gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, significantly outperforming a baseline that collapses from 67.6% to 28.6%.</div>
<div class="mono" style="margin-top:8px">本文旨在解决多智能体系统中因部分可观测性和有限通信带宽导致难以构建一致共享空间记忆的挑战，提出了一种多智能体预测编码框架，将协调问题转化为通过信息瓶颈最小化相互不确定性。该方法使智能体能够通过自监督运动预测学习内部网格细胞状空间编码，自发形成带宽高效的通信机制及编码同伴位置的特化神经群体，类似于社会位置细胞。在Memory-Maze基准测试中的实验结果表明，该方法对带宽限制具有卓越的鲁棒性：当带宽从128比特/步缩减至4比特/步时，成功率从73.5%平缓下降至64.4%，显著优于从67.6%暴跌至28.6%的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Analysis of Control Bellman Residual Minimization for Markov Decision Problem</div>
<div class="meta-line">Authors: Donghwan Lee, Hyukjun Yang</div>
<div class="meta-line">First: 2026-01-26T10:58:27+00:00 · Latest: 2026-01-26T10:58:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18840v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18840v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Markov decision problems are most commonly solved via dynamic programming. Another approach is Bellman residual minimization, which directly minimizes the squared Bellman residual objective function. However, compared to dynamic programming, this approach has received relatively less attention, mainly because it is often less efficient in practice and can be more difficult to extend to model-free settings such as reinforcement learning. Nonetheless, Bellman residual minimization has several advantages that make it worth investigating, such as more stable convergence with function approximation for value functions. While Bellman residual methods for policy evaluation have been widely studied, methods for policy optimization (control tasks) have been scarcely explored. In this paper, we establish foundational results for the control Bellman residual minimization for policy optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>马尔可夫决策问题中控制贝尔曼残差最小化的分析</div>
<div class="mono" style="margin-top:8px">马尔可夫决策问题通常通过动态规划求解。另一种方法是贝尔曼残差最小化，它直接最小化平方贝尔曼残差目标函数。然而，与动态规划相比，该方法在实践中效率较低，且更难扩展至无模型设置（如强化学习），因此关注度相对较低。尽管如此，贝尔曼残差最小化具有多项优势，例如在价值函数逼近时收敛更稳定，值得深入研究。虽然策略评估的贝尔曼残差方法已得到广泛研究，但策略优化（控制任务）的方法仍鲜有探索。本文为策略优化中的控制贝尔曼残差最小化建立了基础性结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the relative neglect of Bellman residual minimization for policy optimization despite its potential advantages like stable convergence with function approximation, this paper establishes foundational results for control Bellman residual minimization in Markov decision problems. The method focuses on directly minimizing the squared Bellman residual objective as an alternative to dynamic programming, addressing its under-exploration for control tasks. The main experimental results provide theoretical groundwork, analyzing the approach&#x27;s properties and implications for policy optimization, though specific empirical outcomes are not detailed in the abstract.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，尽管贝尔曼残差最小化在策略优化中具有如函数逼近下稳定收敛等潜在优势，但相较于动态规划方法，其在控制任务中研究较少。该方法通过直接最小化平方贝尔曼残差目标函数，作为动态规划的替代方案，重点探索了其在策略优化中的应用基础。主要实验结果提供了理论分析基础，阐述了该方法在控制任务中的性质与影响，但摘要中未详述具体实证结果。</div>
</details>
</div>
<div class="card">
<div class="title">Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaoyan Gong, Zhiqiang Liu, Songze Li, Xiaoke Guo, Yuanxiang Liu, Xinle Deng, Zhizhen Liu, Lei Liang, Huajun Chen, Wen Zhang</div>
<div class="meta-line">First: 2026-01-26T09:23:53+00:00 · Latest: 2026-01-26T09:23:53+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18296v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18296v1">PDF</a> · <a href="https://github.com/zjukg/Temp-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Temp-R1：基于逆向课程强化学习的复杂时序知识图谱问答统一自主智能体</div>
<div class="mono" style="margin-top:8px">时序知识图谱问答（TKGQA）因其需要对具有多跳依赖关系和复杂时序约束的动态事实进行精细推理而具有内在挑战性。现有方法依赖固定流程和昂贵的闭源API，限制了灵活性和可扩展性。我们提出Temp-R1——首个通过强化学习训练的端到端自主TKGQA智能体。为应对单步推理中的认知过载问题，我们在外部动作基础上扩展了包含专用内部动作的动作空间。为避免在简单问题上出现捷径学习，我们引入逆向课程学习机制，优先训练困难问题，迫使智能体在迁移至简单案例前先发展出复杂推理能力。我们拥有80亿参数的Temp-R1在MultiTQ和TimelineKGQA基准上取得最先进性能，在复杂问题上较基线模型提升19.8%。本研究为自主时序推理智能体确立了新范式，代码即将发布于https://github.com/zjukg/Temp-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of existing Temporal Knowledge Graph Question Answering (TKGQA) methods, which rely on rigid workflows and costly APIs, by creating a more flexible and scalable autonomous agent. The method introduces Temp-R1, an end-to-end agent trained with reinforcement learning, which expands the action space with specialized internal actions and employs a reverse curriculum learning strategy that starts training on difficult questions to foster sophisticated reasoning before handling easier ones. The main experimental results show that the 8B-parameter Temp-R1 achieves state-of-the-art performance on benchmarks like MultiTQ and TimelineKGQA, with a 19.8% improvement over strong baselines on complex questions, establishing a new paradigm for autonomous temporal reasoning agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机是克服现有时序知识图谱问答方法依赖固定流程和昂贵API的限制，旨在开发更灵活、可扩展的自主智能体。方法上提出了Temp-R1，这是一种通过强化学习训练的端到端智能体，它通过引入专门的内部动作扩展了动作空间，并采用逆向课程学习策略，即先训练困难问题以培养复杂推理能力，再迁移至简单问题。主要实验结果表明，参数量为80亿的Temp-R1在MultiTQ和TimelineKGQA等基准测试中取得了最先进的性能，在复杂问题上比强基线提升了19.8%，为自主时序推理智能体确立了新范式。</div>
</details>
</div>
<div class="card">
<div class="title">TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment</div>
<div class="meta-line">Authors: Zhewen Tan, Wenhan Yu, Jianfeng Si, Tongxin Liu, Kaiqi Guan, Huiyan Jin, Jiawen Tao, Xiaokun Yuan, Duohe Ma, Xiangzheng Zhang, Tong Yang, Lin Sun</div>
<div class="meta-line">First: 2026-01-26T09:21:43+00:00 · Latest: 2026-01-26T09:21:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18292v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18292v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TriPlay-RL：面向大语言模型安全对齐的三角色自博弈强化学习框架</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型的安全风险日益凸显，亟需抑制其生成有害内容。主流的安全对齐范式通常采用包含攻击者（生成对抗性提示）、防御者（实施安全防护）和评估者（评估响应）的三角色协作框架。本文提出一种名为TriPlay-RL的闭环强化学习框架，可在近乎无需人工标注的情况下实现三角色迭代式协同优化。实验表明：攻击者在保持高输出多样性的同时，对抗效果提升20%-50%；防御者在未损害通用推理能力的情况下，安全性能提升10%-30%；评估者通过迭代持续优化细粒度判别能力，能准确区分不安全响应、简单拒绝和有效指导。该框架为大语言模型安全对齐构建了一个高效可扩展的范式，实现了统一学习循环内的持续协同进化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing safety risks of large language models (LLMs) and the need to reduce toxic content generation, this paper introduces TriPlay-RL, a closed-loop reinforcement learning framework that facilitates iterative collaboration among three roles—an attacker generating adversarial prompts, a defender implementing safety measures, and an evaluator assessing responses—with minimal manual annotation. The method enables these roles to co-evolve within a unified learning loop. Experimental results demonstrate that the attacker improves adversarial effectiveness by 20%-50% while maintaining high output diversity, the defender achieves 10%-30% gains in safety performance without compromising general reasoning, and the evaluator continuously refines its ability to accurately distinguish between unsafe responses, simple refusals, and useful guidance. Overall, the framework establishes an efficient and scalable paradigm for LLM safety alignment.</div>
<div class="mono" style="margin-top:8px">针对大语言模型日益突出的安全风险及减少有害内容生成的迫切需求，本文提出了TriPlay-RL，一个闭环强化学习框架，通过攻击者生成对抗性提示、防御者实施安全防护、评估者进行响应评估这三个角色的迭代协作，实现了近乎零人工标注下的协同改进。该方法使三个角色在统一学习循环中共同进化。实验结果表明，攻击者在保持高输出多样性的同时，对抗有效性提升了20%-50%；防御者在安全性能上获得10%-30%的增益，且未损害一般推理能力；评估者通过迭代持续优化其细粒度判断能力，能准确区分不安全响应、简单拒绝和有用指导。总体而言，该框架为大语言模型安全对齐建立了一个高效且可扩展的范式。</div>
</details>
</div>
<div class="card">
<div class="title">The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</div>
<div class="meta-line">Authors: Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</div>
<div class="meta-line">First: 2026-01-21T16:41:58+00:00 · Latest: 2026-01-26T08:29:32+00:00</div>
<div class="meta-line">Comments: Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15165v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15165v2">PDF</a> · <a href="https://github.com/LeapLabTHU/JustGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://nzl-thu.github.io/the-flexibility-trap">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵活性陷阱：为何任意顺序限制扩散语言模型的推理潜力</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）突破了传统LLM严格的从左到右约束，实现了按任意顺序生成词元。直观上，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上为数学和编程等通用任务解锁了更优的推理潜力。因此，许多研究采用强化学习（RL）来激发dLLMs的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但没有扩展dLLMs的推理边界，反而使其收窄。我们发现dLLMs倾向于利用这种顺序灵活性来规避对探索至关重要的高不确定性词元，导致解空间过早坍缩。这一观察促使我们重新思考针对dLLMs的RL方法——现有方法常为保持这种灵活性而投入大量复杂度（如处理组合轨迹和难解似然）。我们证明，通过主动放弃任意顺序生成并改用标准组相对策略优化（GRPO），反而能更有效地激发推理能力。我们的方法JustGRPO极简却异常有效（如在GSM8K上达89.1%准确率），同时完全保留了dLLMs的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the intuitive assumption that the arbitrary token generation order in diffusion language models inherently enhances reasoning capabilities, revealing instead that such flexibility can lead models to avoid high-uncertainty tokens crucial for exploration, thereby prematurely collapsing the solution space. Motivated by this finding, the authors propose a minimalist method, JustGRPO, which intentionally forgoes arbitrary order generation and applies standard Group Relative Policy Optimization to elicit reasoning more effectively. Experimental results demonstrate that this approach is surprisingly effective, achieving high performance on tasks like mathematics, with 89.1% accuracy on GSM8K, while fully retaining the parallel decoding ability of diffusion models.</div>
<div class="mono" style="margin-top:8px">本文挑战了扩散语言模型中任意顺序生成令牌能增强推理能力的直观假设，揭示这种灵活性反而可能导致模型规避对探索至关重要的高不确定性令牌，从而过早地缩小解空间。基于这一发现，作者提出了一种极简方法JustGRPO，它有意放弃任意顺序生成，转而应用标准的组相对策略优化来更有效地激发推理能力。实验结果表明该方法效果显著，在数学等任务上表现优异，如在GSM8K上达到89.1%的准确率，同时完全保留了扩散模型的并行解码能力。</div>
</details>
</div>
<div class="card">
<div class="title">GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation</div>
<div class="meta-line">Authors: Shasha Guo, Liang Pang, Xi Wang, Yanling Wang, Huawei Shen, Jing Zhang</div>
<div class="meta-line">First: 2025-10-13T05:33:51+00:00 · Latest: 2026-01-26T07:35:43+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11020v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11020v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Auxiliary lines are essential for solving complex geometric problems but remain challenging for large vision-language models (LVLMs). Recent attempts construct auxiliary lines via code-driven rendering, a strategy that relies on accurate and executable code generation to produce visual renderings of the auxiliary lines for subsequent reasoning. However, in complex solid geometry settings, such a strong dependence on precise specifications substantially restricts the robustness of this strategy. Alternatively, we turn to a simpler and more stable solution, representing auxiliary-line constructions as structured textual descriptions. To bridge the gap between textual descriptions and spatial structure, we propose a reinforcement learning framework that enhances diagram-text alignment. The core is a cross-modal reward model that evaluates how well the generated auxiliary-line description matches the ground-truth auxiliary-line diagram. The reward signal drives a GRPO-based RL stage to yield informative auxiliary-line descriptions for the reasoning. To support the training and evaluation, we develop a scalable data pipeline and construct AuxSolidMath, a dataset of 3,018 real-exam geometry problems with paired diagrams and aligned textual fields. Based on this framework, we derive GeoVLMath, an LVLM for solving complex solid geometry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoVLMath：通过跨模态奖励增强视觉语言模型在辅助线构建中的几何推理能力</div>
<div class="mono" style="margin-top:8px">辅助线对解决复杂几何问题至关重要，但对大型视觉语言模型（LVLM）仍具挑战。现有方法通过代码驱动渲染构建辅助线，该策略依赖准确且可执行的代码生成来呈现辅助线视觉化以支持后续推理。然而在复杂立体几何场景中，这种对精确规范的高度依赖严重限制了策略的鲁棒性。为此，我们转向更简洁稳定的解决方案：将辅助线构建表示为结构化文本描述。为弥合文本描述与空间结构间的差距，我们提出强化学习框架以增强图文对齐，其核心是评估生成辅助线描述与真实辅助线图示匹配度的跨模态奖励模型。该奖励信号驱动基于GRPO的强化学习阶段，生成具有信息量的辅助线描述以支持推理。为支撑训练与评估，我们开发了可扩展数据流水线，构建了包含3,018道真实考题、配备配对图示与对齐文本字段的立体几何数据集AuxSolidMath。基于此框架，我们最终得到用于解决复杂立体几何问题的LVLM模型GeoVLMath。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling large vision-language models (LVLMs) to create auxiliary lines for solving complex geometry problems, where existing code-driven rendering methods are fragile in solid geometry settings due to their reliance on precise code generation. The proposed method introduces a reinforcement learning framework that represents auxiliary lines as structured textual descriptions and uses a cross-modal reward model to align these descriptions with ground-truth diagrams, optimizing the generation via GRPO-based reinforcement learning to produce informative descriptions for reasoning. Experimental results, supported by a new dataset AuxSolidMath of 3,018 geometry problems, demonstrate that the derived model GeoVLMath effectively enhances geometry reasoning in LVLMs for complex solid geometry.</div>
<div class="mono" style="margin-top:8px">本文针对大型视觉语言模型在解决复杂几何问题时创建辅助线的挑战，指出现有基于代码渲染的方法在立体几何中因依赖精确代码生成而脆弱。所提方法采用强化学习框架，将辅助线表示为结构化文本描述，并通过跨模态奖励模型评估描述与真实几何图的对齐程度，利用基于GRPO的强化学习优化生成，以产生用于推理的信息性描述。实验基于新构建的包含3018个几何问题的AuxSolidMath数据集，结果表明所推导的GeoVLMath模型能有效增强大型视觉语言模型在复杂立体几何中的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks</div>
<div class="meta-line">Authors: Haotian Li, Shijun Yang, Weizhen Qi, Silei Zhao, Rui Hua, Mingzhu Song, Xiaojian Yang, Chao Peng</div>
<div class="meta-line">First: 2026-01-26T07:27:47+00:00 · Latest: 2026-01-26T07:27:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18226v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18226v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system&#x27;s capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>云觉智能体技术报告：面向开放任务的完全可复现、零启动原位自进化智能体系统</div>
<div class="mono" style="margin-top:8px">传统智能体系统在任务分布持续漂移且外部监督稀缺的开放环境中常面临挑战。其依赖静态工具集或离线训练的模式难以适应动态变化，导致系统能力边界僵化且未知。为此，我们提出原位自进化范式，将序列化任务交互视为连续经验流，使系统能在无真实标签的情况下，将短期执行反馈提炼为长期可复用能力。在此框架中，我们将工具进化定位为能力扩展的关键路径，因其提供可验证的二元反馈信号。基于此，我们开发了云觉智能体系统，通过迭代合成、优化和复用工具以应对新兴挑战。为提升进化效率，我们进一步提出并行批量进化策略。在零启动设置下对五个多样化基准的实证评估显示，该系统性能显著优于专有基线。补充性热启动实验也证实，系统积累的通用知识可无缝迁移至新领域。最后，我们提出一种监测进化收敛的新指标，其功能类似于传统优化中的训练损失。我们开源了代码库、系统轨迹及进化工具，以促进韧性自进化智能研究的未来发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of conventional agent systems in open-ended environments with drifting task distributions and scarce supervision, this paper proposes the In-Situ Self-Evolving paradigm, which treats sequential task interactions as an experience stream to distill feedback into reusable capabilities without ground-truth labels. The method centers on tool evolution as a pathway for capability expansion, implemented in the Yunjue Agent system that iteratively synthesizes, optimizes, and reuses tools, enhanced by a Parallel Batch Evolution strategy for efficiency. Experimental results from five benchmarks in a zero-start setting show significant performance gains over proprietary baselines, with warm-start evaluations confirming transferable accumulated knowledge, and a novel metric is introduced to monitor evolution convergence.</div>
<div class="mono" style="margin-top:8px">针对传统智能体系统在任务分布持续变化且缺乏外部监督的开放环境中能力受限的问题，本文提出了原位自进化范式，将序列任务交互视为经验流，从而在没有真实标签的情况下将短期执行反馈提炼为长期可重用的能力。该方法以工具进化为核心扩展途径，开发了云雀智能体系统，通过迭代合成、优化和重用工具来应对新挑战，并采用并行批量进化策略提升效率。在零启动设置下的五个多样化基准测试中，实验结果表明其性能显著优于现有基线，补充的热启动评估验证了所积累的通用知识可迁移至新领域，同时引入了一种监测进化收敛的新度量指标。</div>
</details>
</div>
<div class="card">
<div class="title">ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants</div>
<div class="meta-line">Authors: Pei Wang, Yanan Wu, Xiaoshuai Song, Weixun Wang, Gengru Chen, Zhongwen Li, Kezhong Yan, Ken Deng, Qi Liu, Shuaibing Zhao, Shaopan Xiong, Xuepeng Liu, Xuefeng Chen, Wanxi Deng, Wenbo Su, Bo Zheng</div>
<div class="meta-line">First: 2026-01-26T07:24:28+00:00 · Latest: 2026-01-26T07:24:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18225v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18225v1">PDF</a> · <a href="https://github.com/ShopAgent-Team/ShopSimulator">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShopSimulator：面向购物助手的RL驱动LLM智能体评估与探索</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的智能体在电子商务购物中的应用日益广泛。为执行全面且用户定制的产品搜索，智能体需解析个人偏好、进行多轮对话，并最终在高度相似的产品中进行检索与甄别。然而，现有研究尚未提供一个能系统涵盖所有这些方面的统一仿真环境，且往往仅关注评估基准而缺乏训练支持。本文提出ShopSimulator——一个大规模、高挑战性的中文购物仿真环境。基于该环境，我们在多样化场景中评估LLM性能，发现即使最优模型的完全成功率仍低于40%。错误分析表明，智能体在长轨迹任务中难以实现深度搜索与产品选择，无法平衡个性化线索的运用，且与用户的有效互动能力不足。进一步的训练探索为克服这些缺陷提供了实践指导，其中监督微调（SFT）与强化学习（RL）的结合带来了显著的性能提升。代码与数据将在https://github.com/ShopAgent-Team/ShopSimulator发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of a unified simulation environment for evaluating and training LLM-based shopping assistants that need to interpret preferences, conduct multi-turn dialogues, and discriminate among similar products, this paper introduces ShopSimulator, a large-scale Chinese shopping environment. The method involves using this simulator to evaluate various LLMs and then exploring training strategies like supervised fine-tuning and reinforcement learning to address identified weaknesses. The main experimental results show that even top models achieve under a 40% full-success rate, struggling with deep search, product selection in long trajectories, and balancing personalization cues, but the combination of SFT and RL yields significant performance improvements.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有研究缺乏一个统一的模拟环境来评估和训练基于大语言模型的购物助手，这些助手需要理解用户偏好、进行多轮对话并区分相似产品。为此，本文提出了ShopSimulator，一个大规模的中文购物模拟环境。方法上，利用该模拟器评估多种大语言模型，并探索监督微调和强化学习等训练策略以解决发现的缺陷。主要实验结果表明，即使表现最佳的模型其完全成功率也不足40%，在深度搜索、长轨迹中的产品选择以及平衡个性化线索方面存在困难，但结合监督微调和强化学习能显著提升性能。</div>
</details>
</div>
<div class="card">
<div class="title">Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents</div>
<div class="meta-line">Authors: Zhihan Liu, Lin Guan, Yixin Nie, Kai Zhang, Zhuoqun Hao, Lin Chen, Asli Celikyilmaz, Zhaoran Wang, Na Zhang</div>
<div class="meta-line">First: 2026-01-26T07:07:03+00:00 · Latest: 2026-01-26T07:07:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18217v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18217v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>降低泛化税：面向LLM智能体的强化学习训练跨域泛化研究</div>
<div class="mono" style="margin-top:8px">通用型LLM智能体通常在有限环境集上进行后训练，却需部署至更广泛、未见过的领域。本研究探讨了当最终测试领域未知时，智能体后训练面临的挑战。具体而言，我们分析了强化学习（RL）环境特性与建模选择对跨域性能的关键影响。首先，我们识别出与跨域泛化强相关的两个环境维度：（i）状态信息丰富度，即智能体从状态中需处理的信息量；（ii）规划复杂度，通过基础策略下的目标可达性与轨迹长度估算。值得注意的是，领域真实性与文本相似度并非主要因素；例如，简单的网格世界环境Sokoban在SciWorld中展现出比更真实的ALFWorld更强的泛化能力。基于这些发现，我们进一步证明仅提升状态信息丰富度即可有效增强跨域鲁棒性。我们提出一种低开销、广泛适用的随机化技术：在状态中添加少量与目标无关的干扰特征以丰富状态信息，同时不改变任务本质。除环境特性外，我们还检验了多种建模选择：（a）监督微调（SFT）预热或中期训练有助于防止RL期间的灾难性遗忘，但会削弱对未包含在中期训练数据混合中的领域的泛化能力；（b）在RL中启用逐步推理，虽不总能提升域内性能，但对保持泛化能力具有关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how to improve the cross-domain generalization of LLM agents trained with reinforcement learning (RL) when test environments are unknown. The authors identify that state information richness and planning complexity are key environmental factors influencing generalization, more so than domain realism or textual similarity, and demonstrate that increasing state richness via low-overhead randomization with distractive features enhances robustness. Their experiments also reveal that supervised fine-tuning (SFT) warmup helps prevent catastrophic forgetting but can harm generalization to excluded domains, while incorporating step-by-step thinking during RL training is crucial for maintaining out-of-domain performance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在测试环境未知的情况下，如何提升通过强化学习训练的LLM智能体的跨领域泛化能力。作者发现，状态信息丰富度和规划复杂度是影响泛化的关键环境因素，其重要性超过领域真实性或文本相似性；通过添加少量分散注意力的目标无关特征来低成本地增加状态丰富度，可以有效提升鲁棒性。实验还表明，监督微调预热有助于防止灾难性遗忘，但会损害对未包含领域的泛化，而在强化学习中启用逐步思考对于保持跨领域性能至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR</div>
<div class="meta-line">Authors: James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano, Min Woo Sun, Emma Lundberg, Serena Yeung-Levy</div>
<div class="meta-line">First: 2026-01-26T06:46:16+00:00 · Latest: 2026-01-26T06:46:16+00:00</div>
<div class="meta-line">Comments: EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18207v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18207v1">PDF</a> · <a href="https://huggingface.co/collections/jmhb/papersearchqa">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PaperSearchQA：基于RLVR的科研文献检索与推理学习框架</div>
<div class="mono" style="margin-top:8px">检索智能体是通过语言模型对知识库（或网络）进行检索推理以回答问题的系统；当前主流方法仅采用可验证奖励强化学习监督最终答案准确性。现有RLVR检索智能体多面向通用领域问答，限制了其在科学、工程、医学等专业AI系统中的适用性。本研究提出训练面向科研文献检索推理的智能体——既能检验技术性问答能力，又直接服务于科研实践，其能力对未来的AI科学家系统至关重要。具体而言，我们发布了包含1600万篇生物医学论文摘要的检索语料库，并构建了含6万样本、可从语料库获取答案的挑战性事实型问答数据集PaperSearchQA及基准测试。在该环境中训练的检索智能体性能超越非强化学习检索基线；定量分析进一步揭示了智能体在规划、推理、自我验证等方面的行为特征。本研究的语料库、数据集及基准测试可通过RLVR训练框架Search-R1代码库调用，并发布于https://huggingface.co/collections/jmhb/papersearchqa。数据构建方法具备可扩展性，可便捷迁移至其他科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces PaperSearchQA, a system designed to train language models as search agents for answering technical questions by searching and reasoning over scientific papers, motivated by the need to extend reinforcement learning with verifiable rewards (RLVR) methods beyond general-domain QA to support real-world scientific applications. The method involves creating a search corpus of 16 million biomedical paper abstracts and a factoid QA dataset with 60k samples, training agents within this environment to outperform non-RL retrieval baselines. Experimental results show that the trained agents exhibit improved performance and demonstrate behaviors such as planning, reasoning, and self-verification, with the resources made publicly available for further research and scalability to other scientific domains.</div>
<div class="mono" style="margin-top:8px">本文提出了PaperSearchQA系统，旨在训练语言模型作为搜索代理，通过搜索和推理科学论文来回答技术问题，其动机是将基于可验证奖励的强化学习方法从通用领域问答扩展到支持现实世界的科学应用。该方法构建了一个包含1600万篇生物医学论文摘要的搜索语料库和一个包含6万个样本的事实型问答数据集，并在此环境中训练代理以超越非强化学习检索基线。实验结果表明，训练后的代理表现出更好的性能，并展示了规划、推理和自我验证等行为，相关资源已公开供进一步研究，并可扩展至其他科学领域。</div>
</details>
</div>
<div class="card">
<div class="title">When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards</div>
<div class="meta-line">Authors: Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun Zhou</div>
<div class="meta-line">First: 2026-01-22T03:15:57+00:00 · Latest: 2026-01-26T06:32:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15609v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15609v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当锐化演变为坍缩：可验证奖励强化学习中的采样偏差与语义耦合</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是将大语言模型转化为可靠问题求解器的核心范式，尤其在逻辑密集型领域表现突出。尽管实证研究已证实其有效性，但RLVR究竟是激发了新能力，还是仅对现有知识分布进行了锐化，仍不明确。本研究通过形式化“过度锐化”现象——即策略坍缩至有限模式并压制有效替代方案的过程——来探讨这一问题。研究发现，有限批次更新本质上会使学习偏向已采样模式，从而引发通过语义耦合全局传播的坍缩效应。为缓解此问题，我们提出逆成功优势校准（优先处理困难查询）与分布级校准（通过记忆网络实现采样多样化）两种方法。实证评估表明，这些策略能有效提升模型的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether Reinforcement Learning with Verifiable Rewards (RLVR) genuinely enhances large language models&#x27; capabilities or merely refines existing knowledge, identifying a risk of policy collapse termed &#x27;over-sharpening&#x27; where learning becomes biased toward sampled modes due to finite-batch updates and semantic coupling. To address this, the authors propose two mitigation strategies: inverse-success advantage calibration to prioritize challenging queries and distribution-level calibration using a memory network to diversify sampling. Experimental results demonstrate that these methods effectively improve model generalization by countering the collapse phenomenon.</div>
<div class="mono" style="margin-top:8px">本文研究了基于可验证奖励的强化学习（RLVR）是真正提升大语言模型能力还是仅优化已有知识，发现了一种称为&#x27;过度锐化&#x27;的策略崩溃风险，即有限批次更新会固有地使学习偏向采样模式，并通过语义耦合全局传播。为缓解此问题，作者提出了两种策略：逆成功优势校准以优先处理困难查询，以及通过记忆网络进行分布级校准以多样化采样。实验验证表明，这些方法能有效对抗崩溃现象，从而提升模型的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning</div>
<div class="meta-line">Authors: Sihan Zeng, Sujay Bhatt, Sumitra Ganesh, Alec Koppel</div>
<div class="meta-line">First: 2026-01-23T02:12:24+00:00 · Latest: 2026-01-26T05:27:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16399v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16399v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于双层强化学习的正则化行动者-评论家算法</div>
<div class="mono" style="margin-top:8px">本文研究一个结构化的双层优化问题，其中上层目标为光滑函数，下层问题是在马尔可夫决策过程中的策略优化。上层决策变量参数化下层MDP的奖励函数，上层目标取决于诱导的最优策略。现有双层优化与强化学习方法常需二阶信息、对下层施加强正则化，或通过嵌套循环过程低效使用样本。本研究提出一种单循环一阶行动者-评论家算法，通过基于惩罚的重构优化双层目标。我们在下层RL目标中引入衰减熵正则化，使得无需精确求解无正则化RL问题即可实现渐近无偏的上层超梯度估计。通过特殊类型Polyak-Lojasiewicz条件下的新型下层残差分析，我们证明了所提算法在有限时间和有限样本内收敛至原始无正则化双层优化问题的驻点。通过在GridWorld目标定位问题及基于人类反馈强化学习的积极推文生成任务上的实验，验证了方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the computational challenges in bi-level reinforcement learning, where an upper-level objective depends on the optimal policy of a lower-level Markov decision process with a parameterized reward. To avoid inefficient nested loops and reliance on second-order information, the authors propose a single-loop, first-order actor-critic algorithm that reformulates the problem using a penalty method and incorporates an attenuating entropy regularization in the lower-level policy optimization. This approach enables asymptotically unbiased gradient estimation for the upper level without exact solution of the unregularized RL problem. Experimental validation on a GridWorld task and a reinforcement learning from human feedback application for tweet generation demonstrates the method&#x27;s effectiveness, with theoretical convergence guarantees to a stationary point under a Polyak-Lojasiewicz condition.</div>
<div class="mono" style="margin-top:8px">本文针对双层强化学习中的计算难题展开研究，其中上层目标函数依赖于下层马尔可夫决策过程（其奖励由参数化定义）的最优策略。为避免低效的嵌套循环和对二阶信息的依赖，作者提出了一种单循环、一阶的行动者-评论家算法，通过惩罚方法重构问题，并在下层策略优化中引入衰减的熵正则化。该方法能在无需精确求解无正则化强化学习问题的情况下，为上层提供渐近无偏的梯度估计。在GridWorld目标定位任务和基于人类反馈的强化学习（用于生成积极推文）的实验验证中，该方法表现出有效性，并在满足Polyak-Lojasiewicz条件的理论框架下保证了收敛到稳定点。</div>
</details>
</div>
<div class="card">
<div class="title">FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaopeng Qiu, Shuang Yu, Jingqi Zhang, Shuai Zhang, Xue Huang, Jingyi Yang, Junjie Lai</div>
<div class="meta-line">First: 2026-01-26T05:12:05+00:00 · Latest: 2026-01-26T05:12:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18150v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FP8-RL：面向大语言模型强化学习的实用稳定低精度技术栈</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的强化学习（RL）日益受限于生成阶段：长输出序列导致注意力机制与KV缓存内存成为端到端步骤的主要耗时瓶颈。FP8通过降低生成阶段的计算成本与内存流量，为加速RL提供了有效途径，但其在RL中的应用面临独特的工程与算法挑战：策略权重每步更新（需重复量化并同步至推理引擎），且低精度生成轨迹可能偏离训练器预设的高精度策略，引发训练-推理失配及潜在不稳定问题。本报告提出面向LLM RL的实用FP8生成技术栈，基于veRL生态系统实现，支持主流训练后端（如FSDP/Megatron-LM）与推理引擎（如vLLM/SGLang）。我们（i）采用分块FP8量化实现W8A8线性层生成，（ii）通过逐步QKV缩放重校准将FP8扩展至KV缓存以消除长上下文内存瓶颈，（iii）利用基于重要性采样的轨迹校正技术（词元级TIS/MIS变体）缓解失配效应。在稠密与混合专家模型中，该方案在保持与BF16基线相当学习性能的同时，实现最高44%的生成吞吐量提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the computational and memory bottlenecks in reinforcement learning (RL) for large language models (LLMs), particularly during the rollout phase where long sequences strain attention and KV-cache memory. The method introduces a practical FP8 rollout stack that employs blockwise FP8 quantization for linear layers, extends FP8 to the KV-cache with per-step scale recalibration, and mitigates train-inference mismatch using importance-sampling-based correction techniques. Experimental results show that across dense and mixture-of-experts models, this approach achieves up to 44% rollout throughput gains while maintaining learning behavior comparable to BF16 baselines, demonstrating stability and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大语言模型强化学习中计算和内存瓶颈问题，特别是在生成阶段，长序列会加剧注意力机制和KV缓存的负担。方法上，提出了一个实用的FP8生成堆栈，采用分块FP8量化处理线性层，将FP8扩展到KV缓存并通过每步重新校准尺度来消除长上下文内存瓶颈，同时使用基于重要性采样的校正技术来缓解训练与推理的不匹配。实验结果表明，在密集和混合专家模型中，该技术实现了高达44%的生成吞吐量提升，同时保持了与BF16基线相当的学习性能，证明了其稳定性和高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes</div>
<div class="meta-line">Authors: Heguang Lin, Binhao Chen, Mengze Li, Daniel Pimentel-Alarcón, Matthew L. Malloy</div>
<div class="meta-line">First: 2026-01-26T05:07:47+00:00 · Latest: 2026-01-26T05:07:47+00:00</div>
<div class="meta-line">Comments: 15 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18145v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18145v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多项分布结果的最小体积置信集精确交集</div>
<div class="mono" style="margin-top:8px">置信集的计算是数据科学与机器学习的核心，作为A/B测试的主要工具，并支撑强化学习算法的运行与分析。在所有针对多项分布参数的有效置信集中，最小体积置信集（MVCs）因其最小化平均体积而最优，但它们由精确p值的水平集定义，该p值不连续且难以计算。本文不直接刻画MVCs的几何特性，而是研究一个实际驱动的决策问题：给定两个观测到的多项分布结果，能否判定其MVCs是否相交？我们提出了一种经过认证、容错感知的交集判定算法。该方法利用似然排序在对数几率坐标中诱导半空间约束，实现对参数空间的自适应几何划分，并计算每个单元上p值的可计算上下界。对于三类情况，该算法高效且可证明可靠，能够认证交集、认证不相交，或在决策处于预设容差范围内时返回不确定结果。我们进一步展示了该方法如何扩展到更高维度。结果表明，尽管MVCs几何结构不规则，但其在A/B测试核心任务中允许可靠的认证决策过程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable confidence sets in A/B testing and reinforcement learning, this paper addresses the challenge of determining whether minimum-volume confidence sets (MVCs) for multinomial parameters intersect, given two observed outcomes. The method leverages likelihood ordering to transform the problem into halfspace constraints in log-odds coordinates, enabling an adaptive geometric partitioning of parameter space with computable bounds on p-values over each cell. Experimental results show that for three categories, the algorithm efficiently certifies intersection, disjointness, or an indeterminate result within a margin, with extensions to higher dimensions, demonstrating that MVCs support certified decision procedures despite their complex geometry.</div>
<div class="mono" style="margin-top:8px">本文的动机源于A/B测试和强化学习中需要可靠置信集的问题，旨在解决给定两个观测结果时，如何判定多项分布参数的最小体积置信集是否相交的难题。方法利用似然排序将问题转化为对数几率坐标中的半空间约束，从而实现对参数空间的自适应几何划分，并计算每个单元上p值的可计算边界。实验结果表明，对于三个类别，该算法能高效地认证相交、不相交或在预设容差内返回不确定结果，并可推广到更高维度，证明了尽管最小体积置信集几何结构复杂，但仍支持核心任务的可认证决策过程。</div>
</details>
</div>
<div class="card">
<div class="title">Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods</div>
<div class="meta-line">Authors: Mingxu Zhang, Huicheng Zhang, Jiaming Ji, Yaodong Yang, Ying Sun</div>
<div class="meta-line">First: 2026-01-26T04:54:57+00:00 · Latest: 2026-01-26T04:54:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18142v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于ADRC拉格朗日方法增强强化学习的安全性</div>
<div class="mono" style="margin-top:8px">安全强化学习（Safe RL）旨在最大化奖励的同时满足安全约束，通常通过基于拉格朗日的方法实现。然而，现有方法（包括PID和经典拉格朗日方法）因参数敏感性和固有相位滞后问题，常出现振荡和频繁的安全违规。为克服这些局限，我们提出ADRC-拉格朗日方法，利用自抗扰控制（ADRC）提升鲁棒性并减少振荡。该统一框架将经典及PID拉格朗日方法作为特例，同时显著提升安全性能。大量实验表明，本方法将安全违规次数降低74%，违规幅度减少89%，平均成本下降67%，在复杂环境中为安全强化学习确立了卓越的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing Lagrangian-based methods in safe reinforcement learning, which often suffer from oscillations and frequent safety violations due to parameter sensitivity and phase lag. The authors propose integrating Active Disturbance Rejection Control (ADRC) into the Lagrangian framework to enhance robustness and reduce oscillations, presenting a unified approach that subsumes classical and PID Lagrangian methods as special cases. Experimental results demonstrate that the ADRC-Lagrangian method significantly improves safety performance, reducing safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67% in complex environments.</div>
<div class="mono" style="margin-top:8px">本文针对安全强化学习中现有拉格朗日方法因参数敏感性和相位滞后导致的振荡与频繁安全违规问题，提出将自抗扰控制（ADRC）集成到拉格朗日框架中以增强鲁棒性并减少振荡，形成了一个统一框架，将经典和PID拉格朗日方法涵盖为特例。实验结果表明，该方法显著提升了安全性能，在复杂环境中将安全违规次数降低达74%、违规幅度减少89%、平均成本下降67%。</div>
</details>
</div>
<div class="card">
<div class="title">Tail Distribution of Regret in Optimistic Reinforcement Learning</div>
<div class="meta-line">Authors: Sajad Khodadadian, Mehrdad Moharrami</div>
<div class="meta-line">First: 2025-11-23T02:23:09+00:00 · Latest: 2026-01-26T04:16:22+00:00</div>
<div class="meta-line">Comments: 17 pages, 0 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18247v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18247v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\Pr(R_K \ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观强化学习中遗憾值的尾部分布</div>
<div class="mono" style="margin-top:8px">本文针对具有未知转移动态的有限时域表格型马尔可夫决策过程，推导了基于乐观策略的强化学习算法遗憾值的实例相关尾部边界。聚焦于UCBVI类算法，我们刻画了K幕中累积遗憾$R_K$的尾部分布特性，而非仅分析其期望或单一高概率分位数。我们研究了两种典型的探索奖励调度方案：(i) 显式依赖总幕数$K$的$K$相关方案；(ii) 仅依赖当前幕索引的$K$无关方案。针对两种设置，我们获得了$\Pr(R_K \ge x)$的上界，该上界呈现独特的双区制结构：从实例相关尺度$m_K$开始至转移阈值呈现亚高斯尾部，超过该阈值后则呈现亚威布尔尾部。我们进一步推导了期望遗憾$\mathbb{E}[R_K]$对应的实例相关边界。所提算法依赖调节参数$α$，该参数平衡了期望遗憾与遗憾呈现亚高斯尾部的范围。据我们所知，本研究首次为幕式强化学习中标准乐观算法提供了系统的尾部遗憾保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to understand the full tail distribution of cumulative regret in optimistic reinforcement learning, moving beyond just expected regret or single high-probability bounds. The method focuses on a UCBVI-type algorithm for finite-horizon tabular Markov decision processes with unknown transitions, analyzing two exploration-bonus schedules: one dependent on the total number of episodes K and another independent of K. The main experimental results show that the tail probability of regret exhibits a two-regime structure: a sub-Gaussian tail up to an instance-dependent transition threshold, followed by a sub-Weibull tail beyond it, with the algorithm&#x27;s tuning parameter α balancing expected regret and the sub-Gaussian range.</div>
<div class="mono" style="margin-top:8px">本文的动机是需要理解乐观强化学习中累积遗憾的完整尾部分布，而不仅仅是期望遗憾或单一高概率界限。方法聚焦于一种UCBVI型算法，用于处理具有未知转移的有限时域表格马尔可夫决策过程，分析了两种探索奖励调度方案：一种依赖于总回合数K，另一种独立于K。主要实验结果表明，遗憾的尾部概率呈现两阶段结构：在实例相关的转换阈值之前是亚高斯尾部，之后是亚威布尔尾部，算法中的调优参数α可平衡期望遗憾和亚高斯范围。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions</div>
<div class="meta-line">Authors: Pedram Agand, Mo Chen</div>
<div class="meta-line">First: 2026-01-26T03:38:27+00:00 · Latest: 2026-01-26T03:38:27+00:00</div>
<div class="meta-line">Comments: 11 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random&#x27;&#x27; and ``suboptimal&#x27;&#x27; data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越静态数据集：基于验证合成转移的稳健离线策略优化</div>
<div class="mono" style="margin-top:8px">离线强化学习（ORL）在工业机器人等安全关键领域具有巨大潜力，这些领域通常难以进行实时环境交互。ORL的主要障碍仍是静态数据集与学习策略之间的分布偏移，这通常要求高度保守性，从而限制了策略的潜在改进。我们提出了MoReBRAC，一种基于模型的框架，通过不确定性感知的潜在合成来解决这一局限。MoReBRAC不依赖固定数据，而是利用双循环世界模型合成高保真转移，以扩展训练流形。为确保合成数据的可靠性，我们实施了分层不确定性流程，整合了变分自编码器（VAE）流形检测、模型敏感性分析和蒙特卡洛（MC）丢弃法。这种多层过滤过程保证仅使用位于学习动态高置信区域的转移。我们在D4RL Gym-MuJoCo基准测试中的结果显示显著性能提升，尤其在“随机”和“次优”数据机制中。我们进一步探讨了VAE作为几何锚点的作用，并分析了从近最优数据集学习时遇到的分布权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of distributional shift in offline reinforcement learning (ORL), which limits policy improvement due to conservatism when learning from static datasets. The authors propose MoReBRAC, a model-based framework that augments training data by synthesizing high-fidelity transitions using a dual-recurrent world model, with a hierarchical uncertainty pipeline involving VAE manifold detection, model sensitivity analysis, and MC dropout to vet synthetic data for reliability. Experimental results on D4RL Gym-MuJoCo benchmarks demonstrate significant performance gains, especially in random and suboptimal data regimes, while also analyzing the VAE&#x27;s role as a geometric anchor and trade-offs in near-optimal datasets.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习（ORL）中的分布偏移问题展开研究，该问题导致从静态数据集学习时需保持保守性，从而限制策略改进。作者提出MoReBRAC这一基于模型的框架，它通过双循环世界模型合成高保真状态转移以扩充训练数据，并采用包含VAE流形检测、模型敏感性分析和MC dropout的分层不确定性流程来确保合成数据的可靠性。在D4RL Gym-MuJoCo基准测试上的实验结果表明，该方法尤其在随机和次优数据环境中取得了显著性能提升，同时分析了VAE作为几何锚点的作用及在近最优数据集中的分布权衡。</div>
</details>
</div>
<div class="card">
<div class="title">RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS</div>
<div class="meta-line">Authors: Cong Wang, Changfeng Gao, Yang Xiang, Zhihao Du, Keyu An, Han Zhao, Qian Chen, Xiangang Li, Yingming Gao, Ya Li</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-12-04T08:12:49+00:00 · Latest: 2026-01-26T03:09:00+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04552v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04552v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lrwinr.github.io/RRPO-CosyVoice">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RRPO：基于大语言模型的情感语音合成的鲁棒奖励策略优化</div>
<div class="mono" style="margin-top:8px">DiffRO等可微分强化学习框架为可控文本转语音提供了强大方法，但在情感控制等精细任务中易受奖励攻击影响。策略模型可能通过生成声学伪影来利用普通奖励模型获取虚假奖励，却会损害感知质量。为此，我们提出鲁棒奖励策略优化框架，采用混合正则化方案构建鲁棒奖励模型，使其奖励信号更可靠地符合人类感知，促使策略放弃有害捷径，转而学习真实情感的复杂特征。消融实验证实了奖励模型的鲁棒性提升，其强大的跨语言泛化能力即为明证。主观评估表明，该鲁棒奖励模型有效缓解了奖励攻击，在情感表现力和自然度上均显著超越所有基线。演示页面：https://lrwinr.github.io/RRPO-CosyVoice。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the vulnerability of differentiable reinforcement learning frameworks like DiffRO to reward hacking in controllable text-to-speech, where models exploit acoustic artifacts to achieve spurious rewards, degrading perceptual quality. The method proposed is Robust Reward Policy Optimization (RRPO), which employs a hybrid regularization scheme to develop a robust reward model whose signals are more reliably aligned with human perception, thereby compelling the policy to learn genuine emotional features instead of detrimental shortcuts. The main experimental results from ablation studies confirm the enhanced robustness and strong cross-lingual generalization of the reward model, while subjective evaluations demonstrate that RRPO effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决可微分强化学习框架在可控文本转语音中易受奖励攻击的脆弱性问题，即模型通过生成声学伪影获取虚假奖励，从而降低感知质量。所提出的方法是鲁棒奖励策略优化，它采用混合正则化方案来开发一个鲁棒的奖励模型，其奖励信号更可靠地与人类感知对齐，从而迫使策略学习真实的情感特征而非有害的捷径。主要实验结果通过消融研究证实了奖励模型的增强鲁棒性和强大的跨语言泛化能力，同时主观评估表明该方法有效缓解了奖励攻击，在情感表现力和自然度上均显著优于所有基线。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control</div>
<div class="meta-line">Authors: Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan</div>
<div class="meta-line">First: 2026-01-26T01:59:46+00:00 · Latest: 2026-01-26T01:59:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18069v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18069v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散模型的强化学习在信息版本年龄调度中的应用：平均与尾部风险敏感控制</div>
<div class="mono" style="margin-top:8px">在实时无线系统中，确保信息传递的及时性与语义准确性至关重要。信息年龄（AoI）量化了时间新鲜度，而信息版本年龄（VAoI）通过考虑收发端之间的版本演化，捕捉了语义陈旧性。现有VAoI调度方法主要聚焦于最小化平均VAoI，忽略了在随机数据包到达和不可靠信道下可能损害系统可靠性的罕见但严重的陈旧事件。本文研究了多用户状态更新系统中，在长期传输成本约束下的平均导向与尾部风险敏感VAoI调度。首先将平均VAoI最小化问题建模为约束马尔可夫决策过程，并提出一种基于深度扩散的柔性演员-评论家（D2SAC）算法。通过扩散去噪过程生成动作，D2SAC增强了策略表达能力，并为平均性能建立了强基准。在此基础上，我们提出RS-D3SAC算法——一种风险敏感的深度分布式扩散柔性演员-评论家算法。RS-D3SAC将基于扩散的演员网络与基于分位数的分布式评论家网络相结合，显式建模完整的VAoI回报分布。这使得通过条件风险价值（CVaR）进行原则性尾部风险优化成为可能，同时满足长期传输成本约束。大量仿真表明，D2SAC能降低平均VAoI，而RS-D3SAC在不牺牲平均性能的前提下持续实现CVaR的显著降低。尾部风险降低的主要增益源于分布式评论家，而基于扩散的演员网络通过补充性优化来稳定和丰富策略决策，突显了二者在多用户无线系统中实现鲁棒且风险感知的VAoI调度的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to ensure both timely and semantically accurate information delivery in multi-user wireless systems, where existing Version Age of Information (VAoI) scheduling methods focus on average performance and neglect rare but severe staleness events that threaten reliability. The method introduces two algorithms: first, a deep diffusion-based Soft Actor-Critic (D2SAC) for average VAoI minimization, formulated as a constrained Markov decision process, which uses a diffusion-based denoising process to enhance policy expressiveness; second, a risk-sensitive extension called RS-D3SAC that integrates a diffusion-based actor with a quantile-based distributional critic to explicitly model the full VAoI return distribution and optimize tail risk via Conditional Value-at-Risk (CVaR) under long-term transmission cost constraints. Experimental results from simulations demonstrate that D2SAC effectively reduces average VAoI, while RS-D3SAC achieves substantial reductions in CVaR (tail risk) without sacrificing mean performance, with the distributional critic driving major gains and the diffusion actor providing complementary refinement for stable, risk-aware scheduling.</div>
<div class="mono" style="margin-top:8px">本文的动机在于确保多用户无线系统中信息传递的及时性和语义准确性，现有版本信息年龄调度方法主要关注平均性能，忽略了可能损害可靠性的罕见但严重的陈旧事件。方法上提出了两种算法：首先是一种基于深度扩散的软演员-评论家算法，用于平均版本信息年龄最小化，它通过扩散去噪过程增强策略表达能力；其次是一种风险敏感的扩展算法RS-D3SAC，它结合了基于扩散的演员和基于分位数的分布评论家，以显式建模版本信息年龄回报的完整分布，并在长期传输成本约束下通过条件风险价值优化尾部风险。实验仿真结果表明，D2SAC能有效降低平均版本信息年龄，而RS-D3SAC在不牺牲平均性能的情况下显著降低了条件风险价值，其中分布评论家贡献了主要增益，扩散演员则提供了补充优化，实现了稳健且风险感知的调度。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Attention Reasoning via Hierarchical Search and Self-Verification</div>
<div class="meta-line">Authors: Wei Cai, Jian Zhao, Yuchen Yuan, Tianle Zhang, Ming Zhu, Haichuan Tang, Xuelong Li</div>
<div class="meta-line">First: 2025-10-21T13:18:44+00:00 · Latest: 2026-01-26T01:56:26+00:00</div>
<div class="meta-line">Comments: The paper is withdrawn by the authors after discovering a flaw in the theoretical derivation presented in the Method section. This incorrect step leads to conclusions that are not supported by the corrected derivation. The authors plan to reconstruct the argument and will release an updated version once the issue is fully resolved</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18619v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.18619v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) frequently hallucinate due to their reliance on fragile, linear reasoning and weak visual grounding. We propose Visual Attention Reasoning (VAR), a reinforcement learning framework that reformulates reasoning as a hierarchical search with self-verification. VAR enforces traceable evidence grounding by generating explicit bounding boxes, guided by a novel reward function combining geometric precision and semantic sufficiency. Furthermore, it replaces linear Chain-of-Thought with a tree-search policy capable of backtracking to correct logical errors. Theoretical analysis validates the framework&#x27;s reliability, and extensive experiments demonstrate that VAR significantly outperforms state-of-the-art methods on complex hallucination and safety benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于分层搜索与自验证的视觉注意力推理</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）因其依赖脆弱、线性的推理机制和薄弱的视觉基础而频繁产生幻觉。我们提出视觉注意力推理（VAR），一种强化学习框架，将推理重构为具有自验证功能的分层搜索过程。VAR通过生成显式边界框，在结合几何精度与语义充分性的新型奖励函数引导下，实现可追溯的证据基础。此外，它用具备回溯能力以修正逻辑错误的树搜索策略取代了线性的思维链。理论分析验证了该框架的可靠性，大量实验表明VAR在复杂幻觉与安全性基准测试中显著优于现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of hallucinations in Multimodal Large Language Models (MLLMs) by proposing Visual Attention Reasoning (VAR), a reinforcement learning framework that reformulates reasoning as a hierarchical search process with self-verification. The method generates explicit bounding boxes for traceable visual grounding, guided by a reward function that combines geometric precision and semantic sufficiency, and replaces linear Chain-of-Thought with a backtracking-capable tree-search policy to correct logical errors. Experimental results show VAR significantly outperforms state-of-the-art methods on complex hallucination and safety benchmarks, though the paper was later withdrawn due to a flaw in the theoretical derivation, with plans for an updated version.</div>
<div class="mono" style="margin-top:8px">该论文针对多模态大语言模型（MLLMs）中的幻觉问题，提出了视觉注意力推理（VAR）框架，这是一种将推理重构为具有自验证功能的分层搜索过程的强化学习方法。该方法通过生成显式边界框实现可追溯的视觉基础，并采用结合几何精度和语义充分性的奖励函数进行指导，同时用支持回溯的树搜索策略替代线性思维链以修正逻辑错误。实验结果表明，VAR在复杂幻觉和安全基准测试上显著优于现有先进方法，但论文因理论推导存在缺陷后被作者撤回，计划发布修正后的版本。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
