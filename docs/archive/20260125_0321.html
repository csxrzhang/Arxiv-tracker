<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-25 03:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260125_0321</div>
    <div class="row"><div class="card">
<div class="title">LLM-in-Sandbox Elicits General Agentic Intelligence</div>
<div class="meta-line">Authors: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</div>
<div class="meta-line">First: 2026-01-22T18:57:09+00:00 · Latest: 2026-01-22T18:57:09+00:00</div>
<div class="meta-line">Comments: Project Page: https://llm-in-sandbox.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16206v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16206v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://llm-in-sandbox.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#x27;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM-in-Sandbox激发通用智能体智能</div>
<div class="mono" style="margin-top:8px">我们提出LLM-in-Sandbox方法，使大语言模型能在代码沙盒（即虚拟计算机）内进行探索，从而激发非代码领域的通用智能。我们首先证明，未经额外训练的强大大语言模型展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能自发访问外部资源获取新知识、利用文件系统处理长上下文、执行脚本以满足格式要求。我们进一步表明，通过LLM-in-Sandbox强化学习（仅使用非智能体数据训练模型进行沙盒探索），可增强这些智能体能力。实验表明，LLM-in-Sandbox在免训练和训练后两种设置下，均实现了跨越数学、物理、化学、生物医学、长上下文理解及指令遵循的稳健泛化。最后，我们从计算与系统视角分析其效率，并将其开源为Python包以促进实际部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to elicit general agentic intelligence from large language models (LLMs) for non-code domains, this paper introduces LLM-in-Sandbox, a method that enables LLMs to explore within a code sandbox (a virtual computer) without additional training. The method leverages the sandbox for tasks like accessing external resources, managing long contexts via the file system, and executing scripts, with capabilities further enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL) trained on non-agentic data. Experimental results show robust generalization across mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following in both training-free and post-trained settings, with efficiency analyzed from computational and system perspectives and the tool open-sourced as a Python package.</div>
<div class="mono" style="margin-top:8px">本文旨在激发大型语言模型（LLM）在非代码领域的通用智能，提出了LLM-in-Sandbox方法，允许LLM在代码沙盒（虚拟计算机）中探索而无需额外训练。该方法利用沙盒执行访问外部资源、通过文件系统处理长上下文和运行脚本等任务，并通过基于非智能数据训练的LLM-in-Sandbox强化学习（LLM-in-Sandbox-RL）进一步增强能力。实验结果表明，该方法在训练无关和训练后设置下，在数学、物理、化学、生物医学、长上下文理解和指令遵循等方面均表现出强大的泛化能力，并从计算和系统角度分析了效率，同时作为Python包开源以促进实际部署。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-01-22T18:24:00+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v1">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题探索新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时实施强化学习，使大型语言模型能够持续训练，并针对测试问题积累特定经验。这种持续学习形式具有特殊性：其目标是产生单一卓越解决方案而非平均意义上的多个良好方案，且专注于解决当前问题而非泛化至其他问题。因此，我们设计了优先探索最有潜力解决方案的学习目标与搜索子程序，并将该方法命名为“测试时训练发现法”。延续先前研究，我们聚焦于连续奖励问题，并在数学、GPU内核工程、算法设计和生物学领域报告了所有尝试问题的结果：1）埃尔德什最小重叠问题与自相关不等式；2）GPUMode内核竞赛（较现有技术提速达2倍）；3）往届AtCoder算法竞赛；4）单细胞分析中的去噪问题。所有方案均经专家或主办方评审。与先前依赖封闭前沿模型的最佳成果不同，本研究全部结果均基于开源模型OpenAI gpt-oss-120b实现，并可通过公开代码复现。测试时训练通过Thinking Machines的Tinker API执行，每个问题仅需数百美元成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the goal of using AI to achieve new state-of-the-art solutions for scientific problems, this paper introduces Test-Time Training to Discover (TTT-Discover), a method that performs reinforcement learning at test time to allow a large language model (LLM) to continually train on experience specific to a given problem, prioritizing the most promising solutions. The approach focuses on problems with continuous rewards and is designed to produce one great solution rather than average performance across many. Experimental results demonstrate that TTT-Discover sets new state-of-the-art benchmarks across multiple domains: it advances solutions in Erdős&#x27; minimum overlap problem and an autocorrelation inequality in mathematics, achieves up to 2x speedup in a GPU kernel engineering competition, excels in past AtCoder algorithm contests, and improves denoising in single-cell biology analysis, with all solutions verified by experts. Notably, these results are achieved using an open model (OpenAI gpt-oss-120b) and publicly available code, contrasting with prior work that relied on closed frontier models, with test-time training costs kept to a few hundred dollars per problem.</div>
<div class="mono" style="margin-top:8px">本文旨在利用人工智能为科学问题实现新的最先进解决方案，提出了测试时训练发现方法（TTT-Discover），该方法在测试时通过强化学习使大型语言模型能够针对特定问题持续训练，并优先探索最有希望的解决方案。该方法专注于具有连续奖励的问题，旨在产生单个优秀解而非平均性能。实验结果表明，TTT-Discover在多个领域设立了新的最先进基准：在数学中改进了埃尔德什最小重叠问题和自相关不等式，在GPU内核工程竞赛中实现了比先前技术快2倍的速度，在过去的AtCoder算法竞赛中表现出色，并在单细胞生物学分析中提升了去噪性能，所有解决方案均经专家审核。值得注意的是，这些成果使用开源模型（OpenAI gpt-oss-120b）和公开代码实现，与依赖封闭前沿模型的先前工作形成对比，且每个问题的测试时训练成本仅需数百美元。</div>
</details>
</div>
<div class="card">
<div class="title">Structured Hints for Sample-Efficient Lean Theorem Proving</div>
<div class="meta-line">Authors: Zachary Burton</div>
<div class="meta-line">First: 2026-01-22T18:16:46+00:00 · Latest: 2026-01-22T18:16:46+00:00</div>
<div class="meta-line">Comments: 9 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构化提示在样本高效Lean定理证明中的应用</div>
<div class="mono" style="margin-top:8px">当前最先进的神经定理证明器（如DeepSeek-Prover-V1.5）将大语言模型与强化学习相结合，通过复杂训练取得了显著成果。我们提出：这些经过高度训练的模型在推理阶段是否仍能从简单的结构化引导中获益？我们在miniF2F基准测试中评估了一种轻量级干预方法——基于15种常见策略骨架的固定提示调度方案。这种简单方法在相同样本数（k=16）和相同最大生成长度（1024个标记）条件下，实现了21.7%的pass@16，而标准采样方法仅为15.2%，相对提升达43%。结果表明，即使经过RL训练的成熟证明器也未能充分利用策略语言中可用的结构化先验，而简单的推理阶段引导仍能提供低成本、互补性的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether state-of-the-art neural theorem provers, which are already highly trained via reinforcement learning, can benefit from simple structural guidance during inference. The method introduces a lightweight intervention using a fixed prompt schedule over 15 common tactic skeletons to guide the model. Experimental results on the miniF2F benchmark show that this approach increases the pass rate from 15.2% to 21.7% with 16 samples, representing a 43% relative improvement, indicating that inference-time guidance provides a cheap and complementary performance boost.</div>
<div class="mono" style="margin-top:8px">本文研究了经过强化学习高度训练的先进神经定理证明器是否能在推理时受益于简单的结构化引导。方法上引入了一种轻量级干预，即使用一个基于15种常见策略骨架的固定提示调度来引导模型。在miniF2F基准测试上的实验结果表明，该方法将16个样本下的通过率从15.2%提升至21.7%，相对改进达43%，这表明推理时的引导是一种廉价且互补的性能提升手段。</div>
</details>
</div>
<div class="card">
<div class="title">ViSymRe: Vision-guided Multimodal Symbolic Regression</div>
<div class="meta-line">Authors: Da Li, Junping Yin, Jin Xu, Xinxin Li, Juan Zhang</div>
<div class="meta-line">First: 2024-12-15T10:05:31+00:00 · Latest: 2026-01-22T17:29:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.11139v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.11139v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extracting simple mathematical expression from an observational dataset to describe complex natural phenomena is one of the core objectives of artificial intelligence (AI). This field is known as symbolic regression (SR). Traditional SR models are based on genetic programming (GP) or reinforcement learning (RL), facing well-known challenges, such as low efficiency and overfitting. Recent studies have integrated SR with large language models (LLMs), enabling fast zero-shot inference by learning mappings from millions of dataset-expression pairs. However, since the input and output are inherently different modalities, such models often struggle to converge effectively. In this paper, we introduce ViSymRe, a vision-guided multimodal SR model that incorporates the third resource, expression graph, to bridge the modality gap. Different from traditional multimodal models, ViSymRe is trained to extract vision, termed virtual vision, from datasets, without relying on the global availability of expression graphs, which addresses the essential challenge of visual SR, i.e., expression graphs are not available during inference. Evaluation results on multiple mainstream benchmarks show that ViSymRe achieves more competitive performance than the state-of-the-art dataset-only baselines. The expressions predicted by ViSymRe not only fit the dataset well but are also simple and structurally accurate, goals that SR models strive to achieve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSymRe：视觉引导的多模态符号回归</div>
<div class="mono" style="margin-top:8px">从观测数据集中提取简洁的数学表达式以描述复杂的自然现象，是人工智能（AI）的核心目标之一，这一领域被称为符号回归（SR）。传统SR模型基于遗传编程（GP）或强化学习（RL），面临效率低下和过拟合等常见挑战。近期研究将SR与大型语言模型（LLMs）结合，通过从数百万数据集-表达式对中学习映射关系，实现了快速的零样本推理。然而，由于输入与输出本质属于不同模态，此类模型常难以有效收敛。本文提出ViSymRe，一种视觉引导的多模态SR模型，通过引入第三种资源——表达式图——来弥合模态差异。与传统多模态模型不同，ViSymRe训练从数据集中提取视觉信息（称为虚拟视觉），无需依赖表达式图的全局可用性，从而解决了视觉SR的关键挑战：推理过程中表达式图不可得。在多个主流基准测试上的评估结果表明，ViSymRe相比当前最先进的仅使用数据集的基线模型表现出更具竞争力的性能。ViSymRe预测的表达式不仅与数据集高度吻合，而且结构简洁准确，这正是SR模型追求的目标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ViSymRe, a vision-guided multimodal symbolic regression model motivated by the need to overcome the modality gap and inefficiencies in traditional genetic programming or reinforcement learning-based approaches, as well as the convergence struggles of recent large language model integrations. The method uniquely incorporates a third resource, expression graphs, to bridge the modality gap by training the model to extract &quot;virtual vision&quot; from datasets without relying on globally available expression graphs during inference, thus addressing the core challenge of visual symbolic regression. Experimental results on multiple mainstream benchmarks demonstrate that ViSymRe outperforms state-of-the-art dataset-only baselines, producing expressions that are not only well-fitting but also simple and structurally accurate, aligning with key goals of symbolic regression.</div>
<div class="mono" style="margin-top:8px">本文提出了ViSymRe，一种视觉引导的多模态符号回归模型，其动机在于克服传统基于遗传编程或强化学习方法效率低下和过拟合的问题，以及近期大语言模型集成中因输入输出模态不同而难以有效收敛的挑战。该方法创新性地引入第三种资源——表达式图，通过训练模型从数据集中提取“虚拟视觉”来弥合模态差距，且无需在推理时依赖全局可用的表达式图，从而解决了视觉符号回归的核心难题。在多个主流基准测试上的评估结果表明，ViSymRe的性能优于当前最先进的仅基于数据集的基线方法，其预测的表达式不仅与数据拟合良好，而且结构简单准确，实现了符号回归模型追求的目标。</div>
</details>
</div>
<div class="card">
<div class="title">GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning</div>
<div class="meta-line">Authors: Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya Wang, Jun Wang, Ye Shi</div>
<div class="meta-line">First: 2025-05-24T15:57:07+00:00 · Latest: 2026-01-22T17:10:05+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18763v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.18763v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO&#x27;s superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenPO：生成扩散模型与在线策略强化学习的融合</div>
<div class="mono" style="margin-top:8px">强化学习领域的最新进展揭示了基于生成扩散的策略在探索能力与多模态特性方面的优势。尽管离线强化学习和离线策略强化学习已取得显著进步，但将扩散策略整合至PPO等在线策略框架的研究仍显不足。这一空白在IsaacLab等专为在线策略算法优化的大规模并行GPU加速模拟器广泛应用的背景下尤为突出，此类模拟器能高效训练复杂机器人任务。核心挑战在于计算扩散策略下的状态-动作对数似然：高斯策略可直接计算，而基于流的模型因不可逆的前向-反向过程及离散化误差（如欧拉-丸山近似）导致计算困难。为填补这一空白，我们提出GenPO——一种利用精确扩散反演构建可逆动作映射的生成式策略优化框架。GenPO通过创新的双重虚拟动作机制，借助交替更新实现可逆性，突破了对数似然计算障碍。此外，我们利用动作对数似然进行无偏熵与KL散度估计，实现在线策略更新中的KL自适应学习率与熵正则化。在IsaacLab的八项基准测试（包括足式运动、灵巧操作、空中控制与机械臂任务）中，大量实验证明了GenPO优于现有强化学习基线。值得注意的是，GenPO是首个成功将扩散策略整合至在线策略强化学习的方法，为大规模并行化训练与真实机器人部署开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to integrate generative diffusion models into on-policy reinforcement learning frameworks like PPO, which are widely used with large-scale parallel simulators for robotic training, but where computing exact state-action log-likelihoods for diffusion policies has been a key challenge due to their irreversible forward-reverse processes. The proposed method, GenPO, introduces a generative policy optimization framework that uses exact diffusion inversion to create invertible action mappings via a novel doubled dummy action mechanism, enabling tractable log-likelihood computation and allowing for unbiased entropy and KL divergence estimation to support KL-adaptive learning and entropy regularization. Experimental results across eight IsaacLab benchmarks, including legged locomotion, dexterous manipulation, aerial control, and robotic arm tasks, demonstrate that GenPO outperforms existing RL baselines and is the first successful integration of diffusion policies into on-policy RL, facilitating large-scale parallelized training for robotics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是将生成扩散模型整合到如PPO这样的同策略强化学习框架中，这类框架广泛用于机器人训练的大规模并行模拟器，但扩散策略因其不可逆的前向-反向过程，其精确的状态-动作对数似然计算一直是个关键挑战。所提出的方法GenPO引入了一个生成策略优化框架，通过精确的扩散反演和一种新颖的双重虚拟动作机制来创建可逆的动作映射，从而实现了可处理的似然计算，并支持无偏的熵和KL散度估计，以用于KL自适应学习和熵正则化。在八个IsaacLab基准测试（包括足式运动、灵巧操作、空中控制和机械臂任务）上的实验结果表明，GenPO优于现有的强化学习基线，并首次成功将扩散策略整合到同策略强化学习中，为机器人领域的大规模并行化训练提供了可能。</div>
</details>
</div>
<div class="card">
<div class="title">GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Zhengqing Yan, Xinyang Liu, Yi Zhang, Fan Guo, ChengXun Jia, Junchen Wan, Yao Liu, Qi Liu, Jihao Huang, Kang Song</div>
<div class="meta-line">First: 2026-01-11T07:34:41+00:00 · Latest: 2026-01-22T14:58:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06795v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.06795v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDEPO：面向样本受限强化学习的增强训练数据利用率的群组双动态等权优势策略优化方法</div>
<div class="mono" style="margin-top:8px">自动定理证明（ATP）是人工智能（AI）领域的核心挑战，需在Lean等形式化语言中构建机器可验证的证明以评估AI推理能力。强化学习（RL），尤其是高性能的群组相对策略优化（GRPO）算法，已成为该任务的主流方法。然而在ATP场景中，GRPO面临两个关键问题：使用复合奖励时，其相对优势估计可能与形式验证器的二元反馈产生冲突；同时，其静态采样策略若未找到有效证明会丢弃整批数据，导致模型更新零贡献和严重数据浪费。为解决这些局限，我们提出群组双动态等权优势策略优化（GDEPO），该方法包含三个核心机制：1）动态补充采样：对无效批次重新采样直至发现有效证明；2）等权优势：将优势函数的符号（基于正确性）与其幅度（由辅助奖励调节）解耦，确保策略更新的稳定性和正确性；3）动态补充迭代：对初始失败但最终成功的样本施加额外梯度步，加速困难案例的学习。在三个不同难度数据集（MinF2F-test、MathOlympiadBench、PutnamBench）上的实验验证了GDEPO的有效性，消融研究证实了其协同组件的必要性。该方法提升了数据利用率和优化效率，为ATP提供了新颖的训练范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing reinforcement learning methods like GRPO in Automated Theorem Proving (ATP), where static sampling wastes data and composite rewards conflict with verifier feedback, this paper introduces GDEPO. The method employs dynamic additional sampling to reuse failed batches, an equal-right advantage mechanism to decouple reward signals for stable policy updates, and dynamic additional iterations to focus learning on hard cases. Experimental results on datasets including MinF2F-test, MathOlympiadBench, and PutnamBench demonstrate GDEPO&#x27;s effectiveness in improving data utilization and optimization efficiency, with ablation studies confirming the contribution of each component.</div>
<div class="mono" style="margin-top:8px">针对自动定理证明中现有强化学习方法如GRPO存在的静态采样导致数据浪费、复合奖励与验证器反馈冲突等问题，本文提出了GDEPO方法。该方法通过动态附加采样重用无效批次数据，采用平等权利优势机制分离奖励信号以实现稳定策略更新，并利用动态附加迭代加强对困难案例的学习。在MinF2F-test、MathOlympiadBench和PutnamBench等不同难度数据集上的实验结果表明，GDEPO有效提升了数据利用率和优化效率，消融研究验证了各协同组件的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Representation-Driven Reinforcement Learning</div>
<div class="meta-line">Authors: Ofir Nabati, Guy Tennenholtz, Shie Mannor</div>
<div class="meta-line">Venue: ICML 2023</div>
<div class="meta-line">First: 2023-05-31T14:59:12+00:00 · Latest: 2026-01-22T14:39:55+00:00</div>
<div class="meta-line">Comments: Accepted to ICML 2023</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2305.19922v3">Abs</a> · <a href="https://arxiv.org/pdf/2305.19922v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>表征驱动的强化学习</div>
<div class="mono" style="margin-top:8px">我们提出了一种表征驱动的强化学习框架。通过将策略表示为其期望值的估计，我们利用上下文赌博机技术来指导探索与利用。具体而言，将策略网络嵌入线性特征空间，使我们能够将探索-利用问题重新定义为表征-利用问题，其中良好的策略表征能够实现最优探索。我们通过将该框架应用于进化和基于策略梯度的方法，证明了其有效性，与传统方法相比性能显著提升。该框架为强化学习提供了新视角，强调了策略表征在确定最优探索-利用策略中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better balance exploration and exploitation in reinforcement learning, this paper introduces a representation-driven framework that reinterprets policies as estimates of their expected values, applying contextual bandit techniques to guide decision-making. The method embeds policy networks into a linear feature space, reformulating the classic exploration-exploitation dilemma as a representation-exploitation problem where effective policy representations facilitate optimal exploration. Experimental results applying this framework to both evolutionary and policy gradient-based methods show significantly improved performance over traditional approaches, underscoring the critical role of policy representation in determining efficient learning strategies.</div>
<div class="mono" style="margin-top:8px">本文的动机是为了改进强化学习中探索与利用的平衡，提出了一个表征驱动的框架，将策略重新定义为对其期望值的估计，并利用上下文赌博机技术来指导决策。该方法将策略网络嵌入线性特征空间，从而将经典的探索-利用问题重构为表征-利用问题，其中有效的策略表征能够促进最优探索。实验结果表明，该框架应用于进化和基于策略梯度的方法时，相比传统方法性能显著提升，强调了策略表征在决定高效学习策略中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour</div>
<div class="meta-line">Authors: Liang Wang, Kanzhong Yao, Yang Liu, Weikai Qin, Jun Wu, Zhe Sun, Qiuguo Zhu</div>
<div class="meta-line">First: 2026-01-22T14:16:12+00:00 · Latest: 2026-01-22T14:16:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15995v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15995v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot&#x27;s real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA&#x27;s exceptional agility and robustness in challenging scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PUMA：面向移动增强型四足跑酷的感知驱动统一落脚点先验</div>
<div class="mono" style="margin-top:8px">四足机器人的跑酷任务已成为敏捷运动能力的重要测试基准。人类运动员能通过感知环境特征选择合适落脚点以跨越障碍，但赋予腿式机器人同等感知推理能力仍面临巨大挑战。现有方法多依赖遵循预计算落脚点的分层控制器，限制了机器人的实时适应性与强化学习的探索潜力。为突破这些局限，我们提出PUMA——一种将视觉感知与落脚点先验整合至单阶段训练过程的端到端学习框架。该方法利用地形特征估计以自我为中心的极坐标落脚点先验（包含相对距离与航向角），引导机器人在跑酷任务中主动调整姿态。通过在仿真与真实环境中对多种离散复杂地形进行大量实验，验证了PUMA在挑战性场景中卓越的敏捷性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable quadruped robots to perform agile parkour by autonomously selecting footholds based on environmental perception, addressing the limitations of hierarchical controllers that rely on pre-computed footholds and hinder real-time adaptability. The method introduces PUMA, an end-to-end learning framework that integrates visual perception with egocentric polar foothold priors—encoding relative distance and heading—into a single-stage training process to guide active posture adaptation. Experimental results from simulation and real-world tests across various discrete complex terrains demonstrate that PUMA achieves exceptional agility and robustness in challenging parkour scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是让四足机器人能够基于环境感知自主选择落脚点以执行敏捷跑酷，克服了依赖预计算落脚点的分层控制器在实时适应性和探索潜力上的局限。方法上提出了PUMA，这是一个端到端学习框架，将视觉感知与以自我为中心的极坐标落脚点先验（包含相对距离和朝向）整合到单阶段训练过程中，以引导机器人进行主动姿态调整。在多种离散复杂地形的仿真和真实环境实验中，结果表明PUMA在挑战性跑酷场景中展现出卓越的敏捷性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Return-to-Go for Efficient Decision Transformer</div>
<div class="meta-line">Authors: Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li, Qirui Zheng, Xionghui Yang, Wenxin Li</div>
<div class="meta-line">First: 2026-01-22T13:42:08+00:00 · Latest: 2026-01-22T13:42:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15953v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT&#x27;s performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦回报到目标以实现高效决策Transformer</div>
<div class="mono" style="margin-top:8px">决策Transformer（DT）为离线强化学习建立了一种强大的序列建模方法，其动作预测以回报到目标（RTG）为条件，既用于训练中区分轨迹质量，也用于推理时指导动作生成。本研究揭示了该设计中的一个关键冗余：将整个RTG序列输入Transformer在理论上是不必要的，因为只有最近的RTG会影响动作预测。我们通过实验证明这种冗余可能损害DT的性能。为此，我们提出解耦DT（DDT）。DDT通过仅将观测和动作序列输入Transformer处理，并使用最新的RTG指导动作预测，简化了架构。这种精简方法不仅提升了性能，还降低了计算成本。实验表明，DDT在多个离线RL任务中显著优于DT，并与最先进的DT变体相比展现出有竞争力的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper identifies a redundancy in the Decision Transformer (DT), where feeding the entire sequence of Return-to-Go (RTG) values into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction, and shows this can impair performance. To resolve this, the authors propose the Decoupled DT (DDT), which simplifies the architecture by processing only observation and action sequences through the Transformer while using the latest RTG to guide action prediction. Experimental results demonstrate that DDT not only improves performance over the original DT but also reduces computational cost and achieves competitive results against state-of-the-art DT variants across multiple offline reinforcement learning tasks.</div>
<div class="mono" style="margin-top:8px">本文指出决策变换器（DT）中存在冗余设计，即将整个回报目标（RTG）序列输入变换器在理论上是不必要的，因为只有最近的RTG影响动作预测，且实验表明这会损害性能。为解决此问题，作者提出了解耦决策变换器（DDT），它简化了架构，仅通过变换器处理观测和动作序列，同时使用最新的RTG来指导动作预测。实验结果表明，DDT不仅性能优于原始DT、降低了计算成本，而且在多个离线强化学习任务中与先进的DT变体相比取得了有竞争力的结果。</div>
</details>
</div>
<div class="card">
<div class="title">VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing</div>
<div class="meta-line">Authors: Guanyuan Pan, Shuai Wang, Yugui Lin, Tiansheng Zhou, Pietro Liò, Yaqi Wang, Zhenxin Zhao</div>
<div class="meta-line">First: 2026-01-12T08:37:32+00:00 · Latest: 2026-01-22T11:46:08+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures, submitted to the 10th International Conference on Control, Automation and Diagnosis (ICCAD&#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07315v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07315v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches rely solely on netlists, ignoring the circuit schematic, which hinders the cognitive link between the schematic and its performance. Furthermore, the black-box nature of machine learning methods and hallucination risks in large language models fail to provide the necessary ground-truth explainability required for industrial sign-off. To address these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-start from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance while maintaining physics-based explainability. VLM-CAD meets all specification requirements while maintaining low power consumption in optimizing an amplifier with a complementary input and a class-AB output stage, with a total runtime under 66 minutes across all experiments on two amplifiers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLM-CAD：面向模拟电路尺寸优化的视觉语言模型协同智能体设计流程</div>
<div class="mono" style="margin-top:8px">模拟混合信号电路尺寸优化涉及高维设计空间中的复杂权衡。现有自动优化方法仅依赖网表而忽略电路原理图，阻碍了原理图与性能间的认知关联。机器学习方法的黑箱特性及大语言模型的幻觉风险，均无法满足工业签核所需的可解释性要求。为此，我们提出视觉语言模型优化的协同智能体设计流程（VLM-CAD），通过电路分析、直流工作点优化、基于推理的尺寸确定及外部尺寸优化四个阶段，集成Image2Net工具标注原理图并生成结构化JSON描述供视觉语言模型精准解析。进一步提出可解释信任域贝叶斯优化方法（ExTuRBO），采用智能体生成种子的协同热启动策略，提供双粒度灵敏度分析支持外部尺寸优化，并生成完整设计报告。基于180nm、90nm和45nm预测技术模型的放大器尺寸优化实验表明，VLM-CAD在保持物理可解释性的同时有效平衡功耗与性能。在优化具有互补输入级和AB类输出级的放大器时，VLM-CAD满足全部指标要求且保持低功耗，两个放大器的全实验总运行时间低于66分钟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing automatic analog circuit sizing methods, which rely solely on netlists and lack explainability, by introducing VLM-CAD, a Vision Language Model-optimized collaborative agent workflow. The method integrates Image2Net to annotate circuit schematics into structured JSON for VLM interpretation and employs ExTuRBO for explainable Bayesian optimization with sensitivity analysis. Experimental results on amplifier sizing across 180nm, 90nm, and 45nm technology nodes show that VLM-CAD effectively balances power and performance while maintaining physics-based explainability, meeting all specifications with low power consumption and a total runtime under 66 minutes for two amplifiers.</div>
<div class="mono" style="margin-top:8px">本文针对现有自动模拟电路尺寸设计方法仅依赖网表且缺乏可解释性的问题，提出了VLM-CAD，一种视觉语言模型优化的协同智能体工作流。该方法集成Image2Net将电路原理图标注为结构化JSON供VLM解析，并采用ExTuRBO进行可解释的贝叶斯优化与灵敏度分析。在180nm、90nm和45nm工艺节点上的放大器尺寸设计实验表明，VLM-CAD能有效平衡功耗与性能，同时保持基于物理的可解释性，满足所有设计指标且功耗较低，两个放大器的总运行时间在66分钟以内。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling for Strategic Multiagent Settings</div>
<div class="meta-line">Authors: Georgios Chalkiadakis, Charilaos Akasiadis, Gerasimos Koresis, Stergios Plataniotis, Leonidas Bakopoulos</div>
<div class="meta-line">First: 2025-11-13T17:06:56+00:00 · Latest: 2026-01-22T10:36:51+00:00</div>
<div class="meta-line">Comments: 28 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10501v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.10501v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper provides a comprehensive review of mainly GNN, DRL, and PTM methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) ML methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of GNN. Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of RL, and in particular that of multiagent deep reinforcement learning. Single-agent deep RL has been widely used for decision making in demanding game settings. Its application in multiagent settings though is hindered due to, e.g., varying relationships between agents, and non-stationarity of the environment. We describe existing relevant game theoretic solution concepts, and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes probabilistic topic modeling (PTM) in domains other than that of document analysis and classification. Finally, we identify certain open challenges -- specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向战略多智能体场景的图神经网络、深度强化学习与概率主题建模</div>
<div class="mono" style="margin-top:8px">本文系统综述了图神经网络（GNN）、深度强化学习（DRL）与概率主题建模（PTM）方法在战略多智能体场景中的潜在融合应用。重点关注：（1）适用于战略对手建模任务、能揭示未知模型结构的机器学习方法；（2）这些方法与博弈论概念的结合，以规避现实场景中常失效的假设（如共同先验假设与自利假设）。我们分析了方法处理不确定性与异质性（现实应用中的常见特征）的能力及可扩展性。针对多智能体场景中关系与交互的有效建模，我们倡导采用GNN——该方法专为图结构数据设计，在节点分类与链接预测等任务中已展现强大能力。继而综述强化学习领域，特别是多智能体深度强化学习。单智能体深度强化学习已在复杂博弈决策中广泛应用，但其在多智能体场景的应用受限于智能体间动态关系与环境非平稳性等因素。我们阐述了现有相关博弈论解概念，并考量公平性、稳定性等特性。综述还涵盖概率主题建模在文档分析分类以外领域的应用文献。最后提出若干开放挑战：需（1）适应非平稳环境；（2）平衡稳定性与适应性；（3）应对不确定性与异质性；（4）保障可扩展性与解的可处理性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This review paper is motivated by the need to model strategic multiagent interactions in complex, real-world settings where traditional game theory assumptions like the Common Prior Assumption and Self-Interest Hypothesis often fail. The method involves a comprehensive analysis of three machine learning approaches: Graph Neural Networks (GNNs) are championed for modeling agent relationships on graph-structured data; Deep Reinforcement Learning (DRL), especially multiagent variants, is reviewed for decision-making despite challenges like non-stationarity; and Probabilistic Topic Modeling (PTM) is considered for applications beyond text. The main experimental and analytical results highlight these methods&#x27; potential to handle key characteristics such as uncertainty, heterogeneity, and scalability, while also identifying open challenges including adapting to non-stationary environments and balancing stability with adaptation.</div>
<div class="mono" style="margin-top:8px">本文旨在为复杂现实世界中的战略多智能体交互建模，因为传统博弈论中的共同先验假设和自利假设等前提常不成立。方法上，论文全面综述了三种机器学习途径：倡导使用图神经网络（GNN）在图结构数据上建模智能体关系；回顾了深度强化学习（DRL），特别是多智能体版本，以应对环境非平稳性等决策挑战；并探讨了概率主题模型（PTM）在文本分析以外的应用。主要分析与综述结果表明，这些方法在应对不确定性、异质性和可扩展性等关键特性上具有潜力，同时指出了开放挑战，如适应非平稳环境、平衡稳定性与适应性等。</div>
</details>
</div>
<div class="card">
<div class="title">PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling</div>
<div class="meta-line">Authors: Ai Jian, Jingqing Ruan, Xing Ma, Dailin Li, Weipeng Zhang, Ke Zeng, Xunliang Cai</div>
<div class="meta-line">First: 2025-10-28T09:43:47+00:00 · Latest: 2026-01-22T10:20:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24235v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24235v2">PDF</a> · <a href="https://github.com/JaneEyre0530/PaTaRM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are central to reinforcement learning from human feedback (RLHF), providing the critical supervision signals that align large language models (LLMs) with human preferences. Generative reward models (GRMs) provide greater interpretability than traditional scalar RMs, but they come with a critical trade-off: pairwise methods are hindered by a training-inference mismatch, while pointwise methods require expensive absolute annotations. To bridge this gap, we propose the Preference-aware Task-adaptive Reward Model (PaTaRM). Unlike prior approaches, PaTaRM enables robust pointwise training using readily available pairwise data via a novel Preference-Aware Reward (PAR) mechanism, eliminating the need for explicit rating labels. Furthermore, it incorporates a Task-Adaptive Rubric system that dynamically generates instance-specific criteria for precise evaluation. Extensive experiments demonstrate that PATRM achieves a 8.7% average improvement on RewardBench and RMBench across Qwen3-8B/14B models. Crucially, it boosts downstream RLHF performance by an average relative improvement of 13.6% across IFEval and InFoBench, validating its effectiveness for policy alignment. Our code is available at https://github.com/JaneEyre0530/PaTaRM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PaTaRM：通过偏好感知任务自适应奖励建模桥接成对与点对信号</div>
<div class="mono" style="margin-top:8px">奖励模型（RMs）是基于人类反馈的强化学习（RLHF）的核心，为对齐大语言模型（LLMs）与人类偏好提供关键监督信号。生成式奖励模型（GRMs）比传统标量RMs具有更强的可解释性，但面临关键权衡：成对方法受训推不匹配制约，而点对方法需要昂贵的绝对标注。为弥合此差距，我们提出偏好感知任务自适应奖励模型（PaTaRM）。与先前方法不同，PaTaRM通过新颖的偏好感知奖励（PAR）机制，利用现成的成对数据实现鲁棒的点对训练，无需显式评分标注。此外，其任务自适应准则系统能动态生成实例专属标准以进行精确评估。大量实验表明，PaTaRM在Qwen3-8B/14B模型上的RewardBench和RMBench平均提升8.7%。关键的是，其在IFEval和InFoBench的下游RLHF性能平均相对提升13.6%，验证了其对策略对齐的有效性。代码发布于https://github.com/JaneEyre0530/PaTaRM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a key trade-off in generative reward models (GRMs) for RLHF: pairwise methods suffer from training-inference mismatch, while pointwise methods require costly absolute human annotations. To bridge this gap, the authors propose PaTaRM, a method that enables robust pointwise training using only pairwise data via a novel Preference-Aware Reward mechanism and incorporates a Task-Adaptive Rubric system for instance-specific evaluation. Experimental results show PaTaRM achieves an average 8.7% improvement on RewardBench and RMBench with Qwen3 models and boosts downstream RLHF performance by 13.6% on IFEval and InFoBench, validating its effectiveness for policy alignment.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习人类反馈中生成式奖励模型的关键权衡问题：成对方法存在训练-推断不匹配，而逐点方法需要昂贵的绝对标注。为弥补这一差距，作者提出了PaTaRM，该方法通过新颖的偏好感知奖励机制，仅使用成对数据实现稳健的逐点训练，并引入任务自适应准则系统进行实例化评估。实验结果表明，PaTaRM在Qwen3模型上于RewardBench和RMBench平均提升8.7%，并在IFEval和InFoBench的下游RLHF性能中平均相对提升13.6%，验证了其对策略对齐的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking</div>
<div class="meta-line">Authors: Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang, Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang, Ailin Ren, Xin Li, Pengjun Xie, Jiawei Liu, Ning Guo, Jingren Zhou, Zheng-Jun Zha</div>
<div class="meta-line">First: 2026-01-10T08:43:07+00:00 · Latest: 2026-01-22T09:16:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06487v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06487v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ArenaRL：基于锦标赛相对排序的开放式智能体强化学习规模化方法</div>
<div class="mono" style="margin-top:8px">强化学习显著提升了LLM智能体在可验证结果任务上的表现，但在解决方案空间广阔的开放式智能体任务（如复杂旅行规划）中仍面临挑战。由于此类任务缺乏客观真值，现有RL算法主要依赖为单条响应分配标量分数的奖励模型。我们认为这种逐点评分存在固有的判别力坍缩问题：奖励模型难以区分不同轨迹间的细微优势，导致组内分数被压缩至狭窄区间。有效奖励信号因此被奖励模型的噪声主导，引发优化停滞。为此，我们提出ArenaRL强化学习范式，将逐点标量评分转变为组内相对排序。ArenaRL引入过程感知的成对评估机制，采用多级量规为轨迹分配细粒度相对分数。同时构建组内对抗竞技场，设计基于锦标赛的排序方案以获取稳定的优势信号。实证结果表明，所构建的种子单败淘汰制在仅需O(N)复杂度的情况下，其优势估计精度与O(N²)复杂度的全成对比较近乎等价，实现了效率与精度的最优平衡。此外，针对开放式智能体缺乏全周期基准的问题，我们构建了Open-Travel与Open-DeepResearch两个高质量基准，涵盖SFT、RL训练和多维评估的完整流程。大量实验表明，ArenaRL显著优于标准RL基线，使LLM智能体能为复杂现实任务生成更稳健的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying reinforcement learning to open-ended agent tasks, such as complex travel planning, where vast solution spaces and the absence of objective ground-truth cause conventional pointwise reward models to suffer from discrimination collapse, compressing scores and stalling optimization. To overcome this, the authors propose ArenaRL, a paradigm that shifts from scalar scoring to intra-group relative ranking via a process-aware pairwise evaluation mechanism with multi-level rubrics and a tournament-based ranking scheme in an adversarial arena, achieving O(N) complexity with accuracy comparable to O(N^2) full comparisons. Experimental results on newly built benchmarks, Open-Travel and Open-DeepResearch, demonstrate that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to produce more robust solutions for complex real-world tasks.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在开放端智能体任务（如复杂旅行规划）中的应用挑战，其中庞大的解空间和缺乏客观真值导致传统点式奖励模型出现判别崩溃，使分数压缩并阻碍优化。为解决此问题，作者提出ArenaRL，该范式通过基于过程感知的成对评估机制（采用多级评分标准）和在对抗性竞技场中的锦标赛式排名方案，从标量评分转向组内相对排名，以O(N)复杂度实现了与O(N^2)全比较相当的精度。在新构建的基准测试Open-Travel和Open-DeepResearch上的实验结果表明，ArenaRL显著优于标准强化学习基线，使LLM智能体能为复杂现实任务生成更稳健的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-22T09:09:47+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗性输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但这些模型仍易受对抗性越狱攻击影响——攻击者通过精心设计的细微扰动绕过安全机制，触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等问题，难以应用于现实黑盒场景。为此，我们提出基于同步扰动随机逼近零阶优化（ZO-SPSA）的黑盒越狱攻击方法，其具备三大优势：（1）通过输入输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）较低的资源需求与GPU内存消耗。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，且扰动不可感知性与白盒方法相当。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱的现实可行性，并暴露了当前LVLMs安全机制的关键弱点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods that require full model access, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free, model-agnostic approach approximates gradients through input-output interactions without needing internal model knowledge, thereby reducing computational and memory costs. Experimental results on models like InstructBLIP, LLaVA, and MiniGPT-4 demonstrate high jailbreak success rates, up to 83.0% on InstructBLIP, with imperceptible perturbations, and show strong transferability of adversarial examples across models, with attack success rates reaching 64.18%, highlighting significant safety weaknesses in LVLMs.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型（LVLMs）易受对抗性越狱攻击的脆弱性，以及现有需要完整模型访问权限的白盒方法不切实际的问题，本文提出了一种使用基于同时扰动随机逼近的零阶优化（ZO-SPSA）的黑盒攻击方法。这种无需梯度、与模型无关的方法通过输入-输出交互来近似梯度，无需了解模型内部知识，从而降低了计算和内存成本。在InstructBLIP、LLaVA和MiniGPT-4等模型上的实验结果表明，该方法实现了较高的越狱成功率（在InstructBLIP上最高达83.0%），且扰动难以察觉，同时生成的对抗样本在模型间表现出强可迁移性，攻击成功率可达64.18%，揭示了LVLMs安全机制的重大缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</div>
<div class="meta-line">Authors: Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T17:59:44+00:00 · Latest: 2026-01-22T08:52:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025 Track on Datasets and Benchmarks. Project page: https://faceong.github.io/VIKI-R/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09049v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09049v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://faceong.github.io/VIKI-R/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIKI-R：基于强化学习的具身多智能体协同控制框架</div>
<div class="mono" style="margin-top:8px">在动态环境中协调多个具身智能体仍是人工智能的核心挑战，需同时具备感知驱动推理与可扩展的协作策略。现有研究虽已利用大语言模型进行多智能体规划，并开始探索视觉语言模型在视觉推理中的应用，但这些基于视觉语言模型的方法对多样化具身形态的支持仍显不足。本研究提出首个面向具身多智能体协作的层次化基准测试平台VIKI-Bench，其包含智能体激活、任务规划与轨迹感知三个结构化层级，涵盖多类机器人具身形态、多视角视觉观测及结构化监督信号，以评估基于视觉输入的推理能力。为验证该平台效能，我们提出两阶段框架VIKI-R：首先利用思维链标注样本微调预训练视觉语言模型，随后在多层级奖励信号下进行强化学习。大量实验表明，VIKI-R在所有任务层级上均显著优于基线方法。进一步研究发现，强化学习能促使异构智能体间涌现组合式协作模式。VIKI-Bench与VIKI-R共同为推进具身人工智能系统中的多智能体视觉驱动协作提供了统一测试平台与方法体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of coordinating multiple embodied agents in dynamic environments, where existing vision-language model (VLM) approaches often lack support for diverse robot embodiments. The authors introduce VIKI-Bench, a hierarchical benchmark for embodied multi-agent cooperation with structured levels for agent activation, task planning, and trajectory perception, and propose VIKI-R, a two-stage method that fine-tunes a pretrained VLM using Chain-of-Thought demonstrations and then applies reinforcement learning with multi-level rewards. Experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all task levels and enables the emergence of compositional cooperation patterns among heterogeneous agents.</div>
<div class="mono" style="margin-top:8px">本文针对动态环境中多具身智能体协调的挑战，现有视觉语言模型方法通常缺乏对多样化机器人具身的支持。作者提出了VIKI-Bench，这是一个用于具身多智能体协作的分层基准，包含智能体激活、任务规划和轨迹感知三个结构化层级，并提出了VIKI-R方法，该方法首先使用思维链标注演示微调预训练的视觉语言模型，然后通过多级奖励进行强化学习。实验结果表明，VIKI-R在所有任务层级上均显著优于基线方法，并能促进异构智能体之间出现组合式协作模式。</div>
</details>
</div>
<div class="card">
<div class="title">Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning</div>
<div class="meta-line">Authors: Xiefeng Wu, Mingyu Hu, Shu Zhang</div>
<div class="meta-line">First: 2026-01-22T08:51:16+00:00 · Latest: 2026-01-22T08:51:16+00:00</div>
<div class="meta-line">Comments: 7 pages main text 2 page reference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15761v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15761v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Sigmoid有界熵的离策略演员-评论家方法在真实世界机器人学习中的应用</div>
<div class="mono" style="margin-top:8px">在现实世界中部署强化学习仍面临样本效率低、奖励稀疏和视觉观测噪声等挑战。先前研究通过示范和人工反馈提升学习效率与鲁棒性，但离线转在线方法需大规模数据集且稳定性不足，而视觉语言模型辅助强化学习依赖大规模预训练与微调。因此，尚缺乏数据需求极低的低成本现实世界强化学习方法。本文提出\textbf{SigEnt-SAC}——一种仅需单条专家轨迹即可从零学习的离策略演员-评论家方法。其核心设计是sigmoid有界熵项，可防止负熵驱动优化导致分布外动作，并减少Q函数振荡。我们在D4RL基准任务中对比代表性基线方法，实验表明SigEnt-SAC显著缓解Q函数振荡，并以更快速度达到100%成功率。最后，我们在四种跨不同实体形态的真实机器人任务中验证该方法，智能体仅通过原始图像和稀疏奖励进行学习。结果表明SigEnt-SAC仅需少量现实交互即可习得有效策略，为现实世界强化学习部署提供了低成本实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of deploying reinforcement learning in real-world robotics, where issues like sample inefficiency, sparse rewards, and noisy visual observations hinder practical application, and existing methods often require large datasets or extensive pretraining. To overcome this, the authors propose SigEnt-SAC, an off-policy actor-critic method that learns from scratch using only a single expert trajectory, incorporating a sigmoid-bounded entropy term to prevent negative-entropy-driven optimization toward out-of-distribution actions and reduce Q-function oscillations. Experimental results on D4RL benchmarks show that SigEnt-SAC significantly mitigates Q-function oscillations and achieves a 100% success rate more quickly than prior methods, and validation on four real-world robotic tasks with raw images and sparse rewards demonstrates that it can learn successful policies with minimal real-world interactions, offering a low-cost pathway for real-world RL deployment.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在现实世界机器人应用中的挑战，如样本效率低、奖励稀疏和视觉观测噪声等问题，现有方法通常需要大量数据或预训练，难以低成本部署。为此，作者提出了SigEnt-SAC方法，这是一种离策略的演员-评论家算法，仅使用单条专家轨迹从头学习，通过引入sigmoid有界熵项来避免负熵驱动优化导致的分布外动作，并减少Q函数振荡。在D4RL基准测试中，实验结果表明SigEnt-SAC显著缓解了Q函数振荡，并比先前方法更快达到100%成功率；在四个真实机器人任务中，使用原始图像和稀疏奖励进行验证，显示该方法能以极少的真实世界交互学习到成功策略，为现实世界强化学习部署提供了一条低成本实用途径。</div>
</details>
</div>
<div class="card">
<div class="title">PhysProver: Advancing Automatic Theorem Proving for Physics</div>
<div class="meta-line">Authors: Hanning Zhang, Ruida Wang, Rui Pan, Wenyuan Wang, Bingxu Meng, Tong Zhang</div>
<div class="meta-line">First: 2026-01-22T08:05:32+00:00 · Latest: 2026-01-22T08:05:32+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15737v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15737v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysProver：推进物理学领域的自动定理证明</div>
<div class="mono" style="margin-top:8px">可验证语言与大型语言模型的结合，因其为定理证明提供了严谨基础，已显著影响数学与计算机科学界。该领域的最新进展提供了基础模型与复杂智能体系统，将形式化数学推理的边界推向接近大型语言模型的自然语言能力。然而，同样严重依赖类似问题解决与定理证明框架的形式化物理推理却鲜有关注。为解决此问题，本文提出了据我们所知首个增强物理学领域形式化定理证明的方法。我们为此任务构建了专用数据集PhysLeanData，该数据集由从PhysLean采样的定理以及基于猜想的形式化数据生成流程生成的数据组成。在训练流程中，我们利用强大的开源数学定理证明器DeepSeek-Prover-V2-7B，并应用带可验证奖励的强化学习来训练我们的模型PhysProver。综合实验表明，仅使用约5K训练样本，PhysProver在多个子领域实现了总体2.4%的性能提升。此外，经过形式化物理训练后，我们在MiniF2F-Test基准上观察到1.3%的增益，这表明模型在物理学领域之外具有非平凡的泛化能力，同时增强了形式化数学能力。结果凸显了我们方法的有效性与效率，为将形式化证明器扩展到数学领域之外提供了范式。为促进进一步研究，我们将向社区发布数据集与模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of attention to formal physics reasoning despite advances in mathematical theorem proving, this paper introduces PhysProver, the first approach to enhance automatic theorem proving specifically for physics. The method involves creating a dedicated dataset, PhysLeanData, from sampled theorems and a conjecture-based generation pipeline, and then training the DeepSeek-Prover-V2-7B model using Reinforcement Learning with Verifiable Rewards (RLVR). Experimental results show that with only about 5,000 training samples, PhysProver achieves an overall 2.4% improvement across multiple physics sub-domains and also demonstrates a 1.3% gain on the MiniF2F-Test math benchmark, indicating effective generalization and enhanced formal reasoning capabilities beyond physics.</div>
<div class="mono" style="margin-top:8px">本文的动机是，尽管数学定理证明领域已取得进展，但形式化的物理推理却鲜有关注，因此提出了首个专门用于增强物理领域自动定理证明的方法PhysProver。该方法通过从采样定理和基于猜想的生成流程构建专用数据集PhysLeanData，并利用可验证奖励的强化学习（RLVR）对DeepSeek-Prover-V2-7B模型进行训练。主要实验结果表明，仅使用约5,000个训练样本，PhysProver在多个物理子领域中实现了整体2.4%的性能提升，同时在MiniF2F-Test数学基准测试上取得了1.3%的增益，这表明该方法能有效泛化并增强物理领域之外的形式推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Online Operator Design in Evolutionary Optimization for Flexible Job Shop Scheduling via Large Language Models</div>
<div class="meta-line">Authors: Rongjie Liao, Junhao Qiu, Xin Chen, Xiaoping Li</div>
<div class="meta-line">First: 2025-11-20T15:56:09+00:00 · Latest: 2026-01-22T07:54:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16485v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.16485v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Customized static operator design has enabled widespread application of Evolutionary Algorithms (EAs), but their search effectiveness often deteriorates as evolutionary progresses. Dynamic operator configuration approaches attempt to alleviate this issue, but they typically rely on predefined operator structures and localized parameter control, lacking sustained adaptive optimization throughout evolution. To overcome these limitations, this work leverages Large Language Models (LLMs) to perceive evolutionary dynamics and enable operator-level meta-evolution. The proposed framework, LLMs for online operator design in Evolutionary Optimization, named LLM4EO, comprises three components: knowledge-transfer-based operator design, evolution perception and analysis, and adaptive operator evolution. Firstly, operators are initialized by leveraging LLMs to distill and transfer knowledge from well-established operators. Then, search behaviors and potential limitations of operators are analyzed by integrating fitness performance with evolutionary features, accompanied by suggestions for improvement. Upon stagnation of population evolution, an LLM-driven meta-operator dynamically optimizes gene selection of operators by prompt-guided improvement strategies. This approach achieves co-evolution of solutions and operators within a unified optimization framework, introducing a novel paradigm for enhancing the efficiency and adaptability of EAs. Finally, extensive experiments on multiple benchmarks of flexible job shop scheduling problem demonstrate that LLM4EO accelerates population evolution and outperforms tailored EAs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的柔性作业车间调度进化优化在线算子设计</div>
<div class="mono" style="margin-top:8px">定制化静态算子设计促进了进化算法的广泛应用，但其搜索效率常随进化进程而衰减。动态算子配置方法试图缓解此问题，但通常依赖预定义算子结构和局部参数控制，缺乏贯穿进化过程的持续自适应优化。为突破这些局限，本研究利用大语言模型感知进化动态，实现算子层面的元进化。所提出的进化优化在线算子设计框架LLM4EO包含三个组件：基于知识迁移的算子设计、进化感知与分析、自适应算子进化。首先，通过大语言模型从成熟算子中提炼并迁移知识以初始化算子；其次，结合适应度表现与进化特征分析算子的搜索行为与潜在局限，并提出改进建议；当种群进化停滞时，由大语言模型驱动的元算子通过提示引导的改进策略动态优化算子的基因选择。该方法在统一优化框架内实现解与算子的协同进化，为提升进化算法效率与适应性提供了新范式。最终，在柔性作业车间调度问题的多组基准测试中，LLM4EO显著加速种群进化并优于定制化进化算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static and dynamic operator designs in Evolutionary Algorithms (EAs), which often degrade in effectiveness over time or rely on predefined structures, this paper introduces LLM4EO, a framework that leverages Large Language Models (LLMs) for online operator design. The method involves initializing operators via LLM-based knowledge transfer from established operators, analyzing evolutionary dynamics through fitness and features to identify limitations, and using an LLM-driven meta-operator to adaptively optimize operators via prompt-guided strategies during stagnation. Experimental results on flexible job shop scheduling benchmarks show that LLM4EO accelerates population evolution and outperforms tailored EAs, demonstrating enhanced efficiency and adaptability.</div>
<div class="mono" style="margin-top:8px">针对进化算法中静态和动态算子设计效果随进化进程下降或依赖预定义结构的局限性，本文提出了LLM4EO框架，利用大语言模型实现在线算子设计。该方法通过大语言模型从成熟算子中迁移知识来初始化算子，结合适应度与进化特征分析搜索行为与局限，并在种群停滞时使用大语言模型驱动的元算子通过提示引导策略动态优化算子选择。在柔性作业车间调度问题的多个基准测试中，实验结果表明LLM4EO加速了种群进化，性能优于定制进化算法，提升了效率与适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind</div>
<div class="meta-line">Authors: Zhitao He, Zongwei Lyu, Yi R Fung</div>
<div class="meta-line">First: 2026-01-22T07:36:48+00:00 · Latest: 2026-01-22T07:36:48+00:00</div>
<div class="meta-line">Comments: Preprint, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15715v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15715v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author&#x27;s own critical analysis and response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>镣铐之舞：基于心智理论的学术反驳策略性说服研究</div>
<div class="mono" style="margin-top:8px">尽管人工智能已深度融入科研工作流并取得显著进展，学术反驳仍是一个重要且尚未充分探索的挑战。这是因为反驳是在严重信息不对称下进行的复杂策略性沟通过程，而非简单的技术辩论。现有方法因主要模仿表层语言特征而难以奏效，缺失了有效说服所需的核心要素——观点采择能力。本文提出首个基于心智理论的学术反驳框架RebuttalAgent，通过心智理论-策略-响应的三级流程实现：建模审稿人心理状态、制定说服策略、生成策略驱动的回应。为训练智能体，我们构建了大规模数据集RebuttalBench，采用创新的批判-精炼合成方法。训练过程分为两阶段：首先通过监督微调使智能体掌握基于心智理论的分析与策略规划能力，随后利用自奖励机制进行强化学习以实现可扩展的自我改进。为建立可靠高效的自动评估体系，我们进一步开发了专用评估器Rebuttal-RM，该模型基于超10万条多源反驳数据训练，其评分与人类偏好的一致性已超越GPT-4.1。大量实验表明，RebuttalAgent在自动指标上平均超越基线模型18.3%，同时在自动与人工评估中均优于先进的专有模型。声明：生成的反驳内容仅供作者参考启发与辅助草拟，不替代作者自身的批判性分析与回应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complex strategic communication and information asymmetry inherent in academic rebuttal, which current AI approaches fail to address by merely imitating surface-level language, this paper introduces RebuttalAgent, a framework grounded in Theory of Mind (ToM). The method employs a ToM-Strategy-Response pipeline to model reviewer mental states and formulate persuasive strategies, trained via supervised fine-tuning and reinforcement learning on a novel large-scale dataset, RebuttalBench, synthesized through a critique-and-refine approach. Experimental results demonstrate that RebuttalAgent, evaluated by a specialized automated evaluator, Rebuttal-RM, outperforms the base model by an average of 18.3% on automated metrics and surpasses advanced proprietary models in both automated and human evaluations.</div>
<div class="mono" style="margin-top:8px">本文的动机在于学术反驳是一个存在严重信息不对称的复杂战略沟通过程，而当前人工智能方法仅模仿表面语言，未能解决这一挑战。为此，论文提出了首个基于心智理论（ToM）的框架RebuttalAgent，其方法通过ToM-策略-响应流程建模审稿人心理状态并制定说服策略，并利用基于批判-精炼方法合成的大规模数据集RebuttalBench进行监督微调和强化学习训练。主要实验结果表明，RebuttalAgent在专门开发的自动评估器Rebuttal-RM的评测下，其自动指标平均优于基线模型18.3%，并在自动和人工评估中均超越了先进的专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</div>
<div class="meta-line">Authors: Shaocong Ma, Heng Huang</div>
<div class="meta-line">First: 2025-10-22T18:22:25+00:00 · Latest: 2026-01-22T06:31:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19950v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19950v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>金融领域的鲁棒强化学习：基于椭圆不确定性集合的市场冲击建模</div>
<div class="mono" style="margin-top:8px">在金融应用中，强化学习智能体通常基于历史数据进行训练，此时其行为不会影响市场价格。然而，在实际部署时，这些智能体在实时市场中进行交易，其自身交易行为可能导致资产价格变动，这种现象称为市场冲击。训练环境与部署环境之间的这种不匹配会显著降低模型性能。传统的鲁棒强化学习方法通过在一组不确定性上优化最坏情况性能来解决模型误设问题，但通常依赖对称结构，无法捕捉市场冲击的方向性特征。为解决这一问题，我们开发了一类新型椭圆不确定性集合。针对这些集合下的最坏情况不确定性，我们建立了隐式和显式闭式解，实现了高效且可处理的鲁棒策略评估。在单资产和多资产交易任务上的实验表明，我们的方法获得了更优的夏普比率，并在交易量增加时保持鲁棒性，为金融市场中的强化学习提供了更可靠且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the performance degradation of reinforcement learning agents in finance due to market impact, where actions during live trading affect prices, unlike in historical training data. The authors propose a novel robust RL method using elliptic uncertainty sets to better model the directional nature of market impact, deriving implicit and explicit closed-form solutions for worst-case uncertainty to enable efficient policy evaluation. Experimental results on trading tasks show that this approach achieves a higher Sharpe ratio and maintains robustness with increasing trade volumes, providing a more realistic and scalable solution for financial applications.</div>
<div class="mono" style="margin-top:8px">本文针对金融中强化学习智能体因市场冲击导致的性能下降问题，即实盘交易时自身行为会影响价格，而历史训练数据中则无此影响。作者提出了一种使用椭圆不确定性集的新型鲁棒强化学习方法，以更好地建模市场冲击的方向性，并推导了最坏不确定性的隐式和显式闭式解，从而实现高效策略评估。在单资产和多资产交易任务上的实验结果表明，该方法获得了更高的夏普比率，并在交易量增加时保持鲁棒性，为金融市场提供了更真实且可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models</div>
<div class="meta-line">Authors: Jiaxin Zhang, Wendi Cui, Zhuohang Li, Lifu Huang, Bradley Malin, Caiming Xiong, Chien-Sheng Wu</div>
<div class="meta-line">First: 2026-01-22T06:21:31+00:00 · Latest: 2026-01-22T06:21:31+00:00</div>
<div class="meta-line">Comments: 20 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从被动度量到主动信号：不确定性量化在大语言模型中的角色演变</div>
<div class="mono" style="margin-top:8px">尽管大语言模型展现出卓越能力，但其不可靠性仍是高风险领域部署的关键障碍。本综述描绘了应对这一挑战的功能演进路径：不确定性从被动的诊断度量演变为指导实时模型行为的主动控制信号。我们通过三个前沿领域展示不确定性如何作为主动控制信号发挥作用：在\textbf{高级推理}中优化计算并触发自我修正；在\textbf{自主智能体}中调控工具使用与信息获取的元认知决策；在\textbf{强化学习}中缓解奖励破解并通过内在奖励实现自我改进。通过将这些进展锚定于贝叶斯方法、保形预测等新兴理论框架，我们为这一变革性趋势提供了统一视角。本综述提供全面概述、批判性分析及实用设计模式，论证掌握不确定性的新趋势对于构建下一代可扩展、可靠、可信的人工智能至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the critical need to enhance the reliability of Large Language Models (LLMs) for high-stakes applications, this survey paper examines a paradigm shift where uncertainty quantification evolves from a passive diagnostic metric into an active control signal for real-time model guidance. The method involves a comprehensive analysis of how this active uncertainty signal is applied across three key frontiers: advanced reasoning to optimize computation and self-correction, autonomous agents to govern metacognitive decisions like tool use, and reinforcement learning to prevent reward hacking and enable self-improvement through intrinsic rewards, all grounded in theoretical frameworks like Bayesian methods and Conformal Prediction. The main experimental results, synthesized from the surveyed literature, demonstrate that leveraging uncertainty as an active control mechanism significantly improves model reliability, scalability, and trustworthiness, providing practical design patterns for next-generation AI systems.</div>
<div class="mono" style="margin-top:8px">本文的动机是大型语言模型在关键领域应用中的不可靠性，旨在探讨将不确定性量化从被动的诊断指标转变为指导模型实时行为的主动控制信号这一范式演进。方法上，该综述系统分析了如何将主动不确定性信号应用于三个前沿领域：在高级推理中优化计算并触发自我修正，在自主智能体中管理关于工具使用和信息寻求的元认知决策，以及在强化学习中通过内在奖励缓解奖励黑客问题并实现自我改进，这些分析均基于贝叶斯方法和共形预测等新兴理论框架。主要的实验成果，基于所综述的文献，表明将不确定性作为主动控制机制能显著提升模型的可信性、可扩展性和可靠性，为构建下一代可扩展、可靠且可信的人工智能提供了实用的设计模式。</div>
</details>
</div>
<div class="card">
<div class="title">Performance-guided Reinforced Active Learning for Object Detection</div>
<div class="meta-line">Authors: Zhixuan Liang, Xingyu Zeng, Rui Zhao, Ping Luo</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-22T06:17:08+00:00 · Latest: 2026-01-22T06:17:08+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026. Camera-ready Version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15688v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15688v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data&#x27;s distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL&#x27;s active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向目标检测的性能引导强化主动学习</div>
<div class="mono" style="margin-top:8px">主动学习策略旨在以最小标注成本训练高性能模型，仅选择信息量最大的样本进行标注。现有评估数据信息量的方法主要关注数据分布或内在信息内容，未能与下游任务性能（如目标检测中的平均精度均值mAP）直接关联。为此，我们提出面向目标检测的性能引导（即mAP引导）强化主动学习方法MGRAL，该创新方法以模型预期输出变化作为信息量度量标准。针对批量样本选择的组合爆炸问题，以及模型性能与选定批次间不可微分的关联性，MGRAL巧妙采用基于强化学习的采样智能体，以mAP提升作为奖励，通过策略梯度优化选择过程。此外，为降低未标注样本的mAP估计计算开销，MGRAL采用基于快速查找表的无监督方式，确保部署可行性。我们在PASCAL VOC和COCO基准检测任务上评估MGRAL的主动学习性能，该方法展现出最优的主动学习曲线与可信的可视化结果，为强化学习驱动的主动目标检测建立了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of current active learning methods for object detection, which often select data based on distribution or intrinsic information without directly optimizing for task performance like mean average precision (mAP). To overcome this, the authors propose MGRAL, a reinforced active learning approach that uses expected model output changes as informativeness and employs a reinforcement learning agent with policy gradient optimization, where mAP improvement serves as the reward, while leveraging unsupervised fast look-up tables to reduce computational overhead from mAP estimation. Experimental results on PASCAL VOC and COCO benchmarks show that MGRAL achieves the highest active learning curve with convincing visualizations, establishing a new reinforcement learning-driven paradigm for active object detection.</div>
<div class="mono" style="margin-top:8px">本文针对当前目标检测主动学习方法效率不足的问题，这些方法通常基于数据分布或内在信息选择样本，而未直接优化如平均精度均值（mAP）等任务性能。为此，作者提出MGRAL，一种强化主动学习方法，以期望模型输出变化作为信息量度量，采用强化学习代理通过策略梯度优化进行样本选择，其中mAP提升作为奖励，并利用无监督快速查找表降低mAP估计的计算开销。在PASCAL VOC和COCO基准上的实验结果表明，MGRAL实现了最高的主动学习曲线，并提供了可信的可视化效果，为强化学习驱动的主动目标检测建立了新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving in Tasks: Empowering the Multi-modality Large Language Model as the Computer Use Agent</div>
<div class="meta-line">Authors: Yuhao Cheng, Liang Tang, Shuxian Li, Yukang Huo, Tiaonan Duan, Kaer Huang, Yanzhe Jing, Yiqiang Yan</div>
<div class="meta-line">First: 2025-08-06T02:57:22+00:00 · Latest: 2026-01-22T05:22:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.04037v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.04037v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computer use agents represent an emerging area in artificial intelligence, aiming to operate computers autonomously to fulfill user tasks, attracting significant attention from both industry and academia. However, the performance of existing agents remains insufficient for practical deployment. In this paper, we propose the Self-Evolution Agent (SEA) for computer operation, alongside three core innovations in data generation, reinforcement learning, and model enhancement to develop this agent. Specifically, we first design an automatic pipeline to generate verifiable task trajectories for training. Second, we propose Efficient Step-wise Reinforcement Learning to reduce the substantial computational overhead of long-horizon training. Finally, we introduce a model enhancement method that integrates grounding and planning capabilities into a single model without additional training. Leveraging these innovations, our SEA (with only 7B parameters) outperforms existing models of the same parameter scale and achieves performance comparable to larger models (e.g., 32B/72B parameters) on computer use tasks. We plan to release the model weights and related code as open-source resources in the future.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>任务演进：赋能多模态大语言模型成为计算机使用智能体</div>
<div class="mono" style="margin-top:8px">计算机使用智能体是人工智能领域的新兴方向，旨在通过自主操作计算机完成用户任务，已引起工业界与学术界的广泛关注。然而现有智能体的性能尚不足以支撑实际部署。本文提出用于计算机操作的自我演进智能体（SEA），并通过数据生成、强化学习和模型增强三个核心创新来构建该智能体。具体而言，我们首先设计了自动生成可验证任务轨迹的训练流水线；其次提出高效分步强化学习方法以降低长周期训练的巨大计算开销；最后引入一种无需额外训练即可将基础能力与规划能力融合至单一模型的增强方法。依托这些创新，我们的SEA（仅70亿参数）在计算机使用任务上超越了同参数规模模型，并达到与更大规模模型（如320亿/720亿参数）相当的性能。我们计划未来将模型权重及相关代码作为开源资源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the insufficient performance of existing computer use agents for practical deployment, this paper introduces the Self-Evolution Agent (SEA) to enhance autonomous computer operation. The method involves three core innovations: an automatic pipeline for generating verifiable task trajectories, Efficient Step-wise Reinforcement Learning to reduce computational overhead in long-horizon training, and a model enhancement technique that integrates grounding and planning capabilities without extra training. Experimentally, the 7B-parameter SEA outperforms same-scale models and achieves performance comparable to much larger models (e.g., 32B/72B parameters) on computer use tasks, with plans to release the model and code as open-source.</div>
<div class="mono" style="margin-top:8px">针对现有计算机使用智能体性能不足、难以实际部署的问题，本文提出了自我进化智能体（SEA）以提升自主操作计算机的能力。方法包含三项核心创新：设计自动流水线生成可验证的任务轨迹用于训练，提出高效分步强化学习以减少长序列训练的计算开销，以及引入一种无需额外训练即可将基础定位与规划能力集成到单一模型中的增强技术。实验结果表明，仅70亿参数的SEA在计算机使用任务上超越了同参数规模的现有模型，并达到了与更大参数模型（如320亿/720亿参数）相当的性能，未来计划开源模型权重及相关代码。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors</div>
<div class="meta-line">Authors: Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang Wang, Yao Hu, Shaosheng Cao, Kam-Fai Wong</div>
<div class="meta-line">First: 2026-01-22T03:57:35+00:00 · Latest: 2026-01-22T03:57:35+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15625v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#x27;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Fission-GRPO的鲁棒工具使用：学习从执行错误中恢复</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）能有效调用工具，但在多轮执行中仍显脆弱：工具调用出错后，较小模型常陷入重复无效调用，无法解读错误反馈并自我修正。这种脆弱性阻碍了实际部署的可靠性，因为在工具交互过程中执行错误本就难以避免。我们发现现有方法的关键局限：标准强化学习（RL）将错误视为稀疏负奖励，未提供恢复指导；而预收集的合成纠错数据集与模型在策略错误模式存在分布失配。为弥合此差距，我们提出Fission-GRPO框架，将执行错误转化为RL训练循环内的纠正监督。核心机制通过微调的错误模拟器为每个失败轨迹添加诊断反馈，将其裂解为新训练实例，再按策略重采样恢复轨迹。这使得模型能从探索过程中的具体错误中学习，而非依赖静态预收集错误案例。在BFCL v4多轮测试中，Fission-GRPO将Qwen3-8B的错误恢复率绝对提升5.7%，关键的是，整体准确率较GRPO提升4%（42.75%→46.75%），并超越专用工具使用智能体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the brittleness of large language models (LLMs) in multi-turn tool use, where they often fail to recover from execution errors, hindering reliable deployment. To overcome the limitations of standard reinforcement learning and pre-collected error datasets, the authors propose Fission-GRPO, a framework that converts execution errors into corrective supervision by augmenting failed trajectories with diagnostic feedback from a finetuned Error Simulator and resampling recovery rollouts on-policy. Experimental results on the BFCL v4 Multi-Turn benchmark show that Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolutely and yields a 4% overall accuracy gain over GRPO, outperforming specialized tool-use agents.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在多轮工具使用中的脆弱性问题展开研究，即模型在执行错误后难以自我纠正，这阻碍了实际部署的可靠性。为解决标准强化学习和预收集错误数据集的不足，作者提出了Fission-GRPO框架，该框架通过使用微调的错误模拟器为失败轨迹添加诊断反馈，并基于策略重采样恢复路径，从而将执行错误转化为纠正性监督。在BFCL v4多轮基准测试中，实验结果表明Fission-GRPO将Qwen3-8B的错误恢复率绝对提升了5.7%，整体准确率相比GRPO提高了4%，优于专用工具使用代理。</div>
</details>
</div>
<div class="card">
<div class="title">Your Group-Relative Advantage Is Biased</div>
<div class="meta-line">Authors: Fengkai Yang, Zherui Chen, Xiaohan Wang, Xiaodong Lu, Jiajun Chai, Guojun Yin, Wei Lin, Shuai Ma, Fuzhen Zhuang, Deqing Wang, Yaodong Yang, Jianxin Li, Yikun Ban</div>
<div class="meta-line">First: 2026-01-13T13:03:15+00:00 · Latest: 2026-01-22T03:42:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08521v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08521v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>群体相对优势估计存在偏差</div>
<div class="mono" style="margin-top:8px">基于验证器奖励的强化学习已成为推理任务上对大语言模型进行后训练的常用方法，其中GRPO及其变体等基于群体的方法获得广泛应用。这些方法依赖群体相对优势估计来避免学习批评器，但其理论性质仍鲜为人知。本研究揭示了基于群体强化学习的一个根本问题：群体相对优势估计量相对于真实期望优势存在固有偏差。我们首次通过理论分析证明，该方法会系统性地低估困难提示的优势、高估简单提示的优势，导致探索与利用失衡。为解决此问题，我们提出历史感知自适应难度加权法——一种基于动态难度锚点与训练过程调整优势估计的自适应重加权方案。在五个数学推理基准上的理论分析与实验均表明，HA-DW在融入GRPO及其变体后能持续提升性能。我们的结果表明，纠正有偏差的优势估计对于实现稳健高效的RLVR训练至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the widespread use of group-relative advantage estimation in Reinforcement Learning from Verifier Rewards (RLVR) for post-training language models, despite a lack of theoretical understanding of its properties. The method identifies a fundamental bias in group-based RL, where the estimator systematically underestimates advantages for hard prompts and overestimates them for easy ones, leading to imbalanced training; to address this, the authors propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on evolving difficulty anchors and training dynamics. Experimental results on five mathematical reasoning benchmarks show that HA-DW consistently improves performance when integrated into GRPO and its variants, demonstrating that correcting biased advantage estimation is crucial for robust and efficient RLVR training.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于基于验证器奖励的强化学习在大型语言模型后训练中被广泛采用，其中分组相对优势估计方法如GRPO及其变体应用普遍，但其理论特性尚未得到充分理解。方法上，研究揭示了分组强化学习的一个根本问题：分组相对优势估计器相对于真实期望优势存在固有偏差，会系统性地低估困难提示的优势并高估简单提示的优势，导致探索与利用失衡；为此，作者提出了历史感知自适应难度加权方法，这是一种基于动态难度锚点和训练过程的自适应重加权方案，以调整优势估计。在五个数学推理基准上的实验结果表明，该方法在集成到GRPO及其变体后能持续提升性能，证明纠正有偏的优势估计对于实现稳健高效的RLVR训练至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards</div>
<div class="meta-line">Authors: Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun Zhou</div>
<div class="meta-line">First: 2026-01-22T03:15:57+00:00 · Latest: 2026-01-22T03:15:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15609v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当锐化演变为坍缩：可验证奖励强化学习中的采样偏差与语义耦合</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是将大语言模型转化为可靠问题求解器的核心范式，尤其在逻辑密集型领域表现突出。尽管实证研究已证实其有效性，但RLVR究竟是激发了新能力，还是仅强化了现有知识的分布，仍不明确。本研究通过形式化“过度锐化”现象对此展开探讨——即策略坍缩至有限模式、压制有效替代方案的过程。研究发现，有限批次更新本质上会使学习偏向已采样模式，进而通过语义耦合引发全局性坍缩。为缓解此问题，我们提出逆成功优势校准（优先处理困难查询）与分布级校准（通过记忆网络实现采样多样化）两种方法。实证评估表明，这些策略能有效提升模型的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the phenomenon of over-sharpening in Reinforcement Learning with Verifiable Rewards (RLVR), a method used to enhance large language models for logic-heavy tasks, motivated by the need to determine whether RLVR develops new capabilities or merely narrows existing knowledge. The authors formalize over-sharpening as a policy collapse onto limited modes, attributing it to sampling bias in finite-batch updates and global propagation via semantic coupling. To address this, they propose two calibration strategies: inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration using a memory network to diversify sampling. Experimental results demonstrate that these methods effectively mitigate collapse and improve the model&#x27;s generalization performance.</div>
<div class="mono" style="margin-top:8px">本文研究了可验证奖励强化学习中的过度锐化现象，该方法用于增强大语言模型在逻辑密集型任务中的表现，其动机是需厘清RLVR是开发了新能力还是仅窄化了现有知识。作者将过度锐化形式化为策略坍缩到有限模式，并将其归因于有限批次更新中的采样偏差以及通过语义耦合的全局传播。为解决此问题，他们提出了两种校准策略：逆成功优势校准以优先处理困难查询，以及通过记忆网络进行分布级校准以多样化采样。实验结果表明，这些方法能有效缓解坍缩并提升模型的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Report for NSF Workshop on AI for Electronic Design Automation</div>
<div class="meta-line">Authors: Deming Chen, Vijay Ganesh, Weikai Li, Yingyan Celine Lin, Yong Liu, Subhasish Mitra, David Z. Pan, Ruchir Puri, Jason Cong, Yizhou Sun</div>
<div class="meta-line">First: 2026-01-20T23:45:40+00:00 · Latest: 2026-01-22T02:12:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14541v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14541v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ai4eda-workshop.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>美国国家科学基金会人工智能赋能电子设计自动化研讨会报告</div>
<div class="mono" style="margin-top:8px">本报告提炼了2024年12月10日于温哥华NeurIPS 2024同期举办的&#x27;人工智能赋能电子设计自动化（EDA）&#x27;美国国家科学基金会（NSF）研讨会的讨论成果与建议。研讨会汇聚机器学习与EDA领域的专家，探讨了人工智能技术——涵盖大语言模型（LLM）、图神经网络（GNN）、强化学习（RL）、神经符号方法等——如何促进EDA流程并缩短设计周期。会议聚焦四大主题：（1）面向物理综合与制造设计（DFM）的人工智能，探讨物理制造工艺中的挑战及潜在AI应用；（2）面向高层次与逻辑级综合（HLS/LLS）的人工智能，涉及编译指示插入、程序转换、RTL代码生成等；（3）用于优化与设计的人工智能工具箱，讨论可应用于EDA任务的前沿AI进展；（4）面向测试与验证的人工智能，包括LLM辅助验证工具、机器学习增强的SAT求解、安全性与可靠性挑战等。报告建议NSF推动AI/EDA跨领域合作，投资EDA基础性AI研究，建设稳健的数据基础设施，发展可扩展的计算资源，并加强人才培养以普及硬件设计能力、赋能下一代硬件系统。研讨会详情可访问网站 https://ai4eda-workshop.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This report summarizes the NSF Workshop on AI for Electronic Design Automation (EDA), motivated by the need to leverage AI technologies like large language models, graph neural networks, and reinforcement learning to accelerate electronic design processes and reduce turnaround time. The method involved expert discussions organized around four key themes: AI for physical synthesis and manufacturing, AI for high-level and logic synthesis, AI toolboxes for optimization, and AI for test and verification. The main experimental results are not applicable as this is a workshop report, but the outcomes include concrete recommendations for NSF to foster AI-EDA collaboration, invest in foundational AI research, develop data and compute infrastructures, and support workforce development to democratize hardware design and enable next-generation systems.</div>
<div class="mono" style="margin-top:8px">本报告总结了美国国家科学基金会（NSF）关于人工智能用于电子设计自动化（EDA）的研讨会，其动机在于利用大型语言模型、图神经网络和强化学习等人工智能技术来加速电子设计流程并缩短周转时间。方法包括围绕四个主题组织专家讨论：用于物理合成和制造的人工智能、用于高层次和逻辑合成的人工智能、用于优化的AI工具箱，以及用于测试和验证的人工智能。由于这是一份研讨会报告，未涉及具体实验，但主要成果包括向NSF提出的具体建议，如促进AI与EDA合作、投资基础AI研究、开发数据和计算基础设施，以及支持人才培养，以普及硬件设计并推动下一代硬件系统的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation</div>
<div class="meta-line">Authors: Yushen Liu, Yanfu Zhang, Xugui Zhou</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2025-11-16T02:11:33+00:00 · Latest: 2026-01-21T23:25:37+00:00</div>
<div class="meta-line">Comments: 5 pages, 3 figures, ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12417v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12417v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>融合神经微分预测与安全强化学习的血糖调控方法</div>
<div class="mono" style="margin-top:8px">针对1型糖尿病的自动胰岛素输送系统需在不确定的饮食和生理波动下平衡血糖控制与安全性。虽然强化学习（RL）能实现自适应个性化调控，但现有方法难以同时保障安全性，导致在实现个性化与风险感知的血糖控制（如餐前过量注射或叠加校正）方面存在不足。为填补这一空白，我们提出TSODE——一种集成汤普森采样强化学习（Thompson Sampling RL）与神经常微分方程（NeuralODE）预测器的安全感知控制器。具体而言，NeuralODE根据拟定的胰岛素剂量预测短期血糖轨迹，而保形校准层通过量化预测不确定性来拒绝或调整风险动作。在FDA认证的UVa/Padova模拟器（成人队列）中，TSODE实现了87.9%的血糖在目标范围内时间，低于70 mg/dL的时间不足10%，性能优于相关基线。结果表明，将自适应强化学习经过校准的NeuralODE预测相结合，可实现可解释、安全且稳健的血糖调控。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for automated insulin delivery systems that balance personalized glucose control with safety guarantees against risks like overdosing, this paper introduces TSODE, a safety-aware controller integrating Thompson Sampling reinforcement learning with a NeuralODE forecaster. The method uses the NeuralODE to predict short-term glucose trajectories based on proposed insulin doses, enhanced by a conformal calibration layer to quantify uncertainty and reject or scale risky actions. Experimental results on the FDA-approved UVa/Padova simulator show that TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming baselines and demonstrating robust, interpretable glucose regulation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决1型糖尿病自动胰岛素输送中个性化血糖控制与安全性保障之间的平衡问题，特别是避免餐前过量或叠加校正等风险。为此，论文提出了TSODE安全感知控制器，该方法将汤普森采样强化学习与神经常微分方程预测器相结合，其中神经ODE根据胰岛素剂量预测短期血糖轨迹，并通过保形校准层量化预测不确定性以拒绝或调整风险动作。在FDA批准的UVa/Padova模拟器（成人队列）上的实验结果表明，TSODE实现了87.9%的血糖在目标范围内时间，且低于70 mg/dL的时间小于10%，性能优于相关基线，证明了该方法可实现可解释、安全且稳健的血糖调控。</div>
</details>
</div>
<div class="card">
<div class="title">Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter</div>
<div class="meta-line">Authors: Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han</div>
<div class="meta-line">First: 2025-11-20T18:59:25+00:00 · Latest: 2026-01-21T23:19:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16665v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16665v2">PDF</a> · <a href="https://github.com/mit-han-lab/fastrl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>驯服长尾分布：基于自适应草稿模型的高效推理强化学习训练</div>
<div class="mono" style="margin-top:8px">具备强大推理能力的大语言模型（LLMs）的出现标志着重要里程碑，为复杂问题求解开辟了新前沿。然而，这类通常采用强化学习（RL）训练的推理模型面临关键效率瓶颈：RL训练中的响应生成呈现持续的长尾分布，少数极长响应主导执行时间，造成资源浪费与成本激增。为此，我们提出TLT系统，通过集成自适应推测解码无损加速推理RL训练。在RL中应用推测解码面临动态工作负载、持续演进的目标模型及草稿模型训练开销等挑战。TLT通过两个协同组件突破障碍：（1）自适应草稿模型——在长尾生成期间利用空闲GPU持续训练的轻量级草稿模型，以零额外成本保持与目标模型的对齐；（2）自适应执行引擎——维护内存高效的预捕获CUDAGraph池，并为每批输入自适应选择适宜的推测解码策略。评估表明，TLT相比前沿系统实现超过1.7倍的端到端RL训练加速，保持模型精度，并免费获得适用于高效部署的高质量草稿模型。代码发布于https://github.com/mit-han-lab/fastrl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the efficiency bottleneck in training large language models for reasoning tasks via reinforcement learning, where the long-tail distribution of response lengths leads to wasted computational resources and high costs. The proposed method, TLT, introduces an adaptive speculative decoding system with two key components: an Adaptive Drafter that trains a lightweight draft model on idle GPUs to stay aligned with the target model without extra overhead, and an Adaptive Rollout Engine that manages a pool of CUDAGraphs and selects optimal speculative decoding strategies per batch. Experimental results show that TLT achieves over 1.7x end-to-end training speedup compared to state-of-the-art systems while maintaining model accuracy and producing a high-quality draft model as a deployable byproduct.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决通过强化学习训练大型语言模型进行推理任务时的效率瓶颈，即响应长度的长尾分布导致计算资源浪费和成本高昂。提出的方法TLT采用自适应推测解码系统，包含两个关键组件：自适应草稿模型在空闲GPU上持续训练轻量级草稿模型以与目标模型保持对齐而无额外开销，以及自适应执行引擎维护CUDAGraphs池并为每个输入批次自适应选择推测解码策略。实验结果表明，TLT相比最先进系统实现了超过1.7倍的端到端训练加速，同时保持了模型准确性，并生成了高质量草稿模型作为可部署的副产品。</div>
</details>
</div>
<div class="card">
<div class="title">On the Exponential Convergence for Offline RLHF with Pairwise Comparisons</div>
<div class="meta-line">Authors: Zhirui Chen, Vincent Y. F. Tan</div>
<div class="meta-line">Venue: AAAI 2026 oral presentation</div>
<div class="meta-line">First: 2024-06-18T02:03:12+00:00 · Latest: 2026-01-21T21:33:45+00:00</div>
<div class="meta-line">Comments: Accepted as an oral presentation at AAAI 2026 (AI Alignment Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.12205v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.12205v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of offline reinforcement learning from human feedback (RLHF) with pairwise comparisons proposed by Zhu et al. (2023), where the implicit reward is a linear function of an unknown parameter. Given an offline dataset, our objective consists in ascertaining the optimal action for each state, with the ultimate goal of minimizing the {\em simple regret}. We propose an algorithm, \underline{RL} with \underline{L}ocally \underline{O}ptimal \underline{W}eights or {\sc RL-LOW}, which yields an exponential form of simple regret of $\exp ( - Ω(n/H) )$ where $n$ is the number of data samples and $H$ denotes an instance-dependent hardness quantity that depends explicitly on the suboptimality gap of each action. Furthermore, we derive a first-of-its-kind instance-dependent lower bound in offline RLHF with pairwise comparisons. Interestingly, we observe that the lower and upper bounds on the simple regret match order-wise in the exponent, demonstrating order-wise optimality of our {\sc RL-LOW}. In view of privacy considerations in practical applications, we also extend {\sc RL-LOW} to the setting of $(\varepsilon,δ)$-differential privacy and show, somewhat surprisingly, that the hardness parameter $H$ is unchanged in the asymptotic regime as $n$ tends to infinity; this underscores the inherent efficiency of {\sc RL-LOW} in terms of preserving the privacy of the observed rewards. Given our focus on establishing instance-dependent bounds of exponential convergence, our research fills the research gap in existing studies that concentrate on establishing worst-case regrets of {\em inverse polynomial convergence} (e.g., $\widetilde{O}(\frac{1}{\sqrt{n}})$) for offline RLHF with pairwise comparisons.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于成对比较的离线RLHF指数收敛性研究</div>
<div class="mono" style="margin-top:8px">本文研究Zhu等人（2023）提出的基于成对比较的离线人类反馈强化学习问题，其中隐式奖励是未知参数的线性函数。给定离线数据集，我们的目标是为每个状态确定最优动作，最终最小化简单遗憾。我们提出一种算法——局部最优权重强化学习（RL-LOW），该算法能实现指数形式的简单遗憾$\exp ( - Ω(n/H) )$，其中$n$为数据样本数，$H$是依赖于实例的硬度量，显式取决于各动作的次优间隙。此外，我们首次推导出成对比较离线RLFH的实例相关下界。有趣的是，简单遗憾的上下界在指数阶上匹配，证明了RL-LOW的阶次最优性。针对实际应用中的隐私考量，我们将RL-LOW扩展至$(\varepsilon,δ)$-差分隐私场景，并发现当$n$趋于无穷时，硬度参数$H$在渐近状态下保持不变，这凸显了RL-LOW在保护观测奖励隐私方面的内在效率。本研究聚焦于建立指数收敛的实例相关界，填补了现有研究仅关注成对比较离线RLHF的逆多项式收敛最坏情况遗憾（如$\widetilde{O}(\frac{1}{\sqrt{n}})$）的理论空白。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses offline reinforcement learning from human feedback (RLHF) with pairwise comparisons, motivated by the need to move beyond worst-case inverse-polynomial regret bounds and achieve faster, instance-dependent convergence. The authors propose an algorithm called RL-LOW, which estimates an implicit linear reward function from offline data to determine optimal actions and minimize simple regret. The main result is an exponential convergence rate for simple regret, expressed as exp(-Ω(n/H)), where n is the sample size and H is an instance-dependent hardness measure; this is shown to be order-optimal via a matching lower bound, and the method is also extended to differentially private settings without asymptotic loss in efficiency.</div>
<div class="mono" style="margin-top:8px">本文研究基于成对比较的离线人类反馈强化学习（RLHF），其动机是超越现有的最坏情况逆多项式遗憾收敛速度，实现更快的、依赖于问题实例的收敛。作者提出了RL-LOW算法，通过离线数据估计隐式的线性奖励函数以确定最优动作并最小化简单遗憾。主要实验结果表明，该算法实现了简单遗憾的指数级收敛，形式为exp(-Ω(n/H))，其中n为样本量，H为依赖于实例的难度度量；通过匹配的下界证明该结果在阶数上是最优的，并且该方法可扩展至差分隐私设置，在渐近意义上不损失效率。</div>
</details>
</div>
<div class="card">
<div class="title">Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias</div>
<div class="meta-line">Authors: Xia Chen</div>
<div class="meta-line">First: 2025-12-30T00:34:24+00:00 · Latest: 2026-01-21T20:37:33+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23916v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23916v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional deep learning prioritizes unconstrained optimization, yet biological systems operate under strict metabolic constraints. We propose that these physical constraints shape dynamics to function not as limitations, but as a temporal inductive bias that breeds generalization. Through a phase-space analysis of signal propagation, we reveal a fundamental asymmetry: expansive dynamics amplify noise, whereas proper dissipative dynamics compress phase space that aligns with the network&#x27;s spectral bias, compelling the abstraction of invariant features. This condition can be imposed externally via input encoding, or intrinsically through the network&#x27;s own temporal dynamics. Both pathways require architectures capable of temporal integration and proper constraints to decode induced invariants, whereas static architectures fail to capitalize on temporal structure. Through comprehensive evaluations across supervised classification, unsupervised reconstruction, and zero-shot reinforcement learning, we demonstrate that a critical &quot;transition&quot; regime maximizes generalization capability. These findings establish dynamical constraints as a distinct class of inductive bias, suggesting that robust AI development requires not only scaling and removing limitations, but computationally mastering the temporal characteristics that naturally promote generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束催生泛化：作为归纳偏置的时序动态机制</div>
<div class="mono" style="margin-top:8px">传统深度学习以无约束优化为核心，而生物系统却在严格代谢约束下运行。我们提出：这些物理约束塑造的动态机制并非限制，而是催生泛化的时序归纳偏置。通过对信号传播的相空间分析，我们揭示了根本性不对称：扩张性动态会放大噪声，而恰当的耗散动态则压缩相空间，使其与网络频谱偏置对齐，从而强制提取不变特征。该条件可通过外部输入编码施加，或由网络自身时序动态内禀实现。两种路径均需具备时序整合能力与恰当约束的架构以解码诱导不变性，而静态架构无法利用时序结构。通过在监督分类、无监督重建与零样本强化学习的综合评估，我们证明临界&#x27;过渡&#x27;机制能最大化泛化能力。这些发现确立了动态约束作为一类独特的归纳偏置，表明稳健AI发展不仅需要扩展规模与消除限制，更需通过计算手段掌握自然促进泛化的时序特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that biological systems operate under strict metabolic constraints unlike unconstrained deep learning, this paper proposes that such physical constraints act as a temporal inductive bias to improve generalization. The method involves a phase-space analysis showing that proper dissipative dynamics compress phase space and align with spectral bias, forcing abstraction of invariant features, which can be imposed via input encoding or intrinsic network dynamics. Experimental results across supervised classification, unsupervised reconstruction, and zero-shot reinforcement learning demonstrate that a critical transition regime maximizes generalization, establishing dynamical constraints as a distinct inductive bias for robust AI.</div>
<div class="mono" style="margin-top:8px">本文的动机源于生物系统在严格代谢约束下运行，而无约束的深度学习则不然，提出这些物理约束可作为时间归纳偏置以提升泛化能力。方法上通过相空间分析表明，适当的耗散动力学能压缩相空间并与谱偏置对齐，从而强制抽象不变特征，这可通过输入编码或网络内在动力学实现。在监督分类、无监督重建和零样本强化学习的综合评估中，实验结果显示临界“过渡”机制能最大化泛化能力，确立了动力学约束作为一类独特的归纳偏置，为鲁棒AI开发提供了新方向。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
