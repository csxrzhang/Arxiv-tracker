<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-19 04:06</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260219_0406</div>
    <div class="row"><div class="card">
<div class="title">Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching</div>
<div class="meta-line">Authors: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza, Guanya Shi, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-17T18:59:11+00:00 · Latest: 2026-02-17T18:59:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15827v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知型人形机器人跑酷：通过运动匹配串联动态人体技能</div>
<div class="mono" style="margin-top:8px">尽管人形机器人运动控制领域近期已在多变地形稳定行走方面取得进展，但如何复现高度动态人体动作的敏捷性与适应性仍是开放难题。复杂环境中的敏捷跑酷不仅需要底层鲁棒性，更需类人动作表现力、长时程技能组合及感知驱动决策。本文提出感知型人形机器人跑酷框架——一种模块化系统，使人形机器人能基于视觉自主执行长时程跨障碍跑酷。该方法首先通过特征空间最近邻搜索实现运动匹配，将重定向的原子化人体技能组合为长时程运动轨迹，在保持动态人体动作优雅流畅性的同时，实现复杂技能链的灵活组合与平滑过渡。其次，我们为组合动作训练运动跟踪强化学习专家策略，并融合DAgger与强化学习将其蒸馏为基于深度的多技能学生策略。关键创新在于感知与技能组合的协同实现了自主情境感知决策：机器人仅凭机载深度传感与离散二维速度指令，即可根据障碍物几何形态与高度自主选择并执行跨越、攀爬、撑越或滚落等动作。通过在Unitree G1人形机器人上的大量实体验证，本框架成功展示了攀越1.25米高障碍（相当于机器人身高96%）等高动态跑酷技能，以及能够实时适应障碍扰动的长时程多障碍闭环穿越能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling humanoid robots to perform agile, long-horizon parkour in complex environments, which requires not only robustness but also human-like motion expressiveness and perception-driven decision-making. The proposed method, Perceptive Humanoid Parkour (PHP), first composes retargeted human motion skills into long kinematic trajectories via motion matching, then trains and distills reinforcement learning policies into a single depth-based policy that autonomously selects skills like climbing or vaulting. Experimental results on a Unitree G1 robot demonstrate successful execution of dynamic skills, including climbing obstacles up to 1.25 meters tall and adapting to perturbations in multi-obstacle courses.</div>
<div class="mono" style="margin-top:8px">本文旨在解决人形机器人在复杂环境中执行敏捷、长时程跑酷的挑战，这需要不仅具备鲁棒性，还需人类般的运动表现力和感知驱动决策。所提出的方法——感知人形跑酷（PHP）——首先通过运动匹配将重定向的人类运动技能合成为长时运动轨迹，然后训练并蒸馏强化学习策略为一个基于深度的单一策略，以自主选择如攀爬或跨越等技能。在Unitree G1机器人上的实验结果表明，该方法能成功执行动态技能，包括攀爬高达1.25米的障碍物，并在多障碍物场景中适应实时扰动。</div>
</details>
</div>
<div class="card">
<div class="title">Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning</div>
<div class="meta-line">Authors: Oswin So, Eric Yang Yu, Songyuan Zhang, Matthew Cleaveland, Mitchell Black, Chuchu Fan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-17T18:53:31+00:00 · Latest: 2026-02-17T18:53:31+00:00</div>
<div class="meta-line">Comments: ICLR 2026. The project page can be found at https://oswinso.xyz/fge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15817v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15817v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习解决参数鲁棒性规避问题中的未知可行性</div>
<div class="mono" style="margin-top:8px">深度强化学习（RL）近期在高维控制任务中取得显著进展，但将其应用于可达性问题时存在根本性错配：可达性旨在最大化系统能无限期保持安全的状态集合，而RL则针对用户指定的分布优化期望回报。这种错配可能导致策略在安全集内但概率较低的状态上表现不佳。一种自然的替代方案是将问题构建为对一组指定初始状态、动力学和安全集的初始条件的鲁棒优化，但该问题是否存在解取决于指定集合的可行性——这是先验未知的。我们提出可行性引导探索（FGE）方法，该方法能同时识别存在安全策略的可行初始条件子集，并学习策略以解决该初始条件集合上的可达性问题。实验结果表明，在MuJoCo模拟器和基于像素观测的Kinetix模拟器中，FGE针对挑战性初始条件学得的策略，其覆盖范围比现有最佳方法提高50%以上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the mismatch between deep reinforcement learning (RL), which optimizes expected returns, and reachability problems, which aim to maximize the set of states from which a system remains safe indefinitely. The authors propose Feasibility-Guided Exploration (FGE), a method that concurrently identifies a feasible subset of initial conditions—where a safe policy exists—and learns a policy to solve the reachability problem over this set. Experimental results in MuJoCo and Kinetix simulators show that FGE learns policies achieving over 50% more coverage than the best existing method for challenging initial conditions.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习优化期望回报与可达性问题旨在最大化系统保持安全的初始状态集合之间的不匹配问题，提出了一种可行性引导探索方法。该方法同时识别存在安全策略的可行初始条件子集，并学习在该子集上解决可达性问题的策略。在MuJoCo和Kinetix模拟器中的实验结果表明，对于具有挑战性的初始条件，该方法学习的策略比现有最佳方法覆盖范围提高了50%以上。</div>
</details>
</div>
<div class="card">
<div class="title">Horizon Imagination: Efficient On-Policy Rollout in Diffusion World Models</div>
<div class="meta-line">Authors: Lior Cohen, Ofir Nabati, Kaixin Wang, Navdeep Kumar, Shie Mannor</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-08T16:07:04+00:00 · Latest: 2026-02-17T18:50:34+00:00</div>
<div class="meta-line">Comments: This paper will be published in the ICLR 2026 proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08032v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08032v2">PDF</a> · <a href="https://github.com/leor-c/horizon-imagination">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>地平线想象：扩散世界模型中的高效同策略推演</div>
<div class="mono" style="margin-top:8px">本文研究基于扩散的世界模型在强化学习中的应用，这类模型虽具备高生成保真度，但在控制任务中面临严重的效率挑战。现有方法要么依赖推理时的高复杂度模型，要么采用高度序列化的想象过程，两者均带来难以承受的计算开销。我们提出地平线想象（HI），一种面向离散随机策略的同策略想象过程，能够并行地对多个未来观测进行去噪。HI融合了稳定化机制和一种新颖的采样调度方案，该方案将去噪计算预算与去噪应用的有效时间范围解耦，同时支持子帧级预算。在Atari 100K和Craftium上的实验表明，我们的方法仅用半数去噪步数的子帧预算即可保持控制性能，并在不同调度策略下实现更优的生成质量。代码发布于https://github.com/leor-c/horizon-imagination。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational inefficiency of diffusion-based world models in reinforcement learning, which suffer from high inference costs due to either large models or sequential imagination processes. The authors propose Horizon Imagination (HI), an on-policy method that enables parallel denoising of multiple future observations for discrete stochastic policies, incorporating a stabilization mechanism and a novel sampling schedule to decouple denoising steps from the effective horizon and allow sub-frame budgets. Experimental results on Atari 100K and Craftium demonstrate that HI maintains control performance with only half the denoising steps and achieves superior generation quality across different schedules.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中基于扩散的世界模型计算效率低下的问题，这类模型因需要大型模型或顺序想象过程而面临高昂推理成本。作者提出了地平线想象（HI），这是一种用于离散随机策略的在线策略方法，能够并行去噪多个未来观测，结合了稳定机制和新颖的采样计划，以解耦去噪步骤与有效视野，并支持子帧预算。在Atari 100K和Craftium上的实验结果表明，HI仅用一半的去噪步骤即可保持控制性能，并在不同计划下实现了更优的生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">GLM-5: from Vibe Coding to Agentic Engineering</div>
<div class="meta-line">Authors: GLM-5 Team, :, Aohan Zeng, Xin Lv, Zhenyu Hou, Zhengxiao Du, Qinkai Zheng, Bin Chen, Da Yin, Chendi Ge, Chengxing Xie, Cunxiang Wang, Gengzheng Pan, Hao Zeng, Haoke Zhang, Haoran Wang, Huilong Chen, Jiajie Zhang, Jian Jiao, Jiaqi Guo, Jingsen Wang, Jingzhao Du, Jinzhu Wu, Kedong Wang, Lei Li, Lin Fan, Lucen Zhong, Mingdao Liu, Mingming Zhao, Pengfan Du, Qian Dong, Rui Lu, Shuang-Li, Shulin Cao, Song Liu, Ting Jiang, Xiaodong Chen, Xiaohan Zhang, Xuancheng Huang, Xuezhen Dong, Yabo Xu, Yao Wei, Yifan An, Yilin Niu, Yitong Zhu, Yuanhao Wen, Yukuo Cen, Yushi Bai, Zhongpei Qiao, Zihan Wang, Zikang Wang, Zilin Zhu, Ziqiang Liu, Zixuan Li, Bojie Wang, Bosi Wen, Can Huang, Changpeng Cai, Chao Yu, Chen Li, Chen Li, Chenghua Huang, Chengwei Hu, Chenhui Zhang, Chenzheng Zhu, Congfeng Yin, Daoyan Lin, Dayong Yang, Di Wang, Ding Ai, Erle Zhu, Fangzhou Yi, Feiyu Chen, Guohong Wen, Hailong Sun, Haisha Zhao, Haiyi Hu, Hanchen Zhang, Hanrui Liu, Hanyu Zhang, Hao Peng, Hao Tai, Haobo Zhang, He Liu, Hongwei Wang, Hongxi Yan, Hongyu Ge, Huan Liu, Huan Liu, Huanpeng Chu, Jia&#x27;ni Zhao, Jiachen Wang, Jiajing Zhao, Jiamin Ren, Jiapeng Wang, Jiaxin Zhang, Jiayi Gui, Jiayue Zhao, Jijie Li, Jing An, Jing Li, Jingwei Yuan, Jinhua Du, Jinxin Liu, Junkai Zhi, Junwen Duan, Kaiyue Zhou, Kangjian Wei, Ke Wang, Keyun Luo, Laiqiang Zhang, Leigang Sha, Liang Xu, Lindong Wu, Lintao Ding, Lu Chen, Minghao Li, Nianyi Lin, Pan Ta, Qiang Zou, Rongjun Song, Ruiqi Yang, Shangqing Tu, Shangtong Yang, Shaoxiang Wu, Shengyan Zhang, Shijie Li, Shuang Li, Shuyi Fan, Wei Qin, Wei Tian, Weining Zhang, Wenbo Yu, Wenjie Liang, Xiang Kuang, Xiangmeng Cheng, Xiangyang Li, Xiaoquan Yan, Xiaowei Hu, Xiaoying Ling, Xing Fan, Xingye Xia, Xinyuan Zhang, Xinze Zhang, Xirui Pan, Xunkai Zhang, Yandong Wu, Yanfu Li, Yidong Wang, Yifan Zhu, Yijun Tan, Yilin Zhou, Yiming Pan, Ying Zhang, Yinpei Su, Yipeng Geng, Yipeng Geng, Yong Yan, Yonglin Tan, Yuean Bi, Yuhan Shen, Yuhao Yang, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yurong Wu, Yutao Zhang, Yuxi Duan, Yuxuan Zhang, Zezhen Liu, Zhengtao Jiang, Zhenhe Yan, Zheyu Zhang, Zhixiang Wei, Zhuo Chen, Zhuoer Feng, Zijun Yao, Ziwei Chai, Ziyuan Wang, Zuzhou Zhang, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</div>
<div class="meta-line">First: 2026-02-17T17:50:56+00:00 · Latest: 2026-02-17T17:50:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15763v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15763v1">PDF</a> · <a href="https://github.com/zai-org/GLM-5">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLM-5：从氛围编码到智能体工程</div>
<div class="mono" style="margin-top:8px">我们推出新一代基础模型GLM-5，旨在实现从氛围编码到智能体工程的范式转变。该模型在其前代智能体、推理与编码能力基础上，采用DSA架构显著降低训练与推理成本，同时保持长上下文保真度。为提升模型对齐与自主性，我们构建了新型异步强化学习基础设施，通过解耦生成与训练大幅提升后训练效率。此外，我们提出创新的异步智能体强化学习算法，进一步提升强化学习质量，使模型能更有效地从复杂长程交互中学习。通过这些创新，GLM-5在主流开放基准测试中达到最先进性能。尤为关键的是，GLM-5在实际编程任务中展现出前所未有的能力，在处理端到端软件工程挑战方面超越以往基线。代码、模型及更多信息详见https://github.com/zai-org/GLM-5。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GLM-5, a next-generation foundation model motivated by advancing from vibe coding to agentic engineering, aiming to enhance agentic, reasoning, and coding capabilities. The method employs DSA to reduce training and inference costs while preserving long-context fidelity, alongside a new asynchronous reinforcement learning infrastructure and algorithms that decouple generation from training to improve post-training efficiency and handle complex interactions. Experimental results show GLM-5 achieves state-of-the-art performance on major open benchmarks and demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in end-to-end software engineering challenges.</div>
<div class="mono" style="margin-top:8px">本文介绍了GLM-5，这是一个新一代基础模型，其动机是从氛围编码转向代理工程，旨在提升代理、推理和编码能力。方法上采用DSA以降低训练和推理成本同时保持长上下文保真度，并引入新的异步强化学习基础设施和算法，通过解耦生成与训练来提高后训练效率和处理复杂交互。实验结果表明，GLM-5在主要开放基准测试中实现了最先进的性能，并在现实世界编码任务中展现出前所未有的能力，在端到端软件工程挑战中超越了先前基线。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Gradients for Cumulative Prospect Theory in Reinforcement Learning</div>
<div class="meta-line">Authors: Olivier Lepel, Anas Barakat</div>
<div class="meta-line">First: 2024-10-03T15:45:39+00:00 · Latest: 2026-02-17T17:15:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.02605v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.02605v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We derive a policy gradient theorem for Cumulative Prospect Theory (CPT) objectives in finite-horizon Reinforcement Learning (RL), generalizing the standard policy gradient theorem and encompassing distortion-based risk objectives as special cases. Motivated by behavioral economics, CPT combines an asymmetric utility transformation around a reference point with probability distortion. Building on our theorem, we design a first-order policy gradient algorithm for CPT-RL using a Monte Carlo gradient estimator based on order statistics. We establish statistical guarantees for the estimator and prove asymptotic convergence of the resulting algorithm to first-order stationary points of the (generally non-convex) CPT objective. Simulations illustrate qualitative behaviors induced by CPT and compare our first-order approach to existing zeroth-order methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中累积前景理论的政策梯度方法</div>
<div class="mono" style="margin-top:8px">本文针对有限时域强化学习中的累积前景理论目标，推导了政策梯度定理，该定理推广了标准政策梯度定理，并将基于扭曲的风险目标作为特例涵盖。受行为经济学启发，累积前景理论结合了参考点附近的不对称效用变换与概率扭曲。基于此定理，我们设计了一种基于顺序统计量的蒙特卡洛梯度估计器的一阶政策梯度算法，用于累积前景理论强化学习。我们为该估计器建立了统计保证，并证明了算法对（通常非凸的）累积前景理论目标的一阶稳定点的渐近收敛性。仿真实验展示了累积前景理论诱导的定性行为，并将我们的一阶方法与现有的零阶方法进行了比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by behavioral economics, this paper generalizes policy gradient methods to optimize Cumulative Prospect Theory (CPT) objectives in finite-horizon reinforcement learning, which model risk-sensitive decision-making through asymmetric utility and probability distortion. The method derives a CPT policy gradient theorem and designs a first-order algorithm using a Monte Carlo gradient estimator based on order statistics, with proven statistical guarantees and asymptotic convergence to stationary points. Experimental simulations demonstrate the qualitative behaviors induced by CPT and show that this first-order approach outperforms existing zeroth-order methods.</div>
<div class="mono" style="margin-top:8px">受行为经济学启发，本文推广了策略梯度方法，以优化有限时域强化学习中的累积前景理论目标，该目标通过非对称效用和概率扭曲来建模风险敏感决策。该方法推导了累积前景理论的策略梯度定理，并设计了一种基于顺序统计量的蒙特卡洛梯度估计器的一阶算法，具有统计保证并能渐近收敛至平稳点。实验模拟展示了累积前景理论诱导的定性行为，并表明该一阶方法优于现有的零阶方法。</div>
</details>
</div>
<div class="card">
<div class="title">FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring</div>
<div class="meta-line">Authors: Yousef Emami, Hao Zhou, Miguel Gutierrez Gaitan, Kai Li, Luis Almeida</div>
<div class="meta-line">First: 2025-07-14T10:24:43+00:00 · Latest: 2026-02-17T17:12:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10134v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.10134v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncrewed Aerial Vehicles (UAVs) play a vital role in public safety, especially in monitoring wildfires, where early detection reduces environmental impact. In UAV-Assisted Wildfire Monitoring (UAWM) systems, jointly optimizing the data collection schedule and UAV velocity is essential to minimize the average Age of Information (AoI) for sensory data. Deep Reinforcement Learning (DRL) has been used for this optimization, but its limitations-including low sampling efficiency, discrepancies between simulation and real-world conditions, and complex training make it unsuitable for time-critical applications such as wildfire monitoring. Recent advances in Large Language Models (LLMs) provide a promising alternative. With strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation using natural language prompts and example-based guidance without retraining. This paper proposes a novel online Flight Resource Allocation scheme based on LLM-Enabled In-Context Learning (FRSICL) to jointly optimize the data collection schedule and UAV velocity along the trajectory in real time, thereby asymptotically minimizing the average AoI across all ground sensors. Unlike DRL, FRSICL generates data collection schedules and velocities using natural language task descriptions and feedback from the environment, enabling dynamic decision-making without extensive retraining. Simulation results confirm the effectiveness of FRSICL compared to state-of-the-art baselines, namely Proximal Policy Optimization, Block Coordinate Descent, and Nearest Neighbor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FRSICL：基于大语言模型情境学习的无人机辅助森林火灾监测中实时数据采集的飞行资源分配方案</div>
<div class="mono" style="margin-top:8px">无人机在公共安全领域，特别是森林火灾监测中发挥着关键作用，早期发现能有效减轻环境影响。在无人机辅助火灾监测系统中，联合优化数据采集调度与飞行速度对最小化传感数据的平均信息年龄至关重要。深度强化学习曾用于此类优化，但其采样效率低、仿真与实境差异大、训练复杂等局限使其难以适用于火灾监测等时效敏感场景。大语言模型的最新进展提供了新思路：凭借强大的推理与泛化能力，大语言模型可通过情境学习仅凭自然语言提示和示例指导适应新任务，无需重新训练。本文提出基于大语言模型情境学习的在线飞行资源分配方案，实时联合优化轨迹上的数据采集调度与无人机速度，从而渐进最小化所有地面传感器的平均信息年龄。与深度强化学习不同，该方案通过自然语言任务描述和环境反馈生成调度策略与速度决策，实现动态优化而无需大量重训练。仿真实验表明，相较于近端策略优化、块坐标下降和最近邻等先进基线方法，该方案具有显著优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for timely wildfire monitoring using UAVs, which requires minimizing the Age of Information (AoI) for sensor data, this paper addresses the limitations of Deep Reinforcement Learning (DRL) approaches, such as low sampling efficiency and simulation-to-reality gaps. The method introduces FRSICL, a novel online flight resource allocation scheme that leverages Large Language Models (LLM) with In-Context Learning (ICL) to dynamically optimize data collection schedules and UAV velocities in real-time using natural language prompts and environmental feedback, eliminating the need for extensive retraining. Experimental simulations demonstrate that FRSICL outperforms state-of-the-art baselines like Proximal Policy Optimization, Block Coordinate Descent, and Nearest Neighbor in reducing average AoI, confirming its effectiveness for time-critical applications.</div>
<div class="mono" style="margin-top:8px">本文针对无人机辅助森林火灾监测中需最小化传感器数据信息年龄（AoI）的实时性需求，旨在克服深度强化学习（DRL）方法存在的采样效率低、仿真与真实环境差异大等局限性。方法上提出了FRSICL，一种基于大语言模型（LLM）上下文学习（ICL）的在线飞行资源分配方案，通过自然语言任务描述和环境反馈动态优化数据收集调度与无人机速度，无需大量重新训练即可实现实时决策。实验仿真结果表明，与近端策略优化、块坐标下降和最近邻等先进基线方法相比，FRSICL能有效降低平均AoI，验证了其在时间敏感应用中的优越性能。</div>
</details>
</div>
<div class="card">
<div class="title">MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction</div>
<div class="meta-line">Authors: Qiang Zhang, Jiahao Ma, Peiran Liu, Shuai Shi, Zeran Su, Zifan Wang, Jingkai Sun, Wei Cui, Jialin Yu, Gang Han, Wen Zhao, Pihai Sun, Kangning Yin, Jiaxu Wang, Jiahang Cao, Lingfeng Zhang, Hao Cheng, Xiaoshuai Hao, Yiding Ji, Junwei Liang, Jian Tang, Renjing Xu, Yijie Guo</div>
<div class="meta-line">First: 2026-02-17T17:09:45+00:00 · Latest: 2026-02-17T17:09:45+00:00</div>
<div class="meta-line">Comments: 17 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15733v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled &quot;motion-terrain&quot; interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MeshMimic：通过三维场景重建实现几何感知的人形机器人运动学习</div>
<div class="mono" style="margin-top:8px">近年来，人形机器人运动控制领域取得重大突破，深度强化学习已成为实现复杂拟人行为的主要推动力。然而，人形机器人高维度与复杂动力学的特性使得手动运动设计难以实现，导致高度依赖昂贵的动作捕捉数据。这些数据集不仅采集成本高昂，且常缺乏周围物理环境必要的几何上下文信息。因此，现有运动合成框架常面临运动与场景解耦的问题，导致在感知地形任务中出现接触滑移或网格穿透等物理不一致现象。本研究提出MeshMimic创新框架，通过融合三维场景重建与具身智能技术，使人形机器人能够直接从视频中学习耦合的“运动-地形”交互。借助前沿三维视觉模型，本框架精确分割并重建人体运动轨迹及地形物体的底层三维几何结构。我们提出基于运动学一致性的优化算法，从含噪声的视觉重建中提取高质量运动数据，同时采用接触不变的重定向方法，将人-环境交互特征迁移至人形智能体。实验结果表明，MeshMimic在多样复杂地形中均能实现鲁棒且高度动态的性能。该方法证明，仅需消费级单目传感器的低成本流程即可训练复杂物理交互，为人形机器人在非结构化环境中的自主演进提供了可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for MeshMimic stems from the high cost and geometric limitations of motion capture data for humanoid control, which often leads to physical inconsistencies like slippage or penetration in terrain-aware tasks. The method introduces a framework that bridges 3D scene reconstruction and embodied intelligence, leveraging 3D vision models to segment and reconstruct human trajectories and terrain geometry from video, followed by an optimization algorithm for motion extraction and contact-invariant retargeting to transfer interactions to a humanoid agent. Experimental results show that MeshMimic achieves robust, dynamic performance across diverse terrains, demonstrating that a low-cost pipeline using consumer monocular sensors can effectively train complex physical interactions for autonomous humanoid evolution in unstructured environments.</div>
<div class="mono" style="margin-top:8px">MeshMimic的动机源于人形机器人运动控制中动作捕捉数据成本高昂且缺乏几何环境信息，常导致地形感知任务中出现滑动或穿透等物理不一致问题。该方法提出了一个连接3D场景重建与具身智能的框架，利用先进的3D视觉模型从视频中分割并重建人体轨迹和地形几何，通过基于运动一致性的优化算法从噪声视觉重建中提取高质量运动数据，并采用接触不变的重定向方法将人与环境交互特征迁移至人形智能体。实验结果表明，MeshMimic在多样且具有挑战性的地形上实现了鲁棒、高度动态的性能，证明仅使用消费级单目传感器的低成本流程能有效训练复杂物理交互，为人形机器人在非结构化环境中的自主演进提供了可扩展路径。</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Concept Evolution for Compositional Reasoning in Large Language Models</div>
<div class="meta-line">Authors: Sarim Chaudhry</div>
<div class="meta-line">First: 2026-02-17T17:01:42+00:00 · Latest: 2026-02-17T17:01:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15725v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15725v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model&#x27;s latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中组合推理的递归概念演化</div>
<div class="mono" style="margin-top:8px">大语言模型在众多复杂推理任务中表现优异，但在需要组合推理的基准测试（包括ARC-AGI-2、GPQA、MATH、BBH和HLE）中，其准确性显著下降。现有方法通过思维链提示、自洽性或强化学习扩展词元级搜索以改进推理，但保持模型的潜在表示空间固定。当所需抽象概念未预先编码在该空间中时，性能会急剧下降。我们提出递归概念演化框架，使预训练语言模型能在推理过程中动态调整其内部表示几何结构。该框架引入动态生成的低秩概念子空间：在检测到表示能力不足时生成子空间，通过最小描述长度准则进行筛选，在协同作用时进行融合，并通过约束优化实现稳定性固化。这一过程使模型能够构建新抽象概念而非仅重组现有概念。我们将该框架与Mistral-7B集成，在组合推理基准测试中进行评估。结果显示：在ARC-AGI-2上获得12-18分提升，在GPQA和BBH上实现8-14分改进，并在MATH和HLE上持续降低深度诱导误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of large language models in compositional reasoning tasks, where performance drops despite existing token-level search methods, because the models&#x27; fixed latent representations cannot encode new required abstractions. The authors propose Recursive Concept Evolution (RCE), a framework that dynamically generates low-rank concept subspaces during inference, detected via representational inadequacy, selected with minimum description length, merged synergistically, and consolidated through constrained optimization to build new abstractions while maintaining stability. Experimental results show that integrating RCE with Mistral-7B leads to significant improvements, including 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent error reductions on MATH and HLE.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在组合推理任务中的局限性提出研究，现有基于令牌级搜索的方法因模型潜在表示空间固定而无法编码新抽象，导致性能下降。作者提出递归概念演化框架，通过在推理时动态生成低秩概念子空间，检测表示不足，依据最小描述长度准则选择，协同合并概念，并利用约束优化进行整合以构建新抽象同时保持稳定性。实验结果表明，将RCE与Mistral-7B结合后，在ARC-AGI-2上获得12-18分提升，在GPQA和BBH上提高8-14分，并在MATH和HLE上持续降低深度诱导错误。</div>
</details>
</div>
<div class="card">
<div class="title">On the Role of Iterative Computation in Reinforcement Learning</div>
<div class="meta-line">Authors: Raj Ghugare, Michał Bortkiewicz, Alicja Ziarko, Benjamin Eysenbach</div>
<div class="meta-line">First: 2026-02-05T18:45:57+00:00 · Latest: 2026-02-17T16:47:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05999v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05999v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论迭代计算在强化学习中的作用</div>
<div class="mono" style="margin-top:8px">强化学习策略可用的计算量如何影响其学习效果？使用固定参数量的策略是否仍能受益于额外计算？标准强化学习框架缺乏正式回答这些问题的理论语言。实践中，深度强化学习策略常被参数化为静态架构的神经网络，混淆了计算量与参数数量。本文形式化了计算受限策略，并证明使用更多计算量的策略能够解决计算量较少策略无法处理的问题，并泛化至更长时域的任务。基于算法学习和无模型规划的前期研究，我们提出一种能够使用可变计算量的最小化架构。实验与理论相互印证：在涵盖在线与离线强化学习的31项不同任务中，该架构（1）仅通过增加计算量即可获得更强性能，（2）与使用多达5倍参数的标准前馈网络或深度残差网络相比，在更长时域测试任务中展现出更强的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how the amount of computational resources, distinct from model parameters, influences reinforcement learning (RL) performance and generalization. The authors formalize compute-bounded policies and propose a minimal architecture that can dynamically utilize variable compute, building on concepts from algorithmic learning and model-free planning. Their experiments across 31 online and offline RL tasks demonstrate that this architecture achieves stronger performance with increased compute and better generalization to longer-horizon tasks compared to standard feedforward or deep residual networks, even those with many more parameters.</div>
<div class="mono" style="margin-top:8px">本文研究了计算资源（区别于模型参数）如何影响强化学习的性能和泛化能力。作者形式化了计算受限策略，并基于算法学习和无模型规划的概念，提出了一种能够动态利用可变计算量的最小化架构。在31个在线和离线强化学习任务上的实验表明，该架构通过使用更多计算资源实现了更强的性能，并且在更长视野的测试任务上比标准前馈网络或参数多出5倍的深度残差网络具有更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV</div>
<div class="meta-line">Authors: Yousef Emami, Hao Zhou, Miguel Gutierrez Gaitan, Kai Li, Luis Almeida, Zhu Han</div>
<div class="meta-line">First: 2025-06-03T09:01:33+00:00 · Latest: 2026-02-17T16:45:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.02649v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.02649v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A public safety Uncrewed Aerial Vehicle (UAV) enhances situational awareness during emergency response. Its agility, mobility optimization, and ability to establish Line-of-Sight (LoS) communication make it increasingly important for managing emergencies such as disaster response, search and rescue, and wildfire monitoring. Although Deep Reinforcement Learning (DRL) has been used to optimize UAV navigation and control, its high training complexity, low sample efficiency, and the simulation-to-reality gap limit its practicality in public safety applications. Recent advances in Large Language Models (LLMs) present a promising alternative. With strong reasoning and generalization abilities, LLMs can adapt to new tasks through In-Context Learning (ICL), enabling task adaptation via natural language prompts and example-based guidance without retraining. Deploying LLMs at the network edge, rather than in the cloud, further reduces latency and preserves data privacy, making them suitable for real-time, mission-critical public safety UAVs. This paper proposes integrating LLM-assisted ICL with public safety UAVs to address key functions such as path planning and velocity control in emergency response. We present a case study on data collection scheduling, demonstrating that the LLM-assisted ICL framework can significantly reduce packet loss compared to conventional approaches while also mitigating potential jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and outline future research directions. The ICL framework enables adaptive, context-aware decision-making for public safety UAVs, offering a lightweight and efficient solution to enhance UAV autonomy and responsiveness in emergencies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从提示到防护：基于大语言模型的上下文学习赋能智能公共安全无人机</div>
<div class="mono" style="margin-top:8px">公共安全无人机在应急响应中增强态势感知能力。其敏捷性、移动优化能力及建立视距通信的特性，使其在灾害响应、搜救、野火监测等应急管理中日益重要。尽管深度强化学习已用于优化无人机导航控制，但其训练复杂度高、样本效率低及仿真与现实差距等问题限制了其在公共安全领域的实用性。大语言模型的最新进展提供了有前景的替代方案：凭借强大的推理与泛化能力，大语言模型可通过上下文学习适应新任务，仅需自然语言提示和示例引导即可实现任务适配而无需重新训练。将大语言模型部署于网络边缘而非云端，能进一步降低延迟并保护数据隐私，使其适用于实时关键任务型公共安全无人机。本文提出将大语言模型辅助的上下文学习与公共安全无人机集成，以解决应急响应中的路径规划、速度控制等关键功能。通过数据采集调度的案例研究，证明该框架较传统方法能显著降低丢包率，同时缓解潜在越狱漏洞风险。最后探讨了大语言模型优化器并展望未来研究方向。该上下文学习框架为公共安全无人机提供自适应、情境感知的决策能力，以轻量高效的解决方案提升无人机在紧急情况下的自主性与响应能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of Deep Reinforcement Learning (DRL) in public safety UAVs, such as high training complexity and a simulation-to-reality gap, this paper proposes using Large Language Models (LLMs) with In-Context Learning (ICL) for adaptive decision-making. The method integrates LLM-assisted ICL at the network edge to enable task adaptation via natural language prompts, addressing functions like path planning and velocity control without retraining. In a case study on data collection scheduling, the framework significantly reduces packet loss compared to conventional approaches and mitigates jailbreaking vulnerabilities, demonstrating its potential for lightweight, real-time emergency response.</div>
<div class="mono" style="margin-top:8px">针对深度强化学习在公共安全无人机应用中训练复杂、样本效率低及存在仿真与现实差距等问题，本文提出利用大语言模型（LLM）的上下文学习（ICL）能力实现自适应决策。该方法通过在网络边缘部署LLM辅助的ICL框架，借助自然语言提示进行任务适配，无需重新训练即可处理路径规划与速度控制等关键功能。在数据收集调度的案例研究中，该框架相比传统方法显著降低了数据包丢失率，并缓解了越狱漏洞风险，为紧急情况下的轻量级实时响应提供了高效解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">cadrille: Multi-modal CAD Reconstruction with Reinforcement Learning</div>
<div class="meta-line">Authors: Maksim Kolodiazhnyi, Denis Tarasov, Dmitrii Zhemchuzhnikov, Alexander Nikulin, Ilya Zisman, Anna Vorontsova, Anton Konushin, Vladislav Kurenkov, Danila Rukhovich</div>
<div class="meta-line">Venue: ICLR 2026 Oral</div>
<div class="meta-line">First: 2025-05-28T22:32:31+00:00 · Latest: 2026-02-17T16:31:55+00:00</div>
<div class="meta-line">Comments: ICLR 2026 (Oral)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22914v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.22914v3">PDF</a> · <a href="https://github.com/col14m/cadrille">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one. Code is avaliable at https://github.com/col14m/cadrille .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>cadrille：基于强化学习的多模态CAD重建</div>
<div class="mono" style="margin-top:8px">计算机辅助设计（CAD）在工程与制造领域具有核心地位，能够创建精确且可编辑的三维模型。利用各类传感器或用户提供的数据作为CAD重建的输入，可降低设计应用的使用门槛。然而，现有方法通常仅关注单一输入模态（如点云、图像或文本），限制了其泛化能力与鲁棒性。借助视觉-语言模型（VLM）的最新进展，我们提出一种能同时处理三种输入模态的多模态CAD重建模型。受大语言模型（LLM）训练范式启发，我们采用两阶段流程：首先在大规模程序生成数据上进行监督微调（SFT），随后通过程序化获取的在线反馈进行强化学习（RL）微调。此外，我们首次探索将LLM的RL微调应用于CAD任务，证明在线RL算法（如组相对偏好优化GRPO）优于离线方法。在DeepCAD基准测试中，我们的SFT模型在三种输入模态上均优于现有单模态方法。更重要的是，经过RL微调后，cadrille在三个挑战性数据集（包括真实场景数据集）上取得了最新最优性能。代码发布于https://github.com/col14m/cadrille。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to democratize CAD design through flexible input modalities and to overcome the limitations of single-modal reconstruction methods, this paper introduces cadrille, a multi-modal CAD reconstruction model that processes point clouds, images, and text simultaneously. The method employs a two-stage pipeline: first, supervised fine-tuning on procedurally generated data using a vision-language model backbone, followed by reinforcement learning fine-tuning with online feedback via algorithms like Group Relative Preference Optimization. Experimental results on the DeepCAD benchmark show that the supervised model outperforms existing single-modal approaches across all three input types, and after RL fine-tuning, cadrille achieves state-of-the-art performance on three challenging datasets, including a real-world one.</div>
<div class="mono" style="margin-top:8px">为了通过灵活输入方式普及CAD设计应用，并克服现有单模态重建方法的局限性，本文提出了cadrille，一种能同时处理点云、图像和文本的多模态CAD重建模型。该方法采用两阶段流程：首先基于视觉语言模型主干，在程序生成的大规模数据上进行监督微调，随后通过在线反馈的强化学习微调，采用了如组相对偏好优化等算法。在DeepCAD基准测试中，监督微调模型在所有三种输入模态上均优于现有单模态方法；更重要的是，经过强化学习微调后，cadrille在包括真实世界数据在内的三个挑战性数据集上取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis</div>
<div class="meta-line">Authors: Sara Giordano, Kornikar Sen, Miguel A. Martin-Delgado</div>
<div class="meta-line">Venue: Quantum Mach. Intell. 8, 9 (2026)</div>
<div class="meta-line">First: 2025-07-22T14:39:20+00:00 · Latest: 2026-02-17T15:53:12+00:00</div>
<div class="meta-line">Comments: 35 pages, 7 figures, color figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.16641v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.16641v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A reinforcement learning (RL) framework is introduced for the efficient synthesis of quantum circuits that generate specified target quantum states from a fixed initial state, addressing a central challenge in both the Noisy Intermediate-Scale Quantum (NISQ) era and future fault-tolerant quantum computing. The approach utilizes tabular Q-learning, based on action sequences, within a discretized quantum state space, to effectively manage the exponential growth of the space dimension. The framework introduces a hybrid reward mechanism, combining a static, domain-informed reward that guides the agent toward the target state with customizable dynamic penalties that discourage inefficient circuit structures such as gate congestion and redundant state revisits. This is a circuit-aware reward, in contrast to the current trend of works on this topic, which are primarily fidelity-based. By leveraging sparse matrix representations and state-space discretization, the method enables practical navigation of high-dimensional environments while minimizing computational overhead. Benchmarking on graph-state preparation tasks for up to seven qubits, we demonstrate that the algorithm consistently discovers minimal-depth circuits with optimized gate counts. Moreover, extending the framework to a universal gate set still yields low depth circuits, highlighting the algorithm robustness and adaptability. The results confirm that this RL-driven approach, with our completely circuit-aware method, efficiently explores the complex quantum state space and synthesizes near-optimal quantum circuits, providing a resource-efficient foundation for quantum circuit optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合奖励驱动的强化学习在高效量子电路合成中的应用</div>
<div class="mono" style="margin-top:8px">本文提出一种强化学习框架，用于高效合成从固定初始态生成指定目标量子态的量子电路，以应对含噪声中等规模量子时代及未来容错量子计算的核心挑战。该方法在离散化量子态空间中采用基于动作序列的表格Q学习，有效应对空间维度的指数增长。框架引入混合奖励机制：结合静态领域知识奖励引导智能体趋近目标态，以及可定制的动态惩罚机制抑制低效电路结构（如门拥堵和冗余态重访）。这是一种电路感知的奖励机制，区别于当前该领域主要基于保真度的研究趋势。通过利用稀疏矩阵表示和态空间离散化，本方法能在最小化计算开销的同时实现高维环境的实际导航。在最多七量子比特的图态制备任务上进行基准测试，结果表明该算法能持续发现具有优化门数量的最小深度电路。此外，将框架扩展至通用门集仍能获得低深度电路，突显了算法的鲁棒性与适应性。结果证实，这种采用全电路感知方法的强化学习驱动方案，能高效探索复杂量子态空间并合成近似最优量子电路，为量子电路优化提供了资源高效的基础框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient quantum circuit synthesis in both NISQ and fault-tolerant quantum computing, this paper introduces a reinforcement learning framework using tabular Q-learning within a discretized quantum state space to manage exponential dimensionality. The method employs a hybrid reward mechanism that combines a static, domain-informed reward to guide toward target states with dynamic penalties to discourage inefficient circuit structures like gate congestion and redundant revisits, distinguishing it from fidelity-based approaches. Experimental results on graph-state preparation for up to seven qubits show the algorithm consistently discovers minimal-depth circuits with optimized gate counts, and extension to a universal gate set confirms its robustness and adaptability in synthesizing near-optimal circuits.</div>
<div class="mono" style="margin-top:8px">本文针对嘈杂中型量子（NISQ）时代和未来容错量子计算中量子电路高效合成的核心挑战，提出了一种强化学习框架，采用基于动作序列的表格Q学习，在离散化量子态空间中管理维度指数增长。该方法引入混合奖励机制，结合静态领域知识奖励引导智能体朝向目标态，以及可定制的动态惩罚来抑制门拥堵和状态重复访问等低效电路结构，区别于当前主流的保真度驱动方法。在多达七个量子比特的图态制备任务上的实验结果表明，该算法能一致地发现具有优化门数量的最小深度电路，且扩展到通用门集后仍能生成低深度电路，验证了其鲁棒性和适应性，为量子电路优化提供了资源高效的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Optimization for Design Parameters of 3D Image Data Analysis</div>
<div class="meta-line">Authors: David Exler, Joaquin Eduardo Urrutia Gómez, Martin Krüger, Maike Schliephake, John Jbeily, Mario Vitacolonna, Rüdiger Rudolf, Markus Reischl</div>
<div class="meta-line">First: 2026-02-17T15:31:48+00:00 · Latest: 2026-02-17T15:31:48+00:00</div>
<div class="meta-line">Comments: 10 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15660v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15660v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三维图像数据分析设计参数的贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">基于深度学习的分割与分类技术对大规模生物医学影像分析至关重要，尤其对于人工分析不切实际的三维数据。尽管现有方法众多，但在实践中选择合适的模型并调整参数仍是主要瓶颈。为此，我们提出三维数据分析优化流程，该方法通过两个贝叶斯优化阶段来促进分割与分类的设计与参数化。首先，该流程利用领域适应的合成基准数据集选择分割模型并优化后处理参数。为简明评估分割性能，我们引入作为目标函数的分割质量度量指标。其次，该流程优化分类器的设计选择，包括编码器与分类头架构、先验知识融合及预训练策略。为减少人工标注工作量，此阶段包含辅助类别标注工作流：从分割结果提取预测实例并顺序呈现给操作者，无需人工追踪。在四个案例研究中，三维数据分析优化流程高效地为各数据集确定了有效的模型与参数配置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the practical bottleneck of selecting and tuning models for deep learning-based segmentation and classification in large-scale 3D biomedical imaging, where manual analysis is infeasible. It introduces the 3D data Analysis Optimization Pipeline, which employs a two-stage Bayesian Optimization approach: first, it selects a segmentation model and optimizes postprocessing parameters using a domain-adapted benchmark and a novel segmentation quality metric as the objective function; second, it optimizes classifier design choices like encoder architecture and pretraining strategies, incorporating an assisted class-annotation workflow to reduce manual labeling effort by leveraging segmentation predictions. Experimental results from four case studies demonstrate that the pipeline efficiently identifies effective model and parameter configurations tailored to individual datasets.</div>
<div class="mono" style="margin-top:8px">本文针对大规模三维生物医学成像中深度学习分割与分类模型选择和参数调优这一实际瓶颈问题，提出了一种三维数据分析优化流程。该方法采用两阶段贝叶斯优化：第一阶段利用领域适应的基准数据集和一种新的分割质量度量作为目标函数，选择分割模型并优化后处理参数；第二阶段优化分类器的编码器架构、预训练策略等设计选择，并通过辅助类别标注工作流减少人工标注负担，该工作流利用分割预测结果自动提取实例供操作者审核。在四个案例研究中，该流程能高效地为不同数据集确定有效的模型和参数配置。</div>
</details>
</div>
<div class="card">
<div class="title">Latency-aware Human-in-the-Loop Reinforcement Learning for Semantic Communications</div>
<div class="meta-line">Authors: Peizheng Li, Xinyi Lin, Adnan Aijaz</div>
<div class="meta-line">First: 2026-02-17T15:07:41+00:00 · Latest: 2026-02-17T15:07:41+00:00</div>
<div class="meta-line">Comments: 6 pages, 8 figures. This paper has been accepted for publication in IEEE ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15640v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15640v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic communication promises task-aligned transmission but must reconcile semantic fidelity with stringent latency guarantees in immersive and safety-critical services. This paper introduces a time-constrained human-in-the-loop reinforcement learning (TC-HITL-RL) framework that embeds human feedback, semantic utility, and latency control within a semantic-aware Open radio access network (RAN) architecture. We formulate semantic adaptation driven by human feedback as a constrained Markov decision process (CMDP) whose state captures semantic quality, human preferences, queue slack, and channel dynamics, and solve it via a primal--dual proximal policy optimization algorithm with action shielding and latency-aware reward shaping. The resulting policy preserves PPO-level semantic rewards while tightening the variability of both air-interface and near-real-time RAN intelligent controller processing budgets. Simulations over point-to-multipoint links with heterogeneous deadlines show that TC-HITL-RL consistently meets per-user timing constraints, outperforms baseline schedulers in reward, and stabilizes resource consumption, providing a practical blueprint for latency-aware semantic adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向语义通信的时延感知人机协同强化学习</div>
<div class="mono" style="margin-top:8px">语义通信有望实现任务对齐传输，但在沉浸式与安全关键服务中需兼顾语义保真度与严格时延保障。本文提出一种时延约束人机协同强化学习框架，将人类反馈、语义效用和时延控制嵌入语义感知的开放无线接入网架构。我们将人类反馈驱动的语义适配建模为约束马尔可夫决策过程，其状态涵盖语义质量、用户偏好、队列余量与信道动态，并通过结合动作屏蔽和时延感知奖励塑形的原始-对偶近端策略优化算法求解。所得策略在保持PPO级语义奖励的同时，显著降低了空口接口与近实时RAN智能控制器处理预算的波动性。基于异构截止时间的点对多点链路仿真表明，该框架能持续满足单用户时延约束，在奖励指标上优于基线调度器，并稳定资源消耗，为时延感知语义适配提供了实用技术方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of balancing semantic fidelity with strict latency requirements in semantic communication systems for immersive and safety-critical services. The authors propose a time-constrained human-in-the-loop reinforcement learning (TC-HITL-RL) framework, integrating human feedback, semantic utility, and latency control within a semantic-aware Open RAN architecture. They formulate semantic adaptation as a constrained Markov decision process, solved via a primal-dual proximal policy optimization algorithm with action shielding and latency-aware reward shaping. Experimental simulations on point-to-multipoint links demonstrate that TC-HITL-RL consistently meets per-user timing constraints, outperforms baseline schedulers in reward, and stabilizes resource consumption, offering a practical approach for latency-aware semantic adaptation.</div>
<div class="mono" style="margin-top:8px">本文针对沉浸式和关键安全服务中语义通信系统需平衡语义保真度与严格时延要求的挑战，提出了一种时间约束的人机交互强化学习框架，将人类反馈、语义效用和时延控制集成到语义感知的开放式无线接入网架构中。该方法将语义适应建模为约束马尔可夫决策过程，并通过原始-对偶近端策略优化算法结合动作屏蔽和时延感知奖励塑形进行求解。在点对多点链路上的仿真实验表明，该框架能持续满足用户时延约束，在奖励方面优于基线调度器，并稳定了资源消耗，为时延感知的语义适应提供了实用方案。</div>
</details>
</div>
<div class="card">
<div class="title">STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens</div>
<div class="meta-line">Authors: Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng, Jiang Wu, Yinuo Wang, Yang Guan, Kehua Sheng, Bo Zhang, Keqiang Li, Jingliang Duan, Shengbo Eben Li</div>
<div class="meta-line">First: 2026-02-17T14:46:48+00:00 · Latest: 2026-02-17T14:46:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15620v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15620v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% over GRPO, 20-Entropy and JustRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STAPO：通过抑制罕见伪标记实现大语言模型强化学习的稳定化</div>
<div class="mono" style="margin-top:8px">强化学习显著提升了大语言模型的推理能力，但现有RL微调方法严重依赖熵正则化与权重调整等启发式技术以维持稳定性，实践中常出现后期性能崩溃，导致推理质量下降与训练失稳。我们推导出RL中逐标记策略梯度的幅度与标记概率及局部策略熵呈负相关，并据此证明训练失稳由约0.01%的极少数标记驱动，这类标记被称为伪标记。当伪标记出现在正确响应中时，其对推理结果的贡献微乎其微，却继承了完整的序列级奖励，导致梯度更新异常放大。基于此发现，我们提出面向大规模模型精调的伪标记感知策略优化方法，该方法选择性屏蔽此类更新并对有效标记的损失进行重归一化。在基于Qwen 1.7B、8B和14B基础模型的六项数学推理基准测试中，该方法始终展现出更优的熵稳定性，相较GRPO、20-Entropy和JustRL平均性能提升达7.13%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the instability and late-stage performance collapse observed in existing reinforcement learning fine-tuning methods for large language models, which degrade reasoning quality despite heuristic stabilization techniques. The method introduces Spurious-Token-Aware Policy Optimization (STAPO), which identifies and masks rare spurious tokens—approximately 0.01% of tokens that cause abnormally amplified gradient updates due to inheriting full sequence-level rewards without contributing to reasoning—and renormalizes the loss over valid tokens to stabilize training. Experimental results across six mathematical reasoning benchmarks using Qwen models of 1.7B, 8B, and 14B parameters show that STAPO consistently improves entropy stability and achieves an average performance gain of 7.13% over baseline methods like GRPO, 20-Entropy, and JustRL.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于现有大型语言模型强化学习微调方法存在的不稳定性和后期性能崩溃问题，尽管采用了启发式稳定技术，但仍导致推理质量下降。方法上提出了稀疏令牌感知策略优化，通过识别并屏蔽罕见虚假令牌——这些令牌约占0.01%，会因继承完整序列级奖励而不贡献推理，导致梯度更新异常放大——并对有效令牌的损失进行重归一化以稳定训练。在基于Qwen 1.7B、8B和14B基础模型的六个数学推理基准测试中，实验结果表明该方法能持续提升熵稳定性，相比GRPO、20-Entropy和JustRL等基线方法平均性能提高了7.13%。</div>
</details>
</div>
<div class="card">
<div class="title">Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL</div>
<div class="meta-line">Authors: Julian Lemmel, Felix Resch, Mónika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu</div>
<div class="meta-line">First: 2026-02-02T15:41:53+00:00 · Latest: 2026-02-17T14:36:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02236v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.02236v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents&#x27; performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于实时循环强化学习的自动驾驶预训练控制器在线微调</div>
<div class="mono" style="margin-top:8px">在现实应用中部署预训练策略面临重大挑战，从根本上限制了基于学习的控制系统的实际适用性。当自主系统遭遇系统动力学变化、传感器漂移或任务目标改变时，固定策略的性能会迅速下降。研究表明，采用实时循环强化学习（RTRRL）——一种具有生物合理性的在线适应算法——能够有效微调预训练策略，提升自主智能体在驾驶任务中的表现。进一步研究发现，RTRRL可与近期受生物启发的循环网络模型（液阻-液容循环神经网络）产生协同效应。我们在模拟CarRacing环境及配备事件相机的RoboRacer实车循线任务中，验证了该闭环方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that pretrained policies for autonomous driving degrade when faced with environmental changes, limiting their real-world applicability. The method proposes using Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible online adaptation algorithm, to fine-tune a pretrained policy, and synergizes it with a Liquid-Resistance Liquid-Capacitance recurrent network model. The main experimental results demonstrate the effectiveness of this closed-loop approach in both a simulated CarRacing environment and a real-world line-following task using a RoboRacer car with an event camera.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，自动驾驶的预训练策略在面临环境变化时性能会下降，这限制了其实际应用。方法上，提出使用一种生物启发的在线适应算法——实时循环强化学习（RTRRL）来微调预训练策略，并将其与一种液体电阻-液体电容循环网络模型相结合。主要实验结果表明，这种闭环方法在模拟的CarRacing环境和搭载事件相机的RoboRacer小车实景循线任务中均有效。</div>
</details>
</div>
<div class="card">
<div class="title">Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents</div>
<div class="meta-line">Authors: Davide Paglieri, Bartłomiej Cupiał, Jonathan Cook, Ulyana Piterbarg, Jens Tuyls, Edward Grefenstette, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim Rocktäschel</div>
<div class="meta-line">First: 2025-09-03T18:00:13+00:00 · Latest: 2026-02-17T14:21:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03581v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.03581v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities and highlighting the potential for safer and more collaborative agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习何时规划：为LLM智能体高效分配测试时计算资源</div>
<div class="mono" style="margin-top:8px">通过强化学习训练大语言模型进行推理，能显著提升其问题解决能力。在智能体场景中，现有方法（如ReAct）要求LLM在每次行动前显式规划；但我们证明，持续规划会带来高昂计算成本并损害长周期任务性能，而从不规划则会进一步限制性能。为此，我们提出了一个形式化LLM智能体动态规划的概念框架，使其能灵活决定何时分配测试时计算资源进行规划。我们设计了一个简单的两阶段训练流程：（1）在多样化合成数据上进行监督微调，为动态规划建立基础；（2）在长周期环境中通过强化学习优化该能力。在Crafter环境中的实验表明，采用此方法训练的动态规划智能体具有更高的样本效率，并能持续完成更复杂的目标。此外，这些智能体可有效遵循人工编写的规划方案，超越其独立能力，展现了构建更安全、更具协作性智能体系统的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of always or never planning in LLM agents by introducing a dynamic planning framework that allows agents to decide when to allocate test-time compute for planning. The method involves a two-stage training pipeline: first, supervised fine-tuning on synthetic data to prime models for dynamic planning, and second, reinforcement learning to refine this capability in long-horizon environments. Experimental results in the Crafter environment show that dynamic planning agents are more sample-efficient and achieve more complex objectives, and they can be effectively steered by human-written plans, surpassing independent performance and enabling safer, collaborative systems.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLM）智能体中始终规划或从不规划的低效问题，提出了一个动态规划框架，使智能体能够灵活决定何时分配测试时计算资源进行规划。方法采用两阶段训练流程：首先，在合成数据上进行监督微调以初始化动态规划能力；其次，通过强化学习在长视野环境中优化该能力。在Crafter环境中的实验结果表明，动态规划智能体具有更高的样本效率并能实现更复杂的目标，同时它们能有效受人类编写的规划引导，超越独立性能，展现了更安全、协作的智能体系统潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL</div>
<div class="meta-line">Authors: Yihan Wang, Peiyu Liu, Runyu Chen, Wei Xu</div>
<div class="meta-line">First: 2026-02-17T13:24:56+00:00 · Latest: 2026-02-17T13:24:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15564v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15564v1">PDF</a> · <a href="https://github.com/Satissss/SquRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs&#x27; reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越静态流程：面向文本到SQL的动态工作流学习</div>
<div class="mono" style="margin-top:8px">文本到SQL技术近期取得显著进展，但在实际场景中仍难以有效应用。这一差距源于对单一静态工作流的依赖，从根本上限制了其在分布外和长尾场景的可扩展性。我们尝试让系统在推理时自适应构建工作流，而非要求用户通过大量实验选择合适方法。通过理论与实证分析，我们证明最优动态策略始终优于最佳静态工作流，其性能提升主要由候选工作流间的异质性驱动。基于此，我们提出强化学习框架SquRL，以增强大语言模型在自适应工作流构建中的推理能力。我们设计了基于规则的奖励函数，并引入两种高效训练机制：动态参与者掩码以促进广泛探索，以及伪奖励以提高训练效率。在广泛使用的文本到SQL基准测试中，动态工作流构建始终优于最佳静态工作流方法，在复杂查询和分布外查询上提升尤为显著。代码已开源：https://github.com/Satissss/SquRL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static workflows in real-world Text-to-SQL applications, which struggle with out-of-distribution and long-tail queries, this paper proposes a method for adaptive dynamic workflow construction at inference time. The approach introduces SquRL, a reinforcement learning framework that enhances large language models&#x27; reasoning by using a rule-based reward function and training mechanisms like dynamic actor masking and pseudo rewards for efficient exploration. Experimental results on standard benchmarks show that this dynamic policy consistently outperforms the best static workflows, with significant improvements on complex and out-of-distribution queries.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决文本到SQL任务中静态工作流在现实场景中的局限性，特别是在处理分布外和长尾查询时难以扩展的问题。方法上提出了SquRL，一个基于强化学习的框架，通过设计基于规则的奖励函数以及动态参与者掩码和伪奖励等训练机制，来增强大语言模型在自适应工作流构建中的推理能力。主要实验结果表明，在广泛使用的文本到SQL基准测试中，这种动态工作流构建方法持续优于最佳静态工作流，尤其在复杂和分布外查询上取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning</div>
<div class="meta-line">Authors: Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang, Rui Mao, Erik Cambria</div>
<div class="meta-line">First: 2026-02-02T05:30:42+00:00 · Latest: 2026-02-17T11:19:20+00:00</div>
<div class="meta-line">Comments: 41 pages, 7 figures, 6 tables. Project page: http://flowsteer.org/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01664v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.01664v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowSteer：基于端到端强化学习的交互式智能体工作流编排</div>
<div class="mono" style="margin-top:8px">近年来，多种强大的智能体工作流已被应用于解决广泛的人类问题。然而，现有工作流编排仍面临关键挑战，包括高昂的人工成本、对特定算子/大语言模型（LLM）的依赖，以及稀疏的奖励信号。为应对这些挑战，我们提出FlowSteer——一种端到端强化学习框架，以轻量级策略模型作为智能体，结合可执行画布环境，通过多轮交互实现工作流编排的自动化。在此过程中，策略模型分析执行状态并选择编辑动作，而画布则执行算子并返回反馈以进行迭代优化。此外，FlowSteer提供即插即用框架，支持多样化的算子库与可替换的LLM后端。为有效训练此交互范式，我们提出画布工作流相对策略优化（CWRPO），通过引入带条件释放的多样性约束奖励来稳定学习过程并抑制捷径行为。在十二个数据集上的实验结果表明，FlowSteer在多种任务中显著优于基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces FlowSteer, a framework designed to automate agentic workflow orchestration, motivated by the high manual effort, dependency on specific operators or large language models (LLMs), and sparse rewards in existing methods. It employs an end-to-end reinforcement learning approach where a lightweight policy model interacts with an executable canvas environment, selecting editing actions based on execution states while the canvas runs operators and provides feedback for iterative refinement. The method includes Canvas Workflow Relative Policy Optimization (CWRPO) to stabilize training with diversity-constrained rewards. Experiments on twelve datasets demonstrate that FlowSteer significantly outperforms baseline approaches across various tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了FlowSteer框架，旨在解决现有工作流编排中手动成本高、依赖特定算子或大语言模型（LLM）以及奖励信号稀疏等挑战。该方法采用端到端强化学习，通过轻量级策略模型与可执行画布环境进行多轮交互，策略模型分析执行状态并选择编辑动作，画布则运行算子并返回反馈以迭代优化。此外，FlowSteer支持即插即用的算子库和可互换的LLM后端，并引入了画布工作流相对策略优化（CWRPO）来稳定学习过程。在十二个数据集上的实验结果表明，FlowSteer在多种任务中显著优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">NPG-Muse: Scaling Long Chain-of-Thought Reasoning with NP-Hard Graph Problems</div>
<div class="meta-line">Authors: Yuyao Wang, Bowen Liu, Jianheng Tang, Nuo Chen, Yuhan Li, Qifan Zhang, Chenyi Zi, Chen Zhang, Jia Li</div>
<div class="meta-line">First: 2025-08-28T02:40:27+00:00 · Latest: 2026-02-17T11:02:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20373v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.20373v2">PDF</a> · <a href="https://github.com/littlewyy/NPG-Muse">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are the core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long-CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. The resulting NPG-Muse-series models exhibit substantially enhanced Long CoT reasoning capabilities, achieving consistent gains across mathematics, coding, logical, and graph reasoning benchmarks. NPG-Muse-7B even surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLM post-training. Our implementation is available at https://github.com/littlewyy/NPG-Muse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NPG-Muse：基于NP难图问题扩展长链思维推理能力</div>
<div class="mono" style="margin-top:8px">推理大语言模型（RLLMs）近期在复杂推理任务上取得显著进展，这主要得益于其长链思维（Long CoT）能力。然而，开发此类长链思维行为严重依赖高质量数据集的训练后优化，这类数据集通常成本高昂且需人工标注（如数学与代码领域），导致可扩展的替代方案尚未充分探索。本研究引入NP难（NPH）图问题作为新型合成训练语料，因其本质上需要深度推理、广泛探索与反思策略，这正是长链思维推理的核心特征。基于此洞见，我们开发了两阶段训练后框架：（1）在经拒绝采样的NPH图实例上进行长链思维监督微调（SFT），显著提升推理深度；（2）采用细粒度奖励设计的强化学习（RL），优化推理效率。由此产生的NPG-Muse系列模型展现出显著增强的长链思维推理能力，在数学、编程、逻辑与图推理基准测试中均取得稳定提升。NPG-Muse-7B模型在NP难图问题上甚至超越QwQ-32B模型，在准确率与推理效率上均表现更优。这些成果确立了NP难图问题作为推进大语言模型训练后长链思维推理的有效可扩展资源。项目实现已开源：https://github.com/littlewyy/NPG-Muse。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high cost and limited scalability of human-curated datasets for developing long chain-of-thought (Long CoT) reasoning in large language models. To address this, it introduces NP-hard graph problems as a synthetic training corpus, leveraging their inherent need for deep reasoning and exploration. The method involves a two-stage post-training framework: first, supervised fine-tuning on rejection-sampled graph instances to enhance reasoning depth, followed by reinforcement learning with fine-grained rewards to improve efficiency. Experimental results show that the resulting NPG-Muse models achieve consistent gains across mathematics, coding, logical, and graph reasoning benchmarks, with the 7B variant even surpassing a 32B model on NP-hard graph problems in accuracy and efficiency, demonstrating the effectiveness of this scalable approach.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，为大型语言模型开发长思维链推理能力通常依赖成本高昂且可扩展性有限的人工标注数据集。为解决这一问题，研究引入NP难图问题作为合成训练语料，利用其固有的深度推理和探索需求。方法采用两阶段后训练框架：首先，基于拒绝采样的图实例进行监督微调以增强推理深度；其次，通过细粒度奖励设计的强化学习提升推理效率。实验结果表明，所得NPG-Muse模型在数学、编程、逻辑和图推理基准上均取得稳定提升，其中7B版本在NP难图问题上甚至超越了32B模型的准确性和效率，证明了该方法的可扩展性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">POP: Prior-fitted Optimizer Policies</div>
<div class="meta-line">Authors: Jan Kobiolka, Christian Frey, Gresa Shala, Arlind Kadra, Erind Bedalli, Josif Grabocka</div>
<div class="meta-line">First: 2026-02-17T10:27:07+00:00 · Latest: 2026-02-17T10:27:07+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15473v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POP：先验拟合优化器策略</div>
<div class="mono" style="margin-top:8px">优化任务旨在寻找目标函数的极值点。传统基于梯度的优化器对超参数选择极为敏感，在高度非凸场景中，其性能依赖精心调整的学习率、动量和梯度累积策略。为突破这些局限，我们提出POP（先验拟合优化器策略）——一种元学习优化器，能根据优化轨迹提供的上下文信息预测逐坐标步长。该模型通过从涵盖凸与非凸目标函数的新颖先验分布中采样数百万个合成优化问题进行训练。我们在包含47种不同复杂度优化函数的基准测试中评估POP，在相同计算预算下，其性能持续优于一阶梯度方法、非凸优化方法（如进化策略）、贝叶斯优化及近期元学习竞品。评估结果表明该方法具备无需任务特定调优的强泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the sensitivity of classical gradient-based optimizers to hyperparameter tuning in non-convex optimization by introducing POP, a meta-learned optimizer that predicts coordinate-wise step sizes using contextual information from the optimization trajectory. The method is trained on millions of synthetic problems sampled from a prior covering both convex and non-convex objectives. Experimental results on a benchmark of 47 functions show that POP consistently outperforms first-order gradient methods, non-convex approaches like evolutionary strategies, Bayesian optimization, and a recent meta-learned competitor, demonstrating strong generalization without task-specific tuning.</div>
<div class="mono" style="margin-top:8px">本文针对经典梯度优化器在非凸优化中对超参数调整敏感的问题，提出了POP，一种元学习的优化器，它利用优化轨迹中的上下文信息预测坐标步长。该方法通过在涵盖凸和非凸目标的先验分布上采样数百万合成问题进行训练。在包含47个函数的基准测试中，POP一致优于一阶梯度方法、非凸优化方法（如进化策略）、贝叶斯优化及近期元学习竞争对手，展现了无需任务特定调优的强大泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Reinforcement Learning via Recovery-based Shielding with Gaussian Process Dynamics Models</div>
<div class="meta-line">Authors: Alexander W. Goodall, Francesco Belardinelli</div>
<div class="meta-line">First: 2026-02-12T22:03:35+00:00 · Latest: 2026-02-17T09:48:52+00:00</div>
<div class="meta-line">Comments: Accepted at AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12444v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a powerful framework for optimal decision-making and control but often lacks provable guarantees for safety-critical applications. In this paper, we introduce a novel recovery-based shielding framework that enables safe RL with a provable safety lower bound for unknown and non-linear continuous dynamical systems. The proposed approach integrates a backup policy (shield) with the RL agent, leveraging Gaussian process (GP) based uncertainty quantification to predict potential violations of safety constraints, dynamically recovering to safe trajectories only when necessary. Experience gathered by the &#x27;shielded&#x27; agent is used to construct the GP models, with policy optimization via internal model-based sampling - enabling unrestricted exploration and sample efficient learning, without compromising safety. Empirically our approach demonstrates strong performance and strict safety-compliance on a suite of continuous control environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于高斯过程动力学模型与恢复性屏蔽的安全强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是最优决策与控制的强大框架，但在安全关键应用中常缺乏可证明的安全保障。本文提出一种创新的恢复性屏蔽框架，为未知非线性连续动态系统提供具备可证明安全下界的安全强化学习。该方法将备份策略（屏蔽器）与RL智能体相结合，利用基于高斯过程（GP）的不确定性量化预测安全约束的潜在违反情况，仅在必要时动态恢复至安全轨迹。通过“受屏蔽”智能体收集的经验构建GP模型，并采用基于内部模型的采样策略优化——在保障安全的前提下实现无限制探索与高效样本学习。实验表明，本方法在连续控制环境套件中展现出卓越性能与严格的安全合规性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for provable safety guarantees in reinforcement learning for safety-critical applications, this paper introduces a recovery-based shielding framework that integrates a backup policy with an RL agent, using Gaussian process dynamics models to quantify uncertainty and predict safety violations. The method dynamically recovers to safe trajectories only when necessary, allowing unrestricted exploration while maintaining safety through internal model-based sampling for policy optimization. Experimental results on continuous control environments demonstrate that the approach achieves strong performance and strict safety compliance.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在安全关键应用中缺乏可证明安全保证的问题，提出了一种基于恢复的屏蔽框架，通过集成备份策略与强化学习智能体，利用高斯过程动力学模型量化不确定性并预测安全违规。该方法仅在必要时动态恢复到安全轨迹，允许无限制探索，同时通过基于内部模型的采样进行策略优化以保持安全性。在连续控制环境上的实验结果表明，该方法实现了强劲的性能和严格的安全合规性。</div>
</details>
</div>
<div class="card">
<div class="title">MARS-Sep: Multimodal-Aligned Reinforced Sound Separation</div>
<div class="meta-line">Authors: Zihan Zhang, Xize Cheng, Zhennan Jiang, Dongjie Fu, Jingyuan Chen, Zhou Zhao, Tao Jin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-12T09:05:28+00:00 · Latest: 2026-02-17T09:47:18+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10509v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10509v2">PDF</a> · <a href="https://github.com/mars-sep/MARS-Sep">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://mars-sep.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Universal sound separation faces a fundamental misalignment: models optimized for low-level signal metrics often produce semantically contaminated outputs, failing to suppress perceptually salient interference from acoustically similar sources. We introduce a preference alignment perspective, analogous to aligning LLMs with human intent. To address this, we introduce MARS-Sep, a reinforcement learning framework that reformulates separation as decision making. Instead of simply regressing ground-truth masks, MARS-Sep learns a factorized Beta mask policy that is steered by a preference reward model and optimized by a stable, clipped trust-region surrogate. The reward, derived from a progressively-aligned audio-text-vision encoder, directly incentivizes semantic consistency with query prompts. Extensive experiments on multiple benchmarks demonstrate consistent gains in Text-, Audio-, and Image-Queried separation, with notable improvements in signal metrics and semantic quality. Our code is available at https://github.com/mars-sep/MARS-Sep. Sound separation samples are available at https://mars-sep.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARS-Sep：多模态对齐的强化声音分离方法</div>
<div class="mono" style="margin-top:8px">通用声音分离面临一个根本性错位问题：针对低层信号指标优化的模型常产生语义污染的输出，难以抑制声学相似源产生的感知显著干扰。我们引入一种偏好对齐视角，类似于将大语言模型与人类意图对齐。为此，我们提出MARS-Sep——一个将分离重构为决策过程的强化学习框架。该方法不直接回归真实掩码，而是学习由偏好奖励模型引导、通过稳定裁剪信任域代理优化的因子化Beta掩码策略。奖励源自逐步对齐的音频-文本-视觉编码器，直接激励输出与查询提示的语义一致性。在多个基准测试上的广泛实验表明，该方法在文本查询、音频查询和图像查询的分离任务中均取得稳定提升，信号指标与语义质量均有显著改善。代码发布于https://github.com/mars-sep/MARS-Sep，声音分离样本可访问https://mars-sep.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the misalignment between low-level signal metrics and semantic quality in universal sound separation, where models often fail to suppress perceptually salient interference from similar sources, this paper introduces MARS-Sep, a reinforcement learning framework that reformulates separation as decision-making. The method employs a factorized Beta mask policy steered by a preference reward model derived from a progressively-aligned audio-text-vision encoder, optimized via a stable, clipped trust-region surrogate to directly incentivize semantic consistency with query prompts. Experimental results on multiple benchmarks show consistent improvements in Text-, Audio-, and Image-Queried separation, with gains in both signal metrics and semantic quality.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决通用声音分离中低级信号指标与语义质量之间的错位问题，即模型常无法抑制来自相似声源的感知显著干扰。为此，作者提出了MARS-Sep，这是一个强化学习框架，将分离任务重新定义为决策过程。该方法采用基于逐步对齐的音频-文本-视觉编码器构建的偏好奖励模型，引导因子化Beta掩码策略，并通过稳定的裁剪信任域代理进行优化，以直接激励与查询提示的语义一致性。在多个基准测试上的实验结果表明，该方法在文本、音频和图像查询分离任务中均取得一致提升，信号指标和语义质量均有显著改善。</div>
</details>
</div>
<div class="card">
<div class="title">Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward</div>
<div class="meta-line">Authors: Renjun Xu, Yang Yan</div>
<div class="meta-line">First: 2026-02-12T21:33:25+00:00 · Latest: 2026-02-17T09:08:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12430v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.12430v3">PDF</a> · <a href="https://github.com/scienceaix/agentskills">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The transition from monolithic language models to modular, skill-equipped agents marks a defining shift in how large language models (LLMs) are deployed in practice. Rather than encoding all procedural knowledge within model weights, agent skills -- composable packages of instructions, code, and resources that agents load on demand -- enable dynamic capability extension without retraining. It is formalized in a paradigm of progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP). This survey provides a comprehensive treatment of the agent skills landscape, as it has rapidly evolved during the last few months. We organize the field along four axes: (i) architectural foundations, examining the SKILL$.$md specification, progressive context loading, and the complementary roles of skills and MCP; (ii) skill acquisition, covering reinforcement learning with skill libraries, autonomous skill discovery (SEAgent), and compositional skill synthesis; (iii) deployment at scale, including the computer-use agent (CUA) stack, GUI grounding advances, and benchmark progress on OSWorld and SWE-bench; and (iv) security, where recent empirical analyses reveal that 26.1% of community-contributed skills contain vulnerabilities, motivating our proposed Skill Trust and Lifecycle Governance Framework -- a four-tier, gate-based permission model that maps skill provenance to graduated deployment capabilities. We identify seven open challenges -- from cross-platform skill portability to capability-based permission models -- and propose a research agenda for realizing trustworthy, self-improving skill ecosystems. Unlike prior surveys that broadly cover LLM agents or tool use, this work focuses specifically on the emerging skill abstraction layer and its implications for the next generation of agentic systems. Project repo: https://github.com/scienceaix/agentskills</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型的智能体技能：架构、获取、安全性与发展路径</div>
<div class="mono" style="margin-top:8px">从单一语言模型向模块化、具备技能装备的智能体转变，标志着大语言模型在实际部署方式上的根本性变革。智能体技能——即智能体按需加载的指令、代码和资源的可组合包——无需重新训练即可实现动态能力扩展，取代了将所有程序性知识编码于模型权重中的传统方式。这一范式通过渐进式上下文加载、可移植技能定义以及与模型上下文协议的集成得以形式化。本综述对近几个月快速演进的智能体技能领域进行了全面梳理，围绕四个维度展开：（一）架构基础，剖析SKILL$.$md规范、渐进式上下文加载机制，以及技能与模型上下文协议的互补关系；（二）技能获取，涵盖基于技能库的强化学习、自主技能发现（SEAgent）与组合式技能合成；（三）规模化部署，包括计算机使用智能体技术栈、图形用户界面 grounding 技术进展，以及在OSWorld和SWE-bench基准测试上的进展；（四）安全领域，近期实证分析显示26.1%的社区贡献技能存在漏洞，据此我们提出技能信任与生命周期治理框架——采用四级门控权限模型，将技能溯源映射至分级部署能力。我们提出从跨平台技能可移植性到基于能力的权限模型等七大开放挑战，并构建了实现可信、自演进技能生态系统的研究路线图。与以往广泛涵盖大语言模型智能体或工具使用的研究不同，本文聚焦于新兴的技能抽象层及其对下一代智能体系统的深远影响。项目仓库：https://github.com/scienceaix/agentskills</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey paper is motivated by the shift from monolithic large language models (LLMs) to modular agents equipped with composable, on-demand skills, enabling dynamic capability extension without retraining. The method involves formalizing this paradigm through progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP), organizing the landscape along architectural foundations, skill acquisition, deployment at scale, and security. Key experimental results include benchmark progress on OSWorld and SWE-bench, and a security analysis revealing that 26.1% of community-contributed skills contain vulnerabilities, which motivates the proposed Skill Trust and Lifecycle Governance Framework.</div>
<div class="mono" style="margin-top:8px">本综述论文的动机是从单一的大型语言模型（LLM）向配备可组合、按需加载技能的模块化智能体转变，从而实现无需重新训练的动态能力扩展。其方法是通过渐进式上下文加载、可移植技能定义以及与模型上下文协议（MCP）的集成来形式化这一范式，并从架构基础、技能获取、规模化部署和安全性四个维度梳理该领域。主要的实验结果包括在OSWorld和SWE-bench基准测试上的进展，以及一项安全性分析，该分析揭示26.1%的社区贡献技能存在漏洞，从而推动了所提出的技能信任与生命周期治理框架。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action</div>
<div class="meta-line">Authors: Gong Gao, Weidong Zhao, Xianhui Liu, Ning Jia</div>
<div class="meta-line">Venue: Neural Networks,2026</div>
<div class="meta-line">First: 2026-01-27T15:43:02+00:00 · Latest: 2026-02-17T08:40:07+00:00</div>
<div class="meta-line">Comments: 13pages 11figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19720v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19720v2">PDF</a> · <a href="https://github.com/2706853499/IRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing value-based online reinforcement learning (RL) algorithms suffer from slow policy exploitation due to ineffective exploration and delayed policy updates. To address these challenges, we propose an algorithm called Instant Retrospect Action (IRA). Specifically, we propose Q-Representation Discrepancy Evolution (RDE) to facilitate Q-network representation learning, enabling discriminative representations for neighboring state-action pairs. In addition, we adopt an explicit method to policy constraints by enabling Greedy Action Guidance (GAG). This is achieved through backtracking historical actions, which effectively enhances the policy update process. Our proposed method relies on providing the learning algorithm with accurate $k$-nearest-neighbor action value estimates and learning to design a fast-adaptable policy through policy constraints. We further propose the Instant Policy Update (IPU) mechanism, which enhances policy exploitation by systematically increasing the frequency of policy updates. We further discover that the early-stage training conservatism of the IRA method can alleviate the overestimation bias problem in value-based RL. Experimental results show that IRA can significantly improve the learning efficiency and final performance of online RL algorithms on eight MuJoCo continuous control tasks.The code is available at https://github.com/2706853499/IRA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于即时回溯动作的在线强化学习策略利用改进方法</div>
<div class="mono" style="margin-top:8px">现有基于价值的在线强化学习算法因探索效率低下与策略更新延迟导致策略利用缓慢。为解决这些问题，本文提出即时回溯动作算法。具体而言，我们提出Q表示差异演化机制以促进Q网络表示学习，使相邻状态-动作对获得判别性表示；同时采用贪婪动作引导的显式策略约束方法，通过回溯历史动作有效增强策略更新过程。该方法通过提供精确的k近邻动作价值估计，并学习设计具有快速适应能力的约束策略。进一步提出即时策略更新机制，通过系统增加策略更新频率提升策略利用效率。研究发现IRA方法在训练早期的保守性可缓解基于价值RL中的过高估计偏差问题。实验表明，该算法在八项MuJoCo连续控制任务中能显著提升在线RL算法的学习效率与最终性能。代码发布于https://github.com/2706853499/IRA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the slow policy exploitation and delayed updates in existing online reinforcement learning algorithms, this paper introduces the Instant Retrospect Action (IRA) method to enhance learning efficiency. The method employs Q-Representation Discrepancy Evolution (RDE) to learn discriminative representations for state-action pairs and incorporates Greedy Action Guidance (GAG) via historical action backtracking for explicit policy constraints, alongside an Instant Policy Update (IPU) mechanism to increase policy update frequency. Experimental results on eight MuJoCo continuous control tasks demonstrate that IRA significantly improves both learning efficiency and final performance, while also alleviating overestimation bias during early training stages.</div>
<div class="mono" style="margin-top:8px">针对现有在线强化学习算法中策略利用缓慢和更新延迟的问题，本文提出了即时回溯行动（IRA）方法以提升学习效率。该方法采用Q表示差异演化（RDE）来学习状态-动作对的区分性表示，并通过回溯历史动作引入贪婪行动指导（GAG）实现显式策略约束，同时结合即时策略更新（IPU）机制提高策略更新频率。在八个MuJoCo连续控制任务上的实验结果表明，IRA能显著提升学习效率和最终性能，并在早期训练阶段缓解了值估计过高偏差问题。</div>
</details>
</div>
<div class="card">
<div class="title">Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting</div>
<div class="meta-line">Authors: Md Muhtasim Munif Fahim, Soyda Humyra Yesmin, Saiful Islam, Md. Palash Bin Faruque, Md. A. Salam, Md. Mahfuz Uddin, Samiul Islam, Tofayel Ahmed, Md. Binyamin, Md. Rezaul Karim</div>
<div class="meta-line">First: 2026-01-30T19:03:21+00:00 · Latest: 2026-02-17T08:06:08+00:00</div>
<div class="meta-line">Comments: Accepted at the 2026 IEEE 2nd International Conference on Quantum Photonics, Artificial Intelligence &amp; Networking</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00240v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00240v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to &#x27;Green AI&#x27; principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Green-NAS：面向稳健高效边缘原生天气预报的全球尺度多目标神经架构搜索</div>
<div class="mono" style="margin-top:8px">本文提出Green-NAS——一个以天气预报为案例研究、面向低资源环境设计的多目标神经架构搜索框架。该框架遵循&#x27;绿色人工智能&#x27;原则，明确最小化计算能耗与碳足迹，将可持续部署置于原始计算规模之上。Green-NAS架构搜索方法针对模型精度与效率进行协同优化，通过同步优化多目标的流程，寻找参数量极少的高精度轻量化模型。我们性能最优的模型Green-NAS-A仅使用15.3万参数即达到0.0988的均方根误差（较人工调优基线误差仅1.4%），参数量比GraphCast等全球应用的天气预报模型少239倍。此外，我们还论证了在目标城市历史气象数据有限时，采用迁移学习相比为每个城市单独训练新模型的朴素方法，可将天气预报准确率提升约5.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for sustainable and efficient AI in low-resource edge environments, this paper introduces Green-NAS, a multi-objective neural architecture search framework that prioritizes minimizing computational energy and carbon footprint while maintaining accuracy for weather forecasting. The method employs a multi-objective optimization process to simultaneously search for models that are both accurate and highly parameter-efficient. Key experimental results show that the best model, Green-NAS-A, achieves an RMSE of 0.0988 (within 1.4% of a manual baseline) with only 153k parameters—239 times fewer than models like GraphCast—and that transfer learning can improve forecasting accuracy by approximately 5.2% in data-scarce scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在为资源受限的边缘环境开发可持续、高效的人工智能，提出了Green-NAS，这是一个多目标神经架构搜索框架，其动机是遵循“绿色AI”原则，在保持天气预测精度的同时，显式最小化计算能耗与碳足迹。该方法通过多目标优化过程，同步搜索高精度与极低参数量的轻量级模型。主要实验结果表明，最佳模型Green-NAS-A仅用15.3万个参数就达到了0.0988的均方根误差（与人工基准模型相差1.4%以内），参数量比GraphCast等全球应用模型少239倍；此外，迁移学习在历史数据有限的城市中，可将预测精度提升约5.2%。</div>
</details>
</div>
<div class="card">
<div class="title">SR-Scientist: Scientific Equation Discovery With Agentic AI</div>
<div class="meta-line">Authors: Shijie Xia, Yuhan Sun, Pengfei Liu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-13T17:35:23+00:00 · Latest: 2026-02-17T07:50:43+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11661v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11661v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Large Language Models (LLMs) have been applied to scientific equation discovery, leveraging their embedded scientific knowledge for hypothesis generation. However, current methods typically confine LLMs to the role of an equation proposer within search algorithms like genetic programming. In this paper, we present SR-Scientist, a framework that elevates the LLM from a simple equation proposer to an autonomous AI scientist that writes code to analyze data, implements the equation as code, submits it for evaluation, and optimizes the equation based on experimental feedback. Specifically, we wrap the code interpreter into a set of tools for data analysis and equation evaluation. The agent is instructed to optimize the equation by utilizing these tools over a long horizon with minimal human-defined pipelines. Empirical results show that SR-Scientist outperforms baseline methods by an absolute margin of 6% to 35% on datasets covering four science disciplines. Additionally, we demonstrate our method&#x27;s robustness to noise, the generalization of the discovered equations to out-of-domain data, and their symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning framework to enhance the agent&#x27;s capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SR-Scientist：基于智能体AI的科学方程发现</div>
<div class="mono" style="margin-top:8px">近期，大语言模型（LLMs）被应用于科学方程发现，利用其内嵌的科学知识进行假设生成。然而，现有方法通常将LLMs局限于遗传编程等搜索算法中的方程提议者角色。本文提出SR-Scientist框架，将LLM从简单的方程提议者提升为自主AI科学家：编写代码分析数据、将方程实现为代码、提交评估，并根据实验反馈优化方程。具体而言，我们将代码解释器封装为一套数据分析与方程评估工具，指导智能体在最小化人工流程干预下，长期利用这些工具优化方程。实验结果表明，在覆盖四个科学领域的数据集上，SR-Scientist以6%至35%的绝对优势超越基线方法。此外，我们验证了该方法对噪声的鲁棒性、所发现方程在域外数据上的泛化能力及其符号准确性。进一步，我们开发了端到端强化学习框架以增强智能体能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of current LLMs being confined as mere equation proposers in scientific discovery, this paper introduces SR-Scientist, a framework that elevates an LLM to an autonomous AI scientist. The method equips the agent with code interpreter tools for data analysis and equation evaluation, enabling it to write code, test hypotheses, and iteratively optimize equations through experimental feedback with minimal predefined pipelines. Experimental results demonstrate that SR-Scientist outperforms baseline methods by 6% to 35% across four scientific disciplines, showing robustness to noise, generalization to out-of-domain data, and high symbolic accuracy, with an end-to-end reinforcement learning framework further enhancing agent capabilities.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有方法通常将大语言模型局限于作为遗传编程等搜索算法中的方程提议者。为此，论文提出了SR-Scientist框架，将大语言模型提升为自主的AI科学家，其方法是为智能体配备代码解释器工具，用于数据分析和方程评估，使其能够编写代码、测试假设，并通过实验反馈以最小化人工流程的方式迭代优化方程。主要实验结果表明，SR-Scientist在覆盖四个科学领域的数据集上比基线方法绝对提升了6%至35%，并展现出对噪声的鲁棒性、对域外数据的泛化能力以及较高的符号准确性，同时通过端到端强化学习框架进一步增强了智能体的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Fairness over Equality: Correcting Social Incentives in Asymmetric Sequential Social Dilemmas</div>
<div class="meta-line">Authors: Alper Demir, Hüseyin Aydın, Kale-ab Abebe Tessera, David Abel, Stefano V. Albrecht</div>
<div class="meta-line">First: 2026-02-17T07:31:20+00:00 · Latest: 2026-02-17T07:31:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15407v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15407v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sequential Social Dilemmas (SSDs) provide a key framework for studying how cooperation emerges when individual incentives conflict with collective welfare. In Multi-Agent Reinforcement Learning, these problems are often addressed by incorporating intrinsic drives that encourage prosocial or fair behavior. However, most existing methods assume that agents face identical incentives in the dilemma and require continuous access to global information about other agents to assess fairness. In this work, we introduce asymmetric variants of well-known SSD environments and examine how natural differences between agents influence cooperation dynamics. Our findings reveal that existing fairness-based methods struggle to adapt under asymmetric conditions by enforcing raw equality that wrongfully incentivize defection. To address this, we propose three modifications: (i) redefining fairness by accounting for agents&#x27; reward ranges, (ii) introducing an agent-based weighting mechanism to better handle inherent asymmetries, and (iii) localizing social feedback to make the methods effective under partial observability without requiring global information sharing. Experimental results show that in asymmetric scenarios, our method fosters faster emergence of cooperative policies compared to existing approaches, without sacrificing scalability or practicality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>公平优于平等：非对称序贯社会困境中的社会激励修正</div>
<div class="mono" style="margin-top:8px">序贯社会困境为研究个体激励与集体利益冲突时合作如何形成提供了关键框架。在多智能体强化学习中，通常通过引入鼓励亲社会或公平行为的内在驱动力来解决这类问题。然而，现有方法大多假设智能体在困境中面临相同的激励，且需要持续获取其他智能体的全局信息以评估公平性。本研究在经典序贯社会困境环境中引入非对称变体，探究智能体间的自然差异如何影响合作动态。研究发现，现有基于公平的方法在非对称条件下难以适应，因其强制推行原始平等反而错误激励了背叛行为。为此，我们提出三项改进：（1）通过考虑智能体奖励范围重新定义公平性；（2）引入基于智能体的加权机制以更好处理固有不对称性；（3）将社会反馈局部化，使方法在部分可观测条件下无需全局信息共享仍能有效运作。实验结果表明，在非对称场景中，我们的方法相较于现有方案能更快催生合作策略，且不牺牲可扩展性或实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fostering cooperation in Sequential Social Dilemmas (SSDs) when agents face asymmetric incentives, a scenario overlooked by existing fairness-based methods that assume identical conditions and rely on global information. The authors propose three key modifications: redefining fairness to account for individual reward ranges, introducing an agent-based weighting mechanism to handle inherent asymmetries, and localizing social feedback to operate under partial observability without global sharing. Experimental results demonstrate that this approach enables faster emergence of cooperative policies in asymmetric environments compared to prior methods, while maintaining scalability and practicality.</div>
<div class="mono" style="margin-top:8px">本文针对顺序社会困境中智能体面临不对称激励时如何促进合作的挑战，现有基于公平的方法通常假设条件相同且依赖全局信息，难以适应此类场景。作者提出三项关键改进：根据智能体奖励范围重新定义公平性，引入基于智能体的加权机制以处理固有不对称性，以及局部化社会反馈使其在部分可观测下无需全局信息即可运作。实验结果表明，在不对称环境中，该方法相比现有方法能更快地促成合作策略的出现，同时保持了可扩展性和实用性。</div>
</details>
</div>
<div class="card">
<div class="title">General Exploratory Bonus for Optimistic Exploration in RLHF</div>
<div class="meta-line">Authors: Wendi Li, Changdae Oh, Sharon Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-27T04:54:59+00:00 · Latest: 2026-02-17T07:26:18+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03269v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.03269v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $α$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $α$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLHF中乐观探索的通用探索奖励</div>
<div class="mono" style="margin-top:8px">乐观探索对于提升基于人类反馈的强化学习样本效率至关重要，但现有激励探索的奖励方法常未能实现乐观性。理论分析表明，当前基于KL散度或α-散度正则化的方法会无意中将探索偏向参考模型的高概率区域，从而强化保守行为而非促进对不确定区域的发现。为解决此缺陷，本文提出通用探索奖励（GEB）这一新型理论框架，可证明其满足乐观原则。GEB通过参考依赖的奖励调节抵消散度诱导的偏差，将先验启发式奖励统一为特例，并自然扩展至完整α-散度族。实证表明，GEB在多种散度设置和大语言模型基座上的一致性对齐任务中均优于基线方法。这些结果证明GEB为RLHF中的乐观探索提供了兼具原则性与实用性的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that existing exploratory bonus methods in reinforcement learning with human feedback (RLHF) fail to achieve optimistic exploration, as they unintentionally bias exploration toward high-probability regions of the reference model, reinforcing conservative behavior. To address this, the authors propose the General Exploratory Bonus (GEB), a theoretical framework that counteracts divergence-induced bias through reference-dependent reward regulation, unifying prior heuristic bonuses and extending across the α-divergence family. Experimental results show that GEB consistently outperforms baseline methods on alignment tasks across various divergence settings and large language model backbones, demonstrating its effectiveness as a principled and practical solution for optimistic exploration in RLHF.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到，在基于人类反馈的强化学习中，现有的探索奖励方法未能实现乐观探索，因为它们无意中将探索偏向参考模型的高概率区域，从而强化了保守行为。为解决这一问题，作者提出了通用探索奖励（GEB）这一理论框架，它通过参考依赖的奖励调节来抵消散度引起的偏差，统一了先前的启发式奖励方法，并自然扩展到完整的α-散度族。实验结果表明，在不同的散度设置和大语言模型骨干上，GEB在对齐任务中始终优于基线方法，证明了其作为RLHF中乐观探索的一种原则性且实用的解决方案的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control</div>
<div class="meta-line">Authors: Ehsan Futuhi, Shayan Karimi, Chao Gao, Martin Müller</div>
<div class="meta-line">First: 2024-10-07T17:31:52+00:00 · Latest: 2026-02-17T07:23:17+00:00</div>
<div class="meta-line">Comments: We have expanded the related work section with more detailed discussions and enhanced our experiments by incorporating additional data and analysis</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.05225v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.05225v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider deep deterministic policy gradient (DDPG) in the context of reinforcement learning with sparse rewards. To enhance exploration, we introduce a search procedure, \emph{$ε{t}$-greedy}, which generates exploratory options for exploring less-visited states. We prove that search using $εt$-greedy has polynomial sample complexity under mild MDP assumptions. To more efficiently use the information provided by rewarded transitions, we develop a new dual experience replay buffer framework, \emph{GDRB}, and implement \emph{longest n-step returns}. The resulting algorithm, \emph{ETGL-DDPG}, integrates all three techniques: \bm{$εt$}-greedy, \textbf{G}DRB, and \textbf{L}ongest $n$-step, into DDPG. We evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms DDPG, as well as other state-of-the-art methods, across all tested sparse-reward continuous environments. Ablation studies further highlight how each strategy individually enhances the performance of DDPG in this setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ETGL-DDPG：一种面向稀疏奖励连续控制的深度确定性策略梯度算法</div>
<div class="mono" style="margin-top:8px">本文研究稀疏奖励强化学习场景下的深度确定性策略梯度（DDPG）算法。为增强探索能力，我们引入一种搜索策略——εt-贪婪搜索，该策略通过生成探索性选项来访问较少到达的状态。我们证明在温和的马尔可夫决策过程假设下，εt-贪婪搜索具有多项式级样本复杂度。为更高效利用奖励转移提供的信息，我们开发了新型双重经验回放缓冲框架（GDRB），并实现了最长n步回报机制。最终算法ETGL-DDPG将三项技术——εt-贪婪搜索、GDRB框架与最长n步回报——集成至DDPG架构。通过在标准测试环境中的评估，ETGL-DDPG在所有稀疏奖励连续控制任务中均优于DDPG及其他前沿方法。消融实验进一步揭示了各项策略对DDPG性能的独立提升作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of sparse rewards in continuous control tasks by enhancing the Deep Deterministic Policy Gradient (DDPG) algorithm. The motivation is to improve exploration and data efficiency in such environments. The method introduces three key techniques: an εt-greedy search procedure for generating exploratory options to visit less-visited states, a dual experience replay buffer framework called GDRB to better utilize rewarded transitions, and longest n-step returns for more effective learning. Experimental results on standard benchmarks show that the integrated algorithm, ETGL-DDPG, outperforms DDPG and other state-of-the-art methods across all tested sparse-reward continuous environments, with ablation studies confirming the individual contributions of each strategy.</div>
<div class="mono" style="margin-top:8px">本文针对连续控制任务中奖励稀疏的挑战，改进了深度确定性策略梯度（DDPG）算法，旨在提升此类环境中的探索能力和数据效率。方法上引入了三项关键技术：εt-greedy搜索过程以生成探索选项访问较少到达的状态，称为GDRB的双重经验回放缓冲框架以更有效地利用奖励转移，以及最长n步回报以优化学习。在标准基准测试中的实验结果表明，整合这些技术的ETGL-DDPG算法在所有测试的稀疏奖励连续环境中均优于DDPG及其他先进方法，消融研究进一步验证了每种策略对性能的单独提升作用。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Deep Reinforcement Learning against Adversarial Behavior Manipulation</div>
<div class="meta-line">Authors: Shojiro Yamabe, Kazuto Fukuchi, Jun Sakuma</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2024-06-06T08:49:51+00:00 · Latest: 2026-02-17T05:50:40+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.03862v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.03862v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study investigates behavior-targeted attacks on reinforcement learning and their countermeasures. Behavior-targeted attacks aim to manipulate the victim&#x27;s behavior as desired by the adversary through adversarial interventions in state observations. Existing behavior-targeted attacks have some limitations, such as requiring white-box access to the victim&#x27;s policy. To address this, we propose a novel attack method using imitation learning from adversarial demonstrations, which works under limited access to the victim&#x27;s policy and is environment-agnostic. In addition, our theoretical analysis proves that the policy&#x27;s sensitivity to state changes impacts defense performance, particularly in the early stages of the trajectory. Based on this insight, we propose time-discounted regularization, which enhances robustness against attacks while maintaining task performance. To the best of our knowledge, this is the first defense strategy specifically designed for behavior-targeted attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对对抗性行为操纵的鲁棒深度强化学习</div>
<div class="mono" style="margin-top:8px">本研究探讨针对强化学习的行为导向攻击及其防御策略。行为导向攻击旨在通过对状态观测施加对抗性干预，使受害者的行为按攻击者意图发生改变。现有攻击方法存在局限，如需白盒访问受害者策略。为此，我们提出一种基于对抗性演示模仿学习的新型攻击方法，该方法在有限策略访问条件下生效且与环境无关。理论分析证明，策略对状态变化的敏感性影响防御效果，尤其在轨迹早期阶段。基于此发现，我们提出时间折扣正则化方法，在保持任务性能的同时提升对攻击的鲁棒性。据我们所知，这是首个专为行为导向攻击设计的防御策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of deep reinforcement learning to behavior-targeted adversarial attacks, which manipulate an agent&#x27;s actions by perturbing its state observations. To overcome limitations of prior attacks that require full policy access, the authors propose a novel black-box attack method using imitation learning from adversarial demonstrations, which is effective with limited policy knowledge and across different environments. Theoretically, they link a policy&#x27;s sensitivity to state changes to defense efficacy, leading to the development of a time-discounted regularization defense that improves robustness against such attacks while preserving task performance, representing the first dedicated defense against behavior-targeted attacks.</div>
<div class="mono" style="margin-top:8px">本文研究了深度强化学习面临的行为目标对抗攻击的脆弱性，这类攻击通过扰动状态观测来操控智能体的行为。针对现有攻击方法需要完全访问策略等限制，作者提出了一种基于对抗演示模仿学习的黑盒攻击方法，该方法在有限策略知识下有效且与环境无关。理论分析表明策略对状态变化的敏感性影响防御效果，据此作者提出了时间折扣正则化防御方法，在保持任务性能的同时增强了对攻击的鲁棒性，这是首个专门针对行为目标攻击的防御策略。</div>
</details>
</div>
<div class="card">
<div class="title">CDRL: A Reinforcement Learning Framework Inspired by Cerebellar Circuits and Dendritic Computational Strategies</div>
<div class="meta-line">Authors: Sibo Zhang, Rui Jing, Liangfu Lv, Jian Zhang, Yunliang Zang</div>
<div class="meta-line">First: 2026-02-17T05:25:09+00:00 · Latest: 2026-02-17T05:25:09+00:00</div>
<div class="meta-line">Comments: 14pages, 8 figures, 6 tabels</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15367v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15367v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has achieved notable performance in high-dimensional sequential decision-making tasks, yet remains limited by low sample efficiency, sensitivity to noise, and weak generalization under partial observability. Most existing approaches address these issues primarily through optimization strategies, while the role of architectural priors in shaping representation learning and decision dynamics is less explored. Inspired by structural principles of the cerebellum, we propose a biologically grounded RL architecture that incorporate large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. Experiments on noisy, high-dimensional RL benchmarks show that both the cerebellar architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to conventional designs. Sensitivity analysis of architectural parameters suggests that cerebellum-inspired structures can offer optimized performance for RL with constrained model parameters. Overall, our work underscores the value of cerebellar structural priors as effective inductive biases for RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CDRL：受小脑回路与树突计算策略启发的强化学习框架</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在高维序列决策任务中已取得显著性能，但仍受限于样本效率低、噪声敏感性强及部分可观测条件下泛化能力弱等问题。现有方法主要通过优化策略应对这些挑战，而架构先验在表征学习与决策动态中的作用尚未充分探索。受小脑结构原理启发，我们提出一种基于生物机制的强化学习架构，融合大规模扩展、稀疏连接、稀疏激活及树突层级调制机制。在含噪声的高维强化学习基准测试中，小脑架构与树突调制均较传统设计持续提升样本效率、鲁棒性与泛化能力。架构参数的敏感性分析表明，受小脑启发的结构能以受限模型参数为强化学习提供优化性能。总体而言，本研究揭示了小脑结构先验作为强化学习有效归纳偏置的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of conventional reinforcement learning (RL) in sample efficiency, noise sensitivity, and generalization under partial observability, this paper introduces a biologically inspired RL framework that incorporates architectural principles from the cerebellum, including large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. The method involves designing an RL architecture grounded in these cerebellar structural priors to shape representation learning and decision dynamics. Experimental results on noisy, high-dimensional RL benchmarks demonstrate that this approach consistently improves sample efficiency, robustness, and generalization compared to standard designs, with sensitivity analysis indicating that cerebellum-inspired structures can optimize performance under constrained model parameters.</div>
<div class="mono" style="margin-top:8px">本文针对传统强化学习在样本效率、噪声敏感性和部分可观测性下泛化能力方面的局限性，提出了一种受小脑结构启发的生物启发强化学习框架，该框架融合了大规模扩展、稀疏连接、稀疏激活和树突级调制等小脑计算策略。方法上，基于这些小脑结构先验设计强化学习架构，以塑造表征学习和决策动态。在噪声高维强化学习基准测试中的实验结果表明，相较于传统设计，该框架能持续提升样本效率、鲁棒性和泛化能力，敏感性分析进一步表明小脑启发的结构可在受限模型参数下实现性能优化。</div>
</details>
</div>
<div class="card">
<div class="title">MyoInteract: A Framework for Fast Prototyping of Biomechanical HCI Tasks using Reinforcement Learning</div>
<div class="meta-line">Authors: Ankit Bhattarai, Hannah Selder, Florian Fischer, Arthur Fleig, Per Ola Kristensson</div>
<div class="meta-line">First: 2026-02-16T22:51:57+00:00 · Latest: 2026-02-16T22:51:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15245v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15245v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL)-based biomechanical simulations have the potential to revolutionise HCI research and interaction design, but currently lack usability and interpretability. Using the Human Action Cycle as a design lens, we identify key limitations of biomechanical RL frameworks and develop MyoInteract, a novel framework for fast prototyping of biomechanical HCI tasks. MyoInteract allows designers to setup tasks, user models, and training parameters from an easy-to-use GUI within minutes. It trains and evaluates muscle-actuated simulated users within minutes, reducing training times by up to 98%. A workshop study with 12 interaction designers revealed that MyoInteract allowed novices in biomechanical RL to successfully setup, train, and assess goal-directed user movements within a single session. By transforming biomechanical RL from a days-long expert task into an accessible hour-long workflow, this work significantly lowers barriers to entry and accelerates iteration cycles in HCI biomechanics research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MyoInteract：基于强化学习的生物力学人机交互任务快速原型设计框架</div>
<div class="mono" style="margin-top:8px">基于强化学习（RL）的生物力学模拟有望革新人机交互（HCI）研究与交互设计，但目前存在可用性与可解释性不足的问题。本研究以人类行为循环为设计视角，剖析了现有生物力学RL框架的核心局限，进而提出MyoInteract——一个面向生物力学HCI任务快速原型设计的新型框架。该框架允许设计者通过易用的图形界面在数分钟内完成任务配置、用户模型构建及训练参数设置，并能在数分钟内完成肌肉驱动模拟用户的训练与评估，将训练时间最高缩短98%。一项包含12名交互设计师的研讨研究表明，即使生物力学RL零基础的新手也能在单次会话中成功配置、训练并评估目标导向的用户动作。本工作将生物力学RL从耗时数日的专家级任务转化为可在一小时内完成的标准化流程，显著降低了HCI生物力学研究的入门门槛并加速了迭代周期。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to make reinforcement learning (RL)-based biomechanical simulations more usable and interpretable for HCI research, this paper introduces MyoInteract, a framework for fast prototyping of biomechanical interaction tasks. The method involves an easy-to-use GUI that allows designers to quickly configure tasks, user models, and training parameters, enabling the training and evaluation of muscle-actuated simulated users within minutes, which reduces training times by up to 98%. Experimental results from a workshop with 12 interaction designers showed that novices could successfully set up, train, and assess goal-directed user movements in a single session, transforming a previously expert-driven, days-long process into an accessible hour-long workflow.</div>
<div class="mono" style="margin-top:8px">本文旨在解决基于强化学习的生物力学模拟在人机交互研究中可用性和可解释性不足的问题，为此提出了MyoInteract框架，用于快速原型化生物力学交互任务。该方法通过易于使用的图形界面，允许设计者在几分钟内配置任务、用户模型和训练参数，从而在几分钟内训练和评估肌肉驱动的模拟用户，将训练时间减少高达98%。一项有12名交互设计师参与的研讨会实验结果表明，即使是新手也能在单次会话中成功设置、训练和评估目标导向的用户运动，将原本需要数天的专家任务转变为可在一小时内完成的工作流程。</div>
</details>
</div>
<div class="card">
<div class="title">Individualized Federated Learning for Traffic Prediction with Error Driven Aggregation</div>
<div class="meta-line">Authors: Hang Chen, Collin Meese, Mark Nejad, Chien-Chung Shen</div>
<div class="meta-line">First: 2024-07-17T00:42:47+00:00 · Latest: 2026-02-16T21:11:24+00:00</div>
<div class="meta-line">Comments: 30 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.12226v2">Abs</a> · <a href="https://arxiv.org/pdf/2407.12226v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-latency traffic prediction is vital for smart city traffic management. Federated Learning has emerged as a promising technique for Traffic Prediction (FLTP), offering several advantages such as privacy preservation, reduced communication overhead, improved prediction accuracy, and enhanced adaptability to changing traffic conditions. However, majority of the current FLTP frameworks lack a real-time model updating scheme, which hinders their ability to continuously incorporate new incoming traffic data and adapt effectively to the changing dynamics of traffic trends. Another concern with the existing FLTP frameworks is their reliance on the conventional FL model aggregation method, which involves assigning an identical model (i.e., the global model) to all traffic monitoring devices to predict their individual local traffic trends, thereby neglecting the non-IID characteristics of traffic data collected in different locations. Building upon these findings and harnessing insights from reinforcement learning, we propose NeighborFL, an individualized real-time federated learning scheme that introduces a haversine distance-based and error-driven, personalized local models grouping heuristic from the perspective of each individual traffic node. This approach allows NeighborFL to create location-aware and tailored prediction models for each client while fostering collaborative learning. Simulations demonstrate the effectiveness of NeighborFL, offering improved real-time prediction accuracy over three baseline models, with one experimental setting showing a 16.9% reduction in MSE value compared to a naive FL setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于误差驱动聚合的个性化联邦学习交通预测方法</div>
<div class="mono" style="margin-top:8px">低延迟交通预测对智慧城市交通管理至关重要。联邦学习作为一种新兴的交通预测技术，具有隐私保护、通信开销低、预测精度高及对动态交通条件适应性强等优势。然而，现有多数联邦学习交通预测框架缺乏实时模型更新机制，难以持续整合新增交通数据并有效适应交通趋势的动态变化。另一局限在于依赖传统联邦学习模型聚合方法，即向所有交通监测设备分配相同全局模型以预测本地交通趋势，忽视了不同区域交通数据的非独立同分布特性。基于此，本研究融合强化学习思想，提出NeighborFL——一种个性化实时联邦学习方案。该方案从单个交通节点视角出发，引入基于半正矢距离与误差驱动的个性化本地模型分组启发式策略，使每个客户端能构建具有位置感知特性的定制化预测模型，同时促进协同学习。仿真实验表明，NeighborFL在三种基线模型对比中展现出更优的实时预测精度，其中一组实验的均方误差值较基础联邦学习设置降低16.9%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in existing Federated Learning for Traffic Prediction (FLTP) frameworks, which often lack real-time model updating and rely on a single global model that fails to account for the non-IID nature of traffic data across different locations. The authors propose NeighborFL, an individualized real-time federated learning scheme that uses a haversine distance-based and error-driven heuristic to group personalized local models from each traffic node&#x27;s perspective, enabling location-aware and tailored prediction while maintaining collaborative learning. Experimental simulations show that NeighborFL improves real-time prediction accuracy over three baseline models, with one setting achieving a 16.9% reduction in mean squared error compared to a naive FL approach.</div>
<div class="mono" style="margin-top:8px">本文针对现有交通预测联邦学习框架的局限性展开研究，这些框架通常缺乏实时模型更新机制，且依赖单一的全局模型，无法处理不同位置交通数据的非独立同分布特性。作者提出了NeighborFL，一种个性化的实时联邦学习方案，它基于半正矢距离和误差驱动的启发式方法，从每个交通节点的视角对个性化本地模型进行分组，从而在保持协作学习的同时实现位置感知和定制化的预测。实验仿真表明，NeighborFL在三个基线模型上提升了实时预测准确性，其中一种实验设置相比朴素联邦学习方法实现了均方误差降低16.9%。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-Based Planning for Improving Science Return of Earth Observation Satellites</div>
<div class="meta-line">Authors: Abigail Breitfeld, Alberto Candela, Juan Delfa, Akseli Kangaslahti, Itai Zilberstein, Steve Chien, David Wettergreen</div>
<div class="meta-line">First: 2025-09-05T13:11:50+00:00 · Latest: 2026-02-16T20:37:05+00:00</div>
<div class="meta-line">Comments: International Symposium on Artificial Intelligence, Robotics and Automation in Space, November 2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.07997v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.07997v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Earth observing satellites are powerful tools for collecting scientific information about our planet, however they have limitations: they cannot easily deviate from their orbital trajectories, their sensors have a limited field of view, and pointing and operating these sensors can take a large amount of the spacecraft&#x27;s resources. It is important for these satellites to optimize the data they collect and include only the most important or informative measurements. Dynamic targeting is an emerging concept in which satellite resources and data from a lookahead instrument are used to intelligently reconfigure and point a primary instrument. Simulation studies have shown that dynamic targeting increases the amount of scientific information gathered versus conventional sampling strategies. In this work, we present two different learning-based approaches to dynamic targeting, using reinforcement and imitation learning, respectively. These learning methods build on a dynamic programming solution to plan a sequence of sampling locations. We evaluate our approaches against existing heuristic methods for dynamic targeting, showing the benefits of using learning for this application. Imitation learning performs on average 10.0\% better than the best heuristic method, while reinforcement learning performs on average 13.7\% better. We also show that both learning methods can be trained effectively with small amounts of data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的规划提升地球观测卫星科学回报</div>
<div class="mono" style="margin-top:8px">地球观测卫星是收集地球科学信息的强大工具，但其存在局限性：难以偏离轨道轨迹、传感器视场有限、传感器指向与操作需消耗大量航天器资源。优化数据采集并仅纳入最重要或信息量最大的测量至关重要。动态瞄准是一种新兴概念，利用卫星资源与前瞻仪器数据智能重构并指向主仪器。仿真研究表明，相比传统采样策略，动态瞄准能提升科学信息获取量。本研究提出两种基于学习的动态瞄准方法，分别采用强化学习与模仿学习。这些学习方法基于动态规划解来规划采样位置序列。通过与现有启发式动态瞄准方法对比评估，展现了学习方法的优势：模仿学习平均优于最佳启发式方法10.0%，强化学习平均优于13.7%。同时证明两种学习方法均能通过少量数据有效训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to maximize the scientific return of Earth observation satellites constrained by fixed orbits, limited sensor fields of view, and resource-intensive operations, this paper introduces learning-based planning for dynamic targeting. The method employs reinforcement learning and imitation learning, building upon a dynamic programming foundation to plan optimal sequences for sensor pointing, using data from a lookahead instrument. Experimental results demonstrate that both learning approaches outperform existing heuristic methods in simulation, with imitation learning improving science return by 10.0% on average and reinforcement learning by 13.7%, while also showing effectiveness with small training datasets.</div>
<div class="mono" style="margin-top:8px">针对地球观测卫星受固定轨道、有限传感器视场和操作资源密集所限的问题，本研究旨在通过动态目标规划提升其科学数据回报。方法上，分别采用强化学习和模仿学习，基于动态规划框架，利用前瞻仪器数据规划传感器指向序列。实验结果表明，在仿真中两种学习方法均优于现有启发式方法，模仿学习平均提升科学回报10.0%，强化学习平均提升13.7%，且两者仅需少量数据即可有效训练。</div>
</details>
</div>
<div class="card">
<div class="title">Generalized Parallel Scaling with Interdependent Generations</div>
<div class="meta-line">Authors: Harry Dong, David Brandfonbrener, Eryk Helenowski, Yun He, Mrinal Kumar, Han Fang, Yuejie Chi, Karthik Abinav Sankararaman</div>
<div class="meta-line">First: 2025-10-01T17:33:35+00:00 · Latest: 2026-02-16T19:52:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01143v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.01143v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parallel LLM inference scaling involves sampling a set of $N&gt;1$ responses for a single input prompt. However, these $N$ parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 39% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有代际依赖性的广义并行扩展</div>
<div class="mono" style="margin-top:8px">并行大语言模型推理扩展涉及对单个输入提示采样一组$N&gt;1$个响应。然而，这些$N$个并行响应往往相互独立生成，既分割了计算资源，又使单次生成中潜在的有用信息无法被其他生成过程利用。这与响应长度扩展形成对比——后者会在所有后续步骤中复用历史计算。为获得更高质量的响应及响应集合，我们提出Bridge方法，通过将批量大语言模型隐藏状态重新构想为整体张量而非独立切片，实现具有依赖关系的并行响应生成。仅需引入少量新参数（2.8%-5.1%），Bridge将可验证奖励强化学习的相对平均准确率提升幅度提高至39%，并增强正确响应的一致性。经单次训练后，Bridge可扩展至任意生成宽度，其性能始终优于独立生成，由此解锁了一种更通用的并行扩展模式——该模式能有效利用序列间信息，且兼容所有后生成聚合技术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation in parallel LLM inference where multiple responses to a single prompt are generated independently, failing to share useful information across them. To enhance response quality and set consistency, the authors propose Bridge, a method that treats batched hidden states as holistic tensors to create interdependent parallel responses, adding only 2.8%-5.1% new parameters. Experiments show Bridge improves relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 39% and boosts correct response consistency, scaling effectively to any generation width while outperforming independent generation methods.</div>
<div class="mono" style="margin-top:8px">本文针对并行大语言模型推理中多个响应独立生成、无法共享有用信息的问题，提出了一种改进方法。通过将批处理隐藏状态视为整体张量而非独立切片，作者设计了Bridge方法，以仅增加2.8%-5.1%的新参数实现响应间的相互依赖生成。实验结果表明，Bridge将基于可验证奖励的强化学习的相对平均准确率提升高达39%，并增强了正确响应的一致性，该方法一次训练即可适应任意生成宽度，性能优于独立生成，为并行扩展提供了更通用的模式。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Reinforcement Learning: Fast and Scalable Quantum Circuit Synthesis</div>
<div class="meta-line">Authors: Lukas Theissinger, Thore Gerlach, David Berghaus, Christian Bauckhage</div>
<div class="meta-line">First: 2026-02-16T19:43:43+00:00 · Latest: 2026-02-16T19:43:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15146v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15146v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum unitary synthesis addresses the problem of translating abstract quantum algorithms into sequences of hardware-executable quantum gates. Solving this task exactly is infeasible in general due to the exponential growth of the underlying combinatorial search space. Existing approaches suffer from misaligned optimization objectives, substantial training costs and limited generalization across different qubit counts. We mitigate these limitations by using supervised learning to approximate the minimum description length of residual unitaries and combining this estimate with stochastic beam search to identify near optimal gate sequences. Our method relies on a lightweight model with zero-shot generalization, substantially reducing training overhead compared to prior baselines. Across multiple benchmarks, we achieve faster wall-clock synthesis times while exceeding state-of-the-art methods in terms of success rate for complex circuits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越强化学习：快速可扩展的量子电路合成方法</div>
<div class="mono" style="margin-top:8px">量子幺正合成旨在将抽象量子算法转化为硬件可执行的量子门序列。由于底层组合搜索空间呈指数级增长，精确求解该任务通常不可行。现有方法存在优化目标失准、训练成本高昂及对不同量子比特数泛化能力有限等问题。我们通过监督学习逼近剩余幺正算符的最小描述长度，并将该估计与随机束搜索相结合以识别近似最优的门序列，从而缓解这些局限。该方法依赖具备零样本泛化能力的轻量模型，相比现有基线显著降低训练开销。在多项基准测试中，我们实现了更快的实际合成速度，同时在复杂电路成功率方面超越现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of quantum unitary synthesis, where the exponential search space makes finding optimal gate sequences for quantum algorithms difficult, and existing methods suffer from high training costs and poor generalization. The proposed method uses supervised learning to estimate the minimum description length of residual unitaries and combines this with stochastic beam search to efficiently identify near-optimal gate sequences. Experimental results show that this approach achieves faster synthesis times and higher success rates for complex circuits compared to state-of-the-art methods, while requiring substantially less training overhead and demonstrating zero-shot generalization across different qubit counts.</div>
<div class="mono" style="margin-top:8px">本文针对量子幺正合成中因指数级搜索空间导致寻找最优量子门序列困难，以及现有方法训练成本高、泛化能力差的问题展开研究。所提出的方法利用监督学习来近似残差幺正矩阵的最小描述长度，并结合随机束搜索以高效识别接近最优的门序列。实验结果表明，该方法在多个基准测试中实现了更快的合成速度和更复杂的电路成功率，相比现有先进方法显著降低了训练开销，并能在不同量子比特数上实现零样本泛化。</div>
</details>
</div>
<div class="card">
<div class="title">Privileged Information Distillation for Language Models</div>
<div class="meta-line">Authors: Emiliano Penaloza, Dheeraj Vattikonda, Nicolas Gontier, Alexandre Lacoste, Laurent Charlin, Massimo Caccia</div>
<div class="meta-line">First: 2026-02-04T18:46:17+00:00 · Latest: 2026-02-16T18:57:38+00:00</div>
<div class="meta-line">Comments: Abstract border should have been purple</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04942v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.04942v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, which typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable, but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically, we find that π-Distill and, in some cases, OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型的特权信息蒸馏</div>
<div class="mono" style="margin-top:8px">训练时的特权信息（PI）能使语言模型在原本会失败的任务上取得成功，成为困难、长视野场景下强化学习的强大工具。然而，将利用PI习得的能力迁移至推理时无法使用PI的策略，仍是一个根本性挑战。我们在为多轮智能体环境蒸馏前沿模型的背景下研究此问题，这类环境通常隐藏内部推理过程，仅暴露行动轨迹。这打破了标准蒸馏流程，因为成功行为可观测，但推理过程不可见。为此，我们提出π-Distill——一种联合师生目标，使用同一模型同步训练PI条件化教师模型与无条件化学生模型。此外，我们还提出替代方法“策略上自蒸馏”（OPSD），通过强化学习（RL）结合学生与PI条件化教师间的反向KL惩罚进行训练。实验表明，这两种算法均能有效利用仅含行动信息的PI蒸馏前沿智能体。具体而言，π-Distill（在某些情况下OPSD）在多个智能体基准测试、模型及PI形式中，均优于假设能获取完整思维链监督的行业标准流程（监督微调后接RL）。我们通过深入分析补充实验结果，重点以π-Distill为核心阐明实现PI有效学习的关键因素，并界定OPSD具备竞争力的适用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of distilling capabilities from language models trained with privileged information (PI), which is unavailable at inference time, particularly in multi-turn agentic environments where only action trajectories are observable. To overcome this, the authors introduce two methods: π-Distill, a joint teacher-student objective that trains both a PI-conditioned teacher and an unconditioned student simultaneously, and On-Policy Self-Distillation (OPSD), which uses reinforcement learning with a reverse KL-penalty between the student and teacher. Experimental results demonstrate that both π-Distill and, in some cases, OPSD outperform standard practices like supervised fine-tuning followed by RL, which assume full chain-of-thought supervision, across multiple benchmarks, models, and forms of PI, with analysis highlighting factors enabling effective learning.</div>
<div class="mono" style="margin-top:8px">本文研究了在推理时无法获取特权信息（PI）的情况下，如何从训练时使用PI的语言模型中蒸馏能力，特别是在多轮智能体环境中仅能观察到行动轨迹的场景。为解决这一问题，作者提出了两种方法：π-Distill，一种联合教师-学生目标，同时训练PI条件教师和无条件学生；以及基于策略的自蒸馏（OPSD），该方法使用强化学习并引入学生与教师之间的反向KL惩罚。实验结果表明，π-Distill和在某些情况下的OPSD，在多个基准测试、模型和PI形式上，均优于假设拥有完整思维链监督的标准实践（如监督微调后接强化学习），并通过分析揭示了实现有效学习的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Cold-Start Personalization via Training-Free Priors from Structured World Models</div>
<div class="meta-line">Authors: Avinandan Bose, Shuyue Stella Li, Faeze Brahman, Pang Wei Koh, Simon Shaolei Du, Yulia Tsvetkov, Maryam Fazel, Lin Xiao, Asli Celikyilmaz</div>
<div class="meta-line">First: 2026-02-16T18:52:13+00:00 · Latest: 2026-02-16T18:52:13+00:00</div>
<div class="meta-line">Comments: 24 pages, 4 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15012v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15012v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users&#x27; stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结构化世界模型无训练先验的冷启动个性化方法</div>
<div class="mono" style="margin-top:8px">冷启动个性化需在缺乏用户历史数据时通过交互推断用户偏好。核心挑战是路由问题：每项任务涉及数十个偏好维度，但个体用户仅关注少数维度，且关键维度因用户而异。在有限提问次数下，无结构化提问将遗漏重要维度。强化学习虽是自然框架，但其多轮设置中的终端奖励无法利用偏好数据的因子化、按准则结构，实践中学习到的策略会退化为忽略用户响应的静态提问序列。我们提出将冷启动偏好获取分解为离线结构学习与在线贝叶斯推断。Pep（基于先验的偏好获取）框架离线从完整画像中学习偏好关联的结构化世界模型，在线执行无需训练的贝叶斯推断以选择信息量最大的问题并预测完整偏好画像（包括未询问维度）。该框架对下游求解器具有模块化特性，仅需简单信念模型。在医学、数学、社会及常识推理任务中，Pep生成响应与用户声明偏好的对齐度达80.8%（强化学习为68.5%），交互次数减少3-5倍。当用户对同一问题给出不同答案时，Pep调整后续问题的概率为39-62%（强化学习为0-28%）。该框架仅需约1万参数（强化学习需80亿），表明冷启动获取的瓶颈在于利用偏好数据因子化结构的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of cold-start personalization, where systems must infer new users&#x27; preferences without prior data, by framing it as a structured routing problem to identify relevant preference dimensions efficiently. The proposed method, Pep, decomposes the task into offline learning of a structured world model from complete user profiles to capture preference correlations, followed by online Bayesian inference that selects informative questions and predicts full profiles without additional training. Experimental results across diverse domains show Pep achieves 80.8% alignment with user preferences compared to 68.5% for reinforcement learning baselines, using 3-5 times fewer interactions and adapting follow-up questions more responsively, all while requiring far fewer parameters.</div>
<div class="mono" style="margin-top:8px">本文针对冷启动个性化问题展开研究，该问题要求系统在无用户历史数据时通过交互推断其偏好，核心挑战在于高效识别相关偏好维度的结构化路由。所提出的Pep方法将任务分解为离线学习和在线推断：离线从完整用户档案中学习结构化世界模型以捕获偏好关联，在线则通过无需训练的贝叶斯推断选择信息性问题并预测完整偏好档案。实验结果表明，在医疗、数学、社交和常识推理等多个领域，Pep与用户偏好的对齐度达到80.8%，优于强化学习基线的68.5%，交互次数减少3-5倍，且能更灵活地根据用户回答调整后续问题，参数量仅为约1万，远低于基线的80亿。</div>
</details>
</div>
<div class="card">
<div class="title">Evolution Strategies at the Hyperscale</div>
<div class="meta-line">Authors: Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio León Villares, Anya Sims, Clarisse Wibault, Dmitry Samsonov, Dylan Cope, Jarek Liesen, Kang Li, Lukas Seier, Theo Wolf, Uljad Berdica, Valentin Mohl, Alexander David Goldie, Aaron Courville, Karin Sevegnani, Shimon Whiteson, Jakob Nicolaus Foerster</div>
<div class="meta-line">First: 2025-11-20T18:56:05+00:00 · Latest: 2026-02-16T18:01:18+00:00</div>
<div class="meta-line">Comments: 76 pages, 15 figures, Website at https://eshyperscale.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16652v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16652v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://eshyperscale.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evolution Strategies (ES) is a class of powerful black-box optimisation methods that are highly parallelisable and can handle non-differentiable and noisy objectives. However, naïve ES becomes prohibitively expensive at scale on GPUs due to the low arithmetic intensity of batched matrix multiplications with unstructured random perturbations. We introduce Evolution Guided GeneRal Optimisation via Low-rank Learning (EGGROLL), which improves arithmetic intensity by structuring individual perturbations as rank-$r$ matrices, resulting in a hundredfold increase in training speed for billion-parameter models at large population sizes, achieving up to 91% of the throughput of pure batch inference. We provide a rigorous theoretical analysis of Gaussian ES for high-dimensional parameter objectives, investigating conditions needed for ES updates to converge in high dimensions. Our results reveal a linearising effect, and proving consistency between EGGROLL and ES as parameter dimension increases. Our experiments show that EGGROLL: (1) enables the stable pretraining of nonlinear recurrent language models that operate purely in integer datatypes, (2) is competitive with GRPO for post-training LLMs on reasoning tasks, and (3) does not compromise performance compared to ES in tabula rasa RL settings, despite being faster.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超大规模下的进化策略</div>
<div class="mono" style="margin-top:8px">进化策略（ES）是一类强大的黑盒优化方法，具有高度可并行性，能够处理不可微且含噪声的目标函数。然而，由于非结构化随机扰动批量矩阵乘法的算术强度较低，朴素ES在GPU上大规模运行时成本过高。我们提出了基于低秩学习的进化引导通用优化方法（EGGROLL），通过将个体扰动构建为秩-$r$矩阵来提高算术强度，使得十亿参数模型在大种群规模下的训练速度提升百倍，达到纯批量推理吞吐量的91%。我们对高维参数目标的高斯ES进行了严格的理论分析，探讨了ES更新在高维空间中收敛所需的条件。研究结果揭示了线性化效应，并证明了EGGROLL与ES在参数维度增加时的一致性。实验表明EGGROLL：（1）能够稳定预训练纯整数数据类型的非线性循环语言模型；（2）在推理任务的后训练LLM中与GRPO具有竞争力；（3）在空白强化学习环境中相比ES未损失性能，且速度更快。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the computational inefficiency of naive Evolution Strategies (ES) at large scales, particularly on GPUs, where low arithmetic intensity from unstructured random perturbations makes optimization prohibitively expensive. The method introduces EGGROLL, which structures perturbations as low-rank matrices to significantly boost arithmetic intensity, enabling up to a hundredfold speed increase for billion-parameter models and achieving near-inference throughput. Experimental results demonstrate that EGGROLL facilitates stable pretraining of integer-based recurrent language models, competes with GRPO in post-training LLMs on reasoning tasks, and maintains performance comparable to ES in reinforcement learning settings without speed compromises.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决传统进化策略在大规模GPU计算中的低效问题，由于非结构化随机扰动导致的低算术强度使得优化成本过高。方法上提出了EGGROLL，通过将扰动结构化为低秩矩阵来显著提升算术强度，实现了百倍加速，使十亿参数模型在大型种群规模下达到接近纯推理的吞吐量。实验结果表明，EGGROLL能够稳定预训练纯整数数据类型的非线性循环语言模型，在大型语言模型后训练推理任务上与GRPO竞争，并在强化学习场景中保持与进化策略相当的性能而不损失速度。</div>
</details>
</div>
<div class="card">
<div class="title">DiffusionNFT: Online Diffusion Reinforcement with Forward Process</div>
<div class="meta-line">Authors: Kaiwen Zheng, Huayu Chen, Haotian Ye, Haoxiang Wang, Qinsheng Zhang, Kai Jiang, Hang Su, Stefano Ermon, Jun Zhu, Ming-Yu Liu</div>
<div class="meta-line">Venue: ICLR 2026 Oral</div>
<div class="meta-line">First: 2025-09-19T16:09:33+00:00 · Latest: 2026-02-16T17:14:06+00:00</div>
<div class="meta-line">Comments: ICLR 2026 Oral</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16117v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.16117v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffusionNFT：基于前向过程的在线扩散强化学习</div>
<div class="mono" style="margin-top:8px">在线强化学习（RL）已成为语言模型后训练的核心方法，但由于扩散模型似然函数难以处理，将其扩展至扩散模型仍具挑战。近期研究通过离散化逆向采样过程实现GRPO式训练，但仍存在求解器限制、前向-逆向不一致性、与无分类器引导（CFG）整合复杂等固有缺陷。本文提出扩散负感知微调（DiffusionNFT），这是一种基于流匹配直接在前向过程优化扩散模型的新型在线RL范式。DiffusionNFT通过对比正负样本来定义隐式策略改进方向，自然地将强化信号融入监督学习目标。该框架支持任意黑盒求解器训练，无需似然估计，且策略优化仅需干净图像而非采样轨迹。在直接对比中，DiffusionNFT效率最高可达FlowGRPO的25倍，且无需CFG。例如，DiffusionNFT在1千步内将GenEval分数从0.24提升至0.98，而FlowGRPO需超过5千步并依赖额外CFG才达到0.95。通过整合多个奖励模型，DiffusionNFT在各项基准测试中显著提升了SD3.5-Medium的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces DiffusionNFT, a novel online reinforcement learning paradigm for diffusion models that addresses the inefficiencies and complexities of existing methods like FlowGRPO, which rely on discretizing the reverse sampling process and face issues such as solver restrictions and forward-reverse inconsistency. The method optimizes diffusion models directly on the forward process via flow matching, contrasting positive and negative generations to define an implicit policy improvement direction, thereby integrating reinforcement signals into a supervised learning objective without needing likelihood estimation or sampling trajectories. Experimental results show that DiffusionNFT is up to 25 times more efficient than FlowGRPO, improving the GenEval score from 0.24 to 0.98 within 1,000 steps without classifier-free guidance, and it significantly boosts the performance of SD3.5-Medium across multiple benchmarks when leveraging multiple reward models.</div>
<div class="mono" style="margin-top:8px">本文提出了DiffusionNFT，这是一种用于扩散模型的新型在线强化学习范式，旨在解决现有方法（如FlowGRPO）的低效性和复杂性，这些方法依赖于反向采样过程的离散化，并面临求解器限制和前向-反向不一致等问题。该方法通过流匹配直接在正向过程上优化扩散模型，对比正负生成以定义隐式的策略改进方向，从而将强化学习信号融入监督学习目标，无需似然估计或采样轨迹。实验结果表明，DiffusionNFT的效率比FlowGRPO高出25倍，在1,000步内将GenEval分数从0.24提升至0.98，且无需分类器引导，当结合多个奖励模型时，它能显著提升SD3.5-Medium在所有测试基准上的性能。</div>
</details>
</div>
<div class="card">
<div class="title">MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design</div>
<div class="meta-line">Authors: Gen Zhou, Sugitha Janarthanan, Lianghong Chen, Pingzhao Hu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-16T17:01:47+00:00 · Latest: 2026-02-16T17:01:47+00:00</div>
<div class="meta-line">Comments: This paper is published in ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14926v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14926v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a &#x27;black box&#x27;. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAC-AMP：用于多目标抗菌肽设计的闭环多智能体协作系统</div>
<div class="mono" style="margin-top:8px">为应对全球抗菌素耐药性威胁，抗菌肽（AMP）因其对抗耐药病原体的强大潜力而备受关注。尽管人工智能（AI）已应用于AMP发现与设计，但多数模型难以平衡活性、毒性与新颖性等关键目标，且采用僵化或模糊的评分方法，导致结果难以解释和优化。随着大语言模型（LLM）能力快速演进，我们转向基于此类模型的多智能体协作（多智能体LLMs），其在复杂科学设计场景中展现出迅速增长的潜力。基于此，我们提出MAC-AMP——一个用于多目标AMP设计的闭环多智能体协作系统。该系统实现了全自主模拟同行评审-自适应强化学习框架，仅需任务描述和示例数据集即可设计新型AMP。本研究的创新在于引入具有跨领域可迁移性的闭环多智能体系统，支持多目标优化且保持可解释性，而非“黑箱”模型。实验表明，MAC-AMP通过有效优化多个关键分子属性，在抗菌活性、AMP相似性、毒性合规性和结构可靠性方面表现卓越，优于其他AMP生成模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to combat antimicrobial resistance and the limitations of existing AI models in balancing multiple design objectives like activity, toxicity, and novelty, this paper proposes MAC-AMP, a closed-loop multi-agent collaboration system based on large language models for multi-objective antimicrobial peptide design. The method employs a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to generate novel peptides, emphasizing explainability and cross-domain transferability rather than black-box optimization. Experimental results demonstrate that MAC-AMP outperforms other generative models by effectively optimizing key molecular properties, achieving exceptional performance in antibacterial activity, peptide likeliness, toxicity compliance, and structural reliability.</div>
<div class="mono" style="margin-top:8px">为应对抗菌素耐药性的全球健康威胁，并针对现有人工智能模型在平衡活性、毒性和新颖性等多重设计目标上的不足，本文提出了MAC-AMP，一个基于大语言模型的闭环多智能体协作系统，用于多目标抗菌肽设计。该方法采用完全自主的模拟同行评审-自适应强化学习框架，仅需任务描述和示例数据集即可生成新型肽，强调可解释性和跨领域可迁移性，而非黑箱优化。实验结果表明，MAC-AMP在优化关键分子属性方面优于其他生成模型，在抗菌活性、肽似然性、毒性合规性和结构可靠性上均取得了优异表现。</div>
</details>
</div>
<div class="card">
<div class="title">BFS-PO: Best-First Search for Large Reasoning Models</div>
<div class="meta-line">Authors: Fiorenzo Parascandolo, Wenhui Tan, Enver Sangineto, Ruihua Song, Rita Cucchiara</div>
<div class="meta-line">First: 2026-02-16T16:53:41+00:00 · Latest: 2026-02-16T16:53:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14917v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BFS-PO：面向大型推理模型的最佳优先搜索算法</div>
<div class="mono" style="margin-top:8px">大型推理模型（如OpenAI o1和DeepSeek-R1）在长推理链任务中表现出色，但这也导致计算成本显著增加并产生冗长输出，即“过度思考”现象。强化学习算法（如GRPO/DAPO）常加剧此倾向。本文提出BFS-PO算法，采用最佳优先搜索探索策略缓解该问题：通过基于最大熵节点的回溯机制寻找最短正确答案，在训练中逐步生成更简短响应，从而学习生成简洁推理链。实验表明，BFS-PO能在提升模型准确率的同时缩短答案长度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the overthinking problem in Large Reasoning Models (LRMs), where models like OpenAI o1 and DeepSeek-R1 generate excessively long reasoning chains, increasing computational costs and verbosity, often worsened by RL algorithms such as GRPO/DAPO. The method proposed, BFS-PO, employs a Best-First Search exploration strategy with backtracking based on maximum entropy nodes to find the shortest correct answer, training the model to produce progressively shorter responses. Experimental results across various benchmarks and base LRMs demonstrate that BFS-PO simultaneously improves model accuracy and reduces answer length.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型推理模型中的过度思考问题，即如OpenAI o1和DeepSeek-R1等模型在推理任务中生成过长推理链，导致计算成本增加和输出冗长，且GRPO/DAPO等强化学习算法常加剧此现象。所提出的BFS-PO方法采用最佳优先搜索探索策略，基于最大熵节点进行回溯以寻找最短正确答案，通过训练逐步缩短响应来学习生成简洁推理链。在不同基准测试和基础大型推理模型上的实验结果表明，BFS-PO能同时提高模型准确率并缩短答案长度。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Bayesian Optimisation with Unbounded Corruptions</div>
<div class="meta-line">Authors: Abdelhamid Ezzerg, Ilija Bogunovic, Jeremias Knoblauch</div>
<div class="meta-line">First: 2025-11-19T10:28:56+00:00 · Latest: 2026-02-16T16:22:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.15315v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.15315v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/4})$ and $O(T^{1/7})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB&#x27;s regret bounds match those of the standard GP-UCB algorithm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无界扰动下的鲁棒贝叶斯优化</div>
<div class="mono" style="margin-top:8px">贝叶斯优化对极端异常值极为敏感。现有可证明鲁棒的方法通常假设累积扰动预算有界，这导致其面对单次足够强度的扰动时完全失效。为此，我们提出一种新型对抗模型，其扰动预算仅受频率限制而不受幅度约束。基于此，我们推导出RCGP-UCB算法，将经典上置信界方法与鲁棒共轭高斯过程相结合。我们提出了稳定版与自适应版RCGP-UCB，并证明其在最多$O(T^{1/4})$和$O(T^{1/7})$次（幅度可能无限）扰动下仍能实现次线性遗憾。这种鲁棒性几乎无需代价：在无异常值时，RCGP-UCB的遗憾界与标准GP-UCB算法保持一致。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of Bayesian Optimization to extreme outliers, noting that existing robust methods assume a bounded cumulative corruption budget and thus fail against even a single corruption of large magnitude. To overcome this, the authors introduce a new adversary model where the corruption budget is bounded only in frequency, not magnitude, and propose RCGP-UCB, an algorithm combining upper confidence bound (UCB) with a Robust Conjugate Gaussian Process (RCGP). Experimental results show that stable and adaptive versions of RCGP-UCB achieve sublinear regret with up to O(T^{1/4}) and O(T^{1/7}) corruptions of potentially infinite magnitude, while maintaining performance comparable to standard GP-UCB in outlier-free scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对贝叶斯优化对极端异常值的脆弱性问题，指出现有鲁棒方法假设累积污染预算有界，因此无法应对单个大污染。为解决此问题，作者引入了一种新的对抗模型，其中污染预算仅在频率上有界而非幅度，并提出了RCGP-UCB算法，该算法将上置信界（UCB）与鲁棒共轭高斯过程（RCGP）相结合。实验结果表明，RCGP-UCB的稳定和自适应版本在存在多达O(T^{1/4})和O(T^{1/7})个可能无限幅度的污染时，仍能实现次线性遗憾，同时在无异常值情况下保持与标准GP-UCB算法相近的性能。</div>
</details>
</div>
<div class="card">
<div class="title">On the Learning Dynamics of RLVR at the Edge of Competence</div>
<div class="meta-line">Authors: Yu Huang, Zixin Wen, Yuejie Chi, Yuting Wei, Aarti Singh, Yingbin Liang, Yuxin Chen</div>
<div class="meta-line">First: 2026-02-16T16:03:08+00:00 · Latest: 2026-02-16T16:03:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14872v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14872v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model&#x27;s capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论能力边界处RLVR的学习动态机制</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是推动大型推理模型近期突破的主要动力。然而，仅基于最终结果的奖励如何帮助克服长程推理障碍仍是一个谜团。为理解这一机制，我们建立了针对组合推理任务的Transformer强化学习训练动态理论。该理论揭示了RLVR的有效性如何受难度谱平滑度的调控：当数据存在难度突变时，学习过程会出现顿悟型相变，在重新取得进展前产生长期平台期；反之，平滑的难度谱会引发接力效应——较简单问题上持续的梯度信号将模型能力提升至可处理更难问题的水平，从而实现稳定持续的改进。我们的理论阐释了RLVR如何在能力边界提升性能，并表明适当设计的数据混合能产生可扩展的增益。作为技术贡献，本研究将有限群上的傅里叶分析工具发展并适配至当前场景，最终通过合成实验对预测机制进行了实证验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the learning dynamics of reinforcement learning with verifiable rewards (RLVR) in compositional reasoning tasks, motivated by the need to understand how outcome-based rewards can overcome long-horizon reasoning barriers. The method develops a theoretical framework using Fourier analysis on finite groups to model RL training dynamics for transformers, showing that effectiveness depends on the smoothness of the difficulty spectrum in data. Experimental results from synthetic tasks confirm that abrupt difficulty discontinuities cause grokking-like plateaus, while smooth spectra enable a relay effect where easier problems progressively bootstrap capability to solve harder ones, leading to steady improvement and scalable gains with well-designed data mixtures.</div>
<div class="mono" style="margin-top:8px">本文研究了在组合推理任务中，基于可验证奖励的强化学习（RLVR）的学习动态，旨在理解仅依赖最终结果的奖励如何克服长时程推理障碍。方法上，通过将有限群上的傅里叶分析工具适配到该场景，建立了针对Transformer的RL训练动态理论框架，表明学习效果受数据中难度谱平滑度调控。合成实验验证了理论预测：难度突变会导致类似顿悟的停滞平台期，而平滑的难度谱则引发接力效应，即较易问题上的持续梯度信号逐步提升模型能力，使其能处理更难问题，从而实现稳定改进，并通过恰当设计的数据混合获得可扩展的收益。</div>
</details>
</div>
<div class="card">
<div class="title">Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning</div>
<div class="meta-line">Authors: Ilia Mahrooghi, Aryo Lotfi, Emmanuel Abbe</div>
<div class="meta-line">First: 2026-02-16T16:01:27+00:00 · Latest: 2026-02-16T16:01:27+00:00</div>
<div class="meta-line">Comments: 21 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question&#x27;s difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student&#x27;s performance on seen samples, the teacher continuously adapts to the student&#x27;s evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Goldilocks强化学习：通过调节任务难度规避稀疏奖励的推理困境</div>
<div class="mono" style="margin-top:8px">强化学习已成为解锁大型语言模型推理能力的重要范式。然而，依赖稀疏奖励导致该过程样本效率极低，因为模型必须在反馈极少的情况下探索巨大搜索空间。传统课程学习试图通过按复杂度排序数据来缓解此问题，但针对特定模型的最佳排序往往难以确定。为此，我们提出Goldilocks——一种新颖的教师驱动数据采样策略，旨在预测每个问题对学生模型的难度。教师模型依据Goldilocks原则（难度适中原则）为学生选择既不过易也不过难的问题，同时使用GRPO方法训练学生模型。通过利用学生在已见样本上的表现，教师能持续适应学生动态演进的能力。在OpenMathReasoning数据集上，Goldilocks数据采样策略在相同计算预算下显著提升了标准GRPO训练模型的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the sample inefficiency of reinforcement learning for reasoning in large language models caused by sparse rewards. It introduces Goldilocks, a teacher-driven data sampling strategy that predicts question difficulty for a student model, applying the Goldilocks principle to select questions that are neither too easy nor too hard, and trains the student using GRPO while the teacher adapts based on the student&#x27;s performance. Experimental results on the OpenMathReasoning dataset show that this method improves model performance compared to standard GRPO under the same computational budget.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型推理中因稀疏奖励导致的强化学习样本效率低下问题，提出了一种名为Goldilocks的教师驱动数据采样策略。该方法基于Goldilocks原则，通过教师模型预测学生模型的问题难度，选择难度适中的问题，并利用GRPO训练学生模型，同时教师根据学生表现动态调整。在OpenMathReasoning数据集上的实验结果表明，在相同计算资源下，该方法相比标准GRPO提升了模型性能。</div>
</details>
</div>
<div class="card">
<div class="title">Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment</div>
<div class="meta-line">Authors: Elias Malomgré, Pieter Simoens</div>
<div class="meta-line">First: 2026-02-16T15:40:10+00:00 · Latest: 2026-02-16T15:40:10+00:00</div>
<div class="meta-line">Comments: Accepted for the AAMAS 2026 Blue Sky Ideas track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14844v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14844v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent&#x27;s policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无交互逆强化学习：面向持久对齐的数据中心化框架</div>
<div class="mono" style="margin-top:8px">人工智能对齐的重要性日益凸显，但现有方法存在将安全目标与智能体策略相纠缠的结构性缺陷。基于人类反馈的强化学习和直接偏好优化等方法会产生不透明、一次性使用的对齐产物，我们称之为“对齐浪费”。本文提出无交互逆强化学习方法，将对齐产物学习与策略优化解耦，生成可检查、可编辑且模型无关的奖励模型。此外，我们引入“对齐飞轮”——一种人机协同的生命周期，通过自动化审计与精炼迭代强化奖励模型。该架构将安全性从一次性消耗转化为持久、可验证的工程资产。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the structural flaw in current AI alignment methods, which entangle safety objectives with agent policies and produce opaque, single-use artifacts termed Alignment Waste, this paper proposes Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, yielding an inspectable, editable, and model-agnostic reward model. The method is complemented by the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. Experimental results demonstrate that this framework transforms safety from a disposable expense into a durable, verifiable engineering asset, as evidenced by its acceptance for the AAMAS 2026 Blue Sky Ideas track.</div>
<div class="mono" style="margin-top:8px">本文的动机在于当前AI对齐方法存在结构性缺陷，即将安全目标与智能体策略纠缠，产生被称为“对齐浪费”的不透明、一次性产物。为此，论文提出了无交互逆强化学习方法，将对齐产物学习与策略优化解耦，生成可检查、可编辑且模型无关的奖励模型。该方法还引入了对齐飞轮，这是一种人在循环的生命周期，通过自动化审计和精炼迭代强化奖励模型。实验结果表明，该框架将安全性从一次性消耗转变为持久、可验证的工程资产，这从其被AAMAS 2026蓝天空想轨道接收得到印证。</div>
</details>
</div>
<div class="card">
<div class="title">Virne: A Comprehensive Benchmark for RL-based Network Resource Allocation in NFV</div>
<div class="meta-line">Authors: Tianfu Wang, Liwei Deng, Xi Chen, Junyang Wang, Huiguo He, Zhengyu Hu, Wei Wu, Leilei Ding, Qilin Fan, Hui Xiong</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-07-25T12:58:32+00:00 · Latest: 2026-02-16T14:54:58+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.19234v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.19234v2">PDF</a> · <a href="https://github.com/GeminiLight/virne">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Virne：面向NFV网络资源分配的强化学习综合基准测试框架</div>
<div class="mono" style="margin-top:8px">资源分配（RA）是网络功能虚拟化（NFV）这一变革性网络范式中实现高效服务部署的关键。近年来，基于深度强化学习（RL）的方法在应对此类复杂问题上展现出巨大潜力。然而，系统性基准测试框架与深入分析的缺失，不仅导致评估标准不一致，也阻碍了对新兴网络的探索及更鲁棒算法的开发。本文提出Virne——一个面向NFV-RA问题的综合基准测试框架，重点支持基于深度RL的方法。Virne提供可定制化的多场景网络模拟（包括云、边缘及5G环境），采用模块化可扩展的实现流程，支持超过30种不同类型的算法，并在有效性之外纳入可扩展性、泛化性等实用评估维度。通过大量实验的深入分析，本研究揭示了性能权衡的关键见解，为高效实现提供参考，并为未来研究方向提出可行建议。总体而言，凭借多样化的模拟场景、丰富的算法实现和全面的评估能力，Virne可作为推进NFV-RA方法与深度RL应用的综合基准。代码已开源：https://github.com/GeminiLight/virne。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the lack of a systematic benchmarking framework for deep Reinforcement Learning (RL) methods in Network Function Virtualization Resource Allocation (NFV-RA), which hinders consistent evaluation and algorithm development. The method introduces Virne, a comprehensive benchmarking framework that provides customizable simulations for diverse network scenarios like cloud and 5G, and features a modular pipeline supporting over 30 methods. The main experimental results from extensive analysis offer insights into performance trade-offs, including scalability and generalization, providing actionable guidance for future research and establishing Virne as a tool to advance NFV-RA and RL applications.</div>
<div class="mono" style="margin-top:8px">本工作的动机是，网络功能虚拟化资源分配领域缺乏针对深度强化学习方法的系统性基准测试框架，这阻碍了一致的评估和算法开发。方法上，研究引入了Virne这一综合性基准测试框架，它为云、边缘和5G等多种网络场景提供可定制的模拟，并采用模块化管道支持超过30种方法。通过大量实验进行深入分析的主要结果，揭示了包括可扩展性和泛化性在内的性能权衡，为未来研究方向提供了实用指导，使Virne成为推动NFV-RA和强化学习应用发展的工具。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning via Self-Distillation</div>
<div class="meta-line">Authors: Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</div>
<div class="meta-line">First: 2026-01-28T17:45:12+00:00 · Latest: 2026-02-16T14:49:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20802v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20802v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#x27;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自蒸馏的强化学习</div>
<div class="mono" style="margin-top:8px">大型语言模型在代码和数学等可验证领域越来越多地通过强化学习进行后训练。然而，当前基于可验证奖励的强化学习方法仅从每次尝试的标量结果奖励中学习，形成了严重的信用分配瓶颈。许多可验证环境实际上提供了丰富的文本反馈（如运行时错误或评判评估），用以解释尝试失败的原因。我们将此设定形式化为具有丰富反馈的强化学习，并引入自蒸馏策略优化方法，该方法将标记化反馈转化为密集学习信号，无需外部教师或显式奖励模型。SDPO将基于反馈的当前模型视为自教师，并将其反馈感知的下一标记预测蒸馏回策略中。通过这种方式，SDPO利用了模型在上下文中回溯识别自身错误的能力。在科学推理、工具使用和LiveCodeBench v6的竞技编程任务中，SDPO相比强RLVR基线显著提升了样本效率和最终准确率。值得注意的是，在仅返回标量反馈的标准RLVR环境中，SDPO通过将成功轨迹作为失败尝试的隐式反馈，同样超越了基线方法。最后，在测试时对单个问题应用SDPO可加速困难二元奖励任务的探索，仅需最佳k采样或多轮对话三分之一尝试次数即可达到同等发现概率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the credit-assignment bottleneck in reinforcement learning with verifiable rewards (RLVR), where models only receive scalar outcome rewards, this paper introduces Self-Distillation Policy Optimization (SDPO) to leverage rich textual feedback, such as error messages, that explains failures. The method treats the current model conditioned on this feedback as a self-teacher, distilling its feedback-informed next-token predictions back into the policy to enable retrospective mistake identification. Experimental results across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6 show that SDPO improves sample efficiency and final accuracy over RLVR baselines, even outperforming them in scalar-feedback environments by using successful rollouts as implicit feedback, and accelerates discovery on difficult tasks with fewer attempts compared to sampling-based methods.</div>
<div class="mono" style="margin-top:8px">针对可验证奖励强化学习（RLVR）中仅依赖标量结果奖励导致的信用分配瓶颈问题，本文提出了自蒸馏策略优化（SDPO），旨在利用解释失败原因的丰富文本反馈（如错误信息）。该方法将当前模型在反馈条件下的输出视为自教师，通过蒸馏其基于反馈的下一词预测来更新策略，从而实现模型对自身错误的回顾性识别。在科学推理、工具使用和LiveCodeBench v6竞争性编程等实验表明，SDPO相比RLVR基线提高了样本效率和最终准确率，即使在仅提供标量反馈的环境中，通过将成功轨迹作为隐式反馈也优于基线，并在困难二元奖励任务上以更少尝试次数加速了发现过程，性能媲美最佳采样或多轮对话方法。</div>
</details>
</div>
<div class="card">
<div class="title">Extending Multi-Source Bayesian Optimization With Causality Principles</div>
<div class="meta-line">Authors: Luuk Jacobs, Mohammad Ali Javidian</div>
<div class="meta-line">First: 2026-02-16T14:38:16+00:00 · Latest: 2026-02-16T14:38:16+00:00</div>
<div class="meta-line">Comments: An extended abstract version of this work was accepted for the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14791v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于因果原理扩展多源贝叶斯优化</div>
<div class="mono" style="margin-top:8px">多源贝叶斯优化（MSBO）是传统贝叶斯优化（BO）框架的变体，适用于在仿真、代理模型或真实实验等多信息源场景中优化目标黑箱函数。然而传统MSBO假设目标函数的输入变量独立同分布，这在具备因果信息且可实施干预（如临床试验或政策制定）的场景中存在局限。在单源领域，因果贝叶斯优化（CBO）通过引入因果原理扩展了标准BO框架，能更精准建模变量依赖关系，从而提升优化精度、改进决策效率并优化低成本信息源的使用。本文提出在多源领域系统整合MSBO与CBO方法论，融合二者优势以提升高维问题的优化效率并降低计算复杂度。我们阐述因果贝叶斯优化与多源贝叶斯优化的理论基础，论证其协同机制如何支撑我们提出的多源因果贝叶斯优化（MSCBO）算法。通过在含不同噪声水平的合成数据集与真实数据集上对比MSCBO与其基础模型的性能，凸显了MSCBO的鲁棒性与适用性。研究表明，将MSBO与CBO的因果原理相结合，能有效实现降维并降低运算成本，最终提升收敛速度、优化性能及可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional Multi-Source Bayesian Optimization (MSBO), which assumes independent input variables and thus underutilizes causal information in domains like clinical trials, this work integrates causality principles from Causal Bayesian Optimization (CBO) into the multi-source framework. The method proposes a Multi-Source Causal Bayesian Optimization (MSCBO) algorithm that synergistically combines MSBO and CBO to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. Experimental results on synthetic and real-world datasets with varying noise levels demonstrate that MSCBO outperforms its foundational counterparts, improving convergence speed, performance, and scalability by facilitating dimensionality reduction and lowering operational costs.</div>
<div class="mono" style="margin-top:8px">传统多源贝叶斯优化（MSBO）假设输入变量独立同分布，在临床实验等可利用因果信息的场景中效果受限，本文旨在解决这一局限性。方法上，该研究将因果贝叶斯优化（CBO）的因果原理与多源框架相结合，提出了多源因果贝叶斯优化（MSCBO）算法，以提升高维问题中的优化效率并降低计算复杂度。在合成和真实数据集上的实验结果表明，MSCBO在不同噪声水平下均优于其基础方法，通过促进降维和降低操作成本，有效提高了收敛速度、性能和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Flow Is All You Need to Sample Out-of-Distribution Chemical Spaces</div>
<div class="meta-line">Authors: Nianze Tao, Minori Abe</div>
<div class="meta-line">First: 2024-12-16T04:43:54+00:00 · Latest: 2026-02-16T14:03:22+00:00</div>
<div class="meta-line">Comments: 34 pages, 14 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.11439v5">Abs</a> · <a href="https://arxiv.org/pdf/2412.11439v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating novel molecules with higher properties than the training space, namely the out-of-distribution generation, is important for de novo drug design. However, it is not easy for distribution learning-based models, for example diffusion models, to solve this challenge as these methods are designed to fit the distribution of training data as close as possible. In this paper, we show that Bayesian flow network, especially ChemBFN model, is capable of intrinsically generating high quality out-of-distribution samples that meet several scenarios. A reinforcement learning strategy is added to the ChemBFN and a controllable ordinary differential equation solver-like generating process is employed that accelerate the sampling processes. Most importantly, we introduce a semi-autoregressive strategy during training and inference that enhances the model performance and surpass the state-of-the-art models. A theoretical analysis of out-of-distribution generation in ChemBFN with semi-autoregressive approach is included as well.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯流网络是采样分布外化学空间的唯一所需</div>
<div class="mono" style="margin-top:8px">生成性质优于训练空间的新分子（即分布外生成）对于从头药物设计至关重要。然而，基于分布学习的模型（如扩散模型）难以应对这一挑战，因为这些方法旨在尽可能拟合训练数据的分布。本文证明，贝叶斯流网络（特别是ChemBFN模型）能够本质性地生成满足多种场景的高质量分布外样本。我们在ChemBFN中引入强化学习策略，并采用类似可控常微分方程求解器的生成过程以加速采样。最重要的是，我们提出在训练和推理阶段使用半自回归策略，该策略显著提升了模型性能并超越了现有最优模型。文中还包含对采用半自回归方法的ChemBFN进行分布外生成的理论分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating novel molecules with superior properties beyond the training distribution, a key task in de novo drug design where standard diffusion models often fail due to their focus on fitting existing data. The method introduces ChemBFN, a Bayesian flow network enhanced with a reinforcement learning strategy and a controllable ODE solver-like process to accelerate sampling, alongside a semi-autoregressive training and inference approach to boost performance. Experimental results demonstrate that ChemBFN intrinsically produces high-quality out-of-distribution samples across multiple scenarios, outperforming state-of-the-art models, with theoretical analysis supporting its efficacy.</div>
<div class="mono" style="margin-top:8px">本文针对在训练分布之外生成具有更优性能的新分子这一挑战展开研究，这在从头药物设计中至关重要，而标准扩散模型因专注于拟合现有数据往往难以胜任。方法上提出了ChemBFN，一种贝叶斯流网络，结合了强化学习策略和类似可控ODE求解器的生成过程以加速采样，并采用半自回归训练和推理策略来提升性能。实验结果表明，ChemBFN能够在多种场景下本质地生成高质量的外分布样本，超越了现有最先进模型，同时提供了理论分析以支持其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay</div>
<div class="meta-line">Authors: Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-05T17:55:43+00:00 · Latest: 2026-02-16T13:48:46+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05316v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.05316v4">PDF</a> · <a href="https://github.com/ASTRAL-Group/data-efficient-llm-rl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism inspired by experience replay in traditional RL. This technique reuses recent rollouts, lowering per-step computation while maintaining stable updates. Experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 23% to 62% while reaching the same level of performance as the original GRPO algorithm. Our code is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过难度导向的在线数据选择与轨迹回放提升大语言模型强化学习微调的数据效率</div>
<div class="mono" style="margin-top:8px">强化学习已成为微调大语言模型的有效方法，尤其在提升推理能力方面。然而，强化学习微调仍高度消耗资源，现有研究大多忽视了数据效率问题。本文提出两种提升大语言模型强化学习微调数据效率的技术：难度导向的在线数据选择与轨迹回放。我们引入自适应难度概念来指导在线数据选择，优先选择中等难度、更可能产生有效学习信号的问题。为高效估计自适应难度，我们开发了基于注意力的框架，仅需对少量参考问题集进行轨迹推演，其余问题的难度则根据其与参考集的相似度进行估计。为进一步降低推演成本，我们借鉴传统强化学习中的经验回放机制，提出轨迹回放技术，通过复用近期轨迹在保持更新稳定性的同时降低单步计算量。在6种大语言模型-数据集组合上的实验表明，本方法将强化学习微调时间减少23%至62%，同时达到与原始GRPO算法相当的性能。代码已开源：https://github.com/ASTRAL-Group/data-efficient-llm-rl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the high computational cost of reinforcement learning (RL) fine-tuning for large language models (LLMs) by proposing two techniques to improve data efficiency. The method introduces difficulty-targeted online data selection, which prioritizes questions of moderate difficulty using an attention-based framework to estimate adaptive difficulty efficiently, and a rollout replay mechanism that reuses recent rollouts to reduce per-step computation. Experimental results across six LLM-dataset combinations demonstrate that this approach reduces RL fine-tuning time by 23% to 62% while achieving performance comparable to the original GRPO algorithm.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习微调的高计算成本问题，提出了两种提高数据效率的技术。方法包括难度定向的在线数据选择，通过基于注意力的框架高效估计自适应难度以优先选择中等难度问题，以及受传统强化学习经验回放启发的轨迹回放机制，通过重用近期轨迹降低单步计算量。在六个大语言模型与数据集组合上的实验结果表明，该方法将强化学习微调时间减少了23%至62%，同时达到了与原始GRPO算法相当的性能水平。</div>
</details>
</div>
<div class="card">
<div class="title">A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting</div>
<div class="meta-line">Authors: Ons Saadallah, Mátyás andó, Tamás Gábor Orosz</div>
<div class="meta-line">First: 2026-02-01T21:26:57+00:00 · Latest: 2026-02-16T13:31:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01445v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01445v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向时间序列预测超参数优化的元知识增强型大语言模型框架</div>
<div class="mono" style="margin-top:8px">超参数优化（HPO）对深度学习模型性能至关重要，但其计算成本高昂且可解释性差，在时间序列预测领域尤为突出。虽然贝叶斯优化（BO）是标准方法，但通常独立处理调优任务且决策依据不透明。大语言模型（LLM）的最新进展为将结构化先验知识与推理融入优化流程提供了新途径。本文提出LLM-AutoOpt——一种融合BO与基于LLM的上下文推理的混合HPO框架。该框架将数据集元特征、模型描述、历史优化结果和目标指标编码为LLM提示中的结构化元知识，利用BO初始化搜索并缓解冷启动效应。这种设计实现了上下文感知的稳定超参数优化，同时揭示了决策背后的推理逻辑。在多变量时间序列预测基准测试中，LLM-AutoOpt相比无元知识的BO和LLM基线方法，展现出更优的预测性能和更强的优化行为可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational expense and limited interpretability of hyperparameter optimization (HPO) in time-series forecasting, where standard Bayesian Optimization (BO) lacks contextual reasoning. The method introduces LLM-AutoOpt, a hybrid framework that integrates BO with large language models (LLMs) by encoding dataset meta-features, model descriptions, and historical outcomes as structured meta-knowledge in prompts, using BO to initialize searches and reduce cold-start issues. Experimental results on a multivariate time-series benchmark show that LLM-AutoOpt outperforms BO and LLM baselines in predictive performance while offering more interpretable optimization behavior.</div>
<div class="mono" style="margin-top:8px">本文的动机在于时间序列预测中超参数优化计算成本高且可解释性差，而标准的贝叶斯优化缺乏上下文推理能力。方法上提出了LLM-AutoOpt，这是一个混合框架，通过将数据集元特征、模型描述和历史结果编码为结构化元知识并融入提示，将贝叶斯优化与大型语言模型结合，利用贝叶斯优化初始化搜索以减少冷启动问题。在多变量时间序列基准测试中的实验结果表明，LLM-AutoOpt在预测性能上优于贝叶斯优化和无元知识的LLM基线，同时提供了更可解释的优化行为。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis</div>
<div class="meta-line">Authors: Matthieu Mastio, Paul Saves, Benoit Gaudou, Nicolas Verstaevel</div>
<div class="meta-line">Venue: AAMAS 2026, Paphos, IFAAMAS, 10 pages</div>
<div class="meta-line">First: 2025-12-19T13:24:43+00:00 · Latest: 2026-02-16T13:27:34+00:00</div>
<div class="meta-line">Comments: AAMAS CC-BY 4.0 licence. Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis. Full paper. In Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 - 29, 2026, IFAAMAS, 10 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17979v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17979v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers&#x27; strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间双重拍卖市场中的自适应智能体：工业共生涌现机制建模</div>
<div class="mono" style="margin-top:8px">工业共生通过促使企业重新利用残余资源促进循环经济，但其涌现过程受制于影响成本、匹配机会和市场效率的社会空间摩擦。现有模型常忽视空间结构、市场设计与自适应企业行为间的相互作用，限制了对共生现象产生条件与机制的理解。本研究构建基于智能体的模型：异质企业通过空间嵌入的双重拍卖市场交易副产品，价格与数量通过本地互动内生形成。企业运用强化学习调整竞价策略以最大化利润，同时考量运输成本、处置惩罚与资源稀缺性。仿真实验揭示了分散化交易趋于稳定高效结果的经济与空间条件。反事实遗憾分析表明卖方策略接近纳什均衡，敏感性分析则凸显空间结构与市场参数如何共同调控循环性。本模型为探索政策干预如何协调企业激励与可持续目标提供基础，更广泛地论证了空间约束市场中自适应智能体如何实现分散化协调。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand how industrial symbiosis emerges despite socio-spatial frictions that affect costs and matching, this paper develops an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market. The method employs reinforcement learning to enable firms to adapt their bidding strategies, accounting for transport costs, disposal penalties, and resource scarcity, with prices and quantities emerging endogenously from local interactions. Experimental results from simulations reveal the economic and spatial conditions leading to stable, efficient decentralized exchanges, with counterfactual regret analysis showing sellers&#x27; strategies approach a near Nash equilibrium, and sensitivity analysis highlighting how spatial structures and market parameters jointly govern circularity.</div>
<div class="mono" style="margin-top:8px">本文旨在探究工业共生如何在影响成本和匹配的社会空间摩擦下形成，为此构建了一个基于主体的模型，其中异质企业通过空间嵌入的双重拍卖市场交易副产品。方法上采用强化学习使企业能自适应调整竞价策略，考虑运输成本、处置惩罚和资源稀缺性，价格和数量从局部互动中内生涌现。仿真实验结果表明，经济和空间条件共同促使分散交换趋于稳定高效，反事实遗憾分析显示卖方策略接近纳什均衡，敏感性分析则强调了空间结构和市场参数如何共同调控循环性。</div>
</details>
</div>
<div class="card">
<div class="title">ManeuverNet: A Soft Actor-Critic Framework for Precise Maneuvering of Double-Ackermann-Steering Robots with Optimized Reward Functions</div>
<div class="meta-line">Authors: Kohio Deflesselle, Mélodie Daniel, Aly Magassouba, Miguel Aranda, Olivier Ly</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-16T13:19:04+00:00 · Latest: 2026-02-16T13:19:04+00:00</div>
<div class="meta-line">Comments: 8 pages, 5, figures, Accepted for 2026 IEEE International Conference on Robotics &amp; Automation (ICRA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14726v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14726v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous control of double-Ackermann-steering robots is essential in agricultural applications, where robots must execute precise and complex maneuvers within a limited space. Classical methods, such as the Timed Elastic Band (TEB) planner, can address this problem, but they rely on parameter tuning, making them highly sensitive to changes in robot configuration or environment and impractical to deploy without constant recalibration. At the same time, end-to-end deep reinforcement learning (DRL) methods often fail due to unsuitable reward functions for non-holonomic constraints, resulting in sub-optimal policies and poor generalization. To address these challenges, this paper presents ManeuverNet, a DRL framework tailored for double-Ackermann systems, combining Soft Actor-Critic with CrossQ. Furthermore, ManeuverNet introduces four specifically designed reward functions to support maneuver learning. Unlike prior work, ManeuverNet does not depend on expert data or handcrafted guidance. We extensively evaluate ManeuverNet against both state-of-the-art DRL baselines and the TEB planner. Experimental results demonstrate that our framework substantially improves maneuverability and success rates, achieving more than a 40% gain over DRL baselines. Moreover, ManeuverNet effectively mitigates the strong parameter sensitivity observed in the TEB planner. In real-world trials, ManeuverNet achieved up to a 90% increase in maneuvering trajectory efficiency, highlighting its robustness and practical applicability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ManeuverNet：基于优化奖励函数的双阿克曼转向机器人精确操控软演员-评论家框架</div>
<div class="mono" style="margin-top:8px">双阿克曼转向机器人的自主控制在农业应用中至关重要，机器人需在有限空间内执行精确复杂的机动动作。传统方法（如定时弹性带规划器）虽能解决此问题，但依赖参数调优，对机器人配置或环境变化极为敏感，需持续重新校准才能部署。同时，端到端深度强化学习方法常因奖励函数不适用于非完整约束而失效，导致策略次优且泛化能力差。为此，本文提出专为双阿克曼系统设计的深度强化学习框架ManeuverNet，融合软演员-评论家与CrossQ算法，并引入四种专门设计的奖励函数以支持机动学习。与先前研究不同，ManeuverNet不依赖专家数据或人工引导。我们通过大量实验将ManeuverNet与前沿深度强化学习基线及定时弹性带规划器进行对比，结果表明该框架显著提升机动性与成功率，较深度强化学习基线提升超40%，且有效缓解了定时弹性带规划器的强参数敏感性。实际测试中，ManeuverNet的机动轨迹效率最高提升90%，彰显其鲁棒性与实用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces ManeuverNet, a deep reinforcement learning framework designed to address the challenge of precise autonomous maneuvering for double-Ackermann-steering robots in constrained agricultural spaces. The method combines the Soft Actor-Critic algorithm with CrossQ and employs four specifically designed reward functions to effectively learn control policies without relying on expert demonstrations or manual tuning. Experimental results show that ManeuverNet significantly outperforms both deep reinforcement learning baselines and the classical Timed Elastic Band planner, achieving over a 40% improvement in success rates and reducing parameter sensitivity, with real-world trials demonstrating up to a 90% increase in trajectory efficiency.</div>
<div class="mono" style="margin-top:8px">本文提出了ManeuverNet，一个专为双阿克曼转向机器人在有限农业空间内进行精确自主机动而设计的深度强化学习框架。该方法将Soft Actor-Critic算法与CrossQ相结合，并设计了四种特定的奖励函数，从而无需依赖专家数据或手工指导即可学习控制策略。实验结果表明，ManeuverNet在机动性和成功率上显著优于先进的深度强化学习基准和经典的Timed Elastic Band规划器，成功率提升超过40%，并有效降低了参数敏感性，真实世界试验中轨迹效率最高提升了90%，凸显了其鲁棒性和实际应用价值。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs</div>
<div class="meta-line">Authors: Lunjun Zhang, Ryan Chen, Bradly C. Stadie</div>
<div class="meta-line">First: 2026-02-16T12:34:27+00:00 · Latest: 2026-02-16T12:34:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14697v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14697v1">PDF</a> · <a href="https://github.com/LunjunZhang/E-SPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化系统提示学习可促进大语言模型的强化学习</div>
<div class="mono" style="margin-top:8px">构建能够从经验中自主自我改进的智能体系统是人工智能的长期目标。当前大语言模型主要通过两种机制实现自我改进：通过自我反思更新上下文，以及通过强化学习更新权重。本研究提出进化系统提示学习方法，用于协同改进模型上下文与模型权重。在每次强化学习迭代中，该方法并行选择多个系统提示并执行推演，针对每个系统提示条件更新模型权重，同时通过大语言模型驱动的突变与交叉对系统提示种群进行进化更新。每个系统提示拥有用于进化选择的TrueSkill评级，该评级根据每批强化学习迭代中的相对表现更新。该方法促进了提示编码的陈述性知识与权重编码的程序性知识之间的自然分工，从而在推理和智能体任务中实现性能提升。例如在易到难（AIME→BeyondAIME）泛化场景中，该方法将强化学习成功率从38.8%提升至45.1%，同时优于反思式提示进化方法（40.0%）。总体而言，研究结果表明强化学习与系统提示进化的耦合能持续提升样本效率与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to advance autonomous AI systems by enabling large language models (LLMs) to self-improve more effectively, moving beyond isolated methods like self-reflection or reinforcement learning (RL). The proposed method, Evolutionary System Prompt Learning (E-SPL), jointly optimizes model contexts and weights by running parallel RL rollouts with multiple system prompts, applying RL updates to weights conditioned on each prompt and evolving the prompt population through LLM-driven mutation and crossover, with selection based on TrueSkill ratings updated from batch performance. Experimental results demonstrate that E-SPL improves performance on reasoning and agentic tasks, notably increasing RL success rates from 38.8% to 45.1% in an easy-to-hard generalization setting and outperforming reflective prompt evolution (40.0%), showing consistent gains in sample efficiency and generalization.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提升大型语言模型（LLMs）的自主改进能力来推动智能体系统发展，超越自我反思或强化学习（RL）等孤立方法。提出的方法——进化系统提示学习（E-SPL）——通过并行运行多个系统提示的RL滚动来联合优化模型上下文和权重，对每个提示条件下的权重应用RL更新，并通过LLM驱动的突变和交叉进化提示种群，选择基于批次性能更新的TrueSkill评分。实验结果表明，E-SPL在推理和智能体任务上提升了性能，特别是在易到难泛化设置中将RL成功率从38.8%提高至45.1%，优于反思性提示进化方法（40.0%），在样本效率和泛化性方面取得了稳定增益。</div>
</details>
</div>
<div class="card">
<div class="title">GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses</div>
<div class="meta-line">Authors: Attila Lischka, Balázs Kulcsár</div>
<div class="meta-line">First: 2026-02-16T12:04:14+00:00 · Latest: 2026-02-16T12:04:14+00:00</div>
<div class="meta-line">Comments: 29 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14676v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14676v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GREAT-EER：面向应急疏散响应的图边注意力网络</div>
<div class="mono" style="margin-top:8px">城市区域应急疏散需求可能源于人为因素（如恐怖袭击或工业事故）或自然灾害，后者因气候变化而日益频发。因此，开发高效快速的疏散规划方法至关重要。本研究提出并定义了公交疏散定向问题（BEOP），这是一个NP难组合优化问题，目标是在预设的短时间内通过公交车从受灾区域疏散尽可能多的人员。采用公交疏散旨在缓解纯私家车疏散场景中产生的拥堵与混乱。为解决BEOP，我们提出一种基于深度强化学习与图学习的方法，该方法训练后具备快速推理能力，可在秒级时间内生成疏散路线。我们通过混合整数线性规划模型对疏散方案的优化间隙进行界定。为验证方法有效性，基于真实道路网络与行程时间构建了旧金山疏散场景。实验表明，该方法能获得接近最优的解决方案，并能在合理运行时间内，进一步探究在给定疏散时限下达成特定公交疏散配额所需的车辆规模。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing frequency of natural disasters and man-made emergencies requiring urban evacuations, this paper introduces the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization challenge aimed at maximizing bus-based evacuations within a limited time to reduce congestion compared to car-focused plans. The method employs a deep reinforcement learning approach with graph learning, specifically a Graph Edge Attention Network, to rapidly generate evacuation routes with fast inference speeds. Experimental results using real-world San Francisco road networks demonstrate near-optimal solution quality, with the approach effectively determining the necessary number of evacuation vehicles to meet quotas while maintaining adequate runtimes, as validated against an MILP formulation.</div>
<div class="mono" style="margin-top:8px">本文针对自然灾害和人为紧急事件频发、需进行城市疏散的背景，提出了公交疏散定向问题（BEOP），这是一个NP难组合优化问题，旨在通过公交在有限时间内最大化疏散人数，以减少纯汽车疏散导致的拥堵。方法采用基于图学习的深度强化学习，特别是图边注意力网络，以实现快速推理并生成疏散路线。实验利用旧金山真实道路网络进行验证，结果表明该方法能获得接近最优的解决方案质量，并能有效确定所需疏散车辆数以达成预设配额，同时保持合理的运行时间，其性能通过混合整数线性规划模型得到了边界验证。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR</div>
<div class="meta-line">Authors: Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang</div>
<div class="meta-line">First: 2025-09-02T17:22:46+00:00 · Latest: 2026-02-16T11:42:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02522v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.02522v2">PDF</a> · <a href="https://github.com/ritzz-ai/PACS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, inherent to RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while providing more stable and efficient training. Extensive experiments demonstrate that PACS significantly outperforms strong open-source models and RLVR baselines, yielding substantial average gains of $\textbf{+8.26\%}$ (4B) and $\textbf{+9.57\%}$ (8B) over base models offering a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于监督学习框架的RLVR隐式行动者-评论者耦合方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）的最新进展使大语言模型（LLM）能够应对数学和编程等复杂推理任务。尽管前景广阔，但RLVR范式面临显著挑战，现有方法常受限于稀疏奖励信号和基于RL方法固有的策略梯度更新不稳定问题。为此，我们提出$\textbf{PACS}$——一种通过$\textbf{S}$监督学习框架实现隐式$\textbf{P}$行动者-$\textbf{C}$评论者$\textbf{A}$耦合的新型RLVR框架。通过将结果奖励视为可预测标签，我们将RLVR问题重构为对策略模型参数化评分函数的监督学习任务，并采用交叉熵损失进行优化。梯度分析表明，该监督形式在提供更稳定高效训练的同时，本质恢复了经典策略梯度更新。大量实验证明，PACS显著优于主流开源模型与RLVR基线，在4B和8B基础模型上分别实现$\textbf{+8.26\%}$和$\textbf{+9.57\%}$的平均性能提升，为LLM的可验证奖励后训练提供了新途径。代码与数据已开源：https://github.com/ritzz-ai/PACS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of sparse rewards and unstable policy gradient updates in Reinforcement Learning with Verifiable Rewards (RLVR) for large language models, this paper introduces PACS, a novel framework that implicitly couples the actor and critic by reformulating RLVR as a supervised learning task. The method treats the outcome reward as a predictable label, parameterizes a score function with the policy model, and optimizes it using cross-entropy loss, which gradient analysis shows inherently recovers the policy gradient while ensuring more stable and efficient training. Experimental results demonstrate that PACS significantly outperforms strong open-source models and RLVR baselines, achieving average gains of +8.26% and +9.57% over base models at 4B and 8B scales, respectively, offering a promising approach for LLM post-training with verifiable rewards.</div>
<div class="mono" style="margin-top:8px">针对基于可验证奖励的强化学习（RLVR）在大语言模型中面临的奖励稀疏和策略梯度更新不稳定等挑战，本文提出了PACS框架，通过将RLVR重新构建为监督学习任务，实现了演员与评论家的隐式耦合。该方法将结果奖励视为可预测标签，利用策略模型参数化评分函数，并通过交叉熵损失进行优化，梯度分析表明该监督公式本质上恢复了经典策略梯度更新，同时确保了更稳定高效的训练。实验结果表明，PACS显著优于开源模型和RLVR基线，在4B和8B规模上相比基础模型平均分别提升了8.26%和9.57%，为基于可验证奖励的大语言模型后训练提供了一条有前景的途径。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application</div>
<div class="meta-line">Authors: Thomas Hickling, Maxwell Hogan, Abdulla Tammam, Nabil Aouf</div>
<div class="meta-line">First: 2025-02-27T17:53:16+00:00 · Latest: 2026-02-16T11:36:33+00:00</div>
<div class="meta-line">Comments: 22 Pages, 24 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.20326v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.20326v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents the first end-to-end framework that combines guidance, navigation, and centralised task allocation for multiple UAVs performing autonomous search-and-rescue (SAR) in GNSS-denied indoor environments. A Twin Delayed Deep Deterministic Policy Gradient controller is trained with an Artificial Potential Field (APF) reward that blends attractive and repulsive potentials with continuous control, accelerating convergence and yielding smoother, safer trajectories than distance-only baselines. Collaborative mission assignment is solved by a deep Graph Attention Network that, at each decision step, reasons over the drone-task graph to produce near-optimal allocations with negligible on-board compute. To arrest the notorious Z-drift of indoor LiDAR-SLAM, we fuse depth-camera altimetry with IMU vertical velocity in a lightweight complementary filter, giving centimetre-level altitude stability without external beacons. The resulting system was deployed on two 1m-class quad-rotors and flight-tested in a cluttered, multi-level disaster mock-up designed for the NATO-Sapience Autonomous Cooperative Drone Competition. Compared with prior DRL guidance that remains largely in simulation, our framework demonstrates an ability to navigate complex indoor environments, securing first place in the 2024 event. These results demonstrate that APF-shaped DRL and GAT-driven cooperation can translate to reliable real-world SAR operations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的协作无人机自主决策：搜救现实应用</div>
<div class="mono" style="margin-top:8px">本文首次提出一种端到端框架，将引导、导航与集中式任务分配相结合，用于多架无人机在无GNSS信号的室内环境中执行自主搜救任务。采用融合吸引与排斥势场的人工势场奖励训练双延迟深度确定性策略梯度控制器，结合连续控制加速收敛，相比仅基于距离的基线方法生成更平滑、安全的轨迹。协作任务分配通过深度图注意力网络解决，该网络在每个决策步骤对无人机-任务图进行推理，以可忽略的机载计算成本生成接近最优的分配方案。为抑制室内LiDAR-SLAM固有的Z轴漂移，采用轻量级互补滤波器融合深度相机测高与IMU垂直速度，在不依赖外部信标的情况下实现厘米级高度稳定性。该系统部署于两架1米级四旋翼无人机，在专为北约Sapience自主协作无人机竞赛设计的杂乱多层灾难模拟场景中进行飞行测试。与多数仍停留在仿真阶段的现有DRL引导方法相比，本框架展现出在复杂室内环境中的导航能力，并于2024年赛事中获得第一名。结果表明，APF优化的DRL与GAT驱动的协作机制可转化为可靠的实际搜救操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for autonomous cooperative UAVs to perform search and rescue in GPS-denied indoor environments, this paper introduces an end-to-end framework integrating guidance, navigation, and centralized task allocation. The method employs a Twin Delayed Deep Deterministic Policy Gradient controller trained with an Artificial Potential Field reward for smooth trajectory planning and a deep Graph Attention Network for efficient multi-drone task assignment, alongside a sensor fusion technique using depth cameras and IMU to stabilize altitude. Experimental results from real-world flight tests with two quad-rotors in a cluttered disaster mock-up show the system successfully navigates complex indoor settings, achieving first place in a 2024 competition and demonstrating reliable real-world performance.</div>
<div class="mono" style="margin-top:8px">本文旨在解决无人机在无GPS室内环境中自主协同执行搜救任务的挑战，提出了一个集引导、导航和集中任务分配于一体的端到端框架。方法上，采用基于人工势场奖励的双延迟深度确定性策略梯度控制器进行平滑轨迹规划，利用深度图注意力网络实现高效的多无人机任务分配，并结合深度相机与IMU的传感器融合技术以稳定高度。通过在杂乱多层灾难模拟场景中对两架四旋翼无人机进行实际飞行测试，实验结果表明该系统能成功导航复杂室内环境，在2024年比赛中获得第一名，验证了其在真实世界搜救操作中的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Heterogeneous RBCs via Deep Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Federico Gabriele, Aldo Glielmo, Marco Taboga</div>
<div class="meta-line">Venue: Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026, https://cyprusconferences.org/aamas2026/)</div>
<div class="meta-line">First: 2025-10-14T08:26:18+00:00 · Latest: 2026-02-16T10:49:57+00:00</div>
<div class="meta-line">Comments: 14 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.12272v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.12272v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current macroeconomic models with agent heterogeneity can be broadly divided into two main groups. Heterogeneous-agent general equilibrium (GE) models, such as those based on Heterogeneous Agent New Keynesian (HANK) or Krusell-Smith (KS) approaches, rely on GE and &#x27;rational expectations&#x27;, somewhat unrealistic assumptions that make the models very computationally cumbersome, which in turn limits the amount of heterogeneity that can be modelled. In contrast, agent-based models (ABMs) can flexibly encompass a large number of arbitrarily heterogeneous agents, but typically require the specification of explicit behavioural rules, which can lead to a lengthy trial-and-error model-development process. To address these limitations, we introduce MARL-BC, a framework that integrates deep multi-agent reinforcement learning (MARL) with real business cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover textbook RBC results when using a single agent; (2) recover the results of the mean-field KS model using a large number of identical agents; and (3) effectively simulate rich heterogeneity among agents, a hard task for traditional GE approaches. Our framework can be thought of as an ABM if used with a variety of heterogeneous interacting agents, and can reproduce GE results in limit cases. As such, it is a step towards a synthesis of these often opposed modelling paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度多智能体强化学习的异质性实际经济周期模型</div>
<div class="mono" style="margin-top:8px">当前具有智能体异质性的宏观经济模型主要分为两大类：一类是基于异质性智能体新凯恩斯主义或克鲁塞尔-史密斯方法的异质性智能体一般均衡模型，这类模型依赖一般均衡和&#x27;理性预期&#x27;假设，计算复杂度高且限制了异质性建模的规模；另一类是基于智能体的模型虽能灵活容纳大量异质智能体，但需预设明确行为规则，导致模型开发过程冗长。为突破这些局限，我们提出MARL-BC框架，将深度多智能体强化学习与实际经济周期模型相结合。该框架能够：（1）在单智能体场景下复现经典实际经济周期结果；（2）通过大量同质智能体重现平均场克鲁塞尔-史密斯模型结果；（3）有效模拟智能体间的丰富异质性——这对传统一般均衡方法极具挑战。该框架既可视为包含异质交互智能体的基于智能体模型，又能在极限情况下复现一般均衡结果，为融合这两种常对立的建模范式提供了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing macroeconomic models—where heterogeneous-agent general equilibrium models are computationally cumbersome and rely on unrealistic assumptions, while agent-based models require explicit behavioral rules and trial-and-error development—this paper introduces MARL-BC, a framework integrating deep multi-agent reinforcement learning with real business cycle models. The method employs MARL to enable agents to learn adaptive behaviors through interaction, avoiding the need for pre-specified rules or complex equilibrium computations. Experimental results demonstrate that MARL-BC successfully recovers standard RBC outcomes with a single agent, replicates mean-field Krusell-Smith model results with many identical agents, and effectively simulates rich agent heterogeneity, which is challenging for traditional approaches, thereby bridging agent-based and general equilibrium modeling paradigms.</div>
<div class="mono" style="margin-top:8px">针对现有宏观经济模型的局限性——异质性主体一般均衡模型计算繁琐且依赖不现实假设，而基于主体的模型需要明确行为规则和试错开发——本文提出了MARL-BC框架，将深度多智能体强化学习与真实商业周期模型相结合。该方法利用多智能体强化学习使主体通过交互学习自适应行为，避免了预先指定规则或复杂均衡计算。实验结果表明，MARL-BC在单个主体下成功恢复了标准RBC结果，在多个相同主体下复现了平均场Krusell-Smith模型结果，并有效模拟了丰富的异质性，这是传统方法难以实现的，从而桥接了基于主体与一般均衡建模范式。</div>
</details>
</div>
<div class="card">
<div class="title">A representational framework for learning and encoding structurally enriched trajectories in complex agent environments</div>
<div class="meta-line">Authors: Corina Catarau-Cotutiu, Esther Mondragon, Eduardo Alonso</div>
<div class="meta-line">First: 2025-03-17T14:04:27+00:00 · Latest: 2026-02-16T10:37:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.13194v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.13194v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability of artificial intelligence agents to make optimal decisions and generalise them to different domains and tasks is compromised in complex scenarios. One way to address this issue has focused on learning efficient representations of the world and on how the actions of agents affect them in state-action transitions. Whereas such representations are procedurally efficient, they lack structural richness. To address this problem, we propose to enhance the agent&#x27;s ontology and extend the traditional conceptualisation of trajectories to provide a more nuanced view of task execution. Structurally Enriched Trajectories (SETs) extend the encoding of sequences of states and their transitions by incorporating hierarchical relations between objects, interactions, and affordances. SETs are built as multi-level graphs, providing a detailed representation of the agent dynamics and a transferable functional abstraction of the task. SETs are integrated into an architecture, Structurally Enriched Trajectory Learning and Encoding (SETLE), that employs a heterogeneous graph-based memory structure of multi-level relational dependencies essential for generalisation. We demonstrate that SETLE can support downstream tasks, enabling agents to recognise task relevant structural patterns across CREATE and MiniGrid environments. Finally, we integrate SETLE with reinforcement learning and show measurable improvements in downstream performance, including breakthrough success rates in complex, sparse-reward tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复杂智能体环境中结构增强轨迹学习与编码的表征框架</div>
<div class="mono" style="margin-top:8px">人工智能智能体在复杂场景中做出最优决策并将其泛化至不同领域与任务的能力受到限制。现有研究多聚焦于学习世界的有效表征，以及智能体行为在状态-动作转换中的影响，此类表征虽具过程效率，但缺乏结构丰富性。为此，我们提出通过增强智能体本体并扩展传统轨迹概念化方式，以提供更精细的任务执行视角。结构增强轨迹通过纳入对象间层次关系、交互作用与功能可供性，扩展了对状态序列及其转换的编码方式。该框架构建为多层图结构，既能详细表征智能体动态，又可形成任务的可迁移功能抽象。基于此开发的“结构增强轨迹学习与编码”架构，采用基于异质图的多层关系依赖记忆结构，这对实现泛化至关重要。实验表明，该架构能支持下游任务，使智能体在CREATE和MiniGrid环境中识别任务相关的结构模式。最后，通过将架构与强化学习结合，在下游任务性能（包括复杂稀疏奖励任务中的突破性成功率）上实现了可量化的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for AI agents to make optimal decisions and generalize in complex environments, this paper proposes Structurally Enriched Trajectories (SETs) to enhance traditional state-action representations with hierarchical relations between objects, interactions, and affordances, encoded as multi-level graphs. The method, SETLE, integrates these into a heterogeneous graph-based memory architecture to capture relational dependencies for generalization. Experimental results demonstrate that SETLE enables agents to recognize task-relevant structural patterns in CREATE and MiniGrid environments, and when combined with reinforcement learning, it leads to measurable performance improvements, including higher success rates in complex, sparse-reward tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决人工智能智能体在复杂环境中做出最优决策并实现泛化能力不足的问题，提出了一种结构增强轨迹（SETs）方法，通过将对象、交互和可供性之间的层次关系编码为多层图，来丰富传统的状态-动作轨迹表示。该方法SETLE将这些轨迹集成到一个基于异质图的内存架构中，以捕获对泛化至关重要的多级关系依赖。实验结果表明，SETLE能使智能体在CREATE和MiniGrid环境中识别任务相关的结构模式，并且与强化学习结合后，在下游任务中带来了可衡量的性能提升，包括在复杂、稀疏奖励任务中实现了突破性的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">A Bayesian Approach to Low-Discrepancy Subset Selection</div>
<div class="meta-line">Authors: Nathan Kirk</div>
<div class="meta-line">First: 2026-02-16T10:11:07+00:00 · Latest: 2026-02-16T10:11:07+00:00</div>
<div class="meta-line">Comments: 13 pages, 3 figures, mODa14</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14607v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-discrepancy designs play a central role in quasi-Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP-hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP-hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低差异子集选择的贝叶斯方法</div>
<div class="mono" style="margin-top:8px">低差异设计在拟蒙特卡洛方法中占据核心地位，并日益影响机器学习、机器人学和计算机图形学等多个领域。近年来，一种称为子集选择的低差异构造方法备受关注。该方法从大规模总体中，基于差异度目标函数最优选取小型低差异子集。已知该问题的若干变体属于NP难问题。本文首次证明，基于核差异的子集选择问题同样具有NP难性。受此计算困难性驱动，我们提出一种利用深度嵌入核新概念的贝叶斯优化方法，用于解决子集选择问题。实验表明该BO算法能有效最小化差异度量，且该框架可广泛适用于各类设计准则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the NP-hardness of selecting low-discrepancy subsets from large populations, which is crucial for applications in quasi-Monte Carlo and machine learning, this paper proposes a Bayesian Optimization (BO) method using deep embedding kernels to tackle the subset selection problem. The method efficiently searches for subsets that minimize kernel-based discrepancy measures. Experimental results demonstrate that the BO framework effectively reduces discrepancy and is broadly applicable to various design criteria.</div>
<div class="mono" style="margin-top:8px">本文针对从大规模群体中选择低差异子集这一NP难问题（在拟蒙特卡罗和机器学习中至关重要），提出了一种基于深度嵌入核的贝叶斯优化方法来解决子集选择问题。该方法能高效搜索以最小化核差异度量的子集。实验结果表明，该贝叶斯优化框架能有效降低差异，并广泛适用于多种设计准则。</div>
</details>
</div>
<div class="card">
<div class="title">From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism</div>
<div class="meta-line">Authors: Sarthak Wanjari</div>
<div class="meta-line">First: 2026-02-09T13:48:25+00:00 · Latest: 2026-02-16T09:49:12+00:00</div>
<div class="meta-line">Comments: 10 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08655v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08655v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds. Current solutions necessitate a trade-off between computational efficiency and performance. Methods like CQL offer rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair, our method injects OOD conservatism via reward shaping with a O(1) training overhead to the training loop. Evaluated on the D4RL MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed standard-deviation by 4 times. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, it achieves 86.4% terminal agreement with clinicians compared to IQL&#x27;s 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从机器人学到脓毒症治疗：基于几何悲观主义的离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）旨在从静态数据集中恢复最优策略，但其仍易高估分布外（OOD）动作，尤其在断裂稀疏的数据流形中。现有方法需在计算效率与性能间权衡：CQL等方法虽提供严格保守性但需巨大算力，而基于期望分位数的高效方法（如IQL）常无法修正病态数据集上的OOD误差，退化为行为克隆。本研究提出几何悲观主义——一种模块化、计算高效的框架，通过基于k近邻距离的状态-动作嵌入空间密度惩罚来增强标准IQL。通过预计算应用于各状态-动作对的惩罚项，本方法以O(1)训练开销通过奖励塑形注入OOD保守性。在D4RL MuJoCo基准测试中，我们的Geo-IQL方法在敏感不稳定的medium-replay任务上超越标准IQL超过18分，同时将随机种子间标准差降低4倍，且在稳定流形上性能无衰减。关键的是，我们在MIMIC-III脓毒症重症监护数据集上验证了算法：当标准IQL退化为行为克隆时，Geo-IQL展现出主动策略改进，在保持安全约束下与临床医生决策的终局符合率达86.4%（IQL为75%）。结果表明几何悲观主义为关键现实决策系统安全克服局部最优提供了必要正则化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in offline reinforcement learning where existing methods either demand high computational resources or fail to correct out-of-distribution errors, often collapsing to behavioral cloning. The authors propose Geometric Pessimism, a modular framework that enhances the efficient IQL method by adding a density-based penalty derived from k-nearest-neighbor distances in the state-action embedding space, introducing conservatism with minimal training overhead. Experimental results on the D4RL MuJoCo benchmark show that Geo-IQL outperforms standard IQL by over 18 points on sensitive tasks and reduces performance variability, while on the MIMIC-III Sepsis dataset, it achieves active policy improvement with 86.4% terminal agreement with clinicians, compared to IQL&#x27;s 75%, demonstrating safer and more effective decision-making in critical real-world applications.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中现有方法要么计算成本高昂，要么无法纠正分布外误差并常退化为行为克隆的问题，提出了一种名为几何悲观主义的模块化框架。该方法通过基于状态-动作嵌入空间中k近邻距离的密度惩罚来增强高效的IQL算法，以极小的训练开销引入保守性。在D4RL MuJoCo基准测试中，Geo-IQL在敏感任务上比标准IQL性能提升超过18分，并降低了性能波动；在MIMIC-III脓毒症数据集上，它实现了主动的策略改进，与临床医生决策的终端一致性达到86.4%，而IQL仅为75%，表明其在关键现实世界决策系统中能提供更安全有效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow</div>
<div class="meta-line">Authors: Minh Nguyen</div>
<div class="meta-line">First: 2026-02-16T09:35:25+00:00 · Latest: 2026-02-16T09:35:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14587v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14587v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于哈密顿流解耦的连续时间强化学习</div>
<div class="mono" style="margin-top:8px">从金融到机器人领域的许多现实控制问题都在连续时间中演化，涉及非均匀、事件驱动的决策。基于固定步长贝尔曼更新的标准离散时间强化学习在此场景中面临困难：随着时间间隔缩小，Q函数会坍缩为价值函数V，导致动作排序失效。现有连续时间方法通过优势率函数q重新引入动作信息，但需借助复杂的鞅损失或正交约束来保证最优性，这些约束对测试过程的选择敏感，且将V和q耦合为庞大复杂的优化问题，难以稳定训练。为突破这些局限，我们提出一种新型解耦连续时间行动者-评判者算法，采用交替更新机制：通过V的扩散生成器学习q，并基于哈密顿价值流更新V——该机制在无穷小时间步下仍保持信息有效性，而标准最大/软最大值备份在此场景中会失效。理论上，我们通过新颖的概率论证证明了严格收敛性，规避了基于生成器的哈密顿量在无穷范数下缺乏贝尔曼式收缩特性的难题。实证表明，在挑战性连续控制基准和真实交易任务中，本方法优于现有连续时间及主流离散时间基线，单季度实现21%收益——接近次优方法的两倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of standard discrete-time reinforcement learning (RL) and existing continuous-time methods in handling real-world control problems with continuous, event-driven decisions, where fixed-step approaches struggle as time gaps shrink, causing the Q-function to collapse. The proposed method introduces a decoupled continuous-time actor-critic algorithm that alternates updates: it learns an advantage-rate function q from diffusion generators on the value function V, and updates V via a Hamiltonian-based value flow that remains effective under infinitesimal time steps, avoiding the entanglement and training difficulties of prior approaches. Theoretically, the authors prove convergence using new probabilistic arguments, and empirically, the method outperforms both continuous-time and leading discrete-time baselines on continuous-control benchmarks and a real-world trading task, achieving a 21% profit over a single quarter, nearly doubling the second-best method.</div>
<div class="mono" style="margin-top:8px">本文针对标准离散时间强化学习和现有连续时间方法在处理具有连续、事件驱动决策的现实世界控制问题时的局限性，其中固定步长方法在时间间隔缩小时会导致Q函数崩溃。所提出的方法引入了一种解耦的连续时间行动者-评论家算法，交替更新：通过价值函数V的扩散生成器学习优势率函数q，并通过基于哈密顿量的价值流更新V，该流在无限小时间步下仍保持有效，避免了先前方法的纠缠和训练困难。理论上，作者使用新的概率论证证明了收敛性；实证上，该方法在连续控制基准测试和现实世界交易任务中优于先前的连续时间和领先的离散时间基线，在一个季度内实现了21%的利润，几乎是第二佳方法的两倍。</div>
</details>
</div>
<div class="card">
<div class="title">RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch</div>
<div class="meta-line">Authors: Isam Vrce, Andreas Kassler, Gökçe Aydos</div>
<div class="meta-line">First: 2026-02-16T09:17:29+00:00 · Latest: 2026-02-16T09:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14578v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RNM-TD3：从零开始的N:M半结构化稀疏强化学习</div>
<div class="mono" style="margin-top:8px">稀疏性是一种经过深入研究的深度神经网络压缩技术，可在不影响性能的前提下实现压缩。在深度强化学习中，仅保留原始权重5%的神经网络经过训练后，其性能损失相较于稠密网络仍可控制在最小范围内。然而，现有方法大多依赖非结构化细粒度稀疏性，其不规则的计算模式限制了硬件加速潜力。结构化粗粒度稀疏性虽能实现硬件加速，但通常会导致性能下降并增加剪枝复杂度。本研究首次在强化学习领域探索N:M结构化稀疏性，旨在平衡压缩率、性能与硬件效率。该框架在离线策略强化学习的所有网络中全程实施行级N:M稀疏性训练，保持了对支持N:M稀疏矩阵运算加速器的兼容性。在连续控制基准测试中，我们的N:M稀疏智能体RNM-TD3在50%-75%稀疏度下性能优于稠密网络，如在Ant环境中2:4稀疏度下性能提升达14%。即使在87.5%稀疏度下，RNM-TD3仍保持竞争力，同时具备潜在训练加速能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to leverage structured sparsity in deep reinforcement learning to achieve efficient model compression without sacrificing performance, addressing the limitations of unstructured sparsity that hinder hardware acceleration and structured sparsity that often degrades performance. The method introduces RNM-TD3, a framework that enforces row-wise N:M semi-structured sparsity throughout training in the TD3 off-policy algorithm, ensuring compatibility with accelerators supporting such sparse operations. Experimental results on continuous-control benchmarks demonstrate that RNM-TD3 outperforms dense counterparts at 50%-75% sparsity, with up to a 14% performance increase at 2:4 sparsity in the Ant environment, and remains competitive even at 87.5% sparsity while enabling potential training speedups.</div>
<div class="mono" style="margin-top:8px">本研究的动机是利用深度强化学习中的结构化稀疏性，在实现高效模型压缩的同时不牺牲性能，以解决非结构化稀疏性阻碍硬件加速和结构化稀疏性常导致性能下降的问题。方法上提出了RNM-TD3框架，在TD3离线策略算法中全程实施行级N:M半结构化稀疏，确保与支持此类稀疏操作的加速器兼容。在连续控制基准测试中的实验结果表明，RNM-TD3在50%-75%稀疏度下优于密集模型，如在Ant环境中2:4稀疏度下性能提升高达14%，即使在87.5%稀疏度下仍保持竞争力，并可能加速训练过程。</div>
</details>
</div>
<div class="card">
<div class="title">Fluid-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Shishir Sharma, Doina Precup, Theodore J. Perkins</div>
<div class="meta-line">First: 2026-02-16T08:37:46+00:00 · Latest: 2026-02-16T08:37:46+00:00</div>
<div class="meta-line">Comments: Published in the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14559v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14559v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>流体智能体强化学习</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）的主要研究焦点一直是探讨嵌入环境中的固定数量智能体之间的交互。然而，在现实世界中，智能体的数量既非固定也非预先可知。此外，智能体可以决定创建其他智能体（例如，细胞可能分裂，或公司可能拆分部门）。本文提出一个允许智能体创建其他智能体的框架，我们称之为流体智能体环境。我们提出了流体智能体博弈的博弈论解概念，并在此框架内实证评估了多种MARL算法的性能。实验包括经典基准的流体变体，如捕食者-猎物和基于等级的觅食环境，其中智能体可动态生成，以及我们引入的新环境，突显流动性如何能解锁固定群体设置中无法观察到的新颖解决策略。我们证明该框架能产生动态调整规模以适应环境需求的智能体团队。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of traditional multi-agent reinforcement learning (MARL) in handling fixed agent populations, this paper introduces a fluid-agent framework where agents can dynamically create new agents, reflecting real-world scenarios like cell division or corporate spin-offs. The method proposes game-theoretic solution concepts for such fluid-agent games and evaluates several MARL algorithms on fluid variants of benchmarks like Predator-Prey and Level-Based Foraging, plus a new environment designed to showcase novel strategies enabled by fluidity. Experimental results demonstrate that this approach yields agent teams that dynamically adjust their size to meet environmental demands, unlocking solution strategies not possible in fixed-population settings.</div>
<div class="mono" style="margin-top:8px">本文针对传统多智能体强化学习（MARL）中智能体数量固定的局限性，提出了一种流体智能体框架，允许智能体动态创建新智能体，以模拟细胞分裂或公司分拆等现实场景。方法上，为这类流体智能体博弈提出了博弈论解概念，并在Predator-Prey和Level-Based Foraging等基准的流体变体以及一个新设计的环境中评估了多种MARL算法。实验结果表明，该框架能产生根据环境需求动态调整规模的智能体团队，从而解锁了固定数量智能体设置中无法实现的新颖解决策略。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Width Neural Networks</div>
<div class="meta-line">Authors: Federico Errica, Henrik Christiansen, Viktor Zaverkin, Mathias Niepert, Francesco Alesiani</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-01-27T09:25:56+00:00 · Latest: 2026-02-16T08:15:11+00:00</div>
<div class="meta-line">Comments: International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.15889v5">Abs</a> · <a href="https://arxiv.org/pdf/2501.15889v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For almost 70 years, researchers have typically selected the width of neural networks&#x27; layers either manually or through automated hyperparameter tuning methods such as grid search and, more recently, neural architecture search. This paper challenges the status quo by introducing an easy-to-use technique to learn an unbounded width of a neural network&#x27;s layer during training. The method jointly optimizes the width and the parameters of each layer via standard backpropagation. We apply the technique to a broad range of data domains such as tables, images, text, sequences, and graphs, showing how the width adapts to the task&#x27;s difficulty. A by product of our width learning approach is the easy truncation of the trained network at virtually zero cost, achieving a smooth trade-off between performance and compute resources. Alternatively, one can dynamically compress the network until performances do not degrade. In light of recent foundation models trained on large datasets, requiring billions of parameters and where hyper-parameter tuning is unfeasible due to huge training costs, our approach introduces a viable alternative for width learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自适应宽度神经网络</div>
<div class="mono" style="margin-top:8px">近70年来，研究者通常通过人工选择或网格搜索等自动化超参数调优方法（近年来更多采用神经架构搜索）来确定神经网络各层的宽度。本文提出一种易于使用的技术，可在训练过程中学习神经网络层的无界宽度，从而挑战这一传统范式。该方法通过标准反向传播联合优化每层的宽度与参数。我们将该技术应用于表格、图像、文本、序列及图结构等广泛数据领域，展示了宽度如何根据任务难度自适应调整。该宽度学习方法的副产品是能够以近乎零成本对训练后的网络进行截断，实现性能与计算资源间的平滑权衡；亦可动态压缩网络直至性能不出现衰减。鉴于近期基于海量数据训练的基础模型需数十亿参数，且因训练成本巨大导致超参数调优难以实施，本方法为宽度学习提供了可行的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of manual or automated hyperparameter tuning for neural network width, this paper introduces a method to learn layer widths adaptively during training via standard backpropagation, jointly optimizing width and parameters. The approach is applied across diverse data domains including tables, images, text, sequences, and graphs, demonstrating that width adapts to task difficulty, and it enables easy network truncation or compression for a smooth performance-compute trade-off without significant degradation. This offers a viable alternative to traditional tuning, especially for large-scale foundation models where exhaustive hyperparameter search is impractical due to high training costs.</div>
<div class="mono" style="margin-top:8px">针对神经网络宽度需手动或自动化超参数调优的低效问题，本文提出了一种在训练过程中通过标准反向传播自适应学习层宽度的方法，联合优化宽度和参数。该方法应用于表格、图像、文本、序列和图等多种数据领域，结果表明宽度能根据任务难度自适应调整，并支持轻松截断或压缩网络，实现性能与计算资源的平滑权衡且性能不显著下降。这为传统调优提供了可行替代方案，尤其适用于训练成本高昂、难以进行超参数搜索的大规模基础模型。</div>
</details>
</div>
<div class="card">
<div class="title">MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs</div>
<div class="meta-line">Authors: Baorong Shi, Bo Cui, Boyuan Jiang, Deli Yu, Fang Qian, Haihua Yang, Huichao Wang, Jiale Chen, Jianfei Pan, Jieqiong Cao, Jinghao Lin, Kai Wu, Lin Yang, Shengsheng Yao, Tao Chen, Xiaojun Xiao, Xiaozhong Ji, Xu Wang, Yijun He, Zhixiong Yang</div>
<div class="meta-line">First: 2026-02-13T08:19:38+00:00 · Latest: 2026-02-16T08:13:49+00:00</div>
<div class="meta-line">Comments: XIAOHE Medical AI team. Currently, the model is exclusively available on XIAOHE AI Doctor, accessible via both the App Store and the Douyin Mini Program</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12705v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12705v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedXIAOHE：构建医疗多模态大语言模型的完整方案</div>
<div class="mono" style="margin-top:8px">我们推出MedXIAOHE——一个旨在提升真实临床场景中通用医疗理解与推理能力的医疗视觉语言基础模型。该模型在多项医疗基准测试中达到最先进水平，并在多项能力上超越领先的闭源多模态系统。为实现这一目标，我们提出了一种实体感知的持续预训练框架，通过组织异构医疗语料库来扩展知识覆盖范围并减少长尾差距（如罕见疾病）。针对医疗专家级推理与交互，MedXIAOHE通过强化学习和工具增强的智能体训练融合了多样化医疗推理模式，支持具有可验证决策轨迹的多步骤诊断推理。为提升实际应用可靠性，模型整合了用户偏好评估标准、证据驱动的推理机制以及低幻觉长文本报告生成功能，显著增强了对医疗指令的遵循度。本报告旨在记录我们的实践设计选择、规模化洞察与评估框架，以期推动后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for advanced general-purpose medical understanding in real-world clinical applications, this paper introduces MedXIAOHE, a medical vision-language foundation model. The method employs an entity-aware continual pretraining framework to organize heterogeneous medical corpora, broadening knowledge coverage and addressing long-tail issues like rare diseases, and incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training for multi-step diagnostic reasoning. Main experimental results show that MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks, surpassing leading closed-source multimodal systems on multiple capabilities, while also demonstrating improved reliability through evidence-grounded reasoning and low-hallucination long-form report generation with better adherence to medical instructions.</div>
<div class="mono" style="margin-top:8px">本文旨在推动现实世界临床应用中通用医疗理解和推理的发展，提出了医疗视觉语言基础模型MedXIAOHE。方法上，采用实体感知的持续预训练框架组织异构医疗语料，以拓宽知识覆盖并减少罕见疾病等长尾问题，同时通过强化学习和工具增强的智能体训练融入多样医疗推理模式，实现可验证决策轨迹的多步诊断推理。主要实验结果表明，MedXIAOHE在多种医疗基准测试中取得了最先进的性能，在多项能力上超越了领先的闭源多模态系统，并通过证据驱动的推理、低幻觉长文本报告生成及更好的医疗指令遵循，提升了实际应用的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">TWISTED-RL: Hierarchical Skilled Agents for Knot-Tying without Human Demonstrations</div>
<div class="meta-line">Authors: Guy Freund, Tom Jurgenson, Matan Sudry, Erez Karpas</div>
<div class="meta-line">First: 2026-02-16T07:21:02+00:00 · Latest: 2026-02-16T07:21:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14526v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic knot-tying represents a fundamental challenge in robotics due to the complex interactions between deformable objects and strict topological constraints. We present TWISTED-RL, a framework that improves upon the previous state-of-the-art in demonstration-free knot-tying (TWISTED), which smartly decomposed a single knot-tying problem into manageable subproblems, each addressed by a specialized agent. Our approach replaces TWISTED&#x27;s single-step inverse model that was learned via supervised learning with a multi-step Reinforcement Learning policy conditioned on abstract topological actions rather than goal states. This change allows more delicate topological state transitions while avoiding costly and ineffective data collection protocols, thus enabling better generalization across diverse knot configurations. Experimental results demonstrate that TWISTED-RL manages to solve previously unattainable knots of higher complexity, including commonly used knots such as the Figure-8 and the Overhand. Furthermore, the increase in success rates and drop in planning time establishes TWISTED-RL as the new state-of-the-art in robotic knot-tying without human demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TWISTED-RL：无需人类演示的分层技能代理结绳系统</div>
<div class="mono" style="margin-top:8px">机器人结绳因涉及可变形物体的复杂交互与严格拓扑约束，是机器人学的基础性难题。本文提出TWISTED-RL框架，该框架改进了此前无需演示的结绳技术（TWISTED）——原方法通过智能分解将单一结绳任务转化为可处理的子问题，并由专用代理分别处理。我们的方法将TWISTED中通过监督学习训练的单步逆模型，替换为基于抽象拓扑动作（而非目标状态）的多步强化学习策略。这一改进实现了更精细的拓扑状态转换，同时避免了高成本低效的数据收集流程，从而提升了不同绳结构型的泛化能力。实验结果表明，TWISTED-RL能够完成先前无法实现的高复杂度绳结，包括八字结、反手结等常用绳结。成功率的提升与规划时间的减少，共同确立了TWISTED-RL在无需人类演示的机器人结绳领域的新标杆地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robotic knot-tying, which involves complex interactions with deformable objects under strict topological constraints. The motivation is to improve upon the prior TWISTED framework by enabling more delicate state transitions and better generalization without relying on human demonstrations. The method introduces TWISTED-RL, which replaces the original single-step inverse model with a multi-step reinforcement learning policy conditioned on abstract topological actions. Experimental results show that this approach successfully ties previously unattainable, more complex knots like the Figure-8 and Overhand, achieving higher success rates and reduced planning time, thus establishing a new state-of-the-art.</div>
<div class="mono" style="margin-top:8px">本文针对机器人打结的挑战展开研究，该任务涉及与可变形物体的复杂交互及严格的拓扑约束。其动机是在先前TWISTED框架的基础上进行改进，旨在实现更精细的状态转换和更好的泛化能力，且无需依赖人类示范。方法上提出了TWISTED-RL，用基于抽象拓扑动作的多步强化学习策略取代了原有的单步逆模型。实验结果表明，该方法能够成功完成先前无法实现的更复杂绳结（如八字结和单结），并提高了成功率、减少了规划时间，从而确立了无需人类示范的机器人打结新标杆。</div>
</details>
</div>
<div class="card">
<div class="title">Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC</div>
<div class="meta-line">Authors: Dennis Gross</div>
<div class="meta-line">First: 2026-02-16T06:37:34+00:00 · Latest: 2026-02-16T06:37:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14505v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14505v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC&#x27;s capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient&#x27;s evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC&#x27;s integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用COOL-MC对脓毒症治疗策略进行形式化验证与解释</div>
<div class="mono" style="margin-top:8px">在医疗领域，安全且可解释的序贯决策至关重要，然而用于脓毒症治疗优化的强化学习策略往往不透明且难以验证。传统概率模型检测器需处理完整状态空间，对于较大规模的马尔可夫决策过程不可行，且无法解释学习策略的决策依据。COOL-MC封装了模型检测器Storm，并新增三项核心功能：仅构建训练策略诱导的可达状态空间，生成更小的离散时间马尔可夫链以支持完整MDP分析不可行时的验证；自动用临床意义原子命题标注状态；集成可解释性方法与概率计算树逻辑查询，揭示治疗轨迹中驱动决策的特征。我们在ICU-Sepsis MDP（基于约17,000份脓毒症患者记录的基准数据集）上展示COOL-MC的功能，作为将其应用于脓毒症治疗策略形式化分析的案例研究。通过完整MDP验证建立严格边界，训练出达到最优生存概率的安全强化学习策略，并基于诱导的DTMC进行PCTL验证与可解释性分析。分析发现，训练策略主要依赖既往用药史而非患者实时病情变化——这一传统评估难以察觉的缺陷通过COOL-MC的形式化验证与可解释性集成得以暴露。研究结果表明，COOL-MC可作为临床医生在部署前研究和调试脓毒症治疗策略的有效工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for safe and interpretable sequential decision-making in healthcare, particularly for opaque reinforcement learning policies in sepsis treatment, this paper introduces COOL-MC, a tool that enhances the model checker Storm by constructing only the reachable state space of a trained policy to form a tractable discrete-time Markov chain for verification, automatically labeling states with clinical propositions, and integrating explainability methods with probabilistic computation tree logic queries. The method is demonstrated on the ICU-Sepsis MDP, derived from about 17,000 patient records, where experimental results show that COOL-MC enables full MDP verification to establish hard bounds, trains a safe RL policy achieving optimal survival probability, and uses PCTL verification to reveal that the policy overly relies on prior dosing history rather than the patient&#x27;s evolving condition, exposing weaknesses invisible to standard evaluation.</div>
<div class="mono" style="margin-top:8px">本文的动机源于医疗保健领域对安全且可解释的序列决策的需求，特别是针对脓毒症治疗中不透明的强化学习策略，为此提出了COOL-MC工具，该方法通过仅构建训练策略的可达状态空间以形成可处理的离散时间马尔可夫链进行验证，自动用临床命题标记状态，并将可解释性方法与概率计算树逻辑查询相结合。在基于约17,000份患者记录的ICU-Sepsis MDP基准测试中，实验结果表明，COOL-MC能够通过完整MDP验证建立硬性边界，训练出实现最优生存概率的安全RL策略，并利用PCTL验证揭示该策略过度依赖先前的给药历史而非患者病情变化，从而暴露了标准评估无法发现的弱点。</div>
</details>
</div>
<div class="card">
<div class="title">TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Hao Ding, Zhichuan Yang, Weijie Ge, Ziqin Gao, Chaoyi Lu, Lei Zhao</div>
<div class="meta-line">First: 2026-02-16T05:46:47+00:00 · Latest: 2026-02-16T05:46:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14482v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TikArt：基于强化学习的孔径引导细粒度视觉推理观察方法</div>
<div class="mono" style="margin-top:8px">本研究针对多模态大语言模型中的细粒度视觉推理问题，其关键证据常存在于微小物体、杂乱区域或细微标记中，而单一全局图像编码易丢失这些信息。我们提出TikArt（思维孔径），一种孔径引导的智能体，将多步视觉语言推理建模为对感兴趣区域的决策过程。TikArt遵循“思维-孔径-观察”循环，交替进行语言生成与两种孔径操作：Zoom提取矩形裁剪区域，Segment调用SAM2获取针对不规则目标的掩码裁剪。每次操作后，模型需生成显式观察结果，将局部视觉线索转化为持久性语言记忆。基于Qwen3-VL-8B构建的TikArt采用AGRPO强化学习算法优化推理策略，该GRPO风格算法包含两阶段课程：先预热分割操作，再联合优化视觉数学、细粒度VQA和分割任务，其奖励机制将任务成功率与有目的的孔径使用相耦合。在V*、HR-Bench-4K/8K、MME-RealWorld-Lite、MMStar、RefCOCO和ReasonSeg数据集上的实验表明，该方法相比基线模型取得稳定提升，并为高分辨率推理生成可解释的孔径轨迹。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of fine-grained visual reasoning in multimodal large language models (MLLMs), where critical details in small or cluttered image regions are often lost with standard global encoding. To tackle this, the authors introduce TikArt, an aperture-guided agent that frames reasoning as a sequential decision process over image regions, operating through a Think-Aperture-Observe loop that alternates between language generation and two aperture actions—Zoom for rectangular crops and Segment using SAM2 for mask-based crops—with each action followed by an explicit observation to convert visual cues into linguistic memory. Built upon Qwen3-VL-8B and optimized via a two-stage curriculum reinforcement learning algorithm called AGRPO, which first warms up segmentation and then jointly optimizes across tasks, TikArt demonstrates consistent performance improvements over the backbone model on benchmarks including V*, HR-Bench, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg, while producing interpretable aperture trajectories for high-resolution reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型在细粒度视觉推理中的挑战，即图像中微小物体、杂乱区域或细微标记的关键证据在单一全局编码下容易丢失的问题，提出了TikArt（思维光圈）这一光圈引导的智能体，将多步视觉语言推理构建为对感兴趣区域的决策过程。该方法采用Think-Aperture-Observe循环，交替进行语言生成和两种光圈操作：Zoom提取矩形裁剪，Segment调用SAM2获取基于掩码的不规则目标裁剪，每次操作后模型必须生成显式观察，将局部视觉线索转化为持久语言记忆。基于Qwen3-VL-8B构建，TikArt通过AGRPO（一种GRPO风格的强化学习算法）优化推理策略，采用两阶段课程学习：先预热分割操作，然后联合优化视觉数学、细粒度VQA和分割任务，奖励机制将任务成功与有目的的光圈使用相结合。在V*、HR-Bench-4K/8K、MME-RealWorld-Lite、MMStar、RefCOCO和ReasonSeg等基准测试中，实验结果显示其性能持续超越骨干模型，并为高分辨率推理提供了可解释的光圈轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">Making Slow Thinking Faster: Compressing LLM Chain-of-Thought via Step Entropy</div>
<div class="meta-line">Authors: Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, Qiang Xu</div>
<div class="meta-line">First: 2025-08-05T11:48:18+00:00 · Latest: 2026-02-16T05:36:50+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.03346v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.03346v2">PDF</a> · <a href="https://github.com/staymylove/COT_Compresstion_via_Step_entropy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies \emph{the informational contribution of individual reasoning steps} to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly improves LLM inference efficiency while preserving accuracy, paving the way for more scalable LLM deployments and a better understanding of their internal reasoning. The code and data are released in https://github.com/staymylove/COT_Compresstion_via_Step_entropy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>加速慢思考：基于步骤熵压缩大语言模型思维链</div>
<div class="mono" style="margin-top:8px">采用思维链提示的大语言模型在复杂推理任务中表现出色，但会生成包含大量冗余信息的冗长思考过程，导致推理成本增加和效率降低。我们提出一种基于步骤熵的新型思维链压缩框架，该指标通过量化单个推理步骤的信息贡献度来识别冗余。通过理论分析和数学推理基准测试的广泛实证验证，我们证明低熵步骤确实具有高度冗余性。实验表明，在DeepSeek-R1-7B、14B和Qwen3-8B模型上，可修剪高达80%的低熵中间步骤，且最终答案准确率仅轻微下降。这一发现与随机或高熵修剪形成鲜明对比，后者会严重损害推理性能。基于此，我们提出结合监督微调与分组相对策略优化强化学习的两阶段训练策略，使大语言模型能通过策略性插入[SKIP]标记，在推理过程中自主学习生成压缩版思维链。该方法在保持准确性的同时显著提升推理效率，为大规模部署和深入理解模型内部推理机制开辟了新路径。代码与数据已发布于https://github.com/staymylove/COT_Compresstion_via_Step_entropy。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that Chain-of-Thought (CoT) prompting in Large Language Models (LLMs) produces verbose and redundant reasoning steps, which increases inference costs and reduces efficiency. The method introduces a compression framework based on step entropy, a metric to quantify the informational contribution of each reasoning step, and employs a two-stage training strategy combining Supervised Fine-Tuning and Group Relative Policy Optimization reinforcement learning to teach models to generate compressed CoTs using [SKIP] tokens. The main experimental results, validated on mathematical reasoning benchmarks with models like DeepSeek-R1 and Qwen3, show that up to 80% of low-entropy steps can be pruned with minimal accuracy loss, whereas random or high-entropy pruning severely harms performance, demonstrating the framework&#x27;s effectiveness in improving inference efficiency while preserving reasoning accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，大语言模型（LLM）的思维链（CoT）提示会生成冗长且冗余的推理步骤，导致推理成本增加和效率降低。方法上，提出了一个基于步骤熵的压缩框架，该指标用于量化每个推理步骤的信息贡献度，并采用结合监督微调和组相对策略优化强化学习的两阶段训练策略，教导模型在推理时使用[SKIP]令牌自主生成压缩后的思维链。主要实验结果在数学推理基准上使用DeepSeek-R1和Qwen3等模型验证表明，高达80%的低熵步骤可以被剪枝且最终答案准确率仅有轻微下降，而随机或高熵剪枝则会严重损害推理性能，这证明了该框架在保持推理准确性的同时显著提升了推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization</div>
<div class="meta-line">Authors: Subhojyoti Mukherjee, Viet Dac Lai, Raghavendra Addanki, Ryan Rossi, Seunghyun Yoon, Trung Bui, Anup Rao, Jayakumar Subramanian, Branislav Kveton</div>
<div class="meta-line">First: 2025-06-08T01:59:30+00:00 · Latest: 2026-02-16T05:33:56+00:00</div>
<div class="meta-line">Comments: Advances in Neural Information Processing Systems 38</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06964v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.06964v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) is a variant of RL where the policy is learned from a previously collected dataset of trajectories and rewards. In our work, we propose a practical approach to offline RL with large language models (LLMs). We recast the problem as reward-weighted fine-tuning, which can be solved using similar techniques to supervised fine-tuning (SFT). To showcase the value of our approach, we apply it to learning short-horizon question-answering policies of a fixed length, where the agent reasons about potential answers or asks clarifying questions. Our work stands in a stark contrast to state-of-the-art methods in this domain, based on SFT and direct preference optimization, which have additional hyper-parameters and do not directly optimize for rewards. We compare to them empirically, and report major gains in both optimized rewards and language quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于奖励加权微调的对话优化离线强化学习方法</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）是一种从预先收集的轨迹与奖励数据集中学习策略的RL变体。本研究提出一种面向大语言模型（LLM）的实用离线RL方法，将问题重构为奖励加权微调，其求解技术可与监督微调（SFT）类比。为验证方法价值，我们将其应用于固定长度的短视程问答策略学习，使智能体能对潜在答案进行推理或提出澄清性问题。与当前基于SFT和直接偏好优化的主流方法相比，我们的方法无需额外超参数且直接优化奖励目标，实验结果表明其在优化奖励和语言质量方面均取得显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of applying offline reinforcement learning to large language models in a practical manner, particularly for conversational tasks. The method proposed reformulates offline RL as a reward-weighted fine-tuning problem, enabling the use of techniques similar to supervised fine-tuning without the complexities of traditional RL algorithms. Experimental results on short-horizon question-answering policies demonstrate that this approach achieves significant improvements in both optimized rewards and language quality compared to state-of-the-art methods like supervised fine-tuning and direct preference optimization, which often involve extra hyperparameters and do not directly optimize rewards.</div>
<div class="mono" style="margin-top:8px">本文的动机在于如何将离线强化学习实际应用于大型语言模型，特别是在对话任务中。所提出的方法将离线强化学习重新定义为奖励加权的微调问题，使得能够使用类似于监督微调的技术，而无需传统强化学习算法的复杂性。在短视界问答策略上的实验结果表明，与监督微调和直接偏好优化等现有先进方法相比，该方法在优化奖励和语言质量方面均取得了显著提升，这些现有方法通常涉及额外超参数且不直接优化奖励。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Transferability: A Two-Stage Reinforcement Learning Approach for Enhancing Quadruped Robots&#x27; Performance in U-Shaped Stair Climbing</div>
<div class="meta-line">Authors: Baixiao Huang, Baiyu Huang, Yu Hou</div>
<div class="meta-line">First: 2026-02-16T05:19:06+00:00 · Latest: 2026-02-16T05:19:06+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, International Conference on Computing in Civil Engineering (i3CE 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14473v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quadruped robots are employed in various scenarios in building construction. However, autonomous stair climbing across different indoor staircases remains a major challenge for robot dogs to complete building construction tasks. In this project, we employed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize a robot&#x27;s performance on U-shaped stairs. The training robot-dog modality, Unitree Go2, was first trained to climb stairs on Isaac Lab&#x27;s pyramid-stair terrain, and then to climb a U-shaped indoor staircase using the learned policies. This project explores end-to-end RL methods that enable robot dogs to autonomously climb stairs. The results showed (1) the successful goal reached for robot dogs climbing U-shaped stairs with a stall penalty, and (2) the transferability from the policy trained on U-shaped stairs to deployment on straight, L-shaped, and spiral stair terrains, and transferability from other stair models to deployment on U-shaped terrain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习可迁移性：一种两阶段强化学习方法用于提升四足机器人在U型楼梯攀爬中的性能</div>
<div class="mono" style="margin-top:8px">四足机器人被广泛应用于建筑施工现场的多种场景。然而，在不同室内楼梯间实现自主攀爬仍是机器人完成建筑施工任务的主要挑战。本项目采用一种两阶段端到端深度强化学习方法，以优化机器人在U型楼梯上的性能。训练机器人模型Unitree Go2首先在Isaac Lab的金字塔阶梯地形上学习攀爬，随后运用习得的策略攀爬U型室内楼梯。本研究探索了使机器人能够自主攀爬楼梯的端到端强化学习方法。结果表明：（1）在引入停滞惩罚机制下，机器人成功实现了U型楼梯攀爬的目标；（2）在U型楼梯上训练的策略可迁移部署至直梯、L型梯及螺旋阶梯地形，且其他楼梯模型训练的策略亦可迁移部署至U型地形。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling quadruped robots to autonomously navigate diverse indoor staircases, a critical capability for construction tasks. The authors propose a two-stage end-to-end deep reinforcement learning method, where the robot is first trained on a simulated pyramid-stair terrain and then on a target U-shaped staircase. Experimental results demonstrate that the approach successfully allows the robot to reach its goal on U-shaped stairs and exhibits strong policy transferability, with policies trained on U-shaped stairs generalizing to straight, L-shaped, and spiral stairs, and vice versa.</div>
<div class="mono" style="margin-top:8px">本文针对四足机器人在建筑任务中自主攀爬不同室内楼梯的挑战展开研究。作者提出了一种两阶段的端到端深度强化学习方法，先在模拟金字塔楼梯地形上训练机器人，再针对目标U型楼梯进行训练。实验结果表明，该方法成功实现了机器人在U型楼梯上的目标抵达，并展现出良好的策略可迁移性：在U型楼梯上训练的策略可迁移至直梯、L型梯和螺旋楼梯，反之亦然。</div>
</details>
</div>
<div class="card">
<div class="title">Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems</div>
<div class="meta-line">Authors: Furkan Mumcu, Yasin Yilmaz</div>
<div class="meta-line">First: 2026-02-16T05:17:58+00:00 · Latest: 2026-02-16T05:17:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14471v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14471v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent&#x27;s private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>社会加权对齐：多智能体大语言模型系统的博弈论框架</div>
<div class="mono" style="margin-top:8px">在共享环境中部署大语言模型（LLM）智能体时，个体对齐与集体稳定性之间存在根本性张力：局部理性决策可能产生负外部性，损害系统级性能。本文提出社会加权对齐（SWA），这是一种博弈论框架，通过社会权重$λ\in[0,1]$在智能体私有目标与群体福利估计值之间进行插值，从而修改推理阶段的决策机制。在包含$n$个智能体且拥塞强度为$β$的共享资源拥塞博弈中，我们证明SWA会诱导出临界阈值$λ^*=(n-β)/(n-1)$，当超过该阈值时，智能体在过载状态下不再具有增加需求的边际激励，从而引发从持续拥塞到接近容量稳定运行的相变。我们进一步提出SWA的推理阶段算法实现，该方法无需参数更新或多智能体强化学习，并通过多智能体仿真实证验证了预测的阈值行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the conflict between individual alignment and collective stability when multiple LLM agents operate in shared environments, where self-interested decisions can cause negative externalities and reduce overall system performance. It introduces Socially-Weighted Alignment (SWA), a game-theoretic framework that adjusts inference-time decisions by balancing an agent&#x27;s private objective with estimated group welfare through a social weight parameter λ. In a congestion game simulation with n agents and congestion severity β, the authors theoretically derive a critical threshold λ* = (n-β)/(n-1), beyond which agents cease to increase demand under overload, leading to a phase transition from persistent congestion to stable operation near capacity; this is empirically validated via multi-agent simulations, and SWA is implemented as an inference-time algorithm without requiring parameter updates or multi-agent reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本文针对多个大语言模型智能体在共享环境中运行时个体对齐与集体稳定性之间的冲突问题展开研究，指出自利决策可能导致负外部性并降低系统整体性能。作者提出了社会加权对齐框架，这是一种博弈论方法，通过社会权重参数λ在推理时平衡智能体的私人目标与估计的群体福利。在一个包含n个智能体和拥堵严重度β的共享资源拥堵博弈模拟中，理论推导出临界阈值λ* = (n-β)/(n-1)，超过该阈值后智能体在过载下不再增加需求，从而实现从持续拥堵到接近容量稳定运行的相变；该结果通过多智能体仿真得到实证验证，且该框架以推理时算法实现，无需参数更新或多智能体强化学习。</div>
</details>
</div>
<div class="card">
<div class="title">Near-Optimal Sample Complexity for Online Constrained MDPs</div>
<div class="meta-line">Authors: Chang Liu, Yunfan Li, Lin F. Yang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-02-16T05:16:13+00:00 · Latest: 2026-02-16T05:16:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15076v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15076v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety is a fundamental challenge in reinforcement learning (RL), particularly in real-world applications such as autonomous driving, robotics, and healthcare. To address this, Constrained Markov Decision Processes (CMDPs) are commonly used to enforce safety constraints while optimizing performance. However, existing methods often suffer from significant safety violations or require a high sample complexity to generate near-optimal policies. We address two settings: relaxed feasibility, where small violations are allowed, and strict feasibility, where no violation is allowed. We propose a model-based primal-dual algorithm that balances regret and bounded constraint violations, drawing on techniques from online RL and constrained optimization. For relaxed feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with $\varepsilon$-bounded violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$ learning episodes, matching the lower bound for unconstrained MDPs. For strict feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with zero violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^5}{\varepsilon^2ζ^2}\right)$ learning episodes, where $ζ$ is the problem-dependent Slater constant characterizing the size of the feasible region. This result matches the lower bound for learning CMDPs with access to a generative model.
  Our results demonstrate that learning CMDPs in an online setting is as easy as learning with a generative model and is no more challenging than learning unconstrained MDPs when small violations are allowed.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线约束马尔可夫决策过程的近最优样本复杂度</div>
<div class="mono" style="margin-top:8px">安全性是强化学习（RL）中的一个基本挑战，尤其是在自动驾驶、机器人和医疗等现实应用中。为解决此问题，约束马尔可夫决策过程（CMDPs）通常用于在优化性能的同时强制执行安全约束。然而，现有方法常存在显著的安全违规或需要高样本复杂度才能生成近优策略。我们处理两种设置：允许轻微违规的松弛可行性，以及不允许任何违规的严格可行性。我们提出一种基于模型的原对偶算法，结合在线RL和约束优化技术，平衡遗憾和有界约束违规。对于松弛可行性，我们证明该算法以任意高概率返回具有ε有界违规的ε最优策略，需要Ō(SAH³/ε²)学习回合数，匹配无约束MDPs的下界。对于严格可行性，我们证明该算法以任意高概率返回零违规的ε最优策略，需要Ō(SAH⁵/(ε²ζ²))学习回合数，其中ζ是问题相关的Slater常数，用于表征可行域大小。该结果匹配了使用生成模型学习CMDPs的下界。我们的结果表明，在线学习CMDPs与使用生成模型学习同样容易，且在允许轻微违规时，其难度不高于学习无约束MDPs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of safety in reinforcement learning for real-world applications by focusing on online Constrained Markov Decision Processes (CMDPs), where existing methods often lead to excessive safety violations or high sample complexity. The authors propose a model-based primal-dual algorithm that balances regret and constraint violations, considering both relaxed feasibility (allowing small violations) and strict feasibility (no violations). For relaxed feasibility, the algorithm achieves an ε-optimal policy with ε-bounded violation using Õ(SAH³/ε²) episodes, matching the lower bound for unconstrained MDPs; for strict feasibility, it achieves ε-optimality with zero violation using Õ(SAH⁵/(ε²ζ²)) episodes, where ζ is a Slater constant, matching the generative model lower bound. These results show that learning CMDPs online is as efficient as with a generative model and no harder than unconstrained MDPs when small violations are permitted.</div>
<div class="mono" style="margin-top:8px">该论文针对强化学习在自动驾驶、机器人等现实应用中的安全性挑战，研究了在线约束马尔可夫决策过程（CMDPs），现有方法常存在严重安全违规或高样本复杂度问题。作者提出了一种基于模型的原对偶算法，平衡遗憾与约束违反，考虑了宽松可行性（允许小违规）和严格可行性（无违规）两种设置。对于宽松可行性，算法以Õ(SAH³/ε²)学习回合数实现ε最优策略和ε有界违规，匹配无约束MDPs的下界；对于严格可行性，以Õ(SAH⁵/(ε²ζ²))回合数实现ε最优且零违规，其中ζ为Slater常数，匹配生成模型下界。结果表明，在线学习CMDPs与生成模型同样高效，且在允许小违规时与无约束MDPs难度相当。</div>
</details>
</div>
<div class="card">
<div class="title">LACONIC: Length-Aware Constrained Reinforcement Learning for LLM</div>
<div class="meta-line">Authors: Chang Liu, Yiran Zhao, Lawrence Liu, Yaoqi Ye, Csaba Szepesvári, Lin F. Yang</div>
<div class="meta-line">First: 2026-02-16T05:09:40+00:00 · Latest: 2026-02-16T05:09:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14468v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LACONIC：面向大语言模型的长度感知约束强化学习方法</div>
<div class="mono" style="margin-top:8px">强化学习通过奖励驱动的训练增强了大语言模型的能力，但该过程可能导致生成冗长响应，增加推理延迟与计算开销。现有长度控制方法多依赖固定的启发式奖励塑形，易与任务目标冲突且需脆弱调参。本研究提出LACONIC——一种在训练中强制执行目标令牌预算的强化学习方法。该方法通过结合任务奖励与基于长度的成本构建增强目标来更新策略模型，并动态调整成本规模以平衡简洁性与任务性能，在保持任务奖励的同时实现鲁棒的长度控制。理论分析为该方法提供支撑。在数学推理模型与数据集的实验中，LACONIC在保持或提升pass@1指标的同时将输出长度缩减超50%，在通用知识与多语言基准测试中以减少44%令牌量的条件保持跨域性能。该方法无需修改推理过程即可集成至标准RL微调流程，部署开销极低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for LACONIC stems from the issue that reinforcement learning (RL) for large language models often leads to excessively long responses, increasing latency and computational costs, while prior length-control methods rely on brittle heuristic tuning. The method introduces a constrained RL approach that enforces a target token budget by augmenting the training objective with an adaptively scaled length-based cost, balancing brevity and task performance with theoretical support. Experimentally, on mathematical reasoning tasks, it reduces output length by over 50% while preserving or improving pass@1 accuracy, and maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens, requiring no inference changes.</div>
<div class="mono" style="margin-top:8px">LACONIC的提出动机在于，针对大语言模型的强化学习常导致响应过长，增加延迟和计算开销，而现有长度控制方法依赖脆弱的启发式调整。该方法采用约束强化学习，通过在训练目标中引入自适应调整的长度成本来强制实现目标令牌预算，从而在理论保证下平衡简洁性与任务性能。实验结果表明，在数学推理任务中，它在保持或提升pass@1准确率的同时将输出长度减少超过50%，并在通用知识和多语言基准测试中以减少44%的令牌数保持域外性能，且无需推理阶段改动。</div>
</details>
</div>
<div class="card">
<div class="title">Online reinforcement learning via sparse Gaussian mixture model Q-functions</div>
<div class="meta-line">Authors: Minh Vu, Konstantinos Slavakis</div>
<div class="meta-line">First: 2025-09-18T03:37:11+00:00 · Latest: 2026-02-16T02:50:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14585v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.14585v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a structured and interpretable online policy-iteration framework for reinforcement learning (RL), built around the novel class of sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work that trained GMM-QFs offline, the proposed framework develops an online scheme that leverages streaming data to encourage exploration. Model complexity is regulated through sparsification by Hadamard overparametrization, which mitigates overfitting while preserving expressiveness. The parameter space of S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing for principled parameter updates via online gradient descent on a smooth objective. Numerical experiments show that S-GMM-QFs match or even outperform dense deep RL (DeepRL) methods on standard benchmarks while using significantly fewer parameters. Moreover, they maintain strong performance even in low-parameter regimes where sparsified DeepRL methods fail to generalize.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于稀疏高斯混合模型Q函数的在线强化学习</div>
<div class="mono" style="margin-top:8px">本文提出了一种结构化且可解释的在线策略迭代强化学习框架，其核心是新型稀疏高斯混合模型Q函数（S-GMM-QFs）。该框架在离线训练GMM-QFs的先前工作基础上，开发了利用流数据促进探索的在线方案。通过哈达玛超参数化实现稀疏化调控模型复杂度，在保持表达力的同时缓解过拟合。S-GMM-QFs的参数空间天然具备黎曼流形结构，支持在光滑目标函数上通过在线梯度下降进行理论完备的参数更新。数值实验表明，在标准测试基准上，S-GMM-QFs使用显著更少的参数即可达到甚至超越密集深度强化学习方法。此外，在稀疏化深度强化学习方法泛化失效的低参数量场景下，S-GMM-QFs仍能保持强劲性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces an online reinforcement learning framework using sparse Gaussian mixture model Q-functions (S-GMM-QFs) to enhance interpretability and efficiency. The motivation is to extend prior offline GMM-QF methods to an online setting that actively explores via streaming data, while controlling model complexity through Hadamard overparametrization for sparsification to prevent overfitting. The method leverages the Riemannian manifold structure of S-GMM-QFs for principled online gradient descent updates. Experimental results demonstrate that S-GMM-QFs achieve comparable or superior performance to dense deep RL methods on benchmarks with far fewer parameters, and they maintain robustness in low-parameter scenarios where sparsified deep RL methods falter.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于稀疏高斯混合模型Q函数（S-GMM-QFs）的在线强化学习框架，旨在提升可解释性和效率。其动机是将先前的离线GMM-QF方法扩展到在线设置，通过流数据促进探索，同时利用Hadamard过参数化进行稀疏化以控制模型复杂度并避免过拟合。该方法利用S-GMM-QFs固有的黎曼流形结构，通过在线梯度下降进行参数更新。实验结果表明，S-GMM-QFs在标准基准测试中匹配甚至优于密集深度强化学习方法，且参数数量显著减少，并在低参数区域保持强劲性能，而稀疏化的深度强化学习方法在此情况下泛化能力不足。</div>
</details>
</div>
<div class="card">
<div class="title">AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation</div>
<div class="meta-line">Authors: Morgan Byrd, Donghoon Baek, Kartik Garg, Hyunyoung Jung, Daesol Cho, Maks Sorokin, Robert Wright, Sehoon Ha</div>
<div class="meta-line">First: 2026-02-16T00:29:53+00:00 · Latest: 2026-02-16T00:29:53+00:00</div>
<div class="meta-line">Comments: Website: https://morganbyrd03.github.io/adaptmanip/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://morganbyrd03.github.io/adaptmanip/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaptManip：通过在线循环状态估计学习自适应全身物体抓取与递送</div>
<div class="mono" style="margin-top:8px">本文提出自适应全身移动操作框架AdaptManip，使人形机器人能够完全自主地执行集成导航、物体抓取与递送任务。与以往依赖人类演示且易受干扰的模仿学习方法不同，AdaptManip旨在通过强化学习训练鲁棒的移动操作策略，无需人类演示或遥操作数据。该框架包含三个耦合组件：(1) 循环物体状态估计器，在有限视野和遮挡条件下实时追踪被操作物体；(2) 全身基座策略，通过残差操作控制实现鲁棒移动与稳定物体抓取递送；(3) 基于激光雷达的机器人全局位置估计器，提供抗漂移定位能力。所有组件均通过强化学习在仿真中训练，并以零样本方式部署至实体机器人。实验表明，AdaptManip在适应性和整体成功率上显著优于包括模仿学习方法在内的基线方法，且精确的物体状态估计能在遮挡环境下提升操作性能。我们进一步在人形机器人上实现了完全自主的真实环境导航、物体抓取与递送演示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the brittleness of prior imitation learning methods for humanoid robot manipulation, this paper introduces AdaptManip, a reinforcement learning framework for autonomous whole-body object lifting and delivery without human demonstrations. The method integrates an online recurrent object state estimator for tracking under occlusion, a whole-body locomotion policy with residual manipulation control, and a drift-robust LiDAR-based global position estimator, all trained in simulation and deployed zero-shot. Experimental results demonstrate that AdaptManip outperforms baseline imitation learning approaches in adaptability and success rate, with the object estimator enhancing performance under occlusion, and it enables fully autonomous real-world navigation and manipulation on a humanoid robot.</div>
<div class="mono" style="margin-top:8px">针对以往基于模仿学习的人形机器人操作方法在干扰下易失效的问题，本文提出了AdaptManip框架，旨在通过强化学习实现无需人类演示的自主全身物体搬运与递送。该方法结合了在线循环物体状态估计器以在遮挡下跟踪物体、带有残余操作控制的全身运动策略，以及基于激光雷达的抗漂移全局定位器，所有组件均在仿真中训练并零样本部署到真实硬件。实验结果表明，AdaptManip在适应性和整体成功率上显著优于包括模仿学习在内的基线方法，物体状态估计器在遮挡下提升了操作性能，并在真实人形机器人上实现了完全自主的导航、抓取与递送任务。</div>
</details>
</div>
<div class="card">
<div class="title">WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control</div>
<div class="meta-line">Authors: Mehran Aghabozorgi, Alireza Moazeni, Yanshu Zhang, Ke Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-15T23:53:16+00:00 · Latest: 2026-02-15T23:53:16+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026. OpenReview: https://openreview.net/forum?id=mzLOnTb3WH</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14351v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14351v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WIMLE：基于不确定性感知与IMLE的世界模型实现样本高效的连续控制</div>
<div class="mono" style="margin-top:8px">基于模型的强化学习虽具备样本效率优势，但在实践中常因模型误差累积、单模态世界模型对多模态动态的平均化处理，以及过度自信预测导致学习偏差而表现不佳。本文提出WIMLE方法，将隐式最大似然估计（IMLE）扩展至基于模型的强化学习框架，通过集成与隐空间采样实现无需迭代采样的随机多模态世界模型学习及预测不确定性估计。训练过程中，WIMLE依据预测置信度加权合成状态转移，在保留有效模型推演的同时抑制不确定预测的偏差，实现稳定学习。在涵盖DeepMind Control、MyoSuite和HumanoidBench的40个连续控制任务中，WIMLE在样本效率上显著优于强基准模型（包括无模型与基于模型方法），并达到可比或更优的渐进性能。尤其在Humanoid-run任务中，WIMLE的样本效率较最强竞争者提升超50%；在HumanoidBench中成功解决14项任务中的8项（对比BRO的4项与SimbaV2的5项）。这些结果凸显了基于IMLE的多模态建模与不确定性感知加权对稳定模型强化学习的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the practical shortcomings of model-based reinforcement learning, such as compounding model error and overconfident predictions from unimodal world models. The method, WIMLE, integrates Implicit Maximum Likelihood Estimation (IMLE) to learn stochastic, multi-modal world models without iterative sampling and estimates predictive uncertainty using ensembles and latent sampling, weighting synthetic transitions by confidence during training to stabilize learning. Experimental results across 40 continuous-control tasks, including DeepMind Control, MyoSuite, and HumanoidBench, show that WIMLE achieves superior sample efficiency and competitive asymptotic performance, notably improving sample efficiency by over 50% on Humanoid-run and solving 8 of 14 tasks on HumanoidBench, outperforming strong baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机是基于模型的强化学习在实践中存在模型误差累积和单模态世界模型导致过度自信预测等缺陷。方法WIMLE将隐式最大似然估计（IMLE）扩展到基于模型的强化学习框架中，通过集成和潜在采样学习随机多模态世界模型并估计预测不确定性，在训练中根据置信度加权合成转移以稳定学习。在涵盖DeepMind Control、MyoSuite和HumanoidBench的40个连续控制任务上的实验结果表明，WIMLE实现了卓越的样本效率和竞争性的渐近性能，特别是在Humanoid-run任务上样本效率提升超过50%，并在HumanoidBench上解决了14个任务中的8个，优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dual Goal Representations</div>
<div class="meta-line">Authors: Seohong Park, Deepinder Mann, Sergey Levine</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-08T07:07:39+00:00 · Latest: 2026-02-15T23:51:55+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.06714v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.06714v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we introduce dual goal representations for goal-conditioned reinforcement learning (GCRL). A dual goal representation characterizes a state by &quot;the set of temporal distances from all other states&quot;; in other words, it encodes a state through its relations to every other state, measured by temporal distance. This representation provides several appealing theoretical properties. First, it depends only on the intrinsic dynamics of the environment and is invariant to the original state representation. Second, it contains provably sufficient information to recover an optimal goal-reaching policy, while being able to filter out exogenous noise. Based on this concept, we develop a practical goal representation learning method that can be combined with any existing GCRL algorithm. Through diverse experiments on the OGBench task suite, we empirically show that dual goal representations consistently improve offline goal-reaching performance across 20 state- and pixel-based tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双重目标表示</div>
<div class="mono" style="margin-top:8px">本研究针对目标条件强化学习（GCRL）提出了双重目标表示方法。该表示通过“从所有其他状态出发的时序距离集合”来刻画状态特征，即利用状态与环境中所有其他状态之间的时序距离关系进行编码。此表示具备多项理想的理论特性：首先，它仅依赖于环境的内在动态特性，且对原始状态表示具有不变性；其次，它能提供足以恢复最优目标达成策略的可证明充分信息，同时有效过滤外生噪声。基于此概念，我们开发了一种可与现有任意GCRL算法结合的实际目标表示学习方法。通过在OGBench任务套件上的多样化实验，我们实证表明双重目标表示在20项基于状态和像素的任务中持续提升离线目标达成性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for robust and informative goal representations in goal-conditioned reinforcement learning (GCRL), this paper introduces dual goal representations, which characterize a state by the set of temporal distances to all other states, thereby encoding relational information based on environment dynamics. The method involves learning this representation, which is theoretically invariant to the original state encoding and sufficient for optimal policy recovery while filtering exogenous noise, and it can be integrated with any existing GCRL algorithm. Experimental results on the OGBench suite demonstrate that the proposed representation consistently improves offline goal-reaching performance across 20 diverse state- and pixel-based tasks.</div>
<div class="mono" style="margin-top:8px">本文针对目标条件强化学习中需要鲁棒且信息丰富的目标表示这一动机，提出了双重目标表示方法，该方法通过状态到所有其他状态的时间距离集合来表征状态，从而基于环境动态编码关系信息。该方法学习这种表示，理论上它不依赖于原始状态编码且足以恢复最优策略，同时能过滤外生噪声，并可集成到任何现有目标条件强化学习算法中。在OGBench任务套件上的实验结果表明，所提出的表示方法在20个不同的基于状态和像素的任务中，持续提升了离线目标达成性能。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Instruction Following in RL via Structured LTL Representations</div>
<div class="meta-line">Authors: Mathias Jackermeier, Mattia Giuri, Jacques Cloete, Alessandro Abate</div>
<div class="meta-line">First: 2026-02-15T23:22:50+00:00 · Latest: 2026-02-15T23:22:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14344v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14344v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结构化线性时序逻辑表示的强化学习零样本指令跟随</div>
<div class="mono" style="margin-top:8px">本研究探讨多任务强化学习中的指令跟随问题，要求智能体在训练后零样本执行未见过的任务。线性时序逻辑（LTL）作为一种强大的框架，近期被用于描述结构化、时间延展的任务。现有方法虽能训练通用策略，却常难以有效捕捉LTL规范中丰富的逻辑与时间结构。为此，我们提出一种学习结构化任务表示的新方法，以促进训练与泛化。该方法通过基于任务有限自动机构建的布尔公式序列来调节策略，采用分层神经架构编码公式的逻辑结构，并引入注意力机制使策略能够推理未来子目标。在多种复杂环境中的实验表明，该方法具有强大的泛化能力和优越性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of zero-shot instruction following in multi-task reinforcement learning, where agents must execute novel tasks not encountered during training. The motivation stems from the limitations of existing methods in capturing the complex logical and temporal structures of tasks specified via linear temporal logic (LTL). To overcome this, the authors propose a method that conditions policies on sequences of Boolean formulae derived from task automata, employing a hierarchical neural architecture with an attention mechanism to encode logical structure and reason about future subgoals. Experimental results across diverse complex environments show that this approach achieves strong generalization and outperforms prior techniques.</div>
<div class="mono" style="margin-top:8px">本文研究了多任务强化学习中的零样本指令跟随问题，即智能体需执行训练中未见过的新任务。现有方法虽能训练通用策略，但难以有效捕捉线性时序逻辑规范中丰富的逻辑与时间结构。为此，作者提出一种新方法，通过基于任务自动机构建的布尔公式序列来调节策略，采用分层神经架构和注意力机制编码逻辑结构并推理未来子目标。在多种复杂环境中的实验表明，该方法具有强大的泛化能力，性能优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Zhi Zhang, Zhen Han, Costas Mavromatis, Qi Zhu, Yunyi Zhang, Sheng Guan, Dingmin Wang, Xiong Zhou, Shuai Wang, Soji Adeshina, Vassilis Ioannidis, Huzefa Rangwala</div>
<div class="meta-line">First: 2026-02-15T23:14:05+00:00 · Latest: 2026-02-15T23:14:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14338v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14338v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少训练，多学习：基于群组强化学习的自适应高效轨迹优化</div>
<div class="mono" style="margin-top:8px">强化学习在大语言模型后训练中扮演核心角色。现有方法中，群组相对策略优化被广泛采用，尤其适用于带可验证奖励的微调。在GRPO中，每个查询提示大语言模型生成一组固定规模N的轨迹。当组内所有轨迹结果相同（全对或全错）时，组归一化优势值变为零，导致梯度信号缺失并浪费微调算力。本文提出自适应高效轨迹优化，作为GRPO的增强方法。AERO采用自适应轨迹策略，应用选择性剔除以策略性剪枝轨迹，并维护贝叶斯后验以防止零优势死区。在三种模型配置下，AERO在不牺牲性能的前提下提升了计算效率。在相同总轨迹预算下，AERO减少约48%的总训练算力，单步耗时平均缩短约45%。尽管计算量大幅降低，AERO在Pass@8和Avg@8指标上均达到或超越GRPO，为基于强化学习的大语言模型对齐提供了实用、可扩展且高效的计算策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of Group Relative Policy Optimization (GRPO) in reinforcement learning for large language models, where fixed-size rollout groups can yield zero gradient signals when all rollouts share identical outcomes, this paper introduces Adaptive Efficient Rollout Optimization (AERO). The method enhances GRPO by employing an adaptive rollout strategy with selective rejection to prune uninformative rollouts and a Bayesian posterior to avoid zero-advantage scenarios. Experimental results across three model configurations show that AERO reduces total training compute by approximately 48% and wall-clock time per step by about 45% on average, while matching or improving performance metrics like Pass@8 and Avg@8, demonstrating a compute-efficient approach for RL-based LLM alignment.</div>
<div class="mono" style="margin-top:8px">针对大语言模型强化学习中组相对策略优化（GRPO）因固定组大小导致所有轨迹结果相同时梯度信号为零、浪费计算资源的问题，本文提出了自适应高效轨迹优化（AERO）。该方法通过自适应轨迹策略、选择性拒绝来修剪无信息轨迹，并利用贝叶斯后验避免零优势区域，从而改进GRPO。在三种模型配置上的实验结果表明，AERO在相同总轨迹预算下，将总训练计算量减少约48%，每步运行时间平均缩短约45%，同时保持或提升了Pass@8和Avg@8等性能指标，为基于强化学习的LLM对齐提供了一种实用、可扩展且计算高效的策略。</div>
</details>
</div>
<div class="card">
<div class="title">When is Offline Policy Selection Sample Efficient for Reinforcement Learning?</div>
<div class="meta-line">Authors: Vincent Liu, Prabhat Nagarajan, Andrew Patterson, Martha White</div>
<div class="meta-line">First: 2023-12-04T21:35:13+00:00 · Latest: 2026-02-15T22:55:29+00:00</div>
<div class="meta-line">Comments: AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.02355v2">Abs</a> · <a href="https://arxiv.org/pdf/2312.02355v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning algorithms often require careful hyperparameter tuning. Before deployment, we need to select amongst a set of candidate policies. However, there is limited understanding about the fundamental limits of this offline policy selection (OPS) problem. In this work we provide clarity on when sample efficient OPS is possible, primarily by connecting OPS to off-policy policy evaluation (OPE) and Bellman error (BE) estimation. We first show a hardness result, that in the worst case, OPS is just as hard as OPE, by proving a reduction of OPE to OPS. As a result, no OPS method can be more sample efficient than OPE in the worst case. We then connect BE estimation to the OPS problem, showing how BE can be used as a tool for OPS. While BE-based methods generally require stronger requirements than OPE, when those conditions are met they can be more sample efficient. Building on this insight, we propose a BE method for OPS, called Identifiable BE Selection (IBES), that has a straightforward method for selecting its own hyperparameters. We conclude with an empirical study comparing OPE and IBES, and by showing the difficulty of OPS on an offline Atari benchmark dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线强化学习中策略选择何时具备样本效率？</div>
<div class="mono" style="margin-top:8px">离线强化学习算法通常需要细致的超参数调优。在部署前，我们需要从一组候选策略中进行选择。然而，目前对离线策略选择问题的基本限制理解有限。本研究通过将OPS与离轨策略评估及贝尔曼误差估计相关联，明确了样本高效OPS的实现条件。首先通过证明OPE可归约至OPS，得出最坏情况下OPS与OPE难度相当的硬度结论，表明任何OPS方法在最坏情况下都不可能比OPE更具样本效率。随后建立BE估计与OPS问题的关联，阐释了如何将BE作为OPS工具。虽然基于BE的方法通常需要比OPE更强的假设条件，但当条件满足时可实现更高样本效率。基于此，我们提出名为可辨识BE选择的BE方法，该方法具备自主选择超参数的简洁机制。最后通过对比OPE与IBES的实证研究，展示了在离线Atari基准数据集上实施OPS的难度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the sample efficiency of offline policy selection (OPS) in reinforcement learning, motivated by the need to reliably choose among candidate policies without online interaction. The method connects OPS to off-policy policy evaluation (OPE) and Bellman error (BE) estimation, first proving a worst-case reduction showing OPS is as hard as OPE, thus establishing a fundamental efficiency limit. It then demonstrates that BE-based methods can be more sample-efficient under certain conditions, leading to the proposal of Identifiable BE Selection (IBES), which includes a hyperparameter selection strategy. Experimental results from an empirical study and an offline Atari benchmark confirm the theoretical hardness and compare the performance of OPE and IBES.</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习中离线策略选择（OPS）的样本效率问题，其动机是在无需在线交互的情况下可靠地从候选策略中进行选择。方法上将OPS与离线策略评估（OPE）和贝尔曼误差（BE）估计联系起来，首先通过归约证明在最坏情况下OPS与OPE同样困难，从而确立了基本的效率极限。随后指出在特定条件下，基于BE的方法可以更高效，并提出了可识别贝尔曼误差选择（IBES）方法，该方法包含超参数选择策略。在经验研究和离线Atari基准数据集上的实验结果验证了理论上的困难性，并比较了OPE与IBES的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study</div>
<div class="meta-line">Authors: Hani Beirami, M M Manjurul Islam</div>
<div class="meta-line">First: 2026-02-15T22:10:11+00:00 · Latest: 2026-02-15T22:10:11+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14322v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14322v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent&#x27;s actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于鲁棒强化学习控制的共形信号时序逻辑：一项案例研究</div>
<div class="mono" style="margin-top:8px">本研究探讨了形式化时序逻辑规约如何增强航空航天应用中强化学习（RL）控制的安全性与鲁棒性。利用开源AeroBench F-16仿真基准，我们训练了一个近端策略优化（PPO）智能体来调节发动机油门并跟踪指令空速。控制目标被编码为信号时序逻辑（STL）要求，即在每个机动动作的最后数秒内将空速维持在预设区间。为在运行时强制执行该规约，我们引入了一种共形STL防护罩，通过在线共形预测对RL智能体的动作进行过滤。我们在标称条件和包含气动模型失配、执行器速率限制、测量噪声及任务中途设定点跳变的严重压力场景下，比较了三种设置：（i）PPO基线，（ii）带经典基于规则的STL防护罩的PPO，以及（iii）带所提共形防护罩的PPO。实验表明，共形防护罩在保持接近基线性能的同时，能维持STL满足性，并提供比经典防护罩更强的鲁棒性保证。这些结果证明，将形式化规约监测与数据驱动的RL控制相结合，可显著提升自主飞行控制在挑战性环境中的可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to improve the safety and robustness of reinforcement learning (RL) control in aerospace by integrating formal temporal logic specifications. The method employs a Proximal Policy Optimization (PPO) agent trained in an F-16 simulation, with the control objective encoded as a Signal Temporal Logic (STL) requirement for airspeed regulation. A novel conformal STL shield, using online conformal prediction to filter the agent&#x27;s actions, is introduced and compared against a baseline PPO and a classical rule-based STL shield. Experimental results under nominal and severe stress conditions show that the conformal shield maintains STL satisfaction and near-baseline performance while offering stronger robustness guarantees than the classical shield, demonstrating enhanced reliability for autonomous flight control.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过整合形式化时序逻辑规范，提升强化学习在航空航天控制中的安全性与鲁棒性。方法采用在F-16仿真中训练的近端策略优化（PPO）智能体，将控制目标编码为保持空速的信号时序逻辑（STL）要求，并引入一种新颖的保形STL护盾，利用在线保形预测来过滤智能体的动作，与基线PPO及经典规则型STL护盾进行对比。在标称和严重压力场景下的实验结果表明，该保形护盾在维持STL满足度和接近基线性能的同时，提供了比经典护盾更强的鲁棒性保证，证明了其在挑战性环境中提升自主飞行控制可靠性的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning</div>
<div class="meta-line">Authors: Kris Shengjun Dong, Sahil Modi, Dima Nikiforov, Sana Damani, Edward Lin, Siva Kumar Sastry Hari, Christos Kozyrakis</div>
<div class="meta-line">First: 2026-02-15T19:48:43+00:00 · Latest: 2026-02-15T19:48:43+00:00</div>
<div class="meta-line">Comments: 15 pages, 33 pages with appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14293v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KernelBlaster：基于记忆增强上下文强化学习的持续跨任务CUDA优化</div>
<div class="mono" style="margin-top:8px">跨多代GPU架构优化CUDA代码具有挑战性，因为实现峰值性能需探索日益复杂且硬件特定的优化空间。传统编译器受限于固定启发式规则，而微调大语言模型成本高昂。然而，现有CUDA代码优化的智能体工作流难以聚合历史探索知识，导致采样偏差和次优解。我们提出KernelBlaster，一种记忆增强上下文强化学习框架，旨在提升基于大语言模型的GPU编程智能体的CUDA优化搜索能力。该框架通过将知识积累至可检索的持久化CUDA知识库，使智能体能够从经验中学习并对未来任务做出系统化决策。我们设计了一种新颖的基于性能剖析与文本梯度的智能体流程，用于CUDA代码生成与优化，以在多代GPU架构上实现高性能。KernelBlaster引导大语言模型智能体系统化探索超越简单重写的高潜力优化策略。相较于PyTorch基线，本方法在KernelBench 1、2、3级测试中分别实现1.43倍、2.50倍和1.50倍的几何平均加速。我们将KernelBlaster作为开源智能体框架发布，附带测试工具、验证组件及可复现的评估流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of optimizing CUDA code across diverse GPU architectures, where traditional compilers rely on fixed heuristics and fine-tuning LLMs is costly, while existing agentic workflows struggle to accumulate knowledge from prior explorations, leading to biased sampling. To overcome this, the authors propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning framework that enables LLM-based agents to learn from experience by storing knowledge in a retrievable Persistent CUDA Knowledge Base, and it introduces a profile-guided, textual-gradient-based flow for systematic exploration of optimization strategies. Experimentally, the method achieves significant speedups over a PyTorch baseline, with geometric mean improvements of 1.43x, 2.50x, and 1.50x on three levels of the KernelBench benchmark, demonstrating its effectiveness in cross-task CUDA optimization.</div>
<div class="mono" style="margin-top:8px">本文针对跨GPU架构优化CUDA代码的挑战，传统编译器依赖固定启发式方法且微调大语言模型成本高昂，而现有智能体工作流难以积累先前探索的知识，导致采样偏差和次优解。为此，作者提出了KernelBlaster，一种记忆增强的上下文强化学习框架，通过将知识存储于可检索的持久CUDA知识库中，使基于大语言模型的智能体能够从经验中学习，并采用一种基于配置文件引导和文本梯度的流程来系统探索优化策略。实验结果表明，该方法在KernelBench基准测试的三个级别上相比PyTorch基线分别实现了1.43倍、2.50倍和1.50倍的几何平均加速，验证了其在跨任务CUDA优化中的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
