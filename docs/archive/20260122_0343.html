<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 03:43</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_0343</div>
    <div class="row"><div class="card">
<div class="title">Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow</div>
<div class="meta-line">Authors: Haocheng Xi, Charlie Ruan, Peiyuan Liao, Yujun Lin, Han Cai, Yilong Zhao, Shuo Yang, Kurt Keutzer, Song Han, Ligeng Zhu</div>
<div class="meta-line">First: 2026-01-20T18:54:31+00:00 · Latest: 2026-01-20T18:54:31+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14243v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Jet-RL：通过统一的训练与推演精度流实现基于策略的FP8强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）对于提升大语言模型（LLMs）的复杂推理能力至关重要。然而，现有RL训练流程计算效率低下且资源密集，其中推演阶段占总训练时间的70%以上。量化RL训练，特别是使用FP8精度，为解决这一瓶颈提供了可行方案。当前常用策略是在推演阶段采用FP8精度，同时在训练阶段保留BF16精度。本研究首次对FP8 RL训练进行了全面分析，并证明广泛采用的“BF16训练+FP8推演”策略在长序列推演和复杂任务中会出现严重的训练不稳定性和灾难性精度崩溃。分析表明，这些失效源于该方法的离策略特性，导致训练与推理之间存在显著数值失配。基于此，我们提出Jet-RL——一种能够实现稳健RL优化的FP8训练框架。其核心思想是采用统一的FP8精度流同时处理训练与推演，从而最小化数值差异并消除低效的步间校准需求。大量实验验证了Jet-RL的有效性：相较于BF16训练，本方法在推演阶段最高加速33%，训练阶段最高加速41%，端到端整体加速16%，同时在所有设置中保持稳定收敛，且精度损失可忽略不计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the computational inefficiency of reinforcement learning (RL) pipelines for large language models, where the rollout phase consumes over 70% of training time, by exploring FP8 quantization. The method introduces Jet-RL, a framework that employs a unified FP8 precision flow for both training and rollout to eliminate numerical mismatches and avoid calibration overhead, contrasting with prior strategies that used mixed BF16-training and FP8-rollout. Experimental results demonstrate that Jet-RL achieves up to 33% rollout speedup, 41% training speedup, and 16% end-to-end acceleration compared to BF16 training, while maintaining stable convergence with minimal accuracy loss across various tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型强化学习训练流程中计算效率低下的问题，其中推演阶段占训练总时间的70%以上，为此探索了FP8量化方法。该方法提出了Jet-RL框架，采用统一的FP8精度流同时处理训练和推演，以消除数值不匹配并避免校准开销，区别于先前混合使用BF16训练和FP8推演的策略。实验结果表明，与BF16训练相比，Jet-RL实现了高达33%的推演加速、41%的训练加速和16%的端到端加速，同时在各种任务中保持稳定的收敛性且精度损失可忽略不计。</div>
</details>
</div>
<div class="card">
<div class="title">Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression</div>
<div class="meta-line">Authors: Shaurya Mathur, Shreyas Bellary Manjunath, Nitin Kulkarni, Alina Vereshchaka</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-20T18:50:12+00:00 · Latest: 2026-01-20T18:50:12+00:00</div>
<div class="meta-line">Comments: 6 pages, 5 figures (two of them in tables), Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14238v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/firecastrl">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时空野火预测与直升机灭火强化学习抑制策略</div>
<div class="mono" style="margin-top:8px">野火发生频率和强度日益增加，严重破坏生态系统和社区，仅在美国每年就造成数十亿美元的灭火成本和经济损失。传统野火管理多为被动响应，仅在火灾被发现后才采取行动。本文提出《FireCastRL》——一种结合野火预测与智能抑制策略的主动式人工智能框架。该框架首先采用深度时空模型预测野火起火点，针对高风险预测，部署预训练的强化学习智能体在物理信息三维模拟环境中，指挥直升机灭火单元执行实时抑制战术。框架生成威胁评估报告，协助应急响应人员优化资源调配与规划。此外，我们公开了一个包含950万条环境变量样本的大规模时空数据集用于野火预测。本研究展示了深度学习与强化学习如何协同支持野火预测与战术响应。更多细节详见：https://sites.google.com/view/firecastrl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing frequency and devastating impact of wildfires, which are often managed reactively, this paper introduces FireCastRL, a proactive AI framework that integrates wildfire forecasting with intelligent suppression strategies. The method employs a deep spatiotemporal model to predict wildfire ignition and, for high-risk cases, deploys a pre-trained reinforcement learning agent within a physics-informed 3D simulation to execute real-time helitack suppression tactics. Experimental results demonstrate the framework&#x27;s capability to generate threat assessment reports for optimizing resource allocation, supported by the release of a large-scale dataset containing 9.5 million environmental samples to advance wildfire prediction and response.</div>
<div class="mono" style="margin-top:8px">针对日益频繁且破坏性强的野火问题，传统应对方式多为被动响应，本文提出了FireCastRL这一主动式人工智能框架，将野火预测与智能扑救策略相结合。该方法首先利用深度时空模型预测野火起火点，并对高风险情况，在物理信息驱动的三维仿真中部署预训练的强化学习智能体，以执行实时的直升机灭火战术。实验结果表明，该框架能生成威胁评估报告以优化资源调配，并公开了一个包含950万环境样本的大规模时空数据集，以推动野火预测与应对研究的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Q-learning with Adjoint Matching</div>
<div class="meta-line">Authors: Qiyang Li, Sergey Levine</div>
<div class="meta-line">First: 2026-01-20T18:45:34+00:00 · Latest: 2026-01-20T18:45:34+00:00</div>
<div class="meta-line">Comments: 32 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14234v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14234v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic&#x27;s action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伴随匹配Q学习</div>
<div class="mono" style="margin-top:8px">我们提出伴随匹配Q学习（QAM），这是一种基于时序差分的新型强化学习算法，解决了连续动作强化学习中长期存在的挑战：针对参数化Q函数高效优化表达能力强的扩散或流匹配策略。有效优化需要利用评论家的一阶信息，但对流或扩散策略而言，通过多步去噪过程进行基于梯度的直接反向传播优化存在数值不稳定性。现有方法要么仅使用价值函数而丢弃梯度信息，要么依赖近似方法牺牲策略表达能力或引入偏差。QAM通过运用生成建模中最新提出的伴随匹配技术，巧妙规避了这些挑战。该技术将评论家的动作梯度转化为分步目标函数，避免了不稳定的反向传播，同时在最优解处提供无偏且表达能力强的策略。结合评论家学习的时序差分更新，QAM在离线和离线到在线强化学习的困难稀疏奖励任务中持续超越现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of efficiently optimizing expressive diffusion or flow-matching policies in continuous-action reinforcement learning, where direct gradient-based optimization through multi-step denoising processes is numerically unstable. The proposed method, Q-learning with Adjoint Matching (QAM), leverages adjoint matching to transform the critic&#x27;s action gradient into a step-wise objective, avoiding unstable backpropagation while maintaining an unbiased and expressive policy. Experimental results show that QAM consistently outperforms prior methods on hard, sparse reward tasks in both offline and offline-to-online RL settings.</div>
<div class="mono" style="margin-top:8px">本文解决了连续动作强化学习中高效优化扩散或流匹配策略的挑战，其中通过多步去噪过程进行直接梯度优化存在数值不稳定性。提出的方法——伴随匹配Q学习（QAM）利用伴随匹配技术，将评论者的动作梯度转化为逐步目标函数，避免了不稳定的反向传播，同时保持了无偏且表达性强的策略。实验结果表明，在离线及离线到在线强化学习的困难稀疏奖励任务上，QAM始终优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning</div>
<div class="meta-line">Authors: Egor Cherepanov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov</div>
<div class="meta-line">First: 2026-01-20T18:44:28+00:00 · Latest: 2026-01-20T18:44:28+00:00</div>
<div class="meta-line">Comments: 38 pages, 44 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://avanturist322.github.io/KAGEBench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAGE-Bench：面向强化学习的快速已知轴视觉泛化评估基准</div>
<div class="mono" style="margin-top:8px">基于像素的强化学习智能体常在纯视觉分布偏移下失效，即使潜在动态与奖励机制未变，但现有基准常混杂多重偏移源而阻碍系统分析。我们提出KAGE-Env——一个基于JAX的2D平台环境，它将观测过程分解为独立可控的视觉轴，同时保持底层控制问题不变。通过结构设计，改变视觉轴仅通过像素策略引发的状态条件动作分布影响性能，为视觉泛化提供清晰抽象框架。基于此环境，我们构建KAGE-Bench基准，包含6个已知轴测试套件共34组训练-评估配置对，以隔离单一视觉偏移。采用标准PPO-CNN基线实验发现：背景与光度偏移常导致任务完全失败，而智能体外貌偏移影响相对温和；部分偏移能保持前进动作但破坏任务完成，表明仅凭回报指标可能掩盖泛化缺陷。全向量化JAX实现支持单GPU每秒3300万环境步长，实现视觉因素的高效可复现扫描。代码：https://avanturist322.github.io/KAGEBench/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to disentangle and systematically evaluate visual generalization failures in pixel-based reinforcement learning, where agents often struggle under visual distribution shifts despite unchanged dynamics. The method introduces KAGE-Env, a JAX-native 2D platformer that factorizes observation into controllable visual axes, and KAGE-Bench, a benchmark with six suites isolating individual shifts. Experimental results using a PPO-CNN baseline reveal axis-dependent failures: background and photometric shifts often cause complete collapse, while agent-appearance shifts are less severe, and some shifts preserve forward motion but break task completion, indicating return alone can mask generalization issues. The implementation achieves up to 33M steps per second on a single GPU, enabling fast, reproducible sweeps.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要解耦并系统评估基于像素的强化学习中的视觉泛化失败问题，即智能体在视觉分布变化下常表现不佳，尽管潜在动态未变。方法上引入了KAGE-Env，这是一个JAX原生的2D平台游戏，将观察分解为可控的视觉轴，并基于此构建了KAGE-Bench基准，包含六个隔离单个视觉变化的测试套件。使用PPO-CNN基线的实验结果显示轴依赖性失败：背景和光度变化常导致完全崩溃，而智能体外观变化相对温和，某些变化能保持前进运动但破坏任务完成，表明仅靠回报可能掩盖泛化问题。该实现可在单GPU上达到每秒3300万步，支持快速、可重复的视觉因素扫描。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment</div>
<div class="meta-line">Authors: Punit Kumar, Vaibhav Saran, Divyesh Patel, Nitin Kulkarni, Alina Vereshchaka</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-20T18:41:44+00:00 · Latest: 2026-01-20T18:41:44+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14228v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14228v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力机制的离线强化学习与聚类用于可解释性脓毒症治疗</div>
<div class="mono" style="margin-top:8px">脓毒症仍是重症监护病房的主要致死原因之一，及时准确的治疗决策对患者预后至关重要。本研究提出一个可解释的决策支持框架，系统集成四个核心模块：（1）基于聚类的分层模块，通过统计验证的聚类方法在患者入住ICU时将其分为低、中、高风险组；（2）利用变分自编码器（VAE）和扩散模型的合成数据增强流程，以丰富补液或血管加压药使用等代表性不足的治疗轨迹；（3）采用优势加权回归（AWR）训练的离线强化学习智能体，配备轻量级注意力编码器，并集成保守型安全感知治疗推荐模型；（4）基于多模态大语言模型（LLM）的决策依据生成模块，可结合临床情境与检索的专家知识生成自然语言解释。在MIMIC-III和eICU数据集上的评估表明，该方法在实现高治疗准确率的同时，能为临床医生提供可解释且稳健的决策建议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high mortality of sepsis in ICUs and the need for interpretable clinical decision support, this paper proposes a framework integrating patient risk stratification via clustering, synthetic data augmentation using VAE and diffusion models for underrepresented treatments, an offline RL agent with AWR and attention for safety-aware recommendations, and an LLM-based rationale generator. Experimental results on MIMIC-III and eICU datasets show the approach achieves high treatment accuracy while delivering interpretable, robust policy recommendations.</div>
<div class="mono" style="margin-top:8px">针对ICU中脓毒症高死亡率及可解释临床决策支持的需求，本文提出一个框架，整合了基于聚类的患者风险分层、使用VAE和扩散模型对代表性不足治疗进行合成数据增强、采用AWR和注意力机制的离线强化学习智能体以提供安全感知建议，以及基于大语言模型的理由生成模块。在MIMIC-III和eICU数据集上的实验结果表明，该方法实现了高治疗准确性，同时提供可解释且稳健的策略推荐。</div>
</details>
</div>
<div class="card">
<div class="title">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</div>
<div class="meta-line">Authors: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</div>
<div class="meta-line">First: 2025-12-19T17:55:48+00:00 · Latest: 2026-01-20T18:25:48+00:00</div>
<div class="meta-line">Comments: 28 pages, 25 figures. The first four authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17853v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17853v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyTask：一个用于推进仿真到现实策略学习的自动化任务与数据生成框架</div>
<div class="mono" style="margin-top:8px">通用机器人学习仍受数据限制：大规模、多样化且高质量的交互数据在现实世界中收集成本高昂。虽然仿真已成为扩展数据收集的有前景途径，但相关任务——包括仿真任务设计、任务感知场景生成、专家演示合成以及仿真到现实迁移——仍需大量人力投入。本文提出AnyTask，一个自动化框架，将大规模并行GPU仿真与基础模型结合，以设计多样化操作任务并合成机器人数据。我们引入了三个AnyTask智能体，用于生成旨在解决尽可能多任务的专家演示：1) ViPR，一种新型任务与运动规划智能体，采用视觉语言模型循环并行优化；2) ViPR-Eureka，一种强化学习智能体，结合生成密集奖励与大型语言模型引导的接触采样；3) ViPR-RL，一种混合规划与学习方法，仅使用稀疏奖励联合生成高质量演示。我们在生成数据上训练行为克隆策略，在仿真中验证，并直接部署于真实机器人硬件。这些策略能泛化至新物体位姿，在一系列现实世界抓放、抽屉开启、密集接触推动及长时程操作任务中平均成功率可达44%。项目网站：https://anytask.rai-inst.com。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the data scarcity challenge in generalist robot learning by introducing AnyTask, an automated framework that leverages GPU-accelerated simulation and foundation models to generate diverse manipulation tasks and synthesize robot interaction data. The method employs three specialized agents—ViPR for task and motion planning with visual language model refinement, ViPR-Eureka for reinforcement learning with generated dense rewards and LLM-guided contact sampling, and ViPR-RL for hybrid planning and learning—to produce expert demonstrations efficiently. Experimental results show that behavior cloning policies trained on this generated data achieve a 44% average success rate when deployed directly on real robots, generalizing to novel object poses across tasks like pick-and-place, drawer opening, and contact-rich pushing.</div>
<div class="mono" style="margin-top:8px">本文针对通用机器人学习中的数据稀缺问题，提出了AnyTask框架，该框架利用GPU加速仿真和基础模型自动生成多样化的操作任务并合成机器人交互数据。方法采用三个专用智能体：ViPR通过视觉语言模型循环优化进行任务与运动规划，ViPR-Eureka结合生成密集奖励和LLM引导接触采样进行强化学习，ViPR-RL则采用混合规划与学习策略，共同高效生成专家示范。实验结果表明，基于生成数据训练的行为克隆策略在真实机器人上直接部署时，平均成功率可达44%，能够泛化至新物体位姿，适用于抓放、开抽屉、接触式推动等长时程操作任务。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Score-Threshold Optimization for Interpretable Risk Assessment</div>
<div class="meta-line">Authors: Fardin Gankhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Kimia Ghobadi</div>
<div class="meta-line">First: 2025-10-24T18:07:24+00:00 · Latest: 2026-01-20T18:20:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21934v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21934v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Risk assessment tools in healthcare commonly employ point-based scoring systems that map patients to ordinal risk categories via thresholds. While electronic health record (EHR) data presents opportunities for data-driven optimization of these tools, two fundamental challenges impede standard supervised learning: (1) labels are often available only for extreme risk categories due to intervention-censored outcomes, and (2) misclassification cost is asymmetric and increases with ordinal distance. We propose a mixed-integer programming (MIP) framework that jointly optimizes scoring weights and category thresholds in the face of these challenges. Our approach prevents label-scarce category collapse via threshold constraints, and utilizes an asymmetric, distance-aware objective. The MIP framework supports governance constraints, including sign restrictions, sparsity, and minimal modifications to incumbent tools, ensuring practical deployability in clinical workflows. We further develop a continuous relaxation of the MIP problem to provide warm-start solutions for more efficient MIP optimization. We apply the proposed score optimization framework to a case study of inpatient falls risk assessment using the Johns Hopkins Fall Risk Assessment Tool.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可解释风险评估的联合评分-阈值优化</div>
<div class="mono" style="margin-top:8px">医疗健康领域的风险评估工具通常采用基于分数的评分系统，通过阈值将患者映射至有序风险等级。尽管电子健康记录（EHR）数据为这些工具的数据驱动优化提供了契机，但两大根本挑战阻碍了标准监督学习的应用：（1）由于干预截断结果，标签通常仅适用于极端风险等级；（2）误分类成本具有非对称性，并随序数距离增加而上升。针对这些挑战，我们提出一个混合整数规划（MIP）框架，可同步优化评分权重与分类阈值。该方法通过阈值约束防止标签稀缺类别的塌缩，并采用非对称且距离感知的目标函数。该MIP框架支持治理约束，包括符号限制、稀疏性以及对现有工具的最小化修改，从而确保在临床工作流程中的实际可部署性。我们进一步开发了MIP问题的连续松弛形式，为更高效的MIP优化提供预热启动解。我们将所提出的评分优化框架应用于约翰斯·霍普金斯跌倒风险评估工具的住院患者跌倒风险评估案例研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of standard supervised learning for optimizing healthcare risk assessment tools, motivated by two key challenges: labels are often only available for extreme risk categories due to intervention-censored outcomes, and misclassification costs are asymmetric and increase with ordinal distance. The method introduces a mixed-integer programming (MIP) framework that jointly optimizes scoring weights and category thresholds, incorporating threshold constraints to prevent label-scarce category collapse and an asymmetric, distance-aware objective, while supporting governance constraints like sign restrictions and sparsity for clinical deployability. In a case study on inpatient falls risk assessment using the Johns Hopkins Fall Risk Assessment Tool, the approach demonstrates practical applicability, with a continuous relaxation developed to provide warm-start solutions for efficient MIP optimization.</div>
<div class="mono" style="margin-top:8px">本文针对优化医疗风险评分工具时标准监督学习的局限性，其动机源于两个关键挑战：由于干预导致的结果截断，标签通常仅适用于极端风险类别，且误分类成本具有不对称性并随序数距离增加。方法上，提出了一个混合整数规划框架，联合优化评分权重和类别阈值，通过阈值约束防止标签稀缺类别崩溃，并采用不对称、距离感知的目标函数，同时支持符号限制和稀疏性等治理约束以确保临床可部署性。在约翰霍普金斯跌倒风险评估工具的住院患者跌倒风险案例研究中，该方法展示了实际应用性，并开发了连续松弛技术为高效MIP优化提供预热解。</div>
</details>
</div>
<div class="card">
<div class="title">Generalization and Completeness of Stochastic Local Search Algorithms</div>
<div class="meta-line">Authors: Daniel Loscos, Narciso Marti-Oliet, Ismael Rodriguez</div>
<div class="meta-line">First: 2026-01-20T18:17:45+00:00 · Latest: 2026-01-20T18:17:45+00:00</div>
<div class="meta-line">Comments: This paper was published in Swarm and Evolutionary Computation. The present version is the author&#x27;s accepted manuscript</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机局部搜索算法的泛化性与完备性</div>
<div class="mono" style="margin-top:8px">本研究将随机局部搜索（SLS）启发式算法统一归纳为一个形式化模型。该模型包含两个核心组件：一个尽可能大的通用结构，以及一个尽可能小的参数化结构。每种启发式算法通过以不同方式实例化参数部分获得。本文具体展示了遗传算法（GA）、蚁群优化（ACO）和粒子群优化（PSO）的实例。随后，我们运用该模型证明了SLS算法整体具有图灵完备性。证明过程通过构建能模拟任意图灵机的遗传算法实现。该图灵完备性意味着：对于遗传算法乃至广义的SLS方法集（尽管特定方法未必如此），判定输入与计算输出间任何非平凡性质的关系均是不可判定的。文中对PSO和ACO也进行了类似但更非形式化的证明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to unify and formally analyze stochastic local search (SLS) algorithms, this paper proposes a generalized model consisting of a large common structure and a small parametric part, from which specific heuristics like Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) can be instantiated. The method involves using this framework to construct a GA capable of simulating any Turing machine, thereby proving the Turing-completeness of SLS algorithms in general. The main experimental results demonstrate that this Turing-completeness leads to the undecidability of determining any non-trivial property relating inputs to outputs for GA and, by extension, for the broad class of SLS methods, with similar informal proofs provided for PSO and ACO.</div>
<div class="mono" style="margin-top:8px">本文旨在统一和形式化分析随机局部搜索（SLS）算法，提出了一个通用模型，该模型包含一个尽可能大的公共结构和一个尽可能小的参数化部分，通过实例化参数部分可得到如遗传算法（GA）、蚁群优化（ACO）和粒子群优化（PSO）等具体启发式方法。方法上，利用该框架构建了一个能够模拟任何图灵机的GA，从而证明了SLS算法整体的图灵完备性。主要实验结果表明，这种图灵完备性导致对于GA以及广义的SLS方法，确定输入与输出之间任何非平凡性质都是不可判定的，并对PSO和ACO给出了类似的非正式证明。</div>
</details>
</div>
<div class="card">
<div class="title">InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</div>
<div class="meta-line">Authors: Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-20T18:15:38+00:00 · Latest: 2026-01-20T18:15:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14209v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InT：自提干预实现大语言模型推理中的信用分配</div>
<div class="mono" style="margin-top:8px">结果奖励强化学习（RL）已被证明能有效提升大语言模型（LLM）的推理能力。然而，标准RL仅对最终答案层面分配信用：当结果错误时惩罚整个推理轨迹，结果正确时则均匀强化所有步骤。这导致失败轨迹中的正确中间步骤可能被抑制，而成功轨迹中的无效步骤反而被强化。我们将此失效模式称为信用分配问题。虽然训练过程奖励模型是自然的解决方案，但精准优化此类模型以识别纠正式推理步骤仍具挑战性。本文提出干预训练（InT），该训练范式使模型通过提出简短、有针对性的修正（引导轨迹获得更高奖励）对其自身推理轨迹进行细粒度信用分配。利用数学推理数据集中普遍存在的参考答案，并基于验证模型生成解比从头生成正确解更简单的事实，模型首先识别其推理中的首个错误，提出单步干预以将轨迹导向正确解。随后对错误发生前的策略展开轨迹（与干预步骤拼接）进行监督微调（SFT），从而将错误定位至导致失败的具体步骤。实验表明，所得模型可作为RL训练更优的初始化基础。经过InT及后续RL微调，我们在IMO-AnswerBench上将4B参数基模型的准确率提升近14%，性能超越gpt-oss-20b等更大规模开源模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the credit assignment problem in outcome-reward reinforcement learning (RL) for large language models (LLMs), where standard RL penalizes or reinforces entire reasoning traces uniformly based on final answer correctness, potentially discouraging correct intermediate steps or reinforcing spurious ones. To remedy this, the authors propose Intervention Training (InT), a paradigm where the model self-assigns credit by identifying the first error in its reasoning trace and proposing a targeted single-step correction to steer the trajectory toward the correct solution, using reference solutions for verification; this is followed by supervised fine-tuning on the corrected trace to localize errors. Experimental results show that InT significantly improves model initialization for RL, leading to a nearly 14% accuracy gain over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在结果奖励强化学习中存在的信用分配问题，即标准强化学习仅根据最终答案正确性对整个推理轨迹进行统一惩罚或强化，可能抑制正确中间步骤或强化错误步骤。为解决此问题，作者提出干预训练方法，使模型通过识别自身推理轨迹中的首个错误并提出针对性单步修正来引导轨迹朝向正确解，利用参考解进行验证，随后对修正后的轨迹进行监督微调以定位错误。实验结果表明，该方法显著改善了模型作为强化学习初始化的效果，在IMO-AnswerBench上将4B参数基础模型的准确率提升近14%，优于更大的开源模型。</div>
</details>
</div>
<div class="card">
<div class="title">Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery</div>
<div class="meta-line">Authors: Albina Galiullina, Wouter van Heeswijk, Tom van Woensel</div>
<div class="meta-line">First: 2026-01-20T18:00:42+00:00 · Latest: 2026-01-20T18:00:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14196v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>差异化取件点服务在最后一公里配送中的减排应用</div>
<div class="mono" style="margin-top:8px">取件点作为家庭配送的可持续替代方案被广泛认可，通过在取件点整合订单可缩短配送路径并提高首次投递成功率。然而，当顾客驾车取件时，这些效益可能被抵消。本研究提出差异化取件点服务政策，旨在同步减少配送卡车路线与顾客出行的碳排放。该政策为每位顾客推荐单一取件点（而非无限制选择所有点位），同时保留家庭配送选项。研究基于动态随机场景，为顾客推荐的取件点取决于历史顾客位置与配送选择。通过采用强化学习方法设计差异化政策，综合考虑顾客与取件点的空间关系及其对未来路线整合的影响。计算实验表明，差异化取件点服务可显著降低总碳排放量：相较于纯家庭配送减排最高达9%，较无限制取件点选择、最近取件点分配等对照方案平均减排2%。该政策在取件点密集、点位间距短的稠密城区效果尤为显著。此外，当顾客对取件点配送的接受度较低时，动态考量顾客到达与选择行为显得尤为重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to mitigate the environmental impact of last-mile delivery, this study addresses how offering customers a single recommended pickup point, rather than unrestricted choice, can reduce combined emissions from delivery vehicles and customer travel. The method employs a reinforcement learning approach to dynamically assign pickup points based on spatial relationships and prior customer decisions, optimizing for route consolidation in a stochastic setting. Experimental results demonstrate that this differentiated policy reduces total carbon emissions by up to 9% compared to home-only delivery and by an average of 2% over alternative policies like unrestricted choice or nearest-point assignment, with particular efficacy in dense urban areas with many pickup points.</div>
<div class="mono" style="margin-top:8px">本研究旨在减少最后一公里配送的环境影响，其动机在于通过向顾客推荐单一取货点而非提供无限制选择，以降低配送车辆和顾客出行产生的综合排放。方法上采用基于强化学习的策略，根据顾客与取货点的空间关系及历史选择动态分配取货点，从而在随机环境中优化路线整合。实验结果表明，该差异化策略相比仅提供送货上门服务可减少高达9%的总碳排放，相较于无限制选择或最近取货点分配等替代策略平均降低2%，且在取货点密集、距离较短的城区环境中效果尤为显著。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Efficient Agents: Memory, Tool learning, and Planning</div>
<div class="meta-line">Authors: Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu, Yuchen Fan, Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, Zhiqiang Kou, Daizong Liu, Qi Li, Ning Ding, Siheng Chen, Jing Shao</div>
<div class="meta-line">First: 2026-01-20T17:51:56+00:00 · Latest: 2026-01-20T17:51:56+00:00</div>
<div class="meta-line">Comments: 35 pages, 200 references</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向高效智能体：记忆、工具学习与规划</div>
<div class="mono" style="margin-top:8px">近年来，将大语言模型扩展为智能体系统的研究日益受到关注。尽管智能体的效能持续提升，但对实际部署至关重要的效率问题却常被忽视。本文从智能体的三个核心组成部分——记忆、工具学习和规划——出发，结合延迟、令牌消耗、步骤数等成本因素，系统探讨效率优化。为全面研究智能体系统自身的效率，我们综述了近期多种实现各异但常遵循共同高层原则的方法，包括但不限于通过压缩与管理限制上下文、设计强化学习奖励以最小化工具调用、采用受控搜索机制提升效率等，并对此展开详细讨论。我们通过两种互补方式界定效率：在固定成本预算下比较效能，以及在可比效能水平下比较成本。这种权衡亦可从效能与成本的帕累托前沿视角理解。基于此，我们还通过总结各组件评估方案、整合基准与方法研究中常用效率指标，审视了面向效率的基准测试。此外，本文讨论了关键挑战与未来方向，旨在提供前瞻性见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the often-overlooked issue of efficiency in AI agents built from large language models, focusing on three core components: memory, tool learning, and planning, with costs such as latency and token usage in mind. The method involves a comprehensive review of recent approaches that share high-level principles like context compression, reward design to minimize tool use, and controlled search mechanisms to enhance efficiency. The main experimental results are framed through a dual characterization of efficiency—comparing performance under fixed cost budgets and costs at comparable performance levels—and the paper synthesizes evaluation protocols and metrics from existing benchmarks to analyze the effectiveness-cost Pareto frontier, concluding with a discussion of key challenges and future directions.</div>
<div class="mono" style="margin-top:8px">本文针对基于大语言模型构建的智能代理系统中常被忽视的效率问题展开研究，聚焦于记忆、工具学习和规划这三个核心组件，并考虑了延迟、令牌使用等成本因素。研究方法是对近期各种方法进行全面综述，这些方法在实现上各异但共享高级原则，如通过压缩和管理来限制上下文、设计强化学习奖励以减少工具调用，以及采用受控搜索机制提升效率。主要实验结果通过两种互补方式表征效率：在固定成本预算下比较有效性，以及在可比有效性水平下比较成本；论文还从现有基准中综合了评估协议和常用效率指标，以分析有效性与成本的帕累托前沿，最后讨论了关键挑战和未来方向。</div>
</details>
</div>
<div class="card">
<div class="title">UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms</div>
<div class="meta-line">Authors: Denis Belomestny, Ilya Levin, Alexey Naumov, Sergey Samsonov</div>
<div class="meta-line">First: 2021-05-05T15:38:36+00:00 · Latest: 2026-01-20T15:57:19+00:00</div>
<div class="meta-line">Comments: JOTA camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2105.02135v5">Abs</a> · <a href="https://arxiv.org/pdf/2105.02135v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy evaluation is an important instrument for the comparison of different algorithms in Reinforcement Learning (RL). However, even a precise knowledge of the value function $V^π$ corresponding to a policy $π$ does not provide reliable information on how far the policy $π$ is from the optimal one. We present a novel model-free upper value iteration procedure ({\sf UVIP}) that allows us to estimate the suboptimality gap $V^{\star}(x) - V^π(x)$ from above and to construct confidence intervals for \(V^\star\). Our approach relies on upper bounds to the solution of the Bellman optimality equation via the martingale approach. We provide theoretical guarantees for {\sf UVIP} under general assumptions and illustrate its performance on a number of benchmark RL problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UVIP：评估强化学习算法的无模型方法</div>
<div class="mono" style="margin-top:8px">策略评估是强化学习中比较不同算法的重要工具。然而，即使精确掌握策略π对应的价值函数V^π，也无法可靠判断该策略与最优策略的差距。本文提出一种新颖的无模型上界值迭代方法（UVIP），能够从上方估计次优性差距V^⋆(x)-V^π(x)，并为V^⋆构建置信区间。该方法通过鞅方法对贝尔曼最优方程的解进行上界估计。我们在一般性假设下为UVIP提供理论保证，并在多个强化学习基准问题上展示其性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces UVIP, a model-free method to evaluate reinforcement learning algorithms by estimating the suboptimality gap between a policy&#x27;s value function and the optimal value, addressing the limitation that knowing only the value function does not indicate proximity to optimality. The approach uses an upper value iteration procedure based on martingale-derived upper bounds to the Bellman optimality equation, enabling estimation from above and confidence interval construction for the optimal value. Experimental results on benchmark RL problems demonstrate the method&#x27;s effectiveness in providing reliable suboptimality assessments under general theoretical guarantees.</div>
<div class="mono" style="margin-top:8px">本文提出UVIP，一种无模型方法来评估强化学习算法，通过估计策略价值函数与最优价值之间的次优差距，解决了仅知价值函数无法判断接近最优程度的问题。该方法基于鞅方法对贝尔曼最优方程解的上界，采用上值迭代过程，实现了从上方的估计并为最优价值构建置信区间。在多个基准RL问题上的实验结果表明，该方法在一般理论保证下能有效提供可靠的次优性评估。</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning</div>
<div class="meta-line">Authors: Babacar Toure, Dimitrios Tsilimantos, Omid Esrafilian, Marios Kountouris</div>
<div class="meta-line">First: 2026-01-20T15:55:11+00:00 · Latest: 2026-01-20T15:55:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14092v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14092v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力机制的多目标强化学习优化无人机辅助物联网网络中的能量与数据收集</div>
<div class="mono" style="margin-top:8px">凭借其适应性与机动性，无人机在无线网络服务中日益重要，尤其在数据采集任务中。基于人工智能的方法因其能在大规模复杂环境中处理无人机路径规划问题，并弥合与实际部署的差距而备受关注。然而，现有算法常受限于训练数据不足，在高度动态环境中表现欠佳，且往往忽视任务固有的多目标特性，处理方式过于简化。为克服这些局限，本文提出一种基于注意力机制的多目标强化学习架构，能在未知无线信道条件下，显式权衡城市环境中的数据收集与能耗。该方法构建的单一模型无需微调或重新训练，即可适应不同的权衡偏好与动态场景参数。大量仿真表明，本方法在性能、模型紧凑性、样本效率及对未见过场景的泛化能力方面均显著优于现有强化学习方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to improve UAV path planning for IoT data collection in dynamic urban environments, where existing AI methods often lack sufficient training data and oversimplify the multi-objective trade-offs between data harvesting and energy efficiency. The authors propose an attention-based multi-objective reinforcement learning architecture that explicitly balances these competing goals without requiring prior knowledge of wireless channels, enabling a single adaptable model that adjusts to different preference weights and dynamic parameters without retraining. Experimental results from simulations demonstrate that their method significantly outperforms existing RL solutions in overall performance, model compactness, sample efficiency, and generalization to unseen scenarios.</div>
<div class="mono" style="margin-top:8px">本文的动机在于改进无人机在动态城市环境中为物联网数据收集进行路径规划的能力，现有人工智能方法常因训练数据不足且过度简化数据收集与能耗之间的多目标权衡而受限。作者提出了一种基于注意力的多目标强化学习架构，无需无线信道先验知识即可显式平衡这些竞争目标，实现单一自适应模型，能根据不同偏好权重和动态参数调整而无需重新训练。仿真实验结果表明，该方法在整体性能、模型紧凑性、样本效率以及对未见场景的泛化能力上均显著优于现有强化学习解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning</div>
<div class="meta-line">Authors: Cheol-Hui Lee, Hwa-Yeon Lee, Dong-Joo Kim</div>
<div class="meta-line">First: 2026-01-20T13:38:01+00:00 · Latest: 2026-01-20T13:38:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13964v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13964v1">PDF</a> · <a href="https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug">Code1</a> · <a href="https://github.com/dlcjfgmlnasa/RL-BioAug">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\%) of labeled data to guide the agent&#x27;s policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\% and 8.80\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\% probability for sleep stage classification and Crop \&amp; Resize with a 77\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RL-BioAug：面向自监督脑电表征学习的标签高效强化学习方法</div>
<div class="mono" style="margin-top:8px">数据增强的质量是决定脑电任务中对比学习性能的关键因素。尽管该范式在利用未标记数据方面前景广阔，但由于脑电信号具有非平稳性（统计特性随时间变化），静态或随机的增强策略往往难以保留其内在信息。为此，我们提出RL-BioAug框架，通过标签高效的强化学习智能体自主确定最优增强策略。该方法仅使用少量（10%）标记数据指导智能体策略，使编码器能以严格自监督的方式学习鲁棒表征。实验结果表明，RL-BioAug显著优于随机选择策略，在Sleep-EDFX和CHB-MIT数据集上的Macro-F1分数分别大幅提升9.69%和8.80%。值得注意的是，该智能体针对不同任务自主选择最优策略——例如在睡眠分期任务中以62%概率选择时间掩蔽，在癫痫检测任务中以77%概率选择裁剪与缩放。本框架有望替代传统的启发式增强方法，建立数据增强的自主新范式。源代码发布于https://github.com/dlcjfgmlnasa/RL-BioAug。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of static or random data augmentation strategies for non-stationary EEG signals in contrastive learning, this paper proposes RL-BioAug, a framework that uses a label-efficient reinforcement learning agent to autonomously learn optimal augmentation policies. The method requires only 10% of labeled data to guide the RL agent, enabling the encoder to learn robust representations in a self-supervised manner. Experimental results on the Sleep-EDFX and CHB-MIT datasets show the framework significantly outperforms random augmentation, achieving improvements of 9.69% and 8.80% in Macro-F1 score, with the agent learning distinct optimal strategies like Time Masking for sleep stage classification and Crop &amp; Resize for seizure detection.</div>
<div class="mono" style="margin-top:8px">针对脑电图信号的非平稳特性导致静态或随机数据增强策略在对比学习中效果有限的问题，本文提出了RL-BioAug框架，利用标签高效的强化学习代理自主学习最优增强策略。该方法仅需10%的标注数据来指导代理，使编码器能以自监督方式学习鲁棒表征。在Sleep-EDFX和CHB-MIT数据集上的实验结果表明，该框架显著优于随机增强策略，宏F1分数分别提升了9.69%和8.80%，且代理学会了针对不同任务的最优策略，例如在睡眠分期中高概率选择时间掩码，在癫痫检测中高概率选择裁剪与重缩放。</div>
</details>
</div>
<div class="card">
<div class="title">Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning</div>
<div class="meta-line">Authors: Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, Yike Guo</div>
<div class="meta-line">First: 2026-01-20T13:18:18+00:00 · Latest: 2026-01-20T13:18:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13942v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13942v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model&#x27;s capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一瞥或凝视：通过强化学习激励大语言模型自适应聚焦搜索</div>
<div class="mono" style="margin-top:8px">大规模多模态模型在视觉理解方面取得了显著成功，但由于静态参数化知识的限制，其在涉及长尾实体或动态信息的知识密集型查询上仍面临困难。近期基于搜索增强的方法试图解决这一局限，但现有方法依赖无差别的全图像检索，引入了大量视觉冗余和噪声，且缺乏深度迭代反思，限制了其在复杂视觉查询上的有效性。为克服这些挑战，我们提出了“一瞥或凝视”框架，这是一个从被动感知转向主动视觉规划的完全自主框架。该框架引入了选择性凝视机制，能动态选择是全局一瞥还是聚焦高价值区域，在检索前过滤无关信息。我们设计了双阶段训练策略：通过监督微调实现反思性行为对齐以奠定基础范式，再通过复杂度自适应强化学习增强模型处理复杂查询的迭代推理能力。在六个基准测试上的实验证明了其领先性能。消融研究证实选择性凝视机制与复杂度自适应强化学习对实现高效视觉搜索均不可或缺。我们将很快发布相关数据与模型以供进一步探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of Large Multimodal Models (LMMs) in handling knowledge-intensive visual queries, as static parametric knowledge and indiscriminate image retrieval methods introduce redundancy and noise. The proposed solution, Glance-or-Gaze (GoG), is an autonomous framework that employs a Selective Gaze mechanism to dynamically decide between glancing at global context or gazing into specific high-value regions, thereby filtering irrelevant information before retrieval. It is trained via a dual-stage strategy involving supervised fine-tuning for behavior alignment and complexity-adaptive reinforcement learning to enhance iterative reasoning. Experimental results on six benchmarks show state-of-the-art performance, with ablation studies confirming the necessity of both the Selective Gaze mechanism and the reinforcement learning component for effective visual search.</div>
<div class="mono" style="margin-top:8px">本文针对大型多模态模型在处理知识密集型视觉查询时的局限性，即静态参数知识和无差别的图像检索会引入冗余和噪声。提出的解决方案Glance-or-Gaze（GoG）是一个自主框架，采用选择性凝视机制，动态决定是瞥视全局上下文还是凝视特定高价值区域，从而在检索前过滤无关信息。该框架通过双阶段策略进行训练：包括用于行为对齐的监督微调和复杂度自适应强化学习，以增强迭代推理能力。在六个基准测试上的实验结果表明其性能达到最先进水平，消融研究证实选择性凝视机制和强化学习组件对于有效的视觉搜索都至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</div>
<div class="meta-line">Authors: Jianfeng Si, Lin Sun, Zhewen Tan, Xiangzheng Zhang</div>
<div class="meta-line">First: 2025-08-12T02:39:33+00:00 · Latest: 2026-01-20T13:15:53+00:00</div>
<div class="meta-line">Comments: 15 pages,3 figures,5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14904v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.14904v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model&#x27;s safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于魔法令牌引导协同训练的大语言模型高效可切换安全控制</div>
<div class="mono" style="margin-top:8px">当前大语言模型的内容安全方法（如监督微调和基于人类反馈的强化学习）通常依赖多阶段训练流程，且缺乏细粒度的部署后可控性。为解决这些局限，我们提出统一的协同训练框架，在单阶段监督微调中高效集成三种安全行为模式：积极型（合法/亲社会）、消极型（无过滤/风险倾向）和拒绝型（拒绝导向/保守）。每种行为均可通过简单的系统级指令（即魔法令牌）动态激活，实现推理阶段的隐蔽高效行为切换。这种灵活性支持多样化的部署场景：积极模式用于安全用户交互，消极模式用于内部红队测试，拒绝模式用于响应上游审核信号的语境感知拒绝。该协同训练策略在输出空间诱导出显著的安全对齐边界，其特征是各安全模式对应的响应分布高度分离。该边界的存在为模型安全鲁棒性提供了实证依据，并实现了前所未有的细粒度控制。实验表明，本方法在安全对齐质量上媲美监督微调+直接偏好优化组合，其中8B模型的安全性能显著超越671B参数的DeepSeek-R1，同时大幅降低训练复杂度与部署成本。本研究为大语言模型内容安全提供了可扩展、高效且高度可控的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing safety alignment methods for LLMs, such as multi-stage training and lack of post-deployment controllability, by proposing a unified co-training framework that integrates positive, negative, and rejective safety behaviors in a single supervised fine-tuning stage. The method uses magic tokens to dynamically switch between these behaviors at inference, enabling flexible deployment for safe interaction, red-teaming, or context-aware refusals. Experimental results demonstrate that this approach matches the safety quality of SFT+DPO, with an 8B model outperforming DeepSeek-R1 (671B) in safety, while reducing training complexity and deployment costs, and reveals a Safety Alignment Margin in the output space for robust control.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型现有安全对齐方法（如多阶段训练和部署后可控性不足）的局限性，提出了一种统一的协同训练框架，在单一监督微调阶段内整合了积极、消极和拒绝三种安全行为。该方法通过魔法令牌在推理时动态切换行为，支持安全交互、内部红队测试或上下文感知拒绝等灵活部署场景。实验结果表明，该方法在安全性能上匹配SFT+DPO，其8B模型在安全性上超越了DeepSeek-R1（671B），同时显著降低了训练复杂度和部署成本，并在输出空间中揭示了安全对齐边界，为模型的稳健控制提供了实证依据。</div>
</details>
</div>
<div class="card">
<div class="title">GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning</div>
<div class="meta-line">Authors: Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya Wang, Jun Wang, Ye Shi</div>
<div class="meta-line">First: 2025-05-24T15:57:07+00:00 · Latest: 2026-01-20T12:33:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18763v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18763v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO&#x27;s superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenPO：生成扩散模型与在线策略强化学习的融合</div>
<div class="mono" style="margin-top:8px">强化学习领域的最新进展揭示了基于生成扩散的策略在探索能力与多模态特性方面的优势。尽管离线强化学习和离线策略强化学习已取得显著进步，但将扩散策略整合至PPO等在线策略框架的研究仍显不足。这一空白在IsaacLab等大规模并行GPU加速仿真器广泛应用的背景下尤为突出——此类平台专为在线策略强化学习算法优化，能高效训练复杂机器人任务。核心挑战在于计算扩散策略下的状态-动作对数似然：高斯策略可直接计算，而基于流的模型因不可逆的前向-反向过程及离散化误差（如欧拉-丸山近似）导致计算困难。为弥合此差距，我们提出GenPO框架，通过精确扩散反演构建可逆动作映射。该框架创新性地引入双重虚拟动作机制，借助交替更新实现可逆性，突破对数似然计算瓶颈。同时，利用动作对数似然进行无偏熵与KL散度估计，支持在线策略更新中的KL自适应学习率与熵正则化。在IsaacLab八项基准测试（涵盖足式运动、灵巧操作、空中控制及机械臂任务）中，GenPO均优于现有强化学习基线。值得注意的是，GenPO首次成功将扩散策略融入在线策略强化学习，为大规模并行化训练与真实机器人部署开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces GenPO, a framework that integrates generative diffusion models into on-policy reinforcement learning (RL) to address the challenge of computing state-action log-likelihoods, which is straightforward for Gaussian policies but intractable in diffusion models due to irreversible forward-reverse processes and discretization errors. The method leverages exact diffusion inversion to construct invertible action mappings via a novel doubled dummy action mechanism, enabling invertibility through alternating updates and allowing for unbiased entropy and KL divergence estimation to support KL-adaptive learning rates and entropy regularization. Experimental results across eight IsaacLab benchmarks, including legged locomotion, dexterous manipulation, aerial control, and robotic arm tasks, demonstrate GenPO&#x27;s superiority over existing RL baselines, marking it as the first successful integration of diffusion policies into on-policy RL for large-scale parallelized training and real-world robotic deployment.</div>
<div class="mono" style="margin-top:8px">本文提出了GenPO框架，将生成扩散模型整合到在线策略强化学习中，以解决状态-动作对数似然计算的关键挑战，该挑战在高斯策略中简单直接，但在扩散模型中因不可逆的前向-反向过程和离散化误差而难以处理。该方法利用精确的扩散反转，通过一种新颖的双重虚拟动作机制构建可逆的动作映射，实现交替更新的可逆性，并支持无偏的熵和KL散度估计，从而启用KL自适应学习率和熵正则化。在八个IsaacLab基准测试（包括足式运动、灵巧操作、空中控制和机械臂任务）上的实验结果表明，GenPO优于现有强化学习基线，成为首个成功将扩散策略融入在线策略强化学习的方法，为大规模并行化训练和现实世界机器人部署解锁了潜力。</div>
</details>
</div>
<div class="card">
<div class="title">TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography</div>
<div class="meta-line">Authors: Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam</div>
<div class="meta-line">Venue: ISBI</div>
<div class="meta-line">First: 2026-01-20T12:26:38+00:00 · Latest: 2026-01-20T12:26:38+00:00</div>
<div class="meta-line">Comments: Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13897v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13897v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TractRLFusion：基于GPT的多策略融合纤维束追踪框架</div>
<div class="mono" style="margin-top:8px">纤维束追踪在无创重建白质纤维通路中具有关键作用，为脑连接研究及精准神经外科手术规划提供重要信息。传统方法主要依赖经典确定性及概率性方法，而近期进展通过监督式深度学习与深度强化学习提升了纤维束重建效果。当前核心挑战在于精确重建白质纤维束的同时最大限度减少虚假连接。为此，我们提出TractRLFusion——一种基于GPT的策略融合框架，通过数据驱动的融合策略整合多个强化学习策略。该方法采用两阶段训练数据选择实现高效策略融合，并通过多评价器微调阶段增强鲁棒性与泛化能力。在HCP、ISMRM和TractoInferno数据集上的实验表明，TractRLFusion在精度与解剖学可靠性方面均优于单一强化学习策略及当前最先进的经典方法与深度强化学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve the accuracy and anatomical reliability of white matter fiber tractography by reducing spurious connections, a persistent challenge in the field. The method introduces TractRLFusion, a novel framework that uses a GPT-based architecture to fuse multiple reinforcement learning policies through a data-driven strategy, involving a two-stage training data selection process and a multi-critic fine-tuning phase for enhanced robustness. Experimental results on HCP, ISMRM, and TractoInferno datasets show that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and deep reinforcement learning methods in both accuracy and anatomical reliability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过减少虚假连接来提高白质纤维束成像的准确性和解剖学可靠性，这是该领域长期存在的挑战。方法上提出了TractRLFusion，这是一个基于GPT架构的新框架，通过数据驱动的策略融合多个强化学习策略，包括两阶段训练数据选择过程和多批评器微调阶段以增强鲁棒性。在HCP、ISMRM和TractoInferno数据集上的实验结果表明，TractRLFusion在准确性和解剖学可靠性方面优于单个强化学习策略以及最先进的经典方法和深度强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Hierarchical Optimization with Large Language Models</div>
<div class="meta-line">Authors: Andrej Schwanke, Lyubomir Ivanov, David Salinas, Frank Hutter, Arber Zela</div>
<div class="meta-line">First: 2026-01-20T12:10:13+00:00 · Latest: 2026-01-20T12:10:13+00:00</div>
<div class="meta-line">Comments: 23 pages, 21 figures, 9 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13892v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn&#x27;t have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的多目标分层优化方法</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLMs）凭借其强大的推理能力在各领域得到广泛应用，但目前尚不能作为现成的多目标优化驱动工具。传统策略因其处理数值输入的内在能力、平衡探索与帕累托前沿开发的精细建模选择，以及处理多重（冲突）目标的能力，在基准测试中表现优异。本文通过将LLMs作为结构化分层搜索策略中的代理模型和候选采样器，填补了这一空白。通过自适应地将输入空间划分为互不相交的超矩形区域，并用复合评分函数对其进行排序，我们将LLM的生成过程限制在特定高潜力子空间内，从而使问题更易求解——LLM无需推理问题的全局结构，仅需进行局部推理。我们证明，在标准正则性假设下，该算法生成的候选解在豪斯多夫距离意义下收敛于真实帕累托集。实证研究表明，该方法在合成与真实世界基准测试中持续优于基于全局LLM的多目标优化器，并与标准进化算法及贝叶斯优化算法性能相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the gap in using Large Language Models (LLMs) for multi-objective optimization by proposing a hierarchical search strategy that leverages LLMs as surrogate models and candidate samplers. The method adaptively partitions the input space into hyperrectangular regions, ranking them with a composite score to focus the LLM&#x27;s generative process on high-potential sub-spaces, simplifying reasoning from global to local. Experimental results demonstrate that the algorithm generates solutions converging to the true Pareto set theoretically and empirically outperforms global LLM-based optimizers while matching standard evolutionary and Bayesian methods on synthetic and real-world benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在多目标优化中的应用不足，提出了一种分层搜索策略，将LLM用作代理模型和候选采样器。该方法通过自适应地将输入空间划分为超矩形区域，并用复合评分函数进行排序，从而将LLM的生成过程限制在具有高潜力的子空间内，使问题从全局推理简化为局部处理。实验结果表明，该算法生成的候选解在理论上收敛于真实帕累托集，并在合成和真实世界基准测试中，实证上优于基于LLM的全局优化器，同时与标准进化和贝叶斯优化算法表现相当。</div>
</details>
</div>
<div class="card">
<div class="title">Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning</div>
<div class="meta-line">Authors: Shreyansh Jain, Madhav Singhvi, Shreya Rahul Jain, Pranav S, Dishaa Lokesh, Naren Chittibabu, Akash Anandhan</div>
<div class="meta-line">Venue: ICCCA 2025, pp. 1-6</div>
<div class="meta-line">First: 2025-11-20T06:06:30+00:00 · Latest: 2026-01-20T10:04:41+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures, 2 equations, 3 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16073v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most of the traditional Applicant Tracking Systems (ATS) depend on strict matching using keywords, where candidates that are highly qualified are many times disqualified because of minor semantic differences. In this article, the two-stage process of developing a more comprehensive resume assessment system based on a small language model that is trained with fewer than 600M parameters is introduced and fine-tuned by using GRPO with a uniquely designed reward function. The initial stage is Supervised Fine-Tuning (SFT), which is used to create a strong base model with the ability to perceive resumes beyond superficial overlap of keywords. This SFT model is further optimized in the second step with Reinforcement Learning (RL) via GRPO with the help of multi-component-based rewarding, which will not be considered as a commission of tokens matching. In the initial RL experiments, we found a severe difficulty in the shape of reward hacking: overly aggressive penalty terms resulted in unstable training dynamics and prohibitively negative model behavior. This was solved by trial-and-error refinement of the reward and careful training hyperparameter tuning, which led to a stable and controlled process of gentle polishing. The GRPO-refined model shows high real-life performance, as it shows an accuracy of 91% on unseen data used for testing. It has a high recall of 0.85 on the SELECTED class with a perfect precision of 1.0, which highlights its high reliability for identifying qualified applicants. These findings demonstrate that an appropriately structured two-step fine-tuning pipeline can effectively be used to transfer a small language model into human-like candidate evaluation, surpassing the shortcomings of both traditional ATS systems and unrefined uses of reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的求职申请评估中定制奖励函数的数学框架</div>
<div class="mono" style="margin-top:8px">传统申请人追踪系统（ATS）多依赖关键词严格匹配，常因细微语义差异淘汰高资质候选人。本文提出基于参数少于6亿的小型语言模型构建两阶段简历评估系统，采用GRPO算法与定制奖励函数进行微调。第一阶段通过监督微调（SFT）建立基础模型，使其能超越关键词表层匹配理解简历。第二阶段通过GRPO强化学习进行优化，采用多组件奖励机制避免单纯词汇匹配。初期实验发现奖励黑客问题：过度惩罚项导致训练不稳定与模型行为异常。通过奖励函数迭代优化与超参数调整，最终实现稳定可控的精细化训练。GRPO优化模型在测试集上准确率达91%，对“入选”类别的召回率为0.85、精确率达1.0，展现优异实际性能。研究表明：结构化的两阶段微调流程可将小型语言模型转化为类人化评估系统，有效克服传统ATS与原始强化学习的缺陷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional keyword-matching Applicant Tracking Systems (ATS) that often disqualify qualified candidates due to minor semantic differences, this paper introduces a two-stage fine-tuning pipeline using a small language model (&lt;600M parameters) for resume assessment. The method first employs Supervised Fine-Tuning (SFT) to build a base model that understands resumes beyond keyword overlap, then refines it via Reinforcement Learning with GRPO using a custom multi-component reward function designed to avoid token-matching bias. Experimental results show that after overcoming initial reward hacking issues through reward function refinement and hyperparameter tuning, the GRPO-optimized model achieved 91% accuracy on unseen test data, with a recall of 0.85 and perfect precision (1.0) for the SELECTED class, demonstrating high reliability in identifying qualified applicants and surpassing traditional ATS systems.</div>
<div class="mono" style="margin-top:8px">针对传统基于关键词严格匹配的申请人跟踪系统（ATS）常因细微语义差异淘汰合格候选人的问题，本文提出了一种使用小型语言模型（参数少于6亿）进行简历评估的两阶段微调框架。方法首先通过监督微调（SFT）构建一个能超越关键词重叠理解简历的基础模型，然后利用GRPO进行强化学习微调，采用自定义的多组件奖励函数以避免令牌匹配偏差。实验结果表明，在通过奖励函数迭代和超参数调整克服了初始的奖励破解问题后，GRPO优化模型在未见测试数据上达到了91%的准确率，对“选中”类别的召回率为0.85且精确度为1.0，显示出识别合格申请人的高可靠性，超越了传统ATS系统和未优化的强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training</div>
<div class="meta-line">Authors: Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, Nikolay Malkin</div>
<div class="meta-line">First: 2025-01-10T18:18:25+00:00 · Latest: 2026-01-20T09:07:11+00:00</div>
<div class="meta-line">Comments: TMLR; code: https://github.com/GFNOrg/gfn-diffusion/tree/stagger</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.06148v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.06148v2">PDF</a> · <a href="https://github.com/GFNOrg/gfn-diffusion/tree/stagger">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of training neural stochastic differential equations, or diffusion models, to sample from a Boltzmann distribution without access to target samples. Existing methods for training such models enforce time-reversal of the generative and noising processes, using either differentiable simulation or off-policy reinforcement learning (RL). We prove equivalences between families of objectives in the limit of infinitesimal discretization steps, linking entropic RL methods (GFlowNets) with continuous-time objects (partial differential equations and path space measures). We further show that an appropriate choice of coarse time discretization during training allows greatly improved sample efficiency and the use of time-local objectives, achieving competitive performance on standard sampling benchmarks with reduced computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从离散时间策略到连续时间扩散采样器：渐近等价性与加速训练</div>
<div class="mono" style="margin-top:8px">本研究探讨了在无法获取目标样本的情况下，训练神经随机微分方程（即扩散模型）以从玻尔兹曼分布采样的方法。现有训练方法通过可微分模拟或离策略强化学习，强制生成过程与加噪过程的时间可逆性。我们证明了在无穷小离散化步长极限下，不同目标函数族之间的等价关系，从而将熵正则化强化学习方法（GFlowNets）与连续时间对象（偏微分方程及路径空间测度）联系起来。进一步研究表明，训练过程中采用合适的粗粒度时间离散化策略，可显著提升样本效率并实现时间局部化目标函数，在标准采样基准测试中以更低计算成本达到具有竞争力的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of training neural stochastic differential equations (diffusion models) to sample from Boltzmann distributions without target data, this paper establishes theoretical equivalences between discrete-time policy training methods and continuous-time diffusion samplers. The method demonstrates that entropic reinforcement learning approaches, such as GFlowNets, converge to continuous-time partial differential equations and path measures in the limit of infinitesimal time steps. Experimentally, the authors show that employing a coarse time discretization during training significantly improves sample efficiency and enables the use of time-local objectives, leading to competitive performance on standard sampling benchmarks with reduced computational costs.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在无法获取目标样本的情况下，训练神经随机微分方程（扩散模型）从玻尔兹曼分布中采样的难题，建立了离散时间策略训练方法与连续时间扩散采样器之间的理论等价性。研究方法证明了在时间步长趋于无穷小的极限下，熵强化学习方法（如GFlowNets）会收敛于连续时间的偏微分方程和路径测度。实验结果表明，在训练过程中采用粗粒度的时间离散化能显著提高样本效率，并允许使用时间局部目标，从而在标准采样基准上以较低的计算成本实现了有竞争力的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering</div>
<div class="meta-line">Authors: Chak Tou Leong, Dingwei Chen, Heming Xia, Qingyu Yin, Sunbowen Lee, Jian Wang, Wenjie Li</div>
<div class="meta-line">First: 2026-01-20T09:07:01+00:00 · Latest: 2026-01-20T09:07:01+00:00</div>
<div class="meta-line">Comments: Working in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13752v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13752v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model&#x27;s self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model&#x27;s reasoning belief effectively shapes its actual behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>寻找RELIEF：通过信念工程塑造推理行为而无需推理监督</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）在复杂问题解决方面取得了显著成功，但常存在计算冗余或推理不忠实的问题。当前塑造LRM行为的方法通常依赖于强化学习或使用黄金标准推理轨迹进行微调，这种范式计算成本高昂且难以扩展。本文揭示LRMs具有潜在的“推理信念”，可内部追踪其自身推理特征，并能通过简单的逻辑探针捕获。基于此发现，我们提出推理信念工程（RELIEF）——一种通过将模型自我认知与目标信念蓝图对齐来塑造LRM行为的简洁有效框架。关键的是，RELIEF完全绕过了对推理轨迹监督的需求，通过微调合成的、自我反思的问答对（这些问答对确认目标信念）来内化期望特征。在效率与忠实性任务上的大量实验表明，RELIEF在降低训练成本的同时，达到或超越了行为监督与基于偏好的基线方法。进一步分析证实，改变模型的推理信念能有效塑造其实际行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational redundancy and unfaithfulness in large reasoning models (LRMs) by proposing a novel, cost-effective alternative to traditional supervision-heavy methods. The authors introduce Reasoning Belief Engineering (RELIEF), a framework that shapes LRM behavior by first identifying latent reasoning beliefs through logit probing and then aligning these beliefs with a target blueprint via fine-tuning on self-reflective question-answering pairs, eliminating the need for gold-standard reasoning traces. Experimental results on efficiency and faithfulness tasks show that RELIEF matches or surpasses behavior-supervised and preference-based baselines while significantly reducing training costs, confirming that shifting a model&#x27;s self-concept effectively alters its reasoning behavior.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型存在的计算冗余和推理不忠实问题，提出了一种新颖且成本效益高的方法，以替代传统依赖大量监督的方法。作者引入了推理信念工程（RELIEF）框架，该框架通过逻辑探针识别模型潜在的推理信念，然后利用自反思问答对进行微调，使这些信念与目标蓝图对齐，从而完全避免了对标准推理轨迹的监督需求。在效率和忠实性任务上的实验结果表明，RELIEF在匹配或超越基于行为监督和偏好的基线方法的同时，显著降低了训练成本，验证了改变模型的自我概念能有效塑造其实际推理行为。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models</div>
<div class="meta-line">Authors: Jinwu Hu, Dongjin Yang, Langyu Bian, Zhiquan Wen, Yufeng Wang, Yaofo Chen, Bin Xiao, Yuanqing Li, Mingkui Tan</div>
<div class="meta-line">First: 2025-12-17T05:11:58+00:00 · Latest: 2026-01-20T08:55:05+00:00</div>
<div class="meta-line">Comments: under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15089v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15089v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越快与慢：面向大语言模型的认知启发式弹性推理</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在各种语言任务中展现出卓越性能。然而，现有LLM推理策略主要依赖模型自身的快慢模式（如o1思考），难以在不同难度查询间平衡推理效率与准确性。本文提出认知启发式弹性推理（CogER），该框架受人类分层推理机制启发，能动态为每个查询选择最合适的推理策略。具体而言，CogER首先评估输入查询的复杂度，将其归类至预定义的难度层级，每个层级对应定制化处理策略，从而解决查询难度不可观测的挑战。为实现自动策略选择，我们将该过程建模为马尔可夫决策过程，并通过强化学习训练CogER智能体。该智能体由平衡解质量与计算成本的奖励函数引导，确保资源高效推理。此外，针对需要外部工具的查询，我们提出认知工具辅助推理机制，使LLM能在思维链中自主调用外部工具。大量实验表明，CogER优于最先进的测试时扩展方法，在领域内任务的平均精确匹配率相对提升至少13%，在领域外任务相对提升8%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of balancing reasoning efficiency and accuracy in large language models (LLMs), which typically use fixed &#x27;fast&#x27; or &#x27;slow&#x27; modes ill-suited to queries of varying difficulty. The authors propose Cognitive-Inspired Elastic Reasoning (CogER), a framework that dynamically selects tailored reasoning strategies by first assessing query complexity via a trained reinforcement learning agent, modeled as a Markov Decision Process, and incorporating external tools when needed. Experimental results show that CogER outperforms state-of-the-art test-time scaling methods, achieving at least a 13% relative improvement in average exact match on in-domain tasks and an 8% gain on out-of-domain tasks.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在平衡推理效率与准确性方面的挑战，即现有固定“快速”或“慢速”模式难以适应不同难度查询的问题，提出了一种受人类分层推理启发的认知弹性推理框架。该方法通过将策略选择建模为马尔可夫决策过程，并利用强化学习训练智能体来动态评估查询复杂度并选择定制化推理策略，同时在需要时自主调用外部工具。大量实验表明，该框架在领域内任务上的平均精确匹配率相对提升了至少13%，在领域外任务上相对提升了8%，优于当前最先进的测试时扩展方法。</div>
</details>
</div>
<div class="card">
<div class="title">Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study</div>
<div class="meta-line">Authors: Chuang Zhang, Geng Sun, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato</div>
<div class="meta-line">First: 2025-08-01T01:53:58+00:00 · Latest: 2026-01-20T08:53:47+00:00</div>
<div class="meta-line">Comments: This paper has been accepted to IEEE Communications Magazine</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.00256v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.00256v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-altitude wireless networks (LAWNs) have the potential to revolutionize communications by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure communications in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure communications in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models (LLMs) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure communication tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低空无线网络中基于大人工智能模型的安全通信：概念、视角与案例研究</div>
<div class="mono" style="margin-top:8px">低空无线网络（LAWNs）通过支持城市包裹配送、空中巡检和空中出租车等应用，具有革新通信的潜力。然而，与传统无线网络相比，LAWNs因低空作业、频繁移动及依赖非授权频谱而面临独特的安全挑战，使其更易遭受恶意攻击。本文研究了LAWNs中基于大人工智能模型（LAM）的安全通信解决方案。具体而言，我们首先探讨了LAWNs中传统AI方法所加剧的安全风险及其重要局限性，随后介绍了LAM的基本概念，并深入分析了LAM在应对这些挑战中的作用。为展示LAM在LAWNs安全通信中的实际优势，我们提出了一种基于LAM的新型优化框架，该框架利用大语言模型（LLMs）在人工设计表征的基础上生成增强状态特征，并据此设计内在奖励，从而提升安全通信任务中强化学习的性能。通过典型案例研究，仿真结果验证了所提框架的有效性。最后，我们展望了将LAM集成至安全LAWN应用的未来方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the unique security vulnerabilities of low-altitude wireless networks (LAWNs), which arise from their operational environment and mobility, necessitating advanced solutions beyond traditional AI. The proposed method introduces a large AI model (LAM)-enabled framework that utilizes large language models to generate enhanced state features and design intrinsic rewards, thereby optimizing reinforcement learning for secure communication tasks. The main experimental results from a case study demonstrate the framework&#x27;s effectiveness in improving security performance, validating its practical benefit for LAWNs.</div>
<div class="mono" style="margin-top:8px">本文的动机源于低空无线网络因其低空运行、频繁移动等特点面临独特安全挑战，传统人工智能方法存在局限。所提出的方法引入了一个基于大型人工智能模型的优化框架，利用大语言模型生成增强的状态特征并设计内在奖励，从而优化用于安全通信任务的强化学习性能。主要实验结果通过典型案例研究验证了该框架的有效性，表明其能提升低空无线网络的安全通信能力。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Opportunistic Routing in Software-Defined LEO-Terrestrial Systems</div>
<div class="meta-line">Authors: Sivaram Krishnan, Zhouyou Gu, Jihong Park, Sung-Min Oh, Jinho Choi</div>
<div class="meta-line">First: 2026-01-20T07:01:14+00:00 · Latest: 2026-01-20T07:01:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13662v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of large-scale low Earth orbit (LEO) satellite constellations is driving the need for intelligent routing strategies that can effectively deliver data to terrestrial networks under rapidly time-varying topologies and intermittent gateway visibility. Leveraging the global control capabilities of a geostationary (GEO)-resident software-defined networking (SDN) controller, we introduce opportunistic routing, which aims to minimize delivery delay by forwarding packets to any currently available ground gateways rather than fixed destinations. This makes it a promising approach for achieving low-latency and robust data delivery in highly dynamic LEO networks. Specifically, we formulate a constrained stochastic optimization problem and employ a residual reinforcement learning framework to optimize opportunistic routing for reducing transmission delay. Simulation results over multiple days of orbital data demonstrate that our method achieves significant improvements in queue length reduction compared to classical backpressure and other well-known queueing algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>软件定义低轨-地面系统中基于强化学习的机遇路由</div>
<div class="mono" style="margin-top:8px">大规模低地球轨道（LEO）卫星星座的激增，推动了对智能路由策略的需求，以在快速时变的拓扑结构和间歇性网关可见性条件下，有效向地面网络传输数据。利用地球静止轨道（GEO）驻留的软件定义网络（SDN）控制器的全局控制能力，我们引入了机遇路由，其目标是通过将数据包转发至当前任何可用的地面网关而非固定目的地，以最小化传输延迟。这使其成为在高度动态的低轨网络中实现低延迟、鲁棒数据传输的有效方法。具体而言，我们构建了一个约束随机优化问题，并采用残差强化学习框架来优化机遇路由以降低传输延迟。基于多日轨道数据的仿真结果表明，相较于经典背压算法及其他知名排队算法，我们的方法在队列长度缩减方面取得了显著改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to address the challenge of intelligent routing in large-scale low Earth orbit (LEO) satellite constellations, which face rapidly changing topologies and intermittent gateway visibility, by leveraging a geostationary software-defined networking controller for global control. The method introduces opportunistic routing that forwards packets to any available ground gateways to minimize delay, formulating it as a constrained stochastic optimization problem solved via a residual reinforcement learning framework. Experimental results from multi-day orbital simulations show that this approach significantly reduces queue lengths compared to classical backpressure and other queueing algorithms, demonstrating improved low-latency and robust data delivery.</div>
<div class="mono" style="margin-top:8px">本文的动机是针对大规模低地球轨道卫星星座中拓扑快速变化和网关可见性间歇性带来的路由挑战，利用地球静止轨道软件定义网络控制器的全局控制能力，提出机会路由以最小化传输延迟。方法上，通过将数据包转发至任何可用地面网关而非固定目的地，将其建模为约束随机优化问题，并采用残差强化学习框架进行优化。实验结果表明，基于多日轨道数据的仿真显示，该方法相比经典背压算法和其他排队算法，在队列长度减少方面取得了显著改进，实现了更低延迟和更稳健的数据传输。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn&#x27;t</div>
<div class="meta-line">Authors: Quy-Anh Dang, Chris Ngo</div>
<div class="meta-line">First: 2025-03-20T15:13:23+00:00 · Latest: 2026-01-20T06:53:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.16219v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.16219v2">PDF</a> · <a href="https://github.com/knoveleng/open-rs">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at https://github.com/knoveleng/open-rs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习在小规模语言模型推理中的应用：有效方法与局限分析</div>
<div class="mono" style="margin-top:8px">提升大语言模型（LLMs）的推理能力通常依赖海量计算资源和数据集，这在资源受限场景中难以实现。本研究探索了利用强化学习（RL）增强小规模LLMs推理能力的潜力，聚焦于15亿参数的DeepSeek-R1-Distill-Qwen-1.5B模型，并在严格限制下进行实验：使用4张NVIDIA A40 GPU（每张48GB显存）在24小时内完成训练。通过改进分组相对策略优化（GRPO）算法并构建精炼的高质量数学推理数据集，我们开展了三项实验以探究模型行为与性能。结果表明，仅使用7,000个样本和42美元训练成本（基线模型需数千美元），即可实现推理能力的快速提升——例如AMC23准确率从63%升至80%，AIME24达到46.7%并超越o1-preview模型。但延长训练也暴露出优化不稳定和生成长度受限等挑战。这些发现凸显了基于RL的微调对小规模LLMs的有效性，为资源有限环境提供了可扩展的、具备推理能力的LLMs开发基础。我们已开源代码与数据集（https://github.com/knoveleng/open-rs），为相关研究提供技术参考与权衡分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the need to enhance reasoning capabilities in large language models (LLMs) without the prohibitive computational costs typically required, focusing instead on small, resource-constrained models. The method adapts the Group Relative Policy Optimization (GRPO) reinforcement learning algorithm to fine-tune a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, using a compact, high-quality mathematical reasoning dataset and training under strict hardware and time limits (4 GPUs for 24 hours). Experimental results show rapid reasoning improvements, with accuracy on benchmarks like AMC23 increasing from 63% to 80% and AIME24 reaching 46.7%, outperforming larger baseline models at a minimal cost of $42 and only 7,000 samples, though challenges such as optimization instability arose with extended training.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型（LLMs）推理能力提升通常依赖巨大计算资源和数据集的问题，转而探索在资源受限的小型模型上实现高效推理。方法上，采用分组相对策略优化（GRPO）强化学习算法，对15亿参数的DeepSeek-R1-Distill-Qwen-1.5B模型进行微调，使用精心构建的紧凑高质量数学推理数据集，并在严格的硬件和时间约束下（4块GPU训练24小时）进行训练。实验结果表明，推理能力快速提升，例如在AMC23基准上准确率从63%增至80%，AIME24达到46.7%，超越了更大基线模型，且仅消耗42美元成本和7000个样本，但延长训练也带来了优化不稳定等挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Myong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo, Hyondong Oh</div>
<div class="meta-line">First: 2026-01-20T06:46:09+00:00 · Latest: 2026-01-20T06:46:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13657v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13657v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于激光雷达深度强化学习的无人机集群无通信协同导航</div>
<div class="mono" style="margin-top:8px">本文提出一种基于深度强化学习（DRL）的控制器，用于无人机集群在通信中断环境下的协同导航，使其能在复杂多障碍环境中稳定运行。受生物集群中知情个体无需显式通信即可引导群体的启发，我们采用隐式领航-跟随框架。在此范式中，仅领航无人机掌握目标信息，跟随无人机仅通过机载激光雷达感知学习鲁棒策略，无需任何个体间通信或领航者识别。系统利用激光雷达点云聚类和扩展卡尔曼滤波器实现稳定的邻居跟踪，提供不依赖外部定位系统的可靠感知。方法核心是通过GPU加速的Nvidia Isaac Sim训练的DRL控制器，使跟随者仅凭局部感知就能学习复杂涌现行为——平衡集群聚集与避障。这使得集群能隐式跟随领航者，同时鲁棒应对遮挡和视场受限等感知挑战。通过大量仿真和五架无人机集群的实景实验，验证了本方法的鲁棒性与仿真到现实的迁移能力，成功实现了在多样室内外环境中无需通信或外部定位的协同导航。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for robust collective navigation of UAV swarms in communication-denied environments, drawing inspiration from biological swarms where informed individuals guide groups implicitly. The method employs a deep reinforcement learning (DRL) controller within an implicit leader-follower framework, where only the leader knows the goal; followers learn policies using only onboard LiDAR sensing, enhanced by point clustering and an extended Kalman filter for stable neighbor tracking without communication or external positioning. Experimental results from extensive simulations and real-world tests with a five-UAV swarm demonstrate successful collective navigation across diverse indoor and outdoor environments, confirming the robustness and effective sim-to-real transfer of the approach in handling obstacles and perceptual challenges like occlusion.</div>
<div class="mono" style="margin-top:8px">本文的动机源于在通信受限环境中实现无人机集群鲁棒集体导航的需求，其灵感来自生物集群中知情个体隐式引导群体的行为。方法采用基于深度强化学习的控制器，在一个隐式领导者-跟随者框架中，仅领导者知晓目标；跟随者仅利用机载激光雷达感知学习策略，并通过点聚类和扩展卡尔曼滤波实现稳定的邻居跟踪，无需通信或外部定位。在广泛仿真和具有挑战性的真实世界实验中，一个五架无人机的集群成功展示了在多样室内外环境中的集体导航，结果证实了该方法在处理障碍和遮挡等感知挑战方面的鲁棒性及有效的仿真到现实迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning</div>
<div class="meta-line">Authors: Yuchen Jiao, Jiin Woo, Gen Li, Gauri Joshi, Yuejie Chi</div>
<div class="meta-line">First: 2026-01-20T06:21:54+00:00 · Latest: 2026-01-20T06:21:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13642v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13642v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$, where $\|h^{\star}\|_{\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$, with only $\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平均奖励Q学习的样本复杂度：从单智能体到联邦强化学习</div>
<div class="mono" style="margin-top:8px">平均奖励强化学习通过最大化单位时间步长的平均奖励，为长期决策提供了理论框架。尽管Q学习是一种广泛使用的无模型算法，在折扣和有限时域马尔可夫决策过程中已确立样本复杂度，但其在平均奖励设定下的理论保证仍有限。本研究针对弱通信假设下具有有限状态和动作空间的平均奖励MDP，提出一种简单有效的Q学习算法，涵盖单智能体和联邦场景。对于单智能体情况，我们证明采用精心选择参数的Q学习可实现样本复杂度$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$，其中$\|h^{\star}\|_{\mathsf{sp}}$为偏差函数的跨度范数，较先前结果至少提升$\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$倍。在包含$M$个智能体的联邦场景中，我们证明协作可将单智能体样本复杂度降至$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$，且仅需$\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$轮通信。这些成果首次为平均奖励MDP建立了联邦Q学习算法，在样本复杂度和通信复杂度方面均具有可证明的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for theoretical guarantees in average-reward reinforcement learning, which optimizes long-term mean reward but lacks established sample complexity bounds for Q-learning, this paper proposes a Q-learning algorithm for weakly communicating Markov decision processes. The method involves careful parameter selection and extends to a federated setting where multiple agents collaborate. Experimental results demonstrate that in the single-agent case, the algorithm achieves a sample complexity of $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$, improving prior bounds, and in the federated scenario with M agents, per-agent sample complexity reduces to $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$ with only $\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$ communication rounds, establishing the first provably efficient federated Q-learning for average-reward settings.</div>
<div class="mono" style="margin-top:8px">本文的动机在于平均奖励强化学习需要理论保证，该框架通过最大化每步平均奖励进行长期决策，但Q学习算法在此设置下缺乏已知的样本复杂度界限。方法上，针对弱通信马尔可夫决策过程提出了一种Q学习算法，并扩展到多智能体联邦学习场景。主要实验结果表明，在单智能体情况下，该算法实现了$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$的样本复杂度，改进了先前结果；在包含M个智能体的联邦设置中，每个智能体的样本复杂度降至$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$，且仅需$\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$轮通信，这首次为平均奖励MDP建立了具有可证明效率的联邦Q学习算法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Unified View of Large Language Model Post-Training</div>
<div class="meta-line">Authors: Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, Bowen Zhou</div>
<div class="meta-line">First: 2025-09-04T17:40:33+00:00 · Latest: 2026-01-20T05:05:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04419v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04419v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向大语言模型后训练的统一视角</div>
<div class="mono" style="margin-top:8px">现代语言模型后训练存在两类主要数据源：在线数据（模型生成的推演数据）与离线数据（人类或其他模型的示范数据）。这两类数据通常分别被强化学习（RL）和监督微调（SFT）等方法所采用。本文论证了这些方法并非相互矛盾，而是同一优化过程的不同实例。我们推导出统一策略梯度估计器，并通过在不同数据分布假设及多种偏差-方差权衡下计算通用目标的梯度，呈现了广泛后训练方法的计算框架。该梯度估计器由四个可互换组件构成：稳定化掩码、参考策略分母、优势估计和似然梯度。基于理论发现，我们提出混合后训练（HPT）算法，能动态选择不同的训练信号。HPT旨在实现对示范数据的有效利用和稳定探索，同时不牺牲已习得的推理模式。我们通过大量实验与消融研究验证了统一理论框架与HPT的有效性。在六个数学推理基准和两个分布外测试集上，HPT在不同规模与架构的模型中均持续超越强基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the observation that two common post-training data sources for large language models—online model-generated rollouts and offline human demonstrations—are typically handled separately by Reinforcement Learning and Supervised Fine-Tuning, respectively. The authors propose a unified theoretical view, showing these approaches are instances of a single optimization process, and derive a Unified Policy Gradient Estimator composed of four interchangeable parts. Based on this framework, they introduce Hybrid Post-Training (HPT), an algorithm that dynamically selects training signals to balance exploitation of demonstrations and stable exploration. Experimental results across six mathematical reasoning benchmarks and two out-of-distribution suites demonstrate that HPT consistently outperforms strong baselines for models of varying scales and families.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到大型语言模型后训练中两种常见数据源——在线模型生成数据和离线人类演示数据——通常分别由强化学习和监督微调单独处理。作者提出了一个统一的理论视角，表明这些方法是一个单一优化过程的不同实例，并推导出一个由四个可互换部分组成的统一策略梯度估计器。基于此框架，他们提出了混合后训练算法，该算法动态选择训练信号以平衡对演示数据的利用和稳定的探索。在六个数学推理基准和两个分布外测试集上的实验结果表明，该算法在不同规模和系列的模型上均持续优于强基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Continual Knowledge Adaptation for Reinforcement Learning</div>
<div class="meta-line">Authors: Jinwu Hu, Zihao Lian, Zhiquan Wen, Chenghao Li, Guohao Chen, Xutao Wen, Bin Xiao, Mingkui Tan</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-22T07:25:41+00:00 · Latest: 2026-01-20T04:47:18+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19314v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19314v2">PDF</a> · <a href="https://github.com/Fhujinwu/CKA-RL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning enables agents to learn optimal behaviors through interactions with environments. However, real-world environments are typically non-stationary, requiring agents to continuously adapt to new tasks and changing conditions. Although Continual Reinforcement Learning facilitates learning across multiple tasks, existing methods often suffer from catastrophic forgetting and inefficient knowledge utilization. To address these challenges, we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL), which enables the accumulation and effective utilization of historical knowledge. Specifically, we introduce a Continual Knowledge Adaptation strategy, which involves maintaining a task-specific knowledge vector pool and dynamically using historical knowledge to adapt the agent to new tasks. This process mitigates catastrophic forgetting and enables efficient knowledge transfer across tasks by preserving and adapting critical model parameters. Additionally, we propose an Adaptive Knowledge Merging mechanism that combines similar knowledge vectors to address scalability challenges, reducing memory requirements while ensuring the retention of essential knowledge. Experiments on three benchmarks demonstrate that the proposed CKA-RL outperforms state-of-the-art methods, achieving an improvement of 4.20% in overall performance and 8.02% in forward transfer. The source code is available at https://github.com/Fhujinwu/CKA-RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习的持续知识适应方法</div>
<div class="mono" style="margin-top:8px">强化学习使智能体能够通过与环境的交互学习最优行为。然而，现实环境通常是非平稳的，要求智能体持续适应新任务和变化的条件。尽管持续强化学习有助于跨多个任务学习，但现有方法常受灾难性遗忘和知识利用效率低下的困扰。为解决这些挑战，我们提出强化学习的持续知识适应方法（CKA-RL），该方法能够积累并有效利用历史知识。具体而言，我们引入一种持续知识适应策略，包括维护任务特定的知识向量池，并动态利用历史知识使智能体适应新任务。该过程通过保留和调整关键模型参数，缓解了灾难性遗忘，并实现了跨任务的高效知识迁移。此外，我们提出一种自适应知识融合机制，通过合并相似知识向量应对可扩展性挑战，在确保保留核心知识的同时降低内存需求。在三个基准测试上的实验表明，所提出的CKA-RL方法优于现有先进方法，整体性能提升4.20%，前向迁移性能提升8.02%。源代码发布于https://github.com/Fhujinwu/CKA-RL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of catastrophic forgetting and inefficient knowledge transfer in continual reinforcement learning for non-stationary environments. The proposed method, CKA-RL, introduces a Continual Knowledge Adaptation strategy that maintains a pool of task-specific knowledge vectors and dynamically adapts historical knowledge to new tasks, alongside an Adaptive Knowledge Merging mechanism to combine similar vectors for scalability. Experimental results on three benchmarks show that CKA-RL outperforms state-of-the-art methods, improving overall performance by 4.20% and forward transfer by 8.02%.</div>
<div class="mono" style="margin-top:8px">本文针对非平稳环境中持续强化学习存在的灾难性遗忘和知识迁移效率低下问题，提出了CKA-RL方法。该方法采用持续知识适应策略，维护任务特定知识向量池并动态利用历史知识适应新任务，同时引入自适应知识合并机制以合并相似向量来提升可扩展性。在三个基准测试上的实验结果表明，CKA-RL优于现有先进方法，整体性能提升4.20%，前向迁移提升8.02%。</div>
</details>
</div>
<div class="card">
<div class="title">Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement</div>
<div class="meta-line">Authors: Mingyu Xu, Cheng Fang, Keyue Jiang, Yuqian Zheng, Yanghua Xiao, Baojian Zhou, Qifang Zhao, Suhang Zheng, Xiuwen Zhu, Jiyang Tang, Yongchi Zhao, Yijia Luo, Zhiqi Bai, Yuchi Xu, Wenbo Su, Wei Wang, Bing Zhao, Lin Qu, Xiaoxiao Xu</div>
<div class="meta-line">First: 2026-01-04T15:23:18+00:00 · Latest: 2026-01-20T04:18:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01562v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01562v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Logics-STEM：通过失败驱动的后训练与文档知识增强赋能大语言模型推理</div>
<div class="mono" style="margin-top:8px">我们提出Logics-STEM，这是一种基于Logics-STEM-SFT-Dataset微调的最先进推理模型。该数据集规模达1000万，质量高且多样，是最大规模的开源长链思维语料库之一。Logics-STEM专注于科学、技术、工程和数学（STEM）领域的推理任务，在STEM相关基准测试中表现卓越，平均比同等80亿参数规模的次优模型提升4.68%。我们将此提升归因于数据与算法协同设计引擎，两者联合优化以拟合推理背后的黄金标准分布。数据方面，Logics-STEM-SFT-Dataset通过精心设计的五阶段数据策管引擎构建，包括标注、去重、净化、蒸馏和分层采样，以确保质量、多样性和可扩展性。算法方面，我们的失败驱动后训练框架在监督微调阶段，针对模型失败区域进行定向知识检索和数据合成，有效指导第二阶段监督微调或强化学习，以更好地拟合目标分布。Logics-STEM卓越的实证性能揭示了大规模开源数据与精心设计合成数据结合的巨大潜力，强调了数据算法协同设计在通过后训练提升推理能力中的关键作用。我们公开提供Logics-STEM模型（80亿和320亿参数版本）及Logics-STEM-SFT-Dataset（1000万和降采样220万版本），以支持开源社区的未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Logics-STEM, a reasoning model designed to enhance large language model (LLM) performance in STEM domains by addressing limitations in existing reasoning capabilities. The method employs a data-algorithm co-design engine, constructing a high-quality 10M-scale dataset through a multi-stage curation process and implementing a failure-driven post-training framework that synthesizes targeted data around model errors to refine supervised fine-tuning or reinforcement learning. Experimental results show that Logics-STEM achieves state-of-the-art performance on STEM benchmarks, with an average improvement of 4.68% over the next-best 8B-scale model, demonstrating the effectiveness of combining large-scale open-source data with algorithmically generated synthetic data for reasoning enhancement.</div>
<div class="mono" style="margin-top:8px">本文提出了Logics-STEM模型，旨在通过解决现有推理能力的不足，提升大语言模型在科学、技术、工程和数学（STEM）领域的表现。该方法采用数据与算法协同设计引擎，通过多阶段精心构建了一个高质量、规模达1000万的数据集，并实施了一种失败驱动的后训练框架，该框架围绕模型错误区域合成针对性数据，以优化监督微调或强化学习过程。实验结果表明，Logics-STEM在STEM相关基准测试中取得了领先性能，相比次优的80亿参数模型平均提升4.68%，验证了将大规模开源数据与算法生成的合成数据相结合以增强推理能力的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions</div>
<div class="meta-line">Authors: Xiaorui Wu, Fei Li, Xiaofeng Mao, Xin Zhang, Li Zheng, Yuxiang Peng, Chong Teng, Donghong Ji, Zhuang Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-29T14:26:46+00:00 · Latest: 2026-01-20T03:57:59+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23473v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.23473v3">PDF</a> · <a href="https://github.com/FishT0ucher/EVOREFUSE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 85.34% higher average refusal triggering rate across 9 LLMs without a safety-prior system prompt, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. With supervised fine-tuning on EVOREFUSE-ALIGN, LLAMA3.1-8B-INSTRUCT achieves up to 29.85% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context. Our code and datasets are available at https://github.com/FishT0ucher/EVOREFUSE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EVOREFUSE：基于进化提示优化的LLM过度拒绝伪恶意指令评估与缓解方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）常过度拒绝伪恶意指令：这些语义无害的查询因保守的安全对齐策略触发不必要的拒绝，严重影响用户体验。收集此类指令对评估和缓解过度拒绝至关重要，但现有指令构建方法（如人工创建或指令改写）缺乏可扩展性或难以生成足够多样且有效的拒绝诱导提示。为此，我们提出EVOREFUSE——一种通过进化算法生成多样化伪恶意指令的提示优化方法，能持续引发LLMs的明确拒绝。该方法通过变异策略和重组在指令空间进行多方向探索，迭代优化种子指令以最大化LLM拒绝概率的证据下界。基于EVOREFUSE，我们构建了两个新数据集：1）EVOREFUSE-TEST基准集（582条指令），在9个LLMs上实现比次优基准高85.34%的平均拒绝触发率（无安全前置提示时）、34.86%的词汇多样性提升及40.03%的LLM响应置信度改进；2）EVOREFUSE-ALIGN对齐集（3000条带响应指令），支持监督与偏好对齐训练。使用EVOREFUSE-ALIGN进行监督微调后，LLAMA3.1-8B-INSTRUCT的过度拒绝率比次优对齐数据集训练的模型降低29.85%，且不影响安全性。EVOREFUSE-TEST分析表明，模型过度关注敏感关键词而忽略整体语境是触发过度拒绝的主因。代码与数据集已开源：https://github.com/FishT0ucher/EVOREFUSE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of large language models (LLMs) over-refusing to respond to pseudo-malicious instructions—harmless queries that trigger unnecessary safety refusals, degrading user experience. To overcome the limitations of existing instruction curation methods, which lack scalability or diversity, the authors introduce EVOREFUSE, an evolutionary algorithm that optimizes prompts by exploring diverse mutation and recombination strategies to maximize the evidence lower bound on LLM refusal probability. Experimental results show that EVOREFUSE generates a benchmark dataset (EVOREFUSE-TEST) with 85.34% higher average refusal triggering rate and 34.86% greater lexical diversity across nine LLMs compared to the next-best benchmark, and supervised fine-tuning on its alignment dataset (EVOREFUSE-ALIGN) reduces over-refusals by up to 29.85% in LLAMA3.1-8B-INSTRUCT without compromising safety.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型对伪恶意指令过度拒绝的问题展开研究，这些指令本质无害但会触发不必要的安全拒绝，损害用户体验。为解决现有指令收集方法在可扩展性或多样性上的不足，作者提出了EVOREFUSE，这是一种基于进化算法的提示优化方法，通过突变和重组策略探索多样化的指令空间，以最大化模型拒绝概率的证据下界。实验结果表明，EVOREFUSE生成的基准数据集（EVOREFUSE-TEST）在九个大语言模型上的平均拒绝触发率比次优基准高85.34%，词汇多样性提升34.86%，且在其对齐数据集（EVOREFUSE-ALIGN）上进行监督微调后，LLAMA3.1-8B-INSTRUCT模型的过度拒绝减少了29.85%，同时保持了安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Behavior Knowledge Merge in Reinforced Agentic Models</div>
<div class="meta-line">Authors: Xiangchi Yuan, Dachuan Shi, Chunhui Zhang, Zheyuan Liu, Shenglong Yao, Soroush Vosoughi, Wenke Lee</div>
<div class="meta-line">First: 2026-01-20T03:56:53+00:00 · Latest: 2026-01-20T03:56:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13572v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13572v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL&#x27;s non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化智能体模型中的行为知识融合</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在后训练阶段至关重要，尤其对于需要专业推理行为的智能体模型。在此背景下，模型融合提供了一种实用机制，可将来自不同任务的多个RL训练智能体整合为单一通用模型。然而，现有融合方法专为监督微调（SFT）设计，在保持RL训练智能体模型的任务特定能力方面存在不足。其根源在于RL与SFT的任务向量失配：策略RL产生的任务向量具有高度稀疏性和异质性，而SFT式融合隐式假设任务向量具有稠密性和全局可比性。当在此失配情况下应用标准全局平均法时，RL编码关键任务特定行为的非重叠任务向量会被削弱，参数更新亦被稀释。为解决此问题，我们提出强化智能体融合（RAM）——专为RL训练智能体模型设计的分布感知融合框架。RAM解耦共享参数更新与任务特定独有参数更新，对共享组件进行平均处理，同时选择性保留并重新缩放独有组件以抵消参数更新稀释。跨多智能体领域与模型架构的实验表明，RAM不仅超越现有融合基线，更能释放智能体间的协同潜力，实现优于各领域专用智能体的性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need to effectively merge multiple reinforcement learning (RL)-trained agentic models into a single generalist model, as existing merging methods designed for supervised fine-tuning are suboptimal due to a fundamental mismatch: RL induces sparse, heterogeneous task vectors, while standard merging assumes dense, comparable ones, leading to diluted parameter updates and loss of critical behaviors. To address this, the authors propose Reinforced Agent Merging (RAM), a distribution-aware framework that disentangles shared and task-specific parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract dilution. Experimental results across various agent domains and model architectures show that RAM not only outperforms existing merging baselines but also unlocks synergistic potential, achieving performance superior to that of specialized agents in their respective domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要将多个经过强化学习训练的智能体模型有效合并为一个通用模型，因为现有的基于监督微调设计的合并方法存在根本性不匹配：强化学习产生稀疏、异构的任务向量，而标准合并假设向量密集且可比，导致参数更新被稀释并丢失关键行为。为此，作者提出了强化智能体合并（RAM），这是一个分布感知的框架，能解耦共享和任务特定的参数更新，对共享部分进行平均，同时选择性地保留并重新缩放独特部分以抵消稀释效应。在多个智能体领域和模型架构上的实验结果表明，RAM不仅优于现有合并基线，还能释放智能体间的协同潜力，在各自领域实现超越专用智能体的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework</div>
<div class="meta-line">Authors: Yanheng Li, Zhichen Pu, Lijiang Yang, Zehao Zhou, Yi Qin Gao</div>
<div class="meta-line">First: 2026-01-20T03:41:02+00:00 · Latest: 2026-01-20T03:41:02+00:00</div>
<div class="meta-line">Comments: Total 43 pages: 32 pages Main Text + 11 pages SI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13564v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13564v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据与物理双驱动生成框架下的多目标荧光分子设计</div>
<div class="mono" style="margin-top:8px">设计具有定制光学与理化性质的荧光小分子，需在广阔且未充分探索的化学空间中导航，同时满足多目标与约束条件。传统的生成-评分-筛选方法在实际设计规范下因搜索效率低、机器学习预测泛化性不可靠及量子化学计算成本高昂而难以实用。本文提出LUMOS，一种数据与物理双驱动的荧光分子逆向设计框架。LUMOS将生成器与预测器耦合于共享潜在表示中，实现从规格到分子的直接设计与高效探索。此外，LUMOS结合神经网络与快速含时密度泛函理论（TD-DFT）计算流程，构建了一套在速度、精度和泛化性间权衡的互补预测器，确保跨场景的可靠性质预测。最后，LUMOS采用与多目标进化算法集成的性质引导扩散模型，实现多目标与约束下的从头设计与分子优化。在全面基准测试中，LUMOS在荧光性质预测的精度、泛化性与物理合理性上均优于基线模型，并在多目标骨架与片段级分子优化中展现卓越性能。通过TD-DFT与分子动力学（MD）模拟的进一步验证表明，LUMOS能生成满足多种目标规格的有效荧光团。总体而言，这些成果确立了LUMOS作为通用荧光团逆向设计的数据-物理双驱动框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenge of efficiently designing fluorescent small molecules with specific optical and physicochemical properties within a vast chemical space, where conventional methods suffer from low search efficiency, unreliable machine-learning predictions, and high computational costs. The method introduces LUMOS, a data-and-physics dual-driven generative framework that couples a generator and predictor in a shared latent space, integrates neural networks with fast time-dependent density functional theory (TD-DFT) for complementary property prediction, and employs a property-guided diffusion model with multi-objective evolutionary algorithms for molecular design. Experimental results show that LUMOS outperforms baseline models in accuracy, generalizability, and physical plausibility for fluorescence prediction, excels in multi-objective molecular optimization at scaffold and fragment levels, and generates valid fluorophores meeting target specifications as validated by TD-DFT and molecular dynamics simulations.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于高效设计具有特定光学和物理化学性质的荧光小分子面临巨大挑战，因为传统方法搜索效率低、机器学习预测不可靠且计算成本高。方法上提出了LUMOS，这是一个数据与物理双驱动的生成框架，它将生成器和预测器耦合在共享潜在空间中，结合神经网络与快速含时密度泛函理论（TD-DFT）进行互补性质预测，并采用属性引导的扩散模型与多目标进化算法进行分子设计。实验结果表明，LUMOS在荧光预测的准确性、泛化性和物理合理性上优于基线模型，在支架和片段级别的多目标分子优化中表现优异，并通过TD-DFT和分子动力学模拟验证了其能生成符合目标规格的有效荧光团。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Reinforcement Learning from Neural Feedback: Mapping fNIRS Signals to Agent Performance</div>
<div class="meta-line">Authors: Julia Santaniello, Matthew Russell, Benson Jiang, Donatello Sassaroli, Robert Jacob, Jivko SInapov</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-17T00:21:46+00:00 · Latest: 2026-01-20T02:54:22+00:00</div>
<div class="meta-line">Comments: Accepted to the Association for the Advancement of Artificial Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12844v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12844v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating user feedback into the agent&#x27;s training process. This paper introduces a framework that guides agent training through implicit neural signals, with a focus on the neural classification problem. Our work presents and releases a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train multiple classifiers to predict varying levels of agent performance (optimal, suboptimal, or worst-case) from windows of preprocessed fNIRS features, achieving an average F1 score of 67% for binary and 46% for multi-class classification across conditions and domains. We also train multiple regressors to predict the degree of deviation between an agent&#x27;s chosen action and a set of near-optimal policy actions, providing a continuous measure of performance. Finally, we evaluate cross-subject generalization and show that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our results demonstrate that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future Reinforcement Learning from Neural Feedback (RLNF) systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向基于神经反馈的强化学习：将fNIRS信号映射至智能体性能</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）通过将用户反馈整合至智能体训练过程，使其行为与人类偏好保持一致。本文提出一种通过隐式神经信号指导智能体训练的框架，重点关注神经信号分类问题。我们构建并发布了一个新颖的功能性近红外光谱（fNIRS）数据集，采集自25名人类参与者在三个领域（抓放机器人、月球着陆器、Flappy Bird）的神经信号记录。我们训练了多个分类器，通过预处理后的fNIRS特征窗口预测智能体性能的不同水平（最优、次优或最差），在跨条件与跨领域测试中，二分类与多分类的平均F1分数分别达到67%与46%。同时训练了多个回归器，用于预测智能体所选动作与近似最优策略动作之间的偏差程度，提供连续的性能度量。最后，我们评估了跨被试泛化能力，结果表明使用少量被试特定数据对预训练模型进行微调，可使二分类与多分类模型的平均F1分数分别提升17%与41%。我们的研究证明，将隐式fNIRS信号映射至智能体性能具有可行性且可优化，为未来基于神经反馈的强化学习（RLNF）系统奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the goal of advancing Reinforcement Learning from Human Feedback (RLHF) by using implicit neural signals instead of explicit human input to guide agent training. The method involves collecting a novel functional near-infrared spectroscopy (fNIRS) dataset from 25 participants across three task domains and training classifiers and regressors to map neural signals to agent performance levels. The main experimental results show that classifiers achieved average F1 scores of 67% for binary and 46% for multi-class performance prediction, while fine-tuning with subject-specific data improved cross-subject generalization, increasing F1 scores by 17% and 41% respectively, demonstrating the feasibility of mapping fNIRS signals to agent performance for future RLNF systems.</div>
<div class="mono" style="margin-top:8px">本文的动机是通过使用隐式神经信号而非显式人类输入来指导智能体训练，以推进基于人类反馈的强化学习（RLHF）。方法包括从25名参与者在三个任务领域中收集新型功能性近红外光谱（fNIRS）数据集，并训练分类器和回归器将神经信号映射到智能体性能水平。主要实验结果表明，分类器在二元和多类性能预测中分别实现了67%和46%的平均F1分数，而使用特定受试者数据进行微调改善了跨受试者泛化能力，使F1分数分别提高了17%和41%，这证明了将fNIRS信号映射到智能体性能的可行性，为未来基于神经反馈的强化学习（RLNF）系统奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models</div>
<div class="meta-line">Authors: Changshuo Zhang</div>
<div class="meta-line">First: 2026-01-20T02:32:39+00:00 · Latest: 2026-01-20T02:32:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13533v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13533v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model&#x27;s decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the &quot;reason first, recommend later&quot; paradigm to achieve &quot;reasoning while recommending&quot;, specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model&#x27;s effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推荐中的推理：生成式重排序模型中的熵引导隐式推理机制</div>
<div class="mono" style="margin-top:8px">强化学习因其探索-利用能力在生成式重排序场景中至关重要，但现有生成方法大多难以适应列表生成过程中模型动态熵变带来的难度变化，导致精准捕捉复杂偏好面临挑战。鉴于语言模型通过融合推理能力已取得显著突破，本研究借鉴该思路引入隐式推理机制，实验验证表明该机制能有效降低模型决策过程的熵值。基于此，我们提出熵引导隐式推理推荐模型，其具备三大核心优势：首先，摒弃“先推理后推荐”范式，实现“边推荐边推理”，通过生成过程中的实时推理专门应对列表生成的高难度特性；其次，采用上下文感知推理令牌与动态温度调节实现熵引导的变长推理，既拓展推理探索广度，又提升推荐利用精度，达成更精准的自适应探索-利用平衡；再次，模型采用轻量级集成设计，无需复杂独立模块或后处理，可便捷适配现有模型。在两个真实数据集上的实验结果验证了模型有效性，其突出优势在于兼容现有生成式重排序模型以提升性能。进一步分析也证明了其实践部署价值与研究潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of generative re-ranking models struggling to adapt to dynamic entropy changes during list generation, which hinders accurate preference capture. The authors propose the Entropy-Guided Latent Reasoning (EGLR) model, which integrates real-time latent reasoning into the recommendation process using context-aware reasoning tokens and dynamic temperature adjustment to balance exploration and exploitation. Experimental results on two real-world datasets show that EGLR effectively reduces decision entropy, enhances performance, and is lightweight enough to be easily integrated into existing generative re-ranking models.</div>
<div class="mono" style="margin-top:8px">本文针对生成式重排序模型在列表生成过程中难以适应动态熵变化、从而影响偏好准确捕捉的问题，提出了熵引导潜在推理（EGLR）模型。该方法通过上下文感知推理令牌和动态温度调整，在推荐过程中实现实时潜在推理，以优化探索与利用的平衡。在两个真实数据集上的实验结果表明，EGLR能有效降低决策熵、提升性能，且设计轻量，易于与现有生成式重排序模型兼容集成。</div>
</details>
</div>
<div class="card">
<div class="title">Optimistic Gradient Learning with Hessian Corrections for High-Dimensional Black-Box Optimization</div>
<div class="meta-line">Authors: Yedidya Kfir, Elad Sarafian, Sarit Kraus, Yoram Louzoun</div>
<div class="meta-line">Venue: AAAI2026</div>
<div class="meta-line">First: 2025-02-07T11:03:50+00:00 · Latest: 2026-01-19T20:14:58+00:00</div>
<div class="meta-line">Comments: We develop a black-box optimization algorithm that learns gradients with neural models and can be applied to solve non-convex high dimensional real-world problems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04829v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.04829v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Black-box algorithms are designed to optimize functions without relying on their underlying analytical structure or gradient information, making them essential when gradients are inaccessible or difficult to compute. Traditional methods for solving black-box optimization (BBO) problems predominantly rely on non-parametric models and struggle to scale to large input spaces. Conversely, parametric methods that model the function with neural estimators and obtain gradient signals via backpropagation may suffer from significant gradient errors. A recent alternative, Explicit Gradient Learning (EGL), which directly learns the gradient using a first-order Taylor approximation, has demonstrated superior performance over both parametric and non-parametric methods. In this work, we propose two novel gradient learning variants to address the robustness challenges posed by high-dimensional, complex, and highly non-linear problems. Optimistic Gradient Learning (OGL) introduces a bias toward lower regions in the function landscape, while Higher-order Gradient Learning (HGL) incorporates second-order Taylor corrections to improve gradient accuracy. We combine these approaches into the unified OHGL algorithm, achieving state-of-the-art (SOTA) performance on the synthetic COCO suite. Additionally, we demonstrate OHGLs applicability to high-dimensional real-world machine learning (ML) tasks such as adversarial training and code generation. Our results highlight OHGLs ability to generate stronger candidates, offering a valuable tool for ML researchers and practitioners tackling high-dimensional, non-linear optimization challenges</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高维黑盒优化的Hessian修正乐观梯度学习</div>
<div class="mono" style="margin-top:8px">黑盒算法旨在不依赖函数解析结构或梯度信息的情况下优化函数，在梯度难以获取或计算时至关重要。传统黑盒优化方法主要依赖非参数模型，难以扩展至高维输入空间。而通过神经网络估计器建模并借助反向传播获取梯度信号的参数化方法，则可能受显著梯度误差影响。近期提出的显式梯度学习方法通过一阶泰勒近似直接学习梯度，已展现出超越参数化与非参数化方法的性能。本研究提出两种新型梯度学习变体以应对高维、复杂、高度非线性问题带来的鲁棒性挑战：乐观梯度学习引入对函数景观低值区域的偏置，高阶梯度学习则融入二阶泰勒修正以提升梯度精度。我们将这两种方法整合为统一的OHGL算法，在合成测试集COCO上实现了最先进性能。此外，我们验证了OHGL在高维现实机器学习任务（如对抗训练与代码生成）中的适用性。实验结果表明OHGL能生成更优候选解，为应对高维非线性优化挑战的机器学习研究者与实践者提供了有力工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of existing black-box optimization methods, which either struggle with scalability in high dimensions or suffer from gradient inaccuracies. The method introduces two novel gradient learning variants: Optimistic Gradient Learning (OGL), which biases the search toward lower function regions, and Higher-order Gradient Learning (HGL), which incorporates second-order Taylor corrections for improved gradient accuracy; these are unified into the OHGL algorithm. Experimental results show that OHGL achieves state-of-the-art performance on the synthetic COCO benchmark and demonstrates effectiveness in high-dimensional real-world tasks like adversarial training and code generation, producing stronger candidate solutions.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有黑盒优化方法存在局限性，要么难以扩展到高维空间，要么梯度估计不准确。方法上提出了两种新颖的梯度学习变体：乐观梯度学习（OGL）将搜索偏向函数较低区域，高阶梯度学习（HGL）引入二阶泰勒校正以提高梯度精度；两者结合为统一的OHGL算法。实验结果表明，OHGL在合成COCO基准测试中取得了最先进的性能，并在对抗训练和代码生成等高维实际机器学习任务中表现出有效性，能生成更优的候选解。</div>
</details>
</div>
<div class="card">
<div class="title">Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning</div>
<div class="meta-line">Authors: Duygu Nur Yaldiz, Evangelia Spiliopoulou, Zheng Qi, Siddharth Varia, Srikanth Doss, Nikolaos Pappas</div>
<div class="meta-line">First: 2026-01-19T18:31:31+00:00 · Latest: 2026-01-19T18:31:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13284v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13284v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR&#x27;s failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR&#x27;s accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过校准感知强化学习平衡决策型大语言模型的分类性能与校准性能</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正日益应用于决策任务，其中不仅需要准确性，可靠的置信度估计也至关重要。良好校准的置信度能使下游系统判断何时信任模型、何时启用备用机制。本研究系统性地分析了两种主流微调范式中的校准表现：监督微调（SFT）与可验证奖励的强化学习（RLVR）。我们发现，RLVR虽能提升任务性能，却会产生极度过度自信的模型；而SFT即使在分布偏移下仍能保持显著更优的校准效果，尽管性能提升幅度较小。通过针对性实验，我们诊断出RLVR的缺陷：决策标记在推理轨迹中仅作为决策提取步骤，不携带置信度信息，导致强化学习无法呈现校准后的备选方案。基于此洞见，我们提出一种校准感知的强化学习框架，直接调整决策标记的概率分布。该方法在保持RLVR准确率水平的同时有效缓解过度自信问题，将预期校准误差（ECE）分数降低达9个百分点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of ensuring large language models (LLMs) provide both accurate and well-calibrated confidence estimates when used for decision-making, as reliable confidence is crucial for determining when to trust model outputs. The authors systematically compare two fine-tuning paradigms—supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR)—finding that RLVR boosts task performance but leads to severe overconfidence, while SFT yields better calibration albeit with lower accuracy. They diagnose that in RLVR, decision tokens in reasoning traces lack confidence information, preventing calibrated alternatives from emerging. To resolve this, they propose a calibration-aware reinforcement learning method that directly adjusts decision-token probabilities, which maintains RLVR&#x27;s accuracy while significantly reducing overconfidence, as evidenced by up to 9-point reductions in expected calibration error (ECE) scores.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型在决策任务中需同时保证准确性和校准置信度的问题展开，因为可靠的置信度对于判断何时信任模型输出至关重要。作者系统比较了两种微调范式——监督微调和基于可验证奖励的强化学习，发现强化学习能提升任务性能但会导致严重过度自信，而监督微调虽准确性增益较小却具有更好的校准性，即使在分布偏移下也是如此。他们诊断出强化学习中推理轨迹的决策标记缺乏置信度信息，阻碍了校准替代方案的出现。为此，提出了一种校准感知的强化学习方法，直接调整决策标记的概率，该方法在保持强化学习准确性的同时显著减轻了过度自信，预期校准误差分数最多降低了9分。</div>
</details>
</div>
<div class="card">
<div class="title">CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning</div>
<div class="meta-line">Authors: Eric Onyame, Akash Ghosh, Subhadip Baidya, Sriparna Saha, Xiuying Chen, Chirag Agarwal</div>
<div class="meta-line">First: 2026-01-19T17:51:00+00:00 · Latest: 2026-01-19T17:51:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13262v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13262v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cure-med.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CURE-Med：基于课程学习的多语言医学推理强化学习框架</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在单语言的数学与常识推理任务中表现优异，其在多语言医学推理应用中的可靠性仍显不足，阻碍了其在多语言医疗场景中的部署。为此，我们首先构建了CUREMED-BENCH——一个高质量的多语言医学推理数据集，包含具有单一可验证答案的开放式推理查询，涵盖十三种语言（包括阿姆哈拉语、约鲁巴语、斯瓦希里语等资源稀缺语言）。基于此数据集，我们提出CURE-MED框架，该框架采用课程化强化学习策略，融合语码转换感知的监督微调与组相对策略优化技术，以协同提升逻辑正确性与语言稳定性。在十三种语言的测试中，该方法持续超越现有基线模型并展现良好的扩展性：在70亿参数规模下实现85.21%的语言一致性与54.35%的逻辑正确率，在320亿参数规模下达到94.96%的语言一致性与70.04%的逻辑正确率。这些成果为构建可靠、公平的多语言医学推理大语言模型提供了支持。代码与数据集已开源：https://cure-med.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the unreliability of large language models in multilingual medical reasoning, which limits their use in diverse healthcare settings. The method introduces CUREMED-BENCH, a multilingual medical reasoning dataset spanning thirteen languages, and proposes CURE-MED, a curriculum-informed reinforcement learning framework that combines code-switching-aware supervised fine-tuning with Group Relative Policy Optimization to enhance logical correctness and language stability. Experimental results show that the approach outperforms baselines across languages, achieving up to 94.96% language consistency and 70.04% logical correctness at 32B parameters, supporting more reliable and equitable multilingual medical reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型在多语言医疗推理中的不可靠性问题，这限制了其在多样化医疗环境中的应用。方法上，研究首先引入了CUREMED-BENCH，一个涵盖十三种语言的多语言医疗推理数据集，并提出了CURE-MED，一种课程式强化学习框架，结合了代码切换感知的监督微调和组相对策略优化，以提升逻辑正确性和语言稳定性。实验结果表明，该方法在多种语言中均优于基线模型，在320亿参数规模下实现了94.96%的语言一致性和70.04%的逻辑正确性，从而支持更可靠、公平的多语言医疗推理。</div>
</details>
</div>
<div class="card">
<div class="title">Training instability in deep learning follows low-dimensional dynamical principles</div>
<div class="meta-line">Authors: Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang</div>
<div class="meta-line">First: 2026-01-19T15:37:45+00:00 · Latest: 2026-01-19T15:37:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability.
  We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. We operationalize this perspective through controlled perturbation auditing of training trajectories, probing how learning dynamics respond to structured disturbances without modifying learning algorithms.
  Across reinforcement learning and large language model training, we identify three recurring regularities: high final performance is frequently decoupled from training stability; controlled stochasticity consistently buffers learning dynamics across paradigms; and deviations in low-dimensional latent meta-states systematically precede observable performance collapse. Together, these findings establish training stability as a measurable and comparable dynamical property of learning systems, providing a descriptive foundation for studying learning dynamics beyond final performance outcomes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度学习训练不稳定性遵循低维动力学原理</div>
<div class="mono" style="margin-top:8px">深度学习系统虽取得卓越的实证性能，但其训练过程本身的稳定性仍鲜为人知。训练表现为高维动力系统，其中对优化过程、数据、参数或学习信号的微小扰动可能引发突发且不可逆的崩溃，损害可复现性与可扩展性。我们提出统一动力学视角，将训练稳定性定义为学习系统的内在属性，并沿四个相互作用的维度进行组织：优化稳定性、环境/数据稳定性、参数稳定性及学习信号稳定性。我们通过对训练轨迹实施受控扰动审计来操作化这一视角，探究学习动力学如何响应结构化干扰而无需修改学习算法。在强化学习与大语言模型训练中，我们识别出三种反复出现的规律：最终高性能常与训练稳定性脱钩；受控随机性持续缓冲跨范式的学习动力学；低维潜在元状态的系统性偏差先于可观测的性能崩溃。这些发现共同确立了训练稳定性作为学习系统可测量、可比较的动力学属性，为超越最终性能结果研究学习动力学提供了描述性基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the poor understanding of training stability in deep learning despite its empirical success, this paper proposes a unified dynamical perspective that frames stability across four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. The method involves a controlled perturbation audit of training trajectories to probe how learning dynamics respond to structured disturbances without altering the underlying algorithms. Key experimental results from reinforcement learning and large language model training reveal three regularities: high final performance is often decoupled from training stability, controlled stochasticity consistently buffers learning dynamics, and deviations in low-dimensional latent meta-states systematically precede observable performance collapse, establishing training stability as a measurable dynamical property.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，尽管深度学习取得了显著的实证性能，但其训练过程的稳定性仍缺乏深入理解。为此，研究提出了一种统一的动力学视角，将训练稳定性定义为学习系统沿优化、环境/数据、参数和学习信号四个相互作用维度的内在属性。方法上，通过对训练轨迹进行受控扰动审计，在不修改学习算法的情况下探究学习动力学对结构化干扰的响应。在强化学习和大型语言模型训练中的主要实验结果表明三个反复出现的规律：最终高性能常与训练稳定性脱钩；受控随机性持续缓冲学习动力学；低维潜在元状态的偏差系统性地先于可观察的性能崩溃，从而将训练稳定性确立为一种可测量的动力学特性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective</div>
<div class="meta-line">Authors: Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, Jiawei Chen</div>
<div class="meta-line">First: 2025-10-11T10:17:38+00:00 · Latest: 2026-01-19T15:00:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10150v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10150v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM reasoning, its training process carries a critical risk: entropy collapse. This phenomenon is a rapid decrease in policy entropy, which severely limits exploration and diminishes learning effectiveness. Recent methods attempt to mitigate this collapse via heuristic entropy interventions, yet the underlying mechanisms governing entropy remain unclear. In this work, we conduct a theoretical and quantitative analysis of GRPO&#x27;s entropy dynamics, revealing that token-level entropy change in each update step is jointly governed by four key factors: clipping strategy, advantage, token probability, and token entropy. These findings not only explain the mechanisms of existing methods, but also reveal their limitations: they rely on heuristic adjustments to only one or two factors, leaving other relevant factors unconsidered and reducing their effectiveness. This motivates us to propose a new method, STEER, which adaptively reweights tokens based on their estimated entropy change to regulate entropy in a principled manner. Experiments on both math and coding benchmarks demonstrate that STEER effectively mitigates entropy collapse and consistently outperforms state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLVR中熵干预的再思考：基于熵变化视角</div>
<div class="mono" style="margin-top:8px">尽管可验证奖励强化学习（RLVR）能提升大语言模型的推理能力，但其训练过程存在一个关键风险：熵崩溃。这一现象表现为策略熵的急剧下降，严重限制了探索空间并削弱学习效果。现有方法尝试通过启发式熵干预缓解崩溃，但熵变化的内在机制仍不明确。本研究对GRPO的熵动态进行了理论与量化分析，发现每个更新步骤中词元级熵变化由四个关键因素共同决定：裁剪策略、优势值、词元概率和词元熵。这些发现不仅解释了现有方法的机制，也揭示了其局限：它们仅对一两个因素进行启发式调整，未考虑其他相关因素，从而降低了有效性。为此，我们提出新方法STEER，该方法基于估计的熵变化对词元进行自适应重加权，以原则性方式调控熵。在数学与代码基准测试上的实验表明，STEER能有效缓解熵崩溃，并持续优于现有先进基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the problem of entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR), where a rapid decrease in policy entropy limits exploration and learning effectiveness. The method involves a theoretical analysis of GRPO&#x27;s entropy dynamics, identifying four key factors governing token-level entropy change, and proposes a new method called STEER that adaptively reweights tokens based on estimated entropy change to regulate entropy in a principled manner. Experimental results on math and coding benchmarks show that STEER effectively mitigates entropy collapse and consistently outperforms state-of-the-art baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决可验证奖励强化学习中的熵崩溃问题，即策略熵的快速下降会限制探索和学习效果。方法包括对GRPO的熵动态进行理论分析，确定了控制令牌级熵变化的四个关键因素，并提出了一种名为STEER的新方法，该方法基于估计的熵变化自适应地重新加权令牌，以原则性方式调节熵。在数学和编码基准测试上的实验结果表明，STEER有效缓解了熵崩溃，并持续优于最先进的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement</div>
<div class="meta-line">Authors: Jakob Nyberg, Pontus Johnson</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, January 2026</div>
<div class="meta-line">First: 2025-09-11T07:51:38+00:00 · Latest: 2026-01-19T14:07:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09219v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.09219v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present and evaluate Vejde; a framework which combines data abstraction, graph neural networks and reinforcement learning to produce inductive policy functions for decision problems with richly structured states, such as object classes and relations. MDP states are represented as data bases of facts about entities, and Vejde converts each state to a bipartite graph, which is mapped to latent states through neural message passing. The factored representation of both states and actions allows Vejde agents to handle problems of varying size and structure. We tested Vejde agents on eight problem domains defined in RDDL, with ten problem instances each, where policies were trained using both supervised and reinforcement learning. To test policy generalization, we separate problem instances in two sets, one for training and the other solely for testing. Test results on unseen instances for the Vejde agents were compared to MLP agents trained on each problem instance, as well as the online planning algorithm Prost. Our results show that Vejde policies in average generalize to the test instances without a significant loss in score. Additionally, the inductive agents received scores on unseen test instances that on average were close to the instance-specific MLP agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vejde：基于因子图颜色细化的归纳深度强化学习框架</div>
<div class="mono" style="margin-top:8px">本文提出并评估了Vejde框架，该框架结合数据抽象、图神经网络与强化学习，为具有丰富结构化状态（如对象类别与关系）的决策问题生成归纳策略函数。MDP状态被表示为实体事实数据库，Vejde将每个状态转换为二分图，并通过神经消息传递映射至隐状态。状态与动作的因子化表示使Vejde智能体能处理不同规模与结构的问题。我们在RDDL定义的八个问题域（每个域含十个问题实例）中测试Vejde智能体，使用监督学习与强化学习训练策略。为检验策略泛化能力，我们将问题实例分为训练集与独立测试集，并将Vejde智能体在未见实例上的测试结果与针对各实例训练的MLP智能体及在线规划算法Prost进行对比。结果表明：Vejde策略在测试实例上平均未出现显著分数损失，且归纳智能体在未见测试实例上获得的分数平均接近针对特定实例训练的MLP智能体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind Vejde is to create inductive reinforcement learning policies for decision problems with complex, structured states involving objects and relations, enabling generalization across varying problem sizes and structures. The method involves representing Markov Decision Process states as databases of entity facts, converting each state into a bipartite graph, and using neural message passing on graph neural networks to map these to latent states, with factored representations for both states and actions. Experimental results on eight RDDL domains, with separate training and test instances, show that Vejde policies generalize effectively to unseen instances without significant score loss, performing comparably to instance-specific multilayer perceptron agents and close to the online planning algorithm Prost.</div>
<div class="mono" style="margin-top:8px">Vejde的动机是为具有复杂结构化状态（如对象类别和关系）的决策问题创建归纳性强化学习策略，以实现对不同规模和结构问题的泛化。该方法将马尔可夫决策过程状态表示为实体事实数据库，将每个状态转换为二分图，并利用图神经网络上的神经消息传递将其映射到潜在状态，同时对状态和动作进行分解表示。在八个RDDL领域上的实验结果表明，使用独立训练和测试实例时，Vejde策略能有效泛化到未见过的实例，且得分无明显下降，其性能与针对特定实例的多层感知器代理相当，并接近在线规划算法Prost。</div>
</details>
</div>
<div class="card">
<div class="title">DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs</div>
<div class="meta-line">Authors: Ying Jiao, Rodrigo Castellano Ontiveros, Luc De Raedt, Marco Gori, Francesco Giannini, Michelangelo Diligenti, Giuseppe Marra</div>
<div class="meta-line">Venue: AAAI 2026 Oral</div>
<div class="meta-line">First: 2025-11-11T18:58:03+00:00 · Latest: 2026-01-19T13:37:33+00:00</div>
<div class="meta-line">Comments: Accepted as an Oral at AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08581v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08581v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neurosymbolic (NeSy) AI aims to combine the strengths of neural architectures and symbolic reasoning to improve the accuracy, interpretability, and generalization capability of AI models. While logic inference on top of subsymbolic modules has been shown to effectively guarantee these properties, this often comes at the cost of reduced scalability, which can severely limit the usability of NeSy models. This paper introduces DeepProofLog (DPrL), a novel NeSy system based on stochastic logic programs, which addresses the scalability limitations of previous methods. DPrL parameterizes all derivation steps with neural networks, allowing efficient neural guidance over the proving system. Additionally, we establish a formal mapping between the resolution process of our deep stochastic logic programs and Markov Decision Processes, enabling the application of dynamic programming and reinforcement learning techniques for efficient inference and learning. This theoretical connection improves scalability for complex proof spaces and large knowledge bases. Our experiments on standard NeSy benchmarks and knowledge graph reasoning tasks demonstrate that DPrL outperforms existing state-of-the-art NeSy systems, advancing scalability to larger and more complex settings than previously possible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepProofLog：深度随机逻辑程序中的高效证明</div>
<div class="mono" style="margin-top:8px">神经符号（NeSy）人工智能旨在融合神经架构与符号推理的优势，以提升AI模型的准确性、可解释性与泛化能力。尽管在亚符号模块之上进行逻辑推理已被证明能有效保障这些特性，但这往往以牺牲可扩展性为代价，严重制约NeSy模型的实用性。本文提出基于随机逻辑程序的新型NeSy系统DeepProofLog（DPrL），以解决现有方法的可扩展性局限。DPrL通过神经网络参数化所有推导步骤，实现对证明系统的高效神经引导。此外，我们建立了深度随机逻辑程序的归结过程与马尔可夫决策过程的形式化映射，使得动态规划与强化学习技术可应用于高效推理与学习。该理论关联显著提升了复杂证明空间与大规模知识库的可扩展性。在标准NeSy基准测试与知识图谱推理任务上的实验表明，DPrL性能超越现有最先进的NeSy系统，将可扩展性推进至更大规模、更复杂的应用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces DeepProofLog (DPrL), a neurosymbolic system motivated by the need to combine the interpretability of symbolic reasoning with neural network scalability, addressing the efficiency limitations of prior methods. The method employs deep stochastic logic programs where all derivation steps are parameterized by neural networks, and it formally maps the resolution process to a Markov Decision Process to leverage dynamic programming and reinforcement learning for efficient inference. Experimental results on standard neurosymbolic benchmarks and knowledge graph reasoning tasks show that DPrL outperforms existing state-of-the-art systems, significantly improving scalability in larger and more complex settings.</div>
<div class="mono" style="margin-top:8px">本文提出了DeepProofLog（DPrL），一个神经符号系统，其动机在于结合符号推理的可解释性与神经网络的可扩展性，以解决先前方法在效率上的局限。该方法采用深度随机逻辑程序，通过神经网络参数化所有推导步骤，并将解析过程形式化映射到马尔可夫决策过程，从而利用动态规划和强化学习技术实现高效推理。在标准神经符号基准测试和知识图谱推理任务上的实验结果表明，DPrL优于现有最先进的神经符号系统，显著提升了在更大、更复杂场景下的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-19T13:14:16+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但这些模型仍易受对抗性越狱攻击，攻击者通过精心设计的细微扰动绕过安全机制并触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等问题，难以应用于现实黑盒场景。为此，我们提出基于零阶优化与同步扰动随机逼近（ZO-SPSA）的黑盒越狱攻击方法。ZO-SPSA具备三大优势：（1）通过输入输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低资源需求与GPU内存消耗。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，且扰动不可感知性与白盒方法相当。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱的现实可行性，并暴露了当前LVLMs安全机制的关键弱点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods that require full model access, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free, model-agnostic approach approximates gradients through input-output interactions, eliminating the need for internal model knowledge and reducing computational resource demands. Experimental results on models like InstructBLIP, LLaVA, and MiniGPT-4 demonstrate a high jailbreak success rate of up to 83.0% on InstructBLIP with imperceptible perturbations, and adversarial examples from MiniGPT-4 show strong transferability, achieving an attack success rate of 64.18% on other LVLMs, highlighting significant safety weaknesses in current systems.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型（LVLMs）易受对抗性越狱攻击的脆弱性，以及现有白盒方法需要完整模型访问权限且不切实际的问题，本文提出了一种使用基于同时扰动随机逼近的零阶优化（ZO-SPSA）的黑盒攻击方法。这种无需梯度、与模型无关的方法通过输入-输出交互来近似梯度，无需了解模型内部知识，并降低了计算资源需求。在InstructBLIP、LLaVA和MiniGPT-4等模型上的实验结果表明，该方法在InstructBLIP上实现了高达83.0%的越狱成功率，且扰动难以察觉；来自MiniGPT-4的对抗样本还表现出强迁移性，在其他LVLMs上的攻击成功率可达64.18%，揭示了当前模型安全机制的重大缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering</div>
<div class="meta-line">Authors: Wencheng Ye, Xiaoyang Yuan, Yi Bin, Pengpeng Zeng, Hengyu Jin, Liang Peng, Heng Tao Shen</div>
<div class="meta-line">First: 2026-01-14T08:04:33+00:00 · Latest: 2026-01-19T13:00:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09269v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09269v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RISER：基于潜在推理技能的适应性激活引导编排框架</div>
<div class="mono" style="margin-top:8px">近期针对大语言模型（LLMs）的领域特定推理研究多依赖需要参数更新的训练密集型方法。虽然激活引导已成为参数高效的替代方案，但现有方法采用静态人工干预，难以适应复杂推理的动态特性。为此，我们提出RISER（基于路由器的可引导推理增强干预框架），这是一种即插即用的干预框架，可在激活空间中自适应地引导LLM推理。RISER构建可复用的推理向量库，并采用轻量级路由器为每个输入动态组合这些向量。该路由器通过任务级奖励下的强化学习进行优化，以涌现式组合方式激活潜在认知基元。在七个多样化基准测试中，RISER相比基础模型实现3.4-6.5%的平均零样本准确率提升，同时以2-3倍更高的标记效率超越思维链式推理，并获得稳健的准确率增益。进一步分析表明，RISER能自主组合多个向量形成可解释的精确控制策略，为构建更可控高效的LLM推理系统指明方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of static, manual activation steering methods in large language models, which fail to adapt to the dynamic requirements of complex reasoning tasks. The proposed method, RISER, introduces a plug-and-play framework that constructs a library of reusable reasoning vectors and employs a lightweight Router, optimized via reinforcement learning, to dynamically compose these vectors for each input, thereby activating latent cognitive primitives in an emergent manner. Experimental results across seven benchmarks show that RISER achieves an average zero-shot accuracy improvement of 3.4-6.5% over the base model, surpasses chain-of-thought reasoning with 2-3 times higher token efficiency, and demonstrates robust accuracy gains while forming interpretable control strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型中静态、手动激活引导方法的局限性，这些方法难以适应复杂推理任务的动态需求。所提出的RISER方法是一个即插即用框架，它构建了一个可重用的推理向量库，并通过强化学习优化的轻量级路由器动态组合这些向量以处理每个输入，从而以涌现方式激活潜在的认知基元。在七个不同基准测试上的实验结果表明，RISER相比基础模型实现了3.4-6.5%的平均零样本准确率提升，以2-3倍的令牌效率超越了思维链式推理，并获得了稳健的准确率增益，同时形成了可解释的控制策略。</div>
</details>
</div>
<div class="card">
<div class="title">PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient</div>
<div class="meta-line">Authors: Zijian Wang, Tiancheng Huang, Hanqi Li, Da Ma, Lu Chen, Kai Yu</div>
<div class="meta-line">First: 2026-01-19T12:07:51+00:00 · Latest: 2026-01-19T12:07:51+00:00</div>
<div class="meta-line">Comments: 35 pages, 9 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12988v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing&#x27; gap in LLMs. We provide a theoretical analysis that establishes DFPO&#x27;s favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PaperGuide：提升小型语言模型论文阅读代理的效率</div>
<div class="mono" style="margin-top:8px">科学文献的快速增长使得研究者仅通过人工阅读难以追踪最新进展。大语言模型（LLMs）的进步催生了能够阅读科学论文并提取任务相关信息的自主代理。然而，现有方法多依赖复杂工程化提示或传统SFT-RL训练流程，常导致低效探索。受认知科学启发，我们提出PaperCompass框架，通过分离高层规划与细粒度执行来缓解这些问题。该框架先起草明确规划纲要，再通过选择函数调用参数进行分步推理实现。为训练此类行为，我们提出草案跟随策略优化（DFPO）——一种联合优化草案规划与最终解决方案的定制强化学习方法。DFPO可视为轻量级分层强化学习，旨在缩小LLMs的“知行差距”。理论分析证明了DFPO的优化优势，支持稳定可靠的训练过程。在论文问答基准测试中，PaperCompass在保持性能的同时显著提升效率，达到与更大模型相当的效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of efficiently tracking scientific advances amidst the literature&#x27;s rapid growth, this paper introduces PaperCompass, a framework that enhances small language-model paper-reading agents by separating high-level planning from detailed execution to reduce excessive exploration. The method employs Draft-and-Follow Policy Optimization (DFPO), a tailored reinforcement learning approach that jointly optimizes draft plans and final solutions, functioning as a lightweight hierarchical reinforcement learning technique to bridge the &#x27;knowing-doing&#x27; gap. Experimental results on paper-based question answering benchmarks demonstrate that PaperCompass improves efficiency over strong baselines without performance loss, achieving outcomes comparable to much larger models.</div>
<div class="mono" style="margin-top:8px">本文的动机是应对科学文献快速增长带来的跟踪挑战，旨在通过分离高层规划与细粒度执行来提升小型语言模型论文阅读代理的效率，以减少过度探索。方法上提出了PaperCompass框架，采用定制化的强化学习方法——草拟与跟随策略优化（DFPO），联合优化草拟计划和最终解决方案，作为一种轻量级分层强化学习来弥合“知与行”的差距。在基于论文的问答基准测试中，实验结果表明PaperCompass在保持性能不损失的前提下，相比强基线提高了效率，达到了与更大模型相当的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Wasserstein Evolution : Evolutionary Optimization as Phase Transition</div>
<div class="meta-line">Authors: Kaichen Ouyang, Mingyang Yu, Zong Ke, Junbo Jacob Lian, Shengwei Fu, Xiaoyang Hao, Shengju Yu, Dayu Hu</div>
<div class="meta-line">First: 2025-12-05T16:12:31+00:00 · Latest: 2026-01-19T12:03:33+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05837v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.05837v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evolutionary algorithms (EAs) serve as powerful black-box optimizers inspired by biological evolution. However, most existing EAs predominantly focus on heuristic operators such as crossover and mutation, while usually overlooking underlying physical interpretability such as statistical mechanics and thermosdynamics. This theoretical void limits the principled understanding of algorithmic dynamics, hindering the systematic design of evolutionary search beyond ad-hoc heuristics. To bridge this gap, we first point out that evolutionary optimization can be conceptually reframed as a physical phase transition process. Building on this perspective, we establish the theoretical grounds by modeling the optimization dynamics as a Wasserstein gradient flow of free energy. Consequently, a robust and interpretable solver named Wasserstein Evolution (WE) is proposed. WE mathematically frames the trade-off between exploration and exploitation as a competition between potential gradient forces and entropic forces. This formulation guarantees convergence to the Boltzmann distribution, thereby minimizing free energy and maximizing entropy, which promotes highly diverse solutions. Extensive experiments on complex multimodal and physical potential functions demonstrate that WE achieves superior diversity and stability compared to established baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wasserstein演化：作为相变过程的进化优化</div>
<div class="mono" style="margin-top:8px">进化算法（EAs）作为受生物进化启发的强大黑盒优化器，现有研究多聚焦于交叉、变异等启发式算子，而常忽视统计力学与热力学等底层物理解释性。这一理论空白限制了对算法动态的原理性理解，阻碍了超越临时启发式的进化搜索系统设计。为弥合此鸿沟，我们首先指出进化优化在概念上可重构为物理相变过程。基于此视角，通过将优化动态建模为自由能的Wasserstein梯度流，建立了理论基础。进而提出名为Wasserstein演化（WE）的鲁棒可解释求解器。WE从数学上将探索与利用的权衡表述为势梯度力与熵力的竞争，该公式保证收敛至玻尔兹曼分布，从而实现自由能最小化与熵最大化，促进高度多样化的解。在复杂多模态及物理势函数上的大量实验表明，相较于现有基线方法，WE能实现更优的多样性与稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the lack of physical interpretability in existing evolutionary algorithms, which often rely on heuristic operators without grounding in principles like statistical mechanics. To address this, the authors reframe evolutionary optimization as a phase transition process and propose a method called Wasserstein Evolution (WE), which models optimization dynamics as a Wasserstein gradient flow of free energy, mathematically balancing exploration and exploitation through potential and entropic forces. Experimental results on complex multimodal and physical potential functions show that WE achieves superior diversity and stability compared to established baselines, converging to the Boltzmann distribution to minimize free energy and maximize entropy.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有进化算法缺乏物理可解释性，通常依赖启发式算子而未基于统计力学等原理。为解决此问题，作者将进化优化概念上重构为物理相变过程，并提出一种名为Wasserstein Evolution（WE）的方法，该方法将优化动态建模为自由能的Wasserstein梯度流，通过势能和熵力在数学上平衡探索与利用。在复杂多模态和物理势函数上的大量实验表明，与现有基线相比，WE实现了更优的多样性和稳定性，收敛于玻尔兹曼分布以最小化自由能并最大化熵。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Safe Reinforcement Learning for Stochastic Reach-Avoid Problems with Entropy Regularization</div>
<div class="meta-line">Authors: Abhijit Mazumdar, Rafal Wisniewski, Manuela L. Bujorianu</div>
<div class="meta-line">First: 2026-01-13T15:23:19+00:00 · Latest: 2026-01-19T11:54:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08646v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.08646v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于熵正则化的随机可达规避问题可证明安全强化学习</div>
<div class="mono" style="margin-top:8px">我们研究带安全约束的马尔可夫决策过程最优策略学习问题，采用可达规避框架进行建模。目标是设计在线强化学习算法，确保学习阶段以任意高概率满足安全约束。为此，我们首先提出基于不确定性乐观原则的算法，并以此为基础构建采用熵正则化的核心算法。我们对两种算法进行有限样本分析，推导其遗憾界，证明熵正则化的引入能改善遗憾界，并显著控制基于不确定性乐观原则的安全强化学习算法固有的回合间波动性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of ensuring safety during online reinforcement learning in Markov decision processes, specifically for reach-avoid problems where the agent must reach target states while avoiding unsafe ones. The motivation is to guarantee safety with high probability throughout the learning process. The method introduces two algorithms: an initial one based on the optimism in the face of uncertainty (OFU) principle, followed by a main algorithm that incorporates entropy regularization to enhance stability and performance. Experimental results from finite-sample analysis show that the entropy-regularized algorithm achieves improved regret bounds and significantly reduces the high episode-to-episode variability typically associated with OFU-based safe RL approaches.</div>
<div class="mono" style="margin-top:8px">本文针对马尔可夫决策过程中的安全约束问题，特别是在学习阶段确保智能体以高概率满足安全性的到达-规避任务。研究动机是在线强化学习过程中保障安全性。方法上，首先提出了一种基于乐观面对不确定性原则的算法，进而引入了熵正则化的主要算法以提升性能。实验结果表明，通过有限样本分析，熵正则化算法改善了遗憾界，并显著降低了基于乐观面对不确定性原则的安全强化学习算法中固有的回合间波动性。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-shot Generalization in Inventory Management: Train, then Estimate and Decide</div>
<div class="meta-line">Authors: Tarkan Temizöz, Christina Imdahl, Remco Dijkman, Douniel Lamghari-Idrissi, Willem van Jaarsveld</div>
<div class="meta-line">First: 2024-11-01T11:20:05+00:00 · Latest: 2026-01-19T11:14:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.00515v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.00515v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying deep reinforcement learning (DRL) in real-world inventory management presents challenges, including dynamic environments and uncertain problem parameters, e.g. demand and lead time distributions. These challenges highlight a research gap, suggesting a need for a unifying framework to model and solve sequential decision-making under parameter uncertainty. We address this by exploring an underexplored area of DRL for inventory management: training generally capable agents (GCAs) under zero-shot generalization (ZSG). Here, GCAs are advanced DRL policies designed to handle a broad range of sampled problem instances with diverse inventory challenges. ZSG refers to the ability to successfully apply learned policies to unseen instances with unknown parameters without retraining.
  We propose a unifying Super-Markov Decision Process formulation and the Train, then Estimate and Decide (TED) framework to train and deploy a GCA tailored to inventory management applications. The TED framework consists of three phases: training a GCA on varied problem instances, continuously estimating problem parameters during deployment, and making decisions based on these estimates. Applied to periodic review inventory problems with lost sales, cyclic demand patterns, and stochastic lead times, our trained agent, the Generally Capable Lost Sales Network (GC-LSN) consistently outperforms well-known traditional policies when problem parameters are known. Moreover, under conditions where demand and/or lead time distributions are initially unknown and must be estimated, we benchmark against online learning methods that provide worst-case performance guarantees. Our GC-LSN policy, paired with the Kaplan-Meier estimator, is demonstrated to complement these methods by providing superior empirical performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>库存管理中的零样本泛化：训练、估计与决策框架</div>
<div class="mono" style="margin-top:8px">在现实库存管理中部署深度强化学习面临动态环境和不确定问题参数（如需求与提前期分布）等挑战，这揭示了当前研究空白——亟需建立统一框架以解决参数不确定性下的序贯决策问题。本研究通过探索库存管理DRL中尚未充分开发的领域：零样本泛化条件下的通用能力智能体训练，提出解决方案。通用能力智能体是能处理多样化库存挑战样本实例的先进DRL策略，零样本泛化指无需重新训练即可将学习策略成功应用于参数未知的新实例。
我们提出统一的超马尔可夫决策过程建模及“训练-估计-决策”框架，用以训练和部署专为库存管理设计的通用能力智能体。该框架包含三个阶段：在多样化问题实例上训练通用能力智能体、部署期间持续估计问题参数、基于估计结果进行决策。应用于具有销售损失、周期性需求模式和随机提前期的定期盘点库存问题时，我们训练的通用能力销售损失网络在参数已知情况下持续优于传统策略。此外，在需求与提前期分布初始未知需估计的场景下，通过与提供最坏情况性能保证的在线学习方法对比，证明通用能力销售损失网络策略配合Kaplan-Meier估计器能通过卓越的实证性能补足现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of deploying deep reinforcement learning in real-world inventory management, where dynamic environments and uncertain parameters like demand and lead times hinder generalization. The authors propose a unifying Super-Markov Decision Process formulation and a Train, then Estimate and Decide (TED) framework to train a generally capable agent (GCA) that can handle diverse inventory challenges and generalize zero-shot to unseen instances without retraining. In experiments on periodic review inventory problems with lost sales, cyclic demand, and stochastic lead times, their Generally Capable Lost Sales Network (GC-LSN), when paired with a Kaplan-Meier estimator, outperforms traditional policies under known parameters and shows superior empirical performance compared to online learning methods under initially unknown distributions.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习在现实库存管理中的应用挑战，即动态环境和不确定参数（如需求和提前期分布）阻碍了泛化能力。作者提出了一个统一的超马尔可夫决策过程建模和“训练、估计与决策”（TED）框架，用于训练一个通用能力智能体（GCA），该智能体能够处理多样化的库存挑战，并在无需重新训练的情况下零样本泛化到未见过的实例。在具有缺货损失、周期性需求和随机提前期的定期盘点库存问题实验中，他们的通用缺货损失网络（GC-LSN）结合Kaplan-Meier估计器，在参数已知时优于传统策略，并在参数初始未知时相比具有最坏情况性能保证的在线学习方法展现出更优的实证性能。</div>
</details>
</div>
<div class="card">
<div class="title">On the Global Convergence of Risk-Averse Natural Policy Gradient Methods with Expected Conditional Risk Measures</div>
<div class="meta-line">Authors: Xian Yu, Lei Ying</div>
<div class="meta-line">First: 2023-01-26T04:35:28+00:00 · Latest: 2026-01-19T10:32:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2301.10932v5">Abs</a> · <a href="https://arxiv.org/pdf/2301.10932v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Risk-sensitive reinforcement learning (RL) has become a popular tool for controlling the risk of uncertain outcomes and ensuring reliable performance in highly stochastic sequential decision-making problems. While it has been shown that policy gradient methods can find globally optimal policies in the risk-neutral setting, it remains unclear if the risk-averse variants enjoy the same global convergence guarantees. In this paper, we consider a class of dynamic time-consistent risk measures, named Expected Conditional Risk Measures (ECRMs), and derive natural policy gradient (NPG) updates for ECRMs-based RL problems. We provide global optimality and iteration complexity of the proposed risk-averse NPG algorithm with softmax parameterization and entropy regularization under both exact and inexact policy evaluation. Furthermore, we test our risk-averse NPG algorithm on a stochastic Cliffwalk environment to demonstrate the efficacy of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于期望条件风险度量的风险规避自然策略梯度方法的全局收敛性研究</div>
<div class="mono" style="margin-top:8px">风险敏感强化学习已成为控制不确定结果风险、确保高随机性序列决策问题中性能可靠性的重要工具。虽然已有研究表明策略梯度方法在风险中性设定下能找到全局最优策略，但风险规避变体是否具有相同的全局收敛保证仍不明确。本文研究一类动态时间一致风险度量——期望条件风险度量，并针对基于ECRM的强化学习问题推导自然策略梯度更新规则。在精确与非精确策略评估条件下，我们给出了采用softmax参数化与熵正则化的风险规避NPG算法的全局最优性与迭代复杂度分析。此外，通过在随机悬崖行走环境中的实验验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for reliable performance in stochastic sequential decision-making, addressing the lack of global convergence guarantees for risk-averse policy gradient methods in reinforcement learning. The method introduces a natural policy gradient algorithm based on Expected Conditional Risk Measures, a class of dynamic time-consistent risk measures, and analyzes it with softmax parameterization and entropy regularization. The main experimental results include proofs of global optimality and iteration complexity under both exact and inexact policy evaluation, along with empirical validation on a stochastic Cliffwalk environment demonstrating the method&#x27;s efficacy.</div>
<div class="mono" style="margin-top:8px">本文的动机是在随机序列决策中确保可靠性能的需求，针对风险规避策略梯度方法在强化学习中缺乏全局收敛保证的问题。方法基于预期条件风险度量这一动态时间一致风险度量类别，提出了自然策略梯度算法，并采用softmax参数化和熵正则化进行分析。主要实验结果包括在精确和不精确策略评估下证明了全局最优性和迭代复杂度，并在随机Cliffwalk环境中进行了实证验证，展示了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">TranslateGemma Technical Report</div>
<div class="meta-line">Authors: Mara Finkelstein, Isaac Caswell, Tobias Domhan, Jan-Thorsten Peter, Juraj Juraska, Parker Riley, Daniel Deutsch, Geza Kovacs, Cole Dilanni, Colin Cherry, Eleftheria Briakou, Elizabeth Nielsen, Jiaming Luo, Kat Black, Ryan Mullins, Sweta Agrawal, Wenda Xu, Erin Kats, Stephane Jaskiewicz, Markus Freitag, David Vilar</div>
<div class="meta-line">First: 2026-01-13T22:23:24+00:00 · Latest: 2026-01-19T09:52:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09012v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.09012v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TranslateGemma技术报告</div>
<div class="mono" style="margin-top:8px">本文介绍TranslateGemma——一套基于Gemma 3基础模型的开源机器翻译模型。为增强Gemma 3在翻译任务中的多语言能力，我们采用两阶段微调策略：首先使用通过前沿模型生成的大规模高质量合成平行数据与人工翻译平行数据混合进行监督微调；随后通过强化学习阶段，采用包含MetricX-QE和AutoMQM的奖励模型集成优化翻译质量。我们在WMT25测试集的10个语言对上通过人工评估，以及在WMT24++基准的55个语言对上通过自动评估，验证了TranslateGemma的有效性。自动指标显示所有规模的模型均较基线Gemma 3模型取得持续显著提升。值得注意的是，较小规模的TranslateGemma模型常能达到与较大基线模型相当的性能，且具有更优效率。实验还表明TranslateGemma模型保留了强大的多模态能力，在Vistra图像翻译基准上表现提升。开源TranslateGemma模型的发布旨在为研究社区提供强大且适应性强的机器翻译工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind TranslateGemma is to enhance the multilingual translation capabilities of the Gemma 3 foundation models by creating an open suite of machine translation models. The method employs a two-stage fine-tuning process: first, supervised fine-tuning using a mixture of high-quality synthetic and human-translated parallel data, followed by reinforcement learning optimized with an ensemble of reward models like MetricX-QE and AutoMQM. Experimental results on the WMT25 and WMT24++ benchmarks show consistent and substantial gains over baseline Gemma 3 models across 10 and 55 language pairs, with smaller TranslateGemma models often matching the performance of larger baselines for improved efficiency, while also retaining strong multimodal capabilities as evidenced by enhanced performance on the Vistra image translation benchmark.</div>
<div class="mono" style="margin-top:8px">TranslateGemma 旨在基于 Gemma 3 基础模型，通过增强其固有的多语言能力来开发一套开放的机器翻译模型。其方法采用两阶段微调流程：首先使用由先进模型生成的大规模高质量合成平行数据与人工翻译平行数据混合进行监督微调，随后通过强化学习阶段，利用包括 MetricX-QE 和 AutoMQM 在内的奖励模型集成来优化翻译质量。在 WMT25 测试集的 10 个语言对上的人工评估和 WMT24++ 基准的 55 个语言对上的自动评估表明，该模型在所有规模上均比基线 Gemma 3 模型取得了一致且显著的性能提升，较小的 TranslateGemma 模型常能达到与较大基线模型相当的性能，从而提高了效率，同时模型还保留了强大的多模态能力，在 Vistra 图像翻译基准上表现增强。</div>
</details>
</div>
<div class="card">
<div class="title">Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration</div>
<div class="meta-line">Authors: Yuejie Li, Ke Yang, Tao Wang, Bolin Chen, Bowen Li, Chengjun Mao</div>
<div class="meta-line">First: 2026-01-16T10:02:31+00:00 · Latest: 2026-01-19T09:50:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11144v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11144v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度图检索增强生成：一种层次化检索与自适应融合的平衡方法</div>
<div class="mono" style="margin-top:8px">基于图的检索增强生成框架面临全局搜索全面性与局部搜索效率之间的权衡。现有方法在导航大规模层次化图结构、优化检索路径、平衡探索-利用动态性方面常遇挑战，且普遍缺乏鲁棒的多阶段重排序机制。为克服这些缺陷，本文提出深度图检索增强生成框架，采用层次化检索与自适应融合的平衡策略。该框架引入从全局到局部的层次化检索机制，整合宏观社区间与微观社区内的上下文关系。该策略采用三阶段流程：(1) 社区间过滤：利用局部上下文剪枝搜索空间；(2) 社区级优化：通过实体交互分析对相关子图进行优先级排序；(3) 目标社区内的实体级细粒度搜索。通过集束搜索优化的动态重排序模块引导该过程，持续筛选候选结果以平衡效率与全局覆盖性。本框架还配备知识融合模块，采用经动态加权奖励GRPO训练的紧凑大语言模型。该强化学习方法动态调整奖励权重以平衡三个核心目标：相关性、忠实性与简洁性。通过该训练，紧凑模型在融合任务中可接近大型模型的性能表现。在自然问答与热点问答数据集上的评估表明，深度图检索增强生成框架在准确性与效率上均显著优于基线图检索方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the trade-off between global comprehensiveness and local efficiency in existing GraphRAG frameworks, which struggle with navigating hierarchical graphs and lack robust multi-stage re-ranking, this paper proposes Deep GraphRAG. The method employs a hierarchical global-to-local retrieval strategy with a three-stage process—inter-community filtering, community-level refinement, and entity-level fine-grained search—guided by a beam search-optimized dynamic re-ranking module, and integrates a Knowledge Integration Module trained with a novel reinforcement learning approach, Dynamic Weighting Reward GRPO, to dynamically balance relevance, faithfulness, and conciseness. Experimental results on Natural Questions and HotpotQA show that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency, with compact 1.5B models approaching the performance of much larger 70B models in integration tasks.</div>
<div class="mono" style="margin-top:8px">针对现有基于图的检索增强生成框架在全局搜索全面性与局部搜索效率之间存在权衡、难以导航大规模层次图且缺乏鲁棒多阶段重排的挑战，本文提出了Deep GraphRAG。该方法采用一种分层的全局到局部检索策略，包含三个阶段：社区间过滤、社区级细化和目标社区内的实体级细粒度搜索，并通过波束搜索优化的动态重排模块进行引导；同时，它集成了一个知识整合模块，采用新颖的强化学习方法——动态加权奖励GRPO进行训练，以动态平衡相关性、忠实性和简洁性三个目标。在Natural Questions和HotpotQA数据集上的评估表明，Deep GraphRAG在准确性和效率上均显著优于基线图检索方法，且1.5B的紧凑模型在整合任务中性能接近70B的大型模型。</div>
</details>
</div>
<div class="card">
<div class="title">Communication Methods in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Christoph Wittner</div>
<div class="meta-line">First: 2026-01-19T09:39:00+00:00 · Latest: 2026-01-19T09:39:00+00:00</div>
<div class="meta-line">Comments: 12 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12886v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中的通信方法</div>
<div class="mono" style="margin-top:8px">多智能体强化学习是一个前景广阔的研究领域，它将成熟的强化学习方法扩展到多智能体系统问题中。近年来，该领域引入了多种通信方法，以应对部分可观测环境、非平稳性及指数级增长的动作空间等问题。通信进一步促进了环境中所有交互智能体之间的高效协作。本文旨在概述多智能体强化学习中的通信技术，通过对该主题的29篇文献进行深入分析，评估了显式、隐式、基于注意力、基于图以及基于层次/角色的通信方法的优缺点。比较结果表明，不存在适用于所有问题的通用最优通信框架；相反，通信方式的选择在很大程度上取决于具体问题。比较还突显了低计算开销通信方法的重要性，以实现对多智能体交互环境的可扩展性。最后，本文讨论了当前的研究空白，强调需要建立系统级指标的标准化基准测试，并提升在现实通信条件下的鲁棒性，以增强这些方法在实际应用中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper provides a systematic overview of communication methods in multi-agent reinforcement learning (MARL), motivated by the need to address challenges like partial observability, non-stationarity, and large action spaces while enabling efficient cooperation. The method involves an in-depth analysis of 29 publications, evaluating explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication techniques. The main experimental results indicate that no single communication framework is universally optimal; instead, the choice depends heavily on the specific problem, with low computational overhead being crucial for scalability in multi-agent environments, and the paper identifies research gaps such as the need for standardized benchmarking and improved robustness under realistic conditions.</div>
<div class="mono" style="margin-top:8px">本文对多智能体强化学习中的通信方法进行了系统性综述，其动机在于解决部分可观测性、非平稳性和庞大动作空间等挑战，同时促进智能体间的高效协作。研究方法基于对29篇相关文献的深入分析，评估了显式、隐式、基于注意力、基于图以及基于层次/角色的通信技术。主要实验结果表明，不存在适用于所有问题的通用最优通信框架，选择高度依赖于具体问题，其中低计算开销对于多智能体环境中的可扩展性至关重要，论文还指出了当前研究空白，如需要标准化系统级指标基准测试以及在现实通信条件下提升鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning</div>
<div class="meta-line">Authors: Marvin Illian, Ramin Khalili, Antonio A. de A. Rocha, Lin Wang</div>
<div class="meta-line">First: 2026-01-07T16:51:33+00:00 · Latest: 2026-01-19T08:56:16+00:00</div>
<div class="meta-line">Comments: 11 pages, 13 figures, 3 tables, v3: Added analysis of heuristic tuning trade-offs (Config-A vs Config-B) across scenarios with corresponding reference-value table; corrected performance numbers in the conclusion; no change to methodology</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04083v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.04083v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based on operator experience and rarely adapted to dynamic network conditions. In this work, we ask: Can an agent automatically learn and adapt cell (re)selection parameters to consistently improve network performance? We present a reinforcement learning (RL)-based framework called CellPilot that adaptively tunes cell (re)selection parameters by learning spatiotemporal patterns of mobile network dynamics. Our study with real-world data demonstrates that even a lightweight RL agent can outperform conventional heuristic reconfigurations by up to 167%, while generalizing effectively across different network scenarios. These results indicate that data-driven approaches can significantly improve cell (re)selection configurations and enhance mobile network performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶的蜂窝网络：基于强化学习的自适应小区（重）选择</div>
<div class="mono" style="margin-top:8px">5G网络的广泛部署与4G/LTE网络的共存，为移动设备提供了多样化的候选连接小区。然而，如何将移动设备关联至小区以最大化整体网络性能（即小区（重）选择）仍是移动运营商面临的关键挑战。当前，小区（重）选择参数通常基于运营商经验手动配置，极少适应动态网络条件。本研究提出：智能体能否自动学习并调整小区（重）选择参数以持续提升网络性能？我们提出了名为CellPilot的强化学习框架，通过学习移动网络动态的时空模式自适应调整小区（重）选择参数。基于真实数据的实验表明，即使轻量级强化学习智能体也能超越传统启发式重配置方案达167%，并在不同网络场景中展现良好泛化能力。这些结果表明数据驱动方法可显著优化小区（重）选择配置，提升移动网络性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of manually configuring cell (re)selection parameters in dynamic 4G/5G networks, this paper introduces CellPilot, a reinforcement learning framework that autonomously adapts these parameters by learning spatiotemporal network patterns. The method employs a lightweight RL agent to tune configurations in response to real network conditions. Experimental results using real-world data show that this approach outperforms conventional heuristic reconfigurations by up to 167% and generalizes effectively across diverse network scenarios, demonstrating the potential of data-driven methods to enhance mobile network performance.</div>
<div class="mono" style="margin-top:8px">本文针对4G/5G动态网络中手动配置小区（重）选择参数的挑战，提出了名为CellPilot的强化学习框架，该框架通过学习时空网络模式来自主调整这些参数。该方法采用轻量级强化学习代理，根据实际网络条件动态优化配置。基于真实数据的实验结果表明，该方法相比传统启发式重配置性能提升最高达167%，并能有效泛化到不同网络场景，证明了数据驱动方法在提升移动网络性能方面的显著潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Thompson Sampling in Function Spaces via Neural Operators</div>
<div class="meta-line">Authors: Rafael Oliveira, Xuesong Wang, Kian Ming A. Chai, Edwin V. Bonilla</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-27T04:21:57+00:00 · Latest: 2026-01-19T08:55:11+00:00</div>
<div class="meta-line">Comments: Final revision to appear at NeurIPS 2025 proceedings, expanded proof of Proposition 2, added Remark 2 on sublinear information gain, and revised discussion at the end of Appendix C.4</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21894v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.21894v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose an extension of Thompson sampling to optimization problems over function spaces where the objective is a known functional of an unknown operator&#x27;s output. We assume that queries to the operator (such as running a high-fidelity simulator or physical experiment) are costly, while functional evaluations on the operator&#x27;s output are inexpensive. Our algorithm employs a sample-then-optimize approach using neural operator surrogates. This strategy avoids explicit uncertainty quantification by treating trained neural operators as approximate samples from a Gaussian process (GP) posterior. We derive regret bounds and theoretical results connecting neural operators with GPs in infinite-dimensional settings. Experiments benchmark our method against other Bayesian optimization baselines on functional optimization tasks involving partial differential equations of physical systems, demonstrating better sample efficiency and significant performance gains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经算子的函数空间汤普森采样方法</div>
<div class="mono" style="margin-top:8px">本文提出将汤普森采样扩展至函数空间优化问题，其目标函数是未知算子输出的已知泛函。我们假设对算子的查询（如运行高保真模拟器或物理实验）成本高昂，而对算子输出的泛函评估成本低廉。算法采用基于神经算子代理的&#x27;采样后优化&#x27;策略，通过将训练后的神经算子视为高斯过程后验的近似样本，避免显式不确定性量化。我们推导了无限维场景下神经算子与高斯过程关联的遗憾界与理论结果。在涉及物理系统偏微分方程的函数优化任务实验中，本方法较其他贝叶斯优化基线表现出更优的样本效率与显著性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing expensive-to-evaluate functionals of unknown operators, such as those arising from physical simulations, by extending Thompson sampling to infinite-dimensional function spaces. The method employs a sample-then-optimize strategy using neural operators as surrogates, which are treated as approximate samples from a Gaussian process posterior to avoid explicit uncertainty quantification. Theoretical analysis provides regret bounds and connections between neural operators and Gaussian processes. Experimental results on functional optimization tasks involving partial differential equations demonstrate that the proposed approach achieves superior sample efficiency and performance gains compared to standard Bayesian optimization baselines.</div>
<div class="mono" style="margin-top:8px">本文针对未知算子的昂贵函数优化问题，将汤普森采样扩展至无限维函数空间，以优化如物理仿真等成本高昂的算子输出泛函。该方法采用基于神经算子的“采样后优化”策略，将训练后的神经算子视为高斯过程后验的近似样本，从而避免显式的不确定性量化。理论分析给出了遗憾界并建立了神经算子与高斯过程在无限维设置下的联系。在涉及偏微分方程的函数优化任务实验中，该方法相比其他贝叶斯优化基线表现出更优的样本效率和显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum spatial best-arm identification via quantum walks</div>
<div class="meta-line">Authors: Tomoki Yamagami, Etsuo Segawa, Takatomo Mihana, André Röhm, Atsushi Uchida, Ryoichi Horisaki</div>
<div class="meta-line">First: 2025-09-07T01:53:09+00:00 · Latest: 2026-01-19T06:46:47+00:00</div>
<div class="meta-line">Comments: 15 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.05890v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.05890v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum reinforcement learning has emerged as a framework combining quantum computation with sequential decision-making, and applications to the multi-armed bandit (MAB) problem have been reported. The graph bandit problem extends the MAB setting by introducing spatial constraints, yet quantum approaches remain limited. We propose a quantum algorithmic framework for best-arm identification in graph bandits, termed Quantum Spatial Best-Arm Identification (QSBAI), which is applicable to general graph structures. The method employs quantum walks to encode superpositions over graph-constrained actions, extending amplitude amplification and generalizing the Quantum BAI algorithm via Szegedy&#x27;s walk framework. This establishes a link between Grover-type search and reinforcement learning tasks with structural restrictions. We focus our theoretical analysis on complete and bipartite graphs, deriving the maximal success probability of identifying the best arm and the time step at which it is achieved. Our results highlight the potential of quantum walks to accelerate exploration in constrained environments and extend the applicability of quantum algorithms for decision-making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于量子行走的量子空间最优臂识别</div>
<div class="mono" style="margin-top:8px">量子强化学习已成为结合量子计算与序列决策的框架，其在多臂老虎机问题中的应用已有报道。图老虎机问题通过引入空间约束扩展了MAB设置，但量子方法仍有限。我们提出一种用于图老虎机中最优臂识别的量子算法框架，称为量子空间最优臂识别，适用于一般图结构。该方法采用量子行走编码图约束动作的叠加态，扩展了振幅放大并通过Szegedy行走框架推广了量子BAI算法。这建立了Grover型搜索与具有结构限制的强化学习任务间的联系。我们的理论分析聚焦于完全图和二分图，推导出识别最优臂的最大成功概率及其实现的时间步。研究结果突显了量子行走在加速受限环境中探索的潜力，并扩展了量子决策算法的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited quantum approaches to graph-structured multi-armed bandit problems, this paper proposes a quantum algorithmic framework called Quantum Spatial Best-Arm Identification (QSBAI) to accelerate best-arm identification under spatial constraints. The method employs quantum walks to encode superpositions over graph-constrained actions, extending amplitude amplification and generalizing prior quantum algorithms via Szegedy&#x27;s walk framework to link Grover-type search with structured reinforcement learning. Focusing theoretical analysis on complete and bipartite graphs, the main experimental results derive the maximal success probability for identifying the best arm and the optimal time step to achieve it, demonstrating the potential of quantum walks to accelerate exploration in constrained decision-making environments.</div>
<div class="mono" style="margin-top:8px">针对图结构多臂老虎机问题中量子方法有限的现状，本文提出了一个称为量子空间最优臂识别的量子算法框架，旨在加速空间约束下的最优臂识别。该方法采用量子游走来编码图约束动作的叠加态，通过Szegedy游走框架扩展振幅放大并推广了现有量子算法，从而将Grover型搜索与结构化强化学习任务联系起来。理论分析聚焦于完全图和二分图，主要实验结果推导了识别最优臂的最大成功概率及其实现的最优时间步，证明了量子游走在加速受限决策环境探索方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction</div>
<div class="meta-line">Authors: Xingjie Gao, Pengcheng Huang, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Chen Qian, Ge Yu, Yu Gu</div>
<div class="meta-line">First: 2026-01-19T06:46:33+00:00 · Latest: 2026-01-19T06:46:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12762v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12762v1">PDF</a> · <a href="https://github.com/NEUIR/ToolMaster">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过环境交互教学大型语言模型进行工具试用与执行</div>
<div class="mono" style="margin-top:8px">为大型语言模型配备外部工具使其能够解决复杂的现实问题。然而，面对新颖或不断演变的工具时，现有方法的鲁棒性仍是关键挑战。当前以轨迹为中心的范式主要依赖训练时记忆静态解决方案路径，这限制了模型将工具使用泛化到新引入或未见工具的能力。本文提出ToolMaster框架，将工具使用从模仿黄金工具调用轨迹转向通过环境交互主动学习工具使用。为优化模型在工具规划与调用方面的能力，ToolMaster采用“试用-执行”范式：先训练模型模仿包含显式工具试用与自我修正的教师轨迹，再通过强化学习协同优化试用与执行阶段。该过程使智能体能够通过主动与环境交互自主探索正确的工具使用方法，并形成有益于工具执行的经验知识。实验结果表明，ToolMaster在未见或不熟悉工具上的泛化能力与鲁棒性显著优于现有基线。所有代码与数据已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limited robustness of existing methods in equipping Large Language Models (LLMs) with external tools, particularly when facing novel or evolving tools, by shifting from static trajectory memorization to active learning through environment interaction. The proposed ToolMaster framework employs a trial-and-execution paradigm, where LLMs are first trained to imitate teacher-generated trajectories that include explicit tool trials and self-correction, followed by reinforcement learning to jointly coordinate trial and execution phases, enabling autonomous exploration of correct tool usage. Experimental results show that ToolMaster significantly outperforms baselines in generalization and robustness across unseen or unfamiliar tools.</div>
<div class="mono" style="margin-top:8px">本文针对现有方法在为大语言模型配备外部工具时，面对新颖或演化工具时鲁棒性不足的问题，提出从静态轨迹记忆转向通过环境交互进行主动学习。所提出的ToolMaster框架采用试错与执行范式，首先训练大语言模型模仿包含显式工具尝试和自我纠正的教师生成轨迹，随后通过强化学习联合协调试错与执行阶段，从而实现自主探索正确的工具使用方式。实验结果表明，ToolMaster在未见或陌生工具上的泛化能力和鲁棒性显著优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis</div>
<div class="meta-line">Authors: Orit Davidovich, Shimrit Shtern, Segev Wasserkrug, Nimrod Megiddo</div>
<div class="meta-line">First: 2025-12-09T13:40:08+00:00 · Latest: 2026-01-19T06:33:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08601v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.08601v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于价值强化学习的组合优化启发式方法：统一框架与分析</div>
<div class="mono" style="margin-top:8px">自20世纪90年代以来，已有大量实证研究致力于训练神经网络等统计模型作为组合优化问题的学习型启发式方法。该方法若成功应用，可免除专家针对各类问题设计启发式规则的需求。基于其结构特性，许多复杂组合优化问题适合通过强化学习进行处理。现有文献中已涌现大量采用基于价值、策略梯度或行动者-批评者方法训练神经网络的研究，在经验最优性差距和推理运行时间方面均展现出良好效果。然而，支撑强化学习应用于组合优化问题的理论研究仍显不足。为此，我们提出通过马尔可夫决策过程建模组合优化问题的统一框架，并运用强化学习技术求解。我们建立了易于验证的假设条件，在此条件下组合优化问题可表述为等价的无折扣马尔可夫决策过程，且能提供原问题的最优解。进一步地，我们确立了基于价值的强化学习技术收敛至组合优化问题近似解的条件，并给出相关最优性差距的理论保证。收敛性分析包含：(1) 每次强化学习迭代中批量大小与投影梯度下降步长的充分增长率；(2) 基于问题参数与目标强化学习精度的最优性差距量化；(3) 状态空间嵌入选择的重要性。综合分析揭示了深度Q学习算法在此类问题场景中的成功机制与局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the empirical success yet theoretical scarcity of using reinforcement learning (RL) for combinatorial optimization (CO) problems, this paper introduces a unified Markov decision process (MDP) framework to model and solve CO problems via RL. The method establishes easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs and provides conditions for value-based RL techniques to converge. Main experimental results include a convergence analysis that yields a sufficient rate for increasing batch size and gradient steps per iteration, quantifies the optimality gap in terms of problem parameters and RL accuracy, and highlights the importance of state-space embedding, thereby explaining the success and limitations of algorithms like deep Q-learning in this context.</div>
<div class="mono" style="margin-top:8px">本文的动机源于强化学习在组合优化问题中虽取得实证成功但缺乏理论支撑的现状，为此提出了一个统一的马尔可夫决策过程框架来建模和解决组合优化问题。方法上，建立了易于检验的假设，使得组合优化问题可被表述为等价的无折扣马尔可夫决策过程，并给出了基于价值的强化学习技术收敛的条件。主要实验结果包括收敛性分析，该分析提供了每次迭代中批量大小和梯度步长的足够增长率，根据问题参数和强化学习精度量化了最优性差距，并强调了状态空间嵌入的重要性，从而解释了深度Q学习等算法在此类问题中的成功与局限。</div>
</details>
</div>
<div class="card">
<div class="title">Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off</div>
<div class="meta-line">Authors: Zhaochun Li, Chen Wang, Jionghao Bai, Shisheng Cui, Ge Lan, Zhou Zhao, Yue Wang</div>
<div class="meta-line">First: 2026-01-19T05:20:46+00:00 · Latest: 2026-01-19T05:20:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12730v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12730v1">PDF</a> · <a href="https://github.com/597358816/DCPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the &quot;luck&quot; of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \textbf{distribution-centric} perspective for RL, in which exploration is always guided by a &quot;better&quot; target distribution, and reveal that a policy&#x27;s ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以分布为中心的策略优化主导探索-利用权衡</div>
<div class="mono" style="margin-top:8px">探索-利用权衡是大语言模型强化学习的核心挑战。采用组相对策略优化时，训练往往偏向利用驱动：熵值单调下降、样本趋同、探索减弱。现有改进方法多为样本中心式，即寻找或奖励稀有样本，假定探索源于新轨迹和词元。这类启发式方法依赖信息性样本的“运气”，缺乏对策略的原则性控制，常导致收益有限或不稳定。本研究首次引入强化学习的分布中心视角，指出探索始终由“更优”目标分布引导，且策略抵抗熵崩溃的能力由分布本身而非个体样本决定。基于此，我们提出分布中心策略优化，将熵调控重构为分布级正则化。该方法完全在策略内实现可控熵调节，无需外部分布采样，在保持训练稳定性的同时实现高效探索。在多个模型和七项基准测试中，DCPO较GRPO平均提升约20%。总体而言，DCPO以分布级原理替代样本级启发式方法，为可控探索和强化探索-利用权衡提供了理论完备的灵活框架。代码发布于https://github.com/597358816/DCPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the exploration-exploitation trade-off in reinforcement learning for large language models, where methods like GRPO lead to exploitation-driven training with entropy collapse. The authors introduce a distribution-centric perspective, arguing that exploration should be guided by a target distribution rather than relying on sample-centric heuristics, and propose Distribution-Centric Policy Optimization (DCPO) to regulate entropy through distribution-level regularization. Experimental results across multiple models and seven benchmarks show that DCPO improves over GRPO by approximately 20% on average, offering a principled framework for controllable exploration and stable training.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习中的探索-利用权衡问题展开研究，指出如GRPO等方法会导致利用驱动的训练和熵崩溃。作者提出了分布中心视角，认为探索应由目标分布引导而非依赖样本中心启发式方法，并提出了分布中心策略优化（DCPO），通过分布级正则化来调控熵。在多个模型和七个基准测试上的实验结果表明，DCPO平均比GRPO提升约20%，提供了一个理论扎实且灵活的框架来实现可控探索和稳定训练。</div>
</details>
</div>
<div class="card">
<div class="title">An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models</div>
<div class="meta-line">Authors: Yuhiro Ono, Tomohiro Harada, Yukiya Miura</div>
<div class="meta-line">First: 2026-01-19T04:58:15+00:00 · Latest: 2026-01-19T04:58:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12723v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12723v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimization benchmarks play a fundamental role in assessing algorithm performance; however, existing artificial benchmarks often fail to capture the diversity and irregularity of real-world problem structures, while benchmarks derived from real-world problems are costly and difficult to construct. To address these challenges, we propose an evolutionary automatic benchmark generation framework that leverages a large language model (LLM) as a generative operator, termed the LLM-driven evolutionary benchmark generator (LLM-EBG). In this framework, the LLM serves as an evolutionary operator that generates and evolves benchmark problems within a flexible, expressive representation space. As a case study, we generate unconstrained single-objective continuous minimization problems represented as mathematical expressions designed to induce significant performance differences between a genetic algorithm (GA) and differential evolution (DE). Experimental results show that LLM-EBG successfully produces benchmark problems in which the designated target algorithm consistently outperforms the comparative algorithm in more than 80\% of trials. Furthermore, exploratory landscape analysis reveals that benchmarks favoring GA are highly sensitive to variable scaling, demonstrating that the proposed framework can generate problems with distinct geometric characteristics that reflect the intrinsic search behaviors of different optimization algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的进化式自动优化基准生成框架</div>
<div class="mono" style="margin-top:8px">优化基准在评估算法性能方面具有基础性作用；然而，现有的人工基准往往难以捕捉现实问题结构的多样性和不规则性，而基于现实问题构建的基准则成本高昂且难以实现。为应对这些挑战，我们提出一种进化式自动基准生成框架，该框架利用大语言模型作为生成算子，称为LLM驱动的进化基准生成器。在此框架中，LLM作为进化算子，在灵活、富有表现力的表示空间中生成并演化基准问题。作为案例研究，我们生成了以数学表达式表示的无约束单目标连续最小化问题，旨在引发遗传算法与差分进化算法之间的显著性能差异。实验结果表明，LLM-EBG成功生成的基准问题中，指定目标算法在超过80%的试验中持续优于对比算法。此外，探索性景观分析表明，有利于遗传算法的基准对变量缩放高度敏感，这证明所提框架能够生成具有独特几何特征的问题，这些特征反映了不同优化算法的内在搜索行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing optimization benchmarks, which either lack real-world diversity or are costly to construct, this paper introduces an evolutionary framework that uses a large language model (LLM) as a generative operator to automatically create benchmarks. The method, termed LLM-EBG, employs the LLM to evolve mathematical expressions representing unconstrained single-objective continuous minimization problems, specifically designed to highlight performance differences between algorithms like genetic algorithms (GA) and differential evolution (DE). Experimental results demonstrate that LLM-EBG successfully generates benchmarks where the target algorithm outperforms the comparative one in over 80% of trials, with landscape analysis revealing that GA-favoring problems exhibit high sensitivity to variable scaling, indicating the framework&#x27;s ability to produce problems with distinct geometric characteristics aligned with algorithm behaviors.</div>
<div class="mono" style="margin-top:8px">针对现有优化基准测试在捕捉真实问题多样性方面不足或构建成本高昂的问题，本文提出了一种进化框架，利用大语言模型作为生成算子来自动生成基准测试。该方法称为LLM-EBG，通过大语言模型演化表示无约束单目标连续最小化问题的数学表达式，专门设计用于凸显遗传算法与差分进化等算法间的性能差异。实验结果表明，LLM-EBG成功生成的基准测试中，目标算法在超过80%的试验中优于对比算法，且景观分析显示，偏向遗传算法的问题对变量缩放高度敏感，证明了该框架能生成具有不同几何特征、反映算法内在搜索行为的问题。</div>
</details>
</div>
<div class="card">
<div class="title">Teaching Large Reasoning Models Effective Reflection</div>
<div class="meta-line">Authors: Hanbin Wang, Jingwei Song, Jinpeng Li, Qi Zhu, Fei Mi, Ganqu Cui, Yasheng Wang, Lifeng Shang</div>
<div class="meta-line">First: 2026-01-19T04:51:53+00:00 · Latest: 2026-01-19T04:51:53+00:00</div>
<div class="meta-line">Comments: 14 pages (including appendix), 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12720v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12720v1">PDF</a> · <a href="https://github.com/wanghanbinpanda/SCFT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model&#x27;s reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提升大型推理模型的有效反思能力</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）近期在复杂推理任务中展现出卓越性能，常通过自我批判与回溯等反思行为实现。然而，并非所有反思都具有实质效益——许多反思流于表面，对原始答案改进甚微甚至无效，却带来计算开销。本文针对LRMs中的表面化反思问题展开研究。首先提出自我批判微调（SCFT）训练框架，该框架仅利用模型自生成的批判文本来增强其反思推理能力：引导模型批判自身输出，通过拒绝采样筛选高质量批判，并基于批判目标对模型进行微调。在此基础上，进一步提出基于有效反思奖励的强化学习（RLERR）。RLERR利用SCFT初始化的高质量反思构建奖励信号，通过强化学习引导模型内化自我修正过程。在AIME2024和AIME2025两个高难度基准测试上的实验表明，SCFT与RLERR能显著提升推理准确率与反思质量，性能超越现有最优基线方法。所有数据与代码已开源：https://github.com/wanghanbinpanda/SCFT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of superficial self-reflection in Large Reasoning Models (LRMs), where many reflections fail to improve answers and add computational cost. To enhance reflective reasoning, the authors propose Self-Critique Fine-Tuning (SCFT), a framework that uses self-generated critiques filtered via rejection sampling for fine-tuning, and Reinforcement Learning with Effective Reflection Rewards (RLERR), which builds on SCFT by using high-quality reflections as rewards to internalize self-correction through reinforcement learning. Experimental results on the AIME2024 and AIME2025 benchmarks demonstrate that both methods significantly boost reasoning accuracy and reflection quality, outperforming state-of-the-art baselines.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型（LRMs）中存在的表面化自我反思问题展开研究，这些反思往往无法改进答案并增加计算开销。为提升反思推理能力，作者提出了自我批判微调（SCFT）框架，该框架通过拒绝采样过滤自生成的批判用于模型微调，并在此基础上引入有效反思奖励的强化学习（RLERR），利用高质量反思作为奖励信号，通过强化学习内化自我纠正过程。在AIME2024和AIME2025基准测试上的实验结果表明，这两种方法显著提高了推理准确性和反思质量，超越了现有最先进基线。</div>
</details>
</div>
<div class="card">
<div class="title">Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization</div>
<div class="meta-line">Authors: Junyi Liao, Zihan Zhu, Ethan Fang, Zhuoran Yang, Vahid Tarokh</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2026-01-19T04:12:51+00:00 · Latest: 2026-01-19T04:12:51+00:00</div>
<div class="meta-line">Comments: Extended journal version of ICML 2025 paper. Submitted to Operations Research</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating the unknown reward functions driving agents&#x27; behaviors is of central interest in inverse reinforcement learning and game theory. To tackle this problem, we develop a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization, where we aim to reconstruct the underlying reward functions given observed players&#x27; strategies and actions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish the reward function&#x27;s identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building upon this theoretical foundation, we propose a novel algorithm to learn reward functions from observed actions. Our algorithm works in both static and dynamic settings and is adaptable to incorporate different methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample efficiency of our algorithm. Further, we conduct extensive numerical studies to demonstrate the practical effectiveness of the proposed framework, offering new insights into decision-making in competitive environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>竞争性游戏中的奖励解码：基于熵正则化的逆博弈论</div>
<div class="mono" style="margin-top:8px">估计驱动智能体行为的未知奖励函数是逆强化学习与博弈论的核心课题。针对此问题，本文构建了一个统一框架，用于在熵正则化下的双人零和矩阵博弈与马尔可夫博弈中恢复奖励函数，目标是根据观察到的玩家策略与动作重构底层奖励函数。该任务因逆问题的固有模糊性、可行奖励的非唯一性以及观测数据覆盖有限而极具挑战。为应对这些挑战，我们在线性假设下利用量子响应均衡（QRE）建立了奖励函数的可识别性。基于此理论基础，我们提出一种从观测动作中学习奖励函数的新算法。该算法适用于静态与动态场景，并可灵活整合如最大似然估计（MLE）等多种方法。我们为算法的可靠性与样本效率提供了坚实的理论保证，并通过大量数值实验验证了所提框架的实际有效性，为竞争环境中的决策机制提供了新见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of inferring the reward functions that guide agents in competitive settings, a core problem in inverse reinforcement learning and game theory, motivated by the ambiguity and non-uniqueness of solutions when only observing players&#x27; strategies. The method establishes identifiability of rewards under linear assumptions using quantal response equilibrium with entropy regularization and introduces a novel algorithm for reward recovery in both static matrix games and dynamic Markov games, adaptable to techniques like maximum likelihood estimation. Experimental results from extensive numerical studies confirm the algorithm&#x27;s practical effectiveness, reliability, and sample efficiency, providing new insights into decision-making in competitive environments.</div>
<div class="mono" style="margin-top:8px">本文针对竞争环境中推断智能体奖励函数这一核心挑战展开研究，其动机在于仅通过观察玩家策略时存在的解模糊性和非唯一性问题。方法上，在线性假设下利用熵正则化的量化响应均衡建立了奖励函数的可识别性，并提出了一种新颖算法，适用于静态矩阵博弈和动态马尔可夫博弈中的奖励恢复，并可结合最大似然估计等方法。实验结果表明，通过大量数值研究验证了该算法的实际有效性、可靠性和样本效率，为竞争环境中的决策提供了新见解。</div>
</details>
</div>
<div class="card">
<div class="title">Resource-Conscious RL Algorithms for Deep Brain Stimulation</div>
<div class="meta-line">Authors: Arkaprava Gupta, Nicholas Carter, William Zellers, Prateek Ganguli, Benedikt Dietrich, Vibhor Krishna, Parasara Sridhar Duggirala, Samarjit Chakraborty</div>
<div class="meta-line">First: 2026-01-19T03:45:08+00:00 · Latest: 2026-01-19T03:45:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12699v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12699v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Brain Stimulation (DBS) has proven to be a promising treatment of Parkinson&#x27;s Disease (PD). DBS involves stimulating specific regions of the brain&#x27;s Basal Ganglia (BG) using electric impulses to alleviate symptoms of PD such as tremors, rigidity, and bradykinesia. Although most clinical DBS approaches today use a fixed frequency and amplitude, they suffer from side effects (such as slurring of speech) and shortened battery life of the implant. Reinforcement learning (RL) approaches have been used in recent research to perform DBS in a more adaptive manner to improve overall patient outcome. These RL algorithms are, however, too complex to be trained in vivo due to their long convergence time and requirement of high computational resources.
  We propose a new Time &amp; Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL approach for DBS that is more effective than existing algorithms. Further, our T3P agent is lightweight enough to be deployed in the implant, unlike current deep-RL strategies, and even forgoes the need for an offline training phase. Additionally, most existing RL approaches have focused on modulating only frequency or amplitude, and the possibility of tuning them together remains greatly unexplored in the literature. Our RL agent can tune both frequency and amplitude of DBS signals to the brain with better sample efficiency and requires minimal time to converge. We implement an MAB agent for DBS for the first time on hardware to report energy measurements and prove its suitability for resource-constrained platforms. Our T3P MAB algorithm is deployed on a variety of microcontroller unit (MCU) setups to show its efficiency in terms of power consumption as opposed to other existing RL approaches used in recent work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向深部脑刺激的资源感知强化学习算法</div>
<div class="mono" style="margin-top:8px">深部脑刺激（DBS）已被证明是治疗帕金森病（PD）的一种前景广阔的方法。该方法通过电脉冲刺激大脑基底核（BG）特定区域，以缓解震颤、僵硬和运动迟缓等PD症状。尽管目前临床DBS多采用固定频率与振幅，但存在副作用（如言语含糊）及植入设备电池寿命缩短等问题。近期研究尝试运用强化学习（RL）实现自适应DBS以提升疗效，然而传统RL算法因收敛时间长、计算资源需求高而难以进行体内训练。
我们提出一种新型时间与阈值触发的多臂赌博机（T3P MAB）RL方法，其效果优于现有算法。该T3P智能体具有轻量化特性，可直接部署于植入设备，无需离线训练阶段，与当前深度RL策略形成鲜明对比。此外，现有RL研究多集中于单独调节频率或振幅，而对二者协同调控的探索尚不充分。我们的RL智能体能够以更高样本效率同步优化DBS信号的频率与振幅，且收敛时间极短。研究首次在硬件上实现DBS多臂赌博机智能体，通过能耗测量验证了其在资源受限平台上的适用性。T3P MAB算法在多种微控制器单元（MCU）平台上部署测试，其功耗表现显著优于近期文献中其他RL方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of fixed-parameter Deep Brain Stimulation (DBS) for Parkinson&#x27;s Disease, which can cause side effects and drain implant batteries, by proposing a more adaptive, resource-efficient reinforcement learning method. The authors introduce a Time &amp; Threshold-Triggered Multi-Armed Bandit (T3P MAB) algorithm that is lightweight enough for on-implant deployment without offline training and uniquely tunes both stimulation frequency and amplitude simultaneously. Experimental results demonstrate that the T3P MAB agent achieves better sample efficiency and faster convergence than existing RL approaches, with hardware implementations on microcontrollers confirming its significantly lower power consumption, making it suitable for resource-constrained platforms.</div>
<div class="mono" style="margin-top:8px">本文针对帕金森病固定参数脑深部电刺激疗法存在的副作用和植入设备电池寿命短的问题，提出了一种更自适应且资源高效的强化学习方法。作者设计了一种时间与阈值触发的多臂赌博机算法，该算法轻量级，无需离线训练即可在植入设备上直接部署，并能同时调节刺激频率和振幅。实验结果表明，该代理比现有强化学习方法具有更好的样本效率和更快的收敛速度，在微控制器上的硬件实现证实了其功耗显著更低，适合资源受限的平台。</div>
</details>
</div>
<div class="card">
<div class="title">Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</div>
<div class="meta-line">Authors: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti</div>
<div class="meta-line">First: 2026-01-19T02:18:45+00:00 · Latest: 2026-01-19T02:18:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12662v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于图神经网络的去中心化学习策略用于估计误差最小化</div>
<div class="mono" style="margin-top:8px">本文研究动态但结构相似的多跳无线网络中自回归马尔可夫源的实时采样与估计问题。各节点缓存来自其他节点的样本，并通过无线碰撞信道进行通信，旨在通过去中心化策略最小化时间平均估计误差。由于动作空间维度高且网络拓扑复杂，解析推导最优策略不可行。为此，我们提出一种基于图的多智能体强化学习框架进行策略优化。理论上，我们证明了所提策略具有可迁移性，即在单一图上训练的策略可有效应用于结构相似的图。数值实验表明：（i）所提策略优于现有基线方法；（ii）训练后的策略可迁移至更大规模网络，且性能增益随智能体数量增加而提升；（iii）图训练过程能适应非平稳环境，即使采用独立学习技术；（iv）循环机制在独立学习与集中训练分散执行中均起关键作用，并能增强对非平稳性的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of minimizing time-average estimation error in decentralized multi-hop wireless networks, where nodes sample and estimate autoregressive Markovian sources under complex topologies and high-dimensional action spaces that make analytical solutions intractable. The method introduces a graphical multi-agent reinforcement learning framework to optimize decentralized policies for sampling and communication over wireless collision channels. Experimental results show that the proposed policy outperforms existing baselines, is transferable to structurally similar and larger networks with increasing performance gains, withstands non-stationarity even with independent learning, and benefits from recurrence to enhance resilience.</div>
<div class="mono" style="margin-top:8px">本文旨在解决去中心化多跳无线网络中时间平均估计误差最小化的问题，其中节点在复杂拓扑和高维动作空间下对自回归马尔可夫源进行采样和估计，这使得解析求解变得困难。方法上，提出了一种图多智能体强化学习框架，以优化在无线碰撞信道上的分散采样和通信策略。实验结果表明，所提策略优于现有基线，可迁移到结构相似且更大的网络中且性能增益随智能体数量增加而提升，即使在独立学习下也能抵抗非平稳性，并通过循环结构增强了鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Transferable Graphical MARL for Real-Time Estimation in Dynamic Wireless Networks</div>
<div class="meta-line">Authors: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti</div>
<div class="meta-line">First: 2024-04-04T06:24:11+00:00 · Latest: 2026-01-19T02:09:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.03227v4">Abs</a> · <a href="https://arxiv.org/pdf/2404.03227v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study real-time sampling and estimation of autoregressive Markovian sources in decentralized and dynamic multi-hop networks that share similar structures. Nodes cache neighboring samples and communicate over wireless collision channels. The objective is to minimize the time-average estimation error and/or the age of information under decentralized policies, which we address by developing a unified graphical multi-agent reinforcement learning framework. A key feature of the framework is its transferability, enabled by the fact that the number of trainable parameters is independent of the number of agents, allowing a learned policy to be directly deployed on dynamic yet structurally similar graphs without re-training. Building on this design, we establish rigorous theoretical guarantees on the transferability of the resulting policies. Numerical experiments demonstrate that (i) our method outperforms state-of-the-art baselines on dynamic graphs; (ii) the trained policies transfer well to larger networks, with performance gains increasing with the number of nodes; and (iii) incorporating recurrence is crucial, enhancing resilience to non-stationarity in both independent learning and centralized training with decentralized execution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向动态无线网络实时估计的可迁移图多智能体强化学习</div>
<div class="mono" style="margin-top:8px">本研究针对具有相似结构的去中心化动态多跳网络，研究自回归马尔可夫信源的实时采样与估计问题。节点缓存相邻样本并通过无线碰撞信道通信，目标是在去中心化策略下最小化时间平均估计误差和/或信息年龄。我们通过构建统一的图多智能体强化学习框架解决该问题，其核心特性是可迁移性——可训练参数数量与智能体数量无关，使得习得策略可直接部署于结构相似的动态图而无需重新训练。基于此设计，我们为策略迁移性建立了严格的理论保证。数值实验表明：（1）本方法在动态图上优于现有基线；（2）训练策略能有效迁移至更大规模网络，且性能增益随节点数增加而提升；（3）引入循环机制对增强非平稳环境下的鲁棒性至关重要，在独立学习与去中心化执行的集中训练中均有效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for efficient real-time estimation in decentralized, dynamic multi-hop wireless networks where nodes sample autoregressive Markovian sources and communicate over collision channels. The authors propose a unified graphical multi-agent reinforcement learning (MARL) framework designed to minimize time-average estimation error or age of information under decentralized policies; a key innovation is its transferability, as the number of trainable parameters is independent of the agent count, allowing policies learned on one graph to be deployed on structurally similar dynamic graphs without retraining. Experimental results show the method outperforms state-of-the-art baselines on dynamic graphs, transfers effectively to larger networks with increasing performance gains, and demonstrates that incorporating recurrence enhances resilience to non-stationarity in both independent and centralized training setups.</div>
<div class="mono" style="margin-top:8px">本文旨在解决去中心化动态多跳无线网络中实时估计效率低下的问题，其中节点对自回归马尔可夫源进行采样并通过碰撞信道通信。作者提出了一种统一的图多智能体强化学习框架，旨在最小化时间平均估计误差或信息年龄，其关键创新在于可迁移性：可训练参数数量与智能体数量无关，使得在一种图上学习的策略可直接部署于结构相似的动态图上而无需重新训练。实验结果表明，该方法在动态图上优于现有基线，能有效迁移到更大网络且性能增益随节点数增加而提升，同时引入循环机制增强了在独立学习和集中训练分散执行中对非平稳性的适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models</div>
<div class="meta-line">Authors: Xiangyu Shi, Junyang Ding, Xu Zhao, Sinong Zhan, Payal Mohapatra, Daniel Quispe, Kojo Welbeck, Jian Cao, Wei Chen, Ping Guo, Qi Zhu</div>
<div class="meta-line">First: 2026-01-19T01:10:49+00:00 · Latest: 2026-01-19T01:10:49+00:00</div>
<div class="meta-line">Comments: Accepted to the Design, Automation &amp; Test in Europe Conference (DATE) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12641v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12641v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STEP-LLM：基于大语言模型从自然语言生成CAD STEP模型</div>
<div class="mono" style="margin-top:8px">计算机辅助设计（CAD）是现代制造业的关键，但模型创建仍依赖大量人工与专业知识。为使非专业人士能将直观设计意图转化为可制造实体，近期基于大语言模型的文本转CAD研究多聚焦于指令序列或CadQuery等脚本格式。然而这些格式依赖特定内核，缺乏制造通用性。相比之下，产品数据交换标准（STEP，ISO 10303）文件作为广泛采用的中性边界表示格式，可直接兼容制造流程，但其图结构、交叉引用的特性对自回归大语言模型构成独特挑战。为此，我们构建了包含约4万组STEP-描述对的数据集，并针对STEP的图结构格式提出创新预处理方案：包括基于深度优先搜索的重新序列化方法（在保持局部性的同时线性化交叉引用）以及思维链风格的结构标注（引导全局一致性）。我们集成检索增强生成技术，通过相关样本监督微调增强预测可靠性，并采用基于倒角距离的几何奖励函数进行强化学习以优化生成质量。实验表明，STEP-LLM在几何保真度上持续超越Text2CAD基线，改进源于框架的多阶段设计：检索增强模块显著提升完整性与可渲染性，基于深度优先搜索的序列化增强整体准确性，强化学习进一步降低几何误差。量化指标与可视化对比均证实STEP-LLM能生成更高保真度的几何形状。这些成果证明了基于大语言模型的自然语言STEP模型生成可行性，展现了其推动CAD设计制造民主化的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to democratize CAD design for manufacturing and overcome the limitations of kernel-dependent script formats, this paper introduces STEP-LLM, a method for generating universal STEP CAD models from natural language using large language models. The approach involves curating a dataset of STEP-caption pairs and developing novel preprocessing techniques, including a depth-first search-based reserialization to linearize STEP&#x27;s graph structure and chain-of-thought annotations for coherence, combined with retrieval-augmented generation for supervised fine-tuning and reinforcement learning with a Chamfer Distance-based reward. Experimental results show that STEP-LLM consistently outperforms the Text2CAD baseline in geometric fidelity, with improvements attributed to the reserialization, retrieval module, and reinforcement learning, demonstrating the feasibility of high-fidelity text-to-STEP generation.</div>
<div class="mono" style="margin-top:8px">本文旨在降低CAD设计的专业门槛，解决现有基于脚本的文本到CAD方法因内核依赖而缺乏通用性的问题，提出了STEP-LLM，一种利用大语言模型从自然语言生成通用STEP CAD模型的方法。该方法通过构建STEP-描述对数据集，并引入针对STEP图结构的预处理技术，包括基于深度优先搜索的重新序列化以线性化交叉引用和思维链式结构注释，结合检索增强生成进行监督微调，以及使用基于倒角距离奖励的强化学习。实验结果表明，STEP-LLM在几何保真度上持续优于Text2CAD基线，改进源于重新序列化、检索模块和强化学习，验证了从文本生成高保真STEP模型的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">A Deep Reinforcement Learning-Based TCP Congestion Control Algorithm: Design, Simulation, and Evaluation</div>
<div class="meta-line">Authors: Efe Ağlamazlar, Emirhan Eken, Harun Batur Geçici</div>
<div class="meta-line">First: 2025-08-01T20:00:17+00:00 · Latest: 2026-01-19T00:27:29+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 figures. Presents a DRL agent that mitigates bufferbloat and achieves near-zero packet loss. Validated via NS-3 simulations under a strict training-testing protocol. Code: https://github.com/aglamazlarefe/DRL-TCP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01047v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.01047v3">PDF</a> · <a href="https://github.com/aglamazlarefe/DRL-TCP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a Deep Reinforcement Learning (DRL) based TCP congestion-control algorithm that uses a Deep Q-Network (DQN) to adapt the congestion window (cWnd) dynamically based on observed network state. The proposed approach utilizes DQNs to optimize the congestion window by observing key network parameters and taking real-time actions. The algorithm is trained and evaluated within the NS-3 network simulator using the OpenGym interface. The results demonstrate that the DRL-based algorithm provides a superior balance between throughput and latency compared to both traditional TCP New Reno and TCP Cubic algorithms. Specifically: Compared to TCP Cubic, the DRL algorithm achieved comparable throughput (statistically insignificant difference of -3.79%, $p&gt;0.05$) while delivering a massive 46.29% reduction in Round-Trip Time (RTT). Furthermore, the DRL agent maintained near-zero packet loss, whereas Cubic suffered from significant buffer overflow. Compared to TCP New Reno, the DRL algorithm achieved comparable throughput (+0.38%) with a 32.40% reduction in RTT. Results from NS-3 simulations indicate that the proposed DRL agent effectively mitigates bufferbloat without compromising bandwidth utilization. This study emphasizes the potential of reinforcement learning techniques for solving complex congestion control problems in modern networks by learning the network capacity rather than saturating it.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的TCP拥塞控制算法：设计、仿真与评估</div>
<div class="mono" style="margin-top:8px">本文提出一种基于深度强化学习（DRL）的TCP拥塞控制算法，该算法利用深度Q网络（DQN）根据观测到的网络状态动态调整拥塞窗口（cWnd）。该方法通过观察关键网络参数并采取实时动作，使用DQN优化拥塞窗口。算法在NS-3网络仿真器中通过OpenGym接口进行训练与评估。结果表明，相较于传统TCP New Reno和TCP Cubic算法，该DRL算法在吞吐量与延迟之间实现了更优的平衡。具体而言：与TCP Cubic相比，DRL算法在保持相近吞吐量（统计差异不显著，-3.79%，$p&gt;0.05$）的同时，大幅降低往返时延（RTT）达46.29%。此外，DRL智能体维持了近零丢包率，而Cubic算法则出现严重的缓冲区溢出。与TCP New Reno相比，DRL算法在吞吐量相近（+0.38%）的情况下将RTT降低了32.40%。NS-3仿真结果表明，所提出的DRL智能体能在不影响带宽利用率的前提下有效缓解缓冲区膨胀。本研究强调强化学习技术通过主动学习网络容量而非盲目饱和链路，为解决现代网络中复杂拥塞控制问题提供了新思路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve TCP congestion control beyond traditional heuristic algorithms, this paper proposes a Deep Reinforcement Learning (DRL) method using a Deep Q-Network (DQN) to dynamically adjust the congestion window based on real-time network observations. The method is implemented and trained within the NS-3 simulator via the OpenGym interface. Experimental results demonstrate that the DRL-based algorithm achieves a superior trade-off between throughput and latency compared to TCP Cubic and TCP New Reno, notably reducing Round-Trip Time by 46.29% against Cubic with comparable throughput and maintaining near-zero packet loss, effectively mitigating bufferbloat without sacrificing bandwidth utilization.</div>
<div class="mono" style="margin-top:8px">本文旨在改进传统启发式TCP拥塞控制算法，提出了一种基于深度强化学习的方法，利用深度Q网络根据实时网络状态动态调整拥塞窗口。该方法通过OpenGym接口在NS-3模拟器中实现和训练。实验结果表明，与TCP Cubic和TCP New Reno相比，该基于深度强化学习的算法在吞吐量和延迟之间取得了更优的平衡，特别是与Cubic相比，在吞吐量相当的情况下将往返时间降低了46.29%，并保持了接近零的丢包率，有效缓解了缓冲区膨胀且未牺牲带宽利用率。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Robust Universal Perturbation Attacks: A Float-Coded, Penalty-Driven Evolutionary Approach</div>
<div class="meta-line">Authors: Shiqi Wang, Mahdi Khosravy, Neeraj Gupta, Olaf Witkowski</div>
<div class="meta-line">First: 2026-01-18T23:31:12+00:00 · Latest: 2026-01-18T23:31:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12624v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12624v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Universal adversarial perturbations (UAPs) have garnered significant attention due to their ability to undermine deep neural networks across multiple inputs using a single noise pattern. Evolutionary algorithms offer a promising approach to generating such perturbations due to their ability to navigate non-convex, gradient-free landscapes. In this work, we introduce a float-coded, penalty-driven single-objective evolutionary framework for UAP generation that achieves lower visibility perturbations while enhancing attack success rates. Our approach leverages continuous gene representations aligned with contemporary deep learning scales, incorporates dynamic evolutionary operators with adaptive scheduling, and utilizes a modular PyTorch implementation for seamless integration with modern architectures. Additionally, we ensure the universality of the generated perturbations by testing across diverse models and by periodically switching batches to prevent overfitting. Experimental results on the ImageNet dataset demonstrate that our framework consistently produces perturbations with smaller norms, higher misclassification effectiveness, and faster convergence compared to existing evolutionary-based methods. These findings highlight the robustness and scalability of our approach for universal adversarial attacks across various deep learning architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向鲁棒通用扰动攻击：一种浮点编码、惩罚驱动的进化方法</div>
<div class="mono" style="margin-top:8px">通用对抗扰动因其能够通过单一噪声模式破坏多个输入上的深度神经网络而受到广泛关注。进化算法因其在非凸、无梯度空间中的探索能力，为生成此类扰动提供了有前景的途径。本研究提出一种用于生成通用对抗扰动的浮点编码、惩罚驱动的单目标进化框架，该框架在提高攻击成功率的同时实现了更低可见性的扰动。我们的方法采用与当代深度学习规模匹配的连续基因表示，结合具有自适应调度的动态进化算子，并利用模块化的PyTorch实现以无缝集成现代架构。此外，通过在不同模型上进行测试并定期切换批次以防止过拟合，我们确保了生成扰动的通用性。在ImageNet数据集上的实验结果表明，与现有基于进化的方法相比，我们的框架持续产生具有更小范数、更高误分类效率和更快收敛速度的扰动。这些发现凸显了我们的方法在各种深度学习架构上进行通用对抗攻击的鲁棒性和可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for effective universal adversarial perturbations (UAPs) that can deceive deep neural networks across multiple inputs with minimal visibility, this paper proposes a float-coded, penalty-driven evolutionary framework to generate such perturbations. The method employs continuous gene representations compatible with modern deep learning scales, dynamic evolutionary operators with adaptive scheduling, and a modular PyTorch implementation to avoid overfitting by periodically switching batches during training. Experimental results on ImageNet show that the approach yields perturbations with smaller norms, higher attack success rates, and faster convergence compared to existing evolutionary methods, demonstrating robustness and scalability across various architectures.</div>
<div class="mono" style="margin-top:8px">本研究旨在生成高效且隐蔽的通用对抗扰动（UAP），以使用单一噪声模式欺骗多个深度神经网络输入，为此提出了一种基于浮点编码、惩罚驱动的单目标进化框架。该方法采用与当代深度学习规模匹配的连续基因表示，结合自适应调度的动态进化算子，并通过周期性切换训练批次防止过拟合，利用模块化PyTorch实现与现代架构无缝集成。在ImageNet数据集上的实验表明，与现有进化方法相比，该框架生成的扰动具有更小的范数、更高的攻击成功率及更快的收敛速度，凸显了其在多种深度学习架构中攻击的鲁棒性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Wei Zhu, Abirath Raju, Abdulaziz Shamsah, Anqi Wu, Seth Hutchinson, Ye Zhao</div>
<div class="meta-line">First: 2025-03-16T15:11:57+00:00 · Latest: 2026-01-18T23:29:33+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12538v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12538v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://gatech-lidar.github.io/emobipednav.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents an emotion-aware navigation framework -- EmoBipedNav -- using deep reinforcement learning (DRL) for bipedal robots walking in socially interactive environments. The inherent locomotion constraints of bipedal robots challenge their safe maneuvering capabilities in dynamic environments. When combined with the intricacies of social environments, including pedestrian interactions and social cues, such as emotions, these challenges become even more pronounced. To address these coupled problems, we propose a two-stage pipeline that considers both bipedal locomotion constraints and complex social environments. Specifically, social navigation scenarios are represented using sequential LiDAR grid maps (LGMs), from which we extract latent features, including collision regions, emotion-related discomfort zones, social interactions, and the spatio-temporal dynamics of evolving environments. The extracted features are directly mapped to the actions of reduced-order models (ROMs) through a DRL architecture. Furthermore, the proposed framework incorporates full-order dynamics and locomotion constraints during training, effectively accounting for tracking errors and restrictions of the locomotion controller while planning the trajectory with ROMs. Comprehensive experiments demonstrate that our approach exceeds both model-based planners and DRL-based baselines. The hardware videos and open-source code are available at https://gatech-lidar.github.io/emobipednav.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EmoBipedNav：基于深度强化学习的双足机器人情感感知社交导航</div>
<div class="mono" style="margin-top:8px">本研究提出一种情感感知导航框架——EmoBipedNav，利用深度强化学习（DRL）实现双足机器人在社交交互环境中的行走。双足机器人固有的运动约束对其在动态环境中的安全机动能力构成挑战。当结合社交环境的复杂性（包括行人交互和情感等社交线索）时，这些挑战更为突出。为解决这些耦合问题，我们提出一个两阶段流程，同时考虑双足运动约束与复杂社交环境。具体而言，社交导航场景通过序列化激光雷达网格地图（LGM）表示，从中提取潜在特征，包括碰撞区域、情感相关不适区、社交交互以及演化环境的时空动态。提取的特征通过DRL架构直接映射至降阶模型（ROM）的动作空间。此外，所提框架在训练中融入全阶动力学与运动约束，在利用ROM规划轨迹时有效兼顾跟踪误差与运动控制器的限制。综合实验表明，本方法在模型规划器与DRL基线方法上均表现更优。硬件演示视频与开源代码详见：https://gatech-lidar.github.io/emobipednav.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces EmoBipedNav, an emotion-aware navigation framework for bipedal robots operating in social environments, motivated by the need to address both the inherent locomotion constraints of bipedal robots and the complexities of dynamic social interactions, including pedestrian emotions. The method employs a two-stage deep reinforcement learning pipeline that extracts latent features—such as collision zones, emotion-related discomfort areas, and social dynamics—from sequential LiDAR grid maps and maps them to actions via reduced-order models, while incorporating full-order dynamics and locomotion constraints during training to ensure feasible trajectories. Experimental results show that the proposed approach outperforms both model-based planners and existing DRL baselines in navigation performance.</div>
<div class="mono" style="margin-top:8px">本文提出了EmoBipedNav，一种用于双足机器人在社交环境中导航的情感感知框架，其动机在于解决双足机器人固有的运动约束以及动态社交互动（包括行人情感）的复杂性。该方法采用两阶段深度强化学习流程，从序列激光雷达网格地图中提取潜在特征（如碰撞区域、情感相关不适区和社会动态），并通过降阶模型将其映射为动作，同时在训练中融入全阶动力学和运动约束以确保轨迹可行性。实验结果表明，该方法在导航性能上优于基于模型的规划器和现有的深度强化学习基线。</div>
</details>
</div>
<div class="card">
<div class="title">Fast Two-Time-Scale Stochastic Gradient Method with Applications in Reinforcement Learning</div>
<div class="meta-line">Authors: Sihan Zeng, Thinh T. Doan</div>
<div class="meta-line">First: 2024-05-15T19:03:08+00:00 · Latest: 2026-01-18T21:10:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.09660v4">Abs</a> · <a href="https://arxiv.org/pdf/2405.09660v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two-time-scale optimization is a framework introduced in Zeng et al. (2024) that abstracts a range of policy evaluation and policy optimization problems in reinforcement learning (RL). Akin to bi-level optimization under a particular type of stochastic oracle, the two-time-scale optimization framework has an upper level objective whose gradient evaluation depends on the solution of a lower level problem, which is to find the root of a strongly monotone operator. In this work, we propose a new method for solving two-time-scale optimization that achieves significantly faster convergence than the prior arts. The key idea of our approach is to leverage an averaging step to improve the estimates of the operators in both lower and upper levels before using them to update the decision variables. These additional averaging steps eliminate the direct coupling between the main variables, enabling the accelerated performance of our algorithm. We characterize the finite-time convergence rates of the proposed algorithm under various conditions of the underlying objective function, including strong convexity, Polyak-Lojasiewicz condition, and general non-convexity. These rates significantly improve over the best-known complexity of the standard two-time-scale stochastic approximation algorithm. When applied to RL, we show how the proposed algorithm specializes to novel online sample-based methods that surpass or match the performance of the existing state of the art. Finally, we support our theoretical results with numerical simulations in RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>快速双时间尺度随机梯度方法及其在强化学习中的应用</div>
<div class="mono" style="margin-top:8px">双时间尺度优化是Zeng等人（2024）提出的框架，用于抽象强化学习中的策略评估与策略优化问题。该框架类似于特定随机预言下的双层优化，其上层目标函数的梯度计算依赖于下层问题的解——即强单调算子的求根问题。本文提出一种新的双时间尺度优化求解方法，其收敛速度显著优于现有技术。核心思路是在更新决策变量前，通过平均步骤改进上下层算子的估计值。这些额外的平均步骤消除了主变量间的直接耦合，从而实现算法加速。我们在目标函数的不同条件下（包括强凸性、Polyak-Lojasiewicz条件和一般非凸性）刻画了所提算法的有限时间收敛速率，这些速率显著优于标准双时间尺度随机逼近算法的最优已知复杂度。应用于强化学习时，我们展示了该算法如何具体化为新型在线样本方法，其性能超越或匹配现有最优方法。最后通过强化学习的数值模拟验证了理论结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient algorithms in reinforcement learning (RL) policy evaluation and optimization, which can be abstracted as two-time-scale optimization problems, this paper introduces a novel stochastic gradient method that accelerates convergence by incorporating averaging steps to decouple variable updates. The method improves operator estimates at both upper and lower levels before updating decision variables, leading to faster rates under conditions including strong convexity, Polyak-Lojasiewicz, and non-convexity. Experimental results in RL applications demonstrate that the algorithm outperforms or matches state-of-the-art methods, supported by theoretical convergence guarantees and numerical simulations.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的策略评估与优化问题，这些问题可抽象为双时间尺度优化，提出了一种新颖的随机梯度方法，通过引入平均步骤来解耦变量更新以加速收敛。该方法在更新决策变量前改进上下层算子的估计，从而在强凸性、Polyak-Lojasiewicz条件和非凸性等多种情况下实现更快的收敛速度。在强化学习应用中的实验结果表明，该算法超越或匹配现有最优方法，并通过理论收敛保证和数值模拟得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Primate-like perceptual decision making emerges through deep recurrent reinforcement learning</div>
<div class="meta-line">Authors: Nathan J. Wispinski, Scott A. Stone, Anthony Singhal, Patrick M. Pilarski, Craig S. Chapman</div>
<div class="meta-line">First: 2026-01-18T20:43:53+00:00 · Latest: 2026-01-18T20:43:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12577v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12577v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过深度循环强化学习实现类灵长类感知决策机制</div>
<div class="mono" style="margin-top:8px">研究已深入揭示灵长类决策的神经机制，但其演化动因尚不明确。理论认为，灵长类决策机制及其行为能力是在处理动态噪声信息时，为最大化奖赏而演化形成的。为验证该理论，我们采用强化学习训练端到端深度循环神经网络，使其完成噪声感知辨别任务。网络习得了类灵长类决策的关键能力：速度与准确度的权衡、依据新信息灵活修正决策。网络内部动态机制与灵长类神经生理学研究发现的决策机制高度相似。这些结果为解释灵长类灵活决策能力的演化动因提供了实验依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to understand why primates evolved specific neural mechanisms for decision making, this study tests the theory that such mechanisms optimize reward under noisy, time-varying conditions. The method involved training a deep recurrent neural network with reinforcement learning on a perceptual discrimination task, which learned to trade off speed versus accuracy and to flexibly revise decisions as new information arrived. Experimental results showed that the network&#x27;s internal dynamics closely resembled primate neural activity, providing support for the idea that these decision-making abilities emerge from reward maximization pressures.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究灵长类动物决策神经机制为何演化形成，其动机源于验证这些机制是为在噪声和时变信息中最大化奖励的理论。方法上，通过强化学习训练深度循环神经网络完成感知辨别任务，网络学会了权衡速度与准确性，并能根据新信息灵活改变决策。实验结果表明，网络的内部动态与灵长类神经生理学观察到的决策机制相似，从而支持了灵活决策能力源于奖励最大化压力的观点。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
