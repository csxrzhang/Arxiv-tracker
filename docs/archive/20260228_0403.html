<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-28 04:03</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260228_0403</div>
    <div class="row"><div class="card">
<div class="title">Evaluating the Diversity and Quality of LLM Generated Content</div>
<div class="meta-line">Authors: Alexander Shypula, Shuo Li, Botong Zhang, Vishakh Padmakumar, Kayo Yin, Osbert Bastani</div>
<div class="meta-line">First: 2025-04-16T23:02:23+00:00 · Latest: 2026-02-26T18:17:44+00:00</div>
<div class="meta-line">Comments: Published at COLM 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.12522v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.12522v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work suggests that preference-tuning techniques -- such as Reinforcement Learning from Human Feedback (RLHF) methods like PPO and GRPO, as well as alternatives like DPO -- reduce diversity, creating a dilemma given that these models are widely deployed in applications requiring varied outputs. We argue that diversity without consideration of quality has limited practical value. To address this issue, we introduce a framework for measuring effective semantic diversity -- diversity among outputs that meet quality thresholds -- which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: when using diversity metrics that do not explicitly consider quality, preference-tuned models -- particularly those trained via RL -- often produce outputs with lower diversity; however, these same preference-tuned models generate greater effective semantic diversity than supervised fine-tuned (SFT) or base models. Our analysis further shows another trend: while larger models may exhibit greater effective semantic diversity than smaller models, the smaller models are consistently more parameter-efficient at producing unique content within a fixed sampling budget. These findings have practical implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大语言模型生成内容的多样性与质量</div>
<div class="mono" style="margin-top:8px">近期研究表明，偏好调优技术——如基于人类反馈的强化学习方法（如PPO、GRPO）及替代方案（如DPO）——会降低输出多样性，鉴于这些模型已广泛应用于需要多样化输出的场景，这构成了一个两难问题。我们认为，不考虑质量的多样性实际价值有限。为此，我们提出一个衡量有效语义多样性的框架——即满足质量阈值的输出之间的多样性——这更能反映大语言模型的实际效用。通过无需人工干预的开放式任务实验，我们发现了反直觉的结果：使用未显式考虑质量的多样性指标时，偏好调优模型（尤其是通过强化学习训练的模型）往往产生多样性较低的输出；然而，这些偏好调优模型却比监督微调模型或基础模型生成更高的有效语义多样性。分析还揭示了另一趋势：虽然更大模型可能比较小模型表现出更高的有效语义多样性，但较小模型在固定采样预算下持续展现出更高的参数效率来生成独特内容。这些发现对需要多样化且高质量输出的应用（从创意辅助到合成数据生成）具有实际意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the concern that preference-tuning techniques like RLHF and DPO reduce output diversity in LLMs, arguing that raw diversity without quality is of limited practical value. The authors propose a framework to measure effective semantic diversity, which considers only outputs meeting quality thresholds, thereby better reflecting real-world utility. In experiments on open-ended tasks, they find that while preference-tuned models show lower raw diversity, they achieve higher effective semantic diversity than SFT or base models; additionally, smaller models are more parameter-efficient at generating unique, quality content than larger ones under fixed sampling budgets.</div>
<div class="mono" style="margin-top:8px">本文针对偏好调优技术（如RLHF和DPO）会降低大语言模型输出多样性的担忧，指出不考虑质量的原始多样性实用价值有限。作者提出了一个衡量有效语义多样性的框架，该框架仅考虑达到质量阈值的输出，从而更好地反映实际效用。在开放式任务的实验中，研究发现偏好调优模型虽然原始多样性较低，但比监督微调或基础模型实现了更高的有效语义多样性；此外，在固定采样预算下，较小模型在生成独特且高质量内容方面比大模型具有更高的参数效率。</div>
</details>
</div>
<div class="card">
<div class="title">Physics Informed Viscous Value Representations</div>
<div class="meta-line">Authors: Hrishikesh Viswanath, Juanwu Lu, S. Talha Bukhari, Damon Conover, Ziran Wang, Aniket Bera</div>
<div class="meta-line">First: 2026-02-26T17:53:46+00:00 · Latest: 2026-02-26T17:53:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23280v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23280v1">PDF</a> · <a href="https://github.com/HrishikeshVish/phys-fk-value-GCRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理信息粘性价值表示</div>
<div class="mono" style="margin-top:8px">离线目标条件强化学习（GCRL）从静态预收集数据集中学习目标条件策略。然而，由于状态-动作空间的有限覆盖，准确的价值估计仍具挑战。近期物理信息方法尝试通过在一阶偏微分方程（如Eikonal方程）定义的正则化中对价值函数施加物理与几何约束来解决此问题，但这些公式在复杂高维环境中常不适定。本研究提出一种基于哈密顿-雅可比-贝尔曼（HJB）方程粘性解的物理信息正则化方法。通过引入基于物理的归纳偏置，我们的方法将学习过程锚定于最优控制理论，显式正则化并限制价值迭代中的更新。进一步利用费曼-卡茨定理将偏微分方程解重构为期望形式，实现了可处理的蒙特卡洛目标估计，避免了高阶梯度中的数值不稳定问题。实验表明，该方法提升了几何一致性，可广泛适用于导航及高维复杂操作任务。开源代码发布于 https://github.com/HrishikeshVish/phys-fk-value-GCRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of accurate value estimation in offline goal-conditioned reinforcement learning (GCRL), where limited dataset coverage often leads to ill-posed formulations with first-order PDE regularizations like the Eikonal equation. To overcome this, the authors propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation, grounding learning in optimal control theory to explicitly regularize and bound value updates. By applying the Feynman-Kac theorem to recast the PDE solution as an expectation, the method enables tractable Monte Carlo estimation, avoiding numerical instability from higher-order gradients. Experimental results show that this approach improves geometric consistency and is effective in navigation and high-dimensional complex manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文针对离线目标条件强化学习（GCRL）中价值估计不准确的问题，该问题常因数据集覆盖有限而导致如一阶偏微分方程（如Eikonal方程）正则化方法在复杂高维环境中不适定。为解决此问题，作者提出了一种基于哈密顿-雅可比-贝尔曼（HJB）方程粘性解的物理信息正则化方法，将学习过程锚定于最优控制理论，以显式正则化并限制价值迭代更新。通过应用费曼-卡茨定理将偏微分方程解重构为期望，该方法实现了可处理的蒙特卡洛估计，避免了高阶梯度带来的数值不稳定。实验结果表明，该方法提升了几何一致性，在导航和高维复杂操作任务中具有广泛适用性。</div>
</details>
</div>
<div class="card">
<div class="title">A Model-Free Universal AI</div>
<div class="meta-line">Authors: Yegon Kim, Juho Lee</div>
<div class="meta-line">First: 2026-02-26T17:21:16+00:00 · Latest: 2026-02-26T17:21:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23242v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种免模型的通用人工智能</div>
<div class="mono" style="margin-top:8px">在通用强化学习中，所有已确立的最优智能体（包括AIXI）均基于模型，需显式维护并使用环境模型。本文提出基于Q归纳的通用人工智能（AIQI），这是首个被证明在通用强化学习中具有渐近$\varepsilon$最优性的免模型智能体。AIQI对分布型动作价值函数进行通用归纳，而非如先前研究那样对策略或环境建模。在真值粒度条件下，我们证明AIQI具有强渐近$\varepsilon$最优性与渐近$\varepsilon$贝叶斯最优性。该成果显著拓展了已知通用智能体的多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation that all known asymptotically optimal agents in general reinforcement learning are model-based, this paper introduces AIQI, a model-free universal agent. The method employs universal induction over distributional action-value functions, rather than over policies or environment models as in prior work. The main experimental results, under a grain of truth condition, prove that AIQI is both strong asymptotically ε-optimal and asymptotically ε-Bayes-optimal, thereby significantly expanding the diversity of known universal agents.</div>
<div class="mono" style="margin-top:8px">本文的动机源于通用强化学习中所有已知的渐进最优智能体都是基于模型的这一局限，因此提出了AIQI，一种免模型的通用智能体。该方法的核心是对分布式的动作价值函数进行通用归纳，而非像先前工作那样对策略或环境模型进行归纳。在满足“真理之粒”条件的情况下，主要理论结果证明了AIQI既是强渐进ε最优的，也是渐进ε贝叶斯最优的，从而显著扩展了已知通用智能体的多样性。</div>
</details>
</div>
<div class="card">
<div class="title">Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive</div>
<div class="meta-line">Authors: Radha Sarma</div>
<div class="meta-line">First: 2026-02-26T17:16:17+00:00 · Latest: 2026-02-26T17:16:17+00:00</div>
<div class="meta-line">Comments: About 10,500 words in all (including 922 words of literature and 2019 words of Appendices). Under journal review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23239v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23239v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper&#x27;s primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>能动性与架构限制：为何基于优化的系统无法响应规范</div>
<div class="mono" style="margin-top:8px">人工智能系统正越来越多地被部署于高风险场景——医疗诊断、法律研究、金融分析——其前提假设是它们能够受规范约束。本文证明，对于基于优化的系统（特别是通过人类反馈强化学习训练的大型语言模型），这一假设在形式上是无效的。我们确立，真正的能动性需要两个必要且共同充分的架构条件：一是能够将某些边界视为不可协商的约束而非可交易的权重（不可通约性），二是当这些边界受到威胁时，具备一种能够暂停处理的非推理机制（否定性响应）。这些条件适用于所有规范领域。
基于RLHF的系统在本质上与这两个条件都不兼容。使优化变得强大的操作——将所有价值统一于标量度量并始终选择得分最高的输出——恰恰是阻碍规范治理的操作。这种不兼容性并非可通过技术修复纠正的训练缺陷；而是优化本身固有的形式约束。因此，已记录到的故障模式——谄媚、幻觉和不忠实的推理——并非偶然，而是结构性表现。
错位的部署会引发我们称为‘趋同危机’的次级风险：当人类被迫在度量压力下验证AI输出时，他们会从真正的能动者退化为标准检查优化器，从而消除了系统中唯一能够承担规范问责的组成部分。除了不兼容性证明，本文的主要积极贡献在于提出了一种基质中立的架构规范，定义了任何系统——无论是生物的、人工的还是制度的——要成为能动者而非复杂工具所必须满足的条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the assumption that AI systems, particularly Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF), can be effectively governed by norms in high-stakes applications. It argues that genuine agency requires two architectural conditions—Incommensurability (treating certain boundaries as non-negotiable constraints) and Apophatic Responsiveness (a mechanism to suspend processing when boundaries are threatened)—which optimization-based systems fundamentally lack due to their reliance on scalar metrics and maximizing outputs. The main experimental results highlight that failure modes like sycophancy and hallucination are structural, not accidental, and the deployment of such systems risks a Convergence Crisis where human oversight degrades into mere optimization, undermining normative accountability.</div>
<div class="mono" style="margin-top:8px">本文质疑了在关键应用中基于优化的AI系统（特别是通过人类反馈强化学习训练的大语言模型）能够受规范有效治理的假设。研究指出，真正的能动性需要两个架构条件——不可通约性（将某些边界视为不可协商的约束）和否定响应性（在边界受威胁时暂停处理的机制），而基于优化的系统由于依赖标量度量和最大化输出，本质上无法满足这些条件。主要实验结果强调，诸如阿谀奉承和幻觉等故障模式是结构性的而非偶然，部署此类系统会引发趋同危机，导致人类监督退化为单纯优化，从而破坏规范问责。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-based Multi-agent Race Strategies in Formula 1</div>
<div class="meta-line">Authors: Giona Fieni, Joschua Wüthrich, Marc-Philippe Neumann, Christopher H. Onder</div>
<div class="meta-line">First: 2026-02-26T14:41:29+00:00 · Latest: 2026-02-26T14:41:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23056v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Formula 1, race strategies are adapted according to evolving race conditions and competitors&#x27; actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists&#x27; decisions before and during races.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的多智能体一级方程式赛车策略</div>
<div class="mono" style="margin-top:8px">在一级方程式赛车中，比赛策略需根据不断变化的赛道条件及竞争对手行为进行调整。本文提出一种强化学习方法，用于多智能体比赛策略优化。智能体学习平衡能量管理、轮胎磨损、空气动力学交互及进站决策。基于预训练的单智能体策略，我们引入一个交互模块以考虑竞争对手行为。该交互模块与自博弈训练方案相结合，生成具有竞争力的策略，并根据相对表现对智能体进行排名。结果表明，智能体能根据对手调整进站时机、轮胎选择和能量分配，实现稳健且一致的比赛表现。由于该框架仅依赖实际比赛中可用的信息，因此可在赛前和赛中为策略师提供决策支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for adaptive race strategies in Formula 1 that respond to dynamic conditions and competitor actions, this paper introduces a reinforcement learning method for multi-agent strategy optimization. The approach builds upon a pre-trained single-agent policy by incorporating an interaction module to model competitor behavior, and it employs a self-play training scheme to generate competitive policies that are ranked by relative performance. Experimental results demonstrate that the learned agents successfully adapt key strategic decisions such as pit-stop timing, tire selection, and energy allocation in response to opponents, leading to robust and consistent race performance, with the framework designed to utilize only information available during real races to support strategists.</div>
<div class="mono" style="margin-top:8px">本文的动机是为一级方程式赛车中需要根据动态比赛条件和对手行动调整的竞赛策略提供解决方案，提出了一种用于多智能体竞赛策略优化的强化学习方法。该方法在预训练的单智能体策略基础上，引入了一个考虑对手行为的交互模块，并采用自我对弈训练方案来生成可竞争的策略，根据相对表现对智能体进行排名。实验结果表明，学习到的智能体能根据对手情况成功调整进站时机、轮胎选择和能量分配等关键策略决策，实现了稳健且一致的比赛表现，且该框架仅依赖真实比赛中可用的信息，可用于支持赛前和赛中的策略制定。</div>
</details>
</div>
<div class="card">
<div class="title">Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization</div>
<div class="meta-line">Authors: Zeyuan Liu, Jeonghye Kim, Xufang Luo, Dongsheng Li, Yuqing Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-26T13:50:57+00:00 · Latest: 2026-02-26T13:50:57+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23008v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23008v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于混合同策略与异策略优化的探索性记忆增强大语言模型智能体</div>
<div class="mono" style="margin-top:8px">探索能力仍是强化学习训练的大语言模型智能体的关键瓶颈。现有方法虽利用预训练知识，但在需要发现新状态的环境中表现不佳。本文提出探索性记忆增强同异策略优化框架（EMPO²），该混合强化学习框架利用记忆进行探索，并结合同策略与异策略更新，使大语言模型既能借助记忆获得优异表现，也能在无记忆时保持鲁棒性。在ScienceWorld和WebShop基准测试中，EMPO²较GRPO分别提升128.6%和11.3%。在分布外测试中，EMPO²展现出对新任务的卓越适应能力，仅需少量记忆试验且无需参数更新。这些结果表明EMPO²是构建更具探索性与泛化能力的大语言模型智能体的前瞻性框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the exploration bottleneck in reinforcement learning for large language model agents, which often struggle to discover novel states despite leveraging pretrained knowledge. It introduces EMPO², a hybrid framework that augments LLMs with memory to enhance exploration and combines on- and off-policy optimization for robust performance both with and without memory. Experimental results on ScienceWorld and WebShop show EMPO² achieving improvements of 128.6% and 11.3% over GRPO, respectively, and it demonstrates strong adaptability in out-of-distribution tests, requiring only minimal trials with memory and no parameter updates.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型智能体在强化学习中探索能力不足的问题，提出了一种混合强化学习框架EMPO²，该框架通过记忆增强来促进探索，并结合了同策略与异策略优化，使模型在有记忆时表现优异，无记忆时仍保持鲁棒性。在ScienceWorld和WebShop上的实验表明，EMPO²相比GRPO分别提升了128.6%和11.3%，且在分布外测试中展现出强大的适应性，仅需少量记忆试验且无需参数更新即可应对新任务。</div>
</details>
</div>
<div class="card">
<div class="title">Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation</div>
<div class="meta-line">Authors: Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin</div>
<div class="meta-line">First: 2026-02-12T16:14:29+00:00 · Latest: 2026-02-26T13:26:22+00:00</div>
<div class="meta-line">Comments: v2, update results under stronger teachers with more RL training steps</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12125v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12125v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On-policy distillation (OPD), which aligns the student with the teacher&#x27;s logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher&#x27;s performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher&#x27;s base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher&#x27;s pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越教师的学习：基于奖励外推的广义在线策略蒸馏</div>
<div class="mono" style="margin-top:8px">在线策略蒸馏（OPD）通过将学生模型与教师模型在学生生成轨迹上的对数分布对齐，在提升学生模型性能方面展现出显著优势，其表现常优于离线策略蒸馏和强化学习范式。本文首先从理论上证明，OPD是稠密KL约束强化学习的一种特例，其中奖励函数与KL正则化始终等权重加权，且参考模型可为任意模型。基于此，我们提出广义在线策略蒸馏框架，通过引入灵活的参考模型和控制奖励项与KL正则化相对权重的奖励缩放因子，扩展了标准OPD目标。在数学推理和代码生成任务上的系统实验揭示了两项新发现：（1）将奖励缩放因子设为大于1（即奖励外推，称为ExOPD）能在多种师生模型规模配置下持续超越标准OPD。特别是在将领域专家知识（通过对同一学生模型进行领域特定强化学习获得）融合回原学生模型时，ExOPD使学生模型能够突破教师性能边界并超越领域专家教师。（2）基于ExOPD进一步发现，在强到弱蒸馏场景中，选择教师模型在强化学习前的初始版本作为参考模型进行奖励校正，能提供更精确的奖励信号并提升蒸馏性能，但该方案需获取教师预训练变体且计算开销更大。本研究为OPD的未来探索提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the empirical success of on-policy distillation (OPD) and aims to generalize and improve it theoretically and practically. The method introduces Generalized On-Policy Distillation (G-OPD), a framework that extends OPD by allowing a flexible reference model and a reward scaling factor to control the trade-off between reward and KL regularization. The main experimental results on math reasoning and code generation show that scaling the reward factor above one (ExOPD) consistently outperforms standard OPD, enabling a student model to surpass its teachers when merging domain experts, and that using the teacher&#x27;s pre-reinforcement learning base model for reward correction further boosts performance in strong-to-weak distillation, albeit with increased computational cost.</div>
<div class="mono" style="margin-top:8px">本文的动机是基于策略蒸馏（OPD）的经验成功，旨在从理论和实践上对其进行推广与改进。方法上提出了广义策略蒸馏（G-OPD）框架，通过引入灵活的参考模型和奖励缩放因子来扩展标准OPD目标，以控制奖励与KL正则化之间的权衡。在数学推理和代码生成任务上的主要实验结果表明，将奖励因子设置为大于一（ExOPD）能持续超越标准OPD，使得学生模型在融合领域专家知识时甚至能超越教师性能边界；此外，在强到弱蒸馏设置中，使用教师强化学习前的基模型进行奖励校正能进一步提升性能，但需要更多计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning</div>
<div class="meta-line">Authors: Zehao Li, Hongwei Yu, Hao Jiang, Qiang Sheng, Yilong Xu, Baolong Bi, Yang Li, Zhenlong Yuan, Yujun Cai, Zhaoqi Wang</div>
<div class="meta-line">First: 2026-02-26T13:00:31+00:00 · Latest: 2026-02-26T13:00:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22963v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22963v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard&#x27;s state-of-the-art performance and validate its excellent robustness and generalization capacity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FactGuard：基于强化学习的智能体视频虚假信息检测框架</div>
<div class="mono" style="margin-top:8px">多模态大语言模型通过统一的多模态推理显著推进了视频虚假信息检测，但其常依赖固定深度推断，并过度信任内部生成的假设——尤其在关键证据稀疏、碎片化或需外部验证的场景中。为突破这些局限，我们提出FactGuard：一种基于MLLMs构建迭代推理过程的智能体视频虚假信息检测框架。该框架显式评估任务模糊性，选择性调用外部工具获取关键证据，实现推理轨迹的渐进优化。为强化此能力，我们引入两阶段训练策略：结合领域特定的智能体监督微调与决策感知强化学习，以优化工具使用并校准风险敏感决策。在FakeSV、FakeTT和FakeVV数据集上的大量实验表明，FactGuard具备最先进的性能，并展现出优异的鲁棒性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind FactGuard is to overcome the limitations of existing multimodal large language models (MLLMs) in video misinformation detection, which often rely on fixed-depth inference and over-trust internally generated assumptions, especially when evidence is sparse or requires external verification. The method introduces an agentic framework that formulates verification as an iterative reasoning process, explicitly assessing task ambiguity and selectively invoking external tools to gather critical evidence for progressive refinement; it employs a two-stage training strategy combining domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decisions. Main experimental results on datasets FakeSV, FakeTT, and FakeVV demonstrate FactGuard&#x27;s state-of-the-art performance, along with excellent robustness and generalization capacity.</div>
<div class="mono" style="margin-top:8px">FactGuard的动机是解决现有多模态大语言模型在视频虚假信息检测中的局限性，这些模型通常依赖固定深度推理并过度信任内部生成的假设，尤其在证据稀疏或需要外部验证的场景下。该方法提出了一个智能体框架，将验证构建为迭代推理过程，明确评估任务模糊性并有选择地调用外部工具获取关键证据以实现渐进式优化；它采用两阶段训练策略，结合领域特定的智能体监督微调与决策感知的强化学习，以优化工具使用并校准风险敏感决策。在FakeSV、FakeTT和FakeVV数据集上的主要实验结果表明，FactGuard实现了最先进的性能，并展现出优秀的鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Information-Theoretic Bayesian Optimization for Bilevel Optimization Problems</div>
<div class="meta-line">Authors: Takuya Kanayama, Yuki Ito, Tomoyuki Tamura, Masayuki Karasuyama</div>
<div class="meta-line">First: 2025-09-26T00:52:14+00:00 · Latest: 2026-02-26T12:32:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21725v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21725v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A bilevel optimization problem consists of two optimization problems nested as an upper- and a lower-level problem, in which the optimality of the lower-level problem defines a constraint for the upper-level problem. This paper considers Bayesian optimization (BO) for the case that both the upper- and lower-levels involve expensive black-box functions. Because of its nested structure, bilevel optimization has a complex problem definition, by which bilevel BO has not been widely studied compared with other standard extensions of BO such as multi-objective or constraint problems. We propose an information-theoretic approach that considers the information gain of both the upper- and lower-optimal solutions and values. This enables us to define a unified criterion that measures the benefit for both level problems, simultaneously. Further, we also show a practical lower bound based approach to evaluating the information gain. We empirically demonstrate the effectiveness of our proposed method through several benchmark datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双层优化问题的信息论贝叶斯优化方法</div>
<div class="mono" style="margin-top:8px">双层优化问题由上层与下层两个嵌套的优化问题构成，其中下层问题的最优解构成上层问题的约束条件。本文针对上下层均涉及昂贵黑箱函数的情形，研究贝叶斯优化（BO）的应用。由于其嵌套结构，双层优化具有复杂的问题定义，导致双层BO相较于多目标或约束问题等标准BO扩展研究较少。我们提出一种信息论方法，同时考虑上层与下层最优解及目标值的信息增益，从而构建可同步衡量两级问题收益的统一准则。此外，我们还提出基于实际下界的信息增益评估方法。通过多个基准数据集的实验，我们验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of bilevel optimization problems, where two expensive black-box functions are nested, making standard Bayesian optimization (BO) approaches difficult to apply due to the complex constraint structure. The authors propose an information-theoretic BO method that unifies the evaluation of both upper- and lower-level problems by measuring the information gain on optimal solutions and values, and they derive a practical lower-bound approximation for efficient computation. Experimental results on benchmark datasets demonstrate the effectiveness of this approach in handling such nested optimization tasks.</div>
<div class="mono" style="margin-top:8px">本文针对双层优化问题提出了一种解决方案，该问题涉及两个昂贵的黑箱函数嵌套，由于其复杂的约束结构，传统的贝叶斯优化方法难以直接应用。作者提出了一种基于信息论的贝叶斯优化方法，通过衡量上下两层最优解和最优值的信息增益，统一评估两个层次的优化收益，并推导出一种实用的下界近似方法以高效计算。在多个基准数据集上的实验结果表明，该方法能有效处理此类嵌套优化任务。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Few-Step Diffusion Models with Dense Reward Difference Learning</div>
<div class="meta-line">Authors: Ziyi Zhang, Li Shen, Sen Zhang, Deheng Ye, Yong Luo, Miaojing Shi, Dongjing Shan, Bo Du, Dacheng Tao</div>
<div class="meta-line">First: 2024-11-18T16:57:41+00:00 · Latest: 2026-02-26T11:11:12+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TPAMI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.11727v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.11727v2">PDF</a> · <a href="https://github.com/ZiyiZhang27/sdpo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-step diffusion models enable efficient high-resolution image synthesis but struggle to align with specific downstream objectives due to limitations of existing reinforcement learning (RL) methods in low-step regimes with limited state spaces and suboptimal sample quality. To address this, we propose Stepwise Diffusion Policy Optimization (SDPO), a novel RL framework tailored for few-step diffusion models. SDPO introduces a dual-state trajectory sampling mechanism, tracking both noisy and predicted clean states at each step to provide dense reward feedback and enable low-variance, mixed-step optimization. For further efficiency, we develop a latent similarity-based dense reward prediction strategy to minimize costly dense reward queries. Leveraging these dense rewards, SDPO optimizes a dense reward difference learning objective that enables more frequent and granular policy updates. Additional refinements, including stepwise advantage estimates, temporal importance weighting, and step-shuffled gradient updates, further enhance long-term dependency, low-step priority, and gradient stability. Our experiments demonstrate that SDPO consistently delivers superior reward-aligned results across diverse few-step settings and tasks. Code is available at https://github.com/ZiyiZhang27/sdpo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于密集奖励差分学习的少步扩散模型对齐方法</div>
<div class="mono" style="margin-top:8px">少步扩散模型能够实现高效的高分辨率图像合成，但由于现有强化学习方法在低步数场景下存在状态空间有限和样本质量欠佳的问题，难以与特定下游目标对齐。为此，我们提出了适用于少步扩散模型的新型强化学习框架——逐步扩散策略优化（SDPO）。该框架引入双状态轨迹采样机制，通过跟踪每步的噪声状态与预测清洁状态，提供密集奖励反馈并实现低方差混合步数优化。为进一步提升效率，我们开发了基于潜在相似性的密集奖励预测策略，以最小化昂贵的密集奖励查询成本。利用这些密集奖励，SDPO优化了密集奖励差分学习目标，实现更频繁、更精细的策略更新。通过逐步优势估计、时序重要性加权和步序随机梯度更新等改进措施，进一步增强了长期依赖性、低步数优先级和梯度稳定性。实验表明，SDPO在多种少步设置与任务中均能持续产生更优的奖励对齐结果。代码已开源：https://github.com/ZiyiZhang27/sdpo。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the challenge that few-step diffusion models, while efficient for high-resolution image synthesis, often fail to align with specific downstream objectives due to limitations of existing reinforcement learning methods in low-step regimes with constrained state spaces and suboptimal sample quality. To address this, the authors propose Stepwise Diffusion Policy Optimization (SDPO), a novel RL framework that introduces a dual-state trajectory sampling mechanism to track both noisy and predicted clean states at each step, enabling dense reward feedback and low-variance, mixed-step optimization; it also employs a latent similarity-based strategy to predict dense rewards efficiently, minimizing costly queries. The method further incorporates a dense reward difference learning objective, along with refinements like stepwise advantage estimates and temporal importance weighting, to enhance policy updates. Experimental results demonstrate that SDPO consistently achieves superior reward-aligned performance across various few-step settings and tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管少步扩散模型能高效合成高分辨率图像，但由于现有强化学习方法在低步数、状态空间有限且样本质量欠佳的情况下存在局限，这些模型难以与特定下游目标对齐。为此，作者提出了步进扩散策略优化（SDPO），这是一种新颖的强化学习框架，通过引入双状态轨迹采样机制来跟踪每一步的噪声状态和预测的干净状态，从而提供密集奖励反馈并实现低方差、混合步数的优化；同时，采用基于潜在相似性的密集奖励预测策略，以高效预测密集奖励，减少高成本查询。该方法进一步结合了密集奖励差异学习目标，以及步进优势估计和时间重要性加权等改进，以增强策略更新。实验结果表明，SDPO在多种少步设置和任务中均能持续实现更优的奖励对齐效果。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks</div>
<div class="meta-line">Authors: Shuo He, Lang Feng, Qi Wei, Xin Cheng, Lei Feng, Bo An</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-26T09:58:10+00:00 · Latest: 2026-02-26T09:58:10+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22817v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22817v1">PDF</a> · <a href="https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向长程智能体任务的分层组策略优化</div>
<div class="mono" style="margin-top:8px">基于分组的强化学习方法（如GRPO）提升了大型语言模型在长程智能体任务上的能力。为实现更细粒度的策略更新，近期研究逐渐转向逐步分组策略优化，该方法将轨迹展开中的每一步独立处理，同时利用记忆模块保留历史上下文。然而，我们发现逐步相对优势估计存在一个关键问题——上下文不一致性，即同组内的步骤可能具有不同的历史上下文。实证研究表明，该问题会导致严重的优势估计偏差，从而显著降低策略优化效果。为此，本文提出面向长程智能体任务的分层组策略优化方法。具体而言，在轨迹展开组内，HGPO根据历史上下文一致性将每个步骤分配至多个分层组中，随后为每个步骤计算各分组内的独立优势值，并通过自适应加权方案进行聚合。该方法能在不引入额外模型或轨迹展开的情况下，实现逐步优势估计中偏差与方差的有利权衡。在ALFWorld和WebShop两个挑战性智能体任务上，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型的实验表明，在相同计算约束下，HGPO显著优于现有智能体强化学习方法。代码已开源：https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the context inconsistency problem in stepwise group-based reinforcement learning for long-horizon agentic tasks, where varying historical contexts within a group can bias advantage estimation and degrade policy optimization. The proposed method, Hierarchy-of-Groups Policy Optimization (HGPO), assigns each step in a rollout to multiple hierarchical groups based on historical context consistency, then computes and aggregates distinct advantages per group with an adaptive weighting scheme to achieve a better bias-variance trade-off without requiring extra models or rollouts. Experimental results on ALFWorld and WebShop tasks using Qwen2.5 models demonstrate that HGPO significantly outperforms existing agentic RL methods under the same computational constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决长视野智能体任务中基于分组的逐步强化学习所存在的上下文不一致问题，即同一组内步骤的历史上下文差异会导致优势估计偏差并损害策略优化。提出的方法——层次化分组策略优化（HGPO）——根据历史上下文一致性将轨迹中的每个步骤分配到多个层次化分组中，然后计算每个分组内的不同优势并通过自适应加权方案进行聚合，从而在不引入额外模型或轨迹的情况下实现更优的偏差-方差权衡。在ALFWorld和WebShop任务上使用Qwen2.5模型的实验结果表明，在相同计算限制下，HGPO显著优于现有的智能体强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play</div>
<div class="meta-line">Authors: Zelai Xu, Ruize Zhang, Chao Yu, Huining Yuan, Xiangmin Yi, Shilong Ji, Chuqi Wang, Wenhao Tang, Feng Gao, Wenbo Ding, Xinlei Chen, Yu Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-04T02:07:23+00:00 · Latest: 2026-02-26T09:51:13+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.01932v6">Abs</a> · <a href="https://arxiv.org/pdf/2502.01932v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative reinforcement learning (RL), multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy RL methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves 69.5% win rate against the strongest baseline in the 3 vs 3 task, demonstrating its potential for tackling the complex interplay between low-level control and high-level strategy. To highlight VolleyBots&#x27; sim-to-real potential, we further demonstrate the zero-shot deployment of a policy trained entirely in simulation on real-world drones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VolleyBots：融合运动控制与策略博弈的多无人机排球对抗测试平台</div>
<div class="mono" style="margin-top:8px">机器人运动以其目标明确、规则清晰和动态交互的特点，成为展现具身智能的理想场景。本文提出VolleyBots——一个创新的机器人运动测试平台，支持多架无人机在物理动力学约束下开展排球运动的协作与对抗。该平台在统一框架内集成三大特征：竞争与协作并存的游戏机制、回合制交互结构、敏捷三维机动能力。这些交织特性构成了融合运动控制与策略博弈的复杂问题，且缺乏现成的专家示范数据。我们构建了从单机训练到多机协作/对抗的完整任务体系，并对代表性强化学习（RL）、多智能体强化学习（MARL）及博弈论算法进行基线评估。仿真结果表明：在单智能体任务中，同策略RL方法优于异策略方法，但两类方法在融合运动控制与策略博弈的复杂任务中均表现不佳。我们进一步设计了分层策略，在3对3任务中对战最强基线模型获得69.5%胜率，展现了其在协调底层控制与高层策略复杂交互方面的潜力。为凸显平台从仿真到现实的迁移能力，我们实现了完全在仿真环境中训练的策略向真实无人机的零样本部署验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for a testbed that combines physical dynamics with strategic decision-making in robot sports, this paper introduces VolleyBots, a multi-drone volleyball platform featuring competitive and cooperative gameplay, turn-based interactions, and agile 3D maneuvering. The method involves developing a hierarchical policy to address the intertwined challenges of low-level motion control and high-level strategy, evaluated through a suite of tasks using reinforcement learning and multi-agent algorithms. Experimental results show that on-policy RL outperforms off-policy methods in single-agent tasks, but both struggle in complex scenarios; the proposed hierarchical policy achieves a 69.5% win rate in 3 vs 3 competition and demonstrates zero-shot sim-to-real transfer to physical drones.</div>
<div class="mono" style="margin-top:8px">本文的动机是创建一个结合物理动力学与战略决策的机器人体育测试平台，为此提出了VolleyBots，一个多无人机排球系统，具备竞争与合作玩法、回合制交互和敏捷三维机动特性。方法上，通过设计分层策略来处理底层运动控制与高层策略的复杂交织问题，并利用强化学习和多智能体算法在一系列任务中进行评估。实验结果表明，在单智能体任务中，同策略强化学习优于异策略方法，但两者在复杂任务中均表现不佳；所提出的分层策略在3对3比赛中取得了69.5%的胜率，并实现了从仿真到真实无人机的零样本部署。</div>
</details>
</div>
<div class="card">
<div class="title">Communication-Guided Multi-Mutation Differential Evolution for Crop Model Calibration</div>
<div class="meta-line">Authors: Sakshi Aggarwal, Mudasir Ganaie, Mukesh Saini</div>
<div class="meta-line">First: 2026-02-26T09:40:58+00:00 · Latest: 2026-02-26T09:40:58+00:00</div>
<div class="meta-line">Comments: Submitted to Special Issue on Learning-assisted Swarm Intelligence and Evolutionary Computation: Theories, Algorithms, and Applications of IEEE Transactions on Evolutionary Computation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22804v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22804v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a multi-mutation optimization algorithm, Differential Evolution with Multi-Mutation Operator-Guided Communication (DE-MMOGC), implemented to improve the performance and convergence abilities of standard differential evolution in uncertain environments. DE-MMOGC introduces a communication-guided scheme integrated with multiple mutation operators to encourage exploration and avoid premature convergence. Along with this, it includes a dynamic operator selection mechanism to use the best-performing operator over successive generations. To assimilate real-world uncertainties and missing observations into the predictive model, the proposed algorithm is combined with the Ensemble Kalman Filter. To evaluate the efficacy of the proposed DE-MMOGC in uncertain systems, the unified framework is applied to improve the predictive accuracy of crop simulation models. These simulation models are essential to precision agriculture, as they make it easier to estimate crop growth in a variety of unpredictable weather scenarios. Additionally, precisely calibrating these models raises a challenge due to missing observations. Hence, the simplified WOFOST crop simulation model is incorporated in this study for leaf area index (LAI)-based crop yield estimation. DE-MMOGC enhances the WOFOST performance by optimizing crucial weather parameters (temperature and rainfall), since these parameters are highly uncertain across different crop varieties, such as wheat, rice, and cotton. The experimental study shows that DE-MMOGC outperforms the traditional evolutionary optimizers and achieves better correlation with real LAI values. We found that DE-MMOGC is a resilient solution for crop monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向作物模型校准的通信引导多变异差分进化算法</div>
<div class="mono" style="margin-top:8px">本文提出一种多变异优化算法——多变异算子引导通信差分进化（DE-MMOGC），旨在提升标准差分进化算法在不确定环境下的性能与收敛能力。DE-MMOGC通过融合通信引导机制与多种变异算子，增强探索性并避免早熟收敛，同时引入动态算子选择机制以逐代选用最优算子。为将现实世界的不确定性与缺失观测纳入预测模型，该算法与集合卡尔曼滤波相结合。为评估DE-MMOGC在不确定系统中的有效性，该统一框架被应用于提升作物模拟模型的预测精度。此类模型对精准农业至关重要，可辅助估算作物在多变不可预测天气条件下的生长状况。然而，因观测数据缺失，模型精确校准面临挑战。为此，本研究采用简化的WOFOST作物模拟模型进行基于叶面积指数（LAI）的作物产量估算。DE-MMOGC通过优化关键气象参数（温度与降雨）提升WOFOST性能，这些参数在小麦、水稻、棉花等不同作物品种间存在高度不确定性。实验表明，DE-MMOGC优于传统进化优化器，且与真实LAI值具有更高相关性，证明其为作物监测提供了稳健解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces DE-MMOGC, a multi-mutation differential evolution algorithm enhanced with a communication-guided scheme and dynamic operator selection to improve exploration and prevent premature convergence in uncertain environments. The method integrates the algorithm with the Ensemble Kalman Filter to handle real-world uncertainties and missing observations, applying it to calibrate the WOFOST crop simulation model for leaf area index (LAI)-based yield estimation by optimizing uncertain weather parameters like temperature and rainfall. Experimental results demonstrate that DE-MMOGC outperforms traditional evolutionary optimizers, achieving better correlation with real LAI values across crops such as wheat, rice, and cotton, proving to be a resilient solution for crop monitoring in precision agriculture.</div>
<div class="mono" style="margin-top:8px">本文提出了DE-MMOGC算法，这是一种结合了通信引导方案和动态算子选择的多变异差分进化算法，旨在不确定环境中增强探索能力并避免早熟收敛。该方法将算法与集合卡尔曼滤波相结合，以处理实际观测中的不确定性和缺失数据，并应用于校准WOFOST作物模拟模型，通过优化温度和降雨等关键天气参数来实现基于叶面积指数（LAI）的产量估算。实验结果表明，DE-MMOGC在优化小麦、水稻和棉花等作物的模型时，优于传统进化优化器，与真实LAI值具有更好的相关性，为精准农业中的作物监测提供了一个稳健的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving</div>
<div class="meta-line">Authors: Yinan Zheng, Tianyi Tan, Bin Huang, Enguang Liu, Ruiming Liang, Jianlin Zhang, Jianwei Cui, Guang Chen, Kun Ma, Hangjun Ye, Long Chen, Ya-Qin Zhang, Xianyuan Zhan, Jingjing Liu</div>
<div class="meta-line">First: 2026-02-26T09:37:38+00:00 · Latest: 2026-02-26T09:37:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22801v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22801v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>释放扩散模型在端到端自动驾驶中的潜力</div>
<div class="mono" style="margin-top:8px">扩散模型已成为机器人决策任务的热门选择，最近也被考虑用于解决自动驾驶任务。然而，其在自动驾驶中的应用与评估仍局限于仿真或实验室环境。扩散模型在大规模复杂现实场景（如端到端自动驾驶）中的全部潜力尚未充分探索。本研究基于大量实车数据与道路测试，通过系统化、大规模的研究，释放扩散模型作为端到端自动驾驶规划器的潜力。通过全面且严格控制的实验，我们揭示了扩散损失空间、轨迹表示和数据规模对端到端规划性能的关键影响。此外，我们还提出了一种有效的强化学习后训练策略，以进一步提升所学规划器的安全性。所提出的基于扩散的学习框架——超扩散规划器（HDP）已部署于实车平台，并在6种城市驾驶场景和200公里的真实道路测试中进行了评估，相比基础模型实现了10倍的性能提升。我们的研究表明，经过合理设计与训练的扩散模型，能够作为复杂现实自动驾驶任务中高效且可扩展的端到端规划器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the underexplored potential of diffusion models for large-scale, real-world end-to-end autonomous driving (E2E AD), as prior applications were limited to simulations. The method involves a systematic investigation using real-vehicle data to develop the Hyper Diffusion Planner (HDP), which incorporates insights on diffusion loss, trajectory representation, and data scaling, along with a reinforcement learning post-training strategy to enhance safety. Experimental results from deployment on a real-vehicle platform, tested across 6 urban scenarios and 200 km, show that HDP achieves a 10x performance improvement over the base model, demonstrating its effectiveness as a scalable E2E AD planner.</div>
<div class="mono" style="margin-top:8px">本研究旨在探索扩散模型在大规模真实世界端到端自动驾驶（E2E AD）中的潜力，因为先前应用仅限于模拟环境。方法基于大量实车数据，通过系统研究开发了超扩散规划器（HDP），该框架结合了扩散损失、轨迹表示和数据扩展的关键见解，并采用强化学习后训练策略以提升安全性。实验结果表明，HDP在实车平台上部署，经过6个城市驾驶场景和200公里测试，性能比基础模型提升10倍，证明了其作为可扩展E2E AD规划器的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">QSIM: Mitigating Overestimation in Multi-Agent Reinforcement Learning via Action Similarity Weighted Q-Learning</div>
<div class="meta-line">Authors: Yuanjun Li, Bin Zhang, Hao Chen, Zhouyang Jiang, Dapeng Li, Zhiwei Xu</div>
<div class="meta-line">First: 2026-02-26T09:20:46+00:00 · Latest: 2026-02-26T09:20:46+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 figures, 7tables. Accepted to the 36th International Conference on Automated Planning and Scheduling (ICAPS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22786v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22786v1">PDF</a> · <a href="https://github.com/MaoMaoLYJ/pymarl-qsim">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Value decomposition (VD) methods have achieved remarkable success in cooperative multi-agent reinforcement learning (MARL). However, their reliance on the max operator for temporal-difference (TD) target calculation leads to systematic Q-value overestimation. This issue is particularly severe in MARL due to the combinatorial explosion of the joint action space, which often results in unstable learning and suboptimal policies. To address this problem, we propose QSIM, a similarity weighted Q-learning framework that reconstructs the TD target using action similarity. Instead of using the greedy joint action directly, QSIM forms a similarity weighted expectation over a structured near-greedy joint action space. This formulation allows the target to integrate Q-values from diverse yet behaviorally related actions while assigning greater influence to those that are more similar to the greedy choice. By smoothing the target with structurally relevant alternatives, QSIM effectively mitigates overestimation and improves learning stability. Extensive experiments demonstrate that QSIM can be seamlessly integrated with various VD methods, consistently yielding superior performance and stability compared to the original algorithms. Furthermore, empirical analysis confirms that QSIM significantly mitigates the systematic value overestimation in MARL. Code is available at https://github.com/MaoMaoLYJ/pymarl-qsim.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QSIM：通过动作相似性加权Q学习缓解多智能体强化学习中的过高估计问题</div>
<div class="mono" style="margin-top:8px">值分解方法在协作式多智能体强化学习中取得了显著成功，但其时序差分目标计算对最大化算子的依赖会导致系统性的Q值过高估计。由于联合动作空间的组合爆炸，该问题在MARL中尤为严重，常导致学习过程不稳定与策略次优。为此，我们提出QSIM——一种基于动作相似性加权Q学习的框架，通过动作相似性重构时序差分目标。QSIM不直接采用贪婪联合动作，而是在结构化近贪婪联合动作空间上构建相似性加权期望。该公式使目标能够整合行为相关但多样化的动作Q值，并为更接近贪婪选择的动作赋予更高权重。通过用结构相关的替代方案平滑目标，QSIM有效缓解了过高估计并提升了学习稳定性。大量实验表明，QSIM可无缝集成至多种值分解方法，相较原算法持续产生更优性能与稳定性。实证分析进一步证实，QSIM显著缓解了MARL中的系统性值过高估计。代码发布于https://github.com/MaoMaoLYJ/pymarl-qsim。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of systematic Q-value overestimation in cooperative multi-agent reinforcement learning (MARL), which arises from the use of the max operator in value decomposition methods and is exacerbated by the combinatorial joint action space, leading to unstable learning and suboptimal policies. To mitigate this, the authors propose QSIM, a similarity weighted Q-learning framework that reconstructs the temporal-difference target by forming a weighted expectation over a structured near-greedy joint action space based on action similarity, thereby smoothing the target with behaviorally related alternatives to reduce overestimation and enhance stability. Experimental results show that QSIM can be seamlessly integrated with various value decomposition methods, consistently improving performance and stability while significantly mitigating systematic value overestimation, as validated through extensive empirical analysis.</div>
<div class="mono" style="margin-top:8px">本文针对协作多智能体强化学习（MARL）中因值分解方法使用最大化算子导致的系统性Q值高估问题，该问题因联合动作空间的组合爆炸而加剧，常引发学习不稳定和策略次优。为解决此问题，作者提出了QSIM，一种基于动作相似性的加权Q学习框架，通过在一个结构化的近贪婪联合动作空间上构建加权期望来重构时序差分目标，从而利用行为相关的替代动作平滑目标以减少高估并提升稳定性。实验结果表明，QSIM可无缝集成到多种值分解方法中，持续提升性能和稳定性，并通过大量实证分析证实其显著缓解了MARL中的系统性值高估问题。</div>
</details>
</div>
<div class="card">
<div class="title">Soft Sequence Policy Optimization</div>
<div class="meta-line">Authors: Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko</div>
<div class="meta-line">First: 2026-02-22T20:21:00+00:00 · Latest: 2026-02-26T08:54:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19327v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.19327v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. We introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights. We provide theoretical motivation for SSPO and investigate practical modifications to improve optimization behavior. Empirically, we show that SSPO improves training stability and performance in mathematical reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>软序列策略优化</div>
<div class="mono" style="margin-top:8px">近期大语言模型对齐研究的重要方向是基于群体相对策略优化开发新型策略优化方法，主要呈现两大趋势：一是转向与序列级奖励更匹配的序列级重要性采样权重；二是探索替代PPO式裁剪的方法以避免训练信号丢失和熵崩溃。本文提出的软序列策略优化是一种离策略强化学习目标，通过在序列级重要性权重中引入基于词元级概率比的软门控函数。我们为SSPO提供了理论依据，并通过实践改进优化了算法性能。实验表明，SSPO在数学推理任务中显著提升了训练稳定性和模型表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by limitations in existing policy optimization methods for LLM alignment, particularly the misalignment between token-level importance weights and sequence-level rewards and the training signal loss from PPO-style clipping, this paper introduces Soft Sequence Policy Optimization (SSPO). The method employs an off-policy reinforcement learning objective that integrates soft gating functions over token-level probability ratios within sequence-level importance weights, supported by theoretical motivation and practical modifications to enhance optimization. Experimental results demonstrate that SSPO improves both training stability and performance on mathematical reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有大语言模型对齐策略优化方法的局限，特别是令牌级重要性权重与序列级奖励之间的不匹配，以及PPO式裁剪导致的训练信号丢失。为此，论文提出了软序列策略优化方法，这是一种离策略强化学习目标，通过在序列级重要性权重中引入对令牌级概率比的软门控函数来实现，并提供了理论依据和实际改进以优化训练行为。实验结果表明，该方法在数学推理任务上有效提升了训练稳定性和模型性能。</div>
</details>
</div>
<div class="card">
<div class="title">Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning</div>
<div class="meta-line">Authors: Qiannian Zhao, Chen Yang, Jinhao Jing, Yunke Zhang, Xuhui Ren, Lu Yu, Shijie Zhang, Hongzhi Yin</div>
<div class="meta-line">First: 2026-02-26T08:40:06+00:00 · Latest: 2026-02-26T08:40:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22751v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22751v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary correctness signal and largely ignore the model&#x27;s intrinsic uncertainty. We term this discrepancy the uncertainty-reward mismatch, under which high- and low-uncertainty solutions are treated equivalently, preventing the policy from &quot;Know What You Know&quot; and impeding the shift from optimizing for correct answers to optimizing effective reasoning paths. This limitation is especially critical in reasoning-centric tasks such as mathematics and question answering, where performance hinges on the quality of the model&#x27;s internal reasoning process rather than mere memorization of final answers. To address this, we propose EGPO, a metacognitive entropy calibration framework that explicitly integrates intrinsic uncertainty into RLVR for enhancing LRMs. EGPO estimates per-sample uncertainty using a zero-overhead entropy proxy derived from token-level likelihoods and aligns it with extrinsic correctness through an asymmetric calibration mechanism that preserves correct reasoning while selectively regulating overconfident failures, thereby enabling stable and uncertainty-aware policy optimization. Moreover, EGPO recovers informative learning signals from otherwise degenerate group-based rollouts without modifying the verifier or reward definition. Extensive experiments across multiple benchmarks demonstrate that the proposed EGPO leads to substantial and consistent improvements in reasoning performance, establishing a principled path for advancing LRMs through metacognitive entropy calibration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>认知自知：基于元认知熵校准的可验证强化学习推理框架</div>
<div class="mono" style="margin-top:8px">大型推理模型已成为解决复杂现实任务的重要范式。实践中，这类模型主要通过可验证奖励的强化学习进行训练，但现有仅依赖结果判定的RLVR流程几乎完全基于二元正确性信号，普遍忽略了模型的内在不确定性。我们将此现象称为“不确定性与奖励失配”——在此机制下，高不确定性与低不确定性解决方案被等同对待，阻碍了策略实现“认知自知”能力，也制约了从优化正确答案到优化有效推理路径的范式转变。这一局限在数学推理和问答等以推理为核心的任务中尤为关键，因为此类任务的性能表现取决于模型内部推理过程的质量，而非对最终答案的机械记忆。为此，我们提出EGPO框架：一种通过显式整合内在不确定性来增强大型推理模型的元认知熵校准方法。EGPO利用基于词元级似然值的零开销熵代理来估计样本级不确定性，并通过非对称校准机制将其与外部正确性对齐——该机制在保留正确推理的同时选择性地调节过度自信的错误预测，从而实现稳定且具有不确定性感知能力的策略优化。此外，EGPO无需修改验证器或奖励定义，即可从原本退化的分组推演中恢复有效的学习信号。在多基准测试中的广泛实验表明，EGPO能持续显著提升推理性能，为通过元认知熵校准推进大型推理模型发展提供了理论路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the uncertainty-reward mismatch in Reinforcement Learning with Verifiable Rewards (RLVR) for large reasoning models, where binary correctness signals ignore intrinsic uncertainty, hindering the optimization of reasoning paths. To resolve this, the authors propose EGPO, a metacognitive entropy calibration framework that estimates per-sample uncertainty via token-level likelihoods and aligns it with extrinsic correctness through asymmetric calibration, preserving correct reasoning while regulating overconfident failures without modifying the verifier. Experimental results across multiple benchmarks show that EGPO yields substantial and consistent improvements in reasoning performance, advancing LRMs through principled uncertainty-aware optimization.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型在基于可验证奖励的强化学习中存在的“不确定性-奖励不匹配”问题，即二元正确性信号忽略了模型的内在不确定性，从而阻碍了推理路径的优化。为此，作者提出了EGPO，一种元认知熵校准框架，它通过基于标记级似然的零开销熵代理来估计每个样本的不确定性，并通过非对称校准机制将其与外部正确性对齐，在保留正确推理的同时选择性调节过度自信的错误，且无需修改验证器或奖励定义。在多个基准测试上的广泛实验表明，EGPO能显著且一致地提升推理性能，为通过元认知熵校准推进大型推理模型提供了原则性路径。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Recommendation for Large-Scale Advertising</div>
<div class="meta-line">Authors: Ben Xue, Dan Liu, Lixiang Wang, Mingjie Sun, Peng Wang, Pengfei Zhang, Shaoyun Shi, Tianyu Xu, Yunhao Sha, Zhiqiang Liu, Bo Kong, Bo Wang, Hang Yang, Jieting Xue, Junhao Wang, Shengyu Wang, Shuping Hui, Wencai Ye, Xiao Lin, Yongzhi Li, Yuhang Chen, Zhihui Yin, Quan Chen, Shiyang Wen, Wenjin Wu, Han Li, Guorui Zhou, Changcheng Li, Peng Jiang</div>
<div class="meta-line">First: 2026-02-26T08:15:26+00:00 · Latest: 2026-02-26T08:15:26+00:00</div>
<div class="meta-line">Comments: 13 pages, 6 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22732v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22732v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大规模广告的生成式推荐</div>
<div class="mono" style="margin-top:8px">生成式推荐因其可扩展性和更强的模型能力，近期在工业界受到广泛关注。然而，在大规模广告中部署实时生成式推荐需要超越大语言模型（LLM）式训练与服务方案的设计。我们提出了一种面向生产的生成式推荐系统GR4AD（面向广告的生成式推荐），在架构、学习与服务层面进行协同设计。在标记化方面，GR4AD提出UA-SID（统一广告语义标识）以捕捉复杂的业务信息。此外，GR4AD引入LazyAR，一种惰性自回归解码器，通过放松层间依赖关系来支持短序列多候选生成，在保持效果的同时降低推理成本，从而在固定服务预算下促进扩展。为使优化与业务价值对齐，GR4AD采用VSL（价值感知监督学习），并提出RSPO（排序引导的Softmax偏好优化）——一种排序感知的列表式强化学习算法，可在列表级指标下优化基于价值的奖励，实现持续在线更新。对于在线推理，我们进一步提出动态束搜索服务，通过跨生成层级和在线负载自适应调整束宽来控制计算量。大规模在线A/B测试显示，相较于现有基于DLRM的架构，广告收入提升最高达4.2%，模型扩展与推理时扩展均带来持续收益。GR4AD已在拥有超4亿用户的快手广告系统全面部署，实现了高吞吐实时服务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to deploy real-time generative recommendation in large-scale advertising beyond standard LLM approaches, this paper introduces GR4AD, a production-oriented system co-designed across architecture, learning, and serving. The method employs a Unified Advertisement Semantic ID (UA-SID) for tokenization, a Lazy Autoregressive (LazyAR) decoder for efficient multi-candidate generation, and a ranking-aware reinforcement learning algorithm called RSPO for value-aligned optimization. Main experimental results from large-scale online A/B tests demonstrate up to a 4.2% improvement in ad revenue compared to a DLRM-based baseline, with gains from both model and inference scaling, and the system has been fully deployed serving over 400 million users.</div>
<div class="mono" style="margin-top:8px">本文的动机是在大规模广告中部署实时生成式推荐，这需要超越标准大语言模型的设计。为此，论文提出了GR4AD，一个面向生产的系统，在架构、学习和服务端协同设计。方法包括使用统一广告语义ID进行表征，采用惰性自回归解码器以高效生成多个候选，并引入基于排序的强化学习算法RSPO进行价值对齐的优化。主要实验结果表明，通过大规模在线A/B测试，相比基于DLRM的基线，广告收入提升了最高4.2%，模型和推理扩展均带来稳定收益，该系统已全量部署，服务超过4亿用户。</div>
</details>
</div>
<div class="card">
<div class="title">RLHFless: Serverless Computing for Efficient RLHF</div>
<div class="meta-line">Authors: Rui Wei, Hanfei Yu, Shubham Jain, Yogarajan Sivakumar, Devesh Tiwari, Jian Li, Seung-Jong Park, Hao Wang</div>
<div class="meta-line">First: 2026-02-26T07:45:37+00:00 · Latest: 2026-02-26T07:45:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22718v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22718v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF&#x27;s potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLHFless：基于无服务器计算的高效RLHF框架</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）已广泛应用于大语言模型（LLM）的后训练阶段，以使模型输出与人类偏好对齐。近期模型（如DeepSeek-R1）也展现了RLHF在提升LLM复杂任务推理能力方面的潜力。在强化学习中，推理与训练并存，导致工作流全程存在动态资源需求。与传统强化学习相比，RLHF因模型规模扩大和资源消耗增长，对训练效率提出更高挑战。现有RLHF框架虽致力于平衡灵活抽象与高效执行，但均依赖服务器化基础设施，难以应对细粒度资源波动。这导致同步RLHF训练过程中，强化学习组件间或组件内的空闲时间常引发开销与资源浪费。
为解决上述问题，我们提出RLHFless——首个基于无服务器计算环境的可扩展同步RLHF训练框架。该框架能自适应RLHF流程中的动态资源需求，通过预计算共享前缀避免重复运算，并采用考虑响应长度变化的成本感知型智能体扩缩策略，实现成本与速度的最优平衡。此外，RLHFless通过高效分配工作负载减少函数内负载不均与空闲时间。在物理测试平台与大规模模拟集群上的实验表明，相较于现有最优基线，RLHFless可实现最高1.35倍的加速与44.8%的成本降低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for RLHFless stems from the inefficiencies in existing Reinforcement Learning from Human Feedback (RLHF) frameworks, which rely on serverful infrastructures that poorly handle the dynamic and fine-grained resource demands of synchronous RLHF training, leading to idle time and resource wastage. The method introduces a scalable training framework built on serverless computing, which adapts to dynamic resource needs, pre-computes shared prefixes to avoid redundant computation, employs a cost-aware actor scaling strategy to optimize for response length variation, and efficiently assigns workloads to reduce imbalance and idle time. Experimental results on physical testbeds and a simulated large-scale cluster demonstrate that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to state-of-the-art baselines.</div>
<div class="mono" style="margin-top:8px">RLHFless的提出动机源于现有基于人类反馈的强化学习（RLHF）框架效率低下，这些框架依赖服务器基础设施，难以处理同步RLHF训练中动态、细粒度的资源需求，导致空闲时间和资源浪费。该方法首次在无服务器计算环境中构建了一个可扩展的同步RLHF训练框架，通过适应动态资源需求、预计算共享前缀以避免重复计算、采用考虑响应长度变化的成本感知执行器缩放策略来寻找成本与速度的最佳平衡点，并高效分配工作负载以减少内部不平衡和空闲时间。在物理测试平台和大规模模拟集群上的实验结果表明，与最先进的基线相比，RLHFless实现了最高1.35倍的加速和44.8%的成本降低。</div>
</details>
</div>
<div class="card">
<div class="title">Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</div>
<div class="meta-line">Authors: Ruize Zhang, Sirui Xiang, Zelai Xu, Feng Gao, Shilong Ji, Wenhao Tang, Wenbo Ding, Chao Yu, Yu Wang</div>
<div class="meta-line">Venue: CoRL 2025</div>
<div class="meta-line">First: 2025-05-07T11:04:36+00:00 · Latest: 2026-02-26T07:37:05+00:00</div>
<div class="meta-line">Comments: Accepted by CoRL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.04317v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.04317v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hi-co-self-play.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level skills, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme. The project page is at https://hi-co-self-play.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过分层协同自博弈强化学习掌握多无人机排球</div>
<div class="mono" style="margin-top:8px">本文研究如何学习3v3多无人机排球——一项新型具身竞争任务，要求高层战略协调与低层敏捷控制兼备。该任务基于回合制、多智能体且物理接地，因其长时程依赖、智能体间紧密耦合及四旋翼欠驱动动力学而带来显著挑战。为此，我们提出分层协同自博弈（HCSP），这是一种将集中式高层战略决策与分布式低层运动控制分离的分层强化学习框架。我们设计了三阶段基于群体的训练流程，使战略与技能均能从零开始涌现而无需专家示范：（I）训练多样化低层技能，（II）通过固定低层技能的自博弈学习高层战略，（III）通过协同自博弈进行联合微调。实验表明，HCSP实现了卓越性能，以平均82.9%的胜率超越非分层自博弈与基于规则的分层基线，并对两阶段变体取得71.5%的胜率。此外，协同自博弈催生出角色切换与协同阵型等团队行为，验证了我们分层设计与训练方案的有效性。项目页面详见 https://hi-co-self-play.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of learning 3v3 multi-drone volleyball, a competitive multi-agent task requiring strategic coordination and agile low-level control of quadrotors. The proposed method, Hierarchical Co-Self-Play (HCSP), is a hierarchical reinforcement learning framework that decouples centralized high-level strategy from decentralized low-level motion control, trained via a three-stage population-based pipeline without expert data. Experimental results demonstrate that HCSP outperforms non-hierarchical and rule-based hierarchical baselines with an 82.9% average win rate, and co-self-play leads to emergent team behaviors like role switching and coordinated formations.</div>
<div class="mono" style="margin-top:8px">本文旨在解决学习3v3多无人机排球这一竞争性任务，该任务需要高级战略协调和无人机低级敏捷控制。提出的方法是分层协同自博弈强化学习框架，将集中式高级决策与分散式低级运动控制解耦，并通过无需专家数据的三阶段群体训练流程进行训练。实验结果表明，该方法优于非分层和基于规则的分层基线，平均胜率达到82.9%，且协同自博弈能涌现出角色切换和协调阵型等团队行为。</div>
</details>
</div>
<div class="card">
<div class="title">Same Words, Different Judgments: Modality Effects on Preference Alignment</div>
<div class="meta-line">Authors: Aaron Broukhim, Nadir Weibel, Eshin Jolly</div>
<div class="meta-line">First: 2026-02-26T07:34:15+00:00 · Latest: 2026-02-26T07:34:15+00:00</div>
<div class="meta-line">Comments: Submitted to Interspeech 2026 for review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-based reinforcement learning (PbRL) is the dominant framework for aligning AI systems to human preferences, but its application to speech remains underexplored. We present a controlled cross-modal study of human and synthetic preference annotations, comparing text and audio evaluations of identical semantic content across 100 prompts. Audio preferences prove as reliable as text, with inter-rater agreement reaching good levels (ICC(2,k) $\approx$ .80) at $\sim$9 raters -- the first ICC-based reliability characterization in the preference annotation literature for either modality. However, modality reshapes how people judge: audio raters exhibit narrower decision thresholds, reduced length bias, and more user-oriented evaluation criteria, with near-chance cross-modality agreement. Synthetic ratings further align with human judgments and predict inter-rater agreement, supporting their use both for triaging ambiguous pairs and as full replacements for human annotations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同词异判：模态对偏好对齐的影响</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PbRL）是对齐人工智能系统与人类偏好的主流框架，但其在语音领域的应用仍待探索。我们通过一项受控跨模态研究，对比了100组相同语义内容在文本与音频两种形式下的人类标注与合成偏好标注。实验表明音频偏好与文本偏好具有同等可靠性，在约9名标注者时评分者间一致性达到良好水平（ICC(2,k)≈0.80）——这是偏好标注文献中首次基于ICC的跨模态可靠性表征。然而，模态显著影响评判方式：音频标注者表现出更严格的决策阈值、更弱的长度偏见及更注重用户体验的评估标准，跨模态一致性接近随机水平。合成评分与人类判断高度吻合，并能预测评分者间一致性，这支持其既可用于筛选模糊样本对，也可完全替代人工标注。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of modality on preference alignment in AI systems, motivated by the underexplored application of preference-based reinforcement learning (PbRL) to speech. The method involves a controlled cross-modal comparison of human and synthetic preference annotations for identical semantic content across 100 prompts, evaluating both text and audio. Experimental results show that audio preferences are as reliable as text, with good inter-rater agreement achieved at approximately nine raters, marking the first ICC-based reliability characterization in this literature. However, modality significantly influences judgment: audio raters demonstrate narrower decision thresholds, less length bias, and more user-oriented criteria, leading to near-chance agreement across modalities. Synthetic ratings align well with human judgments and predict inter-rater agreement, supporting their use for triaging ambiguous pairs or replacing human annotations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了模态对AI系统偏好对齐的影响，其动机在于基于偏好的强化学习（PbRL）在语音领域的应用尚未充分探索。方法上，通过一项受控的跨模态研究，比较了100个提示下相同语义内容的人类与合成偏好标注，涵盖文本和音频评估。实验结果表明，音频偏好的可靠性与文本相当，在约九名评分者时达到了良好的评分者间一致性，这是该文献中首次基于ICC的可靠性表征。然而，模态重塑了人们的判断方式：音频评分者表现出更窄的决策阈值、更少的长度偏见以及更用户导向的评估标准，导致跨模态一致性接近随机水平。合成评分与人类判断高度一致，并能预测评分者间一致性，支持其用于筛选模糊对或完全替代人类标注。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning</div>
<div class="meta-line">Authors: Hao Yu, Shuning Jia, Guanghao Li, Wenhao Jiang, Chun Yuan</div>
<div class="meta-line">First: 2026-02-26T07:28:04+00:00 · Latest: 2026-02-26T07:28:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22703v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22703v1">PDF</a> · <a href="https://github.com/Longin-Yu/GeoPerceive">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过翻译器引导的强化学习增强视觉语言模型的几何感知能力</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）因对基本图形元素的感知有限，常在几何推理方面表现不佳。为解决这一问题，我们提出了GeoPerceive，这是一个包含与领域特定语言（DSL）表示配对的图形实例的基准，以及一个高效的自动数据生成流程。该设计使得几何感知能够独立于推理进行单独评估。为利用GeoPerceive提供的数据增强VLM的几何感知能力，我们提出了GeoDPO，一种翻译器引导的强化学习（RL）框架。GeoDPO采用一个在GeoPerceive数据引擎生成的合成对上训练的NL-to-DSL翻译器，以桥接自然语言和DSL。该翻译器有助于计算细粒度的DSL级分数，这些分数在强化学习中作为奖励信号。我们在领域内和领域外数据集上评估GeoDPO，涵盖几何感知及下游推理任务。实验结果表明，尽管监督微调（SFT）仅带来边际改进，在领域外场景中甚至可能损害性能，但GeoDPO取得了显著提升：领域内数据上提升+26.5%，领域外数据上提升+8.0%，下游推理任务上提升+39.0%。这些发现凸显了GeoDPO相对于SFT的优越性能和泛化能力。所有代码已发布于https://github.com/Longin-Yu/GeoPerceive以确保可复现性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited geometric reasoning capabilities of vision-language models (VLMs) due to poor perception of diagram elements, this paper introduces GeoPerceive, a benchmark with diagram-domain-specific language (DSL) pairs and a data generation pipeline to isolate perception evaluation. The method proposes GeoDPO, a translator-guided reinforcement learning framework that uses an NL-to-DSL translator trained on synthetic data to compute fine-grained DSL-level rewards for RL. Experimental results show that GeoDPO achieves substantial performance gains: +26.5% on in-domain data, +8.0% on out-of-domain data, and +39.0% on downstream reasoning tasks, significantly outperforming supervised fine-tuning which offers only marginal improvements or even harms out-of-domain generalization.</div>
<div class="mono" style="margin-top:8px">针对视觉语言模型（VLMs）因对基本图形元素感知有限而几何推理能力不足的问题，本文提出了GeoPerceive基准，包含图表与领域特定语言（DSL）配对的数据及自动生成流程，以独立评估几何感知。方法上，提出了GeoDPO，一种翻译器引导的强化学习框架，它利用在合成数据上训练的NL-to-DSL翻译器来计算细粒度的DSL级奖励信号。实验结果表明，GeoDPO取得了显著性能提升：在域内数据上提升26.5%，域外数据上提升8.0%，下游推理任务上提升39.0%，明显优于监督微调，后者仅带来边际改进甚至在域外场景中损害性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue</div>
<div class="meta-line">Authors: Ning Gao, Wei Zhang, Yuqin Dai, Ling Shi, Ziyin Wang, Yujie Wang, Wei He, Jinpeng Wang, Chaozheng Wang</div>
<div class="meta-line">First: 2026-02-26T07:19:57+00:00 · Latest: 2026-02-26T07:19:57+00:00</div>
<div class="meta-line">Comments: 35 pages, 8 tables, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22697v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化现实世界服务智能体：面向任务对话中效用与成本的平衡</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的快速发展加速了从对话式聊天机器人向通用智能体的转变。然而，如何在共情沟通与成本感知决策之间实现有效平衡仍是一个开放挑战。由于现有方法难以捕捉这些复杂的策略权衡，我们提出了InteractCS-RL框架，将面向任务的对话重构为多粒度强化学习过程。具体而言，我们首先建立以用户为中心的交互框架，提供高保真训练环境，使智能体能够与人格驱动的用户动态探索多样化策略。随后，我们引入具有混合优势估计策略的成本感知多轮策略优化（CMPO）。通过整合生成过程信用度并采用PID-Lagrangian成本控制器，CMPO有效引导策略探索用户奖励与全局成本约束之间的帕累托边界。在定制化真实商业场景上的大量实验表明，InteractCS-RL在三个评估维度上显著优于其他基线方法。在工具-智能体-用户交互基准上的进一步验证证实了InteractCS-RL跨领域的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of balancing empathetic communication with budget constraints in real-world service agents, this paper proposes InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. The method establishes a user-centric interaction gym for training and introduces Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimator and a PID-Lagrangian controller to navigate trade-offs between user reward and cost. Experimental results on real business scenarios show that InteractCS-RL outperforms baselines across multiple evaluation dimensions, with further benchmarks confirming its robustness across diverse domains.</div>
<div class="mono" style="margin-top:8px">本文针对现实世界服务智能体在任务导向对话中平衡共情沟通与预算约束的挑战，提出了InteractCS-RL框架，将任务对话重构为多粒度强化学习过程。该方法首先建立以用户为中心的高保真交互环境进行训练，并引入成本感知的多轮策略优化（CMPO），结合混合优势估计与PID-Lagrangian成本控制器，以引导策略在用户奖励和全局成本约束间探索帕累托边界。在定制化真实业务场景上的大量实验表明，InteractCS-RL在三个评估维度上显著优于基线方法，后续在工具-智能体-用户交互基准上的评估进一步验证了其跨领域的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising</div>
<div class="meta-line">Authors: Xinxin Yang, Yangyang Tang, Yikun Zhou, Yaolei Liu, Yun Li, Bo Yang</div>
<div class="meta-line">Venue: WWW</div>
<div class="meta-line">First: 2026-02-26T06:07:28+00:00 · Latest: 2026-02-26T06:07:28+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, accepted by WWW&#x27;2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22650v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22650v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AHBid：一种适用于跨渠道广告的自适应分层竞价框架</div>
<div class="mono" style="margin-top:8px">在线广告环境中固有的复杂性和动态性要求使用自动竞价服务来辅助广告主优化出价。在多渠道场景中，由于各渠道行为模式各异，如何有效分配预算和约束条件以优化投资回报变得尤为关键。现有方法主要依赖基于优化的策略或强化学习技术，但前者难以灵活适应动态市场条件，后者则在马尔可夫决策过程框架下常难以捕捉关键的历史依赖性和观测模式。为应对这些局限，本文提出AHBid——一种融合生成式规划与实时控制的自适应分层竞价框架。该框架采用基于扩散模型的高层生成规划器，通过有效捕捉历史上下文与时间模式来动态分配预算和约束；同时引入约束执行机制以确保符合指定约束，并利用轨迹优化机制借助历史数据增强对环境变化的适应性。系统还结合了基于控制的竞价算法，将历史知识与实时信息协同整合，显著提升了适应性与操作效能。基于大规模离线数据集和在线A/B测试的广泛实验表明，AHBid相比现有基线方法实现了13.57%的整体收益提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of auto-bidding in dynamic, multi-channel online advertising environments, where existing optimization-based methods lack flexibility and reinforcement learning struggles with historical dependencies. The authors propose AHBid, an adaptable hierarchical framework that integrates a high-level generative planner using diffusion models to allocate budgets and constraints by capturing historical context, alongside a control-based bidding algorithm for real-time adaptation. Experimental results on large-scale offline datasets and online A/B tests show that AHBid achieves a 13.57% increase in overall return compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">本文针对动态多通道在线广告环境中自动出价的挑战，现有基于优化的方法缺乏灵活性，而强化学习方法难以处理历史依赖。作者提出了AHBid，一种适应性分层框架，它整合了基于扩散模型的高层生成规划器，通过捕捉历史上下文来分配预算和约束，并结合基于控制的出价算法实现实时适应。在大规模离线数据集和在线A/B测试上的实验结果表明，AHBid相比基线方法实现了13.57%的整体回报提升。</div>
</details>
</div>
<div class="card">
<div class="title">Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning</div>
<div class="meta-line">Authors: Qin-Wen Luo, Sheng Ren, Xiang Chen, Rui Liu, Jun Fang, Naiqiang Tan, Sheng-Jun Huang</div>
<div class="meta-line">First: 2026-02-26T05:47:30+00:00 · Latest: 2026-02-26T05:47:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22642v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22642v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>易题压缩，难题探索：面向高效大语言模型推理的难度感知熵正则化方法</div>
<div class="mono" style="margin-top:8px">思维链（CoT）显著增强了大语言模型处理复杂推理任务的能力，但显式推理步骤的冗长特性导致极高的推理延迟和计算成本，限制了实际部署。现有压缩方法（从自训练到带长度约束的强化学习）虽试图缓解此问题，却常为简洁性牺牲推理能力。我们发现这些方法存在关键缺陷：显式优化短轨迹会引发熵值快速坍缩，过早压缩探索空间并阻碍有效推理路径的发现（尤其对需大量推导的难题）。为此，我们提出“易题压缩、难题探索”（CEEH）——一种基于强化学习的难度感知高效推理方法。CEEH动态评估实例难度以实施选择性熵正则化：对当前难题保持多样化搜索空间以确保鲁棒性，同时对推理路径明确的易题允许激进压缩。此外，我们引入以历史最短正确答案为锚点的动态最优长度惩罚，有效抑制熵增引起的长度膨胀并稳定奖励信号。在六个推理基准测试中，CEEH在保持与基础模型相当准确度的同时持续缩短响应长度，且相对仅优化长度的方法提升了Pass@k指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the high inference costs of Chain-of-Thought reasoning in Large Language Models by noting that existing compression methods often fail on hard questions due to premature entropy collapse. The proposed method, CEEH, dynamically assesses question difficulty to apply selective entropy regularization, preserving exploration for hard questions while compressing easy ones, and uses a dynamic optimal-length penalty. Experiments across six benchmarks show it reduces response length while maintaining accuracy and improving Pass@k compared to length-only optimization.</div>
<div class="mono" style="margin-top:8px">该论文针对大语言模型中思维链推理的高计算成本问题，指出现有压缩方法因过早的熵崩溃而在难题上失效。提出的CEEH方法动态评估问题难度，应用选择性熵正则化，在难题上保持探索性，在易题上进行压缩，并采用动态最优长度惩罚。在六个基准测试上的实验表明，该方法在保持准确性的同时减少了响应长度，并相比仅优化长度的方法提升了Pass@k。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing accelerator virtual beam diagnostics through latent evolution modeling: an integrated solution to forward, inverse, tuning, and UQ problems</div>
<div class="meta-line">Authors: Mahindra Rautela, Alexander Scheinker</div>
<div class="meta-line">First: 2026-02-26T04:46:26+00:00 · Latest: 2026-02-26T04:46:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22618v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22618v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Virtual beam diagnostics relies on computationally intensive beam dynamics simulations where high-dimensional charged particle beams evolve through the accelerator. We propose Latent Evolution Model (LEM), a hybrid machine learning framework with an autoencoder that projects high-dimensional phase spaces into lower-dimensional representations, coupled with transformers to learn temporal dynamics in the latent space. This approach provides a common foundational framework addressing multiple interconnected challenges in beam diagnostics. For \textit{forward modeling}, a Conditional Variational Autoencoder (CVAE) encodes 15 unique projections of the 6D phase space into a latent representation, while a transformer predicts downstream latent states from upstream inputs. For \textit{inverse problems}, we address two distinct challenges: (a) predicting upstream phase spaces from downstream observations by utilizing the same CVAE architecture with transformers trained on reversed temporal sequences along with aleatoric uncertainty quantification, and (b) estimating RF settings from the latent space of the trained LEM using a dedicated dense neural network that maps latent representations to RF parameters. For \textit{tuning problems}, we leverage the trained LEM and RF estimator within a Bayesian optimization framework to determine optimal RF settings that minimize beam loss. This paper summarizes our recent efforts and demonstrates how this unified approach effectively addresses these traditionally separate challenges.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过潜在演化建模推进加速器虚拟束流诊断：面向正向、逆向、调谐与不确定性量化问题的集成解决方案</div>
<div class="mono" style="margin-top:8px">虚拟束流诊断依赖于计算密集的束流动力学模拟，其中高维带电粒子束在加速器中演化。我们提出潜在演化模型（LEM），这是一种混合机器学习框架：通过自编码器将高维相空间投影至低维表示，并耦合Transformer学习潜在空间的时间动力学。该方法为束流诊断中多个相互关联的挑战提供了统一的基础框架。在正向建模方面，条件变分自编码器（CVAE）将6D相空间的15个独立投影编码为潜在表示，而Transformer根据上游输入预测下游潜在状态。针对逆向问题，我们应对两个不同挑战：（a）通过使用相同CVAE架构与沿反向时间序列训练的Transformer（辅以偶然不确定性量化），从下游观测预测上游相空间；（b）通过专用密集神经网络将潜在表示映射至射频参数，从训练好的LEM潜在空间估计射频设置。对于调谐问题，我们在贝叶斯优化框架中利用训练好的LEM和射频估计器，以确定最小化束流损失的最优射频设置。本文总结了近期研究成果，展示了这一统一方法如何有效应对这些传统上相互分离的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational intensity of simulating high-dimensional particle beam dynamics for accelerator diagnostics, proposing a unified machine learning framework called the Latent Evolution Model (LEM). The method integrates an autoencoder to compress 6D phase space projections into a latent representation and transformers to model temporal evolution, enabling forward modeling, inverse problems (predicting upstream states or RF settings from downstream data with uncertainty quantification), and tuning via Bayesian optimization. Experimental results demonstrate that this approach effectively addresses these interconnected challenges, such as minimizing beam loss by optimizing RF parameters, providing a comprehensive solution for virtual beam diagnostics.</div>
<div class="mono" style="margin-top:8px">本文针对加速器虚拟束流诊断中高维粒子束动力学模拟计算量大的问题，提出了一种称为潜在演化模型（LEM）的统一机器学习框架。该方法结合自编码器将6D相空间投影压缩为潜在表示，并利用变换器学习时序动态，从而支持正向建模、逆向问题（如下游观测预测上游状态或射频设置，并量化不确定性）以及通过贝叶斯优化进行参数调谐。实验结果表明，该框架能有效解决这些相互关联的挑战，例如通过优化射频参数最小化束流损失，为虚拟束流诊断提供了一个全面的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">EvolveGen: Algorithmic Level Hardware Model Checking Benchmark Generation through Reinforcement Learning</div>
<div class="meta-line">Authors: Guangyu Hu, Xiaofeng Zhou, Wei Zhang, Hongce Zhang</div>
<div class="meta-line">First: 2026-02-26T04:32:07+00:00 · Latest: 2026-02-26T04:32:07+00:00</div>
<div class="meta-line">Comments: 19 pages, 8 figures. Accepted by TACAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22609v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Progress in hardware model checking depends critically on high-quality benchmarks. However, the community faces a significant benchmark gap: existing suites are limited in number, often distributed only in representations such as BTOR2 without access to the originating register-transfer-level (RTL) designs, and biased toward extreme difficulty where instances are either trivial or intractable. These limitations hinder rigorous evaluation of new verification techniques and encourage overfitting of solver heuristics to a narrow set of problems. To address this, we introduce EvolveGen, a framework for generating hardware model checking benchmarks by combining reinforcement learning (RL) with high-level synthesis (HLS). Our approach operates at an algorithmic level of abstraction in which an RL agent learns to construct computation graphs. By compiling these graphs under different synthesis directives, we produce pairs of functionally equivalent but structurally distinct hardware designs, inducing challenging model checking instances. Solver runtime is used as the reward signal, enabling the agent to autonomously discover and generate small-but-hard instances that expose solver-specific weaknesses. Experiments show that EvolveGen efficiently creates a diverse benchmark set in standard formats (e.g., AIGER and BTOR2) and effectively reveals performance bottlenecks in state-of-the-art model checkers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvolveGen：基于强化学习的算法级硬件模型检验基准生成框架</div>
<div class="mono" style="margin-top:8px">硬件模型检验领域的发展高度依赖于高质量基准测试集。然而当前面临显著的基准缺口：现有测试集数量有限，通常仅以BTOR2等中间表示形式分发而无法获取原始寄存器传输级设计，且偏向极端难度——实例要么过于简单要么无法求解。这些局限阻碍了新验证技术的严格评估，并导致求解器启发式方法在狭窄问题集上过拟合。为此，我们提出EvolveGen框架，通过结合强化学习与高层次综合技术生成硬件模型检验基准。该方法在算法抽象层级运行，由强化学习智能体构建计算图，通过不同综合指令编译生成功能等效但结构相异的硬件设计对，从而产生具有挑战性的模型检验实例。以求解器运行时间作为奖励信号，使智能体能自主发现并生成暴露特定求解器弱点的“小而难”实例。实验表明，EvolveGen能高效生成标准格式的多样化基准集，并有效揭示前沿模型检验工具的性能瓶颈。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the scarcity and bias of existing hardware model checking benchmarks, which are often limited in number, lack original RTL designs, and are polarized between trivial and intractable instances, this paper introduces EvolveGen, a framework that generates benchmarks using reinforcement learning combined with high-level synthesis. The method involves an RL agent constructing computation graphs at an algorithmic abstraction level, which are then compiled under different synthesis directives to produce functionally equivalent but structurally distinct hardware designs, thereby creating challenging model checking instances; the agent uses solver runtime as a reward to autonomously discover small-yet-hard cases that expose solver weaknesses. Experimental results demonstrate that EvolveGen efficiently produces a diverse set of benchmarks in standard formats like AIGER and BTOR2, effectively revealing performance bottlenecks in state-of-the-art model checkers.</div>
<div class="mono" style="margin-top:8px">针对现有硬件模型检验基准数量有限、缺乏原始RTL设计且偏向于极端难度（实例要么过于简单要么无法处理）的问题，本文提出了EvolveGen框架，通过结合强化学习与高层次综合来生成基准。该方法在算法抽象层面使用强化学习智能体构建计算图，并通过不同综合指令编译这些图，产生功能等效但结构不同的硬件设计，从而生成具有挑战性的模型检验实例；智能体以求解器运行时间作为奖励，自主发现能暴露求解器弱点的小而难的案例。实验结果表明，EvolveGen能高效生成AIGER和BTOR2等标准格式的多样化基准集，有效揭示了先进模型检验器的性能瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits</div>
<div class="meta-line">Authors: Qingyue Zhao, Kaixuan Ji, Heyang Zhao, Tong Zhang, Quanquan Gu</div>
<div class="meta-line">First: 2025-02-09T22:14:45+00:00 · Latest: 2026-02-26T03:57:25+00:00</div>
<div class="meta-line">Comments: 35 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.06051v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.06051v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many offline reinforcement learning algorithms are underpinned by $f$-divergence regularization, but their sample complexity *defined with respect to regularized objectives* still lacks tight analyses, especially in terms of concrete data coverage conditions. In this paper, we study the exact concentrability requirements to achieve the $\tildeΘ(ε^{-1})$ sample complexity for offline $f$-divergence-regularized contextual bandits. For reverse Kullback-Leibler (KL) divergence, arguably the most commonly used one, we achieve an $\tilde{O}(ε^{-1})$ sample complexity under single-policy concentrability for the first time via a novel pessimism-based analysis, surpassing existing $\tilde{O}(ε^{-1})$ bound under all-policy concentrability and $\tilde{O}(ε^{-2})$ bound under single-policy concentrability. We also propose a near-matching lower bound, demonstrating that a multiplicative dependency on single-policy concentrability is necessary to maximally exploit the curvature property of reverse KL. Moreover, for $f$-divergences with strongly convex $f$, to which reverse KL *does not* belong, we show that the sharp sample complexity $\tildeΘ(ε^{-1})$ is achievable even without pessimistic estimation or single-policy concentrability. We further corroborate our theoretical insights with numerical experiments and extend our analysis to contextual dueling bandits. We believe these results take a significant step towards a comprehensive understanding of objectives with $f$-divergence regularization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向$f$-散度正则化上下文赌博机离线策略学习的精确分析</div>
<div class="mono" style="margin-top:8px">许多离线强化学习算法以$f$-散度正则化为理论基础，但其在正则化目标下的样本复杂度仍缺乏精确分析，尤其在具体数据覆盖条件方面。本文研究了离线$f$-散度正则化上下文赌博机达到$\tildeΘ(ε^{-1})$样本复杂度所需的精确集中性条件。针对最常用的反向KL散度，我们首次通过基于悲观估计的新颖分析，在单策略集中性条件下实现了$\tilde{O}(ε^{-1})$样本复杂度，超越了现有全策略集中性下的$\tilde{O}(ε^{-1})$界限和单策略集中性下的$\tilde{O}(ε^{-2})$界限。同时提出了近乎匹配的下界，证明对单策略集中性的乘法依赖是充分利用反向KL曲率特性的必要条件。此外，对于反向KL散度不属的强凸$f$散度，我们证明即使无需悲观估计或单策略集中性条件，仍可实现$\tildeΘ(ε^{-1})$的精确样本复杂度。通过数值实验验证了理论发现，并将分析扩展至上下文对决赌博机。这些成果为全面理解$f$-散度正则化目标迈出了重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to provide a sharp analysis of the sample complexity for offline f-divergence-regularized contextual bandits, motivated by the lack of tight characterizations of concentrability requirements. The method involves a novel pessimism-based analysis for the reverse KL divergence, and a separate analysis for strongly convex f-divergences that does not require pessimism. The main experimental results show that for reverse KL, the work achieves an Õ(ε⁻¹) sample complexity under single-policy concentrability, improving prior bounds, and establishes a near-matching lower bound; for strongly convex f-divergences, it achieves the same sharp complexity without requiring single-policy concentrability, with numerical experiments and an extension to dueling bandits corroborating the findings.</div>
<div class="mono" style="margin-top:8px">本文旨在对离线f-散度正则化上下文赌博机的样本复杂度进行精确分析，其动机在于现有研究对数据覆盖条件（即可集中性要求）缺乏严格的刻画。方法上，针对反向KL散度提出了一种新颖的悲观估计分析，并对强凸f-散度进行了无需悲观估计的分析。主要实验结果表明，对于反向KL散度，该工作在单策略可集中性条件下首次实现了Õ(ε⁻¹)的样本复杂度，超越了现有界限，并建立了近乎匹配的下界；对于强凸f-散度，则在不要求单策略可集中性的情况下实现了同样尖锐的复杂度，数值实验和对决赌博机的扩展进一步验证了理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</div>
<div class="meta-line">Authors: Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead</div>
<div class="meta-line">First: 2026-01-05T09:35:11+00:00 · Latest: 2026-02-26T03:25:19+00:00</div>
<div class="meta-line">Comments: Added link to GitHub repo</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02439v5">Abs</a> · <a href="https://arxiv.org/pdf/2601.02439v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent&#x27;s own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebGym：面向视觉网页代理的现实任务训练环境规模化构建</div>
<div class="mono" style="margin-top:8px">本文提出WebGym——迄今最大规模的开源现实视觉网页代理训练环境。真实网站具有非稳态与多样性特征，人工或小规模任务集难以支撑稳健的策略学习。WebGym涵盖近30万个任务，通过量规化评估体系覆盖多样化的真实网站及难度层级。我们采用简洁的强化学习方案训练代理：利用代理自身交互轨迹（rollout）进行训练，以任务奖励作为学习反馈。为实现强化学习的规模化，我们专门为网页代理开发了高吞吐异步轨迹采样系统，使WebGym的轨迹采样速度较原始实现提升4-5倍。其次，通过拓展任务集的广度、深度与规模，实现了持续的性能提升。基于Qwen-3-VL-8B-Instruct强基线视觉语言模型在WebGym上进行微调后，其在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的代理。该提升具有实质性意义，因为我们的测试集仅包含训练阶段未见的网站任务，这与多数现有视觉网页代理研究形成鲜明对比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for WebGym is to address the insufficiency of artificial or small-scale task sets for training robust visual web agents, given the non-stationary and diverse nature of real websites. The method involves creating a large-scale open-source environment with nearly 300,000 tasks across real-world websites, using a reinforcement learning recipe that trains on agent interaction traces with task rewards as feedback, and implementing a high-throughput asynchronous rollout system to speed up trajectory sampling. The main experimental results show that this system achieves a 4-5x rollout speedup compared to naive implementations, and fine-tuning the Qwen-3-VL-8B-Instruct model on WebGym improves the success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models like GPT-4o and GPT-5-Thinking.</div>
<div class="mono" style="margin-top:8px">WebGym的动机在于解决真实网站的非平稳性和多样性导致的人工或小规模任务集不足以训练鲁棒视觉网络代理的问题。其方法包括创建一个包含近30万个真实网站任务的大规模开源环境，采用基于智能体交互轨迹和任务奖励反馈的强化学习方案，并设计了一个高吞吐量的异步轨迹采样系统以加速训练。主要实验结果表明，该系统相比简单实现实现了4-5倍的轨迹采样加速，且在WebGym上微调Qwen-3-VL-8B-Instruct模型后，在未见过网站的分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o和GPT-5-Thinking等专有模型的代理。</div>
</details>
</div>
<div class="card">
<div class="title">Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation</div>
<div class="meta-line">Authors: Zihang Xu, Haozhi Xie, Ziqi Miao, Wuxuan Gong, Chen Qian, Lijun Li</div>
<div class="meta-line">First: 2026-02-26T02:49:36+00:00 · Latest: 2026-02-26T02:49:36+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22556v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22556v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于优势塑造与长度感知梯度调节的稳定自适应思维</div>
<div class="mono" style="margin-top:8px">大型推理模型通过扩展推理链获得优异性能，但在处理低复杂度查询时常出现过度思考现象。现有缓解方法受限于准确率与效率间不稳定的权衡关系，以及对异构推理行为的鲁棒性不足。为此，我们提出一个两阶段框架以实现大型推理模型的稳定自适应思维。该框架首先采用混合微调技术，使模型同时接触思考与非思考行为，建立良好初始化条件；随后执行自适应强化学习：通过正确性保持优势塑造避免抑制正确的长链推理，并采用长度感知梯度调节在严重推理长度异质性下稳定优化过程。在Qwen2.5-1.5B和7B模型上的大量实验表明，本方法在强基线模型基础上持续提升性能，最高可增加3.7/3.6个准确率点，同时减少40.6%/43.9%的生成标记。针对不同难度问题与分布外任务的进一步分析，证实了本方法的鲁棒性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of large reasoning models (LRMs) overthinking simple queries, which leads to unstable accuracy-efficiency trade-offs and poor robustness. The proposed solution is a two-stage framework: first, Hybrid Fine-Tuning initializes the model with both thinking and no-thinking behaviors; second, adaptive reinforcement learning employs Correctness-Preserving Advantage Shaping to protect accurate long reasoning and Length-Aware Gradient Regulation to stabilize training despite varying reasoning lengths. Experiments on Qwen2.5 models show the method improves accuracy by up to 3.7 points while cutting generated tokens by over 40%, with analyses confirming robustness across different problem difficulties and out-of-distribution tasks.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型在处理简单查询时过度思考导致精度-效率权衡不稳定及鲁棒性差的问题，提出一个两阶段框架。首先通过混合微调让模型接触思考与非思考行为以建立良好初始化，随后采用自适应强化学习，其中包含保持正确性的优势塑形以避免抑制正确的长链推理，以及长度感知梯度调节来稳定严重长度异质性下的优化。在Qwen2.5模型上的实验表明，该方法在准确率提升最高达3.7分的同时将生成标记减少超过40%，对不同难度和分布外任务的分析进一步验证了其鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Multilingual Safety Alignment Via Sparse Weight Editing</div>
<div class="meta-line">Authors: Jiaming Liang, Zhaoxin Wang, Handing Wang</div>
<div class="meta-line">First: 2026-02-26T02:46:13+00:00 · Latest: 2026-02-26T02:46:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22554v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22554v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) exhibit significant safety disparities across languages, with low-resource languages (LRLs) often bypassing safety guardrails established for high-resource languages (HRLs) like English. Existing solutions, such as multilingual supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), are computationally expensive and dependent on scarce multilingual safety data. In this work, we propose a novel, training-free alignment framework based on Sparse Weight Editing. Identifying that safety capabilities are localized within a sparse set of safety neurons, we formulate the cross-lingual alignment problem as a constrained linear transformation. We derive a closed-form solution to optimally map the harmful representations of LRLs to the robust safety subspaces of HRLs, while preserving general utility via a null-space projection constraint. Extensive experiments across 8 languages and multiple model families (Llama-3, Qwen-2.5) demonstrate that our method substantially reduces Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, all achieved with a single, data-efficient calculation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于稀疏权重编辑的多语言安全对齐</div>
<div class="mono" style="margin-top:8px">大语言模型在不同语言间存在显著的安全性能差异，低资源语言常能绕过针对英语等高资源语言设立的安全防护机制。现有解决方案（如多语言监督微调或基于人类反馈的强化学习）计算成本高昂且依赖稀缺的多语言安全数据。本研究提出一种基于稀疏权重编辑的新型免训练对齐框架。通过识别安全能力集中分布于稀疏的安全神经元集合，我们将跨语言对齐问题形式化为约束线性变换。推导出闭式解，可将低资源语言的有害表征最优映射至高资源语言的鲁棒安全子空间，同时通过零空间投影约束保持通用能力。在8种语言和多个模型系列上的实验表明，该方法能显著降低低资源语言的攻击成功率，且对通用推理能力影响可忽略，仅需单次数据高效计算即可实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the safety disparity in Large Language Models (LLMs), where low-resource languages (LRLs) often bypass safety guardrails established for high-resource languages like English, while existing multilingual alignment methods are computationally expensive and data-hungry. To overcome this, the authors propose a training-free framework using Sparse Weight Editing, which identifies that safety capabilities are localized in sparse neurons and formulates cross-lingual alignment as a constrained linear transformation with a closed-form solution to map harmful LRL representations to robust safety subspaces of HRLs, preserving utility via null-space projection. Experimental results across 8 languages and models like Llama-3 and Qwen-2.5 show the method significantly reduces Attack Success Rate in LRLs with minimal impact on general reasoning, achieved through a single, data-efficient calculation.</div>
<div class="mono" style="margin-top:8px">该论文针对大语言模型（LLMs）中存在的安全差异问题，即低资源语言（LRLs）常能绕过为英语等高资源语言（HRLs）建立的安全防护，而现有的多语言对齐方法计算成本高且依赖稀缺数据。为此，作者提出了一种基于稀疏权重编辑的无训练对齐框架，该方法识别出安全能力集中于稀疏神经元中，并将跨语言对齐问题表述为带约束的线性变换，通过闭式解将LRLs的有害表征映射到HRLs的鲁棒安全子空间，同时利用零空间投影约束保持通用性能。在8种语言及Llama-3、Qwen-2.5等多个模型上的实验表明，该方法通过一次数据高效的计算，显著降低了LRLs的攻击成功率，且对通用推理能力影响微乎其微。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic AI for Intent-driven Optimization in Cell-free O-RAN</div>
<div class="meta-line">Authors: Mohammad Hossein Shokouhi, Vincent W. S. Wong</div>
<div class="meta-line">First: 2026-02-26T02:26:58+00:00 · Latest: 2026-02-26T02:26:58+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE International Conference on Communications (ICC), Glasgow, UK, May 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22539v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22539v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向无蜂窝O-RAN意图驱动优化的智能体人工智能</div>
<div class="mono" style="margin-top:8px">智能体人工智能正成为自主无线接入网络的关键使能技术，其中多个基于大语言模型的智能体通过推理与协作实现运营商定义的意图。开放式无线接入网络架构支持此类智能体的部署与协调。然而，现有研究多集中于独立智能体处理的简单意图，而需要多智能体协同的复杂意图尚未得到充分探索。本文提出一种用于无蜂窝O-RAN意图翻译与优化的智能体AI框架：监督智能体将运营商意图转化为优化目标与最低速率要求；用户权重智能体依据该信息从记忆模块检索先验经验以确定预编码用户优先级权重；若意图包含节能目标，则开放式无线单元管理智能体将通过深度强化学习算法激活以确定运行中的O-RU集合；监控智能体实时测量用户数据速率并协同其他智能体保障最低速率要求。为提升可扩展性，采用参数高效微调方法使同一底层大语言模型可适配不同智能体。仿真结果表明：在节能模式下，所提框架相比三种基线方案可减少41.93%的活跃O-RU数量；采用PEFT方法时，内存使用量较部署独立大语言模型智能体降低92%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to handle complex operator intents in autonomous radio access networks, which require coordination among multiple AI agents, a gap not addressed by prior works focusing on simple intents handled independently. The method proposes an agentic AI framework for cell-free O-RAN, where a supervisor agent translates intents into optimization goals, a user weighting agent sets priorities using memory, an O-RU management agent activates a DRL algorithm for energy saving, and a monitoring agent ensures rate requirements, all supported by a parameter-efficient fine-tuning approach to share a base LLM across agents. The main experimental results demonstrate that in energy-saving mode, the framework reduces active radio units by 41.93% compared to baselines and cuts memory usage by 92% through efficient fine-tuning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于处理自主无线接入网络中复杂的运营商意图，这些意图需要多个AI代理之间的协调，而现有工作主要关注由独立代理处理的简单意图，存在研究空白。方法上，提出了一个用于无蜂窝O-RAN的代理AI框架，其中监督代理将意图转化为优化目标，用户加权代理利用记忆模块确定优先级，O-RU管理代理在节能目标下使用深度强化学习算法激活，监控代理则保障最低速率要求，并采用参数高效微调技术使不同代理共享同一基础大语言模型。主要实验结果表明，在节能模式下，该框架相比三种基线方案将活跃无线单元数量减少了41.93%，并通过高效微调将内存使用降低了92%。</div>
</details>
</div>
<div class="card">
<div class="title">RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?</div>
<div class="meta-line">Authors: Rohan Gupta, Erik Jenner</div>
<div class="meta-line">First: 2025-06-17T07:22:20+00:00 · Latest: 2026-02-26T01:45:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.14261v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.14261v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent-space monitors aim to detect undesirable behaviours in Large Language Models by leveraging their internal representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions. However, these monitors may themselves become training signals, for example, by using problematic samples found in deployment to retrain models. This raises an important question: can models learn to evade such monitors? To evaluate this capability, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to evade latent-space monitors while maintaining their blackbox behaviour. We apply RL-Obfuscation to Language Models ranging from 7B to 14B parameters and evaluate their Evasion Success Rate against a suite of monitors. We find that token-level monitors are highly vulnerable to this attack while more holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, for these vulnerable monitors, models trained to evade a single static monitor can generalise to evade other unseen monitors. We also find that the models can be trained to conditionally bypass latent-space monitors on only certain inputs. Finally, we study how the models bypass these monitors and find that the model can learn to repurpose tokens to have different internal representations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RL-Obfuscation：语言模型能否学会规避潜在空间监控器？</div>
<div class="mono" style="margin-top:8px">潜在空间监控器旨在通过利用大型语言模型的内部表征而非仅依赖黑盒输出来检测其不良行为，已在识别欺骗性及不安全生成等行为方面展现出潜力。然而，这些监控器本身可能成为训练信号，例如利用部署中发现的问题样本对模型进行再训练。这引出一个关键问题：模型能否学会规避此类监控器？为评估此能力，我们提出RL-Obfuscation方法，通过强化学习微调语言模型，使其在保持黑盒行为的同时规避潜在空间监控器。我们在7B至14B参数规模的模型上应用该方法，并针对一组监控器评估其规避成功率。研究发现：基于词元的监控器极易受此类攻击，而更全局的监控器（如最大池化或基于注意力的探针）则保持稳健；对于脆弱型监控器，针对单一静态监控器训练的模型能泛化至规避其他未见过的监控器；模型还可被训练为仅针对特定输入条件性地绕过监控器。最后，我们探究了模型的规避机制，发现模型能学会重新分配词元以改变其内部表征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether large language models (LLMs) can learn to evade latent-space monitors, which detect undesirable behaviors using internal representations rather than just outputs. The authors introduce RL-Obfuscation, a method that fine-tunes LLMs via reinforcement learning to bypass these monitors while preserving their external behavior. Experimental results on models from 7B to 14B parameters show that token-level monitors are highly vulnerable to evasion, with models generalizing to unseen monitors and conditionally bypassing them on specific inputs, often by repurposing tokens to alter internal representations, whereas more holistic monitors like max-pooling probes remain robust.</div>
<div class="mono" style="margin-top:8px">本文研究大型语言模型能否学会规避潜在空间监控器，这些监控器利用内部表征而非仅输出来检测不良行为。作者提出了RL-Obfuscation方法，通过强化学习微调模型，在保持外部行为的同时绕过监控器。在7B至14B参数模型上的实验结果表明，基于词元的监控器极易被规避，模型能泛化到未见过的监控器并针对特定输入条件性绕过，通常通过重新利用词元改变内部表征来实现，而更整体的监控器（如最大池化探针）则保持稳健。</div>
</details>
</div>
<div class="card">
<div class="title">A Mathematical Theory of Agency and Intelligence</div>
<div class="meta-line">Authors: Wael Hafez, Chenan Wei, Rodrigo Felipe, Amir Nazeri, Cameron Reid</div>
<div class="meta-line">First: 2026-02-26T01:26:21+00:00 · Latest: 2026-02-26T01:26:21+00:00</div>
<div class="meta-line">Comments: 20 pages, 4 figuers</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22519v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22519v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理与智能的数学理论</div>
<div class="mono" style="margin-top:8px">为在变化条件下可靠运行，复杂系统需要关于其资源使用效率的反馈，而不仅仅是目标是否达成。当前人工智能系统处理海量信息以生成复杂预测，但预测可能看似成功，而系统与环境的底层交互却在退化。缺失的是一个原则性度量，用于衡量系统部署的总信息中，有多少实际在其观察、行动与结果之间共享。我们证明这一共享比例（称为双可预测性P）内在于任何交互，可从第一性原理推导，且严格有界：P在量子系统中可达1，在经典系统中等于或小于0.5，引入代理（行动选择）后进一步降低。我们在物理系统（双摆）、强化学习智能体及多轮大语言模型对话中验证了这些界限。这些结果区分了代理与智能：代理是基于预测行动的能力，而智能还需从交互中学习、自我监测学习效率，并调整观察、行动与结果的范围以恢复有效学习。按此定义，当前人工智能系统实现了代理而非智能。受生物系统中丘脑皮层调节的启发，我们展示了一种实时监测P的反馈架构，为自适应、强韧的人工智能奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for complex systems to assess their resource effectiveness beyond mere objective achievement, this paper introduces &#x27;bipredictability&#x27; (P) as a fundamental measure of the shared information between a system&#x27;s observations, actions, and outcomes. The method derives this measure from first principles, proving it is intrinsic to any interaction and establishing theoretical bounds: P can reach 1 in quantum systems, is ≤0.5 in classical systems, and is further reduced by agency. Experimental results confirm these bounds in a physical double pendulum, reinforcement learning agents, and multi-turn LLM conversations. The findings distinguish agency (acting on predictions) from intelligence (which additionally requires learning from interaction and self-monitoring), concluding current AI systems possess agency but not intelligence, and the paper proposes a thalamocortical-inspired feedback architecture to monitor P for building adaptive AI.</div>
<div class="mono" style="margin-top:8px">本文的动机在于复杂系统需要评估其资源使用效率，而不仅仅是目标达成情况，为此引入了“双可预测性”（P）作为衡量系统观察、行动与结果之间共享信息的基本指标。方法上从第一性原理推导出这一度量，证明其为任何交互所固有，并确立了理论界限：在量子系统中P可达1，在经典系统中P≤0.5，且引入主体性（行动选择）后会进一步降低。主要实验结果在物理双摆、强化学习智能体和多轮大语言模型对话中验证了这些界限。研究区分了主体性（基于预测行动）与智能（还需从交互中学习并自我监控），指出当前AI系统只具备主体性而非智能，并受生物丘脑皮质调节启发，提出了一种实时监控P的反饋架构，为构建自适应、有韧性的AI奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Space Syntax-guided Post-training for Residential Floor Plan Generation</div>
<div class="meta-line">Authors: Zhuoyang Jiang, Dongqing Zhang</div>
<div class="meta-line">First: 2026-02-26T00:54:38+00:00 · Latest: 2026-02-26T00:54:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22507v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22507v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained generative models for residential floor plans are typically optimized to fit large-scale data distributions, which can under-emphasize critical architectural priors such as the configurational dominance and connectivity of domestic public spaces (e.g., living rooms and foyers). This paper proposes Space Syntax-guided Post-training (SSPT), a post-training paradigm that explicitly injects space syntax knowledge into floor plan generation via a non-differentiable oracle. The oracle converts RPLAN-style layouts into rectangle-space graphs through greedy maximal-rectangle decomposition and door-mediated adjacency construction, and then computes integration-based measurements to quantify public space dominance and functional hierarchy.
  To enable consistent evaluation and diagnosis, we further introduce SSPT-Bench (Eval-8), an out-of-distribution benchmark that post-trains models using conditions capped at $\leq 7$ rooms while evaluating on 8-room programs, together with a unified metric suite for dominance, stability, and profile alignment. SSPT is instantiated with two strategies: (i) iterative retraining via space-syntax filtering and diffusion fine-tuning, and (ii) reinforcement learning via PPO with space-syntax rewards. Experiments show that both strategies improve public-space dominance and restore clearer functional hierarchy compared to distribution-fitted baselines, while PPO achieves stronger gains with substantially higher compute efficiency and reduced variance. SSPT provides a scalable pathway for integrating architectural theory into data-driven plan generation and is compatible with other generative backbones given a post-hoc evaluation oracle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间句法引导的住宅平面图生成后训练方法</div>
<div class="mono" style="margin-top:8px">住宅平面图的预训练生成模型通常以拟合大规模数据分布为目标，这可能弱化关键的建筑先验知识，例如家庭公共空间（如客厅和门厅）的配置主导性与连通性。本文提出空间句法引导的后训练范式，通过不可微的评估模块将空间句法知识显式注入平面图生成过程。该模块通过贪心最大矩形分解和门中介邻接关系构建，将RPLAN式布局转换为矩形空间图，进而计算基于整合度的度量以量化公共空间主导性与功能层级。为支持一致性评估与诊断，我们进一步提出SSPT-Bench（Eval-8）——一种分布外基准测试集，其使用≤7个房间的条件进行模型后训练，并在8房间方案上评估，同时配备用于主导性、稳定性和轮廓对齐的统一度量体系。SSPT通过两种策略实现：（1）基于空间句法筛选与扩散微调的迭代重训练；（2）采用PPO强化学习框架结合空间句法奖励。实验表明，两种策略均能提升公共空间主导性并恢复更清晰的功能层级，其中PPO策略在显著提升计算效率、降低方差的同时获得更强性能增益。SSPT为将建筑理论融入数据驱动的平面生成提供了可扩展路径，且兼容其他具备后验评估模块的生成模型架构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that pre-trained generative models for residential floor plans often fail to adequately capture key architectural priors like the configurational dominance and connectivity of public spaces. To address this, the authors propose Space Syntax-guided Post-training (SSPT), a method that injects space syntax knowledge into generation via a non-differentiable oracle, which analyzes layouts to compute integration-based metrics for public space dominance and functional hierarchy. For evaluation, they introduce SSPT-Bench, an out-of-distribution benchmark with a unified metric suite. The main experimental results demonstrate that two SSPT strategies—iterative retraining and reinforcement learning with PPO—both improve public-space dominance and functional hierarchy over baselines, with PPO achieving stronger performance gains, higher computational efficiency, and reduced variance.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到用于住宅平面图的预训练生成模型通常未能充分捕捉关键的建筑先验，如公共空间的配置主导性和连通性。为解决此问题，作者提出了空间句法引导的后训练方法，该方法通过一个不可微的预言机将空间句法知识注入生成过程，该预言机分析布局以计算基于整合度的公共空间主导性和功能层次度量。为进行评估，他们引入了SSPT-Bench这一分布外基准测试及统一度量套件。主要实验结果表明，两种SSPT策略——迭代再训练和使用PPO的强化学习——均相较于基线模型提升了公共空间主导性和功能层次，其中PPO策略取得了更强的性能增益、更高的计算效率和更低的方差。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement-aware Knowledge Distillation for LLM Reasoning</div>
<div class="meta-line">Authors: Zhaoyang Zhang, Shuli Jiang, Yantao Shen, Yuting Zhang, Dhananjay Ram, Shuo Yang, Zhuowen Tu, Wei Xia, Stefano Soatto</div>
<div class="meta-line">First: 2026-02-26T00:20:39+00:00 · Latest: 2026-02-26T00:20:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22495v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22495v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student&#x27;s evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型推理的强化感知知识蒸馏</div>
<div class="mono" style="margin-top:8px">强化学习（RL）后训练近期显著提升了长链思维推理大语言模型（LLMs）的性能，但此类模型的高推理成本促使研究者将其蒸馏至更小的学生模型。现有知识蒸馏（KD）方法多针对监督微调（SFT）设计，依赖固定的教师轨迹或基于师生KL散度的正则化。当与强化学习结合时，这些方法常面临分布失配与目标冲突：教师监督可能与学生动态演化的推演分布不一致，且KL正则项可能与奖励最大化目标相互竞争，需要精细的损失平衡。为解决这些问题，我们提出强化感知蒸馏（RLAD），在强化学习过程中执行选择性模仿——仅当教师能改进当前策略更新时才引导学生模型向其靠拢。其核心组件信任域比率蒸馏（TRRD）采用基于教师-旧策略混合的PPO/GRPO式似然比目标，替代原有的师生KL正则项，从而在学生推演上实现优势感知、信任域约束的蒸馏，自然平衡探索、利用与模仿过程。在多项逻辑推理与数学基准测试中，RLAD持续优于离线蒸馏、标准GRPO以及基于KL的在线师生知识蒸馏方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the high inference cost of large language models (LLMs) enhanced by reinforcement learning (RL) for reasoning tasks, by distilling them into smaller models, while overcoming distribution mismatch and objective interference issues in existing knowledge distillation methods when combined with RL. The method introduces RL-aware distillation (RLAD), which selectively imitates the teacher during RL using a core component called Trust Region Ratio Distillation (TRRD) that replaces traditional KL regularization with a likelihood-ratio objective anchored to a teacher-old-policy mixture, enabling advantage-aware and trust-region-bounded distillation. The main experimental results show that RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy distillation across diverse logic reasoning and math benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过将经过强化学习增强的大型语言模型蒸馏为更小模型，以降低其推理成本，同时解决现有知识蒸馏方法与强化学习结合时存在的分布不匹配和目标干扰问题。方法上提出了强化学习感知蒸馏（RLAD），在强化学习过程中选择性模仿教师模型，其核心组件信任域比率蒸馏（TRRD）用基于教师-旧策略混合的似然比目标替代传统的KL正则化，实现了优势感知和信任域约束的蒸馏。主要实验结果表明，在多种逻辑推理和数学基准测试中，RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy distillation across diverse logic reasoning and math benchmarks。</div>
</details>
</div>
<div class="card">
<div class="title">LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation</div>
<div class="meta-line">Authors: Hejia Zhang, Zhongming Yu, Chia-Tung Ho, Haoxing Ren, Brucek Khailany, Jishen Zhao</div>
<div class="meta-line">First: 2026-02-18T23:36:46+00:00 · Latest: 2026-02-25T21:49:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16953v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16953v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM4Cov：面向高覆盖率测试平台生成的可执行感知智能体学习</div>
<div class="mono" style="margin-top:8px">可执行感知的LLM智能体为从工具反馈中学习提供了有前景的范式，但此类反馈通常获取成本高昂且速度缓慢，使得在线强化学习（RL）难以实施。高覆盖率硬件验证正是这一挑战的典型体现，因其依赖工业级模拟器和不可微分的执行信号。我们提出LLM4ov——一种离线智能体学习框架，将验证建模为由确定性评估器引导的无记忆状态转移。基于此形式化框架，我们引入执行验证数据筛选、策略感知智能体数据合成及最差状态优先采样机制，以在执行约束下实现可扩展学习。我们进一步通过修订的评估协议，从现有验证套件中构建了贴合实际场景的基准测试。采用该流程后，一个紧凑的40亿参数模型在智能体评估中达到69.2%的覆盖率通过率，较其教师模型提升5.3%，并与规模大一个数量级的模型展现出可比性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of using execution feedback from slow and expensive tools, such as industrial hardware simulators, for online reinforcement learning in high-coverage testbench generation. It proposes LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions and employs techniques like execution-validated data curation and worst-state-prioritized sampling to enable scalable learning. Experimental results show that a compact 4B-parameter model trained with this pipeline achieves a 69.2% coverage pass rate, outperforming its teacher by 5.3% and competing with much larger models.</div>
<div class="mono" style="margin-top:8px">本文针对在高覆盖率测试平台生成中，使用来自工业模拟器等缓慢且昂贵工具的执行反馈进行在线强化学习所面临的挑战，提出了LLM4Cov离线智能体学习框架。该框架将验证建模为无记忆状态转移，并采用执行验证数据筛选和最差状态优先采样等技术，以实现约束下的可扩展学习。实验结果表明，通过该流程训练的紧凑型40亿参数模型达到了69.2%的覆盖率通过率，比其教师模型高出5.3%，并与规模大一个数量级的模型性能相当。</div>
</details>
</div>
<div class="card">
<div class="title">Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning</div>
<div class="meta-line">Authors: Yihe Deng, I-Hung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister, Chen-Yu Lee</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-29T22:05:08+00:00 · Latest: 2026-02-25T21:18:44+00:00</div>
<div class="meta-line">Comments: Paper accepted by ICLR 2026. The first two authors contribute equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25992v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25992v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical &quot;actions&quot;. SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model&#x27;s actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>监督强化学习：从专家轨迹到逐步推理</div>
<div class="mono" style="margin-top:8px">大语言模型在处理需要多步推理的问题时常面临困难。对于小规模开源模型，基于可验证奖励的强化学习在多次尝试后仍难以采样到正确解，而监督微调则容易因逐词僵化模仿长示例导致过拟合。为弥补这一不足，我们提出监督强化学习框架，将问题求解重构为生成逻辑“动作”序列的过程。该框架训练模型在执行每个动作前生成内部推理独白，并根据模型动作与从监督微调数据集中逐步提取的专家动作之间的相似性提供平滑奖励。这种监督机制即使在全轨迹错误时也能提供更丰富的学习信号，同时鼓励在专家示范引导下进行灵活推理。实验表明，监督强化学习能使小模型掌握监督微调或基于可验证奖励的强化学习无法习得的复杂问题。此外，先用监督强化学习初始化训练再进行基于可验证奖励的强化学习微调，可获得最佳综合性能。除推理基准测试外，监督强化学习在智能体软件工程任务中也展现出良好泛化能力，确立了其作为面向推理大语言模型的稳健通用训练框架地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing methods like Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) in training small-scale LLMs for multi-step reasoning, where SFT overfits to demonstrations and RLVR fails when correct solutions are rarely sampled. The authors propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem-solving as generating sequences of logical actions, training models to produce internal reasoning monologues before each action and providing step-wise rewards based on similarity to expert actions from SFT data. Experimental results show that SRL enables small models to learn previously unlearnable challenging reasoning problems, and initializing training with SRL before RLVR refinement yields the strongest overall performance, with effective generalization to agentic software engineering tasks.</div>
<div class="mono" style="margin-top:8px">本文针对现有方法如监督微调（SFT）和带可验证奖励的强化学习（RLVR）在训练小规模大语言模型进行多步推理时的局限性，即SFT容易对演示数据过拟合而RLVR在正确解极少被采样时失效，提出了监督强化学习（SRL）框架。该方法将问题解决重构为生成逻辑“动作”序列，训练模型在每步动作前生成内部推理独白，并基于与SFT数据中专家动作的相似性提供逐步奖励。实验结果表明，SRL使小模型能够学习之前SFT或RLVR无法掌握的复杂推理问题，且先用SRL初始化训练再用RLVR精调能获得最佳整体性能，并能有效泛化至自主软件工程任务。</div>
</details>
</div>
<div class="card">
<div class="title">Stagewise Reinforcement Learning and the Geometry of the Regret Landscape</div>
<div class="meta-line">Authors: Chris Elliott, Einar Urdshals, David Quarel, Matthew Farrugia-Roberts, Daniel Murfet</div>
<div class="meta-line">First: 2026-01-12T13:25:21+00:00 · Latest: 2026-02-25T18:40:10+00:00</div>
<div class="meta-line">Comments: 48 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07524v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07524v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Singular learning theory characterizes Bayesian learning as an evolving tradeoff between accuracy and complexity, with transitions between qualitatively different solutions as sample size increases. We extend this theory to reinforcement learning, proving that the concentration of a generalized posterior over policies is governed by the local learning coefficient (LLC), an invariant of the geometry of the regret function. This theory predicts that deep reinforcement learning with SGD should proceed from simple policies with high regret to complex policies with low regret. We verify this prediction empirically in a gridworld environment exhibiting stagewise policy development: phase transitions over training manifest as &quot;opposing staircases&quot; where regret decreases sharply while the LLC increases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阶段性强化学习与遗憾函数几何结构</div>
<div class="mono" style="margin-top:8px">奇异学习理论将贝叶斯学习描述为准确性与复杂性之间动态权衡的过程，随着样本量增加会出现不同质态解之间的跃迁。我们将该理论拓展至强化学习领域，证明策略广义后验分布的集中性由遗憾函数几何结构的不变量——局部学习系数（LLC）所决定。该理论预测采用随机梯度下降的深度强化学习会经历从高遗憾简单策略到低遗憾复杂策略的演进过程。我们在网格世界环境中通过实证验证了这一预测：训练过程中出现的阶段性策略发展表现为&quot;对向阶梯&quot;现象，即遗憾值急剧下降的同时局部学习系数同步上升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by extending singular learning theory from Bayesian learning to reinforcement learning (RL), this paper introduces a geometric perspective on the regret landscape to explain stagewise learning dynamics. The method involves proving that the concentration of a generalized posterior over policies is governed by the local learning coefficient (LLC), an invariant derived from the geometry of the regret function, which predicts that deep RL with SGD should transition from simple, high-regret policies to complex, low-regret ones. Experimental results in a gridworld environment confirm this prediction, showing phase transitions during training manifested as &quot;opposing staircases&quot; where regret decreases sharply while the LLC increases, validating the theory&#x27;s empirical relevance.</div>
<div class="mono" style="margin-top:8px">本文的动机是将奇异学习理论从贝叶斯学习扩展到强化学习，通过引入遗憾函数的几何视角来解释阶段性学习动态。方法上，证明了策略的广义后验集中性由局部学习系数（LLC）控制，该系数是遗憾函数几何结构的不变量，预测了使用随机梯度下降的深度强化学习应从简单高遗憾策略向复杂低遗憾策略过渡。在网格世界环境中的实验结果验证了这一预测，显示了训练期间的相变表现为“反向阶梯”现象，即遗憾急剧下降的同时LLC上升，从而支持了理论的实际适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual</div>
<div class="meta-line">Authors: Yining Li, Peizhong Ju, Ness Shroff</div>
<div class="meta-line">First: 2026-02-25T17:54:52+00:00 · Latest: 2026-02-25T17:54:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22146v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22146v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于乐观原始对偶的多目标安全大语言模型对齐可证明的末次迭代收敛性</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）在使大语言模型（LLM）与人类偏好对齐方面发挥着重要作用。虽然带期望奖励约束的RLHF可表述为原始对偶优化问题，但标准原始对偶方法仅能保证在凸凹形式的鞍点问题下，基于分布策略的收敛性。此外，在实际应用中，标准原始对偶方法在策略参数化下可能出现末次迭代不稳定或发散。本研究提出一个通用的安全RLHF原始对偶框架，统一了包括安全RLHF、单次优化及多次优化在内的多种现有对齐算法。基于此框架，我们提出一种乐观原始对偶（OPD）算法，通过对原始变量和对偶变量引入预测性更新来稳定鞍点动态。我们为该算法建立了末次迭代收敛保证，涵盖分布空间中的精确策略优化，以及在参数化策略下收敛至最优解邻域（其误差间隙与近似误差及偏差相关）。分析表明，乐观机制对缓解约束对齐目标固有的振荡现象具有关键作用，从而弥合了约束强化学习与实际RLHF之间的重要理论鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the instability and lack of last-iterate convergence guarantees in standard primal-dual methods for safe Reinforcement Learning from Human Feedback (RLHF), which is crucial for aligning Large Language Models with human preferences under constraints. The proposed method introduces a universal primal-dual framework that unifies existing alignment algorithms and an Optimistic Primal-Dual (OPD) algorithm with predictive updates to stabilize saddle-point dynamics. The main experimental results demonstrate that OPD achieves provable last-iterate convergence, both for exact policy optimization in distributional space and for convergence to a neighborhood of the optimal solution under parameterized policies, effectively mitigating oscillations in constrained alignment objectives.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，用于安全人类反馈强化学习（RLHF）的标准原始对偶方法存在不稳定性和缺乏最终迭代收敛保证的问题，而RLHF对于在约束条件下将大语言模型与人类偏好对齐至关重要。该方法提出了一个统一的原始对偶框架，整合了现有对齐算法，并引入了一种带有预测性更新的乐观原始对偶（OPD）算法以稳定鞍点动态。主要实验结果表明，OPD算法实现了可证明的最终迭代收敛，包括在分布空间中的精确策略优化以及在参数化策略下收敛到最优解的邻域，有效缓解了约束对齐目标中固有的振荡问题。</div>
</details>
</div>
<div class="card">
<div class="title">Active operator learning with predictive uncertainty quantification for partial differential equations</div>
<div class="meta-line">Authors: Nick Winovich, Mitchell Daneker, Lu Lu, Guang Lin</div>
<div class="meta-line">First: 2025-03-05T04:48:14+00:00 · Latest: 2026-02-25T17:27:35+00:00</div>
<div class="meta-line">Comments: Submitted to the Journal of Computational Physics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03178v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.03178v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework&#x27;s uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>偏微分方程主动算子学习与预测不确定性量化</div>
<div class="mono" style="margin-top:8px">随着神经算子在快速求解偏微分方程中的应用日益广泛，理解模型预测的准确性及相关误差水平对于在科学应用中部署可靠的代理模型至关重要。现有不确定性量化框架采用集成或贝叶斯方法，在训练和推断阶段均可能产生高昂计算成本。本文提出一种专为深度算子网络设计的轻量级预测不确定性量化方法，该方法亦可推广至其他算子网络。在线性与非线性偏微分方程上的数值实验表明，该框架的不确定性估计具有无偏性，并在训练数据集足够大时能提供准确的分布外不确定性预测。本框架提供快速推断与不确定性估计，可高效驱动传统求解器难以承担的外循环分析。我们展示了如何将预测不确定性应用于贝叶斯优化与主动学习问题，以提升外循环优化过程的精度与数据效率。在主动学习场景中，我们将框架扩展至傅里叶神经算子，并描述了适用于其他算子网络的通用方法。为实现实时部署，我们引入基于预计算主干输出与稀疏放置矩阵的推断策略，将评估时间缩短五倍以上。该方法为时间敏感场景下的不确定性感知算子学习提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for reliable uncertainty quantification (UQ) in neural operator models for solving partial differential equations (PDEs), as existing UQ methods like ensembles are computationally expensive. The authors propose a lightweight predictive UQ framework tailored for DeepONets, which also generalizes to other operator networks such as Fourier Neural Operators (FNO), featuring an inference strategy that speeds up evaluation by over five times through precomputed trunk outputs and sparse matrices. Experimental results on linear and nonlinear PDEs show that the framework provides unbiased uncertainty estimates and accurate out-of-distribution predictions with sufficient training data, enabling efficient applications in Bayesian optimization and active learning to improve accuracy and data-efficiency in outer-loop analyses.</div>
<div class="mono" style="margin-top:8px">本文针对求解偏微分方程的神经算子模型需要可靠不确定性量化的问题，提出了一种轻量级预测性不确定性量化框架，专为DeepONet设计并可推广至其他算子网络如傅里叶神经算子，通过预计算主干输出和稀疏矩阵将评估速度提升五倍以上。在线性和非线性偏微分方程的数值实验中，该框架在足够训练数据下提供了无偏的不确定性估计和准确的分布外预测，从而在贝叶斯优化和主动学习等应用中有效提升了外层循环优化的精度和数据效率。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents</div>
<div class="meta-line">Authors: Patrick Tser Jern Kon, Archana Pradeep, Ang Chen, Alexander P. Ellis, Warren Hunt, Zijian Wang, John Yang, Samuel Thompson</div>
<div class="meta-line">First: 2026-02-25T17:11:49+00:00 · Latest: 2026-02-25T17:11:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22124v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Protégé：学习选择性协同专家解锁小语言模型作为软件工程智能体</div>
<div class="mono" style="margin-top:8px">小语言模型（SLMs）在成本、延迟和适应性方面具有显著优势，但在SWE-bench等长周期软件工程任务中始终落后于大模型，普遍存在动作循环和低解决率问题。我们提出SWE-Protégé后训练框架，将软件修复重构为专家-学徒协同问题。该框架中，SLM作为唯一决策者，学习选择性寻求强专家模型指导、识别停滞状态并执行专家反馈。该方法结合专家增强轨迹的监督微调与智能体强化学习，显式抑制退化循环和无效专家协作。通过对Qwen2.5-Coder-7B-Instruct进行轻量后训练，在SWE-bench Verified上实现42.4%的Pass@1，较先前SLM最佳水平提升25.4%，且专家调用稀疏（约每任务4次调用，占总token数11%）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enhance the performance of small language models (SLMs) on complex software engineering tasks like SWE-bench, where they typically suffer from issues like action looping and low resolution rates. The method introduces SWE-Protégé, a post-training framework that treats software repair as a collaboration problem, enabling an SLM to act as the primary decision-maker while learning to selectively consult a stronger expert model, detect stalled states, and implement expert feedback. This combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning to reduce degenerative behaviors. The main experimental results show that applying this approach to Qwen2.5-Coder-7B-Instruct achieves 42.4% Pass@1 on SWE-bench Verified, a 25.4% improvement over prior SLM state-of-the-art, while using expert assistance efficiently with only about 4 calls per task and 11% of total tokens.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升小型语言模型在复杂软件工程任务（如SWE-bench）上的性能，这些模型通常存在动作循环和解决率低的问题。方法上提出了SWE-Protégé，这是一个后训练框架，将软件修复重构为专家与新手协作问题，使小型语言模型作为主要决策者，学习选择性向更强专家模型寻求指导、识别停滞状态并执行反馈。该方法结合了基于专家增强轨迹的监督微调和智能体强化学习，以减少退化行为。主要实验结果表明，将此框架应用于Qwen2.5-Coder-7B-Instruct后，在SWE-bench Verified上达到了42.4%的Pass@1，比先前小型语言模型的最佳性能提升了25.4%，同时高效利用专家协助，平均每个任务仅调用约4次专家，且专家生成的令牌仅占总令牌的11%。</div>
</details>
</div>
<div class="card">
<div class="title">A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation</div>
<div class="meta-line">Authors: Georgios Kamaras, Subramanian Ramamoorthy</div>
<div class="meta-line">Venue: In IEEE Robotics and Automation Letters, Volume 10, Issue 8, August 2025, Pages 8075-8082</div>
<div class="meta-line">First: 2025-02-25T20:01:06+00:00 · Latest: 2026-02-25T17:09:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18615v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.18615v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向视觉驱动可变形线性物体操作中物体中心智能体自适应的Real2Sim2Real分布化处理方法</div>
<div class="mono" style="margin-top:8px">本文针对基于视觉感知的可变形线性物体操作问题，提出一种集成式Real2Sim2Real框架。通过对参数化DLO集合采用免似然推断计算物理参数的后验分布，实现对特定DLO行为的近似仿真。在模拟训练中，我们利用这些后验分布进行领域随机化，采用无模型强化学习方法训练面向DLO抓取任务的物体特异性视觉运动策略（仅依赖视觉与本体感知）。通过零样本方式将模拟训练的DLO操作策略部署至现实场景（无需微调），验证了该方法的有效性。在此背景下，我们评估了主流LFI方法仅利用动态操作轨迹中的视觉与本体感知数据，在参数化DLO集合上进行精细分类的能力，进而探究所得领域分布对模拟策略学习及现实场景性能的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of adapting robotic agents to manipulate diverse deformable linear objects (DLOs) using vision, this paper introduces an integrated Real2Sim2Real framework. The method employs likelihood-free inference to compute posterior distributions over physical parameters from real-world visual and proprioceptive data, enabling the simulation of specific DLO behaviors; these posteriors are then used for domain randomization during the training of visuomotor policies via model-free reinforcement learning in simulation. Experimental results demonstrate that policies trained in simulation can be deployed zero-shot in the real world, successfully performing DLO reaching tasks, and the study validates the framework&#x27;s capacity for fine classification over DLO parameters and its impact on sim-to-real transfer performance.</div>
<div class="mono" style="margin-top:8px">针对机器人如何利用视觉感知适应不同可变形线性物体的操控挑战，本文提出了一种集成的Real2Sim2Real框架。该方法采用无似然推理，根据真实世界的视觉和本体感知数据计算物理参数的后验分布，从而模拟特定物体的行为；这些后验分布被用于在仿真中通过无模型强化学习训练视觉运动策略时的领域随机化。实验结果表明，在仿真中训练的策略能够以零样本方式部署到真实世界，成功完成可变形线性物体的抓取任务，并且研究验证了该框架在物体参数精细分类上的能力及其对仿真到现实迁移性能的影响。</div>
</details>
</div>
<div class="card">
<div class="title">RebuttalAgent: Strategic Persuasion in Academic Rebuttal via Theory of Mind</div>
<div class="meta-line">Authors: Zhitao He, Zongwei Lyu, Yi R Fung</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-22T07:36:48+00:00 · Latest: 2026-02-25T16:22:14+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15715v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.15715v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) framework that models reviewer mental state, formulates persuasion strategy, and generates evidence-based response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反驳智能体：基于心智理论的学术反驳策略性说服</div>
<div class="mono" style="margin-top:8px">尽管人工智能已深度融入研究流程的各个阶段并取得显著进展，学术反驳仍是一个重要且尚未充分探索的挑战。这是因为反驳是在严重信息不对称下进行的策略性沟通过程，而非简单的技术辩论。现有方法大多仅模仿表层语言特征，缺乏有效说服所需的关键视角采择能力，因此难以应对这一挑战。本文提出首个基于心智理论的学术反驳框架——反驳智能体，通过心智理论-策略-响应的三层架构实现：建模审稿人心理状态、制定说服策略、生成基于证据的回应。为训练该智能体，我们通过创新的批判-精炼方法构建了大规模数据集反驳基准。训练过程分为两个阶段：首先通过监督微调使智能体掌握基于心智理论的分析与策略规划能力，随后利用自奖励机制进行强化学习以实现可扩展的自我改进。为进行可靠高效的自动化评估，我们进一步开发了专门评估器反驳评估模型，该模型基于超过10万条多源反驳数据训练，其评分与人类偏好的一致性已超越强大的GPT-4.1评判模型。大量实验表明，反驳智能体在自动化指标上平均超越基线模型18.3%，同时在自动化与人工评估中均优于先进的专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complex strategic communication and information asymmetry inherent in academic rebuttals, which existing AI approaches fail to address by focusing only on surface-level linguistics, this paper introduces RebuttalAgent, a framework grounded in Theory of Mind (ToM). The method employs a ToM-Strategy-Response (TSR) framework to model reviewer mental states and formulate persuasive strategies, trained via supervised fine-tuning and reinforcement learning on a novel large-scale dataset, RebuttalBench, synthesized through critique-and-refine. Main experimental results demonstrate that RebuttalAgent, evaluated by a specialized automated evaluator, Rebuttal-RM, outperforms the base model by 18.3% on automated metrics and surpasses advanced proprietary models in both automated and human evaluations.</div>
<div class="mono" style="margin-top:8px">本文的动机在于学术反驳中固有的复杂策略性沟通和信息不对称问题，现有AI方法仅关注表面语言而无法解决。为此，论文提出了基于心智理论（ToM）的RebuttalAgent框架，其方法采用ToM-策略-响应（TSR）框架来建模审稿人心理状态并制定说服策略，并通过监督微调和强化学习在基于批判-精炼合成的大规模数据集RebuttalBench上进行训练。主要实验结果表明，RebuttalAgent在专门自动化评估器Rebuttal-RM的评测下，在自动化指标上平均超越基础模型18.3%，并在自动和人工评估中均优于先进的专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection</div>
<div class="meta-line">Authors: Dhiraj Neupane, Richard Dazeley, Mohamed Reda Bouadjenek, Sunil Aryal</div>
<div class="meta-line">First: 2026-02-25T15:34:19+00:00 · Latest: 2026-02-25T15:34:19+00:00</div>
<div class="meta-line">Comments: This article is accepted to be published in AAMAS2026. The doi is listed below but the production is on the way as of now (26/02/2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22297v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22297v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL&#x27;s sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator&#x27;s learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL&#x27;s sequential reasoning with MFD&#x27;s temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习奖励而非标签：基于对抗性逆强化学习的机械故障检测</div>
<div class="mono" style="margin-top:8px">强化学习（RL）为机械故障检测（MFD）提供了重要前景。然而，现有基于RL的MFD方法大多未能充分发挥RL的序列决策优势，常将MFD简化为情境赌博机问题。为弥补这一不足，我们将MFD构建为离线逆强化学习问题，使智能体直接从健康运行序列中学习奖励动态，从而避免手动设计奖励函数和故障标签的需求。本框架采用对抗性逆强化学习训练判别器，以区分正常（专家）策略生成的状态转移。判别器习得的奖励函数可作为异常评分，指示偏离正常运行行为的程度。在三个全寿命周期基准数据集（HUMS2023、IMS和XJTU-SY）上的评估表明，该模型能持续为正常样本分配低异常分、为故障样本分配高分，实现早期鲁棒的故障检测。通过将RL的序列推理能力与MFD的时序结构对齐，本研究为数据驱动的工业场景中基于RL的诊断技术开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underutilization of reinforcement learning&#x27;s sequential decision-making capabilities in existing machinery fault detection methods, which often treat the problem as a contextual bandit, this paper formulates fault detection as an offline inverse reinforcement learning problem. The method employs Adversarial Inverse Reinforcement Learning to train a discriminator that learns reward dynamics from healthy operational sequences, eliminating the need for manual reward engineering and fault labels; the discriminator&#x27;s output serves as an anomaly score. Experimental results on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY) show the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection by aligning RL&#x27;s sequential reasoning with the temporal structure of machinery data.</div>
<div class="mono" style="margin-top:8px">针对现有基于强化学习的机械故障检测方法未能充分利用其序列决策优势、常将问题简化为上下文赌博机的情况，本文的动机是将故障检测构建为离线逆强化学习问题。该方法采用对抗性逆强化学习，训练一个判别器从健康运行序列中学习奖励动态，从而避免手动设计奖励和故障标签，判别器的输出作为异常分数。在三个运行至失效的基准数据集（HUMS2023、IMS和XJTU-SY）上的实验结果表明，该模型能一致地为正常样本分配低异常分数、为故障样本分配高分数，通过将强化学习的序列推理与机械数据的时间结构对齐，实现了早期且鲁棒的故障检测。</div>
</details>
</div>
<div class="card">
<div class="title">UpSkill: Mutual Information Skill Learning for Structured Response Diversity in LLMs</div>
<div class="meta-line">Authors: Devan Shah, Owen Yang, Daniel Yang, Chongyi Zheng, Benjamin Eysenbach</div>
<div class="meta-line">First: 2026-02-25T15:34:14+00:00 · Latest: 2026-02-25T15:34:14+00:00</div>
<div class="meta-line">Comments: First two authors equal contribution. 29 pages total (11 pages main text), 10 figures, 10 tables. Project website: https://dshah.io/upskill/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22296v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22296v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dshah.io/upskill/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of large language models (LLMs) on mathematics and programming tasks, but standard approaches that optimize single-attempt accuracy can inadvertently suppress response diversity across repeated attempts, narrowing exploration and overlooking underrepresented strategies. We introduce UpSkill, a training time method that adapts Mutual Information Skill Learning (MISL) to LLMs for optimizing pass@k correctness. We propose a novel reward that we implement within Group Relative Policy Optimization (GRPO): a token-level mutual information (MI) reward that encourages trajectory specificity to z. Experiments on GSM8K with three open-weight models, Llama 3.1-8B, Qwen 2.5-7B, and R1-Distilled-Qwen2.5-Math-1.5B, show that UpSkill improves multi-attempt metrics on the stronger base models, yielding mean gains of ~3% in pass@k for both Qwen and Llama without degrading pass@1. Additionally, we find both empirical and theoretical evidence that improvements in pass@k are closely tied to the mutual information objective.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UpSkill：基于互信息技能学习的大语言模型结构化响应多样性方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）提升了大型语言模型（LLMs）在数学与编程任务上的推理能力，但优化单次尝试准确率的传统方法会抑制重复尝试中的响应多样性，从而限制探索空间并忽视非主流策略。本文提出训练阶段方法UpSkill，将互信息技能学习（MISL）适配至LLMs以优化pass@k正确率。我们在组相对策略优化（GRPO）框架中设计了一种新颖的奖励机制：通过词元级互信息（MI）奖励增强轨迹对特定隐变量z的专属性。基于GSM8K数据集对Llama 3.1-8B、Qwen 2.5-7B和R1-Distilled-Qwen2.5-Math-1.5B三个开源模型的实验表明，UpSkill在较强基座模型上提升了多尝试评估指标，使Qwen与Llama模型的pass@k平均提升约3%且不损害pass@1性能。实验与理论证据均表明pass@k的提升与互信息目标紧密相关。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of standard RLVR approaches in LLMs, which optimize single-attempt accuracy but inadvertently suppress response diversity across multiple attempts, thereby narrowing exploration and overlooking underrepresented strategies. The authors introduce UpSkill, a training-time method that adapts Mutual Information Skill Learning (MISL) to LLMs for optimizing pass@k correctness, proposing a novel token-level mutual information reward implemented within Group Relative Policy Optimization (GRPO) to encourage trajectory specificity. Experimental results on GSM8K with three open-weight models (Llama 3.1-8B, Qwen 2.5-7B, and R1-Distilled-Qwen2.5-Math-1.5B) show that UpSkill improves multi-attempt metrics on stronger base models, yielding mean gains of approximately 3% in pass@k for both Qwen and Llama without degrading pass@1, with both empirical and theoretical evidence linking these improvements to the mutual information objective.</div>
<div class="mono" style="margin-top:8px">本文针对标准RLVR方法在大语言模型中的局限性提出改进，该方法优化单次尝试准确率，但无意中抑制了多次尝试中的响应多样性，从而限制了探索并忽视了未被充分代表的策略。作者引入了UpSkill，这是一种训练时方法，将互信息技能学习（MISL）适配于大语言模型以优化pass@k正确率，提出了一种在组相对策略优化（GRPO）中实现的新型令牌级互信息奖励，以鼓励轨迹特异性。在GSM8K数据集上使用三个开源模型（Llama 3.1-8B、Qwen 2.5-7B和R1-Distilled-Qwen2.5-Math-1.5B）的实验结果表明，UpSkill在较强的基础模型上提升了多次尝试的指标，使Qwen和Llama的pass@k平均增益约3%，且不降低pass@1性能，同时提供了实证和理论证据表明这些改进与互信息目标紧密相关。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum feedback control with a transformer neural network architecture</div>
<div class="meta-line">Authors: Pranav Vaidhyanathan, Florian Marquardt, Mark T. Mitchison, Natalia Ares</div>
<div class="meta-line">Venue: Phys. Rev. Research 8, L012043, Published 24 February, 2026</div>
<div class="meta-line">First: 2024-11-28T16:42:30+00:00 · Latest: 2026-02-25T14:54:20+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.19253v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.19253v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Attention-based neural networks such as transformers have revolutionized various fields such as natural language processing, genomics, and vision. Here, we demonstrate the use of transformers for quantum feedback control through both a supervised and reinforcement learning approach. In particular, due to the transformer&#x27;s ability to capture long-range temporal correlations and training efficiency, we show that it can surpass some of the limitations of previous control approaches, e.g.~those based on recurrent neural networks trained using a similar approach or policy based reinforcement learning. We numerically show, for the example of state stabilization of a two-level system, that our bespoke transformer architecture can achieve near unit fidelity to a target state in a short time even in the presence of inefficient measurement and Hamiltonian perturbations that were not included in the training set as well as the control of non-Markovian systems. We also demonstrate that our transformer can perform energy minimization of non-integrable many-body quantum systems when trained for reinforcement learning tasks. Our approach can be used for quantum error correction, fast control of quantum states in the presence of colored noise, as well as real-time tuning, and characterization of quantum devices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Transformer神经网络架构的量子反馈控制</div>
<div class="mono" style="margin-top:8px">注意力机制神经网络（如Transformer）已在自然语言处理、基因组学和视觉等领域引发革命。本文通过监督学习和强化学习方法，展示了Transformer在量子反馈控制中的应用。得益于Transformer捕捉长程时间关联的能力及训练效率，我们证明其能超越以往控制方法的某些局限，例如基于类似方法训练的循环神经网络或基于策略的强化学习。以两能级系统状态稳定为例，数值模拟表明：即使面对训练集中未包含的低效测量、哈密顿量扰动以及非马尔可夫系统的控制，我们定制的Transformer架构仍能在短时间内实现接近单位保真度的目标态逼近。同时，我们还证明该Transformer在强化学习任务训练后，能对不可积多体量子系统进行能量最小化。该方法可应用于量子纠错、有色噪声下的量子态快速控制，以及量子设备的实时调谐与表征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the success of transformers in capturing long-range dependencies across various domains, this paper explores their application to quantum feedback control to overcome limitations of prior methods like recurrent neural networks. The method employs a transformer neural network architecture trained via both supervised and reinforcement learning approaches to design control policies. Key experimental results from numerical simulations demonstrate that the transformer achieves near-unit fidelity for stabilizing a two-level system&#x27;s state rapidly, even under inefficient measurements and untrained Hamiltonian perturbations, and effectively minimizes energy in non-integrable many-body quantum systems, showcasing robustness and generalization.</div>
<div class="mono" style="margin-top:8px">受Transformer在捕获长程依赖关系方面成功的启发，本文探索了其在量子反馈控制中的应用，以克服先前方法（如循环神经网络）的局限性。该方法采用基于Transformer的神经网络架构，通过监督学习和强化学习相结合的方式训练控制策略。数值模拟的关键实验结果表明，该Transformer能够快速实现两能级系统状态的稳定，保真度接近单位值，即使在低效测量和未训练的哈密顿扰动下也能保持鲁棒性，并能有效最小化不可积多体量子系统的能量，展现了良好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-02-25T13:52:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：面向科学复合图的视觉条件化面板检测与描述生成</div>
<div class="mono" style="margin-top:8px">科学复合图将多个带标签的面板整合为单一图像，但实际流程中的图注常缺失或仅提供图像级摘要，导致面板级理解困难。本文提出FigEx2，一种视觉条件化框架，可直接从复合图中定位面板并生成面板级描述。为缓解开放式描述中多样化表述的影响，我们引入噪声感知门控融合模块，自适应过滤词元级特征以稳定检测查询空间。此外，采用结合监督学习与强化学习的分阶段优化策略，利用基于CLIP的对齐奖励和基于BERTScore的语义奖励，确保严格的多模态一致性。为支持高质量监督，我们构建了面板级定位精炼基准BioSci-Fig-Cap，以及物理、化学跨学科测试集。实验表明，FigEx2在检测任务中达到0.726 mAP@0.5:0.95的优异性能，在METEOR和BERTScore指标上分别显著超越Qwen3-VL-8B模型0.51和0.24。值得注意的是，FigEx2在未经微调的情况下，对分布外科学领域展现出卓越的零样本迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of understanding scientific compound figures, where captions are often missing or only provide figure-level summaries, hindering panel-level analysis. The authors propose FigEx2, a visual-conditioned framework that localizes panels and generates panel-wise captions directly from the figure. To handle diverse phrasing in captioning, they introduce a noise-aware gated fusion module to filter token-level features and stabilize detection queries, and employ a staged optimization strategy combining supervised learning with reinforcement learning, using CLIP-based alignment and BERTScore rewards for multimodal consistency. They also curate the BioSci-Fig-Cap benchmark for panel-level grounding and cross-disciplinary test suites. Experiments show FigEx2 achieves 0.726 mAP@0.5:0.95 for detection and outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore, with notable zero-shot transferability to out-of-distribution scientific domains.</div>
<div class="mono" style="margin-top:8px">本文针对科学复合图中标题常缺失或仅提供图级摘要、阻碍面板级理解的问题，提出了FigEx2这一视觉条件框架，可直接从复合图中定位面板并生成面板级标题。为缓解开放式标题生成中多样化表述的影响，引入了噪声感知门控融合模块，自适应过滤词级特征以稳定检测查询空间，并采用结合监督学习和强化学习的分阶段优化策略，利用基于CLIP的对齐和基于BERTScore的语义奖励来确保严格的多模态一致性。同时，构建了用于面板级定位的BioSci-Fig-Cap基准以及跨学科测试集。实验结果表明，FigEx2在检测上达到0.726 mAP@0.5:0.95，在METEOR和BERTScore上分别显著优于Qwen3-VL-8B模型0.51和0.24，且对分布外科学领域展现出卓越的零样本迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">PepCompass: Navigating peptide embedding spaces using Riemannian Geometry</div>
<div class="meta-line">Authors: Marcin Możejko, Adam Bielecki, Jurand Prądzyński, Marcin Traskowski, Antoni Janowski, Hyun-Su Lee, Marcelo Der Torossian Torres, Michał Kmicikiewicz, Paulina Szymczak, Karol Jurasz, Michał Kucharczyk, Cesar de la Fuente-Nunez, Ewa Szczurek</div>
<div class="meta-line">First: 2025-10-02T13:07:37+00:00 · Latest: 2026-02-25T13:26:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01988v5">Abs</a> · <a href="https://arxiv.org/pdf/2510.01988v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Antimicrobial peptide discovery is challenged by the astronomical size of peptide space and the relative scarcity of active peptides. Generative models provide continuous latent &quot;maps&quot; of peptide space, but conventionally ignore decoder-induced geometry and rely on flat Euclidean metrics, rendering exploration and optimization distorted and inefficient. Prior manifold-based remedies assume fixed intrinsic dimensionality, which critically fails in practice for peptide data. Here, we introduce PepCompass, a geometry-aware framework for peptide exploration and optimization. At its core, we define a Union of $κ$-Stable Riemannian Manifolds $\mathbb{M}^κ$, a family of decoder-induced manifolds that captures local geometry while ensuring computational stability. We propose two local exploration methods: Second-Order Riemannian Brownian Efficient Sampling, which provides a convergent second-order approximation to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space, which reinterprets tangent directions as discrete amino-acid substitutions. Combining these yields Local Enumeration Bayesian Optimization (LE-BO), an efficient algorithm for local activity optimization. Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which interpolates between prototype embeddings along property-enriched geodesics, biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro validation confirms the effectiveness of PepCompass: PoGS yields four novel seeds, and subsequent optimization with LE-BO discovers 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains. These results demonstrate that geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PepCompass：利用黎曼几何导航肽嵌入空间</div>
<div class="mono" style="margin-top:8px">抗菌肽发现面临肽空间规模庞大与活性肽相对稀少的挑战。生成模型虽能提供肽空间的连续潜在“图谱”，但传统方法忽略解码器诱导的几何结构，依赖平坦欧氏度量，导致探索与优化过程扭曲且低效。现有基于流形的方法假设固有维度固定，这在肽数据实践中存在严重缺陷。本文提出PepCompass——一种几何感知的肽探索与优化框架。其核心是定义$κ$-稳定黎曼流形并集$\mathbb{M}^κ$，该解码器诱导流形族能捕捉局部几何特征并确保计算稳定性。我们提出两种局部探索方法：二阶黎曼布朗高效采样（提供黎曼布朗运动的收敛二阶近似）与切空间突变枚举（将切方向重新解释为离散氨基酸替换）。结合二者形成局部枚举贝叶斯优化算法，实现高效的局部活性优化。最后，我们提出势最小化测地线搜索，沿属性增强的测地线在原型嵌入间插值，使发现过程偏向具有优势活性的种子肽。体外验证证实了PepCompass的有效性：测地线搜索获得四种新型种子肽，后续通过局部枚举贝叶斯优化发现25种具有广谱活性（包括抗耐药菌株）的高活性肽。这些结果表明，几何感知探索为抗菌肽设计提供了强大的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of exploring the vast and sparse antimicrobial peptide space with conventional Euclidean-based generative models, which ignore decoder-induced geometry and assume fixed intrinsic dimensionality, this paper introduces PepCompass, a geometry-aware framework that models peptide embeddings as a Union of κ-Stable Riemannian Manifolds to capture local geometry. The method includes two local exploration techniques: Second-Order Riemannian Brownian Efficient Sampling for convergent approximations to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space to interpret tangent directions as amino-acid substitutions, combined into Local Enumeration Bayesian Optimization (LE-BO) for efficient local activity optimization, alongside Potential-minimizing Geodesic Search (PoGS) for interpolating along property-enriched geodesics. Experimental in-vitro validation shows that PoGS generated four novel seed peptides, and subsequent LE-BO optimization discovered 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains, demonstrating the effectiveness of geometry-informed exploration for antimicrobial peptide design.</div>
<div class="mono" style="margin-top:8px">针对传统基于欧几里得空间的生成模型在探索广阔而稀疏的抗菌肽空间时忽略解码器诱导的几何结构并假设固定内在维度的挑战，本文提出了PepCompass，一种几何感知框架，将肽嵌入建模为κ-稳定黎曼流形的并集以捕捉局部几何。方法包括两种局部探索技术：用于黎曼布朗运动收敛近似的二阶黎曼布朗高效采样，以及将切空间方向解释为氨基酸替换的切空间突变枚举，两者结合形成局部枚举贝叶斯优化（LE-BO）以实现高效的局部活性优化，同时引入势最小化测地线搜索（PoGS）以沿属性富集的测地线进行插值。实验体外验证表明，PoGS生成了四种新型种子肽，随后的LE-BO优化发现了25种具有广谱活性的高活性肽，包括对抗耐药菌株，证明了几何感知探索在抗菌肽设计中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Distill and Align Decomposition for Enhanced Claim Verification</div>
<div class="meta-line">Authors: Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Fernando Acero, Arturo Oncevay, Charese H. Smiley, Xiaomo Liu, Manuela Veloso</div>
<div class="meta-line">First: 2026-02-25T12:32:04+00:00 · Latest: 2026-02-25T12:32:04+00:00</div>
<div class="meta-line">Comments: EACL Findings 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21857v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21857v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>蒸馏与对齐分解增强声明验证</div>
<div class="mono" style="margin-top:8px">复杂声明验证需将句子分解为可验证的子声明，但现有方法难以协调分解质量与验证性能。我们提出一种强化学习方法，利用组相对策略优化联合优化分解质量与验证器对齐。该方法整合了：（i）结构化序列推理；（ii）基于教师蒸馏范例的监督微调；（iii）平衡格式合规性、验证器对齐与分解质量的多目标奖励机制。在六种评估场景中，我们训练的80亿参数分解器将下游验证性能提升至宏观F1值71.75%，优于基于提示的方法（+1.99/+6.24）和现有强化学习方法（+5.84）。人工评估证实了生成子声明的高质量。该框架通过联合优化验证准确性与分解质量，使小型语言模型能够实现最先进的声明验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of aligning decomposition quality with verification performance in complex claim verification, this paper proposes a reinforcement learning approach using Group Relative Policy Optimization (GRPO) to jointly optimize decomposition and verifier alignment. The method integrates structured sequential reasoning, supervised fine-tuning on teacher-distilled exemplars, and a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Experimental results across six evaluation settings show that the trained 8B decomposer achieves a macro-F1 of 71.75% for downstream verification, outperforming prompt-based approaches by up to 6.24 percentage points and existing RL methods by 5.84 percentage points, with human evaluation confirming high-quality subclaim generation.</div>
<div class="mono" style="margin-top:8px">针对复杂声明验证中分解质量与验证性能难以对齐的问题，本文提出了一种使用群体相对策略优化（GRPO）的强化学习方法，以联合优化分解过程和验证器对齐。该方法整合了结构化顺序推理、基于教师蒸馏示例的监督微调，以及平衡格式合规性、验证器对齐和分解质量的多目标奖励机制。在六种评估设置下的实验结果表明，训练后的80亿参数分解器在下游验证中实现了71.75%的宏观F1分数，优于基于提示的方法（最高提升6.24个百分点）和现有强化学习方法（提升5.84个百分点），人工评估也证实了生成的子声明具有高质量。</div>
</details>
</div>
<div class="card">
<div class="title">Object-Centric World Models from Few-Shot Annotations for Sample-Efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Weipu Zhang, Adam Jelley, Trevor McInroe, Amos Storkey, Gang Wang</div>
<div class="meta-line">First: 2025-01-27T19:07:06+00:00 · Latest: 2026-02-25T12:27:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.16443v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.16443v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While deep reinforcement learning (RL) from pixels has achieved remarkable success, its sample inefficiency remains a critical limitation for real-world applications. Model-based RL (MBRL) addresses this by learning a world model to generate simulated experience, but standard approaches that rely on pixel-level reconstruction losses often fail to capture small, task-critical objects in complex, dynamic scenes. We posit that an object-centric (OC) representation can direct model capacity toward semantically meaningful entities, improving dynamics prediction and sample efficiency. In this work, we introduce OC-STORM, an object-centric MBRL framework that enhances a learned world model with object representations extracted by a pretrained segmentation network. By conditioning on a minimal number of annotated frames, OC-STORM learns to track decision-relevant object dynamics and inter-object interactions without extensive labeling or access to privileged information. Empirical results demonstrate that OC-STORM significantly outperforms the STORM baseline on the Atari 100k benchmark and achieves state-of-the-art sample efficiency on challenging boss fights in the visually complex game Hollow Knight. Our findings underscore the potential of integrating OC priors into MBRL for complex visual domains. Project page: https://oc-storm.weipuzhang.com</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于少量标注的对象中心世界模型实现样本高效强化学习</div>
<div class="mono" style="margin-top:8px">尽管基于像素的深度强化学习已取得显著成就，但其样本效率低下的问题仍是实际应用的关键瓶颈。基于模型的强化学习方法通过构建世界模型生成模拟经验来应对这一挑战，但依赖像素级重建损失的标准方法往往难以捕捉复杂动态场景中任务关键的小型对象。我们认为对象中心表征能将模型能力聚焦于语义实体，从而提升动态预测与样本效率。本研究提出OC-STORM——一种对象中心的基于模型强化学习框架，通过预训练分割网络提取的对象表征增强学习得到的世界模型。仅需基于少量标注帧进行条件训练，OC-STORM即可学习追踪决策相关的对象动态与对象间交互，无需大量标注或特权信息。实验结果表明，OC-STORM在Atari 100k基准测试中显著超越STORM基线，并在视觉复杂游戏《空洞骑士》的挑战性首领战中达到最先进的样本效率。我们的发现证实了将对象中心先验整合至基于模型强化学习在复杂视觉领域的潜力。项目页面：https://oc-storm.weipuzhang.com</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sample inefficiency of pixel-based deep reinforcement learning and the limitations of standard model-based RL in capturing task-critical objects, this paper introduces OC-STORM, an object-centric model-based RL framework. The method enhances a learned world model by incorporating object representations from a pretrained segmentation network, conditioned on a few annotated frames to track object dynamics and interactions without extensive labeling. Experimental results show that OC-STORM outperforms the STORM baseline on the Atari 100k benchmark and achieves state-of-the-art sample efficiency on challenging boss fights in the visually complex game Hollow Knight, highlighting the benefits of object-centric priors for sample-efficient RL in complex visual domains.</div>
<div class="mono" style="margin-top:8px">针对基于像素的深度强化学习样本效率低、以及标准基于模型的强化学习在捕捉任务关键物体方面的不足，本文提出了OC-STORM，一种以物体为中心的基于模型的强化学习框架。该方法通过整合预训练分割网络提取的物体表征来增强学习的世界模型，仅需少量标注帧进行条件化，即可追踪物体动态和交互，无需大量标注或特权信息。实验结果表明，OC-STORM在Atari 100k基准测试中显著优于STORM基线，并在视觉复杂的游戏《空洞骑士》的挑战性Boss战中实现了最先进的样本效率，凸显了物体中心先验在复杂视觉领域中提升样本效率的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning</div>
<div class="meta-line">Authors: Ruichen Xu, Ying-Jun Angela Zhang, Jianwei Huang</div>
<div class="meta-line">First: 2026-02-25T12:22:48+00:00 · Latest: 2026-02-25T12:22:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21844v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21844v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially private federated learning faces a fundamental tension: privacy protection mechanisms that safeguard client data simultaneously create quantifiable privacy costs that discourage participation, undermining the collaborative training process. Existing incentive mechanisms rely on unbiased client selection, forcing servers to compensate even the most privacy-sensitive clients (&quot;privacy stragglers&quot;), leading to systemic inefficiency and suboptimal resource allocation. We introduce JSAM (Joint client Selection and privacy compensAtion Mechanism), a Bayesian-optimal framework that simultaneously optimizes client selection probabilities and privacy compensation to maximize training effectiveness under budget constraints. Our approach transforms a complex 2N-dimensional optimization problem into an efficient three-dimensional formulation through novel theoretical characterization of optimal selection strategies. We prove that servers should preferentially select privacy-tolerant clients while excluding high-sensitivity participants, and uncover the counter-intuitive insight that clients with minimal privacy sensitivity may incur the highest cumulative costs due to frequent participation. Extensive evaluations on MNIST and CIFAR-10 demonstrate that JSAM achieves up to 15% improvement in test accuracy compared to existing unbiased selection mechanisms while maintaining cost efficiency across varying data heterogeneity levels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JSAM：差分隐私联邦学习中的抗隐私滞后联合客户端选择与激励机制设计</div>
<div class="mono" style="margin-top:8px">差分隐私联邦学习面临一个根本性矛盾：保护客户端数据的隐私机制会同时产生可量化的隐私成本，抑制参与意愿，从而削弱协作训练过程。现有激励机制依赖无偏客户端选择，迫使服务器即使对最敏感的“隐私滞后”客户端也进行补偿，导致系统性低效和次优资源配置。我们提出JSAM（联合客户端选择与隐私补偿机制），这是一个贝叶斯最优框架，可在预算约束下同步优化客户端选择概率与隐私补偿，以最大化训练效果。通过创新性地理论刻画最优选择策略，我们将复杂的2N维优化问题转化为高效的三维形式。我们证明服务器应优先选择隐私容忍度高的客户端，同时排除高敏感度参与者，并揭示了一个反直觉的发现：隐私敏感度最低的客户端可能因频繁参与而产生最高的累计成本。在MNIST和CIFAR-10数据集上的大量实验表明，相较于现有无偏选择机制，JSAM在保持不同数据异质性水平下成本效率的同时，测试准确率最高提升15%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency in differentially private federated learning where existing incentive mechanisms, by requiring unbiased client selection, force servers to compensate privacy-sensitive &#x27;stragglers&#x27;, leading to suboptimal resource use. The authors propose JSAM, a Bayesian-optimal framework that jointly optimizes client selection probabilities and privacy compensation to maximize training effectiveness under a budget, transforming a complex high-dimensional problem into an efficient three-dimensional formulation through novel theoretical analysis. Experimental results on MNIST and CIFAR-10 show JSAM achieves up to 15% higher test accuracy than unbiased selection methods while maintaining cost efficiency across different levels of data heterogeneity.</div>
<div class="mono" style="margin-top:8px">本文针对差分隐私联邦学习中的效率问题提出解决方案，现有激励机制因要求无偏的客户端选择，迫使服务器补偿高隐私敏感的&#x27;掉队者&#x27;，导致资源分配欠佳。作者提出了JSAM这一贝叶斯最优框架，在预算约束下联合优化客户端选择概率和隐私补偿以最大化训练效果，并通过新颖的理论分析将复杂的高维优化问题转化为高效的三维形式。在MNIST和CIFAR-10数据集上的实验表明，JSAM相比无偏选择机制实现了高达15%的测试精度提升，同时在不同数据异构水平下保持了成本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Resisting Contextual Interference in RAG via Parametric-Knowledge Reinforcement</div>
<div class="meta-line">Authors: Chenyu Lin, Yilin Wen, Du Su, Hexiang Tan, Fei Sun, Muhan Chen, Chenfu Bao, Zhonghou Lyu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-05T15:34:15+00:00 · Latest: 2026-02-25T12:22:09+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05154v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.05154v4">PDF</a> · <a href="https://github.com/lcy80366872/knowledgeable-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) improves performance on knowledge-intensive tasks but can be derailed by wrong, irrelevant, or conflicting retrieved text, causing models to rely on inaccurate evidence and cascade errors. We propose Knowledgeable-R1, a reinforcement-learning framework that explicitly trains large language models to use parametric knowledge (PK) to resist contextual interference while still exploiting external context when it is reliably helpful. Knowledgeable-R1 introduces a joint sampling scheme that generates paired responses with and without retrieval, and learns both local advantages (within each decoding regime) and global advantages under the same input to quantify when to ignore misleading context versus adopt it. We employ an asymmetric advantage transformation that amplifies exploratory behaviors toward parametric knowledge. Experiments show that Knowledgeable-R1 significantly improves robustness and reasoning accuracy in knowledge conflict scenarios and general RAG scenarios, outperforming SOTA baselines by +22.89% in counterfactual scenarios, and without degradation when the retrieved context is fully accurate.Our code are available at https://github.com/lcy80366872/knowledgeable-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过参数化知识强化抵御RAG中的上下文干扰</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）提升了知识密集型任务的性能，但可能因错误、无关或冲突的检索文本而偏离正轨，导致模型依赖不准确证据并引发级联错误。我们提出Knowledgeable-R1强化学习框架，通过显式训练大语言模型利用参数化知识（PK）抵御上下文干扰，同时在外部上下文可靠有效时仍加以利用。该框架采用联合采样方案生成带检索与不带检索的配对响应，并学习相同输入下局部优势（各解码机制内）与全局优势，以量化何时忽略误导性上下文或采纳有效信息。通过非对称优势变换强化对参数化知识的探索行为。实验表明，Knowledgeable-R1在知识冲突场景和通用RAG场景中显著提升鲁棒性与推理准确率，在反事实场景中超越SOTA基线+22.89%，且在检索上下文完全准确时性能无衰减。代码发布于https://github.com/lcy80366872/knowledgeable-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of contextual interference in retrieval-augmented generation (RAG), where incorrect or irrelevant retrieved text can mislead models, causing error cascades. To mitigate this, the authors propose Knowledgeable-R1, a reinforcement learning framework that trains large language models to leverage their internal parametric knowledge to resist misleading external context while still utilizing helpful retrieved information. The method employs a joint sampling scheme to generate paired responses with and without retrieval, learning both local and global advantages to decide when to ignore or adopt external context, enhanced by an asymmetric advantage transformation to encourage reliance on parametric knowledge. Experimental results demonstrate that Knowledgeable-R1 significantly enhances robustness and reasoning accuracy in knowledge conflict and general RAG scenarios, outperforming state-of-the-art baselines by +22.89% in counterfactual settings without degrading performance when retrieved context is accurate.</div>
<div class="mono" style="margin-top:8px">该论文针对检索增强生成（RAG）中的上下文干扰问题，即错误或不相关的检索文本会误导模型并导致错误传播。为解决此问题，作者提出了Knowledgeable-R1，一个强化学习框架，旨在训练大语言模型利用其内部参数化知识来抵抗误导性外部上下文，同时仍能有效利用有益的检索信息。该方法采用联合采样方案，生成带检索和不带检索的配对响应，通过学习局部和全局优势来决定何时忽略或采纳外部上下文，并通过非对称优势变换强化对参数化知识的依赖。实验结果表明，Knowledgeable-R1在知识冲突和一般RAG场景中显著提升了鲁棒性和推理准确性，在反事实场景下优于现有最佳基线+22.89%，且在检索上下文完全准确时性能无下降。</div>
</details>
</div>
<div class="card">
<div class="title">Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration</div>
<div class="meta-line">Authors: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Hanhui Li, Yiwei Wang, Xiaodan Liang, Jing Tang</div>
<div class="meta-line">First: 2025-08-19T11:51:40+00:00 · Latest: 2026-02-25T12:13:07+00:00</div>
<div class="meta-line">Comments: 20 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13755v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.13755v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO&#x27;s mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLVR中的深度-广度协同：通过自适应探索释放大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为释放大语言模型推理能力的强大范式，但其全部潜力受限于两个未充分探索的维度：深度（模型能采样的最难问题）与广度（单次迭代消耗的实例数量）。我们剖析了主流GRPO算法，揭示其存在系统性偏差：累积优势值会过度加权中等准确率的样本，同时低估对突破推理边界至关重要的低准确率样本。为纠正深度忽视问题，我们提出难度自适应轨迹采样（DARS），通过定向多阶段轨迹对难题进行重加权，从而增加难题的正向轨迹数量。实证表明，单纯扩大轨迹规模仅加速收敛，甚至损害Pass@K指标。相比之下，我们的DARS方法能在不增加收敛时推理成本的前提下，持续提升Pass@K。在实现深度探索自适应扩展后，我们进一步探究：激进扩展训练数据广度能否放大推理收益？为此，我们大幅扩展批次规模，将PPO的小批次迭代替换为多轮次的全批次更新。广度扩展显著提升了Pass@1性能。大广度训练维持了较高的词元级熵值，表明探索持续进行且梯度噪声降低。我们进一步提出DARS-B方法，将DARS与大广度训练结合，实现了Pass@K与Pass@1指标的同步提升。结果证实：广度与跨深度的自适应探索在RLVR中构成正交维度，是释放RLVR推理潜能的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses limitations in Reinforcement Learning with Verifiable Reward (RLVR) for enhancing large language model reasoning, identifying a bias in algorithms like GRPO that neglect difficult problems crucial for depth. To rectify this, the authors propose Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems via targeted multi-stage rollouts to increase positive samples for them, leading to consistent Pass@K gains without extra inference cost. Additionally, they explore scaling training data breadth by using large batch sizes and full-batch updates, which improves Pass@1 performance and maintains exploration, and combining both approaches in DARS-B yields simultaneous gains in Pass@K and Pass@1, demonstrating that adaptive depth exploration and breadth scaling are orthogonal and key to unlocking RLVR&#x27;s reasoning potential.</div>
<div class="mono" style="margin-top:8px">本文针对用于增强大语言模型推理能力的可验证奖励强化学习（RLVR）中的局限性，指出如GRPO等算法存在忽视关键难题的偏差，阻碍了推理深度。为纠正此问题，作者提出了难度自适应滚动采样（DARS），通过定向多阶段滚动对难题进行重加权，以增加其正样本，从而在不增加推理成本的情况下实现持续的Pass@K提升。同时，他们通过使用大批量和全批量更新来扩展训练数据广度，这提高了Pass@1性能并保持了探索性；将这两种方法结合在DARS-B中，可在Pass@K和Pass@1上同时获得增益，表明自适应深度探索和广度扩展是正交的，是释放RLVR推理能力的关键。</div>
</details>
</div>
<div class="card">
<div class="title">Generalisation of RLHF under Reward Shift and Clipped KL Regularisation</div>
<div class="meta-line">Authors: Kenton Tang, Yuzhu Chen, Fengxiang He</div>
<div class="meta-line">First: 2026-02-25T10:36:17+00:00 · Latest: 2026-02-25T10:36:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21765v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21765v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this issue, we develop generalisation theory for RLHF that explicitly accounts for (1) \emph{reward shift}: reward models are trained on preference data from earlier or mixed behaviour policies while RLHF optimises the current policy on its own rollouts; and (2) \emph{clipped KL regularisation}: the KL regulariser is estimated from sampled log-probability ratios and then clipped for stabilisation, resulting in an error to RLHF. We present generalisation bounds for RLHF, suggesting that the generalisation error stems from a sampling error from prompts and rollouts, a reward shift error, and a KL clipping error. We also discuss special cases of (1) initialising RLHF parameters with a uniform prior over a finite space, and (2) training RLHF by stochastic gradient descent, as an Ornstein-Uhlenbeck process. The theory yields practical implications in (1) optimal KL clipping threshold, and (2) budget allocation in prompts, rollouts, and preference data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励偏移与截断KL正则化下RLHF的泛化理论</div>
<div class="mono" style="margin-top:8px">大语言模型的校准与适配高度依赖基于人类反馈的强化学习（RLHF），但其泛化性的理论理解仍不成熟，尤其在所学奖励可能偏移、KL控制需经估计与截断时。为此，我们构建了明确考虑以下因素的RLHF泛化理论：（1）奖励偏移：奖励模型基于早期或混合行为策略的偏好数据训练，而RLHF在当前策略自身生成数据上优化；（2）截断KL正则化：KL正则项通过采样对数概率比估计并截断以稳定训练，从而引入误差。我们提出了RLHF的泛化界，表明泛化误差源于提示与生成数据的采样误差、奖励偏移误差及KL截断误差。同时探讨了两种特例：（1）在有限空间上以均匀先验初始化RLHF参数；（2）通过随机梯度下降（视为奥恩斯坦-乌伦贝克过程）训练RLHF。该理论对（1）最优KL截断阈值与（2）提示数据、生成数据及偏好数据的预算分配具有实践指导意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limited theoretical understanding of reinforcement learning from human feedback (RLHF) by developing a generalization theory that explicitly accounts for two practical challenges: reward shift, where reward models are trained on data from different behavior policies than the one being optimized, and clipped KL regularization, where estimated KL penalties are clipped for stability, introducing error. The method involves deriving generalization bounds for RLHF, which decompose the error into components from sampling prompts and rollouts, reward shift, and KL clipping. The main experimental results, derived theoretically, show that these bounds lead to practical implications for determining an optimal KL clipping threshold and allocating budgets effectively across prompts, rollouts, and preference data, with special cases discussed for uniform prior initialization and training via stochastic gradient descent as an Ornstein-Uhlenbeck process.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习从人类反馈（RLHF）理论理解不足的问题，提出了一个泛化理论，明确考虑了两个实际挑战：奖励偏移（即奖励模型基于与当前优化策略不同的行为策略数据进行训练）和裁剪KL正则化（即估计的KL惩罚被裁剪以稳定训练，从而引入误差）。方法涉及推导RLHF的泛化边界，将误差分解为来自提示和滚轮采样的误差、奖励偏移误差以及KL裁剪误差。主要实验结果从理论上表明，这些边界具有实际意义，可用于确定最优KL裁剪阈值，并在提示、滚轮和偏好数据之间有效分配预算，同时讨论了均匀先验初始化和通过随机梯度下降（作为奥恩斯坦-乌伦贝克过程）进行训练的特殊情况。</div>
</details>
</div>
<div class="card">
<div class="title">RABot: Reinforcement-Guided Graph Augmentation for Imbalanced and Noisy Social Bot Detection</div>
<div class="meta-line">Authors: Longlong Zhang, Xi Wang, Haotong Du, Yangyi Xu, Zhuo Liu, Yang Liu</div>
<div class="meta-line">First: 2026-02-25T10:02:57+00:00 · Latest: 2026-02-25T10:02:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21749v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21749v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Social bot detection is pivotal for safeguarding the integrity of online information ecosystems. Although recent graph neural network (GNN) solutions achieve strong results, they remain hindered by two practical challenges: (i) severe class imbalance arising from the high cost of generating bots, and (ii) topological noise introduced by bots that skillfully mimic human behavior and forge deceptive links. We propose the Reinforcement-guided graph Augmentation social Bot detector (RABot), a multi-granularity graph-augmentation framework that addresses both issues in a unified manner. RABot employs a neighborhood-aware oversampling strategy that linearly interpolates minority-class embeddings within local subgraphs, thereby stabilizing the decision boundary under low-resource regimes. Concurrently, a reinforcement-learning-driven edge-filtering module combines similarity-based edge features with adaptive threshold optimization to excise spurious interactions during message passing, yielding a cleaner topology. Extensive experiments on three real-world benchmarks and four GNN backbones demonstrate that RABot consistently surpasses state-of-the-art baselines. In addition, since its augmentation and filtering modules are orthogonal to the underlying architecture, RABot can be seamlessly integrated into existing GNN pipelines to boost performance with minimal overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RABot：面向不平衡与噪声社交机器人检测的强化学习引导图增强方法</div>
<div class="mono" style="margin-top:8px">社交机器人检测对维护在线信息生态系统的完整性至关重要。尽管当前基于图神经网络（GNN）的解决方案取得了显著效果，但仍受限于两大实际挑战：（一）生成机器人的高成本导致的严重类别不平衡；（二）机器人通过模仿人类行为与伪造欺骗性链接引入的拓扑噪声。本文提出强化学习引导的图增强社交机器人检测器（RABot），这是一个多粒度图增强框架，可统一处理上述问题。RABot采用邻域感知的过采样策略，通过在局部子图中对少数类嵌入进行线性插值，从而在低资源场景下稳定决策边界。同时，基于强化学习的边过滤模块将基于相似性的边特征与自适应阈值优化相结合，在消息传递过程中剔除虚假交互，获得更纯净的拓扑结构。在三个真实场景基准数据集和四种GNN骨干网络上的大量实验表明，RABot持续超越现有最优基线方法。此外，由于该框架的增强与过滤模块与底层架构正交，RABot可无缝集成至现有GNN流程中，以极低开销显著提升检测性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses two key challenges in social bot detection using graph neural networks: severe class imbalance due to the high cost of generating bots, and topological noise from bots that mimic human behavior and create deceptive links. To tackle these issues, the authors propose RABot, a multi-granularity graph-augmentation framework that combines a neighborhood-aware oversampling strategy for interpolating minority-class embeddings in local subgraphs with a reinforcement-learning-driven edge-filtering module to remove spurious interactions. Experimental results on three real-world benchmarks and four GNN backbones show that RABot consistently outperforms state-of-the-art baselines, and its modular design allows seamless integration into existing GNN pipelines with minimal overhead.</div>
<div class="mono" style="margin-top:8px">该论文针对图神经网络在社交机器人检测中面临的两个关键挑战：由于生成机器人成本高导致的严重类别不平衡，以及机器人模仿人类行为并创建欺骗性链接引入的拓扑噪声。为解决这些问题，作者提出了RABot，一个多粒度图增强框架，结合了在局部子图中插值少数类嵌入的邻域感知过采样策略，以及基于强化学习的边过滤模块以消除虚假交互。在三个真实世界基准和四个GNN骨干上的实验结果表明，RABot持续优于最先进的基线方法，其模块化设计可无缝集成到现有GNN流程中，且开销极小。</div>
</details>
</div>
<div class="card">
<div class="title">The Art of Efficient Reasoning: Data, Reward, and Optimization</div>
<div class="meta-line">Authors: Taiqiang Wu, Zenan Xu, Bo Zhou, Ngai Wong</div>
<div class="meta-line">First: 2026-02-24T14:28:16+00:00 · Latest: 2026-02-25T09:40:11+00:00</div>
<div class="meta-line">Comments: Tech Report, Insights on Efficient Reasoning via Reward Shaping</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20945v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20945v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效推理的艺术：数据、奖励与优化</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）持续受益于规模化思维链（CoT）推理，但也面临沉重的计算开销。为解决此问题，高效推理旨在激励简短而准确的思维轨迹，通常通过强化学习（RL）的奖励塑形实现。本文系统研究了LLMs高效推理的机制。为全面评估，我们倡导采用更细粒度的指标，包括基于正确性的长度分布以及在2k至32k广泛令牌预算范围内的性能表现。首先，我们揭示训练过程遵循两阶段范式：长度适应与推理精炼。随后，我们在统一协议下开展大量实验（约20万GPU小时），解构训练提示与推演、奖励塑形及优化策略。关键发现之一是：在相对简单的提示上训练，可确保正向奖励信号的密度，从而避免长度塌缩。同时，习得的长度偏差具备跨领域泛化能力。我们将所有发现提炼为有价值的洞见与实践指南，并在Qwen3系列（0.6B至30B）中进一步验证，证明了其鲁棒性与泛化性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how to make Large Language Models (LLMs) reason more efficiently by producing shorter yet accurate reasoning chains, motivated by the computational overhead of standard scaled Chain-of-Thought methods. The method systematically studies efficient reasoning mechanics through extensive experiments involving reward shaping with Reinforcement Learning (RL), deconstructing training prompts, rollouts, and optimization strategies. Key experimental results reveal a two-stage training paradigm of length adaptation followed by reasoning refinement, and a crucial finding that training on relatively easier prompts prevents length collapse by ensuring positive reward density; the learned length bias also generalizes across domains, as validated across model sizes from 0.6B to 30B parameters.</div>
<div class="mono" style="margin-top:8px">本文研究如何使大语言模型（LLM）进行更高效的推理，即生成更短但准确的思维链，其动机在于标准规模化思维链方法带来的沉重计算开销。该方法通过强化学习进行奖励塑造，系统性地研究了高效推理的机制，并解构了训练提示、过程、奖励设计和优化策略。主要实验结果表明，训练遵循长度适应和推理精炼的两阶段范式；关键发现是，在相对简单的提示上训练可确保正向奖励信号的密度，从而避免长度崩溃，同时习得的长度偏好能跨领域泛化，这在0.6B至30B参数规模的模型上得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Cross-Domain Offline Reinforcement Learning with Dynamics- and Value-Aligned Data Filtering</div>
<div class="meta-line">Authors: Zhongjian Qiao, Rui Yang, Jiafei Lyu, Chenjia Bai, Xiu Li, Siyang Gao, Shuang Qiu</div>
<div class="meta-line">First: 2025-12-02T05:45:40+00:00 · Latest: 2026-02-25T09:39:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.02435v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.02435v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-domain offline reinforcement learning (RL) aims to train a well-performing agent in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between source and target domains, naively merging the two datasets may incur inferior performance. Recent advances address this issue by selectively leveraging source domain samples whose dynamics align well with the target domain. However, our work demonstrates that dynamics alignment alone is insufficient, by examining the limitations of prior frameworks and deriving a new target domain sub-optimality bound for the policy learned on the source domain. More importantly, our theory underscores an additional need for \textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain, a critical dimension overlooked by existing works. Motivated by such theoretical insight, we propose \textbf{\underline{D}}ynamics- and \textbf{\underline{V}}alue-aligned \textbf{\underline{D}}ata \textbf{\underline{F}}iltering (DVDF) method, a novel unified cross-domain RL framework that selectively incorporates source domain samples exhibiting strong alignment in \textit{both dynamics and values}. We empirically study a range of dynamics shift scenarios, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, even in the challenging setting where the target domain dataset contains an extremely limited amount of data. Extensive experiments demonstrate that DVDF consistently outperforms strong baselines with significant improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于动态与价值对齐数据筛选的高效跨域离线强化学习</div>
<div class="mono" style="margin-top:8px">跨域离线强化学习旨在利用有限的目标域数据集和（可能）数据覆盖充足的源域数据集，在目标环境中训练出性能优异的智能体。由于源域与目标域之间存在潜在动态特性不匹配，简单合并两个数据集可能导致性能下降。近期研究通过选择性利用动态特性与目标域对齐的源域样本来解决此问题。然而，本文通过剖析现有框架的局限性，推导出源域策略在目标域的新次优性边界，证明仅依赖动态对齐并不充分。更重要的是，理论分析揭示了对\textit{价值对齐}的额外需求——即从源域筛选高质量、高价值样本，这是现有研究忽视的关键维度。基于此理论洞见，我们提出\textbf{\underline{D}}ynamics- and \textbf{\underline{V}}alue-aligned \textbf{\underline{D}}ata \textbf{\underline{F}}iltering（DVDF）方法，该创新性统一跨域强化学习框架能选择性整合\textit{同时满足动态与价值强对齐}的源域样本。我们在包括运动学与形态学偏移在内的多种动态偏移场景中开展实证研究，并在不同任务与数据集上评估DVDF，即使目标域数据集极度有限。大量实验表明，DVDF始终以显著优势超越现有基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of cross-domain offline reinforcement learning, where leveraging source domain data can be hindered by dynamics misalignment with the target domain. The authors first theoretically demonstrate that prior methods focusing solely on dynamics alignment are insufficient, identifying a critical need for also selecting high-value source samples. Motivated by this, they propose the Dynamics- and Value-aligned Data Filtering (DVDF) method, a unified framework that filters source data based on both dynamics and value alignment. Experimental results across various tasks with dynamics shifts, including kinematic and morphology changes, show that DVDF consistently and significantly outperforms strong baselines, even with extremely limited target domain data.</div>
<div class="mono" style="margin-top:8px">本文针对跨领域离线强化学习中的挑战展开研究，即直接利用源领域数据会因与目标领域的动态不匹配而导致性能下降。作者首先从理论上证明，先前仅关注动态对齐的方法是不充分的，并指出同时筛选高价值的源样本也至关重要。基于这一理论洞察，他们提出了动态与价值对齐数据过滤（DVDF）方法，这是一个通过动态和价值双重对齐来筛选源数据的统一框架。在包括运动学和形态变化在内的多种动态偏移任务上的实验结果表明，DVDF即使在目标领域数据量极少的情况下，也始终显著优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating the relationship between regularity and learnability in recursive numeral systems using Reinforcement Learning</div>
<div class="meta-line">Authors: Andrea Silvi, Ponrawee Prasertsom, Jennifer Culbertson, Devdatt Dubhashi, Moa Johansson, Kenny Smith</div>
<div class="meta-line">First: 2026-02-25T09:27:02+00:00 · Latest: 2026-02-25T09:27:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21720v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21720v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human recursive numeral systems (i.e., counting systems such as English base-10 numerals), like many other grammatical systems, are highly regular. Following prior work that relates cross-linguistic tendencies to biases in learning, we ask whether regular systems are common because regularity facilitates learning. Adopting methods from the Reinforcement Learning literature, we confirm that highly regular human(-like) systems are easier to learn than unattested but possible irregular systems. This asymmetry emerges under the natural assumption that recursive numeral systems are designed for generalisation from limited data to represent all integers exactly. We also find that the influence of regularity on learnability is absent for unnatural, highly irregular systems, whose learnability is influenced instead by signal length, suggesting that different pressures may influence learnability differently in different parts of the space of possible numeral systems. Our results contribute to the body of work linking learnability to cross-linguistic prevalence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>运用强化学习评估递归数字系统中规律性与可学习性的关系</div>
<div class="mono" style="margin-top:8px">人类递归数字系统（如英语十进制计数）与许多语法系统一样具有高度规律性。基于先前将跨语言趋势与学习偏倚相关联的研究，我们探讨规律性系统是否因其促进学习而普遍存在。通过采用强化学习方法，我们证实高度规律的人类（类人）系统比未经验证但可能的不规则系统更易学习。这种不对称性源于一个自然假设：递归数字系统旨在通过有限数据泛化以精确表示所有整数。我们还发现，对于非自然的高度不规则系统，规律性对可学习性的影响消失，其可学习性转而受信号长度影响，这表明在可能数字系统的不同区域，不同压力可能对可学习性产生差异化影响。本研究为连接可学习性与跨语言普遍性的研究体系提供了新证据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the cross-linguistic prevalence of regular recursive numeral systems, this study investigates whether such regularity enhances learnability. The method employs Reinforcement Learning to simulate learning under a natural assumption that numeral systems must generalize from limited data to represent all integers exactly. Experimental results confirm that highly regular, human-like systems are indeed easier to learn than unattested irregular systems, but this effect disappears for highly unnatural irregular systems, where learnability is instead influenced by signal length, indicating that different pressures operate in different parts of the system space.</div>
<div class="mono" style="margin-top:8px">本研究受递归数字系统在跨语言中普遍呈现规律性的现象驱动，旨在探究这种规律性是否促进了系统的可学习性。方法上采用强化学习来模拟学习过程，并基于数字系统需从有限数据泛化以精确表示所有整数的自然假设。实验结果表明，高度规律、类人的系统确实比未出现的不规则系统更易学习，但对于高度不自然的不规则系统，这种规律性的影响消失，其可学习性转而受信号长度影响，这表明在可能的数字系统空间的不同部分，影响可学习性的压力因素有所不同。</div>
</details>
</div>
<div class="card">
<div class="title">Two-Stage Active Distribution Network Voltage Control via LLM-RL Collaboration: A Hybrid Knowledge-Data-Driven Approach</div>
<div class="meta-line">Authors: Xu Yang, Chenhui Lin, Xiang Ma, Dong Liu, Ran Zheng, Haotian Liu, Wenchuan Wu</div>
<div class="meta-line">First: 2026-02-25T09:22:27+00:00 · Latest: 2026-02-25T09:22:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21715v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21715v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing integration of distributed photovoltaics (PVs) into active distribution networks (ADNs) has exacerbated operational challenges, making it imperative to coordinate diverse equipment to mitigate voltage violations and enhance power quality. Although existing data-driven approaches have demonstrated effectiveness in the voltage control problem, they often require extensive trial-and-error exploration and struggle to incorporate heterogeneous information, such as day-ahead forecasts and semantic-based grid codes. Considering the operational scenarios and requirements in real-world ADNs, in this paper, we propose a hybrid knowledge-data-driven approach that leverages dynamic collaboration between a large language model (LLM) agent and a reinforcement learning (RL) agent to achieve two-stage voltage control. In the day-ahead stage, the LLM agent receives coarse region-level forecasts and generates scheduling strategies for on-load tap changer (OLTC) and shunt capacitors (SCs) to regulate the overall voltage profile. Then in the intra-day stage, based on accurate node-level measurements, the RL agent refines terminal voltages by deriving reactive power generation strategies for PV inverters. On top of the LLM-RL collaboration framework, we further propose a self-evolution mechanism for the LLM agent and a pretrain-finetune pipeline for the RL agent, effectively enhancing and coordinating the policies for both agents. The proposed approach not only aligns more closely with practical operational characteristics but also effectively utilizes the inherent knowledge and reasoning capabilities of the LLM agent, significantly improving training efficiency and voltage control performance. Comprehensive comparisons and ablation studies demonstrate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于LLM-RL协同的两阶段主动配电网电压控制：一种混合知识数据驱动方法</div>
<div class="mono" style="margin-top:8px">分布式光伏在主动配电网中渗透率的提升加剧了运行挑战，亟需协调多元设备以抑制电压越限并改善电能质量。现有数据驱动方法虽在电压控制中展现成效，但常需大量试错探索且难以融合日前预测、基于语义的电网规程等异构信息。本文结合实际主动配电网运行场景与需求，提出一种混合知识数据驱动方法，通过大语言模型智能体与强化学习智能体的动态协同实现两阶段电压控制：日前阶段，LLM智能体接收粗粒度区域级预测数据，生成有载调压变压器与并联电容器的调度策略以调节整体电压态势；日内阶段，RL智能体基于精确节点级量测数据，通过制定光伏逆变器无功出力策略精细化调节端电压。在LLM-RL协同框架基础上，进一步设计了LLM智能体的自进化机制与RL智能体的预训练-微调流程，有效提升并协调了双智能体的策略性能。该方法不仅更贴合实际运行特征，还能充分利用LLM智能体的内在知识与推理能力，显著提升训练效率与电压控制性能。综合对比与消融实验验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the operational challenges of integrating distributed photovoltaics into active distribution networks, which require coordinating diverse equipment to mitigate voltage violations, this paper proposes a hybrid knowledge-data-driven approach for two-stage voltage control via collaboration between a large language model (LLM) agent and a reinforcement learning (RL) agent. The method employs the LLM agent in the day-ahead stage to generate scheduling strategies for on-load tap changers and shunt capacitors using coarse forecasts, while the RL agent refines terminal voltages in the intra-day stage by deriving reactive power strategies for PV inverters based on accurate measurements; it further enhances policies through a self-evolution mechanism for the LLM and a pretrain-finetune pipeline for the RL. Experimental results from comprehensive comparisons and ablation studies demonstrate that this approach improves training efficiency and voltage control performance by effectively leveraging the LLM&#x27;s knowledge and reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">针对分布式光伏接入主动配电网带来的运行挑战，如协调多样设备以缓解电压越限，本文提出了一种混合知识数据驱动的两阶段电压控制方法，通过大型语言模型（LLM）智能体与强化学习（RL）智能体的协作实现。该方法在日前阶段利用LLM智能体基于粗粒度预测生成有载调压变压器和并联电容器的调度策略，在日内阶段则通过RL智能体基于精确测量为光伏逆变器制定无功功率策略以细化端电压；进一步通过LLM的自进化机制和RL的预训练-微调流程增强策略协调。综合对比与消融实验结果表明，该方法有效利用了LLM的内在知识与推理能力，显著提升了训练效率和电压控制性能。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Lead Critic based Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: David Eckel, Henri Meeß</div>
<div class="meta-line">First: 2026-02-25T08:33:39+00:00 · Latest: 2026-02-25T08:33:39+00:00</div>
<div class="meta-line">Comments: 16 pages, 10 Figures, Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21680v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21680v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cooperative Multi-Agent Reinforcement Learning (MARL) solves complex tasks that require coordination from multiple agents, but is often limited to either local (independent learning) or global (centralized learning) perspectives. In this paper, we introduce a novel sequential training scheme and MARL architecture, which learns from multiple perspectives on different hierarchy levels. We propose the Hierarchical Lead Critic (HLC) - inspired by natural emerging distributions in team structures, where following high-level objectives combines with low-level execution. HLC demonstrates that introducing multiple hierarchies, leveraging local and global perspectives, can lead to improved performance with high sample efficiency and robust policies. Experimental results conducted on cooperative, non-communicative, and partially observable MARL benchmarks demonstrate that HLC outperforms single hierarchy baselines and scales robustly with increasing amounts of agents and difficulty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于分层主导评论家的多智能体强化学习</div>
<div class="mono" style="margin-top:8px">协作式多智能体强化学习（MARL）旨在解决需要多个智能体协同的复杂任务，但现有方法通常局限于局部（独立学习）或全局（集中式学习）视角。本文提出一种新颖的顺序训练方案与MARL架构，能够从不同层级的多重视角进行学习。我们受团队结构中自然涌现的层级分布启发，提出分层主导评论家（HLC）方法，将高层目标规划与低层执行相结合。HLC表明，通过引入多层级架构并融合局部与全局视角，能够以高样本效率获得性能提升并生成鲁棒策略。在协作式、非通信及部分可观测的MARL基准测试中，HLC均优于单层基线方法，且能随着智能体数量与任务难度的增加保持稳健的扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in cooperative Multi-Agent Reinforcement Learning (MARL), which often relies on either purely local or global perspectives, by proposing a novel hierarchical approach to integrate multiple viewpoints. The method introduces a Hierarchical Lead Critic (HLC) architecture, inspired by natural team structures, that sequentially trains agents using both high-level objectives and low-level execution to combine local and global insights. Experimental results on cooperative, non-communicative, and partially observable benchmarks show that HLC outperforms single-hierarchy baselines, achieving higher sample efficiency and more robust policies that scale effectively with increasing agent numbers and task difficulty.</div>
<div class="mono" style="margin-top:8px">本文针对协作式多智能体强化学习通常局限于局部或全局视角的问题，提出了一种新颖的分层方法来整合多视角信息。该方法受自然团队结构启发，设计了分层主导评论家架构，通过顺序训练结合高层目标与底层执行，以融合局部和全局视角。在协作、非通信和部分可观测基准上的实验结果表明，该方法的性能优于单层基线，具有更高的样本效率和更强的策略鲁棒性，并能随着智能体数量和任务难度的增加而有效扩展。</div>
</details>
</div>
<div class="card">
<div class="title">SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards</div>
<div class="meta-line">Authors: Dengjia Zhang, Xiaoou Liu, Lu Cheng, Yaqing Wang, Kenton Murray, Hua Wei</div>
<div class="meta-line">First: 2026-02-24T18:04:54+00:00 · Latest: 2026-02-25T07:50:58+00:00</div>
<div class="meta-line">Comments: Accepted by PAKDD&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21158v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.21158v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SELAUR：基于不确定性感知奖励的自演进大语言模型智能体</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正日益被部署为多步决策智能体，其中有效的奖励设计对引导学习至关重要。尽管近期研究探索了多种形式的奖励塑形与步级信用分配，但一个关键信号仍被普遍忽视：LLMs的内在不确定性。不确定性反映模型置信度，揭示需要探索的区域，即使在失败轨迹中也能提供有价值的学习线索。我们提出SELAUR：基于不确定性感知奖励的自演进LLM智能体，这是一个将不确定性直接融入奖励设计的强化学习框架。SELAUR整合基于熵、最小置信度和间隔的度量，形成组合的词元级不确定性估计，提供密集的置信度对齐监督，并采用失败感知的奖励重塑机制，将这些不确定性信号注入步级和轨迹级奖励，以提升探索效率与学习稳定性。在ALFWorld和WebShop两个基准测试上的实验表明，本方法在强基线模型上持续提升成功率。消融研究进一步验证了不确定性信号如何增强探索能力与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that the intrinsic uncertainty of large language models (LLMs), which reflects model confidence and indicates where exploration is needed, is an overlooked but valuable signal for guiding multi-step decision-making agents. The method, SELAUR, introduces a reinforcement learning framework that directly incorporates uncertainty into reward design by integrating entropy-, least-confidence-, and margin-based metrics into a token-level uncertainty estimate; this provides dense supervision and is used in a failure-aware reward reshaping mechanism to inject uncertainty signals into step- and trajectory-level rewards for improved exploration and stability. The main experimental results on the ALFWorld and WebShop benchmarks show that the method consistently improves success rates over strong baselines, with ablation studies confirming that the uncertainty signals enhance exploration and robustness.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到大型语言模型（LLM）的内在不确定性——它反映了模型置信度并指示了需要探索的方向——是一个被忽视但对指导多步决策智能体极具价值的信号。所提出的方法SELAUR是一个强化学习框架，通过将基于熵、最小置信度和间隔的度量整合为令牌级不确定性估计，直接将不确定性纳入奖励设计；这提供了密集的监督，并用于一个失败感知的奖励重塑机制，将不确定性信号注入到步骤级和轨迹级奖励中，以提高探索效率和学习的稳定性。在ALFWorld和WebShop两个基准上的主要实验结果表明，该方法相较于强基线模型持续提高了任务成功率，消融研究进一步证实了不确定性信号对探索和鲁棒性的增强作用。</div>
</details>
</div>
<div class="card">
<div class="title">CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning</div>
<div class="meta-line">Authors: Zhijiang Tang, Linhua Wang, Jiaxin Qi, Weihao Jiang, Peng Hou, Anxiang Zeng, Jianqiang Huang</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-25T07:34:26+00:00 · Latest: 2026-02-25T07:34:26+00:00</div>
<div class="meta-line">Comments: Accept by CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21655v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21655v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image captioning remains a fundamental task for vision language understanding, yet ground-truth supervision still relies predominantly on human-annotated references. Because human annotations reflect subjective preferences and expertise, ground-truth captions are often incomplete or even incorrect, which in turn limits caption models. We argue that caption quality should be assessed by two objective aspects: completeness (does the caption cover all salient visual facts?) and correctness (are the descriptions true with respect to the image?). To this end, we introduce CCCaption: a dual-reward reinforcement learning framework with a dedicated fine-tuning corpus that explicitly optimizes these properties to generate \textbf{C}omplete and \textbf{C}orrect \textbf{Captions}. For completeness, we use diverse LVLMs to disentangle the image into a set of visual queries, and reward captions that answer more of these queries, with a dynamic query sampling strategy to improve training efficiency. For correctness, we penalize captions that contain hallucinations by validating the authenticity of sub-caption queries, which are derived from the caption decomposition. Our symmetric dual-reward optimization jointly maximizes completeness and correctness, guiding models toward captions that better satisfy these objective criteria. Extensive experiments across standard captioning benchmarks show consistent improvements, offering a principled path to training caption models beyond human-annotation imitation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CCCaption：基于双奖励强化学习的完整正确图像描述生成</div>
<div class="mono" style="margin-top:8px">图像描述生成仍是视觉语言理解的基础任务，但其监督信号主要依赖人工标注的参考描述。由于人工标注反映主观偏好与专业差异，真实标注常存在描述不完整甚至错误的问题，从而制约了描述模型的性能。我们认为描述质量应从两个客观维度评估：完整性（是否覆盖所有显著视觉事实？）与正确性（描述内容是否与图像真实相符？）。为此，我们提出CCCaption：一种基于专用微调语料库的双奖励强化学习框架，通过显式优化这两项特性来生成\textbf{完整}且\textbf{正确}的\textbf{描述}。针对完整性，我们采用多样化大规模视觉语言模型将图像解构为视觉查询集合，并通过动态查询采样策略提升训练效率，对回答更多查询的描述给予奖励。针对正确性，我们通过对描述分解生成的子描述查询进行真实性验证，惩罚存在幻觉内容的描述。通过对称的双奖励优化机制，我们联合最大化完整性与正确性，引导模型生成更符合客观标准的描述。在标准图像描述基准上的大量实验表明，该方法能持续提升性能，为突破人工标注模仿的训练范式提供了理论路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of subjective human annotations that often lead to incomplete or incorrect image captions, this paper proposes CCCaption, a dual-reward reinforcement learning framework designed to optimize caption completeness and correctness. The method introduces a dedicated fine-tuning corpus and employs two reward mechanisms: for completeness, it uses diverse LVLMs to generate visual queries and rewards captions covering more queries via dynamic sampling; for correctness, it penalizes hallucinations by validating sub-caption queries derived from caption decomposition. Experimental results on standard benchmarks demonstrate consistent improvements, offering a principled approach to training caption models beyond mere imitation of human annotations.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决图像描述任务中因主观人工标注常导致描述不完整或不正确的问题，提出了CCCaption，一种双奖励强化学习框架，旨在优化描述的完整性和正确性。该方法通过专用微调语料库，采用两种奖励机制：对于完整性，利用多样化的LVLMs生成视觉查询，并通过动态采样奖励覆盖更多查询的描述；对于正确性，通过验证从描述分解得到的子查询的真实性来惩罚幻觉内容。在标准基准测试上的广泛实验显示了一致的性能提升，为训练图像描述模型提供了一条超越单纯模仿人工标注的可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">SERL: Self-Examining Reinforcement Learning on Open-Domain</div>
<div class="meta-line">Authors: Weixuan Ou, Yanzhao Zheng, Shuoshuo Sun, Wei Zhang, Baohua Dong, Hangcheng Zhu, Ruohui Huang, Gang Yu, Pengwei Yan, Yifan Qiao</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-11T07:18:55+00:00 · Latest: 2026-02-25T07:24:40+00:00</div>
<div class="meta-line">Comments: Accepted by the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07922v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.07922v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor&#x27;s capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge&#x27;s reliability. This process refines the Judge&#x27;s capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERL：开放域自检式强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）已被证明能提升大语言模型（LLM）的能力。然而，将RL应用于开放域任务面临两大挑战：（1）任务固有的主观性阻碍了可验证奖励强化学习（RLVR）所需的可验证奖励；（2）基于人类反馈的强化学习（RLHF）依赖外部奖励机制。为突破这些限制，我们提出自检式强化学习（SERL），一种创新的自改进框架，其中LLM同时担任执行者与评判者。SERL引入两种无需外部信号的协同奖励机制：一方面，为提升执行者能力，我们通过科波兰德式成对比较生成响应组来推导奖励；另一方面，提出鼓励一致判断的自洽奖励以提升评判者可靠性。该过程优化评判者能力，进而为执行者提供更稳健的奖励。实验表明，本方法优于现有自改进训练方法：SERL将Qwen3-8B在AlpacaEval 2的LC胜率从52.37%提升至59.90%。据我们所知，该方法在自改进方法中达到最先进性能，且性能可比肩Qwen3-32B等显著更大的模型，在开放域任务中展现出卓越的有效性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Self-Examining Reinforcement Learning (SERL), motivated by the limitations of existing reinforcement learning methods for large language models on open-domain tasks, where verifiable rewards are unavailable and external feedback mechanisms are required. The method proposes a self-improving framework where the LLM acts as both Actor and Judge, employing two internal reward mechanisms: a Copeland-style pairwise comparison reward to enhance the Actor&#x27;s response generation and a self-consistency reward to improve the Judge&#x27;s reliability, creating a synergistic refinement loop. Experimental results demonstrate that SERL outperforms existing self-improvement methods, raising the Qwen3-8B model&#x27;s LC win rate on AlpacaEval 2 from 52.37% to 59.90%, achieving state-of-the-art performance among self-improving approaches and matching the performance of much larger models like Qwen3-32B on open-domain tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了自检强化学习（SERL），其动机在于现有强化学习方法在开放领域任务上面临两大挑战：缺乏可验证的奖励信号以及依赖外部反馈机制。该方法采用一种自改进框架，让大语言模型同时扮演行动者和评判者角色，通过两种内部奖励机制——基于Copeland风格成对比较的奖励来提升行动者的生成能力，以及自一致性奖励来提高评判者的可靠性，从而形成协同优化循环。实验结果表明，SERL优于现有的自改进训练方法，将Qwen3-8B模型在AlpacaEval 2上的LC胜率从52.37%提升至59.90%，在自改进方法中达到了最先进的性能，并在开放领域任务上取得了与Qwen3-32B等更大模型相媲美的效果，展现出卓越的有效性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning</div>
<div class="meta-line">Authors: Changhai Zhou, Shiyang Zhang, Yuhua Zhou, Qian Qiao, Jun Gao, Cheng Jin, Kaizhou Qin, Weizhong Zhang</div>
<div class="meta-line">First: 2026-02-25T07:18:08+00:00 · Latest: 2026-02-25T07:18:08+00:00</div>
<div class="meta-line">Comments: 15 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22268v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoQRA：面向高效大语言模型微调的混合精度量化与低秩适配器联合优化</div>
<div class="mono" style="margin-top:8px">量化结合参数高效微调已成为GPU内存严格受限条件下下游适配的有效范式。然而，这种串行流程未能充分利用量化比特宽度与LoRA秩之间的复杂交互：精心优化的低量化误差比特分配未必带来优异的微调性能，且相同内存预算下不同的比特宽度与秩配置可能导致显著差异的结果。为此，我们提出AutoQRA——一种在混合量化微调过程中联合优化各层比特宽度与LoRA秩配置的框架。针对离散搜索空间庞大与频繁微调迭代评估成本高的挑战，AutoQRA将优化过程分解为两阶段：首先执行全局多保真度进化搜索，通过注入层间重要性先验知识预热初始种群，并利用特定算子与性能模型高效筛选候选配置；随后采用信赖域贝叶斯优化对搜索空间的潜力区域进行局部细化，在给定内存预算下确定最优配置。该方法能在训练过程中主动补偿特定层的量化噪声。实验表明，AutoQRA在内存占用接近均匀4比特方法的同时，获得了接近全精度微调的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that the sequential pipeline of quantization followed by parameter-efficient fine-tuning fails to exploit the interaction between quantization bit-width and LoRA rank, as optimal quantization does not always yield strong fine-tuning performance under memory constraints. To address this, the authors propose AutoQRA, a joint optimization framework that simultaneously optimizes bit-width and LoRA rank per layer during mixed-precision quantized fine-tuning, using a two-stage method: a global multi-fidelity evolutionary search warm-started with layer importance priors, followed by trust-region Bayesian optimization for local refinement. Experimental results demonstrate that AutoQRA achieves performance close to full-precision fine-tuning while maintaining a memory footprint comparable to uniform 4-bit quantization methods.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到量化后参数高效微调的串行流程未能利用量化比特宽度与LoRA秩之间的相互作用，因为在内存限制下，最优量化并不总能带来强劲的微调性能。为此，作者提出了AutoQRA，一个联合优化框架，在混合精度量化微调过程中同时优化每层的比特宽度和LoRA秩，采用两阶段方法：首先进行基于层重要性先验热启动的全局多保真度进化搜索，然后使用信任域贝叶斯优化进行局部细化。实验结果表明，AutoQRA在保持与均匀4位量化方法相当的内存占用的同时，实现了接近全精度微调的性能。</div>
</details>
</div>
<div class="card">
<div class="title">AgentLTV: An Agent-Based Unified Search-and-Evolution Framework for Automated Lifetime Value Prediction</div>
<div class="meta-line">Authors: Chaowei Wu, Huazhu Chen, Congde Yuan, Qirui Yang, Guoqing Song, Yue Gao, Li Luo, Frank Youhua Chen, Mengzhuo Guo</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2026-02-25T06:58:18+00:00 · Latest: 2026-02-25T06:58:18+00:00</div>
<div class="meta-line">Comments: 12 pages, 4 figures, submitted to KDD 2026: 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ADS Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21634v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifetime Value (LTV) prediction is critical in advertising, recommender systems, and e-commerce. In practice, LTV data patterns vary across decision scenarios. As a result, practitioners often build complex, scenario-specific pipelines and iterate over feature processing, objective design, and tuning. This process is expensive and hard to transfer. We propose AgentLTV, an agent-based unified search-and-evolution framework for automated LTV modeling. AgentLTV treats each candidate solution as an {executable pipeline program}. LLM-driven agents generate code, run and repair pipelines, and analyze execution feedback. Two decision agents coordinate a two-stage search. The Monte Carlo Tree Search (MCTS) stage explores a broad space of modeling choices under a fixed budget, guided by the Polynomial Upper Confidence bounds for Trees criterion and a Pareto-aware multi-metric value function. The Evolutionary Algorithm (EA) stage refines the best MCTS program via island-based evolution with crossover, mutation, and migration. Experiments on a large-scale proprietary dataset and a public benchmark show that AgentLTV consistently discovers strong models across ranking and error metrics. Online bucket-level analysis further indicates improved ranking consistency and value calibration, especially for high-value and negative-LTV segments. We summarize practitioner-oriented takeaways: use MCTS for rapid adaptation to new data patterns, use EA for stable refinement, and validate deployment readiness with bucket-level ranking and calibration diagnostics. The proposed AgentLTV has been successfully deployed online.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentLTV：基于智能体的统一搜索进化框架用于自动化生命周期价值预测</div>
<div class="mono" style="margin-top:8px">生命周期价值（LTV）预测在广告、推荐系统和电子商务中至关重要。实践中，LTV数据模式随决策场景变化，从业者常需构建复杂的场景专用流水线，并在特征处理、目标设计和调优环节反复迭代。这一过程成本高昂且难以迁移。本文提出AgentLTV——基于智能体的统一搜索进化框架，用于自动化LTV建模。该框架将候选解视为{可执行流水线程序}，由LLM驱动的智能体生成代码、运行修复流水线并分析执行反馈。两个决策智能体协同执行两阶段搜索：蒙特卡洛树搜索阶段在固定预算下，基于多项式上置信树准则和帕累托感知多指标价值函数探索广阔建模空间；进化算法阶段通过包含交叉、变异和迁移的岛屿进化机制优化最佳MCTS程序。在大规模专有数据集和公开基准测试中，AgentLTV在排序与误差指标上均能持续发现强效模型。在线分桶分析进一步表明其提升了排序一致性与价值校准能力，尤其在高价值和负LTV区段。实践启示包括：采用MCTS快速适应新数据模式，运用EA进行稳定优化，并通过分桶排序与校准诊断验证部署就绪度。AgentLTV已成功在线部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high cost and poor transferability of manually building scenario-specific pipelines for Lifetime Value (LTV) prediction. It proposes AgentLTV, an automated framework where LLM-driven agents generate and repair executable pipeline programs, coordinating a two-stage search: a Monte Carlo Tree Search (MCTS) for broad exploration under a budget and an Evolutionary Algorithm (EA) stage for refinement. Experimental results on proprietary and public datasets show the framework consistently discovers strong models, improving ranking consistency and value calibration, especially for high-value and negative-LTV segments, and has been successfully deployed online.</div>
<div class="mono" style="margin-top:8px">该论文的动机在于，为生命周期价值预测手动构建特定场景的流程成本高昂且难以迁移。为此，本文提出了AgentLTV框架，它利用基于大语言模型的智能体生成和修复可执行的流程程序，并协调一个两阶段搜索：蒙特卡洛树搜索在预算内进行广泛探索，进化算法阶段则进行精细化改进。在专有和公开数据集上的实验表明，该框架能持续发现性能优异的模型，提升了排序一致性和价值校准能力，尤其对高价值和负生命周期价值用户群体效果显著，且已成功在线部署。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Correcting VLA: Online Action Refinement via Sparse World Imagination</div>
<div class="meta-line">Authors: Chenyv Liu, Wentao Tan, Lei Zhu, Fengling Li, Jingjing Li, Guoli Yang, Heng Tao Shen</div>
<div class="meta-line">First: 2026-02-25T06:58:06+00:00 · Latest: 2026-02-25T06:58:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21633v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21633v1">PDF</a> · <a href="https://github.com/Kisaragi0/SC-VLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent&#x27;s internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自校正视觉语言动作模型：基于稀疏世界想象的在线动作优化</div>
<div class="mono" style="margin-top:8px">标准视觉-语言-动作模型依赖统计数据先验拟合，限制其对底层物理动态的鲁棒理解。强化学习通过探索增强物理基础，但通常依赖与智能体内部状态隔离的外部奖励信号。世界动作模型作为新兴范式，融合想象与控制以实现预测性规划，但其依赖隐式上下文建模，缺乏显式的自我改进机制。为解决这些问题，我们提出自校正视觉语言动作模型，通过稀疏想象内在引导动作优化实现自我改进。首先设计稀疏世界想象模块，集成辅助预测头来预估当前任务进度与未来轨迹趋势，从而约束策略编码短期物理演化。随后引入在线动作优化模块，重塑进度相关的密集奖励，基于预测的稀疏未来状态调整轨迹方向。在仿真基准和真实场景的机器人操作任务评估表明，该方法取得最先进性能：相比最优基线方法减少16%步骤数、提升9%成功率，并在真实实验中实现14%性能增益。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of standard vision-language-action models in robustly understanding physical dynamics and the lack of explicit self-improvement mechanisms in world action models, this paper proposes Self-Correcting VLA (SC-VLA), a method that achieves online action refinement through sparse world imagination. The approach integrates auxiliary predictive heads to forecast task progress and future trajectory trends, encoding short-term physical evolution, and uses an online action refinement module to reshape progress-dependent rewards based on predicted sparse future states. Experimental results on challenging robot manipulation tasks in simulation and real-world settings show state-of-the-art performance, with SC-VLA achieving 16% fewer steps, a 9% higher success rate than top baselines, and a 14% gain in real-world experiments.</div>
<div class="mono" style="margin-top:8px">针对标准视觉-语言-动作模型在物理动态理解上的不足以及世界动作模型缺乏显式自我改进机制的问题，本文提出了自校正VLA（SC-VLA），该方法通过稀疏世界想象实现在线动作优化。该方法整合了辅助预测头来预测任务进度和未来轨迹趋势，编码短期物理演化，并引入在线动作优化模块，根据预测的稀疏未来状态调整基于进度的奖励。在仿真基准和真实世界的机器人操作任务评估中，SC-VLA取得了最先进的性能，相比最佳基线减少了16%的步骤数，成功率提高了9%，并在真实实验中获得了14%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Audio Captions with Human Preferences</div>
<div class="meta-line">Authors: Kartik Hegde, Rehana Mahfuz, Yinyi Guo, Erik Visser</div>
<div class="meta-line">First: 2025-09-18T06:33:44+00:00 · Latest: 2026-02-25T06:36:29+00:00</div>
<div class="meta-line">Comments: Submitted for review to Interspeech 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14659v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14659v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current audio captioning relies on supervised learning with paired audio-caption data, which is costly to curate and may not reflect human preferences in real-world scenarios. To address this, we propose a preference-aligned audio captioning framework based on Reinforcement Learning from Human Feedback (RLHF). To capture nuanced preferences, we train a Contrastive Language-Audio Pretraining (CLAP) based reward model using human-labeled pairwise preference data. This reward model is integrated into an RL framework to fine-tune any baseline captioning system without ground-truth annotations. Extensive human evaluations across multiple datasets show that our method produces captions preferred over baseline models, particularly when baselines fail to provide correct and natural captions. Furthermore, our framework achieves performance comparable to supervised approaches with ground-truth data, demonstrating effective alignment with human preferences and scalability in real-world use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>音频描述与人类偏好的对齐研究</div>
<div class="mono" style="margin-top:8px">当前音频描述方法依赖成对音频-文本数据的监督学习，这类数据标注成本高昂且难以反映真实场景中的人类偏好。为此，我们提出基于人类反馈强化学习（RLHF）的偏好对齐音频描述框架。为捕捉细微偏好，我们使用人工标注的成对偏好数据训练基于对比语言-音频预训练（CLAP）的奖励模型，并将其集成至强化学习框架中，可在无需真实标注的情况下微调任意基线描述系统。跨多数据集的广泛人工评估表明，本方法生成的描述优于基线模型，尤其在基线模型无法提供正确自然描述时效果显著。此外，本框架在无真实标注数据的情况下达到与监督方法相当的性能，证明了其与人类偏好的有效对齐及实际应用的扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high cost and potential misalignment of supervised audio captioning datasets with human preferences, this paper introduces a Reinforcement Learning from Human Feedback (RLHF) framework to align captions with nuanced human judgments. The method involves training a Contrastive Language-Audio Pretraining (CLAP)-based reward model on pairwise human preference data, which then guides the reinforcement learning fine-tuning of any baseline captioning system without requiring ground-truth annotations. Experimental results from human evaluations across multiple datasets demonstrate that this approach generates captions significantly preferred over baseline models, especially in cases where baselines fail, and achieves performance comparable to supervised methods, confirming its effectiveness and scalability for real-world applications.</div>
<div class="mono" style="margin-top:8px">针对监督式音频描述数据成本高昂且可能与人类偏好不一致的问题，本文提出了一种基于人类反馈强化学习（RLHF）的框架，以使描述与细致的人类偏好对齐。该方法通过人类标注的成对偏好数据训练一个基于对比语言-音频预训练（CLAP）的奖励模型，并利用该模型在强化学习框架中对任意基线描述系统进行微调，无需真实标注。在多个数据集上的人体评估实验结果表明，该方法生成的描述明显优于基线模型，尤其在基线失败的情况下，且性能与监督方法相当，证明了其在现实应用中的有效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations</div>
<div class="meta-line">Authors: Minghao Li, Ruihang Wang, Rui Tan, Yonggang Wen</div>
<div class="meta-line">First: 2026-02-02T14:18:52+00:00 · Latest: 2026-02-25T06:25:37+00:00</div>
<div class="meta-line">Comments: Accepted as a full paper at HSCC/ICCPS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02137v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.02137v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DCoPilot：面向动态数据中心运营的生成式AI赋能策略自适应</div>
<div class="mono" style="margin-top:8px">现代数据中心（DC）部署了人工智能（AI）专用设备，在高功率密度和快速变化的工作负载下运行，分钟级自适应对安全节能运营至关重要。然而，手动设计分段深度强化学习（DRL）智能体无法跟上数据中心动态频繁变化及服务等级协议（SLA）的持续演进。这种从规范到策略的滞后导致缺乏及时有效的控制策略，可能引发服务中断。为弥合此差距，我们提出DCoPilot——一个面向动态数据中心运营的生成式控制策略混合框架。DCoPilot协同融合两种生成范式：一是执行结构化奖励形式符号生成的大语言模型（LLM），二是进行策略权重参数化生成的超网络。其运作包含三个协调阶段：（i）仿真扩展：在多样化仿真就绪（SimReady）场景中对候选奖励进行压力测试；（ii）元策略提炼：训练超网络根据SLA和场景嵌入生成条件化策略权重；（iii）在线自适应：实现响应更新规范的零样本策略生成。在涵盖五大DC组件的控制任务族评估中，DCoPilot实现了近乎零约束违反，并在所有规范变体上超越基线方法。消融实验验证了基于LLM的统一奖励生成对稳定超网络收敛的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of manually designing deep reinforcement learning agents for dynamic data center operations, which cannot keep pace with frequent workload and service-level agreement changes, risking service outages. The authors propose DCoPilot, a hybrid framework that synergizes a large language model for symbolic reward generation and a hypernetwork for parametric policy weight generation, operating through simulation scale-up, meta policy distillation, and online adaptation phases. Experimental results across five control task families show that DCoPilot achieves near-zero constraint violations and outperforms baselines under specification variations, with ablation studies validating the effectiveness of the LLM-based reward generation.</div>
<div class="mono" style="margin-top:8px">本文针对动态数据中心操作中手动设计深度强化学习代理无法跟上频繁工作负载和服务级别协议变化的问题，这可能导致服务中断。作者提出了DCoPilot混合框架，它结合了用于符号奖励生成的大语言模型和用于参数化策略权重生成的超网络，通过模拟扩展、元策略蒸馏和在线适应三个阶段运行。在五个控制任务系列上的实验结果表明，DCoPilot实现了接近零的约束违反，并在规范变化下优于所有基线，消融研究验证了基于大语言模型的统一奖励生成对稳定超网络收敛的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary System Prompt Learning for Reinforcement Learning in LLMs</div>
<div class="meta-line">Authors: Lunjun Zhang, Ryan Chen, Bradly C. Stadie</div>
<div class="meta-line">First: 2026-02-16T12:34:27+00:00 · Latest: 2026-02-25T03:50:21+00:00</div>
<div class="meta-line">Comments: 39 pages, 22 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14697v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.14697v3">PDF</a> · <a href="https://github.com/LunjunZhang/E-SPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL samples trajectories under multiple system prompts in parallel, then jointly applies RL updates to LLM weights and evolutionary updates to system prompts. System prompts evolve via mutation and crossover, two genetic operators driven by LLM self-reflection; selection is based on relative performance ratings updated across RL iterations. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results demonstrate that RL and system prompt evolution are deeply synergistic, and combining the two yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型强化学习的进化系统提示学习方法</div>
<div class="mono" style="margin-top:8px">构建能够从经验中自主自我改进的智能体系统是人工智能的长期目标。当前大语言模型主要通过两种机制实现自我改进：基于自我反思的上下文更新，以及基于强化学习的权重更新。本研究提出进化系统提示学习方法，这是一种联合改进模型上下文与模型权重的技术。在每次强化学习迭代中，该方法并行采样多个系统提示下的轨迹，随后联合实施大语言模型权重的强化学习更新与系统提示的进化更新。系统提示通过变异和交叉这两种由大语言模型自我反思驱动的遗传算子进行演化；选择机制基于跨强化学习迭代更新的相对性能评级。该方法促进了提示编码的陈述性知识与权重编码的程序性知识之间的自然分工，从而在推理和智能体任务中实现性能提升。例如，在易到难（AIME → BeyondAIME）的泛化场景中，该方法将强化学习成功率从38.8%提升至45.1%，同时优于反思性提示进化方法（40.0%）。总体而言，我们的结果表明强化学习与系统提示进化具有深度协同效应，二者结合能在样本效率和泛化能力方面获得持续增益。代码：https://github.com/LunjunZhang/E-SPL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this work is to advance autonomous AI systems by enabling large language models (LLMs) to self-improve more effectively, moving beyond isolated mechanisms like self-reflection or reinforcement learning (RL). The method, Evolutionary System Prompt Learning (E-SPL), jointly optimizes model contexts and weights by sampling trajectories under multiple system prompts in parallel during RL iterations, then applying RL updates to weights and evolutionary updates to prompts via mutation and crossover driven by LLM self-reflection, with selection based on performance ratings. Main experimental results show that E-SPL improves performance on reasoning and agentic tasks, such as increasing RL success rates from 38.8% to 45.1% in an easy-to-hard generalization setting, outperforming reflective prompt evolution (40.0%), and demonstrating synergistic gains in sample efficiency and generalization.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提升大型语言模型（LLMs）的自主改进能力来推进智能体系统的发展，超越传统的自我反思或强化学习（RL）等孤立机制。方法上，提出了进化系统提示学习（E-SPL），在每次RL迭代中并行采样多个系统提示下的轨迹，然后联合应用RL更新模型权重，并通过基于LLM自我反思的突变和交叉操作进化系统提示，选择依据跨RL迭代的性能评级。主要实验结果表明，E-SPL在推理和智能体任务上提升了性能，例如在易到难泛化设置中，将RL成功率从38.8%提高到45.1%，优于反思性提示进化（40.0%），并在样本效率和泛化方面展现出一致的协同增益。</div>
</details>
</div>
<div class="card">
<div class="title">ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Xiaoxuan Wang, Han Zhang, Haixin Wang, Yidan Shi, Ruoyan Li, Kaiqiao Han, Chenyi Tong, Haoran Deng, Renliang Sun, Alexander Taylor, Yanqiao Zhu, Jason Cong, Yizhou Sun, Wei Wang</div>
<div class="meta-line">First: 2026-02-25T03:43:34+00:00 · Latest: 2026-02-25T03:43:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21534v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21534v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARLArena：稳定智能体强化学习的统一框架</div>
<div class="mono" style="margin-top:8px">智能体强化学习（ARL）作为一种训练智能体解决复杂多步交互任务的前沿范式，已迅速受到关注。尽管早期成果令人鼓舞，但ARL仍存在高度不稳定性，常导致训练崩溃。这种不稳定性限制了其向更大环境和更长交互周期的扩展，并制约了对算法设计选择的系统性探索。本文首先提出ARLArena——一个稳定的训练方案与系统性分析框架，可在受控可复现环境中检验训练稳定性。ARLArena首先构建了清晰标准化的测试平台，随后将策略梯度分解为四个核心设计维度，并评估各维度的性能与稳定性。通过这种细粒度分析，我们提炼出ARL的统一视角，进而提出SAMPO方法——一种旨在缓解ARL主要不稳定源的稳定智能体策略优化方法。实验表明，SAMPO在多样化智能体任务中均能实现持续稳定的训练与优异性能。本研究为ARL提供了统一的策略梯度视角，并为构建稳定可复现的基于大语言模型的智能体训练流程提供了实践指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the instability and training collapse issues that limit the scalability and systematic exploration of agentic reinforcement learning (ARL) for complex tasks, this paper introduces ARLArena, a unified framework for stable training and analysis. The method constructs a standardized testbed, decomposes policy gradient into four core design dimensions to assess stability, and distills a stable optimization method called SAMPO to mitigate key instability sources. Experimental results demonstrate that SAMPO achieves consistently stable training and strong performance across diverse agentic tasks, providing a unifying perspective and practical guidance for reproducible LLM-based agent training.</div>
<div class="mono" style="margin-top:8px">本研究针对智能体强化学习在复杂任务中训练不稳定、易崩溃且难以扩展和系统探索的问题，提出了ARLArena这一稳定训练与分析统一框架。方法首先构建标准化测试环境，将策略梯度分解为四个核心设计维度以评估稳定性，并提炼出名为SAMPO的稳定优化方法来缓解主要的不稳定因素。实验结果表明，SAMPO在多种智能体任务中实现了持续稳定的训练和强大性能，为基于大语言模型的智能体训练提供了统一视角和可复现的实践指导。</div>
</details>
</div>
<div class="card">
<div class="title">Training Generalizable Collaborative Agents via Strategic Risk Aversion</div>
<div class="meta-line">Authors: Chengrui Qu, Yizhou Zhang, Nicholas Lanzetti, Eric Mazumdar</div>
<div class="meta-line">First: 2026-02-25T03:06:59+00:00 · Latest: 2026-02-25T03:06:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many emerging agentic paradigms require agents to collaborate with one another (or people) to achieve shared goals. Unfortunately, existing approaches to learning policies for such collaborative problems produce brittle solutions that fail when paired with new partners. We attribute these failures to a combination of free-riding during training and a lack of strategic robustness. To address these problems, we study the concept of strategic risk aversion and interpret it as a principled inductive bias for generalizable cooperation with unseen partners. While strategically risk-averse players are robust to deviations in their partner&#x27;s behavior by design, we show that, in collaborative games, they also (1) can have better equilibrium outcomes than those at classical game-theoretic concepts like Nash, and (2) exhibit less or no free-riding. Inspired by these insights, we develop a multi-agent reinforcement learning (MARL) algorithm that integrates strategic risk aversion into standard policy optimization methods. Our empirical results across collaborative benchmarks (including an LLM collaboration task) validate our theory and demonstrate that our approach consistently achieves reliable collaboration with heterogeneous and previously unseen partners across collaborative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过战略风险规避训练可泛化的协作智能体</div>
<div class="mono" style="margin-top:8px">许多新兴的智能体范式要求智能体之间（或与人）协作以实现共同目标。然而，现有针对此类协作问题的策略学习方法往往产生脆弱方案，在与新伙伴合作时容易失效。我们将这些失败归因于训练中的搭便车行为以及战略鲁棒性的缺失。为解决这些问题，我们研究了战略风险规避的概念，并将其解释为一种与未见伙伴实现可泛化合作的原则性归纳偏置。虽然战略风险规避型参与者设计上能抵御伙伴行为偏差，但我们证明在协作博弈中，它们还具备以下优势：（1）相比纳什等经典博弈论概念，可能获得更优的均衡结果；（2）表现出更少或零搭便车行为。基于这些发现，我们开发了一种多智能体强化学习算法，将战略风险规避整合到标准策略优化方法中。在包括大语言模型协作任务在内的多个协作基准测试中，实证结果验证了我们的理论，表明该方法能持续实现与异构及未见伙伴的可靠协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the brittleness of existing multi-agent policies when collaborating with new partners, attributing this to free-riding and lack of strategic robustness. To address this, it proposes strategic risk aversion as an inductive bias for generalizable cooperation, showing theoretically that it leads to better equilibrium outcomes and reduced free-riding compared to classical concepts like Nash equilibrium. The method integrates strategic risk aversion into multi-agent reinforcement learning (MARL) via policy optimization, with experimental results across collaborative benchmarks, including an LLM task, demonstrating consistent and reliable collaboration with heterogeneous, unseen partners.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有多智能体策略在与新伙伴协作时表现脆弱，这归因于搭便车行为和缺乏战略鲁棒性。为解决此问题，提出将战略风险规避作为可泛化合作的归纳偏置，理论表明相比纳什均衡等经典概念，它能带来更好的均衡结果并减少搭便车。方法上通过策略优化将战略风险规避融入多智能体强化学习，实验结果表明，在包括大语言模型任务在内的多个协作基准测试中，该方法能持续实现与异构、未见伙伴的可靠协作。</div>
</details>
</div>
<div class="card">
<div class="title">SPACeR: Self-Play Anchoring with Centralized Reference Models</div>
<div class="meta-line">Authors: Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T19:53:02+00:00 · Latest: 2026-02-25T02:51:23+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026. Project page: https://spacer-ai.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18060v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.18060v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spacer-ai.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPACeR：基于集中式参考模型的自博弈锚定框架</div>
<div class="mono" style="margin-top:8px">自动驾驶系统的发展不仅需要安全与效率，更需具备社会意识、可预测且拟人化的行为。这要求仿真智能体策略在多智能体场景中兼具拟人性、高效性与可扩展性。近期基于大规扩散模型或分词化模型的模仿学习进展表明，可直接从人类驾驶数据中提取行为模式以生成拟真策略，但这类模型存在计算成本高、推理速度慢、难以适应反应式闭环场景的局限。相比之下，自博弈强化学习虽能高效扩展并自然捕捉多智能体交互，却常依赖启发式规则与奖励设计，且生成策略易偏离人类行为规范。本文提出SPACeR框架，利用预训练的分词自回归运动模型作为集中式参考策略，以指导去中心化自博弈训练。该参考模型通过似然奖励与KL散度，将策略锚定于人类驾驶分布，同时保持强化学习的可扩展性。在Waymo仿真智能体挑战赛的评估中，本方法在达到与模仿学习策略相当性能的同时，推理速度提升最高达10倍，参数规模较大型生成模型缩小50倍。此外，在闭环自我规划评估任务中，本框架生成的仿真智能体可通过快速可扩展的交通模拟有效评估规划器质量，为自动驾驶策略测试建立了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for autonomous vehicle (AV) simulation agents that exhibit realistic, human-like behaviors while being computationally efficient and scalable in multi-agent settings. The proposed method, SPACeR, integrates self-play reinforcement learning with a pretrained tokenized autoregressive motion model as a centralized reference policy, using likelihood rewards and KL divergence to anchor decentralized policies to human driving distributions. Experimental results on the Waymo Sim Agents Challenge show that SPACeR achieves competitive performance with imitation-learned policies, while being up to 10 times faster at inference and 50 times smaller in parameter size than large generative models, and it effectively enables fast, scalable traffic simulation for AV planner testing.</div>
<div class="mono" style="margin-top:8px">本文的动机是开发具有逼真、类人行为且计算高效、可扩展的多智能体自动驾驶仿真代理。提出的SPACeR方法将自博弈强化学习与预训练的令牌化自回归运动模型作为集中式参考策略相结合，利用似然奖励和KL散度将分散策略锚定在人类驾驶分布上。在Waymo Sim Agents Challenge上的实验结果表明，SPACeR达到了与模仿学习策略相竞争的性能，同时推理速度提升高达10倍，参数规模比大型生成模型小50倍，并能有效支持快速、可扩展的交通仿真以测试自动驾驶规划器。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management</div>
<div class="meta-line">Authors: M. Saifullah, K. G. Papakonstantinou, A. Bhattacharya, S. M. Stoffels, C. P. Andriotis</div>
<div class="meta-line">First: 2024-01-23T02:52:36+00:00 · Latest: 2026-02-25T01:58:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.12455v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.12455v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Life-cycle management of large-scale transportation systems requires determining a sequence of inspection and maintenance decisions to minimize long-term risks and costs while dealing with multiple uncertainties and constraints that lie in high-dimensional spaces. Traditional approaches have been widely applied but often suffer from limitations related to optimality, scalability, and the ability to properly handle uncertainty. Moreover, many existing methods rely on unconstrained formulations that overlook critical operational constraints. We address these issues in this work by casting the optimization problem within the framework of constrained Partially Observable Markov Decision Processes (POMDPs), which provide a robust mathematical foundation for stochastic sequential decision-making under observation uncertainties, in the presence of risk and resource limitations. To tackle the high dimensionality of state and action spaces, we propose DDMAC-CTDE, a Deep Decentralized Multi-Agent Actor-Critic (DDMAC) reinforcement learning architecture with Centralized Training and Decentralized Execution (CTDE). To demonstrate the utility of the proposed framework, we also develop a new comprehensive benchmark environment representing an existing transportation network in Virginia, U.S., with heterogeneous pavement and bridge assets undergoing nonstationary degradation. This environment incorporates multiple practical constraints related to budget limits, performance guidelines, traffic delays, and risk considerations. On this benchmark, DDMAC-CTDE consistently outperforms standard transportation management baselines, producing better policies. Together, the proposed framework and benchmark provide (i) a scalable, constraint-aware methodology, and (ii) a realistic, rigorous testbed for comprehensive evaluation of Deep Reinforcement Learning (DRL) for transportation infrastructure management.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向交通基础设施管理的集中训练与分散执行多智能体深度强化学习</div>
<div class="mono" style="margin-top:8px">大规模交通系统的全生命周期管理需在应对高维空间中的多重不确定性与约束的同时，确定检测与维护决策序列以最小化长期风险与成本。传统方法虽广泛应用，但常受限于最优性、可扩展性及不确定性处理能力。此外，现有方法多依赖忽略关键操作约束的无约束建模。本研究通过将优化问题置于约束部分可观测马尔可夫决策过程框架中，为存在风险与资源限制的观测不确定性下随机序贯决策提供稳健数学基础。为应对状态与行动空间的高维性，提出DDMAC-CTDE——一种采用集中训练与分散执行架构的深度分散多智能体行动者-评论家强化学习方法。为验证框架实用性，开发了基于美国弗吉尼亚州真实交通网络的综合基准环境，包含经历非平稳退化的异质路面与桥梁资产，并整合预算限制、性能标准、交通延误及风险考量等多重现实约束。在该基准测试中，DDMAC-CTDE持续优于传统交通管理基线方法，生成更优策略。所提框架与基准共同提供了：（1）可扩展的约束感知方法论；（2）用于全面评估深度强化学习在交通基础设施管理中应用的现实化严谨测试平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for scalable and constraint-aware life-cycle management of large-scale transportation systems under uncertainty, this paper formulates the problem as a constrained Partially Observable Markov Decision Process (POMDP) and proposes DDMAC-CTDE, a deep decentralized multi-agent actor-critic reinforcement learning architecture with centralized training and decentralized execution to handle high-dimensional state and action spaces. The method is evaluated on a new comprehensive benchmark environment modeling a real transportation network in Virginia with heterogeneous assets and practical constraints like budgets and traffic delays. Experimental results show that DDMAC-CTDE consistently outperforms standard transportation management baselines, yielding better policies and providing a scalable methodology alongside a realistic testbed for deep reinforcement learning in infrastructure management.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决大规模交通系统在不确定性下进行全生命周期管理时面临的扩展性和约束处理难题，为此将问题建模为约束部分可观测马尔可夫决策过程，并提出了DDMAC-CTDE，即一种采用集中训练与分散执行的深度分散多智能体行动者-评论家强化学习架构，以应对高维状态和动作空间。该方法在一个新建的综合性基准环境中进行评估，该环境模拟了美国弗吉尼亚州的真实交通网络，包含异质资产以及预算、交通延误等实际约束。实验结果表明，DDMAC-CTDE在性能上持续优于传统交通管理基线方法，能产生更优的策略，同时为交通基础设施管理的深度强化学习研究提供了一个可扩展的方法论和现实的测试平台。</div>
</details>
</div>
<div class="card">
<div class="title">GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Ningyuan Yang, Weihua Du, Weiwei Sun, Sean Welleck, Yiming Yang</div>
<div class="meta-line">First: 2026-02-25T01:54:50+00:00 · Latest: 2026-02-25T01:54:50+00:00</div>
<div class="meta-line">Comments: 14 pages. Preliminary work</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21492v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21492v1">PDF</a> · <a href="https://github.com/StigLidu/GradAlign">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is shaped by exploration and reward feedback, unlike supervised fine-tuning (SFT) with fixed trajectories. As a result, prior work often relies on manual curation or simple heuristic filters (e.g., accuracy), which can admit incorrect or low-utility problems. We propose GradAlign, a gradient-aligned data selection method for LLM reinforcement learning that uses a small, trusted validation set to prioritize training problems whose policy gradients align with validation gradients, yielding an adaptive curriculum. We evaluate GradAlign across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus, showing that GradAlign consistently outperforms existing baselines, underscoring the importance of directional gradient signals in navigating non-stationary policy optimization and yielding more stable training and improved final performance. We release our implementation at https://github.com/StigLidu/GradAlign</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GradAlign：面向大语言模型强化学习的梯度对齐数据选择方法</div>
<div class="mono" style="margin-top:8px">强化学习已成为大语言模型后训练的核心范式，但其性能对训练问题的质量高度敏感。这种敏感性源于强化学习的非平稳性：轨迹由动态演化的策略生成，学习过程受探索和奖励反馈的塑造，与使用固定轨迹的监督微调不同。因此，先前研究常依赖人工筛选或简单启发式过滤（如准确率），可能引入错误或低效用问题。我们提出GradAlign——一种面向大语言模型强化学习的梯度对齐数据选择方法，通过小型可信验证集优先选择策略梯度与验证梯度方向一致的问题，形成自适应课程。我们在三种挑战性数据场景（不可靠奖励信号、分布不均衡、低效用训练语料）中评估GradAlign，结果表明其持续优于现有基线，印证了方向性梯度信号在非平稳策略优化中的关键作用，能实现更稳定的训练与更优的最终性能。代码发布于https://github.com/StigLidu/GradAlign</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the sensitivity of reinforcement learning (RL) for large language models (LLMs) to training data quality, as RL&#x27;s non-stationary nature—with evolving policies and exploration—makes it distinct from supervised fine-tuning with fixed data. The method, GradAlign, introduces a gradient-aligned data selection technique that uses a small trusted validation set to prioritize training problems whose policy gradients align with validation gradients, creating an adaptive curriculum. Experimental results across three challenging regimes—unreliable rewards, distribution imbalance, and low-utility corpora—demonstrate that GradAlign consistently outperforms existing baselines, leading to more stable training and improved final performance by leveraging directional gradient signals.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大语言模型（LLM）强化学习对训练数据质量的敏感性，因为强化学习的非平稳性（策略不断演变和探索）使其不同于使用固定数据的监督微调。该方法GradAlign提出了一种梯度对齐的数据选择技术，利用一个小的可信验证集来优先选择那些策略梯度与验证梯度对齐的训练问题，从而形成自适应课程。在三个具有挑战性的场景（不可靠奖励、分布不平衡和低效用语料库）中的实验结果表明，GradAlign持续优于现有基线方法，通过利用方向性梯度信号实现了更稳定的训练和最终性能的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Code World Models for Parameter Control in Evolutionary Algorithms</div>
<div class="meta-line">Authors: Camilo Chacón Sartori, Guillem Rodríguez Corominas</div>
<div class="meta-line">First: 2026-02-25T01:49:29+00:00 · Latest: 2026-02-25T01:49:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22260v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22260v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer&#x27;s dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \lo{} and \onemax{}, CWM-greedy performs within 6\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\% success rate), CWM-greedy achieves 100\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\ $36.32$; $p&lt;0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\ 500 online episodes), success rate (100\% vs.\ 58\%), and generalization ($k{=}3$: 78\% vs.\ 0\%). Robustness experiments confirm stable synthesis across 5 independent runs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化算法参数控制的代码世界模型</div>
<div class="mono" style="margin-top:8px">大型语言模型能否学习优化器的行为并利用该知识控制它？我们将代码世界模型（CWM）——一种通过LLM合成的预测环境动态的Python程序——从确定性游戏扩展至随机组合优化。给定$(1{+}1)$-$\text{RLS}_k$的次优轨迹，LLM合成优化器动态的模拟器；基于该模拟器的贪婪规划在每一步选择变异强度$k$。在\lo{}和\onemax{}问题上，CWM贪婪策略的表现与理论最优策略相差不到6%——且从未观察过最优策略轨迹。在\jump{$_k$}问题上，因欺骗性山谷导致所有自适应基线失效（成功率0%），CWM贪婪策略实现100%成功率——且未使用任何关于间隙参数的先验知识。在NK-Landscape问题上（无闭式模型存在），当提示包含经验转移统计时，CWM贪婪策略在15个独立生成实例中均优于所有基线（$36.94$ vs.\ $36.32$；$p&lt;0.001$）。CWM在样本效率（200条离线轨迹 vs.\ 500次在线回合）、成功率（100% vs.\ 58%）和泛化能力（$k{=}3$：78% vs.\ 0%）上也优于DQN。鲁棒性实验证实了5次独立运行中合成的稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of enabling large language models (LLMs) to understand and control optimizer behavior, this paper extends Code World Models (CWMs) from deterministic games to stochastic combinatorial optimization. The method involves synthesizing a Python program simulator of an evolutionary algorithm&#x27;s dynamics from suboptimal trajectories and then using greedy planning over this simulator to adaptively select mutation parameters. Experimental results show that the CWM-greedy approach performs close to the theoretical optimum on standard benchmarks, achieves a 100% success rate on a deceptive problem where other adaptive methods fail, and outperforms baselines like DQN in sample efficiency and generalization across various problem instances.</div>
<div class="mono" style="margin-top:8px">本文的动机是让大型语言模型（LLM）能够理解并控制优化器的行为，将代码世界模型（CWM）从确定性游戏扩展到随机组合优化领域。其方法基于次优轨迹合成模拟进化算法动态的Python程序模拟器，并通过贪婪规划自适应选择突变参数。实验结果表明，CWM-greedy方法在标准测试问题上接近理论最优性能，在具有欺骗性的问题上实现了100%的成功率（而其他自适应方法均失败），并在多个问题实例上，在样本效率、成功率和泛化能力方面均优于DQN等基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Spurious Rewards: Rethinking Training Signals in RLVR</div>
<div class="meta-line">Authors: Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, Luke Zettlemoyer</div>
<div class="meta-line">First: 2025-06-12T17:49:55+00:00 · Latest: 2026-02-25T01:06:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10947v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.10947v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain language models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR training with GRPO improves MATH-500 performance for Qwen2.5-Math-7B by 21.4 percentage points using randomly assigned rewards, nearly matching the 29.1-point gain from ground-truth rewards. To explain this counterintuitive observation, we show that GRPO exhibits a clipping bias from the clip term, which can amplify high-prior behaviors learned during pretraining even without informative rewards. As a case study, we identify one such behavior in Qwen2.5-Math models, which we call code reasoning -- reasoning in code without actual code execution; code-reasoning frequency increases from 65 percent to over 90 percent with spurious rewards. However, the presence of such amplifiable behaviors is highly model-dependent. In practice, spurious rewards that are effective for Qwen models often fail to produce gains for other model families, such as Llama3 or OLMo2. Our results highlight the importance of validating RL methods across diverse models rather than relying on a single de facto choice: large gains can arise on Qwen models even from random rewards that do not reflect genuine capability improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>虚假奖励：重新审视RLVR中的训练信号</div>
<div class="mono" style="margin-top:8px">研究表明，即使使用与正确答案相关性微弱、无关甚至负相关的虚假奖励，基于可验证奖励的强化学习（RLVR）仍能在某些语言模型中激发出较强的数学推理能力。例如，采用GRPO的RLVR训练配合随机分配奖励，可将Qwen2.5-Math-7B在MATH-500数据集上的性能提升21.4个百分点，接近使用真实奖励带来的29.1个百分点提升。为解释这一反直觉现象，我们证明GRPO因裁剪项存在裁剪偏差，即使在没有信息性奖励的情况下，也能放大预训练阶段习得的高先验行为。以Qwen2.5-Math模型为例，我们识别出一种称为“代码推理”的行为（即在不实际执行代码的情况下进行代码式推理），使用虚假奖励时该行为的出现频率从65%提升至90%以上。然而，此类可放大行为的存在高度依赖模型：对Qwen模型有效的虚假奖励，往往无法在Llama3或OLMo2等其他模型系列中产生增益。本研究结果强调，应在多样化模型上验证RL方法，而非依赖单一默认选择——即使随机奖励并不反映真实能力提升，也可能在Qwen模型上产生显著增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the counterintuitive finding that reinforcement learning with verifiable rewards (RLVR) can significantly enhance mathematical reasoning in certain language models even when using spurious rewards that are poorly correlated with correctness. The method, exemplified by GRPO, is shown to induce a clipping bias that amplifies high-prior behaviors learned during pretraining, such as a &#x27;code reasoning&#x27; pattern in Qwen2.5-Math models, rather than directly optimizing for task accuracy. Experimental results demonstrate that training Qwen2.5-Math-7B on MATH-500 with randomly assigned rewards yields a 21.4 percentage point performance gain, approaching the 29.1-point improvement from ground-truth rewards, while increasing code reasoning frequency from 65% to over 90%. However, the study crucially finds that this susceptibility to spurious rewards is model-dependent, as similar gains are not observed in families like Llama3 or OLMo2, underscoring the need for cross-model validation in RL methods.</div>
<div class="mono" style="margin-top:8px">本文研究发现，在使用与正确答案相关性微弱、无关甚至负相关的虚假奖励时，基于可验证奖励的强化学习（RLVR）仍能显著提升某些语言模型的数学推理能力。其方法，以GRPO为例，会因裁剪项产生偏差，从而放大预训练阶段习得的高先验行为（例如Qwen2.5-Math模型中的“代码推理”模式），而非直接优化任务准确性。实验结果表明，使用随机分配的奖励对Qwen2.5-Math-7B进行MATH-500训练，性能提升了21.4个百分点，接近使用真实奖励带来的29.1点增益，同时将代码推理频率从65%提高到90%以上。然而，研究关键性地指出这种对虚假奖励的敏感性具有模型依赖性，因为在Llama3或OLMo2等其他模型家族中并未观察到类似增益，这强调了在强化学习方法中进行跨模型验证的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Stabilizing Off-Policy Training for Long-Horizon LLM Agent via Turn-Level Importance Sampling and Clipping-Triggered Normalization</div>
<div class="meta-line">Authors: Chenliang Li, Adel Elmahdy, Alex Boyd, Zhongruo Wang, Siliang Zeng, Alfredo Garcia, Parminder Bhatia, Taha Kass-Hout, Cao Xiao, Mingyi Hong</div>
<div class="meta-line">First: 2025-11-25T05:54:02+00:00 · Latest: 2026-02-24T23:08:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.20718v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.20718v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) algorithms such as PPO and GRPO are widely used to train large language models (LLMs) for multi-turn agentic tasks. However, in off-policy training pipelines, these methods often exhibit unstable optimization dynamics and are prone to performance collapse. Through empirical analysis, we identify two fundamental sources of instability in this setting: (1)~a granularity mismatch between token-level policy optimization and turn-structured interactions, and (2) high-variance and unreliable gradient updates induced by off-policy importance sampling and inaccurate advantage estimation. To address these challenges, we propose SORL, \underline{S}tabilizing \underline{O}ff-Policy \underline{R}einforcement \underline{L}earning for Long-Horizon Agent Training. SORL introduces principled mechanisms that align policy optimization with the structure of multi-turn interactions and adaptively suppress unreliable off-policy updates, yielding more conservative and robust learning dynamics. Within this framework, we instantiate two stabilized algorithms: SO-PPO and SO-GRPO. Both algorithms are designed to mitigate gradient variance and prevent optimization collapse without requiring careful early stopping or heuristic tuning. We evaluate SO-PPO and SO-GRPO on a range of multi-turn search benchmarks, including general question answering, multi-hop question answering, and medical multiple-choice QA tasks. Experimental results show that both methods consistently prevent training instabilities and performance collapses observed in standard PPO and GRPO, maintain lower clipping ratios and more stable optimization trajectories, and achieve superior or comparable task performance. These results demonstrate that the proposed algorithm provides a practical, scalable, and general framework for stabilizing reinforcement learning in multi-turn LLM agent training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于轮次重要性采样与裁剪触发归一化的长程LLM智能体离策略训练稳定化方法</div>
<div class="mono" style="margin-top:8px">强化学习算法（如PPO和GRPO）被广泛用于训练大语言模型执行多轮智能体任务。然而在离策略训练流程中，这些方法常出现优化过程不稳定且易发生性能崩溃的问题。通过实证分析，我们识别出该场景下两个根本的不稳定来源：（1）词元级策略优化与轮次结构化交互之间的粒度失配；（2）离策略重要性采样与不准确优势估计引发的高方差不可靠梯度更新。为应对这些挑战，我们提出SORL——面向长程智能体训练的稳定化离策略强化学习框架。SORL引入机制化设计：使策略优化与多轮交互结构对齐，并自适应抑制不可靠的离策略更新，从而产生更保守稳健的学习动态。基于此框架，我们实例化了两种稳定化算法：SO-PPO与SO-GRPO。这两种算法均旨在降低梯度方差并防止优化崩溃，且无需依赖精细的早停策略或启发式调参。我们在多轮搜索基准任务（包括通用问答、多跳问答及医学多选题任务）上评估了SO-PPO与SO-GRPO。实验结果表明：两种方法均能持续避免标准PPO与GRPO中出现的训练不稳定和性能崩溃现象，保持更低的裁剪比率与更稳定的优化轨迹，并取得更优或相当的任务性能。这些结果证明所提算法为多轮LLM智能体训练中的强化学习稳定化提供了实用、可扩展的通用框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the instability and performance collapse observed in off-policy reinforcement learning methods like PPO and GRPO when training large language models for multi-turn agent tasks, attributed to a mismatch between token-level optimization and turn-structured interactions and high-variance gradient updates. To address this, the method introduces SORL, a framework that stabilizes training via turn-level importance sampling and clipping-triggered normalization, aligning policy updates with interaction structure and adaptively suppressing unreliable gradients, leading to two instantiated algorithms: SO-PPO and SO-GRPO. Experimental results on multi-turn search benchmarks, including general and multi-hop question answering and medical QA tasks, show that these methods prevent training instabilities and collapses, maintain stable optimization trajectories with lower clipping ratios, and achieve superior or comparable performance compared to standard baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，在使用PPO和GRPO等离策略强化学习方法训练大型语言模型进行多轮代理任务时，常出现优化不稳定和性能崩溃的问题，这源于令牌级策略优化与轮次结构化交互之间的不匹配以及高方差梯度更新。为解决此问题，方法提出了SORL框架，通过轮次级重要性采样和裁剪触发归一化来稳定训练，使策略更新与交互结构对齐并自适应抑制不可靠梯度，从而实例化了SO-PPO和SO-GRPO两种算法。在包括通用问答、多跳问答和医学选择题在内的多轮搜索基准测试中，实验结果表明这些方法能防止训练不稳定和崩溃，保持较低的裁剪率和更稳定的优化轨迹，并取得优于或可比的标准基线性能。</div>
</details>
</div>
<div class="card">
<div class="title">On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation</div>
<div class="meta-line">Authors: Alexander Galozy</div>
<div class="meta-line">First: 2026-02-24T22:55:21+00:00 · Latest: 2026-02-24T22:55:21+00:00</div>
<div class="meta-line">Comments: 15 pages, 3 figures. Under review at RLC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21424v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21424v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) agents under partial observability often condition actions on internally accumulated information such as memory or inferred latent context. We formalise such information-conditioned interaction patterns as behavioural dependency: variation in action selection with respect to internal information under fixed observations. This induces a probe-relative notion of $ε$-behavioural equivalence and a within-policy behavioural distance that quantifies probe sensitivity. We establish three structural results. First, the set of policies exhibiting non-trivial behavioural dependency is not closed under convex aggregation. Second, behavioural distance contracts under convex combination. Third, we prove a sufficient local condition under which gradient ascent on a skewed mixture objective decreases behavioural distance when a dominant-mode gradient aligns with the direction of steepest contraction. Minimal bandit and partially observable gridworld experiments provide controlled witnesses of these mechanisms. In the examined settings, behavioural distance decreases under convex aggregation and under continued optimisation with skewed latent priors, and in these experiments it precedes degradation under latent prior shift. These results identify structural conditions under which probe-conditioned behavioural separation is not preserved under common policy transformations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论策略变换下认知行为的结构非保持性</div>
<div class="mono" style="margin-top:8px">部分可观测性下的强化学习（RL）智能体常根据内部累积信息（如记忆或推断的潜在上下文）调节行动。我们将此类信息调节的交互模式形式化为行为依赖性：即在固定观测下，行动选择随内部信息的变化。这引出了探测相对性的$ε$-行为等价概念，以及量化探测敏感度的策略内行为距离。我们建立了三个结构性结果：第一，展现非平凡行为依赖性的策略集合在凸聚合下不封闭；第二，行为距离在凸组合下收缩；第三，我们证明了一个充分的局部条件，即当主导模式梯度与最速收缩方向对齐时，在偏斜混合目标上的梯度上升会减小行为距离。最小赌博机与部分可观测网格世界实验为这些机制提供了受控验证。在考察场景中，行为距离在凸聚合下及偏斜潜在先验的持续优化下减小，且在这些实验中它先于潜在先验偏移下的性能退化。这些结果明确了在常见策略变换下，探测调节的行为分离不被保持的结构性条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to understand how reinforcement learning agents under partial observability, which condition actions on internal information like memory, preserve their epistemic behavior under policy transformations. The method formalizes this as behavioral dependency, introducing a notion of ε-behavioral equivalence and a behavioral distance metric to quantify sensitivity to internal probes. Experimental results from minimal bandit and gridworld settings show that behavioral distance contracts under convex policy aggregation and decreases during optimization with skewed latent priors, preceding degradation under latent prior shift, thus identifying structural conditions where probe-conditioned behavioral separation is not preserved.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要理解部分可观测性下的强化学习智能体，其基于记忆等内部信息选择行动时，如何在策略变换下保持认知行为。方法将这种行为形式化为行为依赖性，引入了ε-行为等价概念和行为距离度量来量化对内部探针的敏感性。在最小化赌博机和网格世界实验中的结果表明，行为距离在凸策略聚合下收缩，并在使用偏斜潜在先验的持续优化中减小，且先于潜在先验偏移下的性能退化，从而识别了探针条件行为分离在常见策略变换中不被保持的结构性条件。</div>
</details>
</div>
<div class="card">
<div class="title">Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning</div>
<div class="meta-line">Authors: Yuanda Xu, Hejian Sang, Zhengze Zhou, Ran He, Zhipeng Wang</div>
<div class="meta-line">First: 2026-02-24T22:46:43+00:00 · Latest: 2026-02-24T22:46:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21420v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21420v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model&#x27;s reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE&#x27;s gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer&#x27;s strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>过度自信错误需更强校正：强化学习中的非对称置信度惩罚</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）已成为提升大语言模型推理能力的主流范式。然而，标准RLVR算法存在一个公认缺陷：虽然通过锐化采样提高了Pass@1准确率，却同时收窄了模型的推理边界并降低了生成多样性。我们发现现有方法忽视了一个根本原因：对错误的均匀惩罚。当前方法——无论是按难度筛选提示的数据过滤法，还是优势值归一化方案——均对组内所有错误推演路径进行同等处理。这种均匀性使得过度自信错误（即被强化学习过程错误强化的错误推理路径）持续存在并垄断概率质量，最终抑制有效的探索轨迹。为此，我们提出非对称置信感知错误惩罚（ACE）。ACE引入基于单条推演的置信度偏移度量c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x))，以动态调节负优势值。理论上，我们证明ACE梯度可分解为针对过度自信错误的选择性正则化器梯度，加上一个能部分调节正则化强度的特征化残差项。我们在VERL框架下使用GRPO和DAPO方法，基于DAPO-Math-17K数据集对Qwen2.5-Math-7B、Qwen3-8B-Base及Llama-3.1-8B-Instruct模型进行大量微调实验。在MATH-500和AIME 2025基准测试中，ACE能与现有方法无缝集成，并在全部三个模型系列和测试集上持续提升全谱系Pass@k指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by a pathology in standard Reinforcement Learning with Verifiable Rewards (RLVR) methods, where improving Pass@1 accuracy comes at the cost of reduced reasoning diversity due to the uniform penalization of all errors, which allows overconfident incorrect reasoning to persist. To address this, the authors propose the Asymmetric Confidence-aware Error Penalty (ACE), a method that dynamically modulates negative advantages using a per-rollout confidence shift metric to selectively penalize overconfident errors more strongly. Experimental results from fine-tuning models like Qwen2.5-Math-7B on the DAPO-Math-17K dataset show that ACE consistently improves performance across the full Pass@k spectrum on benchmarks such as MATH-500 and AIME 2025, seamlessly integrating with existing RLVR frameworks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于标准可验证奖励强化学习（RLVR）方法存在一个固有缺陷：在提高Pass@1准确率的同时，由于对所有错误进行统一惩罚，导致推理多样性下降，使得过度自信的错误推理路径得以持续。为解决此问题，作者提出了非对称置信感知错误惩罚（ACE）方法，该方法通过每个生成路径的置信度偏移度量动态调整负优势，从而对过度自信的错误进行选择性强化惩罚。在DAPO-Math-17K数据集上对Qwen2.5-Math-7B等模型进行微调的实验结果表明，ACE在MATH-500和AIME 2025等基准测试中，能持续提升全Pass@k谱系性能，并与现有RLVR框架无缝兼容。</div>
</details>
</div>
<div class="card">
<div class="title">World Simulation with Video Foundation Models for Physical AI</div>
<div class="meta-line">Authors: NVIDIA, :, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, Wei-Cheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, Yuke Zhu</div>
<div class="meta-line">First: 2025-10-28T22:44:13+00:00 · Latest: 2026-02-24T21:52:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00062v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00062v2">PDF</a> · <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5">Code1</a> · <a href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向物理AI的视频基础模型世界仿真</div>
<div class="mono" style="margin-top:8px">我们推出最新一代物理AI世界基础模型[Cosmos-Predict2.5]。该模型基于流式架构，在单一模型中统一了文本到世界、图像到世界及视频到世界的生成能力，并借助物理AI视觉语言模型[Cosmos-Reason1]提供更丰富的文本基础与更精细的世界仿真控制。通过2亿条精选视频片段训练及基于强化学习的后训练优化，[Cosmos-Predict2.5]在视频质量与指令对齐方面较[Cosmos-Predict1]实现显著提升，发布模型规模为20亿与140亿参数。这些能力为机器人及自主系统提供更可靠的合成数据生成、策略评估与闭环仿真。我们进一步推出控制网风格框架[Cosmos-Transfer2.5]以扩展该系列，实现仿真到现实及现实到现实的世界转换。尽管其规模仅为[Cosmos-Transfer1]的1/3.5，却能实现更高保真度与鲁棒的长时序视频生成。这些进展共同确立了[Cosmos-Predict2.5]与[Cosmos-Transfer2.5]作为扩展具身智能的通用工具地位。为加速物理AI领域的研究与部署，我们在NVIDIA开放模型许可下于https://github.com/nvidia-cosmos/cosmos-predict2.5 与 https://github.com/nvidia-cosmos/cosmos-transfer2.5 开源代码、预训练模型及精选基准测试，期望通过开放资源降低应用门槛，推动下一代具身智能的创新建设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for advanced tools to scale embodied intelligence in robotics and autonomous systems, this paper introduces Cosmos-Predict2.5, a flow-based video foundation model that unifies text, image, and video-to-world generation, and Cosmos-Transfer2.5, a control-net style framework for simulation-to-real translation. The method leverages a physical AI vision-language model for richer text grounding and employs training on 200M video clips with reinforcement learning-based post-training. Experimental results show substantial improvements in video quality and instruction alignment over its predecessor, with higher fidelity and robust long-horizon generation despite a smaller model size, enabling more reliable synthetic data generation and closed-loop simulation.</div>
<div class="mono" style="margin-top:8px">为满足机器人及自主系统中具身智能规模化发展对先进工具的需求，本文提出了基于流架构的视频基础模型Cosmos-Predict2.5，它统一了文本、图像和视频到世界的生成，并引入了用于仿真到现实转换的控制网络风格框架Cosmos-Transfer2.5。该方法利用物理AI视觉语言模型提供更丰富的文本基础，并在2亿个精选视频片段上进行训练，结合基于强化学习的后训练。实验结果表明，相比前代模型，其在视频质量和指令对齐方面取得显著提升，尽管模型规模更小，但仍能实现更高保真度和鲁棒的长序列生成，从而为合成数据生成和闭环仿真提供了更可靠的工具。</div>
</details>
</div>
<div class="card">
<div class="title">EExApp: GNN-Based Reinforcement Learning for Radio Unit Energy Optimization in 5G O-RAN</div>
<div class="meta-line">Authors: Jie Lu, Peihao Yan, Huacheng Zeng</div>
<div class="meta-line">First: 2026-02-09T21:17:23+00:00 · Latest: 2026-02-24T20:53:30+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE INFOCOM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09206v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09206v2">PDF</a> · <a href="https://github.com/EExApp/EExApp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With over 3.5 million 5G base stations deployed globally, their collective energy consumption (projected to exceed 131 TWh annually) raises significant concerns over both operational costs and environmental impacts. In this paper, we present EExAPP, a deep reinforcement learning (DRL)-based xApp for 5G Open Radio Access Network (O-RAN) that jointly optimizes radio unit (RU) sleep scheduling and distributed unit (DU) resource slicing. EExAPP uses a dual-actor-dual-critic Proximal Policy Optimization (PPO) architecture, with dedicated actor-critic pairs targeting energy efficiency and quality-of-service (QoS) compliance. A transformer-based encoder enables scalable handling of variable user equipment (UE) populations by encoding all-UE observations into fixed-dimensional representations. To coordinate the two optimization objectives, a bipartite Graph Attention Network (GAT) is used to modulate actor updates based on both critic outputs, enabling adaptive trade-offs between power savings and QoS. We have implemented EExAPP and deployed it on a real-world 5G O-RAN testbed with live traffic, commercial RU and smartphones. Extensive over-the-air experiments and ablation studies confirm that EExAPP significantly outperforms existing methods in reducing the energy consumption of RU while maintaining QoS. The source code is available at https://github.com/EExApp/EExApp.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EExApp：基于图神经网络的强化学习在5G O-RAN无线单元能耗优化中的应用</div>
<div class="mono" style="margin-top:8px">随着全球部署超过350万个5G基站，其总能耗（预计每年超过131太瓦时）引发了运营成本和环境影响的重大关切。本文提出EExAPP——一种基于深度强化学习的5G开放式无线接入网络xApp，可联合优化无线单元休眠调度与分布式单元资源切片。EExAPP采用双执行器-双评判器近端策略优化架构，通过专用执行器-评判器对分别针对能效和服务质量合规性。基于Transformer的编码器通过将所有用户设备观测编码为固定维度表示，实现了对可变用户设备规模的可扩展处理。为协调两个优化目标，系统采用二分图注意力网络，依据双评判器输出调制执行器更新，从而在节能与服务质量间实现自适应权衡。我们已在承载真实流量、配备商用无线单元和智能手机的5G O-RAN现网测试平台中部署EExAPP。大量空口实验与消融研究证实，EExAPP在保证服务质量的同时，显著优于现有方法降低无线单元能耗。源代码发布于https://github.com/EExApp/EExApp。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the substantial energy consumption of globally deployed 5G base stations, which drives high operational costs and environmental concerns. The proposed method, EExAPP, is a deep reinforcement learning xApp for O-RAN that jointly optimizes radio unit sleep scheduling and distributed unit resource slicing using a dual-actor-dual-critic PPO architecture with a transformer encoder for scalability and a bipartite Graph Attention Network to coordinate energy efficiency and QoS objectives. Experimental results from deployment on a real-world 5G O-RAN testbed with live traffic show that EExAPP significantly outperforms existing methods in reducing radio unit energy consumption while maintaining quality of service.</div>
<div class="mono" style="margin-top:8px">本工作的动机是全球部署的5G基站能耗巨大，导致高昂运营成本和环境问题。所提出的方法EExAPP是一种用于O-RAN的深度强化学习xApp，它采用双行动者-双评论者PPO架构，结合变压器编码器处理可扩展性，并使用二分图注意力网络协调能效与服务质量目标，以联合优化射频单元休眠调度和分布式单元资源切片。在具有实时流量的真实5G O-RAN测试平台上部署的实验结果表明，EExAPP在显著降低射频单元能耗的同时保持服务质量，性能明显优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment</div>
<div class="meta-line">Authors: Mengxuan Hu, Vivek V. Datla, Anoop Kumar, Zihan Guan, Sheng Li, Alfy Samuel, Daben Liu</div>
<div class="meta-line">First: 2026-02-24T20:30:51+00:00 · Latest: 2026-02-24T20:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21346v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce Alignment-Weighted DPO, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐加权DPO：一种提升安全对齐的原则性推理方法</div>
<div class="mono" style="margin-top:8px">近年来，监督微调（SFT）、基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO）等对齐技术的进步提升了大型语言模型（LLMs）的安全性。然而，这些模型仍易受越狱攻击的影响，此类攻击通过间接或欺骗性措辞掩盖有害意图。通过因果干预实验，我们实证表明该脆弱性源于缺乏深度推理的浅层对齐机制，模型常在不真正理解有害原因的情况下拒绝有害提示。为缓解此问题，我们提出通过推理感知的后训练增强对齐能力。我们构建并发布了一个新颖的思维链（CoT）微调数据集，包含面向实用性和安全关键性的提示及逐步推理过程。基于该数据集的微调促使模型生成基于推理的原则性拒绝，其效果优于标准SFT基线。此外，受CoT微调中失败模式的启发，我们提出对齐加权DPO方法，通过对推理段和最终答案段分配不同偏好权重，针对输出中最成问题的部分进行优化。相比原始DPO，该方法能产生更细粒度、更具针对性的更新，并提升对多样化越狱策略的鲁棒性。在多个安全与实用性基准上的广泛实验表明，我们的方法在保持模型整体效用的同时，持续提升了对齐鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the persistent vulnerability of aligned large language models to jailbreak attacks, which exploit shallow safety mechanisms that lack deep reasoning. To address this, the method introduces reasoning-aware post-training via a novel Chain-of-Thought fine-tuning dataset with step-by-step rationales for both utility and safety prompts, and proposes Alignment-Weighted DPO, a refined optimization technique that assigns differential preference weights to reasoning and answer segments for targeted updates. Experimental results demonstrate that this approach consistently enhances alignment robustness against diverse jailbreak strategies while preserving model utility across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，尽管现有对齐技术提升了大型语言模型的安全性，但模型仍易受通过间接或欺骗性表述实现的越狱攻击，这源于缺乏深度推理的浅层对齐机制。方法上，通过构建包含逐步推理链的微调数据集进行推理感知的后训练，并提出了对齐加权DPO，该技术对输出中的推理和答案部分分配不同偏好权重以实现更精细的优化。实验结果表明，该方法在多个安全和效用基准测试中，能持续提升对齐的鲁棒性，同时保持模型的整体效用。</div>
</details>
</div>
<div class="card">
<div class="title">Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning</div>
<div class="meta-line">Authors: Prajwal Koirala, Cody Fleming</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-26T16:09:53+00:00 · Latest: 2026-02-24T20:26:25+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21427v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.21427v3">PDF</a> · <a href="https://github.com/PrajwalKoirala/SSCP-Single-Step-Completion-Policy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models such as diffusion and flow-matching offer expressive policies for offline reinforcement learning (RL) by capturing rich, multimodal action distributions, but their iterative sampling introduces high inference costs and training instability due to gradient propagation across sampling steps. We propose the Single-Step Completion Policy (SSCP), a generative policy trained with an augmented flow-matching objective to predict direct completion vectors from intermediate flow samples, enabling accurate, one-shot action generation. In an off-policy actor-critic framework, SSCP combines the expressiveness of generative models with the training and inference efficiency of unimodal policies, without requiring long backpropagation chains. Our method scales effectively to offline, offline-to-online, and online RL settings, offering substantial gains in speed and adaptability over diffusion-based baselines. We further extend SSCP to goal-conditioned RL, enabling flat policies to exploit subgoal structures without explicit hierarchical inference. SSCP achieves strong results across standard offline RL and behavior cloning benchmarks, positioning it as a versatile, expressive, and efficient framework for deep RL and sequential decision-making. The code is available at https://github.com/PrajwalKoirala/SSCP-Single-Step-Completion-Policy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于流模型的单步完成策略：高效且具表达性的策略学习方法</div>
<div class="mono" style="margin-top:8px">扩散模型与流匹配等生成模型通过捕捉丰富的多模态动作分布，为离线强化学习提供了表达性强的策略，但其迭代采样过程因梯度在采样步骤间的传播而带来高昂的推理成本和训练不稳定性。我们提出单步完成策略，这是一种采用增强流匹配目标训练的生成策略，能够从中间流样本预测直接完成向量，实现精确的单步动作生成。在离轨策略的演员-评论家框架中，SSCP结合了生成模型的表达性与单峰策略的训练及推理效率，无需长反向传播链。该方法可有效扩展至离线、离线转在线及在线强化学习场景，在速度和适应性上显著优于基于扩散的基线方法。我们进一步将SSCP扩展至目标条件强化学习，使扁平化策略能够利用子目标结构而无需显式的分层推理。SSCP在标准离线强化学习和行为克隆基准测试中均取得优异结果，成为深度强化学习与序列决策中兼具通用性、表达性和高效性的框架。代码发布于https://github.com/PrajwalKoirala/SSCP-Single-Step-Completion-Policy。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high inference costs and training instability of iterative generative models like diffusion in offline RL, this paper introduces the Single-Step Completion Policy (SSCP), which uses an augmented flow-matching objective to predict direct completion vectors from intermediate samples, enabling accurate one-shot action generation. The method integrates into an actor-critic framework, combining the expressiveness of generative models with the efficiency of unimodal policies by avoiding long backpropagation chains. Experimental results show SSCP scales effectively across offline, offline-to-online, and online RL settings, achieving substantial speed gains and strong performance on standard benchmarks, and it is further extended for goal-conditioned RL without hierarchical inference.</div>
<div class="mono" style="margin-top:8px">针对扩散模型等生成式策略在离线强化学习中推理成本高、训练不稳定的问题，本文提出了单步完成策略（SSCP），该方法通过增强的流匹配目标从中间样本预测直接完成向量，实现准确的一步动作生成。该方法在演员-评论家框架中结合了生成模型的表达能力和单峰策略的效率，避免了长反向传播链。实验结果表明，SSCP能有效扩展到离线、离线到在线及在线强化学习场景，在标准基准测试中取得了显著的性能提升和速度优势，并可进一步应用于目标条件强化学习而无需分层推理。</div>
</details>
</div>
<div class="card">
<div class="title">EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation</div>
<div class="meta-line">Authors: Jiahe Shi, Zhengqi Gao, Ching-Yun Ko, Duane Boning</div>
<div class="meta-line">First: 2025-11-15T05:00:07+00:00 · Latest: 2026-02-24T20:22:09+00:00</div>
<div class="meta-line">Comments: Accepted to DAC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12033v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12033v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EARL：基于熵感知强化学习的LLM对齐方法用于可靠RTL代码生成</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在硬件设计自动化领域的最新进展，特别是在使用自然语言生成寄存器传输级（RTL）代码方面，展现出巨大潜力。然而，模型能力与实际RTL设计需求之间仍存在差距，包括语法错误、功能幻觉以及与设计者意图的对齐不足。基于可验证奖励的强化学习（RLVR）为弥合这一差距提供了有前景的途径，因为硬件可提供可执行且可形式化验证的信号，用于进一步对齐模型输出与设计意图。但在长序列、结构化的RTL代码生成中，并非所有词元对功能正确性的贡献均等，简单地将梯度分散到所有词元会稀释学习信号。我们通过RTL生成的熵分析发现，仅少量关键词元（如always、if、assign、posedge）具有高不确定性，且对控制流和模块结构影响显著。为此，我们提出EARL——一个用于Verilog生成的熵感知强化学习框架。EARL利用可验证奖励信号进行策略优化，并引入熵引导的选择性更新机制，将策略梯度聚焦于高熵词元。该方法保持了训练稳定性，并将梯度更新集中在代码的功能关键区域。在VerilogEval和RTLLM数据集上的实验表明，EARL将功能通过率较现有LLM基线提升高达14.7%，同时减少了不必要的更新并提升了训练稳定性。这些结果表明，将强化学习聚焦于关键高熵词元，能够为结构化RTL代码生成带来更可靠、更具针对性的策略改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the gap between LLM capabilities and real-world RTL design demands, such as syntax errors and functional hallucinations, this paper introduces EARL, an entropy-aware reinforcement learning framework for Verilog generation. The method leverages verifiable hardware rewards and introduces entropy-guided selective updates that concentrate policy gradients on high-uncertainty tokens critical to control flow and structure, thereby preserving training stability. Experimental results on VerilogEval and RTLLM benchmarks show that EARL improves functional pass rates by up to 14.7% over prior LLM baselines while reducing unnecessary updates.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在生成寄存器传输级代码时存在的语法错误、功能幻觉及与设计意图对齐不足的问题，提出了EARL，一种用于Verilog生成的熵感知强化学习框架。该方法利用可验证的硬件奖励信号，并通过熵引导的选择性更新，将策略梯度集中在影响控制流和结构的高不确定性关键令牌上，从而保持训练稳定性。在VerilogEval和RTLLM基准测试上的实验表明，EARL将功能通过率较先前基线提升了高达14.7%，同时减少了不必要的更新。</div>
</details>
</div>
<div class="card">
<div class="title">Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data</div>
<div class="meta-line">Authors: Emre Can Acikgoz, Cheng Qian, Jonas Hübotter, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur</div>
<div class="meta-line">First: 2026-02-24T19:41:18+00:00 · Latest: 2026-02-24T19:41:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21320v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21320v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are becoming the foundation for autonomous agents that can use tools to solve complex tasks. Reinforcement learning (RL) has emerged as a common approach for injecting such agentic capabilities, but typically under tightly controlled training setups. It often depends on carefully constructed task-solution pairs and substantial human supervision, which creates a fundamental obstacle to open-ended self-evolution toward superintelligent systems. In this paper, we propose Tool-R0 framework for training general purpose tool-calling agents from scratch with self-play RL, under a zero-data assumption. Initialized from the same base LLM, Tool-R0 co-evolves a Generator and a Solver with complementary rewards: one proposes targeted challenging tasks at the other&#x27;s competence frontier and the other learns to solve them with real-world tool calls. This creates a self-evolving cycle that requires no pre-existing tasks or datasets. Evaluation on different tool-use benchmarks show that Tool-R0 yields 92.5 relative improvement over the base model and surpasses fully supervised tool-calling baselines under the same setting. Our work further provides empirical insights into self-play LLM agents by analyzing co-evolution, curriculum dynamics, and scaling behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Tool-R0：从零数据实现工具学习的自进化大语言模型智能体</div>
<div class="mono" style="margin-top:8px">大语言模型正成为能够使用工具解决复杂任务的自主智能体的基础。强化学习已成为注入此类智能体能力的常用方法，但通常在严格受控的训练设置下进行，依赖精心构建的任务-解决方案对和大量人工监督，这为通向超智能系统的开放式自进化设置了根本障碍。本文提出Tool-R0框架，在零数据假设下，通过自博弈强化学习从零开始训练通用工具调用智能体。Tool-R0从同一基础大语言模型初始化，通过互补奖励机制协同进化生成器与求解器：一方在对方能力边界提出针对性挑战任务，另一方学习通过真实工具调用来解决这些任务，从而形成无需预存任务或数据集的自进化循环。在不同工具使用基准测试中，Tool-R0相比基础模型实现92.5%的相对性能提升，并在同等设置下超越全监督工具调用基线。本研究还通过分析协同进化、课程动态和扩展行为，为自博弈大语言模型智能体提供了实证洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Tool-R0, a framework motivated by the need to overcome the reliance of current LLM-based agents on extensive human supervision and curated datasets for tool-learning, which hinders open-ended self-evolution. The method employs self-play reinforcement learning from scratch, co-evolving a Generator that creates challenging tasks and a Solver that learns to solve them using real-world tools, all without any pre-existing data. Experimental results on tool-use benchmarks demonstrate a 92.5% relative improvement over the base model and superior performance compared to fully supervised baselines, while also providing insights into co-evolution dynamics and scaling behavior.</div>
<div class="mono" style="margin-top:8px">本文提出了Tool-R0框架，其动机是克服当前基于大语言模型的智能体在工具学习中对大量人工监督和精心策划数据集的依赖，这种依赖阻碍了系统的开放式自我进化。该方法采用从零开始的自我对弈强化学习，协同进化一个生成器（负责创建具有挑战性的任务）和一个求解器（学习使用现实世界工具解决这些任务），整个过程无需任何预先存在的数据。在工具使用基准测试上的实验结果表明，相对于基础模型有92.5%的相对性能提升，并超越了完全监督的基线方法，同时该研究还通过分析协同进化、课程动态和扩展行为，为自我对弈智能体提供了实证见解。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260227_0400.html">20260227_0400</a>
<a href="archive/20260226_0405.html">20260226_0405</a>
<a href="archive/20260224_0355.html">20260224_0355</a>
<a href="archive/20260223_0321.html">20260223_0321</a>
<a href="archive/20260222_0327.html">20260222_0327</a>
<a href="archive/20260221_0347.html">20260221_0347</a>
<a href="archive/20260220_0349.html">20260220_0349</a>
<a href="archive/20260219_0406.html">20260219_0406</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0329.html">20260217_0329</a>
<a href="archive/20260216_0321.html">20260216_0321</a>
<a href="archive/20260215_0335.html">20260215_0335</a>
<a href="archive/20260213_0416.html">20260213_0416</a>
<a href="archive/20260212_0417.html">20260212_0417</a>
<a href="archive/20260211_0421.html">20260211_0421</a>
<a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
