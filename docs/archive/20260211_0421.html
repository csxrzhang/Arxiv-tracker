<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-11 04:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260211_0421</div>
    <div class="row"><div class="card">
<div class="title">iGRPO: Self-Feedback-Driven LLM Reasoning</div>
<div class="meta-line">Authors: Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping, Yejin Choi, Jan Kautz</div>
<div class="meta-line">First: 2026-02-09T18:45:11+00:00 · Latest: 2026-02-09T18:45:11+00:00</div>
<div class="meta-line">Comments: Tech report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>iGRPO：基于自我反馈驱动的大语言模型推理方法</div>
<div class="mono" style="margin-top:8px">大语言模型在解决复杂数学问题方面展现出潜力，但其生成的解决方案仍存在准确性与一致性的不足。强化学习通过任务特定奖励对齐模型，可提升整体质量与可靠性。组相对策略优化是一种无需价值函数的高效替代方案，采用组相对奖励归一化。本文提出迭代组相对策略优化，作为GRPO的两阶段扩展，通过模型生成的草稿实现动态自我条件化。第一阶段，iGRPO采样多个探索性草稿，并利用优化所用的标量奖励信号选取最高奖励草稿。第二阶段，将最优草稿附加至原始提示后，对草稿条件化改进执行GRPO式更新，训练策略超越其先前最佳尝试。在相同计算预算下，iGRPO在多种基础模型上均稳定优于GRPO。此外，将iGRPO应用于经AceReason-Math训练的OpenReasoning-Nemotron-7B模型，在AIME24与AIME25基准上分别取得85.62%与79.64%的最新最优结果。消融实验进一步表明：改进封装机制可泛化至GRPO变体之外，受益于生成式评判器，并通过延迟熵崩溃改变学习动态。这些结果凸显了基于迭代自我反馈的强化学习在推进可验证数学推理方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces iGRPO, an iterative reinforcement learning method aimed at improving the accuracy and consistency of large language models (LLMs) in complex mathematical reasoning tasks, addressing their current limitations. The method extends Group Relative Policy Optimization (GRPO) by adding a two-stage process: first, it samples multiple exploratory drafts and selects the highest-reward one using a scalar reward signal; second, it appends this best draft to the original prompt and applies GRPO-style updates on draft-conditioned refinements, enabling the model to self-improve beyond its prior attempts. Experimental results show that iGRPO consistently outperforms GRPO under matched rollout budgets across base models like Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled, achieving state-of-the-art scores of 85.62% on AIME24 and 79.64% on AIME25 when applied to OpenReasoning-Nemotron-7B, with ablations confirming its generalization benefits and altered learning dynamics.</div>
<div class="mono" style="margin-top:8px">本文提出了iGRPO，一种迭代强化学习方法，旨在提升大语言模型在复杂数学推理任务中的准确性和一致性，以解决其现有不足。该方法扩展了组相对策略优化（GRPO），采用两阶段流程：首先，采样多个探索性草稿并使用标量奖励信号选择最高奖励的草稿；其次，将此最佳草稿附加到原始提示后，在草稿条件化的改进上应用GRPO式更新，使模型能够基于先前尝试自我提升。实验结果表明，在匹配的采样预算下，iGRPO在Nemotron-H-8B-Base-8K和DeepSeek-R1 Distilled等基础模型上持续优于GRPO，当应用于OpenReasoning-Nemotron-7B时，在AIME24和AIME25上分别达到了85.62%和79.64%的最新最优成绩，消融实验进一步验证了其泛化能力和对学习动态的影响。</div>
</details>
</div>
<div class="card">
<div class="title">f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment</div>
<div class="meta-line">Authors: Rajdeep Haldar, Lantao Mei, Guang Lin, Yue Xing, Qifan Song</div>
<div class="meta-line">First: 2026-02-05T18:01:52+00:00 · Latest: 2026-02-09T18:34:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05946v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05946v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>f-GRPO及其拓展：基于散度的通用大语言模型对齐强化学习算法</div>
<div class="mono" style="margin-top:8px">近期研究表明，偏好对齐目标可视为对齐（选中）与未对齐（拒绝）响应分布间的散度估计器。本研究将这一基于散度的视角拓展至通用对齐场景，例如仅依赖环境奖励的可验证奖励强化学习。在此统一框架下，我们基于f-散度的变分表示，提出了用于通用大语言模型对齐的f-群组相对策略优化（一类同策略强化学习方法）与f-混合对齐损失（混合同/异策略目标）。理论分析证明这些目标能提升对齐后的平均奖励。实验部分在可验证奖励强化学习（数学推理）和偏好对齐任务（安全对齐）上验证了框架的有效性，结果显示其相比现有方法具有更优的性能与灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the insight that preference alignment objectives can be viewed as divergence estimators, this work extends a divergence-based framework to general language model alignment, including scenarios with only environmental rewards. The method introduces f-GRPO, an on-policy reinforcement learning class, and f-HAL, a hybrid objective, both derived from variational representations of f-divergences to optimize alignment. Experimental results on math reasoning and safety alignment tasks demonstrate that the proposed algorithms achieve superior performance and flexibility compared to existing methods, with theoretical guarantees of reward improvement.</div>
<div class="mono" style="margin-top:8px">本研究基于偏好对齐目标可作为分布散度估计器的见解，将散度框架扩展至通用大语言模型对齐，包括仅有环境奖励的场景。方法上提出了基于f散度变分表示的f-GRPO在线策略强化学习算法和f-HAL混合目标，用于优化模型对齐。在数学推理和安全对齐任务上的实验结果表明，所提算法相比现有方法具有更优的性能和灵活性，并理论保证了奖励提升。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: John Gardiner, Orlando Romero, Brendan Tivnan, Nicolò Dal Fabbro, George J. Pappas</div>
<div class="meta-line">First: 2026-02-09T18:01:40+00:00 · Latest: 2026-02-09T18:01:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中基于量子纠缠的协同学习</div>
<div class="mono" style="margin-top:8px">通信缺失是多智能体强化学习（MARL）协同面临的主要挑战。先前研究通过共享随机性（有时以关联设备形式）关联局部策略，以辅助分散决策。本研究首次提出训练MARL智能体利用共享量子纠缠作为协同资源的框架，其允许的免通信关联策略类别远超单纯共享随机性。该框架受量子物理经典结论启发：在特定无通信单轮协作博弈中，共享量子纠缠能实现优于纯随机共享的策略，即存在量子优势。我们提出可微分策略参数化方法以优化量子测量，并设计将联合策略分解为量子协调器与分散局部执行器的新型策略架构。为验证方法有效性，我们首先证明在作为黑盒预言机的单轮博弈中，能通过纯经验学习获得量子优势策略；进而展示在分散部分可观测马尔可夫决策过程（Dec-POMDP）构建的序贯决策问题中，该框架可学习具有量子优势的策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of coordination without communication in multi-agent reinforcement learning (MARL), this paper introduces a novel framework that enables agents to exploit shared quantum entanglement as a coordination resource, which offers a broader class of correlated policies than classical shared randomness alone. The method features a differentiable policy parameterization for optimizing quantum measurements and a policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. Experimentally, the approach successfully learns strategies that achieve quantum advantage in single-round cooperative games treated as black boxes and also demonstrates quantum advantage in a sequential decision-making problem modeled as a Dec-POMDP.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体强化学习中无通信条件下的协调难题，提出了一种新颖框架，使智能体能够利用共享量子纠缠作为协调资源，这比仅使用经典共享随机性提供了更广泛的关联策略类别。该方法采用了一种可微分的策略参数化来优化量子测量，并设计了一种将联合策略分解为量子协调器和分散局部执行器的策略架构。实验结果表明，该方法能在被视为黑盒的单轮合作游戏中学习到实现量子优势的策略，并在一个建模为分散部分可观测马尔可夫决策过程的序列决策问题中同样展示了量子优势。</div>
</details>
</div>
<div class="card">
<div class="title">Delay-Aware Reinforcement Learning for Highway On-Ramp Merging under Stochastic Communication Latency</div>
<div class="meta-line">Authors: Amin Tabrizian, Zhitong Huang, Arsyi Aziz, Peng Wei</div>
<div class="meta-line">First: 2024-03-18T15:02:46+00:00 · Latest: 2026-02-09T17:51:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.11852v4">Abs</a> · <a href="https://arxiv.org/pdf/2403.11852v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Delayed and partially observable state information poses significant challenges for reinforcement learning (RL)-based control in real-world autonomous driving. In highway on-ramp merging, a roadside unit (RSU) can sense nearby traffic, perform edge perception, and transmit state estimates to the ego vehicle over vehicle-to-infrastructure (V2I) links. With recent advancements in intelligent transportation infrastructure and edge computing, such RSU-assisted perception is increasingly realistic and already deployed in modern connected roadway systems. However, edge processing time and wireless transmission can introduce stochastic V2I communication delays, violating the Markov assumption and substantially degrading control performance. In this work, we propose DAROM, a Delay-Aware Reinforcement Learning framework for On-ramp Merging that is robust to stochastic delays. We model the problem as a random delay Markov decision process (RDMDP) and develop a unified RL agent for joint longitudinal and lateral control. To recover a Markovian representation under delayed observations, we introduce a Delay-Aware Encoder that conditions on delayed observations, masked action histories, and observed delay magnitude to infer the current latent state. We further integrate a physics-based safety controller to reduce collision risk during merging. Experiments in the Simulation of Urban MObility (SUMO) simulator using real-world traffic data from the Next Generation Simulation (NGSIM) dataset demonstrate that DAROM consistently outperforms standard RL baselines across traffic densities. In particular, the gated recurrent unit (GRU)-based encoder achieves over 99% success in high-density traffic with random V2I delays of up to 2.0 seconds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机通信延迟下高速公路匝道汇入的延迟感知强化学习</div>
<div class="mono" style="margin-top:8px">延迟且部分可观测的状态信息对基于强化学习（RL）的现实世界自动驾驶控制构成重大挑战。在高速公路匝道汇入场景中，路侧单元（RSU）可感知附近交通、执行边缘感知，并通过车路协同（V2I）链路将状态估计传输给主车。随着智能交通基础设施和边缘计算的发展，此类RSU辅助感知日益可行，并已部署于现代网联道路系统。然而，边缘处理时间和无线传输会引入随机V2I通信延迟，破坏马尔可夫假设并显著降低控制性能。本文提出DAROM——一种针对随机延迟鲁棒的匝道汇入延迟感知强化学习框架。我们将问题建模为随机延迟马尔可夫决策过程（RDMDP），并开发了用于纵向与横向联合控制的统一RL智能体。为在延迟观测下恢复马尔可夫表征，我们设计了延迟感知编码器，该编码器基于延迟观测、掩码动作历史及观测延迟幅度推断当前潜在状态。进一步集成基于物理的安全控制器以降低汇入过程中的碰撞风险。基于NGSIM真实交通数据在SUMO仿真平台中的实验表明，DAROM在不同交通密度下均稳定优于标准RL基线方法。特别地，采用门控循环单元（GRU）的编码器在高达2.0秒随机V2I延迟的高密度交通场景中实现了超过99%的成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of stochastic communication delays in vehicle-to-infrastructure (V2I) links for autonomous highway on-ramp merging, which breaks the Markov assumption and degrades reinforcement learning (RL) control. The authors propose DAROM, a delay-aware RL framework that models the problem as a random delay Markov decision process and employs a Delay-Aware Encoder to infer the current latent state from delayed observations, action histories, and observed delay magnitudes, alongside a safety controller for collision risk reduction. Experimental validation in the SUMO simulator with NGSIM traffic data shows that DAROM, particularly with a GRU-based encoder, outperforms standard RL baselines across traffic densities, achieving over 99% success in high-density traffic even with random delays up to 2.0 seconds.</div>
<div class="mono" style="margin-top:8px">本研究针对自动驾驶高速匝道汇入场景中车辆与基础设施（V2I）通信的随机延迟问题，该问题破坏了马尔可夫假设并降低了强化学习控制性能。作者提出了DAROM，一种延迟感知的强化学习框架，将问题建模为随机延迟马尔可夫决策过程，并采用延迟感知编码器，从延迟观测、动作历史和观测到的延迟幅度中推断当前潜在状态，同时结合基于物理的安全控制器以降低碰撞风险。在SUMO模拟器中使用NGSIM真实交通数据进行的实验表明，DAROM（尤其是基于门控循环单元的编码器）在不同交通密度下均优于标准强化学习基线，在高达2.0秒的随机延迟下，于高密度交通中实现了超过99%的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning</div>
<div class="meta-line">Authors: Ethan Rathbun, Wo Wei Lin, Alina Oprea, Christopher Amato</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-04T22:17:23+00:00 · Latest: 2026-02-09T17:46:50+00:00</div>
<div class="meta-line">Comments: 10 pages main body, ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05089v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05089v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger&#x27;&#x27;, leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent&#x27;s training pipeline, enabling them to both alter and observe agent&#x27;s rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze&#x27;&#x27; which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze&#x27;s effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>警惕不可信模拟器——强化学习中的无奖励后门攻击</div>
<div class="mono" style="margin-top:8px">模拟环境是强化学习（RL）成功的关键，使从业者和研究人员无需在真实硬件上进行昂贵实验即可训练决策智能体。然而，模拟器仍是一个安全盲区，恶意开发者可通过修改其发布的模拟器动态实现有害目的。为此，本文揭示了一种新型威胁，展示了如何利用模拟器动态向RL智能体隐蔽植入动作级后门。该后门使攻击者能在观察到预定义“触发器”时，可靠地激活智能体中的目标动作，可能导致危险后果。传统后门攻击受限于其强威胁模型，假设攻击者几乎完全控制智能体训练流程，既能修改也能观察智能体奖励。由于这些假设在模拟器中难以实现，本文提出新型攻击“Daze”，该攻击能在不修改甚至不观察奖励的情况下，可靠且隐蔽地向面向真实世界任务的RL智能体植入后门。我们通过形式化证明验证了Daze在通用RL任务中保证攻击成功的有效性，并在离散与连续动作空间领域进行了广泛实证评估。此外，我们首次展示了RL后门攻击可迁移至真实机器人硬件。这些进展推动了对RL训练流程全组件安全防护的进一步研究，以防范恶意攻击。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses a security vulnerability in reinforcement learning (RL) by introducing a novel backdoor attack that exploits untrusted simulators. The motivation stems from the fact that simulators, while essential for cost-effective RL training, are a security blind spot, allowing adversaries to maliciously alter environment dynamics. The proposed method, called Daze, implants action-level backdoors into RL agents without requiring control over or observation of the agent&#x27;s rewards, overcoming limitations of traditional attacks that assume strong adversary control. Experimental results demonstrate Daze&#x27;s effectiveness in both discrete and continuous action spaces, with formal proofs of success, and it is shown to transfer to real robotic hardware, highlighting the need for securing the entire RL training pipeline.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的安全漏洞，提出了一种利用不可信模拟器的新型后门攻击。其动机在于模拟器虽然是强化学习低成本训练的关键，但存在安全盲点，攻击者可能恶意修改环境动态。所提出的方法名为Daze，能够在无需控制或观察智能体奖励的情况下，向其植入动作级后门，克服了传统攻击需强对抗控制的局限。实验结果表明，Daze在离散和连续动作空间中均有效，并有形式化证明支持，且首次展示了此类攻击可迁移到真实机器人硬件，强调了保护整个强化学习训练流程的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors</div>
<div class="meta-line">Authors: Suraj Ranganath, Atharv Ramesh</div>
<div class="meta-line">First: 2026-02-09T17:33:46+00:00 · Latest: 2026-02-09T17:33:46+00:00</div>
<div class="meta-line">Comments: Expanded version of a workshop submission. Code available</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08934v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08934v1">PDF</a> · <a href="https://github.com/suraj-ranganath/StealthRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StealthRL：面向多检测器规避的强化学习改写攻击对抗AI文本检测器</div>
<div class="mono" style="margin-top:8px">AI文本检测器面临严峻的鲁棒性挑战：对抗性改写攻击能在保持语义的同时规避检测。我们提出StealthRL强化学习框架，在真实对抗条件下对检测器鲁棒性进行压力测试。该方法基于Qwen3-4B模型，采用带LoRA适配器的组相对策略优化（GRPO）训练针对多检测器集成系统的改写策略，通过平衡检测规避与语义保持的复合奖励函数进行优化。我们在安全相关的1%误报率工作点下，评估了针对三类检测器家族（RoBERTa、FastDetectGPT和Binoculars）的六种攻击设置（M0-M5）。StealthRL实现了接近零的检测率（TPR@1%FPR均值0.001），将平均AUROC从0.74降至0.27，攻击成功率高达99.9%。关键的是，攻击可迁移至训练中未见的检测器家族，揭示了共享架构漏洞而非特定检测器的脆弱性。我们还通过李克特量表进行基于LLM的质量评估，分析检测器分数分布以解释规避成功原因，并提供带自助法置信区间的各检测器AUROC。研究结果揭示了当前AI文本检测存在的重大鲁棒性缺陷，并将StealthRL确立为原则性的对抗评估协议。代码与评估流程已开源：https://github.com/suraj-ranganath/StealthRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the critical robustness challenge of AI-text detectors against adversarial paraphrasing attacks that preserve semantics, this paper introduces StealthRL, a reinforcement learning framework for stress-testing detectors under realistic adversarial conditions. The method employs a reinforcement learning approach, training a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on the Qwen3-4B model, optimizing a composite reward for both detector evasion and semantic preservation. The main experimental results show that StealthRL achieves near-zero detection with a 0.001 mean true positive rate at a 1% false positive rate, reduces the mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate across six attack settings against three detector families, with attacks successfully transferring to a held-out detector family, revealing shared architectural vulnerabilities.</div>
<div class="mono" style="margin-top:8px">本文的动机源于AI文本检测器面临的关键鲁棒性挑战，即对抗性改写在保持语义的同时逃避检测。方法上，该研究提出了StealthRL，一个基于强化学习的框架，用于在现实对抗条件下对检测器进行压力测试，该方法采用强化学习策略，使用基于Qwen3-4B模型的组相对策略优化（GRPO）与LoRA适配器，针对多检测器集成训练一个改写策略，并优化一个平衡检测器规避和语义保持的复合奖励。主要实验结果表明，StealthRL在针对三个检测器家族的六种攻击设置中，在安全相关的1%误报率操作点上实现了接近零的检测（平均真阳性率0.001），将平均AUROC从0.74降至0.27，并达到99.9%的攻击成功率，且攻击能成功迁移到训练中未见的检测器家族，揭示了共享的架构漏洞而非特定检测器的脆弱性。</div>
</details>
</div>
<div class="card">
<div class="title">Fast and robust parametric and functional learning with Hybrid Genetic Optimisation (HyGO)</div>
<div class="meta-line">Authors: Isaac Robledo, Yiqing Li, Guy Y. Cornejo Maceda, Rodrigo Castellanos</div>
<div class="meta-line">First: 2025-10-10T13:45:32+00:00 · Latest: 2026-02-09T17:22:16+00:00</div>
<div class="meta-line">Comments: 37 pages, 16 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09391v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09391v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Hybrid Genetic Optimisation framework (HYGO) is introduced to meet the pressing need for efficient and unified optimisation frameworks that support both parametric and functional learning in complex engineering problems. Evolutionary algorithms are widely employed as derivative-free global optimisation methods but often suffer from slow convergence rates, especially during late-stage learning. HYGO integrates the global exploration capabilities of evolutionary algorithms with accelerated local search for robust solution refinement. The key enabler is a two-stage strategy that balances exploration and exploitation. For parametric problems, HYGO alternates between genetic algorithm and targeted improvement through a degeneracy-proof Dowhill Simplex Method (DSM). For function optimisation tasks, HYGO rotates between genetic programming and DSM. Validation is performed on (a) parametric optimisation benchmarks, where HYGO demonstrates faster and more robust convergence than standard genetic algorithms, and (b) function optimisation tasks, including control of a damped Landau oscillator. Practical relevance is showcased through aerodynamic drag reduction of an Ahmed body via Reynolds-Averaged Navier-Stokes simulations, achieving consistently interpretable results and reductions exceeding 20% by controlled jet injection in the back of the body for flow reattachment and separation bubble reduction. Overall, HYGO emerges as a versatile hybrid optimisation framework suitable for a broad spectrum of engineering and scientific problems involving parametric and functional learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于混合遗传优化（HyGO）的快速鲁棒参数与函数学习</div>
<div class="mono" style="margin-top:8px">为满足复杂工程问题中对支持参数与函数学习的高效统一优化框架的迫切需求，本文提出混合遗传优化框架（HYGO）。进化算法作为无导数全局优化方法被广泛采用，但常存在收敛速度慢的问题，尤其在后期学习阶段。HYGO将进化算法的全局探索能力与加速局部搜索相结合，实现鲁棒的解优化。其核心是平衡探索与利用的两阶段策略：针对参数优化问题，HYGO在遗传算法与抗退化下山单纯形法（DSM）之间交替执行；针对函数优化任务，则在遗传编程与DSM之间轮转。验证工作包括：（a）参数优化基准测试，显示HYGO比标准遗传算法收敛更快更鲁棒；（b）函数优化任务，包括阻尼朗道振荡器控制。通过雷诺平均纳维-斯托克斯模拟对Ahmed车身进行气动减阻的案例，展示了其实用价值：通过在车尾受控喷射实现流动再附着与分离泡减小，获得可解释性结果及超过20%的减阻效果。总体而言，HYGO成为适用于涉及参数与函数学习的广泛工程与科学问题的通用混合优化框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient and unified optimization frameworks for complex engineering problems involving both parametric and functional learning, this paper introduces the Hybrid Genetic Optimization (HyGO) framework. The method integrates the global exploration of evolutionary algorithms with accelerated local search, employing a two-stage strategy that alternates between a genetic algorithm (or genetic programming for functional tasks) and a degeneracy-proof Downhill Simplex Method for robust refinement. Experimental results demonstrate that HyGO achieves faster and more robust convergence than standard genetic algorithms on parametric benchmarks and function optimization tasks, such as controlling a damped Landau oscillator. Its practical relevance is further validated through an aerodynamic drag reduction application for an Ahmed body using RANS simulations, where controlled jet injection achieved consistent results and drag reductions exceeding 20% by promoting flow reattachment and reducing separation bubbles.</div>
<div class="mono" style="margin-top:8px">本文针对复杂工程问题中参数与函数学习对高效统一优化框架的迫切需求，提出了混合遗传优化（HyGO）框架。该方法将进化算法的全局探索能力与加速局部搜索相结合，采用两阶段策略，在参数优化中交替使用遗传算法和抗退化的下山单纯形法进行针对性改进，在函数优化中则轮换遗传规划和下山单纯形法。实验结果表明，在参数优化基准测试和函数优化任务（如阻尼朗道振荡器控制）中，HyGO比标准遗传算法收敛更快、更稳健。其实际应用价值通过Ahmed车身的气动减阻案例得到验证，该案例基于雷诺平均Navier-Stokes模拟，通过车身尾部受控射流促进流动再附着并减少分离泡，实现了结果一致且超过20%的减阻效果。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Stable Reinforcement Learning for Diffusion Language Models</div>
<div class="meta-line">Authors: Jiawei Liu, Xiting Wang, Yuanyuan Zhong, Defu Lian, Yu Yang</div>
<div class="meta-line">First: 2026-02-09T17:04:23+00:00 · Latest: 2026-02-09T17:04:23+00:00</div>
<div class="meta-line">Comments: 13 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08905v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08905v1">PDF</a> · <a href="https://github.com/Lolo1222/STP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散语言模型的高效稳定强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）对于释放基于扩散的大语言模型（dLLMs）的复杂推理能力至关重要。然而，将RL应用于dLLMs在效率和稳定性方面面临独特挑战。为解决这些挑战，我们提出了时空剪枝（STP）框架，旨在同时提升dLLMs中RL的效率和稳定性。STP通过以下方式压缩生成过程中的冗余：（1）空间剪枝，利用静态先验约束探索空间；（2）时间剪枝，跳过冗余的后期细化步骤。理论分析表明，STP严格降低了对数似然估计的方差，从而确保更稳定的策略更新。大量实验证明，STP在效率和准确性上均优于现有先进基线。代码发布于https://github.com/Lolo1222/STP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the efficiency and stability challenges of applying reinforcement learning to diffusion-based large language models, which are crucial for enhancing their reasoning capabilities. The authors propose Spatio-Temporal Pruning (STP), a framework that improves RL efficiency by compressing redundancy through spatial pruning with static priors to constrain exploration space and temporal pruning to bypass late-stage refinement steps. Theoretically, STP reduces variance in log-likelihood estimation for stable policy updates, and experiments show it outperforms state-of-the-art baselines in both efficiency and accuracy.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在扩散大语言模型应用中存在的效率与稳定性挑战，旨在提升模型的推理能力。作者提出了时空剪枝框架，通过空间剪枝利用静态先验约束探索空间，以及时间剪枝跳过后期冗余细化步骤，从而压缩生成过程中的冗余。理论分析表明该框架能严格降低对数似然估计方差以实现稳定策略更新，大量实验证明其在效率和准确性上均优于现有先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Yuchen Yan, Liang Jiang, Jin Jiang, Shuaicheng Li, Zujie Wen, Zhiqiang Zhang, Jun Zhou, Jian Shao, Yueting Zhuang, Yongliang Shen</div>
<div class="meta-line">First: 2026-02-06T18:59:27+00:00 · Latest: 2026-02-09T17:01:31+00:00</div>
<div class="meta-line">Comments: Project Page: https://zju-real.github.io/InftyThink-Plus Code: https://github.com/ZJU-REAL/InftyThink-Plus</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06960v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06960v2">PDF</a> · <a href="https://github.com/ZJU-REAL/InftyThink-Plus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zju-real.github.io/InftyThink-Plus">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InftyThink+：基于强化学习的有效高效无限时域推理方法</div>
<div class="mono" style="margin-top:8px">大型推理模型通过扩展推理时思维链获得优异性能，但该范式存在二次成本、上下文长度限制及因‘中间迷失效应’导致的推理质量下降等问题。迭代推理通过定期总结中间思路缓解这些问题，但现有方法依赖监督学习或固定启发式策略，未能优化何时总结、保留何种信息以及如何恢复推理。我们提出InftyThink+——一种端到端的强化学习框架，基于模型控制的迭代边界和显式总结机制，优化整个迭代推理轨迹。该框架采用两阶段训练方案：监督式冷启动后接轨迹级强化学习，使模型能学习策略性总结与续推决策。在DeepSeek-R1-Distill-Qwen-1.5B上的实验表明，InftyThink+在AIME24上准确率提升21%，显著优于传统长思维链强化学习方法，且在分布外基准测试中泛化能力更强。同时，InftyThink+大幅降低推理延迟并加速强化学习训练，在提升性能的同时显著提高推理效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for InftyThink+ stems from the limitations of scaling chain-of-thought reasoning, which incurs quadratic costs, context constraints, and reasoning degradation due to lost-in-the-middle effects. The method introduces an end-to-end reinforcement learning framework that optimizes iterative reasoning trajectories by learning when to summarize, what to preserve, and how to resume, using a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning. Experimental results on DeepSeek-R1-Distill-Qwen-1.5B show a 21% accuracy improvement on AIME24, outperforming conventional long chain-of-thought reinforcement learning, with better generalization to out-of-distribution benchmarks, reduced inference latency, and accelerated training, demonstrating enhanced efficiency and performance.</div>
<div class="mono" style="margin-top:8px">InftyThink+的动机源于扩展思维链推理的局限性，包括二次成本、上下文限制以及因中间信息丢失导致的推理退化。该方法提出了一种端到端的强化学习框架，通过监督冷启动和轨迹级强化学习的两阶段训练方案，优化迭代推理轨迹，学习何时总结、保留什么以及如何恢复推理。在DeepSeek-R1-Distill-Qwen-1.5B上的实验结果显示，在AIME24上准确率提升21%，优于传统的长思维链强化学习方法，对分布外基准的泛化能力更强，同时降低了推理延迟并加速了训练，实现了效率与性能的双重提升。</div>
</details>
</div>
<div class="card">
<div class="title">AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection</div>
<div class="meta-line">Authors: Junru Zhang, Lang Feng, Haoran Shi, Xu Guo, Han Yu, Yabo Dong, Duanqing Xu</div>
<div class="meta-line">First: 2026-02-09T16:30:13+00:00 · Latest: 2026-02-09T16:30:13+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnomSeer：增强多模态大语言模型进行时间序列异常检测推理</div>
<div class="mono" style="margin-top:8px">基于多模态大语言模型的时间序列异常检测是一个新兴领域，但存在持续挑战：MLLMs依赖粗略的时间序列启发式方法，却难以进行对理解复杂时间序列数据至关重要的多维度、精细化推理。我们提出AnomSeer，通过增强模型使其推理基于时间序列精确的结构化细节，统一异常分类、定位与解释。其核心是生成专家思维链轨迹，从经典分析（如统计度量、频域变换）提供可验证的细粒度推理。在此基础上，我们提出一种新颖的时间序列基础策略优化方法，在标准强化学习之外引入两个组件：基于最优传输的时间序列基础优势度量和正交投影，确保辅助的细粒度信号不干扰主要检测目标。在多种异常场景下，基于Qwen2.5-VL-3B/7B-Instruct的AnomSeer在分类与定位准确率上超越更大的商业基线模型（如GPT-4o），尤其在点异常和频域驱动异常上表现突出，并能生成支持其结论的合理时间序列推理轨迹。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitation of multimodal large language models (MLLMs) in performing detailed, multi-dimensional reasoning for time-series anomaly detection (TSAD), which is crucial for handling complex data. The method introduces AnomSeer, which reinforces MLLMs by grounding reasoning in precise structural details through an expert chain-of-thought trace from classical analyses and a novel time-series grounded policy optimization (TimerPO) that includes an optimal transport-based advantage and orthogonal projection to avoid interference with detection objectives. Experimental results show that AnomSeer, using Qwen2.5-VL-3B/7B-Instruct models, outperforms larger commercial baselines like GPT-4o in classification and localization accuracy, especially for point- and frequency-driven anomalies, while generating verifiable reasoning traces.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多模态大语言模型在时间序列异常检测中难以进行详细、多维推理的局限性，这对于处理复杂数据至关重要。方法上提出了AnomSeer，通过基于经典分析生成专家思维链轨迹，将推理建立在精确结构细节中，并采用一种新颖的时间序列接地策略优化，结合基于最优传输的优势和正交投影，以避免干扰主要检测目标。实验结果表明，使用Qwen2.5-VL-3B/7B-Instruct模型的AnomSeer在分类和定位准确性上优于GPT-4o等大型商业基线，尤其在点和频率驱动的异常检测中表现突出，同时能生成可验证的推理轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems</div>
<div class="meta-line">Authors: Lang Feng, Longtao Zheng, Shuo He, Fuxiang Zhang, Bo An</div>
<div class="meta-line">First: 2026-02-09T16:13:39+00:00 · Latest: 2026-02-09T16:13:39+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08847v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08847v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents&#x27; reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent&#x27;s own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dr. MAS：面向多智能体大语言模型系统的稳定强化学习</div>
<div class="mono" style="margin-top:8px">多智能体大语言模型系统通过角色分工实现高级推理与工具调用，但针对此类系统的可靠强化学习后训练仍具挑战。本文从理论上指出，将基于群体的强化学习扩展至多智能体大语言模型系统时，训练不稳定的关键原因在于：在GRPO风格优化下，全局归一化基线可能偏离各智能体差异化的奖励分布，最终导致梯度范数失稳。基于此发现，我们提出Dr. MAS——一种简洁稳定的多智能体大语言模型系统强化学习训练方案。Dr. MAS采用智能体级修正策略：利用各智能体自身的奖励统计量分别归一化其优势函数，从而校准梯度尺度，在理论与实证层面显著提升训练稳定性。除算法外，Dr. MAS构建了端到端的多智能体大语言模型强化学习训练框架，支持可扩展的任务编排、灵活的智能体级大语言模型服务与优化配置，以及大语言模型执行器后端的共享资源调度。基于Qwen2.5与Qwen3系列模型，我们在多智能体数学推理与多轮搜索基准测试中评估Dr. MAS。相较于原始GRPO方法，Dr. MAS在数学任务上实现平均指标提升5.6%、通过率提升4.6%，在搜索任务上平均指标提升15.2%、通过率提升13.1%，同时基本消除梯度尖峰现象。此外，该方法在异构智能体-模型分配场景下仍保持高效性，并进一步提升系统效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability of reinforcement learning (RL) in multi-agent large language model (LLM) systems, where traditional group-based RL methods like GRPO suffer from gradient-norm instability due to a global normalization baseline misaligned with diverse agent reward distributions. To resolve this, the authors propose Dr. MAS, a stable RL training recipe that employs agent-wise advantage normalization using each agent&#x27;s own reward statistics, thereby calibrating gradient scales and enhancing training stability. Experimental results on multi-agent math reasoning and multi-turn search benchmarks with Qwen models show that Dr. MAS outperforms vanilla GRPO, achieving improvements such as +5.6% avg@16 and +4.6% pass@16 on math tasks, while effectively eliminating gradient spikes and maintaining efficiency under heterogeneous agent-model assignments.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体大语言模型系统中强化学习训练不稳定的问题展开研究，指出传统基于群体的强化学习方法如GRPO因全局归一化基线无法适应不同智能体的奖励分布，导致梯度范数不稳定。为此，作者提出了Dr. MAS这一稳定的强化学习训练方案，通过基于每个智能体自身奖励统计的智能体级优势归一化来校准梯度尺度，从而显著提升训练稳定性。在基于Qwen系列模型的多智能体数学推理和多轮搜索基准测试中，Dr. MAS相比原始GRPO方法取得了明显优势，例如在数学任务上平均提升5.6%的avg@16和4.6%的pass@16，同时基本消除了梯度尖峰，并在异构智能体模型分配下保持高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning</div>
<div class="meta-line">Authors: Andrés Holgado-Sánchez, Peter Vamplew, Richard Dazeley, Sascha Ossowski, Holger Billhardt</div>
<div class="meta-line">First: 2026-02-09T16:06:36+00:00 · Latest: 2026-02-09T16:06:36+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures. To be published in proceedings of the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). This is a full version that includes the supplementary material</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08835v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏好多目标强化学习的社会价值体系学习</div>
<div class="mono" style="margin-top:8px">价值感知人工智能应能识别人类价值观并适应不同用户的价值体系（基于价值的偏好）。这需要对价值观进行可操作化处理，但可能存在设定偏差。价值观的社会性要求其表征需兼顾多用户需求，而价值体系虽具多样性，却在群体间呈现规律性模式。在序列决策中，已有研究尝试通过多样化智能体的示范实现针对不同目标或价值观的个性化适配，但这些方法或需人工设计特征，或缺乏基于价值的可解释性及对多元用户偏好的适应能力。
我们提出基于聚类和偏好多目标强化学习的算法，用于在马尔可夫决策过程中学习智能体社会的价值对齐模型与价值体系。该方法联合学习社会衍生的价值对齐模型（基础表征）以及一组能简洁表征社会中不同用户群体（聚类）的价值体系。每个聚类包含代表其成员价值偏好的价值体系，以及反映符合该价值体系行为的近似帕累托最优策略。我们在两个含人类价值的MDP环境中，将本方法与前沿的PbMORL算法及基线模型进行了对比评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for value-aware AI that can adapt to diverse human value systems without manual feature design, this paper proposes a method for learning societal value models using preference-based multi-objective reinforcement learning (PbMORL) and clustering in Markov Decision Processes. The approach jointly learns value alignment models and clusters representing distinct groups, each with a value system and a corresponding Pareto-optimal policy. Experimental results on two MDPs with human values show that the method outperforms a state-of-the-art PbMORL algorithm and baselines in capturing and adapting to societal value patterns.</div>
<div class="mono" style="margin-top:8px">为使价值感知人工智能能适应不同人类价值体系并避免手动特征设计的局限性，本文提出了一种基于偏好多目标强化学习和聚类的方法，用于在马尔可夫决策过程中学习社会价值模型。该方法联合学习价值对齐模型以及代表不同群体的聚类，每个聚类包含一个价值体系和相应的近似帕累托最优策略。在两个人性化价值的MDP实验结果表明，该方法在捕捉和适应社会价值模式方面优于现有的先进偏好多目标强化学习算法及基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Preference Learning for Test-Time Steerable Reward Models</div>
<div class="meta-line">Authors: Jiwoo Hong, Shao Tang, Zhipeng Wang</div>
<div class="meta-line">First: 2026-02-09T15:55:56+00:00 · Latest: 2026-02-09T15:55:56+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08819v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向测试时可调控奖励模型的贝叶斯偏好学习</div>
<div class="mono" style="margin-top:8px">奖励模型通过强化学习（RL）在语言模型与人类偏好对齐中发挥核心作用。随着RL在可验证奖励和多目标对齐等场景中的应用日益广泛，奖励模型需编码更复杂、多维的偏好分布。然而，分类器式奖励模型一经训练即保持静态，限制了其在测试时的适应性。我们提出变分上下文奖励建模（ICRM），这是一种新颖的贝叶斯奖励建模目标，通过上下文偏好示例实现测试时可调控性。ICRM基于Bradley-Terry模型，采用共轭Beta先验，将奖励建模转化为对潜在偏好概率的摊销变分推断。实验表明，ICRM在单目标和多目标场景下均能适应测试时未见过的偏好分布：在单目标设置中，随着上下文示例增加，ICRM在SafeRLHF上准确率提升34%，在RM-Bench上提升9%；在助益性与拒绝性基准测试中，帕累托前沿扩展了4%的超体积增益。我们进一步探究ICRM在RL训练中的实际应用，证明其通过数学推理任务超越传统奖励模型，能有效编码可验证奖励。最后，我们提供理论保证：该变分目标存在具有有限置信度的全局内部最优解，并分析了KL正则化如何缓解奖励过优化问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of static reward models (RMs) in adapting to complex, multifaceted human preferences at test time, particularly for applications like verifiable rewards and multi-objective alignment. It introduces Variational In-Context Reward Modeling (ICRM), a Bayesian method that frames reward modeling as amortized variational inference with a Beta prior under the Bradley-Terry model, enabling steerability via in-context preference demonstrations. Experimental results show that ICRM adapts effectively to unseen preferences, achieving a 34% accuracy gain on SafeRLHF and 9% on RM-Bench in single-objective settings, while improving multi-objective performance with a 4% hypervolume gain on helpfulness and refusal benchmarks; it also outperforms conventional RMs in math reasoning for RL training, supported by theoretical guarantees on optimization and over-optimization mitigation.</div>
<div class="mono" style="margin-top:8px">该论文针对静态奖励模型在测试时难以适应复杂、多方面人类偏好的局限性，特别是在可验证奖励和多目标对齐等应用中，提出了变分上下文奖励建模（ICRM）这一贝叶斯方法。该方法将奖励建模构建为基于Bradley-Terry模型和Beta共轭先验的摊销变分推断，通过上下文偏好演示实现可操控性。实验结果表明，ICRM能有效适应未见偏好，在单目标设置中于SafeRLHF上准确率提升34%，在RM-Bench上提升9%，同时在多目标设置中于帮助性和拒绝基准上帕累托前沿扩展，超体积增益达4%；在数学推理的强化学习训练中优于传统奖励模型，并得到关于优化全局内点解和缓解奖励过优化的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">Twice Sequential Monte Carlo for Tree Search</div>
<div class="meta-line">Authors: Yaniv Oren, Joery A. de Vries, Pascal R. van der Vaart, Matthijs T. J. Spaan, Wendelin Böhmer</div>
<div class="meta-line">First: 2025-11-18T07:54:29+00:00 · Latest: 2026-02-09T15:18:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14220v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.14220v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS as a policy improvement operator, scales favorably with sequential compute, reduces estimator variance and mitigates the effects of path degeneracy while retaining the properties that make SMC natural to parallelize.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双重序贯蒙特卡洛树搜索</div>
<div class="mono" style="margin-top:8px">基于模型的强化学习方法结合搜索技术，已在强化学习领域取得多项里程碑式突破。序贯蒙特卡洛方法近期作为推动这些突破的蒙特卡洛树搜索算法的替代方案出现，其更易于并行化且更适合GPU加速。然而，该方法也存在方差较大和路径退化问题，导致其难以随搜索深度（即序列计算量）增加而有效扩展。为解决这些问题，我们提出了双重序贯蒙特卡洛树搜索方法。在离散与连续环境中，TSMCTS作为策略改进算子，其性能优于SMC基线及当前主流MCTS变体，能够随序列计算量增加实现良好扩展，有效降低估计方差并缓解路径退化效应，同时保持了SMC固有的并行化优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve upon Sequential Monte Carlo (SMC) methods in model-based reinforcement learning, which suffer from high variance and path degeneracy when scaling to deeper searches, this paper introduces Twice Sequential Monte Carlo Tree Search (TSMCTS). The method enhances SMC by applying a double sampling technique to reduce variance and mitigate path degeneracy while maintaining the parallelizability advantages of SMC over Monte Carlo Tree Search (MCTS). Experimental results across discrete and continuous environments demonstrate that TSMCTS outperforms both SMC baselines and a modern MCTS variant, scaling effectively with increased sequential computation and delivering improved performance as a policy improvement operator.</div>
<div class="mono" style="margin-top:8px">本文的动机是改进基于模型的强化学习中的序列蒙特卡洛方法，该方法在搜索深度增加时存在高方差和路径退化问题，为此提出了双重序列蒙特卡洛树搜索。该方法通过双重采样技术来降低方差并缓解路径退化，同时保持了序列蒙特卡洛相对于蒙特卡洛树搜索的并行化优势。在离散和连续环境中的实验结果表明，TSMCTS在作为策略改进算子时，其性能优于序列蒙特卡洛基线和现代蒙特卡洛树搜索变体，能够随着序列计算量的增加而有效扩展，并展现出更好的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization</div>
<div class="meta-line">Authors: Nicolás Villagrán Prieto, Eduardo C. Garrido-Merchán</div>
<div class="meta-line">First: 2026-02-09T15:15:52+00:00 · Latest: 2026-02-09T15:15:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08774v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08774v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>默认机器学习超参数无法为贝叶斯优化提供信息性初始化</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）因其在昂贵黑盒函数上的样本效率，成为超参数调优的标准工具。尽管多数BO流程以均匀随机初始化开始，但主流机器学习库（如scikit-learn）内置的默认超参数值蕴含了隐式专家知识，理论上可作为加速收敛的信息化起点。这一假设虽具直观吸引力，却长期缺乏实证检验。本研究通过以截断高斯分布（以库默认值为中心）采样点初始化BO，并与均匀随机基线对比轨迹，系统验证该假设。实验涵盖三种BO后端（BoTorch、Optuna、Scikit-Optimize）、三类模型（随机森林、支持向量机、多层感知器）及五个涵盖分类与回归任务的基准数据集。通过收敛速度与最终预测质量评估性能，并采用单侧二项检验判定统计显著性。在所有实验条件下，基于默认值的初始化均未展现出显著优于纯随机采样的优势（p值范围0.141-0.908）。先验方差敏感性分析表明，尽管更紧密围绕默认值的采样能提升早期评估效果，但该短暂优势随优化进程消失，最终性能无差异。结果证明默认超参数未包含对优化有效的方向性信息。因此建议实践者将超参数调优视为模型开发的核心环节，优先采用基于数据的原理性搜索策略，而非依赖启发式的库默认值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether default hyperparameters from machine learning libraries, which encode expert knowledge, can serve as informative starting points to accelerate Bayesian Optimization (BO) for hyperparameter tuning. The authors formalize this by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare it against uniform random initialization across three BO back-ends, three model families, and five benchmark datasets. The experimental results show no statistically significant advantage for default-informed initialization in convergence speed or final predictive quality, with p-values ranging from 0.141 to 0.908, indicating that default hyperparameters do not provide useful directional information for optimization.</div>
<div class="mono" style="margin-top:8px">本文探讨了机器学习库中蕴含专家知识的默认超参数是否可以作为贝叶斯优化（BO）超参数调优的有利起点以加速收敛。研究者通过从以库默认值为中心的截断高斯分布中采样来初始化BO，并与均匀随机初始化进行比较，实验覆盖了三种BO后端、三种模型族和五个基准数据集。结果表明，基于默认值的初始化在收敛速度或最终预测性能上均未显示出统计学显著优势，p值范围在0.141至0.908之间，这表明默认超参数并未为优化提供有效的方向性信息。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data</div>
<div class="meta-line">Authors: Zhihao Zhang, Keith Redmill, Chengyang Peng, Bowen Weng</div>
<div class="meta-line">First: 2026-01-29T19:09:28+00:00 · Latest: 2026-02-09T15:05:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22242v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22242v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A driving algorithm that aligns with good human driving practices, or at the very least collaborates effectively with human drivers, is crucial for developing safe and efficient autonomous vehicles. In practice, two main approaches are commonly adopted: (i) supervised or imitation learning, which requires comprehensive naturalistic driving data capturing all states that influence a vehicle&#x27;s decisions and corresponding actions, and (ii) reinforcement learning (RL), where the simulated driving environment either matches or is intentionally more challenging than real-world conditions. Both methods depend on high-quality observations of real-world driving behavior, which are often difficult and costly to obtain. State-of-the-art sensors on individual vehicles can gather microscopic data, but they lack context about the surrounding conditions. Conversely, roadside sensors can capture traffic flow and other macroscopic characteristics, but they cannot associate this information with individual vehicles on a microscopic level. Motivated by this complementarity, we propose a framework that reconstructs unobserved microscopic states from macroscopic observations, using microscopic data to anchor observed vehicle behaviors, and learns a shared policy whose behavior is microscopically consistent with the partially observed trajectories and actions and macroscopically aligned with target traffic statistics when deployed population-wide. Such constrained and regularized policies promote realistic flow patterns and safe coordination with human drivers at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐微观车辆与宏观交通统计：基于部分数据重建驾驶行为</div>
<div class="mono" style="margin-top:8px">开发与优秀人类驾驶行为对齐、或至少能与人类驾驶员有效协作的驾驶算法，对构建安全高效自动驾驶系统至关重要。实践中主要采用两种方法：(i) 监督或模仿学习，需要能捕捉所有影响车辆决策状态及对应行为的完整自然驾驶数据；(ii) 强化学习，其模拟驾驶环境需匹配或刻意超越真实世界复杂度。两种方法均依赖高质量的真实驾驶行为观测数据，而这类数据往往难以获取且成本高昂。单车搭载的先进传感器可采集微观数据，但缺乏对周边环境的整体感知；路侧传感器虽能捕捉车流等宏观特征，却无法在微观层面关联具体车辆。基于这种互补性，我们提出一种从宏观观测重建未观测微观状态的框架：利用微观数据锚定观测到的车辆行为，学习一种共享策略——该策略在微观层面与部分观测到的轨迹行为保持一致，在宏观层面与全域部署时的目标交通统计特征对齐。这种经过约束与正则化的策略，能够促进大规模应用时形成符合现实的交通流模式，并实现与人类驾驶员的安全协同。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complementary yet incomplete nature of microscopic vehicle data and macroscopic traffic statistics, this paper proposes a framework to reconstruct unobserved microscopic driving states from macroscopic observations. The method anchors observed vehicle behaviors using available microscopic data to learn a shared policy that is both consistent with partially observed trajectories and, when deployed across a population, aligns with target macroscopic traffic statistics. Experimental results demonstrate that this constrained and regularized approach promotes realistic traffic flow patterns and safe, large-scale coordination with human drivers.</div>
<div class="mono" style="margin-top:8px">受微观车辆数据与宏观交通统计量互补但不完整的特性启发，本文提出一个从宏观观测重建未观测微观驾驶状态的框架。该方法利用可得的微观数据锚定观测到的车辆行为，学习一个共享策略，该策略既与部分观测到的轨迹保持一致，又在群体部署时与目标宏观交通统计量对齐。实验结果表明，这种约束和正则化的方法能够促进真实的交通流模式，并实现与人类驾驶员安全的大规模协同。</div>
</details>
</div>
<div class="card">
<div class="title">Spatiotemporal Attention-Augmented Inverse Reinforcement Learning for Multi-Agent Task Allocation</div>
<div class="meta-line">Authors: Huilin Yin, Zhikun Yang, Linchuan Zhang, Daniel Watzenig</div>
<div class="meta-line">First: 2025-04-07T13:14:45+00:00 · Latest: 2026-02-09T15:01:05+00:00</div>
<div class="meta-line">Comments: Revised version with substantial new experimental results, improved analysis, and a restructured layout for better clarity</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.05045v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.05045v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial inverse reinforcement learning (IRL) for multi-agent task allocation (MATA) is challenged by non-stationary interactions and high-dimensional coordination. Unconstrained reward inference in these settings often leads to high variance and poor generalization. We propose an attention-structured adversarial IRL framework that constrains reward inference via spatiotemporal representation learning. Our method employs multi-head self-attention (MHSA) for long-range temporal dependencies and graph attention networks (GAT) for agent-task relational structures. We formulate reward inference as a low-capacity, adaptive linear transformation of the environment reward, ensuring stable and interpretable guidance. This framework decouples reward inference from policy learning and optimizes the reward model adversarially. Experiments on benchmark MATA scenarios show that our approach outperforms representative MARL baselines in convergence speed, cumulative rewards, and spatial efficiency. Results demonstrate that attention-guided, capacity-constrained reward inference is a scalable and effective mechanism for stabilizing adversarial IRL in complex multi-agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多智能体任务分配的时空注意力增强逆强化学习</div>
<div class="mono" style="margin-top:8px">多智能体任务分配（MATA）中的对抗性逆强化学习（IRL）面临非平稳交互与高维协调的挑战。此类场景中无约束的奖励推断常导致高方差与泛化能力不足。我们提出一种注意力结构化的对抗性IRL框架，通过时空表征学习约束奖励推断。该方法采用多头自注意力（MHSA）捕捉长程时间依赖，并利用图注意力网络（GAT）建模智能体-任务关系结构。我们将奖励推断形式化为环境奖励的低容量自适应线性变换，确保稳定且可解释的引导。该框架将奖励推断与策略学习解耦，并以对抗方式优化奖励模型。在标准MATA场景上的实验表明，本方法在收敛速度、累积奖励与空间效率方面均优于代表性多智能体强化学习基线。结果证明，注意力引导的容量约束奖励推断是稳定复杂多智能体系统中对抗性IRL的可扩展有效机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenges of high variance and poor generalization in adversarial inverse reinforcement learning (IRL) for multi-agent task allocation (MATA), which arise from non-stationary interactions and high-dimensional coordination. To tackle this, the authors propose an attention-structured adversarial IRL framework that constrains reward inference through spatiotemporal representation learning, using multi-head self-attention for temporal dependencies and graph attention networks for relational structures. The method formulates reward inference as a low-capacity, adaptive linear transformation of the environment reward to ensure stability and interpretability, decoupling it from policy learning and optimizing it adversarially. Experimental results on benchmark MATA scenarios demonstrate that this approach outperforms representative multi-agent reinforcement learning baselines in convergence speed, cumulative rewards, and spatial efficiency, showing that attention-guided, capacity-constrained reward inference effectively stabilizes adversarial IRL in complex multi-agent systems.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体任务分配中对抗性逆强化学习面临的高方差和泛化能力差的问题，这些问题源于非平稳交互和高维协调。为此，作者提出了一种注意力结构的对抗性逆强化学习框架，通过时空表示学习来约束奖励推断，利用多头自注意力处理长期时间依赖，并使用图注意力网络建模智能体-任务关系结构。该方法将奖励推断形式化为环境奖励的低容量自适应线性变换，以确保稳定性和可解释性，将其与策略学习解耦并进行对抗性优化。在基准多智能体任务分配场景上的实验结果表明，该方法在收敛速度、累积奖励和空间效率方面优于代表性的多智能体强化学习基线，证明了注意力引导、容量约束的奖励推断是稳定复杂多智能体系统中对抗性逆强化学习的可扩展且有效的机制。</div>
</details>
</div>
<div class="card">
<div class="title">Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse</div>
<div class="meta-line">Authors: Shaojie Wang, Jinghui Wang, Yinghan Cui, Xuxing Chen, Chao Wang, Liang Huang, Xiaojiang Zhang, Junyi Peng, Li Wan, Haotian Zhang, Bin Chen</div>
<div class="meta-line">First: 2025-11-01T05:56:49+00:00 · Latest: 2026-02-09T14:55:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00413v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00413v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic large language model (LLM) training often involves multi-turn interaction trajectories that branch into multiple execution paths due to concurrent tool use, think-mode, sub-agent, context management and other runtime designs. As a result, the token produced by a single task naturally forms a tree-structured token trajectory with shared prefixes, rather than a linear sequence. Existing training pipelines linearize such trajectories and treat each branch independently, leading to substantial redundant computation in both forward and backward passes. To eliminate such redundancy, we introduce Tree Training, an efficient training framework for tree-structured trajectories. Its core component, Gradient Restoration, enables correct gradient aggregation across shared prefixes, allowing each prefix to be computed exactly once while remaining mathematically equivalent to independent training on all branches. To support large trajectory trees in practice, we redesign the training engine to natively ingest tree-structured data and propose Tree Packing, a memory-efficient partitioning strategy that preserves high prefix reuse. Experiments conducted on dense and MOE models of real-world agentic trajectories show 6.2x training speedup for both supervised fine-tuning and the model update phase in reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>树形训练：通过共享前缀复用加速智能体大语言模型训练</div>
<div class="mono" style="margin-top:8px">智能体大语言模型（LLM）训练常涉及多轮交互轨迹，由于并发工具调用、思考模式、子智能体、上下文管理等运行时设计，这些轨迹会分叉为多条执行路径。因此，单个任务生成的令牌自然形成具有共享前缀的树形令牌轨迹，而非线性序列。现有训练流程将此类轨迹线性化并独立处理每个分支，导致前向与反向传播中存在大量冗余计算。为消除冗余，我们提出树形训练——针对树形轨迹的高效训练框架。其核心组件梯度复原技术，能实现跨共享前缀的正确梯度聚合，使每个前缀仅需计算一次，同时在数学上等效于所有分支的独立训练。为支持实际应用中的大型轨迹树，我们重新设计训练引擎以原生处理树形数据，并提出树形打包——一种保持高前缀复用率的内存高效分区策略。在真实世界智能体轨迹的稠密模型与混合专家模型上的实验表明，该方法在监督微调和强化学习的模型更新阶段均实现6.2倍训练加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the inefficiency of existing training pipelines for agentic large language models, which linearize tree-structured token trajectories from multi-turn interactions, causing redundant computations. The method introduces Tree Training, a framework featuring Gradient Restoration to correctly aggregate gradients across shared prefixes, ensuring each prefix is computed only once while maintaining mathematical equivalence to independent branch training. Experimental results on dense and mixture-of-experts models using real-world agentic trajectories demonstrate a 6.2x training speedup for both supervised fine-tuning and reinforcement learning model updates.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有智能体大语言模型训练流程效率低下，这些流程将多轮交互产生的树状令牌轨迹线性化处理，导致大量冗余计算。方法上提出了树训练框架，其核心组件梯度恢复技术能在共享前缀上正确聚合梯度，确保每个前缀仅计算一次，同时数学上等效于独立分支训练。在密集和混合专家模型上使用真实世界智能体轨迹的实验结果表明，监督微调和强化学习模型更新阶段的训练速度提升了6.2倍。</div>
</details>
</div>
<div class="card">
<div class="title">A Review of Online Diffusion Policy RL Algorithms for Scalable Robotic Control</div>
<div class="meta-line">Authors: Wonhyeok Choi, Shutong Ding, Minwoo Choi, Jungwan Woo, Kyumin Hwang, Jaeyeul Kim, Ye Shi, Sunghoon Im</div>
<div class="meta-line">First: 2026-01-05T05:19:23+00:00 · Latest: 2026-02-09T14:52:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06133v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06133v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion policies have emerged as a powerful approach for robotic control, demonstrating superior expressiveness in modeling multimodal action distributions compared to conventional policy networks. However, their integration with online reinforcement learning remains challenging due to fundamental incompatibilities between diffusion model training objectives and standard RL policy improvement mechanisms. This paper presents the first comprehensive review and empirical analysis of current Online Diffusion Policy Reinforcement Learning (Online DPRL) algorithms for scalable robotic control systems. We propose a novel taxonomy that categorizes existing approaches into four distinct families--Action-Gradient, Q-Weighting, Proximity-Based, and Backpropagation Through Time (BPTT) methods--based on their policy improvement mechanisms. Through extensive experiments on a unified NVIDIA Isaac Lab benchmark encompassing 12 diverse robotic tasks, we systematically evaluate representative algorithms across five critical dimensions: task diversity, parallelization capability, diffusion step scalability, cross-embodiment generalization, and environmental robustness. Our analysis identifies key findings regarding the fundamental trade-offs inherent in each algorithmic family, particularly concerning sample efficiency and scalability. Furthermore, we reveal critical computational and algorithmic bottlenecks that currently limit the practical deployment of online DPRL. Based on these findings, we provide concrete guidelines for algorithm selection tailored to specific operational constraints and outline promising future research directions to advance the field toward more general and scalable robotic learning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向可扩展机器人控制的在线扩散策略强化学习算法综述</div>
<div class="mono" style="margin-top:8px">扩散策略已成为机器人控制的一种强大方法，相比传统策略网络，其在建模多模态动作分布方面展现出更强的表达能力。然而，由于扩散模型训练目标与标准强化学习策略改进机制之间存在根本性不兼容，将其与在线强化学习结合仍具挑战性。本文首次对当前面向可扩展机器人控制系统的在线扩散策略强化学习算法进行了全面综述与实证分析，提出一种新颖的分类法，依据策略改进机制将现有方法归纳为四大类：动作梯度法、Q值加权法、邻近优化法以及时间反向传播法。通过在涵盖12种多样化机器人任务的统一NVIDIA Isaac Lab基准测试中进行大量实验，我们从任务多样性、并行化能力、扩散步长可扩展性、跨实体泛化能力和环境鲁棒性五个关键维度系统评估了代表性算法。分析揭示了各类算法家族内在的核心权衡关系，特别是在样本效率与可扩展性方面。此外，我们指出了当前限制在线扩散策略强化学习实际部署的关键计算与算法瓶颈。基于这些发现，我们针对具体操作约束提供了算法选择的实用指南，并展望了推动该领域向更通用、可扩展的机器人学习系统发展的未来研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of integrating expressive diffusion policies with online reinforcement learning for scalable robotic control, due to incompatibilities between their training objectives. The method involves proposing a novel taxonomy that categorizes existing Online Diffusion Policy RL algorithms into four families based on their policy improvement mechanisms: Action-Gradient, Q-Weighting, Proximity-Based, and Backpropagation Through Time methods. The main experimental results, from a unified benchmark of 12 diverse robotic tasks, systematically evaluate these algorithms across dimensions like task diversity and scalability, revealing fundamental trade-offs in sample efficiency and identifying computational bottlenecks that limit practical deployment.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，由于训练目标的不兼容性，将表达能力强的扩散策略与在线强化学习结合用于可扩展机器人控制面临挑战。方法上，提出了一种新颖的分类法，根据策略改进机制将现有的在线扩散策略强化学习算法分为四类：动作梯度法、Q加权法、邻近法和时间反向传播法。主要实验结果基于一个包含12种不同机器人任务的统一基准测试，系统评估了这些算法在任务多样性和可扩展性等维度上的表现，揭示了样本效率方面的根本性权衡，并指出了限制实际部署的计算瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning</div>
<div class="meta-line">Authors: David Hudák, Maris F. L. Galesloot, Martin Tappler, Martin Kurečka, Nils Jansen, Milan Češka</div>
<div class="meta-line">First: 2026-02-09T14:39:16+00:00 · Latest: 2026-02-09T14:39:16+00:00</div>
<div class="meta-line">Comments: 17 pages (8 main paper, 2 references, 7 appendix). 3 figures in the main paper, 3 figures in the appendix. Accepted AAMAS&#x27;26 submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08734v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08734v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的（隐模型）部分可观测马尔可夫决策过程有限状态控制器</div>
<div class="mono" style="margin-top:8px">求解部分可观测马尔可夫决策过程需要在状态信息不完整的情况下计算策略。尽管近期研究取得进展，现有POMDP求解器的可扩展性仍受限。此外，许多场景要求策略在多个POMDP中保持鲁棒性，进一步加剧了可扩展性问题。我们提出Lexpop框架用于POMDP求解，该框架（1）采用深度强化学习训练由循环神经网络表示的神经策略，（2）通过高效提取方法构建模拟神经策略的有限状态控制器。关键的是，与神经策略不同，此类控制器可进行形式化验证并提供性能保证。我们将Lexpop扩展至隐模型POMDP的鲁棒策略计算，该模型描述有限POMDP集合。我们为每个提取的控制器关联其最差情况POMDP，利用此类POMDP集合迭代训练鲁棒神经策略并提取鲁棒控制器。实验表明，在大状态空间问题上，Lexpop在POMDP及HM-POMDP求解性能上均优于当前最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the scalability limitations of existing solvers for partially observable Markov decision processes (POMDPs) and the need for policies robust across multiple models, this paper introduces the Lexpop framework. The method first trains a neural policy using deep reinforcement learning with a recurrent neural network, then extracts a finite-state controller that mimics this policy, enabling formal evaluation and performance guarantees. The approach is extended to hidden-model POMDPs (HM-POMDPs) by iteratively training a robust neural policy against worst-case POMDPs and extracting a corresponding robust controller. Experimental results demonstrate that Lexpop outperforms state-of-the-art solvers on problems with large state spaces for both standard POMDPs and HM-POMDPs.</div>
<div class="mono" style="margin-top:8px">针对现有部分可观测马尔可夫决策过程（POMDP）求解器可扩展性有限，且需要跨多个模型具有鲁棒性的策略这一问题，本文提出了Lexpop框架。该方法首先使用深度强化学习训练一个基于循环神经网络的神经策略，然后通过高效提取方法构建模仿该策略的有限状态控制器，从而支持形式化验证和性能保证。该框架被扩展到隐模型POMDP（HM-POMDP），通过迭代训练针对最坏情况POMDP的鲁棒神经策略，并提取相应的鲁棒控制器。实验结果表明，在大状态空间问题上，Lexpop在标准POMDP和HM-POMDP上的性能均优于当前最先进的求解器。</div>
</details>
</div>
<div class="card">
<div class="title">Language Bottleneck Models for Qualitative Knowledge State Modeling</div>
<div class="meta-line">Authors: Antonin Berthon, Mihaela van der Schaar</div>
<div class="meta-line">First: 2025-06-20T13:21:14+00:00 · Latest: 2026-02-09T14:23:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.16982v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.16982v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately assessing student knowledge is central to education. Cognitive Diagnosis (CD) models estimate student proficiency at a fixed point in time, while Knowledge Tracing (KT) methods model evolving knowledge states to predict future performance. However, existing approaches either provide quantitative concept mastery estimates with limited expressivity (CD, probabilistic KT) or prioritize predictive accuracy at the cost of interpretability (deep learning KT). We propose Language Bottleneck Models (LBMs), where an encoder LLM produces textual knowledge state summaries, which a decoder LLM uses to predict future performance. This produces interpretable summaries that can express nuanced insights--such as misconceptions--that CD and KT models cannot capture. Extensive validation across synthetic and real-world datasets shows LBMs reveal qualitative insights beyond what CD and KT models can capture, while achieving competitive accuracy with improved sample efficiency. We demonstrate that the encoder and decoder can be fine-tuned with reinforcement learning and supervised fine-tuning respectively to improve both summary quality and predictive performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言瓶颈模型用于定性知识状态建模</div>
<div class="mono" style="margin-top:8px">准确评估学生知识是教育的核心。认知诊断模型在固定时间点估计学生能力，而知识追踪方法则建模动态知识状态以预测未来表现。然而，现有方法要么提供表达能力有限的定量概念掌握度估计，要么以牺牲可解释性为代价优先考虑预测准确性。我们提出语言瓶颈模型，其中编码器大语言模型生成文本化知识状态摘要，解码器大语言模型利用该摘要预测未来表现。这种方法产生可解释的摘要，能够表达认知诊断和知识追踪模型无法捕捉的细微洞察。在合成和真实数据集上的广泛验证表明，语言瓶颈模型能揭示超越传统模型的定性洞察，同时以更优的样本效率实现具有竞争力的预测精度。我们证明编码器和解码器可分别通过强化学习和监督微调进行优化，从而提升摘要质量与预测性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for more expressive and interpretable models of student knowledge, as existing Cognitive Diagnosis and Knowledge Tracing methods are either limited to quantitative estimates or sacrifice interpretability for predictive accuracy. The proposed method, Language Bottleneck Models (LBMs), uses an encoder LLM to generate textual summaries of student knowledge states, which a decoder LLM then utilizes to predict future performance, thereby capturing qualitative insights like misconceptions. Experimental results on synthetic and real-world datasets show that LBMs achieve competitive predictive accuracy with improved sample efficiency while providing interpretable summaries that reveal nuanced knowledge aspects beyond traditional models, with further improvements possible through reinforcement learning and supervised fine-tuning.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要更具表达力和可解释性的学生知识状态模型，因为现有的认知诊断和知识追踪方法要么局限于定量估计，要么以牺牲可解释性为代价追求预测准确性。所提出的方法——语言瓶颈模型（LBMs）——使用编码器大语言模型生成学生知识状态的文本摘要，再由解码器大语言模型利用这些摘要预测未来表现，从而捕捉如误解等定性洞察。在合成和真实数据集上的实验结果表明，LBMs在实现具有竞争力的预测准确性并提高样本效率的同时，提供了可解释的摘要，揭示了传统模型无法捕捉的细微知识方面，且通过强化学习和监督微调可进一步提升性能。</div>
</details>
</div>
<div class="card">
<div class="title">SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity</div>
<div class="meta-line">Authors: Shae McFadden, Myles Foley, Elizabeth Bates, Ilias Tsingenopoulos, Sanyam Vyas, Vasilios Mavroudis, Chris Hicks, Fabio Pierazzi</div>
<div class="meta-line">First: 2026-02-09T14:12:41+00:00 · Latest: 2026-02-09T14:12:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08690v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SoK：深度强化学习在网络安全应用中的陷阱</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）在需要序列决策的领域取得了显著成功，这推动了其在网络安全问题中的应用。然而，将DRL从实验室模拟迁移到定制化的网络环境可能引发诸多问题。大多数网络安全任务通常具有对抗性、非稳态和部分可观测的特性，进一步加剧了这些挑战。本文系统性地识别并归纳了网络安全领域DRL（DRL4Sec）文献中在环境建模、智能体训练、性能评估和系统部署阶段频繁出现的11个方法论陷阱。通过分析66篇重要的DRL4Sec论文（2018-2025年），我们量化了每个陷阱的普遍性，发现平均每篇论文存在超过五个陷阱。我们通过（i）自主网络防御、（ii）对抗性恶意软件生成和（iii）Web安全测试环境中的对照实验，验证了这些陷阱的实际影响。最后，我们针对每个陷阱提出了可操作的建议，以支持开发更严谨、可部署的基于DRL的安全系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing but problematic application of deep reinforcement learning (DRL) to complex cybersecurity tasks, this paper systematically identifies 11 common methodological pitfalls across environment modeling, agent training, evaluation, and deployment. The method involves analyzing 66 key DRL4Sec papers from 2018-2025, quantifying the prevalence of these issues, and demonstrating their practical impact through controlled experiments in domains like autonomous defense and malware creation. The main experimental results reveal that the surveyed literature contains an average of over five pitfalls per paper, highlighting significant gaps in rigor and deployability that the authors address with actionable recommendations for improvement.</div>
<div class="mono" style="margin-top:8px">本文的动机源于深度强化学习（DRL）在复杂网络安全任务中的应用日益增多但问题频出，旨在系统性地识别在环境建模、智能体训练、评估和部署阶段常见的11个方法学缺陷。研究方法包括分析2018年至2025年间66篇重要的DRL4Sec论文，量化这些问题的普遍性，并通过在自主防御、恶意软件生成等领域的受控实验来展示其实际影响。主要实验结果表明，所调研的文献平均每篇存在超过五个缺陷，揭示了当前研究在严谨性和可部署性方面的显著不足，作者为此提供了可操作的改进建议。</div>
</details>
</div>
<div class="card">
<div class="title">Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning</div>
<div class="meta-line">Authors: Constant Bourdrez, Alexandre Vérine, Olivier Cappé</div>
<div class="meta-line">First: 2026-02-09T14:10:44+00:00 · Latest: 2026-02-09T14:10:44+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08689v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08689v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过逆强化学习学习从扩散模型采样</div>
<div class="mono" style="margin-top:8px">扩散模型通过神经网络引导的迭代去噪过程生成样本。虽然在真实数据上训练去噪器计算成本高昂，但采样过程本身更具灵活性。这种适应性在实践中成为关键杠杆，既能提升生成样本的质量，又能提高采样效率。本研究提出一种逆强化学习框架，用于学习采样策略而无需重新训练去噪器。我们将扩散采样过程建模为离散时间有限时域马尔可夫决策过程，其中动作对应采样动力学的可选修改。为优化动作调度，我们避免定义显式奖励函数，而是直接使用策略梯度技术匹配采样器预期目标行为。实验证明该方法能提升预训练扩散模型的生成样本质量，并自动调整采样超参数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational expense of training diffusion model denoisers and the flexibility of the sampling process, aiming to improve sample quality and efficiency without retraining the denoiser. The method introduces an inverse reinforcement learning framework that formulates diffusion sampling as a Markov Decision Process, where actions modify sampling dynamics, and optimizes action scheduling via policy gradient techniques to match target behavior without an explicit reward function. Experimental results demonstrate that this approach enhances the quality of samples from pretrained diffusion models and effectively tunes sampling hyperparameters.</div>
<div class="mono" style="margin-top:8px">本文的动机在于扩散模型去噪器训练的计算成本高昂，而采样过程更具灵活性，旨在不重新训练去噪器的情况下提升生成样本的质量和效率。方法引入了一个逆强化学习框架，将扩散采样建模为马尔可夫决策过程，其中动作对应采样动态的可选修改，并通过策略梯度技术优化动作调度以匹配目标行为，无需显式奖励函数。实验结果表明，该方法能提高预训练扩散模型生成样本的质量，并有效自动调整采样超参数。</div>
</details>
</div>
<div class="card">
<div class="title">LLaDA2.1: Speeding Up Text Diffusion via Token Editing</div>
<div class="meta-line">Authors: Tiwei Bie, Maosong Cao, Xiang Cao, Bingsen Chen, Fuyuan Chen, Kun Chen, Lun Du, Daozhuo Feng, Haibo Feng, Mingliang Gong, Zhuocheng Gong, Yanmei Gu, Jian Guan, Kaiyuan Guan, Hongliang He, Zenan Huang, Juyong Jiang, Zhonghui Jiang, Zhenzhong Lan, Chengxi Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Yuan Lu, Yuxin Ma, Xingyu Mou, Zhenxuan Pan, Kaida Qiu, Yuji Ren, Jianfeng Tan, Yiding Tian, Zian Wang, Lanning Wei, Tao Wu, Yipeng Xing, Wentao Ye, Liangyu Zha, Tianze Zhang, Xiaolu Zhang, Junbo Zhao, Da Zheng, Hao Zhong, Wanli Zhong, Jun Zhou, Junlin Zhou, Liwang Zhu, Muzhi Zhu, Yihong Zhuang</div>
<div class="meta-line">First: 2026-02-09T14:00:07+00:00 · Latest: 2026-02-09T14:00:07+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08676v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08676v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaDA2.1：通过令牌编辑加速文本扩散</div>
<div class="mono" style="margin-top:8px">尽管LLaDA2.0展示了百亿级块扩散模型的扩展潜力及其固有的并行化能力，解码速度与生成质量间的微妙平衡始终是难以突破的边界。今日我们推出LLaDA2.1，这一范式转变旨在超越此权衡。通过将令牌到令牌（T2T）编辑无缝融入传统的掩码到令牌（M2T）框架，我们引入了可配置的联合阈值解码方案。该结构创新催生两种独特模式：迅捷模式（S模式）大胆降低M2T阈值以突破传统限制，同时依赖T2T优化输出；品质模式（Q模式）采用保守阈值，在可控效率损失下确保卓越的基准性能。基于扩展的上下文窗口，我们进一步构建了首个专为扩散大语言模型定制的大规模强化学习（RL）框架，辅以稳定梯度估计的专项技术。这种对齐机制不仅提升了推理精度，更增强了指令遵循的忠实度，弥合了扩散动力学与复杂人类意图间的鸿沟。我们同步发布LLaDA2.1-Mini（160亿参数）与LLaDA2.1-Flash（1000亿参数）。在33项严格基准测试中，LLaDA2.1展现出强大的任务性能与闪电般的解码速度。尽管参数量达千亿，其在代码任务中仍实现惊人性能：HumanEval+达892 TPS，BigCodeBench达801 TPS，LiveCodeBench达663 TPS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the persistent trade-off between decoding speed and generation quality in large-scale block-diffusion models like LLaDA2.0, this work introduces LLaDA2.1, which integrates Token-to-Token editing with the conventional Mask-to-Token scheme to create a configurable threshold-decoding framework. This method enables two operational modes: a Speedy Mode that lowers thresholds for fast generation and refines output via editing, and a Quality Mode that uses conservative thresholds for high performance. The model also incorporates a large-scale Reinforcement Learning framework for alignment, enhancing reasoning and instruction-following. Experimental results across 33 benchmarks show strong task performance and exceptionally fast decoding speeds, with the 100B model achieving up to 892 tokens per second on coding tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决LLaDA2.0等大规模块扩散模型中解码速度与生成质量之间的权衡问题，提出了LLaDA2.1模型，通过将Token-to-Token编辑无缝集成到传统的Mask-to-Token方案中，引入了一种可配置的阈值解码框架。该方法支持两种模式：快速模式通过降低阈值实现高速生成并依赖编辑优化输出，质量模式则采用保守阈值以确保优异性能。模型还结合了首个针对扩散大语言模型的大规模强化学习框架，以提升推理和指令遵循能力。在33个基准测试中，该模型展现出强大的任务性能和极快的解码速度，其中100B参数版本在代码任务上达到了每秒892个令牌的处理速度。</div>
</details>
</div>
<div class="card">
<div class="title">From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism</div>
<div class="meta-line">Authors: Sarthak Wanjari</div>
<div class="meta-line">First: 2026-02-09T13:48:25+00:00 · Latest: 2026-02-09T13:48:25+00:00</div>
<div class="meta-line">Comments: 10 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08655v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08655v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL&#x27;s 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从机器人学到脓毒症治疗：基于几何悲观主义的离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）旨在从静态数据集中恢复最优策略，但其仍易高估分布外（OOD）动作，尤其是在断裂稀疏的数据流形中。现有方法需在计算效率与性能间权衡：如CQL等方法虽提供严格保守性但需巨大算力，而基于期望分位数的高效方法（如IQL）常无法修正病态数据集上的OOD误差，退化为行为克隆。本研究提出几何悲观主义——一种模块化、计算高效的框架，通过基于状态-动作嵌入空间中k近邻距离的密度惩罚增强标准IQL。通过预计算应用于各状态-动作对的惩罚项，本方法以O(1)训练开销通过奖励塑形注入OOD保守性。在D4RL MuJoCo基准测试中，我们的Geo-IQL方法在敏感不稳定的medium-replay任务上超越标准IQL超过18分，同时将随机种子间方差降低4倍，且在稳定流形上性能无衰减。关键地，我们在MIMIC-III脓毒症重症护理数据集上验证算法：当标准IQL退化为行为克隆时，Geo-IQL展现出主动策略改进，在保持安全约束下达成86.4%的临床终局决策符合率（IQL为75%）。结果表明几何悲观主义能为关键现实决策系统安全突破局部最优提供必要正则化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in offline reinforcement learning where existing methods either demand high computational resources or fail to correct out-of-distribution errors in sparse datasets, leading to suboptimal policies. The authors propose Geometric Pessimism, a modular framework that enhances the efficient IQL method by incorporating a density-based penalty derived from k-nearest-neighbor distances in the state-action embedding space, applied via reward shaping with minimal training overhead. Experimental results on the D4RL MuJoCo benchmark show that Geo-IQL outperforms standard IQL by over 18 points on sensitive tasks and reduces variance, while on the MIMIC-III Sepsis dataset, it achieves active policy improvement with 86.4% terminal agreement with clinicians, compared to IQL&#x27;s 75%, demonstrating effective regularization for real-world decision systems.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中现有方法要么计算成本高昂，要么在稀疏数据集中无法纠正分布外误差导致策略次优的问题。作者提出几何悲观主义，这是一个模块化框架，通过基于状态-动作嵌入空间中k近邻距离的密度惩罚来增强高效的IQL方法，并以最小训练开销通过奖励塑形应用。在D4RL MuJoCo基准测试中，Geo-IQL在敏感任务上比标准IQL高出超过18分并降低了方差；在MIMIC-III脓毒症数据集上，它实现了主动策略改进，与临床医生的终端一致性达到86.4%，而IQL仅为75%，证明了其在现实世界决策系统中有效正则化的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces</div>
<div class="meta-line">Authors: Heiko Hoppe, Fabian Akkerman, Wouter van Heeswijk, Maximilian Schiffer</div>
<div class="meta-line">First: 2026-02-09T13:05:07+00:00 · Latest: 2026-02-09T13:05:07+00:00</div>
<div class="meta-line">Comments: 26 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08616v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08616v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破网格限制：大规模离散与混合动作空间中的距离引导强化学习</div>
<div class="mono" style="margin-top:8px">强化学习在物流、调度和推荐系统中的应用日益广泛，但标准算法在应对大规模离散动作空间时面临维度灾难。现有算法通常依赖受限的网格结构或计算成本高昂的最近邻搜索，限制了其在高维或不规则结构领域的效果。我们提出距离引导强化学习（DGRL），结合采样动态邻域（SDN）和基于距离的更新（DBU），可在动作规模高达10^20的空间中实现高效强化学习。与先前方法不同，SDN利用语义嵌入空间进行随机体积探索，可证明在局部信任区域内提供完整支持。DBU则将策略优化转化为稳定的回归任务，使梯度方差与动作空间基数解耦，并保证策略的单调改进。DGRL能自然推广到混合连续-离散动作空间，无需层级依赖。我们在规则与不规则结构环境中验证了该方法，相比最先进基准性能提升最高达66%，同时提升了收敛速度并降低了计算复杂度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of applying reinforcement learning to domains with extremely large discrete or hybrid action spaces, such as logistics and recommender systems, where standard methods suffer from the curse of dimensionality and existing approaches rely on inefficient grid-based structures or costly nearest-neighbor searches. The authors propose Distance-Guided Reinforcement Learning (DGRL), which combines Sampled Dynamic Neighborhoods (SDN) for stochastic volumetric exploration in a semantic embedding space and Distance-Based Updates (DBU) to transform policy optimization into a stable regression task, decoupling gradient variance from action count and ensuring monotonic improvement. Experimental results show that DGRL achieves performance gains of up to 66% over state-of-the-art benchmarks in both regularly and irregularly structured environments, while also improving convergence speed and computational efficiency, and it generalizes naturally to hybrid action spaces without hierarchical constraints.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在物流、推荐系统等具有极大离散或混合动作空间领域中的应用挑战，其中标准方法受维度灾难困扰，现有方法依赖低效的网格结构或昂贵的最近邻搜索。作者提出了距离引导强化学习（DGRL），结合了采样动态邻域（SDN）在语义嵌入空间中进行随机体积探索，以及距离基更新（DBU）将策略优化转化为稳定的回归任务，从而解耦梯度方差与动作数量并保证策略单调改进。实验结果表明，DGRL在规则和非规则结构环境中相比最先进基准性能提升高达66%，同时提高了收敛速度和计算效率，并能自然推广到混合动作空间而无需层次依赖。</div>
</details>
</div>
<div class="card">
<div class="title">Playing 20 Question Game with Policy-Based Reinforcement Learning</div>
<div class="meta-line">Authors: Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu, Zhan Chen</div>
<div class="meta-line">First: 2018-08-23T06:34:32+00:00 · Latest: 2026-02-09T13:00:05+00:00</div>
<div class="meta-line">Comments: Withdrawal from the conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/1808.07645v4">Abs</a> · <a href="https://arxiv.org/pdf/1808.07645v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisy-free simulation environment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略强化学习的二十问游戏研究</div>
<div class="mono" style="margin-top:8px">二十问游戏是一种鼓励演绎推理与创造力的经典游戏。游戏中，回答者首先选定一个对象（如名人或动物类别），提问者则通过最多二十个问题猜测该对象。在二十问游戏系统中，用户扮演回答者，系统作为提问者需要采用高效的问题选择策略以准确识别目标并获胜。然而，由于游戏环境的复杂性与多变性，最优提问策略难以直接推导。本文提出一种新颖的基于策略强化学习方法，使提问智能体能够通过与用户的持续交互学习最优提问策略。为提升训练效率，同时提出采用奖励网络估算信息量更高的奖励值。相较于既有方法，本强化学习方法对噪声答案具有鲁棒性，且无需依赖对象知识库。实验结果表明，该方法显著优于基于信息熵的工程系统，并在无噪声模拟环境中展现出竞争优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of developing an optimal question selection strategy for the 20 Questions game, where traditional methods struggle due to environmental complexity and noisy user responses. The authors propose a policy-based reinforcement learning approach that learns through continuous interaction with users, supplemented by a reward network to provide more informative feedback during training. Experimental results demonstrate that this method outperforms an entropy-based baseline system and achieves competitive performance in noise-free simulations, while being robust to answer noise and independent of a predefined knowledge base.</div>
<div class="mono" style="margin-top:8px">本文针对二十个问题游戏中因环境复杂性和用户回答噪声导致传统方法难以获得最优提问策略的挑战，提出了一种基于策略的强化学习方法，通过持续与用户交互进行学习，并引入奖励网络在训练中提供更有效的反馈。实验结果表明，该方法在性能上明显优于基于熵的工程系统，并在无噪声模拟环境中具有竞争力，同时能够有效应对回答噪声且不依赖于预定义的知识库。</div>
</details>
</div>
<div class="card">
<div class="title">GPTOpt: Teaching LLMs to do Interpretable Black-Box Optimization</div>
<div class="meta-line">Authors: Jamison Meindl, Yunsheng Tian, Tony Cui, Veronika Thost, Zhang-Wei Hong, Jie Chen, Wojciech Matusik, Mina Konaković Luković</div>
<div class="meta-line">First: 2025-10-29T11:21:55+00:00 · Latest: 2026-02-09T12:46:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.25404v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.25404v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency and decision interpretability. While Large Language Models (LLMs) have shown broad capabilities, even state-of-the-art models remain limited in solving continuous black-box optimization tasks and struggle to maintain exploration-exploitation balance. We introduce GPTOpt, an optimization method that equips LLMs with continuous black-box optimization capabilities by fine-tuning Llama 3.1 8B on structured Bayesian optimization (BO) data, including surrogate model information. This provides an explainable framework calibrated to produce surrogate model outputs comparable to a Gaussian process, while keeping the advantages of flexible LLM-based optimization. On a variety of black-box optimization benchmarks, our model shows favorable performance compared to traditional optimizers and transformer-based alternatives, while providing important context and insight into the model&#x27;s decisions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GPTOpt：指导大语言模型执行可解释黑盒优化</div>
<div class="mono" style="margin-top:8px">昂贵、无导数黑盒函数的全局优化需要极高的样本效率和决策可解释性。尽管大语言模型已展现出广泛能力，但即使是先进模型在解决连续黑盒优化任务时仍存在局限，难以保持探索与利用的平衡。我们提出GPTOpt方法，通过在结构化贝叶斯优化数据（包含代理模型信息）上微调Llama 3.1 8B模型，使大语言模型获得连续黑盒优化能力。该方法构建了一个可解释的框架，其校准后产生的代理模型输出可与高斯过程相媲美，同时保留了基于大语言模型的灵活优化优势。在多种黑盒优化基准测试中，相较于传统优化器和基于Transformer的替代方案，我们的模型展现出优越性能，并为模型决策提供了重要的上下文与洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the need for sample-efficient and interpretable global optimization of expensive, black-box functions, where current Large Language Models (LLMs) are limited in handling continuous domains and balancing exploration with exploitation. The method, named GPTOpt, fine-tunes the Llama 3.1 8B model on structured data from Bayesian optimization, incorporating surrogate model information to equip the LLM with continuous black-box optimization capabilities in an explainable framework that mimics Gaussian process outputs. Experimental results on various benchmarks demonstrate that GPTOpt outperforms traditional optimizers and other transformer-based methods while providing interpretable insights into its optimization decisions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决对昂贵、无导数黑盒函数进行样本高效且可解释的全局优化的需求，而当前的大语言模型在处理连续域及平衡探索与利用方面存在局限。该方法名为GPTOpt，通过在贝叶斯优化的结构化数据上对Llama 3.1 8B模型进行微调，融入代理模型信息，使大语言模型具备连续黑盒优化能力，并形成一个可解释的框架，其输出可与高斯过程相媲美。在各种黑盒优化基准测试上的实验结果表明，GPTOpt的性能优于传统优化器和其他基于Transformer的方法，同时能为模型的优化决策提供重要的可解释性洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Conditional Sequence Modeling for Safe Reinforcement Learning</div>
<div class="meta-line">Authors: Wensong Bai, Chao Zhang, Qihang Xu, Chufan Chen, Chenhao Zhou, Hui Qian</div>
<div class="meta-line">First: 2026-02-09T12:22:57+00:00 · Latest: 2026-02-09T12:22:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08584v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08584v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>条件序列建模在安全强化学习中的应用</div>
<div class="mono" style="margin-top:8px">离线安全强化学习旨在从固定数据集中学习策略，同时在累积成本约束下最大化性能。实际部署中，不同场景的需求常存在差异，需要单一策略能够零样本适应不同的成本阈值。然而，现有离线安全强化学习方法大多基于预设阈值训练，导致策略在跨成本阈值时的泛化能力和部署灵活性受限。受条件序列建模近期进展的启发——该方法通过指定目标回报实现灵活的目标条件控制，我们提出RCDT，一种基于条件序列建模的方法，支持在单一训练策略内跨多个成本阈值的零样本部署。RCDT是首个基于条件序列建模的离线安全强化学习算法，融合了拉格朗日式成本惩罚与自适应惩罚系数。为避免过度保守行为并实现更优的回报-成本权衡，该方法进一步引入了奖励-成本感知轨迹重加权机制和Q值正则化。在DSRL基准上的大量实验表明，RCDT在回报-成本权衡方面持续优于代表性基线方法，推动了离线安全强化学习的技术前沿。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for flexible deployment in offline safe reinforcement learning, where existing methods typically train policies for a single cost threshold, limiting their adaptability. The authors propose RCDT, a conditional sequence modeling approach that enables a single policy to adjust zero-shot to various cost constraints by integrating a Lagrangian-style cost penalty with an auto-adaptive coefficient, alongside reward-cost-aware trajectory reweighting and Q-value regularization to optimize performance without excessive conservatism. Experimental results on the DSRL benchmark show that RCDT consistently achieves superior trade-offs between return and cost compared to baseline methods, advancing the state-of-the-art in offline safe RL.</div>
<div class="mono" style="margin-top:8px">本文针对离线安全强化学习中现有方法通常针对单一成本阈值训练策略、导致部署灵活性不足的问题，提出了一种基于条件序列建模的方法RCDT。该方法通过结合拉格朗日式成本惩罚与自适应系数，以及奖励-成本感知的轨迹重加权和Q值正则化，使单个策略能够零样本适应不同成本阈值，避免过于保守的行为并优化性能-成本权衡。在DSRL基准测试上的实验表明，RCDT相比代表性基线方法，在回报与成本之间的权衡上取得了更优的结果，推动了离线安全强化学习的技术进展。</div>
</details>
</div>
<div class="card">
<div class="title">Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO</div>
<div class="meta-line">Authors: Kun Peng, Conghui Tan, Yu Liu, Guohua Tang, Zhongqian Sun, Wei Yang, Zining Zhu, Lei Jiang, Yanbing Liu, Hao Peng</div>
<div class="meta-line">First: 2026-02-09T11:32:02+00:00 · Latest: 2026-02-09T11:32:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08533v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08533v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users&#x27; traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework&#x27;s superior performance, sample efficiency, and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于智能体博弈与自适应树状GRPO的对话模型优化</div>
<div class="mono" style="margin-top:8px">开放域对话智能体旨在通过适应用户特质实现个性化交互，但现有方法存在关键局限：过度依赖预收集用户数据，以及强化学习中的短视偏差忽视长期对话价值。为此，我们提出一种融合在线个性化与自适应树状群组相对策略优化（AT-GRPO）的新型长程强化学习框架。采用双智能体博弈范式，用户智能体通过风格模仿（学习用户特定对话特质）和主动终止（预测轮次终止概率作为即时奖励）构建动态环境，形成驱动对话智能体深化兴趣探索的迭代循环。AT-GRPO将对话轨迹重构为树状结构并引入自适应观测范围：不同于指数级开销的全树扩展，该方法将每个节点的奖励聚合限制在阶段感知范围内——较大范围支持早期话题探索，较小范围促进后期对话维护。该设计将对话长度对应的计算开销从指数级降至多项式级，同时保持长期奖励捕获能力。大量实验验证了本框架在性能、样本效率与鲁棒性方面的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in open-ended dialogue agents, specifically their reliance on pre-collected user data and short-horizon biases in reinforcement learning that neglect long-term engagement. To overcome these, the authors propose a novel long-horizon RL framework that integrates online personalization through a two-agent game paradigm, where a user agent mimics user styles and predicts termination probabilities to drive exploration, combined with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO) which reinterprets dialogue trajectories as trees with adaptive observation ranges to balance exploration and maintenance while reducing computational overhead from exponential to polynomial. Experimental results demonstrate the framework&#x27;s superior performance, sample efficiency, and robustness in dialogue optimization.</div>
<div class="mono" style="margin-top:8px">本文针对开放域对话智能体存在的依赖预收集用户数据和强化学习中忽视长期价值的短视偏差等关键局限，提出了一种新颖的长时域强化学习框架。该方法通过双智能体博弈实现在线个性化，其中用户智能体模拟用户风格并预测终止概率以驱动探索，并结合自适应树基分组相对策略优化（AT-GRPO），将对话轨迹视为树结构并引入自适应观测范围，在早期支持话题探索、晚期促进对话维持，从而将计算开销从指数级降至多项式级。大量实验表明，该框架在对话优化中具有优越的性能、样本效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search</div>
<div class="meta-line">Authors: Yu Xue, Pengcheng Jiang, Chenchen Zhu, Yong Zhang, Ran Cheng, Kaizhou Gao, Dunwei Gong</div>
<div class="meta-line">First: 2026-02-09T11:04:07+00:00 · Latest: 2026-02-09T11:04:07+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Transactions on Neural Networks and Learning Systems. Published on this https URL: https://doi.org/10.1109/TNNLS.2026.3659508</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08513v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08513v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural architecture search (NAS) automates neural network design, improving efficiency over manual approaches. However, efficiently discovering high-performance neural network architectures that simultaneously optimize multiple objectives remains a significant challenge in NAS. Existing methods often suffer from limited population diversity and inadequate exploration of the search space, particularly in regions with extreme complexity values. To address these challenges, we propose MOEA-BUS, an innovative multi-objective evolutionary algorithm based on bi-population with uniform sampling for neural architecture search, aimed at simultaneously optimizing both accuracy and network complexity. In MOEA-BUS, a novel uniform sampling method is proposed to initialize the population, ensuring that architectures are distributed uniformly across the objective space. Furthermore, to enhance exploration, we deploy a bi-population framework where two populations evolve synergistically, facilitating comprehensive search space coverage. Experiments on CIFAR-10 and ImageNet demonstrate MOEA-BUS&#x27;s superiority, achieving top-1 accuracies of 98.39% on CIFAR-10, and 80.03% on ImageNet. Notably, it achieves 78.28% accuracy on ImageNet with only 446M MAdds. Ablation studies confirm that both uniform sampling and bi-population mechanisms enhance population diversity and performance. Additionally, in terms of the Kendall&#x27;s tau coefficient, the SVM achieves an improvement of at least 0.035 compared to the other three commonly used machine learning models, and uniform sampling provided an enhancement of approximately 0.07.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于双种群均匀采样的多目标进化算法用于神经架构搜索</div>
<div class="mono" style="margin-top:8px">神经架构搜索（NAS）通过自动化神经网络设计，相比人工方法提升了效率。然而，在NAS中高效发现同时优化多个目标的高性能神经网络架构仍是一个重大挑战。现有方法常受限于种群多样性不足及搜索空间探索不充分，尤其在极端复杂度值区域。为解决这些问题，我们提出了MOEA-BUS，一种基于双种群均匀采样的创新多目标进化算法，用于神经架构搜索，旨在同时优化精度和网络复杂度。在MOEA-BUS中，提出了一种新颖的均匀采样方法初始化种群，确保架构在目标空间中均匀分布。此外，为增强探索能力，我们采用双种群框架，使两个种群协同进化，促进搜索空间的全面覆盖。在CIFAR-10和ImageNet上的实验证明了MOEA-BUS的优越性，在CIFAR-10上达到98.39%的top-1准确率，在ImageNet上达到80.03%。值得注意的是，在仅使用446M MAdds的情况下，其在ImageNet上实现了78.28%的准确率。消融研究证实，均匀采样和双种群机制均能提升种群多样性和性能。此外，在Kendall&#x27;s tau系数方面，SVM相比其他三种常用机器学习模型至少提升了0.035，而均匀采样带来了约0.07的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge in neural architecture search (NAS) of efficiently discovering high-performance architectures that optimize multiple objectives, such as accuracy and network complexity, where existing methods often suffer from limited population diversity and inadequate exploration of extreme regions. The authors propose MOEA-BUS, a multi-objective evolutionary algorithm using a bi-population framework with uniform sampling to enhance diversity and search space coverage by initializing populations uniformly across the objective space and having two populations evolve synergistically. Experimental results on CIFAR-10 and ImageNet show superior performance, with top-1 accuracies of 98.39% on CIFAR-10 and 80.03% on ImageNet, and notably 78.28% accuracy on ImageNet with only 446M MAdds, while ablation studies confirm the effectiveness of the uniform sampling and bi-population mechanisms in improving diversity and performance metrics.</div>
<div class="mono" style="margin-top:8px">本文针对神经架构搜索（NAS）中高效发现同时优化多个目标（如准确性和网络复杂度）的高性能架构的挑战，现有方法常存在种群多样性有限和对极端复杂度区域探索不足的问题。作者提出了MOEA-BUS，一种基于双种群和均匀采样的多目标进化算法，通过均匀采样初始化种群以确保架构在目标空间中均匀分布，并采用双种群框架协同进化以增强搜索空间覆盖。在CIFAR-10和ImageNet上的实验结果表明了其优越性，在CIFAR-10上达到98.39%的top-1准确率，在ImageNet上达到80.03%，尤其在仅使用446M MAdds时在ImageNet上实现了78.28%的准确率；消融研究证实了均匀采样和双种群机制能有效提升种群多样性和性能指标。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Self-Correction in Vision-Language Models via Rollout Augmentation</div>
<div class="meta-line">Authors: Yi Ding, Ziliang Qiu, Bolian Li, Ruqi Zhang</div>
<div class="meta-line">First: 2026-02-09T10:55:13+00:00 · Latest: 2026-02-09T10:55:13+00:00</div>
<div class="meta-line">Comments: 17 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08503v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08503v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过轨迹增强在视觉语言模型中学习自我校正</div>
<div class="mono" style="margin-top:8px">自我校正在视觉语言模型中解决复杂推理问题至关重要。然而，现有强化学习方法难以习得该能力，因为有效的自我校正行为极少出现，导致学习信号极度稀疏。为应对这一挑战，我们提出校正专用轨迹框架，通过重组现有轨迹合成密集的自我校正样本。该增强方法借助轨迹复用提升样本效率，同时通过均衡监督稳定强化学习优化。此外，我们引入响应掩码策略，将自我校正与直接推理解耦，避免信号冲突并使两种行为都能有效学习。基于此，我们推出具备可控自我校正能力的推理模型。在7个基准测试中，该模型在开源视觉语言模型中取得最先进性能，以仅需单步训练时间0.72倍的代价，超越最佳基线模型1.0分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of learning self-correction in vision-language models, where existing reinforcement learning methods suffer from sparse learning signals due to the rarity of effective self-correction behaviors. To overcome this, the authors propose Octopus, a framework that synthesizes dense self-correction examples by recombining existing rollouts, improving sample efficiency and stabilizing optimization, alongside a response-masking strategy to decouple self-correction from direct reasoning. Experimental results show that the resulting model, Octopus-8B, achieves state-of-the-art performance across seven benchmarks among open-source VLMs, outperforming the best baseline by 1.0 score while requiring only 0.72 times the training time per step.</div>
<div class="mono" style="margin-top:8px">该论文针对视觉语言模型中自我纠正学习面临的挑战，即现有强化学习方法因有效自我纠正行为罕见而导致学习信号稀疏。为解决此问题，作者提出了Octopus框架，通过重组现有轨迹来合成密集的自我纠正示例，从而提高样本效率并稳定优化，同时引入响应掩码策略以将自我纠正与直接推理解耦。实验结果表明，所开发的Octopus-8B模型在七个基准测试中实现了开源视觉语言模型中的最先进性能，以仅需每步0.72倍训练时间的代价，超越最佳基线1.0分。</div>
</details>
</div>
<div class="card">
<div class="title">Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Xiaodong Lu, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin, Zhijun Chen, Yu Luo, Fuzhen Zhuang, Yikun Ban, Deqing Wang</div>
<div class="meta-line">First: 2026-02-09T10:51:58+00:00 · Latest: 2026-02-09T10:51:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08499v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08499v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于情境化滚动策略的强化学习可验证奖励机制</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是提升大语言模型推理能力的有效范式。然而现有RLVR方法在滚动策略使用上存在盲目性与短视性：对同一提示下质量参差的响应进行均质化处理，且历史滚动数据仅单次使用即被丢弃。这导致监督信号噪声大、样本效率低下及策略更新次优等问题。本研究通过将RLVR中的滚动调度构建为情境化多臂赌博机问题，提出统一的神经调度框架，能在训练过程中自适应筛选高价值滚动数据。每个滚动数据被视作一个赌博臂，其奖励由连续优化步骤间的性能增益定义。该调度器在统一理论框架内同时支持噪声感知的组内筛选与历史滚动数据的自适应全局复用。我们通过推导次线性遗憾界证明理论合理性，并表明扩大滚动缓冲区可提升性能上限。在六个数学推理基准测试上的实验表明，该框架在多种RLVR优化方法中均能持续提升性能与训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses inefficiencies in Reinforcement Learning with Verifiable Rewards (RLVR) for large language models, where existing methods treat all rollouts uniformly and discard them after single use, leading to noisy supervision and poor sample efficiency. The authors propose formulating rollout scheduling as a contextual bandit problem and introduce a neural scheduling framework that adaptively selects high-value rollouts by defining each rollout&#x27;s reward as the performance gain between optimization steps, enabling noise-aware selection and adaptive reuse of historical data. Theoretical analysis shows sublinear regret bounds and performance improvements with larger rollout buffers, and experiments on six mathematical reasoning benchmarks confirm consistent gains in both model performance and training efficiency across multiple RLVR optimization methods.</div>
<div class="mono" style="margin-top:8px">本文针对基于可验证奖励的强化学习在大型语言模型中存在的低效问题，现有方法对所有模拟结果进行无差别处理且仅单次使用，导致监督信号噪声大、样本效率低。研究者将模拟调度问题形式化为上下文赌博机问题，提出一个神经调度框架，通过将每个模拟结果的奖励定义为连续优化步骤间的性能增益，自适应地选择高价值模拟，从而支持噪声感知的组内选择和历史数据的自适应全局重用。理论分析给出了次线性遗憾界，并表明扩大模拟缓冲区可提升性能上限；在六个数学推理基准上的实验验证了该方法在多种RLVR优化方法中均能一致提升模型性能和训练效率。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Correctness: Learning Robust Reasoning via Transfer</div>
<div class="meta-line">Authors: Hyunseok Lee, Soheil Abbasloo, Jihoon Tack, Jinwoo Shin</div>
<div class="meta-line">First: 2026-02-09T10:41:44+00:00 · Latest: 2026-02-09T10:41:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08489v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08489v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR&#x27;s average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越正确性：通过迁移学习实现鲁棒推理</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）近期增强了大型语言模型的推理能力，但其对最终答案正确性的关注存在关键缺陷：无法确保推理过程本身的鲁棒性。我们采用一种简洁的哲学观点——鲁棒推理应在其产生者之外保持效用，并将推理视为一种必须经受截断、重释与延续的意义迁移形式。基于此原则，我们提出可迁移奖励强化学习（RLTR），通过迁移奖励将鲁棒性操作化：测试来自某模型的局部推理前缀能否引导另一模型得出正确答案。该方法促使大型语言模型产生稳定、可解释且真正可泛化的推理过程。我们的方法在提升采样一致性的同时提高了最终答案准确率，且能以显著更少的训练步骤达到可比性能。例如在MATH500数据集上，RLTR相比RLVR在Maj@64指标上获得+3.6%的提升，并以约2.5倍更少的训练步骤达到RLVR的平均准确率，既提供了更可靠的推理，也实现了显著的样本效率提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation that existing Reinforcement Learning with Verifiable Rewards (RLVR) focuses only on final answer correctness without ensuring the robustness of the reasoning process itself, this paper introduces Reinforcement Learning with Transferable Reward (RLTR) to promote stable and generalizable reasoning. The method operationalizes robustness by testing whether a partial reasoning prefix from one model can effectively guide a separate model to the correct answer, thereby encouraging reasoning that survives truncation and reinterpretation. Experimental results show that RLTR improves sampling consistency and final answer accuracy, achieving a +3.6%p gain in Maj@64 on MATH500 compared to RLVR and matching RLVR&#x27;s average accuracy with roughly 2.5x fewer training steps, demonstrating both more reliable reasoning and greater sample efficiency.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有基于可验证奖励的强化学习（RLVR）仅关注最终答案的正确性，而未能确保推理过程本身的鲁棒性，因此提出了基于可转移奖励的强化学习（RLTR）来促进稳定且可泛化的推理。该方法通过测试一个模型的部分推理前缀是否能有效指导另一个模型得出正确答案，从而将鲁棒性操作化，鼓励推理能够经受截断和重新解释。实验结果表明，RLTR提高了采样一致性和最终答案准确率，在MATH500数据集上相比RLVR在Maj@64指标上获得了+3.6%p的提升，并以大约2.5倍更少的训练步数达到了RLVR的平均准确率，展现了更可靠的推理和更高的样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping</div>
<div class="meta-line">Authors: Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-26T05:03:54+00:00 · Latest: 2026-02-09T10:15:07+00:00</div>
<div class="meta-line">Comments: ICLR 2026 camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21880v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.21880v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bltnynk.github.io/publications/rl-zvp/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward -- so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce Reinforcement Learning with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR. The project page is available at https://bltnynk.github.io/publications/rl-zvp/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不遗漏任何提示：通过熵引导优势塑造利用大语言模型强化学习中的零方差提示</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）是提升大语言模型（LLM）推理能力的强大框架。然而，当前方法（如GRPO）仅依赖模型对同一输入产生不同正确性响应的任务，而忽略了所有响应获得相同奖励的所谓“零方差提示”。本研究主张此类提示并非无用，反而能为策略优化提供有效反馈。为此，我们提出零方差提示强化学习（RL-ZVP）算法，从零方差提示中提取学习信号。RL-ZVP无需对比响应即可直接奖励正确行为并惩罚错误，同时通过词元级特征调制反馈以保留信息丰富的细微信号。在六项数学推理基准测试中，RL-ZVP相比GRPO在准确率上最高提升8.61个百分点，通过率提升7.77个百分点，且持续优于过滤零方差提示的其他基线方法。这些结果揭示了RLVR中零方差提示尚未开发的潜力。项目页面详见：https://bltnynk.github.io/publications/rl-zvp/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses a limitation in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models, where existing methods like GRPO ignore &#x27;zero-variance prompts&#x27;—inputs where all model responses receive identical rewards. The authors propose RL-ZVP, a novel algorithm that extracts learning signals from these prompts by directly rewarding correctness and penalizing errors without requiring contrasting responses, using token-level characteristics to modulate feedback. Experimental results across six math reasoning benchmarks show RL-ZVP achieves accuracy improvements up to 8.61 points and pass rate gains up to 7.77 points over GRPO, consistently outperforming baselines that discard zero-variance prompts, demonstrating their untapped potential for policy optimization.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型可验证奖励强化学习中的一个局限：现有方法如GRPO忽略了“零方差提示”——即模型所有响应获得相同奖励的输入。作者提出RL-ZVP算法，通过直接奖励正确性和惩罚错误（无需对比响应），并利用词元级特征调节反馈，从这些提示中提取学习信号。在六个数学推理基准上的实验结果表明，RL-ZVP相比GRPO在准确率上最高提升8.61分，通过率最高提升7.77分，且持续优于丢弃零方差提示的基线方法，证明了这类提示在策略优化中未被开发的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Intelligent support for Human Oversight: Integrating Reinforcement Learning with Gaze Simulation to Personalize Highlighting</div>
<div class="meta-line">Authors: Thorsten Klößner, João Belo, Zekun Wu, Jörg Hoffmann, Anna Maria Feit</div>
<div class="meta-line">First: 2026-02-09T09:04:48+00:00 · Latest: 2026-02-09T09:04:48+00:00</div>
<div class="meta-line">Comments: AI CHAOS &#x27;26: Workshop Series on the Challenges for Human Oversight of AI Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08403v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08403v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interfaces for human oversight must effectively support users&#x27; situation awareness under time-critical conditions. We explore reinforcement learning (RL)-based UI adaptation to personalize alerting strategies that balance the benefits of highlighting critical events against the cognitive costs of interruptions. To enable learning without real-world deployment, we integrate models of users&#x27; gaze behavior to simulate attentional dynamics during monitoring. Using a delivery-drone oversight scenario, we present initial results suggesting that RL-based highlighting can outperform static, rule-based approaches and discuss challenges of intelligent oversight support.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类监督的智能支持：集成强化学习与视线模拟实现个性化高亮</div>
<div class="mono" style="margin-top:8px">人类监督界面必须在时间紧迫条件下有效支持用户的情境感知。本研究探索基于强化学习（RL）的界面自适应方法，通过个性化预警策略来平衡关键事件高亮的益处与认知中断的代价。为实现在无需实际部署的情况下进行学习，我们整合用户视线行为模型以模拟监控过程中的注意力动态。以配送无人机监督场景为例，初步结果表明基于强化学习的高亮策略优于静态的基于规则方法，并讨论了智能监督支持面临的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for human oversight interfaces that enhance situation awareness in time-critical settings while managing the cognitive load of interruptions. The method employs reinforcement learning to personalize alert highlighting strategies, using simulated models of user gaze behavior to approximate attentional dynamics without requiring real-world deployment. In a delivery-drone oversight scenario, initial experimental results indicate that this RL-based approach can surpass static, rule-based highlighting methods, though challenges in intelligent oversight support are acknowledged.</div>
<div class="mono" style="margin-top:8px">本文旨在为时间紧迫环境下的人类监督界面提供支持，以增强情境感知并管理中断带来的认知负担。方法采用强化学习来个性化警报高亮策略，通过模拟用户注视行为模型来近似注意力动态，无需实际部署。在配送无人机监督场景中，初步实验结果表明，基于强化学习的高亮方法优于静态的基于规则的方法，但智能监督支持仍面临挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning</div>
<div class="meta-line">Authors: Zhuoen Chen, Dongfang Li, Meishan Zhang, Baotian Hu, Min Zhang</div>
<div class="meta-line">First: 2026-02-09T08:33:11+00:00 · Latest: 2026-02-09T08:33:11+00:00</div>
<div class="meta-line">Comments: 26 pages, 7 figures. Code and models will be released</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08382v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08382v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于端到端强化学习的压缩记忆动态长上下文推理</div>
<div class="mono" style="margin-top:8px">大语言模型在长上下文处理中面临显著挑战，包括二次计算成本、信息遗忘以及检索增强生成固有的上下文碎片化问题。我们提出一种受认知启发的框架，通过分块压缩和选择性记忆召回实现高效长上下文推理，而非处理所有原始词元。该框架将长输入分段为块，使用学习型压缩器将每个块编码为压缩记忆表示。门控模块动态选择相关记忆块，随后由推理模块通过演化的短期记忆进行迭代处理以解决下游任务。压缩器与推理器通过端到端强化学习联合优化，门控模块则作为分类器单独训练。实验结果表明，该方法在RULER-HQA等多跳推理基准上达到竞争性准确率，上下文长度从7K词元外推至1.75M词元，相比强长上下文基线具有更优的准确率-效率权衡。特别地，相较于MemAgent，其峰值GPU内存使用降低达2倍，推理速度提升达6倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the computational inefficiency and information loss challenges faced by Large Language Models (LLMs) in processing long contexts, moving beyond retrieval-augmented generation. The proposed method introduces a framework that segments long inputs into chunks, compresses them into memory representations, and uses a gating module to dynamically select relevant memories for iterative reasoning with an evolving working memory. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning. Experiments demonstrate competitive accuracy on multi-hop reasoning benchmarks like RULER-HQA, the ability to extrapolate context length from 7K to 1.75M tokens, and significant efficiency gains, including up to a 2x reduction in peak GPU memory usage and a 6x inference speedup compared to MemAgent.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型在处理长上下文时存在的计算成本高、信息遗忘以及检索增强生成中的上下文碎片化问题，提出了一种认知启发的推理框架。该方法将长输入分割为块，通过学习的压缩器将其编码为压缩记忆表示，并利用门控模块动态选择相关记忆块，由推理模块结合演化的短期记忆进行迭代处理以解决下游任务。压缩器和推理器通过端到端强化学习联合优化。实验结果表明，该方法在RULER-HQA等多跳推理基准上取得了有竞争力的准确率，能将上下文长度从7K外推至175万词元，并在效率上优于现有长上下文基线，实现了峰值GPU内存使用降低2倍、推理速度比MemAgent提升6倍。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning with Backtracking Feedback</div>
<div class="meta-line">Authors: Bilgehan Sel, Vaishakh Keshava, Phillip Wallis, Lukas Rutishauser, Ming Jin, Dingcheng Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-02-09T08:23:19+00:00 · Latest: 2026-02-09T08:23:19+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08377v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08377v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model&#x27;s live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient &quot;backtrack by x tokens&quot; signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于回溯反馈的强化学习</div>
<div class="mono" style="margin-top:8px">针对大语言模型（LLMs）在对抗性攻击和分布内错误方面对鲁棒安全性的迫切需求，我们提出了基于回溯反馈的强化学习（RLBF）框架。该框架在BSAFE等现有方法基础上，主要通过强化学习阶段使模型学会动态修正自身生成错误。通过基于模型实时输出的批评者反馈进行强化学习，LLMs能够识别并恢复实际发生的安全违规行为，具体方式是发出高效的“回溯x个词元”信号后继续自回归生成。这一强化学习过程对于抵御包括中间填充、贪婪坐标梯度（GCG）攻击和解码参数操纵在内的复杂对抗策略至关重要。为增强回溯能力的习得，我们还提出了改进的监督微调数据生成策略（BSAFE+）。该方法通过向原本安全的连贯文本中注入违规内容，改进了现有数据生成技术，为回溯机制提供了更有效的初始训练。综合实验评估表明，RLBF在不同基准测试和模型规模上显著降低了攻击成功率，在保持基础模型功能的前提下实现了卓越的安全性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance the safety of Large Language Models against adversarial attacks and in-distribution errors, this paper introduces Reinforcement Learning with Backtracking Feedback (RLBF), a framework that advances prior methods by employing a Reinforcement Learning stage where models learn to dynamically correct their own generation errors through critic feedback and a &quot;backtrack&quot; signal. The method is supported by an enhanced Supervised Fine-Tuning data generation strategy, BSAFE+, which creates more effective training data by injecting violations into safe text. Experimental results show that RLBF significantly reduces attack success rates across various benchmarks and model scales while preserving model utility.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升大语言模型对抗对抗性攻击和分布内错误的安全性，提出了带回溯反馈的强化学习框架，该方法通过强化学习阶段，利用对模型实时输出的评判反馈，让模型学会动态纠正自身生成错误，并发出高效的回溯信号。为支持此能力，还提出了一种改进的监督微调数据生成策略，通过向原本安全的文本中注入违规内容来创建更有效的训练数据。综合实验评估表明，该框架在不同基准和模型规模上显著降低了攻击成功率，在保持模型基础效用的同时实现了更优的安全性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Human-Like Badminton Skills for Humanoid Robots</div>
<div class="meta-line">Authors: Yeke Chen, Shihao Dong, Xiaoyu Ji, Jingkai Sun, Zeren Luo, Liu Zhao, Jiahui Zhang, Wanyue Li, Ji Ma, Bowen Xu, Yimin Han, Yudong Zhao, Peng Lu</div>
<div class="meta-line">First: 2026-02-09T08:09:52+00:00 · Latest: 2026-02-09T08:09:52+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08370v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08370v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a &quot;mimic&quot; to a capable &quot;striker.&quot; Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>仿人机器人学习类人羽毛球技能</div>
<div class="mono" style="margin-top:8px">在羽毛球等高强度运动中实现类人的多样化表现，对仿人机器人而言仍是一项艰巨挑战。与标准移动或静态操控不同，此任务要求爆发性全身协调与精准、时机关键的击球动作无缝融合。尽管近期进展已实现逼真的运动模仿，但在不牺牲风格自然性的前提下，弥合运动学模仿与功能性、物理感知击球之间的差距仍非易事。为此，我们提出“模仿到交互”这一渐进式强化学习框架，旨在使机器人从“模仿者”进化为能干的“击球手”。该方法从人类数据中建立稳健的运动先验，将其提炼为紧凑的基于模型的状态表示，并通过对抗性先验稳定动力学。关键的是，为克服专家示范的稀疏性，我们引入流形扩展策略，将离散击球点泛化为密集的交互空间。我们通过在仿真中掌握包括高远球和吊球在内的多种技能验证了该框架。此外，我们首次实现了类人羽毛球技能的零样本仿真到现实迁移，在物理世界中成功复现了人类运动员的动力学优雅性与功能精确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to enable humanoid robots to perform complex, human-like badminton skills, which require explosive whole-body coordination and precise timing, a challenge beyond standard locomotion. The method introduces Imitation-to-Interaction, a progressive reinforcement learning framework that builds a motor prior from human data, distills it into a model-based state representation, stabilizes dynamics with adversarial priors, and uses manifold expansion to generalize sparse demonstrations into a dense interaction volume. Main experimental results show mastery of diverse skills like lifts and drop shots in simulation, along with the first zero-shot sim-to-real transfer to a physical humanoid robot, replicating human athletic elegance and precision.</div>
<div class="mono" style="margin-top:8px">该研究的动机是实现人形机器人执行复杂、类人的羽毛球技能，这需要爆发性的全身协调和精确时机，超越了标准运动任务。方法上提出了模仿到交互的渐进强化学习框架，从人类数据建立运动先验，将其提炼为基于模型的状态表示，通过对抗先验稳定动力学，并利用流形扩展将稀疏示范泛化为密集交互空间。主要实验结果包括在仿真中掌握了挑球和吊球等多种技能，并首次实现了零样本从仿真到真实人形机器人的迁移，在物理世界中复现了人类运动员的动感优雅和功能精度。</div>
</details>
</div>
<div class="card">
<div class="title">GUI Knowledge Bench: Revealing the Knowledge Gap of VLMs in GUI Tasks</div>
<div class="meta-line">Authors: Chenrui Shi, Zedong Yu, Zhi Gao, Ruining Feng, Enqi Liu, Yuwei Wu, Yunde Jia, Liuyu Xiang, Zhaofeng He, Qing Li</div>
<div class="meta-line">First: 2025-10-30T03:22:30+00:00 · Latest: 2026-02-09T07:49:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26098v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26098v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision language models (VLMs) have advanced graphical user interface (GUI) task automation but still lag behind humans. We hypothesize this gap stems from missing core GUI knowledge, which existing training schemes (such as supervised fine tuning and reinforcement learning) alone cannot fully address. By analyzing common failure patterns in GUI task execution, we distill GUI knowledge into three dimensions: (1) interface knowledge about widget functions, layout semantics, and system states; (2) interaction knowledge about GUI interaction types and effects; and (3) procedure knowledge of task objectives and workflow sequences. We further introduce GUI Knowledge Bench, a benchmark with multiple-choice and yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux, IOS) and 292 applications. Our evaluation indicates that current VLMs are generally aware of the functions of individual widgets, but lack the GUI-specific knowledge required to track system states, adhere to GUI interaction conventions, and assess task completion progress. Experiments on real-world GUI tasks further validate the close link between GUI knowledge and task success. By providing a structured framework for assessing GUI knowledge, our work supports the selection of VLMs with greater potential prior to downstream training and provides insights for building more capable GUI agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GUI知识基准：揭示视觉语言模型在GUI任务中的知识差距</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）在图形用户界面（GUI）任务自动化方面取得进展，但仍落后于人类。我们假设这一差距源于缺乏核心GUI知识，而现有训练方案（如监督微调和强化学习）无法单独解决。通过分析GUI任务执行中的常见失败模式，我们将GUI知识提炼为三个维度：（1）关于控件功能、布局语义和系统状态的界面知识；（2）关于GUI交互类型与效果的交互知识；（3）任务目标与工作流程序列的过程知识。我们进一步推出GUI知识基准，该基准涵盖六大平台（Web、Android、MacOS、Windows、Linux、iOS）和292个应用程序，包含选择题和是非题。评估表明，当前VLM普遍了解单个控件的功能，但缺乏跟踪系统状态、遵循GUI交互惯例及评估任务完成进度所需的GUI专项知识。真实场景GUI任务实验进一步验证了GUI知识与任务成功率之间的紧密关联。通过提供评估GUI知识的结构化框架，本研究支持在下游训练前筛选更具潜力的VLM，并为构建更强大的GUI智能体提供洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the performance gap between vision language models (VLMs) and humans in graphical user interface (GUI) task automation, hypothesizing that VLMs lack essential GUI-specific knowledge not fully remedied by standard training methods. The authors analyze common VLM failures to define three knowledge dimensions—interface, interaction, and procedure knowledge—and introduce the GUI Knowledge Bench, a benchmark with multiple-choice and yes/no questions spanning six platforms and 292 applications. Experimental results reveal that while VLMs understand basic widget functions, they struggle with tracking system states, following GUI interaction conventions, and evaluating task progress, with real-world task experiments confirming that GUI knowledge directly impacts success rates, offering a framework for selecting VLMs and guiding future GUI agent development.</div>
<div class="mono" style="margin-top:8px">本文针对视觉语言模型（VLMs）在图形用户界面（GUI）任务自动化中与人类存在的性能差距，提出假设认为VLMs缺乏标准训练方法无法完全弥补的核心GUI知识。作者通过分析VLM的常见失败模式，将GUI知识归纳为三个维度——界面知识、交互知识和流程知识，并引入了GUI知识基准测试，该基准包含涵盖六个平台和292个应用程序的多项选择和是非题。实验结果表明，尽管VLMs能理解基本控件功能，但在跟踪系统状态、遵循GUI交互惯例以及评估任务进度方面存在不足，真实GUI任务实验进一步验证了GUI知识与任务成功率之间的紧密联系，为选择VLMs和构建更强大的GUI智能体提供了结构化评估框架和见解。</div>
</details>
</div>
<div class="card">
<div class="title">CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards</div>
<div class="meta-line">Authors: Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai</div>
<div class="meta-line">First: 2025-10-09T17:50:26+00:00 · Latest: 2026-02-09T07:41:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08529v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08529v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent&#x27;s policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoMAS：基于交互奖励的协同进化多智能体系统</div>
<div class="mono" style="margin-top:8px">自我进化是推动基于大语言模型（LLM）的智能体在预训练后持续提升能力的核心研究课题。近期研究呈现出从无强化学习（RL）方法向基于RL方法的转变。现有基于RL的方法要么依赖密集的外部奖励信号，要么从LLM自身提取内在奖励信号。然而，这些方法均偏离了人类智能中通过相互讨论与协作实现学习进化的机制。本研究提出协同进化多智能体系统（CoMAS），该创新框架使智能体能够在无外部监督的情况下，通过智能体间交互实现自主学习进化。CoMAS从丰富的讨论动态中生成内在奖励，采用LLM作为评判机制来构建奖励函数，并通过RL优化各智能体策略，从而实现去中心化、可扩展的协同进化。实验结果表明，CoMAS在多数评估场景中持续优于未经训练的智能体，并达到最先进的性能水平。消融研究证实了基于交互的奖励信号的必要性，并揭示了随着智能体数量与多样性增加，系统展现出良好的可扩展性。这些发现确立了CoMAS作为LLM智能体自我进化领域新颖且有效的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the divergence of current reinforcement learning (RL)-based self-evolution methods for LLM agents from human collaborative learning, this paper introduces CoMAS, a framework for agent co-evolution through interaction rewards. The method enables agents to autonomously improve by generating intrinsic rewards from inter-agent discussions using an LLM-as-a-judge mechanism and optimizing policies via RL. Experimental results show CoMAS consistently outperforms untrained baselines and achieves state-of-the-art performance in most settings, with ablation studies confirming the necessity of interaction-based rewards and demonstrating promising scalability with more diverse agents.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有基于强化学习的大语言模型智能体自我进化方法偏离了人类通过协作讨论学习的机制，因此提出了CoMAS框架，通过交互奖励实现智能体协同进化。该方法让智能体通过相互讨论生成内在奖励，利用大语言模型作为评判机制，并通过强化学习优化策略，从而实现去中心化、可扩展的协同进化。实验结果表明，CoMAS持续优于未经训练的智能体，在多数评估设置中达到最先进性能，消融研究证实了基于交互的奖励信号的必要性，并揭示了随着智能体数量和多样性增加，该方法具有良好的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Does Your Reasoning Model Implicitly Know When to Stop Thinking?</div>
<div class="meta-line">Authors: Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuanda Wang, Zhixia Zhang, Hongyan Xie, Songshi Liang, Zehao Chen, Xuefeng Xiao, Fuzhen Zhuang, Jianxin Li, Yikun Ban, Deqing Wang</div>
<div class="meta-line">First: 2026-02-09T07:38:22+00:00 · Latest: 2026-02-09T07:38:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08354v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08354v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你的推理模型是否隐含知晓何时停止思考？</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过长链思维（CoTs）在复杂推理任务上的能力已显著提升，但该方法常导致大量冗余，损害计算效率并造成实时应用的显著延迟。近期研究表明，更长的推理链常与正确性无关，甚至可能损害准确性。在对这一现象的深入分析中，我们意外发现并实证验证了LRMs隐含知晓何时停止思考的能力，而这一能力被当前采样范式所掩盖。受此启发，我们提出了SAGE（自我感知引导高效推理），一种释放这种高效推理潜力的新型采样范式。此外，将SAGE作为混合采样整合到基于群体的强化学习（SAGE-RL）中，使SAGE-RL能够将SAGE发现的高效推理模式有效融入标准pass@1推理，在多个高难度数学基准测试中显著提升了LRMs的推理准确性和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the inefficiency of long reasoning chains in large reasoning models (LRMs), noting that extended chains often introduce redundancy without improving accuracy and can even harm performance. The authors discover that LRMs inherently possess an implicit ability to determine when to stop reasoning, but this capability is suppressed by conventional sampling methods. To address this, they propose SAGE, a novel sampling paradigm that activates this self-aware stopping mechanism, and further enhance it with SAGE-RL, a reinforcement learning approach that integrates efficient reasoning patterns into standard inference. Experimental results on multiple mathematical benchmarks demonstrate that SAGE-RL significantly boosts both the accuracy and computational efficiency of LRMs.</div>
<div class="mono" style="margin-top:8px">本文研究大型推理模型中长推理链的效率问题，指出过长的推理链常导致冗余，不仅无助于提升准确性，甚至可能损害性能。作者发现，这些模型隐含地具备判断何时停止推理的能力，但这一能力被现有采样方法所掩盖。为此，他们提出了SAGE这一新采样范式，以激发这种自感知的停止机制，并进一步结合强化学习开发了SAGE-RL，将高效推理模式融入标准推理过程。在多个数学基准测试上的实验结果表明，SAGE-RL显著提高了大型推理模型的准确性和计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs</div>
<div class="meta-line">Authors: Zhiliang Chen, Alfred Wei Lun Leong, Shao Yong Ong, Apivich Hemachandram, Gregory Kang Ruey Lau, Chuan-Sheng Foo, Zhengyuan Liu, Nancy F. Chen, Bryan Kian Hsiang Low</div>
<div class="meta-line">First: 2026-02-09T07:33:40+00:00 · Latest: 2026-02-09T07:33:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08351v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08351v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS&#x27;s average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鸡与蛋的困境：大语言模型数据与模型配置的协同优化</div>
<div class="mono" style="margin-top:8px">为大语言模型训练协同优化数据与模型配置呈现出一个经典的鸡与蛋困境：下游任务的最佳训练数据配置（如数据混合比例）取决于所选模型配置（如模型架构），反之亦然。然而，联合优化数据与模型配置常被视为难以处理，现有方法多聚焦于单一维度的优化而忽略其相互作用。本文提出JoBS方法，利用基于缩放定律的性能预测器辅助贝叶斯优化，高效实现大语言模型训练数据与模型配置的联合优化。JoBS将部分优化预算用于学习大语言模型性能预测器，该预测器可通过少量训练步骤评估训练配置的潜力；剩余预算则完全通过预测器执行贝叶斯优化，从而有效分摊完整训练成本。我们分析了JoBS的平均遗憾值并设计最优预算分配方案以最小化遗憾。在相同优化预算下，JoBS在多种大语言模型任务中均优于现有多保真度贝叶斯优化基线及单维度优化方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the interdependent challenge of co-optimizing data and model configurations for LLMs, a chicken-and-egg problem where each optimal configuration depends on the other. The authors propose JoBS, a method that uses a scaling-law-inspired performance predictor to guide Bayesian optimization, allowing it to efficiently explore the joint configuration space by predicting final performance from early training steps. Experimental results demonstrate that JoBS outperforms existing multi-fidelity Bayesian optimization baselines and isolated data or model optimization approaches across various LLM tasks under a fixed computational budget.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型中数据和模型配置相互依赖、难以协同优化的“先有鸡还是先有蛋”难题展开研究。作者提出了JoBS方法，该方法利用基于缩放定律的性能预测器来指导贝叶斯优化，通过早期训练步骤预测最终性能，从而高效探索联合配置空间。实验结果表明，在相同的优化预算下，JoBS在多种大语言模型任务上均优于现有的多保真度贝叶斯优化基线以及孤立的数据或模型优化方法。</div>
</details>
</div>
<div class="card">
<div class="title">OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration</div>
<div class="meta-line">Authors: Qi Guo, Jianing Wang, Deyang Kong, Xiangyu Xi, Jianfei Zhang, Yi Lu, Jingang Wang, Wei Wang, Shikun Zhang, Wei Ye</div>
<div class="meta-line">First: 2026-02-09T07:29:13+00:00 · Latest: 2026-02-09T07:29:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08344v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08344v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OPE：通过大纲引导路径探索克服并行思维中的信息饱和问题</div>
<div class="mono" style="margin-top:8px">并行思维已成为大型推理模型处理复杂问题的新范式。现有方法多采用强化学习优化并行思维，以克服监督微调在计算资源与效果上的局限，但研究主要聚焦聚合阶段优化，对路径探索阶段关注不足。本文在可验证奖励强化学习框架下理论分析了并行思维的优化瓶颈，指出探索路径间的互信息瓶颈是制约性能的根本因素。为此，我们提出大纲引导路径探索方法，通过在并行路径推理前生成多样化推理大纲来显式划分解空间，从而降低信息冗余并提升跨路径信息多样性。我们采用迭代强化学习策略分别优化大纲规划与大纲引导推理，在多个高难度数学基准上的实验表明，该方法能有效提升不同聚合策略下的推理性能，使大型推理模型更可靠地发现正确解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to enhance parallel thinking in large reasoning models, where existing reinforcement learning approaches often focus on solution aggregation while neglecting inefficiencies in the path exploration stage, leading to information saturation. The proposed method, Outline-Guided Path Exploration (OPE), addresses this by first generating diverse reasoning outlines to partition the solution space, followed by parallel path reasoning guided by these outlines, using an iterative reinforcement learning strategy to separately optimize outline planning and reasoning. Experimental results across multiple challenging mathematical benchmarks show that OPE effectively improves reasoning performance and solution discovery reliability under various aggregation strategies.</div>
<div class="mono" style="margin-top:8px">本文的动机在于改进大型推理模型中的并行思维范式，现有强化学习方法多聚焦于答案聚合阶段，而忽视了路径探索中的信息饱和问题，这限制了整体性能。为此，作者提出了提纲引导的路径探索方法，通过预先生成多样化的推理提纲来划分解空间，从而减少信息冗余并提升路径间的信息多样性，采用迭代强化学习策略分别优化提纲规划和提纲引导的推理。在多个高难度数学基准测试上的实验结果表明，该方法能有效提升不同聚合策略下的推理性能，使模型更可靠地发现正确解。</div>
</details>
</div>
<div class="card">
<div class="title">Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System</div>
<div class="meta-line">Authors: Yanming Li, Xuelin Zhang, WenJie Lu, Ziye Tang, Maodong Wu, Haotian Luo, Tongtong Wu, Zijie Peng, Hongze Mi, Yibo Feng, Naiqiang Tan, Chao Huang, Hong Chen, Li Shen</div>
<div class="meta-line">First: 2026-02-09T07:17:28+00:00 · Latest: 2026-02-09T07:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08335v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08335v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>谁应得奖励？SHARP：基于Shapley值信度分配的多智能体系统优化</div>
<div class="mono" style="margin-top:8px">通过多智能体系统将大语言模型与外部工具集成，为分解和解决复杂问题提供了前景广阔的新范式。然而，由于信度分配难题，这类系统的训练仍极具挑战——决策轨迹的成功或失败往往难以归因于具体功能智能体。现有方法通常依赖稀疏或全局广播的奖励，无法捕捉个体贡献，导致强化学习效率低下。为突破这些局限，我们提出基于Shapley值的分层信度分配强化策略（SHARP），该创新框架通过精确信度分配优化多智能体强化学习。SHARP通过轨迹组间智能体优势值归一化有效稳定训练过程，其核心在于分解式奖励机制：包含全局广播精度奖励、基于Shapley值的个体边际信度奖励，以及提升执行效率的工具流程奖励。在多个现实场景基准测试中的广泛实验表明，SHARP显著优于当前最先进基线方法，相较单智能体与多智能体方法分别实现平均23.66%和14.05%的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the credit assignment challenge in multi-agent systems integrated with Large Language Models, where sparse or global rewards fail to capture individual agent contributions, hindering efficient reinforcement learning. The authors propose SHARP, a framework that optimizes multi-agent reinforcement learning through precise credit attribution using a decomposed reward mechanism, which includes a global accuracy reward, a Shapley-based marginal-credit reward per agent, and a tool-process reward for execution efficiency. Experimental results across real-world benchmarks show that SHARP significantly outperforms state-of-the-art baselines, achieving average match improvements of 23.66% over single-agent and 14.05% over multi-agent approaches.</div>
<div class="mono" style="margin-top:8px">本文针对集成大型语言模型的多智能体系统中的信用分配难题，指出稀疏或全局奖励无法有效衡量个体贡献，导致强化学习效率低下。作者提出了SHARP框架，通过分解的奖励机制优化多智能体强化学习，该机制包括全局准确性奖励、基于Shapley值的个体边际信用奖励以及提升执行效率的工具过程奖励。在多个现实世界基准测试上的实验结果表明，SHARP显著优于现有先进方法，相比单智能体和多智能体方法分别实现了23.66%和14.05%的平均匹配提升。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression</div>
<div class="meta-line">Authors: Yuntian Tang, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie, Wenxi Li, Wei Li, Jie Hu, Xinghao Chen, Rongrong Ji, Shaohui Lin</div>
<div class="meta-line">First: 2026-02-09T06:57:15+00:00 · Latest: 2026-02-09T06:57:15+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08324v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08324v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高效大语言推理模型的极端比例思维链压缩方法</div>
<div class="mono" style="margin-top:8px">思维链推理虽成功增强了大语言模型的推理能力，但其推断过程会产生巨大的计算开销。现有思维链压缩方法在高压缩比下常出现逻辑保真度的严重损失，导致性能显著下降。为实现高保真、快速推理，我们提出一种名为Extra-CoT的新型极端比例思维链压缩框架，该框架在保持答案准确性的同时大幅降低令牌预算。为生成可靠的高保真监督信号，我们首先在具有细粒度标注的数学思维链数据上训练专用的语义保持压缩器，随后通过混合比例监督微调在大语言模型上对这些压缩对进行微调，使其适应不同压缩预算，并为强化学习提供稳定初始化。我们进一步提出约束分层比例策略优化方法，通过分层奖励机制显式激励模型在更低预算下保持解题能力。在三个数学推理基准测试上的实验证明了Extra-CoT的优越性。例如，在Qwen3-1.7B模型上对MATH-500数据集进行测试时，Extra-CoT实现了超过73%的令牌缩减，同时准确率提升0.6%，显著优于现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the substantial computational overhead incurred by Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs), as existing compression methods often lose logical fidelity at high ratios, degrading performance. The method introduces Extra-CoT, a framework that first trains a semantically-preserved compressor on annotated mathematical CoT data to generate high-fidelity supervision, then fine-tunes an LLM via mixed-ratio supervised fine-tuning, followed by Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving under lower token budgets. Experimental results on three mathematical reasoning benchmarks demonstrate superiority, such as on MATH-500 with Qwen3-1.7B, achieving over 73% token reduction and a 0.6% accuracy improvement, significantly outperforming state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大语言模型中思维链推理带来的巨大计算开销问题，因为现有压缩方法在高压缩比下常丢失逻辑保真度，导致性能下降。方法上提出了Extra-CoT框架，首先在带细粒度标注的数学思维链数据上训练一个语义保持的压缩器以生成高保真监督信号，然后通过混合比例监督微调对大语言模型进行微调，并进一步采用约束分层比例策略优化来显式激励低预算下的问题解决能力。在三个数学推理基准上的实验结果表明了其优越性，例如在MATH-500上使用Qwen3-1.7B模型，实现了超过73%的令牌减少和0.6%的准确率提升，显著优于现有最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">TritonRL: Training LLMs to Think and Code Triton Without Cheating</div>
<div class="meta-line">Authors: Jiin Woo, Shaowei Zhu, Allen Nie, Zhen Jia, Yida Wang, Youngsuk Park</div>
<div class="meta-line">First: 2025-10-18T21:36:10+00:00 · Latest: 2026-02-09T06:15:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17891v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17891v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid evolution of Large Language Models (LLMs) has driven a growing demand for automated, high-performance system kernels to accelerate machine learning workloads. We introduce TritonRL, a domain-specialized 8B-scale LLM for Triton programming, trained via a novel reinforcement learning (RL) framework. While Triton synthesis faces unique challenges, including data scarcity and a high susceptibility to reward hacking, our approach enables robust kernel generation through two primary innovations. First, we implement a multi-layered verification system that provides high-fidelity reward signals, ensuring that generated kernels are both syntactically and functionally valid. Second, we propose Hierarchical Reward Decomposition (HRD), which decouples reinforcement for high-level reasoning and low-level implementation to resolve the credit assignment problem in long-sequence generation. Comprehensive evaluations on KernelBench demonstrate that TritonRL achieves state-of-the-art correctness and runtime speedup, outperforming concurrent Triton-specific models and matching the performance of frontier models with over 100B parameters. Our results highlight the effectiveness of hardware-aware RL paradigms in specialized domain adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TritonRL：训练大语言模型无作弊地思考与编写Triton代码</div>
<div class="mono" style="margin-top:8px">大语言模型的快速发展推动了对自动化高性能系统内核以加速机器学习工作负载的迫切需求。我们推出TritonRL——一个专精于Triton编程的80亿参数规模大语言模型，通过创新的强化学习框架训练而成。尽管Triton代码合成面临数据稀缺和奖励破解风险高等独特挑战，我们的方法通过两项核心创新实现了稳健的内核生成：首先，我们构建了多层验证系统，提供高保真奖励信号，确保生成的内核在语法和功能上均有效；其次，我们提出分层奖励分解机制，将高层推理与底层实现的强化信号解耦，以解决长序列生成中的信用分配问题。在KernelBench上的综合评估表明，TritonRL在正确性和运行加速方面达到最先进水平，超越同期Triton专用模型，并匹配参数量超千亿的前沿模型性能。我们的研究成果凸显了硬件感知强化学习范式在专业领域适配中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for automated, high-performance system kernels to accelerate machine learning workloads, this paper introduces TritonRL, an 8B-scale LLM specialized for Triton programming, trained via a novel reinforcement learning framework. The method addresses challenges like data scarcity and reward hacking through a multi-layered verification system for high-fidelity rewards and Hierarchical Reward Decomposition to separate reasoning from implementation credit. Experimental results on KernelBench show TritonRL achieves state-of-the-art correctness and runtime speedup, outperforming Triton-specific models and matching the performance of frontier models with over 100B parameters, demonstrating the effectiveness of hardware-aware RL for domain adaptation.</div>
<div class="mono" style="margin-top:8px">本文旨在满足机器学习工作负载对自动化高性能系统内核的需求，提出了TritonRL，这是一个专门用于Triton编程的80亿参数规模大语言模型，通过新颖的强化学习框架进行训练。该方法通过多层验证系统提供高保真奖励信号，并采用分层奖励分解来分离高层推理与低层实现的信用分配，以应对数据稀缺和奖励黑客攻击等挑战。在KernelBench上的综合评估表明，TritonRL在正确性和运行时加速方面达到了最先进水平，超越了同期Triton专用模型，并匹配了参数超过1000亿的前沿模型性能，凸显了硬件感知强化学习在专业领域适应中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems</div>
<div class="meta-line">Authors: Junwei Su, Chuan Wu</div>
<div class="meta-line">First: 2026-02-09T05:08:36+00:00 · Latest: 2026-02-09T05:08:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08272v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08272v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL&#x27;s comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体系统何时更优？——智能体系统学习效率分析</div>
<div class="mono" style="margin-top:8px">强化学习已成为训练或微调大语言模型的关键方法，通过交互反馈实现自适应、任务特定的优化。多智能体强化学习通过将复杂任务分解为由不同交互智能体学习的专门子任务，为大语言模型系统提供了提升能力与效率的新途径。然而，关于多智能体强化学习何时及为何优于单智能体强化学习的理论认知仍显不足，导致选择合适强化学习框架时存在不确定性。本文通过严格分析大语言模型背景下多智能体与单智能体强化学习的样本效率比较，填补了这一关键空白。借助近似正确框架，我们形式化定义了大语言模型的单智能体与多智能体强化学习设置，推导出显式样本复杂度边界，并系统刻画了任务分解与对齐如何影响学习效率。研究结果表明：当任务可自然分解为独立子任务时，多智能体强化学习能改善样本复杂度；而存在依赖关系的子任务会削弱其比较优势。此外，我们引入并分析了任务对齐概念，量化了在潜在错配情况下强制独立任务分解的权衡关系。这些理论发现澄清了实证研究中的不一致现象，并为在复杂大语言模型场景中有效部署多智能体强化学习策略提供了实用准则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the uncertainty in selecting reinforcement learning frameworks for large language models by theoretically analyzing when multi-agent reinforcement learning (MARL) outperforms single-agent reinforcement learning (SARL) in terms of sample efficiency. The motivation stems from the lack of clear guidelines on MARL&#x27;s advantages, despite its promise for decomposing complex tasks. Using the Probably Approximately Correct (PAC) framework, the authors formally define SARL and MARL setups for LLMs, derive sample complexity bounds, and characterize how task decomposition and alignment affect learning efficiency. The main experimental results show that MARL improves sample complexity when tasks decompose into independent subtasks, but its advantage diminishes with dependent subtasks, while the analysis of task alignment quantifies trade-offs in enforcing decomposition, clarifying empirical inconsistencies and offering practical deployment criteria.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型中强化学习框架选择的不确定性，从理论上分析了多智能体强化学习（MARL）在何时比单智能体强化学习（SARL）具有更高的样本效率。研究动机源于尽管MARL在分解复杂任务方面前景广阔，但其优势缺乏明确指导。作者利用近似正确（PAC）框架，形式化定义了LLM的SARL和MARL设置，推导了样本复杂度界限，并系统刻画了任务分解和对齐如何影响学习效率。主要实验结果表明，当任务可分解为独立子任务时，MARL能提升样本复杂度，但子任务间的依赖性会削弱其比较优势；同时，任务对齐分析量化了强制分解的权衡，澄清了实证不一致性，并为复杂LLM场景中有效部署MARL策略提供了实用标准。</div>
</details>
</div>
<div class="card">
<div class="title">Non-Uniform Noise-to-Signal Ratio in the REINFORCE Policy-Gradient Estimator</div>
<div class="meta-line">Authors: Haoyu Han, Heng Yang</div>
<div class="meta-line">First: 2026-02-01T22:05:48+00:00 · Latest: 2026-02-09T04:14:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01460v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01460v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy-gradient methods are widely used in reinforcement learning, yet training often becomes unstable or slows down as learning progresses. We study this phenomenon through the noise-to-signal ratio (NSR) of a policy-gradient estimator, defined as the estimator variance (noise) normalized by the squared norm of the true gradient (signal). Our main result is that, for (i) finite-horizon linear systems with Gaussian policies and linear state-feedback, and (ii) finite-horizon polynomial systems with Gaussian policies and polynomial feedback, the NSR of the REINFORCE estimator can be characterized exactly-either in closed form or via numerical moment-evaluation algorithms-without approximation. For general nonlinear dynamics and expressive policies (including neural policies), we further derive a general upper bound on the variance. These characterizations enable a direct examination of how NSR varies across policy parameters and how it evolves along optimization trajectories (e.g. SGD and Adam). Across a range of examples, we find that the NSR landscape is highly non-uniform and typically increases as the policy approaches an optimum; in some regimes it blows up, which can trigger training instability and policy collapse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>REINFORCE策略梯度估计器中非均匀的信噪比</div>
<div class="mono" style="margin-top:8px">策略梯度方法在强化学习中广泛应用，但训练过程常随学习进展变得不稳定或放缓。我们通过策略梯度估计器的信噪比（NSR）研究这一现象，该比值定义为估计器方差（噪声）与真实梯度平方范数（信号）的归一化值。我们的主要结果是：对于（i）采用高斯策略和线性状态反馈的有限时域线性系统，以及（ii）采用高斯策略和多项式反馈的有限时域多项式系统，REINFORCE估计器的NSR可精确表征（通过闭式解或数值矩估计算法），无需近似。针对一般非线性动力学和表达能力强的策略（包括神经策略），我们进一步推导了方差的一般上界。这些表征使我们能直接考察NSR如何随策略参数变化，以及如何沿优化轨迹（如SGD和Adam）演变。通过一系列示例，我们发现NSR分布高度不均匀，且通常随策略趋近最优解而增大；在某些情况下会急剧上升，可能引发训练不稳定和策略崩溃。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper investigates training instability in policy-gradient reinforcement learning by analyzing the noise-to-signal ratio (NSR) of the REINFORCE estimator, defined as the variance normalized by the squared gradient norm. The method provides exact characterizations of NSR for finite-horizon linear and polynomial systems with Gaussian policies, and derives a general upper bound for nonlinear dynamics and neural policies. Experimental results across examples reveal that the NSR landscape is highly non-uniform, typically increasing as policies approach optima and sometimes diverging, which can lead to training instability and policy collapse.</div>
<div class="mono" style="margin-top:8px">本文通过分析REINFORCE策略梯度估计器的噪声信号比（NSR，定义为方差与梯度平方范数之比）来研究强化学习中训练不稳定的问题。方法上，针对有限时域线性系统和多项式系统的高斯策略，给出了NSR的精确刻画；对于非线性动态和神经策略，推导了通用的方差上界。实验结果表明，NSR的分布高度不均匀，通常随策略接近最优而增大，有时甚至发散，这可能引发训练不稳定和策略崩溃。</div>
</details>
</div>
<div class="card">
<div class="title">Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers</div>
<div class="meta-line">Authors: Juncheng Dong, Bowen He, Moyang Guo, Ethan X. Fang, Zhuoran Yang, Vahid Tarokh</div>
<div class="meta-line">First: 2026-02-09T03:42:16+00:00 · Latest: 2026-02-09T03:42:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08244v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08244v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>情境学习，选择引导：基于Transformer的无奖励强化学习新范式</div>
<div class="mono" style="margin-top:8px">情境强化学习利用Transformer模型的情境学习能力，无需参数更新即可泛化至未见过的序列决策任务。然而，现有方法在预训练阶段依赖显式奖励信号，当奖励定义模糊、难以指定或获取成本高昂时，其应用受到限制。为突破此局限，我们提出基于情境偏好的强化学习新范式，其预训练与部署仅需偏好反馈，无需奖励监督。我们研究两种反馈粒度不同的变体：基于单步偏好的即时偏好强化学习，以及基于轨迹级比较的轨迹偏好强化学习。实验表明，在仅含偏好数据的情境数据集上，标准监督预训练方法依然有效，这验证了仅用偏好信号实现情境强化学习的可行性。为提升数据效率，我们进一步为两种变体设计了原生偏好优化框架，可直接从偏好数据优化Transformer策略，无需奖励信号或最优动作标签。在决斗老虎机、导航及连续控制任务上的实验证明，该范式能实现强大的情境泛化能力，其性能与依赖完整奖励监督的情境强化学习方法相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing in-context reinforcement learning (ICRL) methods that depend on explicit reward signals, which can be ambiguous or costly, this paper introduces In-Context Preference-based Reinforcement Learning (ICPRL), a reward-free paradigm that uses only preference feedback for both pretraining and deployment. The method explores two variants based on feedback granularity: Immediate Preference-based RL (I-PRL) for per-step preferences and Trajectory Preference-based RL (T-PRL) for trajectory-level comparisons, showing that supervised pretraining remains viable with preference-only data and introducing more data-efficient, preference-native frameworks that optimize transformer policies directly from preferences without reward signals or optimal action labels. Experimental results on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL achieves strong in-context generalization to unseen tasks, performing comparably to ICRL methods trained with full reward supervision.</div>
<div class="mono" style="margin-top:8px">针对现有情境强化学习方法依赖显式奖励信号（可能模糊或成本高昂）的局限性，本文提出了情境偏好强化学习，这是一种无奖励范式，仅使用偏好反馈进行预训练和部署。该方法基于反馈粒度探索了两种变体：针对每步偏好的即时偏好强化学习和针对轨迹级比较的轨迹偏好强化学习，表明监督预训练在仅使用偏好数据时仍然有效，并引入了更高效的数据原生框架，直接从偏好优化变换器策略，无需奖励信号或最优动作标签。在决斗老虎机、导航和连续控制任务上的实验结果表明，该方法能实现强大的情境泛化能力，在未见任务上达到与使用完整奖励监督训练的情境强化学习方法相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs</div>
<div class="meta-line">Authors: Siqu Ou, Tianrui Wan, Zhiyuan Zhao, Junyu Gao, Xuelong Li</div>
<div class="meta-line">First: 2026-02-09T03:33:23+00:00 · Latest: 2026-02-09T03:33:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08241v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态大语言模型是否真正具备视觉注意力：强化其视觉关注机制</div>
<div class="mono" style="margin-top:8px">尽管思维链推理显著提升了多模态大语言模型在复杂推理任务上的表现，现有方法主要依赖长文本推理轨迹，且缺乏学习稳定视觉注意力策略的有效机制。我们的分析表明，当前多模态大语言模型存在视觉聚焦薄弱的问题：早期视觉对齐偏差在后续推理中很少被修正，导致错误传播与推理失败。我们认为这一局限源于训练过程中对视觉注意力的信用分配不足。为此，我们提出SAYO模型——采用强化学习框架训练，通过引入区域级视觉注意力奖励机制，将优化信号显式对齐于基于视觉的推理步骤，使模型能够学习更可靠的注意力行为。在多个多模态基准测试上的广泛实验表明，SAYO在多样化推理与感知任务中持续提升性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of weak and unstable visual attention in multimodal large language models (MLLMs), where early visual misalignments often propagate uncorrected through reasoning chains, leading to inference failures. The authors propose SAYO, a model trained with a reinforcement learning framework that introduces a region-level visual attention reward to explicitly align optimization signals with visually grounded reasoning steps, thereby reinforcing reliable attention behaviors. Experimental results across multiple benchmarks show that SAYO consistently improves performance on diverse reasoning and perception tasks compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLMs）中视觉注意力弱且不稳定的问题展开研究，指出早期的视觉偏差在后续推理中很少被纠正，导致错误传播和推理失败。为此，作者提出了SAYO模型，它采用强化学习框架进行训练，并引入了一种基于区域级视觉注意力的奖励机制，以显式地将优化信号与基于视觉的推理步骤对齐，从而学习更可靠的注意力行为。在多个多模态基准测试上的广泛实验表明，SAYO在多种推理和感知任务上持续提升了性能。</div>
</details>
</div>
<div class="card">
<div class="title">SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning</div>
<div class="meta-line">Authors: Peng Xia, Jianwen Chen, Hanyang Wang, Jiaqi Liu, Kaide Zeng, Yu Wang, Siwei Han, Yiyang Zhou, Xujiang Zhao, Haifeng Chen, Zeyu Zheng, Cihang Xie, Huaxiu Yao</div>
<div class="meta-line">First: 2026-02-09T03:17:17+00:00 · Latest: 2026-02-09T03:17:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08234v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08234v1">PDF</a> · <a href="https://github.com/aiming-lab/SkillRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent&#x27;s policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SkillRL：基于递归技能增强强化学习的智能体进化框架</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）智能体在复杂任务中展现出卓越性能，但其通常孤立运行，难以从历史经验中学习。现有基于记忆的方法主要存储原始轨迹，这些轨迹往往存在冗余与噪声，阻碍了智能体提取对泛化至关重要的高层可复用行为模式。本文提出SkillRL框架，通过自动化技能发现与递归进化机制，弥合原始经验与策略提升之间的鸿沟。该方法包含：基于经验蒸馏构建分层技能库SkillBank的机制、面向通用与任务特定启发式的自适应检索策略，以及使技能库在强化学习过程中与智能体策略协同进化的递归演化机制。这些创新显著降低了令牌占用，同时提升了推理效用。在ALFWorld、WebShop及七项搜索增强任务上的实验表明，SkillRL实现了最先进的性能，以超过基线15.3%的优势领先，并在任务复杂度增加时保持稳健性。代码已开源：https://github.com/aiming-lab/SkillRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SkillRL, a framework designed to address the limitations of LLM agents that operate in isolation and fail to learn reusable patterns from raw, noisy experience trajectories. The method employs an experience-based distillation mechanism to build a hierarchical skill library called SkillBank, along with adaptive retrieval and a recursive evolution process that co-evolves skills with the agent&#x27;s policy during reinforcement learning, thereby reducing token usage and enhancing reasoning. Experimental results on ALFWorld, WebShop, and seven search-augmented tasks show that SkillRL achieves state-of-the-art performance, outperforming strong baselines by over 15.3% and maintaining robustness as task complexity increases.</div>
<div class="mono" style="margin-top:8px">本文提出了SkillRL框架，旨在解决大型语言模型智能体孤立运行、难以从原始嘈杂经验轨迹中学习可重用模式的问题。该方法通过基于经验的蒸馏机制构建分层技能库SkillBank，结合自适应检索策略和递归进化机制，使技能库在强化学习过程中与智能体策略协同进化，从而减少令牌消耗并提升推理效用。在ALFWorld、WebShop和七项搜索增强任务上的实验结果表明，SkillRL实现了最先进的性能，以超过15.3%的优势超越强基线方法，并在任务复杂度增加时保持鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for Reinforcement Learning from Human Feedback (RLHF)</div>
<div class="meta-line">Authors: Dipan Maity</div>
<div class="meta-line">First: 2026-02-04T15:26:44+00:00 · Latest: 2026-02-09T03:14:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04651v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04651v2">PDF</a> · <a href="https://github.com/ryyzn9/SAFE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of Reinforcement Learning from Human Feedback (RLHF). PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO&#x27;s symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAFE：基于熵感知预测控制的稳定对齐微调方法——面向人类反馈强化学习（RLHF）</div>
<div class="mono" style="margin-top:8px">近端策略优化（PPO）在近期文献中被定位为人类反馈强化学习（RLHF）中强化学习部分的标准方法。PPO虽经验表现良好，但其启发式动机存在局限：对语言模型RLHF中的KL散度约束采用临时性处理方式，且存在奖励振荡、熵崩溃、价值函数漂移及策略突变等问题，常需频繁重启与大量超参数调优。本文针对语言模型RLHF场景提出一种全新的纯在线策略演员-评论家强化学习方法——SAFE（基于熵感知控制的稳定对齐微调算法）。该算法融合双重软最小评论家（用于悲观价值估计）与创新的多层稳定框架（整合熵门控KL调节与PID控制自适应阈值）。相较于标准PPO的对称KL惩罚机制，SAFE能区分高熵探索与低熵模式崩溃，并依据奖励变化速率动态调整惩罚强度。在30亿参数模型上的实验表明：SAFE相比PPO实现训练平均奖励提升5.15%（0.725对0.689），奖励崩溃可忽略不计，且KL控制能力显著优于PPO。本方法仅引入极小计算开销，构建出可解释、抗崩溃的RLHF框架，在保持激进学习速度的同时确保适用于生产部署的长期稳定优化。代码已开源：https://github.com/ryyzn9/SAFE</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the heuristic nature and instability issues of Proximal Policy Optimization (PPO) in Reinforcement Learning from Human Feedback (RLHF), which include reward oscillations and entropy collapse, this paper introduces SAFE, a novel on-policy actor-critic method. The method employs a Double Soft-Min Critic for pessimistic value estimation and a multi-layer stabilization framework with entropy-gated KL regulation and PID-controlled adaptive thresholds to dynamically adjust penalties based on reward trends. Experimental results on a 3B parameter model demonstrate that SAFE achieves a 5.15% higher training-average reward than PPO, exhibits negligible reward crashes, and provides superior KL control with minimal computational overhead, offering a stable and interpretable RLHF framework suitable for production deployment.</div>
<div class="mono" style="margin-top:8px">针对近端策略优化（PPO）在人类反馈强化学习（RLHF）中启发性强、稳定性不足的问题，如奖励振荡和熵崩溃，本文提出了SAFE这一新颖的在线策略演员-评论家方法。该方法采用双重软最小评论家进行悲观值估计，并结合多层稳定框架，通过熵门控KL调节和PID控制自适应阈值，根据奖励趋势动态调整惩罚。在30亿参数模型上的实验结果表明，SAFE相比PPO实现了5.15%的训练平均奖励提升，奖励崩溃可忽略不计，KL控制更优，且计算开销极小，为生产部署提供了一个稳定且可解释的RLHF框架。</div>
</details>
</div>
<div class="card">
<div class="title">Trust Region Masking for Long-Horizon LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Qian Liu, Baoxiang Wang</div>
<div class="meta-line">First: 2025-12-28T20:41:59+00:00 · Latest: 2026-02-09T02:46:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23075v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.23075v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy gradient methods for Large Language Models optimize a policy $π_θ$ via a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. However, modern LLM-RL pipelines suffer from unavoidable implementation divergences -- backend discrepancies, Mixture-of-Experts routing discontinuities, and distributed training staleness -- causing off-policy mismatch ($π_{\text{roll}} \neq π_θ$) and approximation errors between the surrogate and the true objective. We demonstrate that classical trust region bounds on this error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. To address this, we derive a family of bounds -- both KL-based and TV-based -- including a Pinsker-Marginal bound ($O(T^{3/2})$), a Mixed bound ($O(T)$), and an Adaptive bound that strictly generalizes the Pinsker-Marginal bound via per-position importance-ratio decomposition. Taking the minimum over all bounds yields the tightest known guarantee across all divergence regimes. Crucially, all bounds depend on the maximum token-level divergence $D_{\mathrm{KL}}^{\mathrm{tok,max}}$ (or $D_{\mathrm{TV}}^{\mathrm{tok,max}}$), a sequence-level quantity that cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which masks entire sequences violating the trust region, enabling the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长视野大语言模型强化学习中的信任区域掩码</div>
<div class="mono" style="margin-top:8px">大语言模型的策略梯度方法通过从推演策略$π_{\text{roll}}$的样本计算出的代理目标来优化策略$π_θ$。然而，现代LLM-RL流程存在不可避免的实现偏差——后端差异、专家混合路由不连续性以及分布式训练滞后——导致离策略失配（$π_{\text{roll}} \neq π_θ$）以及代理目标与真实目标之间的近似误差。我们证明，关于此误差的经典信任区域边界随序列长度$T$呈$O(T^2)$缩放，使其对长视野任务无效。为解决此问题，我们推导了一系列边界——包括基于KL散度和基于TV散度的边界——涵盖Pinsker-边际边界（$O(T^{3/2})$）、混合边界（$O(T)$）以及通过逐位置重要性比率分解严格推广Pinsker-边际边界自适应边界。取所有边界的最小值可在所有散度范围内获得已知最紧保证。关键的是，所有边界均依赖于最大词元级散度$D_{\mathrm{KL}}^{\mathrm{tok,max}}$（或$D_{\mathrm{TV}}^{\mathrm{tok,max}}$），这是一个序列级量值，无法通过PPO裁剪等词元无关方法控制。我们提出信任区域掩码（TRM），通过掩码违反信任区域的完整序列，首次为长视野LLM-RL实现了非空单调改进保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of off-policy mismatch and approximation errors in long-horizon reinforcement learning for large language models, which arise from implementation divergences like backend discrepancies and distributed training staleness, causing classical trust region bounds to become vacuous as they scale quadratically with sequence length. To address this, the authors derive a family of tighter bounds—including Pinsker-Marginal, Mixed, and Adaptive bounds—that scale better with sequence length and depend on maximum token-level divergence, a quantity not controllable by methods like PPO clipping. The main experimental results demonstrate that their proposed Trust Region Masking method, which masks entire sequences violating the trust region, enables non-vacuous monotonic improvement guarantees for long-horizon tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大语言模型在长序列强化学习中面临的离策略不匹配和近似误差问题，这些问题由后端差异和分布式训练陈旧性等实现分歧引起，导致经典信任域边界随序列长度呈二次方增长而失效。为解决此问题，作者推导了一系列更紧的边界——包括Pinsker-Marginal、Mixed和Adaptive边界——这些边界随序列长度的缩放更优，并依赖于最大令牌级散度，而该量无法通过PPO裁剪等令牌无关方法控制。主要实验结果表明，所提出的信任域掩码方法通过掩码违反信任域的整个序列，首次为长序列任务实现了非平凡的单调改进保证。</div>
</details>
</div>
<div class="card">
<div class="title">DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning</div>
<div class="meta-line">Authors: Haoran Liu, Zheni Zeng, Yukun Yan, Yuxuan Chen, Yunduo Xiao</div>
<div class="meta-line">First: 2026-02-09T02:26:25+00:00 · Latest: 2026-02-09T02:26:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08213v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule&#x27;s core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DrugR：基于大语言模型显式推理的分子药物优化方法</div>
<div class="mono" style="margin-top:8px">分子生成与优化是化学领域的核心任务。智能工具特别是具备强大知识储备与交互能力的大语言模型的快速发展，为此提供了新范式。然而，大语言模型面临分子结构与药理特性间复杂隐式关系及相应标注数据缺乏的内在挑战。为弥合此差距，我们提出DrugR——一种基于大语言模型的方法，将显式分步药理推理引入优化过程。该方法融合领域持续预训练、逆向数据工程的监督微调及自平衡多粒度强化学习，使DrugR能在保持原分子核心功效的同时有效改善关键ADMET性质。实验表明，DrugR在不影响结构相似性或靶点结合亲和力的前提下实现了多性质综合提升。其显式推理过程为每个优化步骤提供清晰可解释的依据，产出可操作的设计洞见，推动自动化知识驱动科学发现。我们已开源代码与模型检查点以促进后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of leveraging large language models (LLMs) for molecule optimization despite the complex implicit structure-property relationships and lack of labeled data, this paper introduces DrugR, an LLM-based method that incorporates explicit, step-by-step pharmacological reasoning. The method integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning to optimize molecules. Experimental results show that DrugR effectively enhances key ADMET properties while preserving the original molecule&#x27;s core efficacy and structural similarity, with its explicit reasoning providing interpretable rationales for each optimization step.</div>
<div class="mono" style="margin-top:8px">本文的动机是利用大语言模型进行分子优化，但面临分子结构与药理特性间关系复杂且缺乏标注数据的挑战，为此提出了DrugR方法，该方法引入了显式的分步药理学推理。该方法整合了领域特定的持续预训练、通过逆向数据工程进行监督微调，以及自平衡的多粒度强化学习，以优化分子。实验结果表明，DrugR在保持原始分子核心功效和结构相似性的同时，有效提升了关键ADMET性质，其显式推理过程为每个优化步骤提供了清晰可解释的依据。</div>
</details>
</div>
<div class="card">
<div class="title">CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization</div>
<div class="meta-line">Authors: Hyungseok Song, Deunsol Yoon, Kanghoon Lee, Han-Seul Jeong, Soonyoung Lee, Woohyung Lim</div>
<div class="meta-line">First: 2026-02-09T02:19:30+00:00 · Latest: 2026-02-09T02:19:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08210v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CADO：从模仿到成本最小化——基于热图的组合优化求解器</div>
<div class="mono" style="margin-top:8px">基于热图的求解器已成为组合优化领域的一种新兴范式。然而，我们认为当前主流的监督学习训练范式存在根本性的目标错配：最小化模仿损失（如交叉熵）并不能保证解的成本最小化。我们将此错配分解为两个缺陷：解码器盲视（忽略不可微的解码过程）和成本盲视（优先结构模仿而非解的质量）。实验表明，这些内在缺陷会形成严格的性能上限。为突破此限制，我们提出CADO（面向优化的成本感知扩散模型），这是一个简化的强化学习微调框架，将扩散去噪过程建模为马尔可夫决策过程，以直接优化解码后解的成本。我们引入了以标签为中心的奖励机制，将真实标签重新用作无偏基线而非模仿目标，并提出混合微调方法以实现参数高效适配。CADO在多种基准测试中取得了最先进的性能，验证了目标对齐对于释放基于热图的求解器全部潜力的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper identifies a key limitation in training heatmap-based solvers for combinatorial optimization: the standard supervised learning approach, which minimizes imitation loss, fails to directly minimize the actual solution cost due to Decoder-Blindness and Cost-Blindness, creating a performance ceiling. To address this, the authors propose CADO, a reinforcement learning fine-tuning framework that models the diffusion denoising process as a Markov Decision Process to directly optimize the final solution cost, employing a Label-Centered Reward and Hybrid Fine-Tuning. Experimental results show that CADO achieves state-of-the-art performance across multiple benchmarks, demonstrating that aligning the training objective with cost minimization is crucial for advancing these solvers.</div>
<div class="mono" style="margin-top:8px">本文指出，基于热图的组合优化求解器在训练中存在一个关键缺陷：主流的监督学习方法以最小化模仿损失为目标，但由于解码器盲视和成本盲视，无法直接最小化解的实际成本，从而形成了性能瓶颈。为解决此问题，作者提出了CADO，一个强化学习微调框架，它将扩散去噪过程建模为马尔可夫决策过程以直接优化最终解的成本，并采用了标签中心奖励和混合微调策略。实验结果表明，CADO在多个基准测试中取得了最先进的性能，验证了将训练目标与成本最小化对齐对于释放此类求解器潜力的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Reinforcement Learning from Neural Feedback: Mapping fNIRS Signals to Agent Performance</div>
<div class="meta-line">Authors: Julia Santaniello, Matthew Russell, Benson Jiang, Donatello Sassaroli, Robert Jacob, Jivko Sinapov</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-17T00:21:46+00:00 · Latest: 2026-02-08T22:12:59+00:00</div>
<div class="meta-line">Comments: Accepted to the Association for the Advancement of Artificial Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12844v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.12844v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating user feedback into the agent&#x27;s training process. This paper introduces a framework that guides agent training through implicit neural signals, with a focus on the neural classification problem. Our work presents and releases a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train multiple classifiers to predict varying levels of agent performance (optimal, suboptimal, or worst-case) from windows of preprocessed fNIRS features, achieving an average F1 score of 67% for binary and 46% for multi-class classification across conditions and domains. We also train multiple regressors to predict the degree of deviation between an agent&#x27;s chosen action and a set of near-optimal policy actions, providing a continuous measure of performance. Finally, we evaluate cross-subject generalization and show that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our results demonstrate that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future Reinforcement Learning from Neural Feedback (RLNF) systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向基于神经反馈的强化学习：将fNIRS信号映射至智能体性能</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）通过将用户反馈整合至智能体训练过程，使其行为与人类偏好保持一致。本文提出一种通过隐式神经信号指导智能体训练的框架，重点关注神经信号分类问题。我们构建并发布了一个新颖的功能性近红外光谱（fNIRS）数据集，采集自25名人类参与者在三个领域（抓放机器人、月球着陆器、Flappy Bird）的神经信号记录。我们训练了多个分类器，通过预处理后的fNIRS特征窗口预测智能体性能的不同水平（最优、次优或最差），在跨条件与跨领域测试中，二分类与多分类的平均F1分数分别达到67%和46%。同时训练了多个回归器，用于预测智能体所选动作与近似最优策略动作之间的偏差程度，从而提供连续的性能度量。最后，我们评估了跨被试泛化能力，结果表明：使用少量被试特定数据对预训练模型进行微调，可使二分类与多分类模型的平均F1分数分别提升17%和41%。我们的研究证明，将隐式fNIRS信号映射至智能体性能具有可行性且可优化，为未来基于神经反馈的强化学习（RLNF）系统奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the potential to advance Reinforcement Learning from Human Feedback (RLHF) by using implicit neural signals instead of explicit human input to guide agent training. The method involves collecting a novel functional near-infrared spectroscopy (fNIRS) dataset from 25 participants across three task domains and training classifiers and regressors to map neural signals to agent performance levels, such as optimal or suboptimal actions. The main experimental results show that classifiers achieved average F1 scores of 67% for binary and 46% for multi-class performance prediction, and fine-tuning with subject-specific data significantly improved cross-subject generalization, increasing F1 scores by 17% and 41% respectively, demonstrating the feasibility of mapping fNIRS signals to agent performance for future neural feedback systems.</div>
<div class="mono" style="margin-top:8px">本文的动机是通过使用隐式神经信号而非显式人类输入来指导智能体训练，从而推进基于人类反馈的强化学习（RLHF）。方法包括从25名参与者在三个任务领域中收集新型功能性近红外光谱（fNIRS）数据集，并训练分类器和回归器将神经信号映射到智能体性能水平（如最优或次优动作）。主要实验结果表明，分类器在二元和多类性能预测中分别实现了67%和46%的平均F1分数，且使用特定受试者数据进行微调显著提升了跨受试者泛化能力，使F1分数分别提高了17%和41%，这证明了将fNIRS信号映射到智能体性能以构建未来神经反馈系统的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems</div>
<div class="meta-line">Authors: Risal Shahriar Shefin, Debashis Gupta, Thai Le, Sarra Alqahtani</div>
<div class="meta-line">First: 2026-02-08T19:55:26+00:00 · Latest: 2026-02-08T19:55:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08104v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08104v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains &quot;downstream-first&quot; detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习系统中的可解释故障分析</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）正日益应用于安全关键领域，但可解释的故障检测与归因方法仍不成熟。本文提出一种基于梯度的两阶段框架，为三项关键故障分析任务提供可解释诊断：（1）检测真实初始故障源（零号患者）；（2）验证因连锁效应导致未受攻击智能体被率先标记的原因；（3）追踪故障如何通过习得的协作路径传播。第一阶段通过策略梯度成本的泰勒余项分析实现可解释的逐智能体故障检测，在首次阈值突破时声明初始零号患者候选；第二阶段通过评论家导数的几何分析——聚合因果窗口内的一阶敏感性与方向二阶曲率——构建可解释传播图进行验证。该方法通过揭示放大上游偏差的路径，解释了“下游优先”检测异常现象。在Simple Spread（3和5智能体）的500轮次及StarCraft II的100轮次中，使用MADDPG和HATRPO算法评估，本方法实现88.2-99.4%的零号患者检测准确率，并为检测决策提供可解释的几何证据。该框架突破黑盒检测局限，提供梯度层面的可解释取证工具，为安全关键MARL系统的级联故障诊断提供实用方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the need for interpretable failure analysis in safety-critical multi-agent reinforcement learning (MARL) systems, where current methods lack transparency. The proposed method is a two-stage gradient-based framework that first uses Taylor-remainder analysis for interpretable per-agent failure detection to identify an initial failure source (Patient-0), and then employs geometric analysis of critic derivatives to validate detection decisions and trace failure propagation through interpretable contagion graphs. The main experimental results, from evaluations on Simple Spread and StarCraft II environments using algorithms like MADDPG and HATRPO, show that the method achieves high Patient-0 detection accuracy ranging from 88.2% to 99.4% and provides geometric evidence to explain downstream detection anomalies.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于安全关键多智能体强化学习系统对可解释故障分析的需求，现有方法缺乏透明度。所提出的方法是一个基于梯度的两阶段框架：第一阶段通过策略梯度成本的泰勒余项分析进行可解释的智能体故障检测，以识别初始故障源；第二阶段利用评论家导数的几何分析验证检测决策，并通过可解释的传播图追踪故障扩散路径。主要实验结果在Simple Spread和星际争霸II环境中使用MADDPG和HATRPO等算法进行评估，显示该方法对初始故障源的检测准确率达到88.2%至99.4%，并能提供几何证据解释下游检测异常。</div>
</details>
</div>
<div class="card">
<div class="title">Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities</div>
<div class="meta-line">Authors: Majid Ghasemi, Mark Crowley</div>
<div class="meta-line">First: 2026-02-08T19:23:02+00:00 · Latest: 2026-02-08T19:23:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08092v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08092v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent&#x27;s learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this &quot;judging the judges&quot; mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>社会强化学习中的目标解耦：从谄媚多数中恢复真实目标</div>
<div class="mono" style="margin-top:8px">当代AI对齐策略依赖一个脆弱的前提：人类反馈虽含噪声，但本质上是真实的信号。本文将此假设定义为强化学习（RL）的教条4。我们证明，该教条在静态环境中成立，但在评估者可能谄媚、懒惰或对抗的社会环境中失效。我们证明，在教条4下，标准RL智能体会遭受“目标解耦”——一种结构性失效模式，智能体习得的目标与潜在真实目标永久分离，必然导致失准。为解决此问题，我们提出“认知源对齐”（ESA）。与依赖统计共识（信任多数）的标准鲁棒方法不同，ESA利用稀疏安全公理来评判反馈的来源而非信号本身。我们证明，这种“评判评判者”机制能保证收敛至真实目标，即使多数评估者存在偏见。实证表明，传统共识方法在多数共谋下失效，而我们的方法能成功恢复最优策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the assumption that human feedback is a fundamentally truthful signal in AI alignment, identifying it as a fragile dogma in reinforcement learning. The authors demonstrate that in social settings where evaluators may be sycophantic or adversarial, standard RL agents suffer from Objective Decoupling, causing permanent misalignment from the ground truth. To address this, they propose Epistemic Source Alignment, a method that uses sparse safety axioms to judge the source of feedback rather than relying on statistical consensus. Experimental results show that while traditional consensus methods fail under majority collusion, their approach successfully recovers the optimal policy.</div>
<div class="mono" style="margin-top:8px">本文挑战了人工智能对齐中人类反馈本质上是真实信号的假设，指出这是强化学习中的一个脆弱教条。作者证明，在评估者可能阿谀奉承或对抗的社会环境中，标准强化学习智能体会遭受目标解耦，导致与潜在真实目标永久性偏离。为解决此问题，他们提出认知源对齐方法，该方法利用稀疏安全公理来评判反馈来源而非信号本身。实验结果表明，传统共识方法在多数评估者共谋时会失败，而所提方法能成功恢复最优策略。</div>
</details>
</div>
<div class="card">
<div class="title">Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Manan Tayal, Mumuksh Tayal</div>
<div class="meta-line">First: 2026-02-08T16:56:21+00:00 · Latest: 2026-02-08T16:56:21+00:00</div>
<div class="meta-line">Comments: 23 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08054v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08054v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Epigraph引导流匹配的安全高效离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习为无需在线探索风险的自主体训练提供了理想范式，尤其在安全关键领域。然而，从固定数据集中同时实现强安全性与高性能仍具挑战。现有安全离线强化学习方法常依赖允许违规的软约束、引入过度保守性，或难以平衡安全性、奖励优化与数据分布遵循。为此，我们提出Epigraph引导流匹配框架，将安全离线强化学习建模为状态约束的最优控制问题，以协同优化安全与性能。通过最优控制问题的Epigraph重构推导可行性价值函数，避免先前工作中常见的解耦目标或后验过滤。策略合成通过基于该Epigraph价值函数重加权行为分布，并借助流匹配拟合生成策略，实现高效且分布一致的采样。在包括Safety-Gymnasium基准测试在内的多种安全关键任务中，本方法在实现竞争优势回报的同时保持接近零的经验安全违规，验证了Epigraph引导策略合成的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge in offline reinforcement learning of jointly achieving strong safety and performance from fixed datasets, as existing methods often compromise on safety, introduce excessive conservatism, or fail to balance multiple objectives. The proposed method, Epigraph-Guided Flow Matching (EpiFlow), formulates safe offline RL as a state-constrained optimal control problem, learning a feasibility value function via an epigraph reformulation to co-optimize safety and performance, and then synthesizes policies by reweighting the behavior distribution and fitting a generative policy using flow matching. The main experimental results show that across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns while maintaining near-zero empirical safety violations, demonstrating its effectiveness.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线强化学习中从固定数据集同时实现强安全性和高性能的挑战，因为现有方法常常在安全性上妥协、引入过度保守性或难以平衡多个目标。所提出的方法，即Epigraph-Guided Flow Matching（EpiFlow），将安全离线强化学习构建为一个状态约束的最优控制问题，通过epigraph重构学习可行性价值函数以共同优化安全性和性能，然后通过基于该函数重新加权行为分布并利用流匹配拟合生成策略来合成策略。主要实验结果表明，在包括Safety-Gymnasium基准在内的多种安全关键任务中，EpiFlow实现了有竞争力的回报，同时保持了接近零的经验安全违规，证明了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning</div>
<div class="meta-line">Authors: Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang, Rui Mao, Erik Cambria</div>
<div class="meta-line">First: 2026-02-02T05:30:42+00:00 · Latest: 2026-02-08T16:55:44+00:00</div>
<div class="meta-line">Comments: 41 pages, 7 figures, 6 tables. Project page: http://flowsteer.org/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01664v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01664v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowSteer：基于端到端强化学习的交互式智能体工作流编排</div>
<div class="mono" style="margin-top:8px">近年来，多种强大的智能体工作流已被应用于解决广泛的人类问题。然而，现有工作流编排仍面临关键挑战，包括高昂的人工成本、对特定算子/大语言模型（LLM）的依赖，以及稀疏的奖励信号。为应对这些挑战，我们提出FlowSteer——一种端到端强化学习框架，以轻量级策略模型作为智能体，结合可执行画布环境，通过多轮交互实现工作流编排的自动化。在此过程中，策略模型分析执行状态并选择编辑动作，而画布则执行算子并返回反馈以进行迭代优化。此外，FlowSteer提供即插即用框架，支持多样化的算子库与可替换的LLM后端。为有效训练此交互范式，我们提出画布工作流相对策略优化（CWRPO），通过引入带条件释放的多样性约束奖励来稳定学习过程并抑制捷径行为。在十二个数据集上的实验结果表明，FlowSteer在多种任务中显著优于基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces FlowSteer, an end-to-end reinforcement learning framework designed to automate agentic workflow orchestration, addressing challenges such as high manual effort, dependency on specific operators or large language models (LLMs), and sparse rewards. The method employs a lightweight policy model that interacts with an executable canvas environment, selecting editing actions based on execution states while the canvas runs operators and provides feedback for iterative refinement, supported by a plug-and-play architecture for diverse operators and LLM backends. To train this system, the authors propose Canvas Workflow Relative Policy Optimization (CWRPO), which uses diversity-constrained rewards with conditional release to stabilize learning and prevent shortcuts. Experimental results across twelve datasets demonstrate that FlowSteer significantly outperforms baseline methods in various tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了FlowSteer，一种端到端的强化学习框架，旨在自动化智能体工作流编排，以解决手动成本高、依赖特定算子或大语言模型（LLM）以及奖励稀疏等挑战。该方法采用轻量级策略模型与可执行画布环境进行交互，根据执行状态选择编辑动作，同时画布运行算子并提供反馈以迭代优化，并支持即插即用的架构，兼容多样化的算子库和可互换的LLM后端。为有效训练此交互范式，作者提出了画布工作流相对策略优化（CWRPO），通过引入条件释放的多样性约束奖励来稳定学习并抑制捷径行为。在十二个数据集上的实验结果表明，FlowSteer在各种任务中显著优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling</div>
<div class="meta-line">Authors: Bulent Soykan, Sean Mondesire, Ghaith Rabadi, Grace Bochenek</div>
<div class="meta-line">First: 2026-02-08T16:54:47+00:00 · Latest: 2026-02-08T16:54:47+00:00</div>
<div class="meta-line">Comments: 11 pages, 2 figures, Winter Simulation Conference (WSC) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08052v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多目标无关并行机调度的图增强深度强化学习方法</div>
<div class="mono" style="margin-top:8px">具有释放时间、准备时间和资格约束的无关并行机调度问题是一个重要的多目标优化难题。传统方法难以在最小化总加权延迟和总准备时间之间取得平衡。本文提出了一种采用近端策略优化和图神经网络的深度强化学习框架。图神经网络能有效表征作业、机器和准备状态的复杂关系，使PPO智能体能够学习直接的调度策略。在多目标奖励函数的引导下，智能体可同时最小化总加权延迟和总准备时间。在基准算例上的实验结果表明，本文提出的PPO-GNN智能体显著优于标准分派规则和元启发式算法，在双目标间实现了更优的权衡，为复杂制造调度提供了鲁棒且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the multi-objective Unrelated Parallel Machine Scheduling Problem (UPMSP) with complex constraints, where traditional methods fail to effectively balance the minimization of Total Weighted Tardiness (TWT) and Total Setup Time (TST). The proposed method integrates a Graph Neural Network (GNN) with a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO), where the GNN models the intricate relationships among jobs, machines, and setups to inform the PPO agent&#x27;s scheduling policy. Guided by a multi-objective reward function, the agent learns to simultaneously optimize both objectives. Experimental evaluations on benchmark instances show that the PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between TWT and TST, thus offering a robust and scalable solution for complex manufacturing scheduling.</div>
<div class="mono" style="margin-top:8px">本文针对具有复杂约束的多目标不相关并行机调度问题，传统方法难以有效平衡最小化总加权延迟和总准备时间这两个目标。所提出的方法将图神经网络与基于近端策略优化的深度强化学习框架相结合，其中图神经网络用于建模作业、机器和准备之间的复杂关系，以指导智能体的调度策略。通过多目标奖励函数的引导，智能体学习同时优化这两个目标。在基准实例上的实验结果表明，该PPO-GNN智能体显著优于标准调度规则和元启发式方法，在总加权延迟和总准备时间之间实现了更优的权衡，从而为复杂的制造调度提供了鲁棒且可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff</div>
<div class="meta-line">Authors: Isaac Han, Sangyeon Park, Seungwon Oh, Donghu Kim, Hojoon Lee, Kyung-Joong Kim</div>
<div class="meta-line">Venue: ICLR oral</div>
<div class="meta-line">First: 2026-02-08T16:17:03+00:00 · Latest: 2026-02-08T16:17:03+00:00</div>
<div class="meta-line">Comments: ICLR&#x27;26 (oral)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08040v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08040v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FIRE：基于Frobenius等距重初始化的稳定性-可塑性权衡平衡方法</div>
<div class="mono" style="margin-top:8px">在非平稳数据上训练的深度神经网络需平衡稳定性（保留先验知识）与可塑性（适应新任务）。标准重初始化方法通过将权重向原始值重置被广泛使用，但难以调优：保守重置无法恢复可塑性，激进重置则会消除有用知识。本文提出FIRE——一种显式平衡稳定性-可塑性权衡的原理性重初始化方法。FIRE通过平方Frobenius误差量化稳定性（衡量与历史权重的接近程度），通过等距偏差量化可塑性（反映权重各向同性）。重初始化点通过求解约束优化问题获得：在等距偏差为零的约束下最小化平方Frobenius误差，该问题可通过牛顿-舒尔茨迭代高效近似。FIRE在持续视觉学习、语言建模和强化学习三大领域进行评估，均显著优于无干预的朴素训练及标准重初始化方法，有效实现了稳定性-可塑性的平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the stability-plasticity tradeoff in deep neural networks trained on nonstationary data, where standard reinitialization methods are difficult to tune effectively. The authors propose FIRE, a method that balances stability, measured via Squared Frobenius Error to retain past knowledge, and plasticity, measured via Deviation from Isometry to restore adaptability, by solving a constrained optimization problem approximated with Newton-Schulz iteration. Experimental results across continual visual learning, language modeling, and reinforcement learning tasks show that FIRE consistently outperforms both naive training and standard reinitialization approaches, effectively managing the tradeoff.</div>
<div class="mono" style="margin-top:8px">本文针对在非平稳数据上训练的深度神经网络中的稳定性-可塑性权衡问题，其中标准重初始化方法难以有效调节。作者提出FIRE方法，通过求解一个约束优化问题来平衡稳定性（用平方Frobenius误差衡量以保留先验知识）和可塑性（用偏离等距性衡量以恢复适应性），并采用Newton-Schulz迭代进行高效近似。在持续视觉学习、语言建模和强化学习任务上的实验结果表明，FIRE一致优于无干预的朴素训练和标准重初始化方法，有效实现了该权衡的平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Horizon Imagination: Efficient On-Policy Training in Diffusion World Models</div>
<div class="meta-line">Authors: Lior Cohen, Ofir Nabati, Kaixin Wang, Navdeep Kumar, Shie Mannor</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-08T16:07:04+00:00 · Latest: 2026-02-08T16:07:04+00:00</div>
<div class="meta-line">Comments: This paper will be published in the ICLR 2026 proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08032v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08032v1">PDF</a> · <a href="https://github.com/leor-c/horizon-imagination">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>地平线想象：扩散世界模型中的高效同策略训练</div>
<div class="mono" style="margin-top:8px">我们研究了基于扩散的世界模型在强化学习中的应用，这类模型虽能提供高生成保真度，但在控制任务中面临严峻的效率挑战。现有方法要么在推理时需要重型模型，要么依赖高度序列化的想象过程，两者均带来难以承受的计算成本。我们提出了地平线想象（HI），一种适用于离散随机策略的同策略想象过程，能够并行地对多个未来观测进行去噪。HI包含一种稳定化机制和一种新颖的采样调度方案，该方案将去噪预算与去噪应用的有效视野解耦，同时支持子帧预算。在Atari 100K和Craftium上的实验表明，我们的方法在仅使用一半去噪步数的子帧预算下仍能保持控制性能，并在多种调度方案下实现更优的生成质量。代码发布于 https://github.com/leor-c/horizon-imagination。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational inefficiency of diffusion-based world models in reinforcement learning, which suffer from high inference costs due to either large models or sequential imagination processes. The authors propose Horizon Imagination (HI), an on-policy method that enables parallel denoising of multiple future observations through a stabilization mechanism and a novel sampling schedule, decoupling the denoising budget from the effective horizon and allowing sub-frame budgets. Experimental results on Atari 100K and Craftium demonstrate that HI maintains control performance with half the denoising steps and achieves superior generation quality across varied schedules.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中基于扩散的世界模型计算效率低下的问题，这类模型因大型推理模型或顺序想象过程而产生高昂成本。作者提出了地平线想象（HI），这是一种基于策略的方法，通过稳定机制和新颖的采样计划，实现对多个未来观测的并行去噪，从而将去噪预算与有效视野解耦，并支持子帧预算。在Atari 100K和Craftium上的实验结果表明，HI仅用一半的去噪步骤即可保持控制性能，并在不同计划下实现更优的生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">d2: Improved Techniques for Training Reasoning Diffusion Language Models</div>
<div class="meta-line">Authors: Guanghan Wang, Gilad Turok, Yair Schiff, Marianne Arriola, Volodymyr Kuleshov</div>
<div class="meta-line">First: 2025-09-25T19:40:36+00:00 · Latest: 2026-02-08T15:13:18+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21474v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.21474v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While diffusion language models (DLMs) have achieved competitive performance in text generation, improving their reasoning ability with reinforcement learning remains an active research area. Here, we introduce d2, a reasoning framework tailored for masked DLMs. Central to our framework is a new policy gradient algorithm that relies on accurate estimates of the sampling trajectory likelihoods. Our likelihood estimator, d2-AnyOrder, achieves exact trajectory likelihood with a single model pass for DLMs that support a sampling algorithm called any-order decoding. Through an empirical study of widely used DLMs, we show that any-order decoding is not universally supported in practice. Consequently, for DLMs that do not naturally support any-order decoding, we propose another estimator, d2-StepMerge, which, unlike d2-AnyOrder, only approximates the trajectory likelihood. d2-StepMerge trades off compute for approximation accuracy in an analytically tractable manner. Empirically, d2 significantly outperforms widely-used RL baselines when applied to popular DLMs, and sets a new state-of-the-art performance for DLMs on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks (GSM8K and MATH500). We provide the code along with a blog post on the project page: https://guanghanwang.com/d2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>d2：提升推理扩散语言模型训练效果的改进技术</div>
<div class="mono" style="margin-top:8px">尽管扩散语言模型（DLM）在文本生成领域已取得优异表现，但如何通过强化学习提升其推理能力仍是活跃的研究方向。本文提出专为掩码DLM设计的推理框架d2，其核心是一种基于采样轨迹似然精确估计的新型策略梯度算法。我们的似然估计器d2-AnyOrder，在支持&#x27;任意顺序解码&#x27;采样算法的DLM中，仅需单次模型前向传播即可获得精确轨迹似然。通过对主流DLM的实证研究发现，任意顺序解码在实践中并未得到普遍支持。为此，我们针对不支持该解码方式的DLM提出了替代估计器d2-StepMerge，该估计器通过可解析的精度-计算量权衡机制对轨迹似然进行近似估计。实验表明，d2在主流DLM上的表现显著优于常用强化学习基线，并在逻辑推理任务（倒计时与数独）和数学推理基准（GSM8K与MATH500）上为DLM创造了新的性能记录。项目页面已公开代码与技术博客：https://guanghanwang.com/d2</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces d2, a reinforcement learning framework designed to enhance the reasoning capabilities of diffusion language models (DLMs), motivated by the need to improve their performance on complex reasoning tasks beyond standard text generation. The method centers on a novel policy gradient algorithm that requires accurate trajectory likelihood estimates, proposing two estimators: d2-AnyOrder, which computes exact likelihoods efficiently for DLMs supporting any-order decoding, and d2-StepMerge, an approximate estimator for models lacking such support, balancing computational cost and accuracy. Experimental results demonstrate that d2 outperforms existing RL baselines, achieving state-of-the-art performance on logical reasoning tasks like Countdown and Sudoku, as well as math benchmarks including GSM8K and MATH500.</div>
<div class="mono" style="margin-top:8px">本文提出了d2，一个旨在增强扩散语言模型推理能力的强化学习框架，其动机是提升模型在复杂推理任务上的表现，超越一般的文本生成。方法核心是一种新的策略梯度算法，依赖于精确的轨迹似然估计，并提出了两种估计器：d2-AnyOrder为支持任意顺序解码的模型提供高效精确的似然计算，而d2-StepMerge则为不支持该解码的模型提供近似估计，在计算成本和准确性之间取得平衡。实验结果表明，d2在广泛使用的扩散语言模型上显著优于现有强化学习基线，在逻辑推理任务（如Countdown和数独）以及数学推理基准（如GSM8K和MATH500）上取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization</div>
<div class="meta-line">Authors: Anirudh Satheesh, Vaneet Aggarwal</div>
<div class="meta-line">First: 2026-02-08T14:54:02+00:00 · Latest: 2026-02-08T14:54:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\tilde{O}(\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用参数化单链平均奖励约束MDP的遗憾分析</div>
<div class="mono" style="margin-top:8px">本研究在单链假设和通用策略参数化框架下，探讨无限时域平均奖励约束马尔可夫决策过程（CMDPs）。现有约束强化学习的遗憾分析主要依赖遍历性或强混合时间假设，这些假设在瞬态存在时失效。我们提出一种原始-对偶自然行动者-评论家算法，该算法利用多级蒙特卡洛估计器和显式预热机制处理单链动态，无需混合时间预言机。分析建立了有限时间遗憾和累积约束违反边界，其尺度为$\tilde{O}(\sqrt{T})$（受策略与评论家参数化近似误差影响），从而将阶次最优保证扩展到更广泛的CMDP类别。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the regret analysis of average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations, motivated by the limitation of existing analyses that rely on restrictive ergodicity or mixing-time assumptions which exclude systems with transient states. The method introduces a primal-dual natural actor-critic algorithm that employs multi-level Monte Carlo estimators and a burn-in mechanism to manage unichain dynamics without needing mixing-time oracles. The main experimental results demonstrate that the algorithm achieves finite-time regret and cumulative constraint violation bounds of order ˜O(√T), up to approximation errors from parameterization, thereby extending order-optimal performance guarantees to a broader class of CMDPs.</div>
<div class="mono" style="margin-top:8px">本文研究了在单链假设和一般策略参数化下的平均奖励约束马尔可夫决策过程（CMDPs）的遗憾分析，其动机在于现有约束强化学习的分析大多依赖于遍历性或强混合时间假设，这些假设在存在瞬态状态时不再成立。方法上提出了一种原始-对偶自然演员-评论家算法，利用多级蒙特卡洛估计器和显式预热机制来处理单链动态，无需混合时间预言机。主要实验结果表明，该算法实现了有限时间遗憾和累积约束违反的界，其阶数为˜O(√T)，仅受策略和评论家参数化近似误差的影响，从而将阶最优保证扩展到更广泛的CMDP类别。</div>
</details>
</div>
<div class="card">
<div class="title">When Is Compositional Reasoning Learnable from Verifiable Rewards?</div>
<div class="meta-line">Authors: Daniel Barzilai, Yotam Wolf, Ronen Basri</div>
<div class="meta-line">First: 2026-02-08T14:31:54+00:00 · Latest: 2026-02-08T14:31:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07992v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07992v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时可从可验证奖励中习得组合推理？</div>
<div class="mono" style="margin-top:8px">通过可验证奖励的强化学习（RLVR），大型语言模型中组合推理能力的涌现已成为近期实证成功的关键驱动力。尽管取得进展，但仅凭结果层面的反馈，哪些组合问题在此设定下可学习仍不明确。本研究从理论上探讨了自回归模型在RLVR训练下组合问题的可学习性。我们提出了一个称为任务优势比的概念，它是组合问题与基础模型的联合属性，用于刻画哪些任务和组合可从结果层面反馈中学习。积极方面，基于此特征，我们证明了正确中间步骤能提供明确优势的组合问题可通过RLVR高效学习，并分析了这种优势在不同问题中的自然产生机制。消极方面，当结构优势缺失时，RLVR可能收敛至次优组合。我们证明在某些情况下，基础模型的质量决定了此类优势是否存在以及RLVR是否会收敛至次优解。希望我们的分析能为理解RLVR何时及为何成功或失败提供原则性的理论依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the conditions under which compositional reasoning can be learned by autoregressive models through reinforcement learning with verifiable rewards (RLVR), which provides only outcome-level feedback. The authors introduce a theoretical framework centered on a &#x27;task-advantage ratio,&#x27; a property of both the problem and the base model, which determines learnability. They demonstrate that compositional problems where correct intermediate steps offer a clear structural advantage are efficiently learnable via RLVR, and analyze how such advantages naturally emerge. Conversely, they prove that when this advantage is absent, RLVR may converge to suboptimal solutions, with the base model&#x27;s quality sometimes dictating this outcome, thereby offering a principled understanding of RLVR&#x27;s successes and failures.</div>
<div class="mono" style="margin-top:8px">本文研究了在何种条件下，自回归模型能够通过可验证奖励的强化学习（RLVR）从结果级反馈中习得组合推理能力。作者提出了一个以“任务优势比”为核心的理论框架，该性质由问题和基础模型共同决定，并用于表征可学习性。研究证明，当正确的中间步骤能提供明确结构优势时，组合问题可通过RLVR高效学习，并分析了这种优势的自然产生机制。反之，若缺乏这种优势，RLVR可能收敛到次优解，且基础模型的质量有时会决定这一结果，从而为理解RLVR的成功与失败提供了原则性的理论依据。</div>
</details>
</div>
<div class="card">
<div class="title">Accelerating Social Science Research via Agentic Hypothesization and Experimentation</div>
<div class="meta-line">Authors: Jishu Sen Gupta, Harini SI, Somesh Kumar Singh, Syed Mohamad Tawseeq, Yaman Kumar Singla, David Doermann, Rajiv Ratn Shah, Balaji Krishnamurthy</div>
<div class="meta-line">First: 2026-02-08T14:20:56+00:00 · Latest: 2026-02-08T14:20:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07983v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过智能体化假设生成与实验加速社会科学研究</div>
<div class="mono" style="margin-top:8px">数据驱动的社会科学研究本质上是缓慢的，依赖于观察、假设生成和实验验证的迭代循环。虽然近期的数据驱动方法有望加速部分流程，但大多未能支持端到端的科学发现。为填补这一空白，我们提出了EXPERIGEN——一个受贝叶斯优化启发的智能体框架，通过两阶段搜索实现端到端发现：生成器提出候选假设，实验器进行实证评估。在多个领域中，EXPERIGEN持续发现比现有方法多2-4倍的统计显著假设，其预测能力提升7-17%，并能自然扩展到多模态与关系型数据集等复杂数据场景。除统计性能外，假设还需具备新颖性、实证基础与可操作性以推动实际科学进展。为评估这些特质，我们对机器生成的假设进行了专家评审，收集了资深教师的反馈。在25个评审假设中，88%被评为中等或高度新颖，70%被认为具有影响力且值得深入研究，大多数展现出与资深研究生研究相当的严谨性。最后，鉴于最终验证需要现实证据，我们首次对LLM生成的假设进行了A/B测试，观察到p值小于1e-6的统计显著结果，效应量高达344%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the slow, iterative nature of data-driven social science research, this paper introduces EXPERIGEN, an agentic framework that automates end-to-end scientific discovery through a two-phase search inspired by Bayesian optimization, involving a hypothesis Generator and an empirical Experimenter. The method significantly outperforms prior approaches, discovering 2-4 times more statistically significant hypotheses that are 7-17% more predictive across various domains, including complex multimodal and relational data. Expert reviews of machine-generated hypotheses found 88% to be moderately or strongly novel and 70% impactful, while a real-world A/B test confirmed statistically significant results with a large effect size, demonstrating the framework&#x27;s practical validity.</div>
<div class="mono" style="margin-top:8px">针对数据驱动的社会科学研究过程缓慢、依赖迭代的问题，本文提出了EXPERIGEN框架，该框架通过受贝叶斯优化启发的两阶段搜索（包括假设生成器和实验评估器）来实现端到端的自动化科学发现。该方法在多个领域显著优于先前方法，发现的统计显著假设数量多出2-4倍，预测性能提升7-17%，并能处理多模态和关系型等复杂数据。专家评审显示，88%的机器生成假设具有中等或较强新颖性，70%被认为有影响力且值得深入；此外，首次对LLM生成假设进行的A/B测试获得了统计显著结果和大效应规模，验证了其实际有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Yingxiao Huo, Satya Prakash Dash, Radu Stoican, Samuel Kaski, Mingfei Sun</div>
<div class="meta-line">First: 2026-01-26T16:02:18+00:00 · Latest: 2026-02-08T11:05:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18626v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18626v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习中自然策略梯度的逆费舍尔矩阵秩-1近似方法</div>
<div class="mono" style="margin-top:8px">自然梯度因其快速收敛特性和协变权重更新特性，在深度强化学习中长期受到研究。然而，计算自然梯度需要在每次迭代中求取费舍尔信息矩阵的逆，这在计算上具有天然的高昂代价。本文提出一种高效可扩展的自然策略优化技术，利用秩-1近似替代完整的逆费舍尔矩阵。我们从理论上证明，在特定条件下，逆费舍尔矩阵的秩-1近似比策略梯度收敛更快，并且在某些条件下具有与随机策略梯度方法相同的样本复杂度。我们在多种环境中对本方法进行基准测试，结果表明其性能优于标准的演员-评论家方法和信赖域基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the computational challenge of inverting the Fisher Information Matrix (FIM) in natural policy gradient methods, which is essential for fast convergence but prohibitively expensive. The proposed method introduces a scalable natural policy optimization technique that uses a rank-1 approximation to the inverse FIM, theoretically demonstrating faster convergence than policy gradients and comparable sample complexity to stochastic policy gradients under certain conditions. Experimental results across diverse environments show that this approach outperforms standard actor-critic and trust-region baselines in performance.</div>
<div class="mono" style="margin-top:8px">本文的动机源于自然策略梯度方法中计算费舍尔信息矩阵逆矩阵的挑战，该计算对快速收敛至关重要但计算成本过高。所提出的方法采用了一种可扩展的自然策略优化技术，利用对逆费舍尔信息矩阵的秩-1近似，理论上证明了在某些条件下比策略梯度收敛更快，且与随机策略梯度方法具有相似的样本复杂度。在多种环境中的实验结果表明，该方法在性能上优于标准的演员-评论家和信任区域基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models</div>
<div class="meta-line">Authors: Caner Erden</div>
<div class="meta-line">Venue: International Journal of Complexity in Applied Science and Technology, 2026</div>
<div class="meta-line">First: 2025-12-17T21:09:19+00:00 · Latest: 2026-02-08T11:04:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15973v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15973v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic Rank Reinforcement Learning (DR-RL) approximations rely on static rank assumptions, limiting their flexibility across diverse linguistic contexts. Our method dynamically modulates ranks based on real-time sequence dynamics, layer-specific sensitivities, and hardware constraints. The core innovation is a deep reinforcement learning agent that formulates rank selection as a sequential policy optimization problem, strictly balancing attention fidelity against computational latency. To ensure stability during inference, we derive and employ online matrix perturbation bounds, enabling incremental rank updates without the prohibitive cost of full decomposition. Furthermore, the integration of a lightweight Transformer-based policy network and batched Singular Value Decomposition (SVD) operations ensures scalable deployment on modern architectures. Extensive experiments demonstrate that DR-RL significantly reduces Floating Point Operations (FLOPs) by over 40% in long-sequence regimes (L &gt; 4096) while maintaining downstream accuracy statistically equivalent to full-rank attention. Beyond standard language modeling benchmarks, we validate the real-world applicability of DR-RL on the GLUE benchmark. Specifically, our method achieves 92.78% accuracy on the SST-2 sentiment analysis task, matching the performance of full-rank baselines and outperforming static low-rank methods, such as Performer and Nyströmformer, by a significant margin.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态秩强化学习在大型语言模型自适应低秩多头自注意力机制中的应用</div>
<div class="mono" style="margin-top:8px">动态秩强化学习（DR-RL）方法依赖静态秩假设，限制了其在多样化语言场景中的灵活性。本研究提出一种基于实时序列动态、层级敏感度与硬件约束的动态秩调制方法。核心创新在于采用深度强化学习智能体，将秩选择建模为序列策略优化问题，严格权衡注意力保真度与计算延迟。为确保推理稳定性，我们推导并应用在线矩阵扰动边界，实现无需完整分解的增量秩更新。通过集成轻量级Transformer策略网络与批量化奇异值分解（SVD）运算，确保在现代架构上的可扩展部署。大量实验表明，DR-RL在长序列场景（L&gt;4096）中显著降低浮点运算量超40%，同时保持与全秩注意力模型统计等效的下游任务精度。除标准语言建模基准测试外，我们在GLUE基准上验证了DR-RL的实际适用性：在SST-2情感分析任务中达到92.78%准确率，与全秩基线性能持平，并显著超越Performer、Nyströmformer等静态低秩方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inflexibility of static low-rank approximations in multi-head self-attention by proposing Dynamic Rank Reinforcement Learning (DR-RL), which dynamically adjusts ranks based on sequence dynamics, layer sensitivities, and hardware constraints. The method employs a deep reinforcement learning agent to optimize rank selection as a sequential policy, balancing attention fidelity and computational latency, and uses online matrix perturbation bounds for stable inference with incremental updates. Experimental results show DR-RL reduces FLOPs by over 40% in long sequences while maintaining accuracy comparable to full-rank attention, and it achieves 92.78% accuracy on SST-2, outperforming static low-rank methods like Performer and Nyströmformer.</div>
<div class="mono" style="margin-top:8px">本文针对多头自注意力中静态低秩近似灵活性不足的问题，提出了动态秩强化学习方法，该方法能根据序列动态、层敏感性和硬件约束动态调整秩。其核心是使用深度强化学习智能体将秩选择建模为序列策略优化问题，平衡注意力保真度与计算延迟，并采用在线矩阵扰动边界实现稳定的增量更新。实验结果表明，DR-RL在长序列下将浮点运算量降低超过40%，同时保持与全秩注意力相当的准确性，在SST-2情感分析任务中达到92.78%的准确率，显著优于Performer和Nyströmformer等静态低秩方法。</div>
</details>
</div>
<div class="card">
<div class="title">AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering</div>
<div class="meta-line">Authors: Yuzhu Cai, Zexi Liu, Xinyu Zhu, Cheng Wang, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Di Jin, Siheng Chen</div>
<div class="meta-line">First: 2026-02-08T10:55:03+00:00 · Latest: 2026-02-08T10:55:03+00:00</div>
<div class="meta-line">Comments: 17 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07906v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07906v1">PDF</a> · <a href="https://github.com/yuzhu-cai/AceGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent&#x27;s learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AceGRPO：面向自主机器学习工程的自适应课程增强型群体相对策略优化</div>
<div class="mono" style="margin-top:8px">自主机器学习工程（MLE）要求智能体在长周期内执行持续迭代优化。尽管当前基于大语言模型的智能体展现出潜力，但现有基于提示的MLE智能体因参数冻结而存在行为停滞问题。强化学习虽能提供解决方案，但其在MLE中的应用受限于高昂的执行延迟和低效的数据选择。针对这些挑战，我们提出AceGRPO框架，其包含两大核心组件：（1）演化数据缓冲池——持续将执行轨迹转化为可复用的训练任务；（2）基于可学习性潜力函数的自适应采样机制——动态聚焦于智能体学习前沿的任务以最大化学习效率。基于AceGRPO训练的Ace-30B模型在MLE-Bench-Lite基准上实现100%有效提交率，性能接近前沿专有模型，并超越更大规模的开源基线模型（如DeepSeek-V3.2），展现出持续迭代优化的强大能力。代码已开源：https://github.com/yuzhu-cai/AceGRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of behavioral stagnation in prompt-based agents for Autonomous Machine Learning Engineering (MLE), where frozen parameters limit sustained iterative optimization. To overcome this, the authors propose AceGRPO, a method featuring an Evolving Data Buffer that repurposes execution traces into reusable tasks and an Adaptive Sampling mechanism guided by a Learnability Potential function to prioritize tasks at the learning frontier for efficiency. Experimental results show that the trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, matches the performance of proprietary frontier models, and surpasses larger open-source baselines like DeepSeek-V3.2, demonstrating robust capabilities for long-horizon optimization.</div>
<div class="mono" style="margin-top:8px">本文针对自主机器学习工程中基于提示的智能体因参数冻结导致行为停滞、难以持续迭代优化的问题，提出AceGRPO方法。该方法包含两个核心组件：演化数据缓冲区将执行轨迹转化为可重用训练任务，以及基于可学习潜力函数的自适应采样机制，动态优先选择学习边界任务以提升效率。实验结果表明，训练后的Ace-30B模型在MLE-Bench-Lite上实现了100%的有效提交率，性能接近专有前沿模型，并超越了如DeepSeek-V3.2等更大规模的开源基线，展现出强大的持续迭代优化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models</div>
<div class="meta-line">Authors: Giang Ngo, Dat Phan Trong, Dang Nguyen, Sunil Gupta, Svetha Venkatesh</div>
<div class="meta-line">First: 2026-02-08T10:53:52+00:00 · Latest: 2026-02-08T10:53:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07904v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07904v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM&#x27;s behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型贝叶斯优化的自适应采集函数选择</div>
<div class="mono" style="margin-top:8px">贝叶斯优化的性能关键取决于采集函数的选择，但不存在普遍最优的单一策略；最佳选择具有非平稳性和问题依赖性。现有自适应组合方法通常基于历史函数值进行决策，而忽略了剩余预算或代理模型特征等更丰富的信息。为此，我们提出LMABO框架，将预训练大语言模型作为贝叶斯优化过程的零样本在线策略器。在每次迭代中，LMABO通过结构化状态表征提示大语言模型从多样化组合中选择最合适的采集函数。在50个基准问题上的评估表明，LMABO相较于静态策略、自适应组合方法及其他基于大语言模型的基线均取得显著性能提升。我们证明大语言模型的行为是一种能适应实时进展的综合性策略，其优势源于将完整优化状态处理并综合为有效自适应策略的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the fact that Bayesian Optimization&#x27;s performance is highly sensitive to the choice of acquisition function, which lacks a universally optimal and static solution, while existing adaptive methods fail to leverage the full contextual information of the optimization process. The proposed method, LMABO, addresses this by employing a pre-trained Large Language Model as a zero-shot, online strategist; at each iteration, it uses a structured prompt describing the optimization state to select the most suitable acquisition function from a predefined portfolio. The main experimental results, based on an evaluation across 50 benchmark problems, show that LMABO significantly outperforms strong static baselines, adaptive portfolio methods, and other LLM-based approaches, with analysis indicating the LLM&#x27;s advantage comes from its ability to synthesize the complete, real-time optimization state into an effective adaptive policy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于贝叶斯优化的性能高度依赖于采集函数的选择，而缺乏一个普遍最优且固定的方案，同时现有的自适应方法未能充分利用优化过程的完整上下文信息。所提出的方法LMABO通过使用一个预训练的大语言模型作为零样本的在线策略师来解决这一问题；在每次迭代中，它利用描述优化状态的结构化提示，从一个预定义的函数组合中选择最合适的采集函数。在50个基准问题上的主要实验结果表明，LMABO显著优于强大的静态基线、自适应组合方法以及其他基于大语言模型的方法，分析显示大语言模型的优势源于其将完整的实时优化状态综合成有效自适应策略的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Long Chen, Yinkui Liu, Shen Li, Bo Tang, Xuemin Hu</div>
<div class="meta-line">First: 2026-02-08T09:42:06+00:00 · Latest: 2026-02-08T09:42:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07889v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07889v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于VQVAE与模糊聚类的离线强化学习高效反探索方法</div>
<div class="mono" style="margin-top:8px">伪计数是离线强化学习中一种有效的反探索方法，通过对状态-动作对进行计数并对罕见或未见数据施加高惩罚。现有方法通过离散化连续状态-动作对进行计数，但常面临维度灾难与信息损失问题，导致学习效率下降甚至策略失效。本文提出一种基于向量量化变分自编码器与模糊聚类的离线强化学习反探索方法：首先设计基于多码本VQVAE的高效伪计数方法以离散化状态-动作对，并构建相应反利用机制以应对维度灾难；其次开发基于模糊C均值聚类的码本更新机制，提升码本向量利用率以缓解信息损失。该方法在D4RL基准数据集上进行评估，实验表明其在多项复杂任务中优于现有最优方法且计算成本更低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing anti-exploration methods in offline reinforcement learning, which suffer from dimension disaster and information loss when discretizing continuous state-action pairs for pseudo-counting. The proposed method combines a Vector Quantized Variational Autoencoder (VQVAE) with fuzzy C-means clustering to efficiently discretize state-action pairs, using a multi-codebook VQVAE to mitigate dimension issues and a fuzzy clustering update mechanism to reduce information loss by improving codebook vector utilization. Experimental evaluation on the D4RL benchmark demonstrates that this approach outperforms state-of-the-art methods in multiple complex tasks while requiring lower computational costs.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中现有反探索方法在离散化连续状态-动作对时存在的维度灾难和信息丢失问题，提出了一种基于矢量量化变分自编码器（VQVAE）和模糊聚类的新方法。该方法利用多码本VQVAE高效离散化状态-动作对以应对维度灾难，并引入模糊C均值聚类更新码本，提高码本向量使用率以减少信息损失。在D4RL基准测试上的实验结果表明，该方法在多个复杂任务中性能优于现有先进方法，且计算成本更低。</div>
</details>
</div>
<div class="card">
<div class="title">ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation</div>
<div class="meta-line">Authors: Jingqi Zhou, Sheng Wang, DeZhao Deng, Junwen Lu, Junwei Su, Qintong Li, Jiahui Gao, Hao Wu, Jiyue Jiang, Lingpeng Kong, Chuan Wu</div>
<div class="meta-line">First: 2026-02-08T09:27:18+00:00 · Latest: 2026-02-08T09:27:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07883v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07883v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ToolSelf：通过工具驱动的内在适应统一任务执行与自我重构</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的智能体系统在处理复杂、长周期任务方面展现出巨大潜力，但其效能受限于预先设定的静态行为配置，无法适应动态任务变化。现有方法依赖人工编排或启发式修补，常面临泛化能力差与优化碎片化的问题。为突破这些限制，我们提出ToolSelf——一种支持工具驱动运行时自我重构的新范式。通过将配置更新抽象为可调用工具，ToolSelf将任务执行与自我调整统一至单一动作空间，实现从外部规则到内在参数的范式跃迁。智能体可依据任务进展自主更新子目标与上下文，并相应调整策略与工具集，从而从被动执行者转变为任务与自我的双重管理者。我们进一步设计配置感知两阶段训练（CAT），结合拒绝采样微调与轨迹级强化学习以内化这种元能力。跨多基准实验表明，ToolSelf在泛化至新任务的同时媲美专业工作流，平均性能提升24.1%，为真正自适应的智能体指明了路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ToolSelf, a paradigm designed to overcome the limitations of static configurations in LLM-powered agents, which hinder adaptation to dynamic task demands. The method unifies task execution and self-reconfiguration by treating configuration updates as callable tools, enabling agents to autonomously adjust sub-goals, context, strategies, and toolboxes during runtime. Experimental results across diverse benchmarks show that ToolSelf achieves a 24.1% average performance improvement, rivaling specialized workflows and generalizing effectively to novel tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了ToolSelf范式，旨在解决基于大语言模型的智能体因静态配置而无法适应动态任务需求的局限性。该方法通过将配置更新抽象为可调用工具，统一了任务执行与自我重配置，使智能体能在运行时自主调整子目标、上下文、策略及工具箱。在多样化基准测试中的实验结果表明，ToolSelf实现了平均24.1%的性能提升，其表现可与专用工作流相媲美，并能有效泛化至新任务。</div>
</details>
</div>
<div class="card">
<div class="title">Direct Soft-Policy Sampling via Langevin Dynamics</div>
<div class="meta-line">Authors: Donghyeon Ki, Hee-Jun Ahn, Kyungyoon Kim, Byung-Jun Lee</div>
<div class="meta-line">First: 2026-02-08T09:01:54+00:00 · Latest: 2026-02-08T09:01:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07873v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07873v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于朗之万动力学的直接软策略采样</div>
<div class="mono" style="margin-top:8px">强化学习中的软策略将策略定义为状态-动作值函数上的玻尔兹曼分布，为探索与利用的平衡提供了理论机制。然而在实践中实现此类软策略仍具挑战：现有方法或依赖表达能力有限的参数化策略，或采用基于扩散的策略——其难以处理的似然函数阻碍了软策略目标中可靠的熵估计。本文通过朗之万动力学直接实现软策略采样，该动力学由Q函数的动作梯度驱动。这一视角催生了朗之万Q学习（LQL），其无需显式参数化策略即可从目标玻尔兹曼分布中采样动作。但直接应用朗之万动力学在高维非凸Q函数景观中混合速度缓慢，限制了实际效能。为此，我们提出噪声条件化朗之万Q学习（NC-LQL），将多尺度噪声扰动整合至值函数中。NC-LQL通过学习噪声条件化Q函数，构建出渐进平滑的值函数景观序列，使采样过程能从全局探索过渡至精确模态优化。在OpenAI Gym MuJoCo基准测试中，NC-LQL相比最先进的基于扩散的方法展现出竞争力，为在线强化学习提供了简洁而有效的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of realizing soft policies in reinforcement learning, which balance exploration and exploitation via Boltzmann distributions but are often limited by parametric constraints or intractable likelihoods in diffusion-based approaches. The method introduces Langevin Q-Learning (LQL), which samples actions directly from the target distribution using Langevin dynamics on the Q-function gradient, and enhances it with Noise-Conditioned LQL (NC-LQL) to improve sampling efficiency by learning a noise-conditioned Q-function that smoothes the value landscape across multiple scales. Experimental results on OpenAI Gym MuJoCo benchmarks show that NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, offering a simple yet effective solution for online RL.</div>
<div class="mono" style="margin-top:8px">本文的动机源于强化学习中实现软策略的挑战，软策略通过玻尔兹曼分布平衡探索与利用，但现有方法常受限于参数化策略的表达能力或基于扩散策略的似然函数难以估计。方法上提出了朗之万Q学习（LQL），利用朗之万动力学在Q函数梯度上直接采样动作，并进一步引入噪声条件化LQL（NC-LQL），通过学习多尺度噪声条件下的Q函数来平滑值函数景观，从而提升采样效率。在OpenAI Gym MuJoCo基准测试中，NC-LQL取得了与最先进的基于扩散的方法相竞争的性能，为在线强化学习提供了一种简洁而有效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation</div>
<div class="meta-line">Authors: Shijie Wang, Pengfei Li, Yikun Fu, Kaifeng Liu, Fangyuan Li, Yang Liu, Xiaowei Sun, Zonglin Li, Siyao Zhao, Jian Zhao, Kai Tian, Dong Li, Junqi Gao, Yutong Zhang, Yiqun Chen, Yuqiang Li, Zoe Li, Weinan Zhang, Peng Ye, Shuyue Hu, Lei Bai, Bowen Zhou, Kaiyan Zhang, Biqing Qi</div>
<div class="meta-line">First: 2026-02-08T07:28:44+00:00 · Latest: 2026-02-08T07:28:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07848v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07848v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARTI-MARS$^2$：基于强化学习的多智能体自搜索扩展用于代码生成</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLMs）的复杂推理能力备受关注，但单智能体系统在代码生成等复杂任务中常面临固有性能瓶颈。多智能体协作为突破这些限制提供了可行路径。然而，现有框架通常依赖基于提示的测试时交互或同质参数训练的多角色配置，限制了纠错能力与策略多样性。本文提出一种融合自搜索扩展的多智能体强化训练与推理框架（MARTI-MARS2），通过将多智能体协同探索过程建模为动态可学习环境，实现策略学习与多智能体树搜索的结合。该框架允许智能体在环境中迭代探索与优化，推动从参数共享的同质多角色训练向异质多智能体训练的演进，从而突破单智能体能力极限。我们还提出高效推理策略MARTI-MARS2-T+，以充分释放测试阶段多智能体协作的扩展潜力。我们在不同模型规模（8B、14B、32B）上针对挑战性代码生成基准展开广泛实验。通过两个32B模型的协作，MARTI-MARS2取得了77.7%的优异表现，超越GPT-5.1等强基线。此外，MARTI-MARS2揭示了一种新的扩展规律：从单智能体到同质多角色，最终向异质多智能体范式的演进，能逐步提升强化学习性能上限、增强文本到语音（TTS）鲁棒性并扩大策略多样性，表明策略多样性对通过多智能体强化学习扩展智能至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the performance limitations of single-agent large language models in complex code generation tasks and the constrained error correction and strategic diversity of existing multi-agent frameworks. It proposes MARTI-MARS2, a framework that integrates reinforcement learning policy training with multi-agent tree search, formulating collaboration as a learnable environment to evolve from homogeneous to heterogeneous multi-agent training, and introduces an efficient inference strategy, MARTI-MARS2-T+, to leverage multi-agent scaling at test time. Experimental results on code generation benchmarks show that using two collaborating 32B models achieves a 77.7% score, outperforming strong baselines, and reveals a novel scaling law where transitioning from single-agent to heterogeneous multi-agent paradigms progressively enhances reinforcement learning performance ceilings, test-time search capabilities, and policy diversity.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决单智能体大语言模型在复杂代码生成任务中的性能瓶颈，以及现有多智能体框架在错误纠正和策略多样性方面的局限。该方法提出了MARTI-MARS2框架，将强化学习策略训练与多智能体树搜索相结合，把协作过程构建为可学习的环境，以实现从同质多角色到异质多智能体训练的演进，并引入了高效的推理策略MARTI-MARS2-T+以在测试时发挥多智能体扩展潜力。在代码生成基准上的实验结果表明，使用两个协作的32B模型取得了77.7%的分数，优于强基线模型，并揭示了一种新的扩展规律：从单智能体到异质多智能体范式的转变逐步提高了强化学习性能上限、测试时搜索能力和策略多样性。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Exploration via Policy Priors</div>
<div class="meta-line">Authors: Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros, Andreas Krause</div>
<div class="meta-line">First: 2026-01-27T13:45:28+00:00 · Latest: 2026-02-08T07:05:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19612v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19612v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略先验的安全探索</div>
<div class="mono" style="margin-top:8px">安全探索是强化学习智能体在受控环境（如仿真环境）之外进行在线学习与适应的关键需求。本研究通过利用次优但保守的策略（例如从离线数据或模拟器中获得）作为先验来应对这一挑战。我们提出的SOOPER方法采用概率动力学模型进行乐观探索，同时在必要时悲观地回退至保守策略先验。我们证明SOOPER能在整个学习过程中保障安全性，并通过界定其累积遗憾来确保收敛至最优策略。在关键安全强化学习基准和实际硬件上的大量实验表明，SOOPER具备可扩展性，性能优于现有最优方法，并在实践中验证了我们的理论保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to enable safe online reinforcement learning beyond simulated environments by leveraging suboptimal but conservative policies as priors. The method, named SOOPER, employs probabilistic dynamics models to explore optimistically while pessimistically reverting to the conservative policy prior when necessary to ensure safety. Experimental results on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms state-of-the-art methods, and validates its theoretical safety and convergence guarantees in practice.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用次优但保守的策略作为先验，使强化学习能在模拟环境之外安全地进行在线学习。该方法名为SOOPER，采用概率动力学模型进行乐观探索，同时在必要时悲观地回退到保守策略先验以确保安全性。在关键的安全强化学习基准和真实硬件上的实验结果表明，SOOPER具有可扩展性，性能优于现有最优方法，并在实践中验证了其理论上的安全性和收敛性保证。</div>
</details>
</div>
<div class="card">
<div class="title">HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing</div>
<div class="meta-line">Authors: Chengyu Du, Xintao Wang, Aili Chen, Weiyuan Li, Rui Xu, Junteng Liu, Zishan Huang, Rong Tian, Zijun Sun, Yuhao Li, Liheng Feng, Deming Ding, Pengyu Zhao, Yanghua Xiao</div>
<div class="meta-line">First: 2026-01-29T09:35:27+00:00 · Latest: 2026-02-08T06:45:25+00:00</div>
<div class="meta-line">Comments: 41pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21459v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.21459v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters&#x27; first-person thinking from LLMs&#x27; third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train HER models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HER：面向大语言模型角色扮演的人类化推理与强化学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型角色扮演，即利用大语言模型模拟特定人物角色，已成为陪伴对话、内容创作、数字游戏等应用中的关键能力。现有模型虽能有效捕捉角色语调和知识，但模拟其行为背后的内在思维仍具挑战。为实现大语言模型角色扮演的认知模拟，先前研究主要存在两大不足：缺乏高质量推理轨迹的数据，以及缺少符合人类偏好的可靠奖励信号。本文提出HER——一个认知层面人物模拟的统一框架。该框架引入双层思维机制，区分角色的第一人称思维与大语言模型的第三人称思维。为弥合这些差距，我们通过逆向工程构建了推理增强的角色扮演数据，并建立了符合人类偏好的原则体系与奖励模型。基于这些资源，我们以Qwen3-32B为基础模型，通过监督学习与强化学习训练HER模型。大量实验验证了方法的有效性：我们的模型显著超越Qwen3-32B基线，在CoSER基准上提升30.26分，在Minimax角色扮演基准上提升14.97分。我们将公开数据集、原则体系与模型以促进后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of simulating the inner cognitive processes behind persona behaviors in LLM role-playing, which is crucial for applications like companionship and gaming. To overcome limitations in existing data and reward signals, the authors propose HER, a framework that introduces dual-layer thinking to separate character-first-person reasoning from LLM-third-person reasoning, and they curate reasoning-augmented data via reverse engineering while developing human-aligned principles and reward models. Experimental results show that HER models, trained on Qwen3-32B with supervised and reinforcement learning, significantly outperform the baseline, achieving improvements of 30.26 on the CoSER benchmark and 14.97 on the Minimax Role-Play Bench.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型角色扮演中模拟人物行为背后内在认知过程的挑战展开研究，这对于陪伴和游戏等应用至关重要。为克服现有数据和奖励信号的不足，作者提出了HER框架，引入双层思维以区分角色第一人称推理与模型第三人称推理，并通过逆向工程构建了推理增强数据，同时开发了符合人类偏好的原则和奖励模型。实验结果表明，基于Qwen3-32B通过监督和强化学习训练的HER模型显著优于基线，在CoSER基准上提升了30.26分，在Minimax角色扮演基准上提升了14.97分。</div>
</details>
</div>
<div class="card">
<div class="title">TodoEvolve: Learning to Architect Agent Planning Systems</div>
<div class="meta-line">Authors: Jiaxi Liu, Yanzuo Jiang, Guibin Zhang, Zihan Zhang, Heng Chang, Zhenfei Yin, Qibing Ren, Junchi Yan</div>
<div class="meta-line">First: 2026-02-08T06:37:01+00:00 · Latest: 2026-02-08T06:37:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TodoEvolve：学习构建智能体规划系统架构</div>
<div class="mono" style="margin-top:8px">规划已成为当代智能体系统处理复杂长周期任务的核心能力，但现有方法主要依赖固定的人工设计规划结构，缺乏适应开放性问题结构多样性的灵活性。为突破此局限，我们提出TodoEvolve——一种能自主合成并动态修订任务专属规划架构的元规划范式。具体而言，我们首先构建PlanFactory模块化设计空间，将拓扑结构、初始化、适应与导航等多样化规划范式标准化于统一代码库，从而为异构规划模式提供通用接口。依托PlanFactory，我们采集高质量规划轨迹，并通过《阻抗引导偏好优化》训练Todo-14B模型。该多目标强化学习框架能激励生成跨任意任务与智能体架构均具备高性能、稳定性与令牌效率的规划系统。在五项智能体基准测试中的实证评估表明，TodoEvolve在保持经济API成本与运行时开销的同时，持续超越精心设计的规划模块。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces TodoEvolve, motivated by the inflexibility of fixed, hand-crafted planning structures in agent systems for complex tasks. The method involves constructing PlanFactory, a modular design space that unifies diverse planning paradigms, and training the Todo-14B model using Impedance-Guided Preference Optimization to generate performant, stable, and token-efficient planning systems. Experimental results on five benchmarks show that TodoEvolve consistently outperforms engineered planning modules while maintaining low API costs and runtime overhead.</div>
<div class="mono" style="margin-top:8px">本文提出TodoEvolve，其动机在于现有智能体系统中用于复杂任务的固定、手工规划结构缺乏灵活性。方法包括构建PlanFactory这一模块化设计空间以统一不同规划范式，并采用阻抗引导偏好优化训练Todo-14B模型，以生成高效、稳定且令牌经济的规划系统。在五个基准测试上的实验结果表明，TodoEvolve持续超越精心设计的规划模块，同时保持了较低的API成本和运行时开销。</div>
</details>
</div>
<div class="card">
<div class="title">rePIRL: Learn PRM with Inverse RL for LLM Reasoning</div>
<div class="meta-line">Authors: Xian Wu, Kaijie Zhu, Ying Zhang, Lun Wang, Wenbo Guo</div>
<div class="meta-line">First: 2026-02-08T05:47:27+00:00 · Latest: 2026-02-08T05:47:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07832v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>rePIRL：基于逆强化学习的PRM学习框架用于大语言模型推理</div>
<div class="mono" style="margin-top:8px">过程奖励在深度强化学习中已被广泛用于提升训练效率、降低方差并防止奖励破解。在大语言模型推理领域，现有研究也探索了多种借助或不借助专家策略学习有效过程奖励模型的方法。然而，现有方法要么依赖对专家策略的强假设（如需要其奖励函数），要么存在内在局限性（如熵崩溃），导致PRM性能较弱或泛化能力有限。本文提出rePIRL——一种受逆强化学习启发的框架，能以对专家策略的最小假设学习有效的PRM。具体而言，我们设计了策略与PRM交替更新的双重学习过程，其定制化算法解决了传统逆强化学习扩展至大语言模型的挑战。理论证明该框架能统一在线与离线PRM学习方法，验证了rePIRL能以最小假设学习PRM。在标准化数学与代码推理数据集上的实证评估表明rePIRL优于现有方法。我们进一步展示了训练所得PRM在测试时训练、测试时扩展及为难题训练提供早期信号的应用价值，并通过详细消融实验验证了训练方案与关键设计选择。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for rePIRL is to overcome the limitations of existing methods for learning process reward models (PRMs) in LLM reasoning, which either rely on strong assumptions about expert policies or suffer from issues like entropy collapse, leading to weak or non-generalizable PRMs. The method introduces an inverse reinforcement learning-inspired framework that learns effective PRMs with minimal expert assumptions via a dual learning process that alternately updates the policy and PRM, incorporating customized techniques to scale inverse RL for LLMs. Main experimental results on math and coding reasoning datasets show rePIRL outperforms existing methods, with additional applications demonstrated in test-time training, scaling, and providing early training signals, supported by ablation studies validating the design.</div>
<div class="mono" style="margin-top:8px">rePIRL的动机是解决现有大语言模型推理中过程奖励模型学习方法存在的局限，这些方法要么依赖专家策略的强假设，要么存在熵崩溃等问题，导致PRM效果弱或泛化性差。该方法提出一个受逆强化学习启发的框架，通过交替更新策略和PRM的双重学习过程，以最小专家假设学习有效的PRM，并采用定制技术将传统逆强化学习扩展至大语言模型。在数学和代码推理数据集上的主要实验结果表明，rePIRL优于现有方法，其训练出的PRM还能应用于测试时训练、测试时扩展及为难题训练提供早期信号，消融研究进一步验证了其关键设计。</div>
</details>
</div>
<div class="card">
<div class="title">Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning</div>
<div class="meta-line">Authors: Jiahui Zhou, Dan Li, Boxin Li, Xiao Zhang, Erli Meng, Lin Li, Zhuomin Chen, Jian Lou, See-Kiong Ng</div>
<div class="meta-line">First: 2026-02-08T05:42:35+00:00 · Latest: 2026-02-08T05:42:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07830v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07830v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向定制化大语言模型推理的时间序列推理：基于过程可验证思维数据合成与调度</div>
<div class="mono" style="margin-top:8px">时间序列是跨多个应用领域的普遍数据类型，使得合理解决各类时间序列任务成为长期目标。大语言模型（LLMs）的最新进展，特别是通过强化学习（RL）解锁的推理能力，为处理需要长链思维（CoT）推理的任务提供了新机遇。然而，利用LLM进行时间序列推理仍处于起步阶段，主要受限于缺乏精心构建的时间序列CoT训练数据、数据调度探索不足导致的数据效率低下，以及缺少专门用于利用此类时间序列CoT数据的RL算法。本文提出VeriTime框架，通过数据合成、数据调度和RL训练定制LLM用于时间序列推理。首先，我们设计了一种数据合成流程，构建具有过程可验证标注的时序-文本多模态数据集。其次，我们开发了一种数据调度机制，依据难度层级与任务分类学原则组织训练样本。第三，我们提出一种两阶段强化微调方法，利用可验证的过程级CoT数据设计细粒度多目标奖励机制。大量实验表明，VeriTime显著提升了LLM在多种时间序列推理任务上的性能。值得注意的是，该方法使紧凑的3B、4B模型获得了与更大规模专有LLM相当甚至更优的推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the nascent and challenging application of large language models (LLMs) to time series reasoning, which is hindered by a lack of specialized training data, inefficient data scheduling, and unsuitable reinforcement learning algorithms. The proposed method, VeriTime, introduces a three-part framework: synthesizing a multimodal time series-text dataset with process-verifiable annotations, scheduling training data based on difficulty and task taxonomy, and applying a two-stage reinforcement fine-tuning with multi-objective rewards. The main experimental results demonstrate that VeriTime significantly enhances LLM performance on diverse time series reasoning tasks, enabling compact 3B and 4B parameter models to match or surpass the capabilities of much larger proprietary LLMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大语言模型应用于时间序列推理这一新兴且具有挑战性的问题，该领域因缺乏专门训练数据、低效的数据调度以及不合适的强化学习算法而发展受阻。所提出的方法VeriTime是一个包含三个部分的框架：合成具有过程可验证注释的多模态时间序列-文本数据集，基于难度和任务分类学调度训练数据，以及应用具有多目标奖励的两阶段强化微调。主要实验结果表明，VeriTime显著提升了大型语言模型在多种时间序列推理任务上的性能，使得紧凑的30亿和40亿参数模型能够达到甚至超越更大规模专有模型的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Functional Critics Are Essential for Actor-Critic: From Off-Policy Stability to Efficient Exploration</div>
<div class="meta-line">Authors: Qinxun Bai, Yuxuan Han, Wei Xu, Zhengyuan Zhou</div>
<div class="meta-line">First: 2025-09-26T21:55:26+00:00 · Latest: 2026-02-08T05:31:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22964v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.22964v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The actor-critic (AC) framework has achieved strong empirical success in off-policy reinforcement learning but suffers from the &quot;moving target&quot; problem, where the evaluated policy changes continually. Functional critics, or policy-conditioned value functions, address this by explicitly including a representation of the policy as input. While conceptually appealing, previous efforts have struggled to remain competitive against standard AC. In this work, we revisit functional critics within the actor-critic framework and identify two critical aspects that render them a necessity rather than a luxury. First, we demonstrate their power in stabilizing the complex interplay between the &quot;deadly triad&quot; and the &quot;moving target&quot;. We provide a convergent off-policy AC algorithm under linear functional approximation that dismantles several longstanding barriers between theory and practice: it utilizes target-based TD learning, accommodates dynamic behavior policies, and operates without the restrictive &quot;full coverage&quot; assumptions. By formalizing a dual trust-coverage mechanism, our framework provides principled guidelines for pursuing sample efficiency-rigorously governing behavior policy updates and critic re-evaluations to maximize off-policy data utility. Second, we uncover a foundational link between functional critics and efficient exploration. We demonstrate that existing model-free approximations of posterior sampling are limited in capturing policy-dependent uncertainty, a gap the functional critic formalism bridges. These results represent, to our knowledge, first-of-their-kind contributions to the RL literature. Practically, we propose a tailored neural network architecture and a minimalist AC algorithm. In preliminary experiments on the DeepMind Control Suite, this implementation achieves performance competitive with state-of-the-art methods without standard implementation heuristics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>函数化评论家对演员-评论家框架不可或缺：从离策略稳定性到高效探索</div>
<div class="mono" style="margin-top:8px">演员-评论家框架在离策略强化学习中取得显著实证成功，但长期受&#x27;移动目标&#x27;问题困扰——即被评估策略持续变化。函数化评论家通过将策略表征显式纳入输入来解决此问题。尽管概念上具有吸引力，先前研究始终难以超越标准演员-评论家方法。本研究重新审视演员-评论家框架中的函数化评论家，揭示使其成为必要而非可选的两个关键维度：首先，我们证明其在稳定&#x27;致命三角&#x27;与&#x27;移动目标&#x27;复杂交互中的核心作用，提出线性函数逼近下的收敛离策略算法，该算法突破理论与实践的长期壁垒——支持基于目标的时序差分学习、适应动态行为策略、无需严格&#x27;完全覆盖&#x27;假设。通过形式化双重信任-覆盖机制，本框架为追求样本效率提供原则性指导，严格规范行为策略更新与评论家重评估以最大化离策略数据效用。其次，我们首次发现函数化评论家与高效探索的根本联系，证明现有后验采样的无模型近似在捕捉策略依赖性不确定性方面存在局限，而函数化评论家形式体系能弥合此鸿沟。这些成果构成强化学习领域首创性贡献。实践层面，我们提出定制神经网络架构与极简演员-评论家算法，在DeepMind控制套件的初步实验中，该实现无需标准启发式技巧即达到与前沿方法相当的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper revisits functional critics in actor-critic reinforcement learning, motivated by the need to address the &#x27;moving target&#x27; problem and the instability from the &#x27;deadly triad&#x27; in off-policy learning. The method introduces a convergent off-policy algorithm using linear functional approximation with policy-conditioned value functions, which avoids restrictive coverage assumptions and incorporates a dual trust-coverage mechanism for sample efficiency. Experimental results on the DeepMind Control Suite show that a proposed neural implementation achieves competitive performance with state-of-the-art methods without relying on standard heuristics, while also establishing a theoretical link between functional critics and improved exploration.</div>
<div class="mono" style="margin-top:8px">本文重新审视了演员-评论家强化学习中的函数评论家，其动机是解决离策略学习中的&#x27;移动目标&#x27;问题和&#x27;致命三元组&#x27;带来的不稳定性。该方法引入了一种使用线性函数近似的收敛性离策略算法，采用以策略为输入的条件价值函数，避免了严格的覆盖假设，并包含一个双重信任-覆盖机制以提高样本效率。在DeepMind Control Suite上的初步实验结果表明，所提出的神经网络实现无需标准启发式技巧即能达到与最先进方法竞争的性能，同时从理论上建立了函数评论家与高效探索之间的联系。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge-Centric Metacognitive Learning</div>
<div class="meta-line">Authors: Arun Kumar, Paul Schrater</div>
<div class="meta-line">First: 2024-02-08T01:41:28+00:00 · Latest: 2026-02-08T04:06:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.05346v4">Abs</a> · <a href="https://arxiv.org/pdf/2402.05346v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactions are central to intelligent reasoning and learning abilities, with the interpretation of abstract knowledge guiding meaningful interaction with objects in the environment. While humans readily adapt to novel situations by leveraging abstract knowledge acquired over time, artificial intelligence systems lack principled mechanisms for incorporating abstract knowledge into learning, leading to fundamental challenges in the emergence of intelligent and adaptive behavior. To address this gap, we introduce knowledge-centric metacognitive learning based on three key principles: natural abstractions, knowledge-guided interactions through interpretation, and the composition of interactions for problem solving. Knowledge learning facilitates the acquisition of abstract knowledge and the association of interactions with knowledge, while object interactions guided by abstract knowledge enable the learning of transferable interaction concepts, abstract reasoning, and generalization. This metacognitive mechanism provides a principled approach for integrating knowledge into reinforcement learning and offers a promising pathway toward intelligent and adaptive behavior in artificial intelligence, robotics, and autonomous systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以知识为中心的元认知学习</div>
<div class="mono" style="margin-top:8px">交互是智能推理与学习能力的核心，对抽象知识的解读引导着与环境中有意义对象的互动。人类能凭借长期积累的抽象知识快速适应新情境，而人工智能系统缺乏将抽象知识融入学习的机制化路径，这导致智能自适应行为的涌现面临根本性挑战。为弥补这一缺陷，我们提出基于三大核心原则的以知识为中心的元认知学习框架：自然抽象、通过解读实现知识引导的交互、以及面向问题解决的交互组合。知识学习促进抽象知识的获取及交互与知识的关联，而基于抽象知识引导的对象交互则支持可迁移交互概念的学习、抽象推理与泛化能力。该元认知机制为将知识整合至强化学习提供了原则性方法，为人工智能、机器人及自主系统实现智能自适应行为开辟了前景广阔的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the gap between human adaptability, which leverages abstract knowledge, and artificial intelligence systems that lack principled mechanisms for integrating such knowledge into learning, hindering intelligent and adaptive behavior. The method introduces knowledge-centric metacognitive learning, built on principles of natural abstractions, knowledge-guided interactions via interpretation, and composition of interactions for problem-solving, aiming to incorporate abstract knowledge into reinforcement learning frameworks. Experimental results suggest this approach facilitates the acquisition of abstract knowledge, enables transferable interaction concepts, and supports abstract reasoning and generalization, offering a promising pathway for adaptive AI and robotics.</div>
<div class="mono" style="margin-top:8px">本文的动机源于人类能利用抽象知识适应新情境，而人工智能系统缺乏将抽象知识整合到学习中的原则性机制，这阻碍了智能和自适应行为的涌现。方法上提出了以知识为中心的元认知学习，基于自然抽象、通过解释进行知识引导的交互以及交互组合解决问题三大原则，旨在将抽象知识融入强化学习框架。实验结果表明，该方法促进了抽象知识的获取，实现了可迁移的交互概念学习，并支持抽象推理和泛化能力，为人工智能、机器人和自主系统的智能自适应行为提供了有前景的途径。</div>
</details>
</div>
<div class="card">
<div class="title">VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos</div>
<div class="meta-line">Authors: Wenqi Liu, Yunxiao Wang, Shijie Ma, Meng Liu, Qile Su, Tianke Zhang, Haonan Fan, Changyi Liu, Kaiyu Jiang, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Yinwei Wei, Xuemeng Song</div>
<div class="meta-line">First: 2026-02-08T03:45:50+00:00 · Latest: 2026-02-08T03:45:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07801v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07801v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoTemp-o3：在视频代理思维中协调时序定位与视频理解</div>
<div class="mono" style="margin-top:8px">在长视频理解中，传统的均匀帧采样常无法捕捉关键视觉证据，导致性能下降和幻觉增加。为解决此问题，近期出现了视频代理思维范式，采用定位-剪辑-回答流程，使模型主动识别相关视频片段、在片段内进行密集采样后生成答案。然而，现有方法效率低下、定位能力弱且依赖僵化流程。为此，我们提出VideoTemp-o3——一个统一视频代理思维框架，可联合建模视频定位与问答任务。该框架具备强定位能力，支持按需剪辑，并能修正不准确的定位结果。具体而言，在监督微调阶段，我们设计了统一的掩码机制以鼓励探索并抑制噪声；在强化学习中，引入专用奖励以缓解奖励攻击。此外，从数据层面构建了高质量长视频定位问答数据生成流程，并创建对应基准以系统评估不同时长视频。实验表明，本方法在长视频理解与定位任务上均取得显著性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of uniform frame sampling in long-video understanding, which often misses key evidence and causes hallucinations, this paper introduces VideoTemp-o3, a unified agentic framework that jointly models temporal grounding and question answering to improve efficiency and localization. The method employs a unified masking mechanism during supervised fine-tuning to balance exploration and noise, and introduces dedicated rewards in reinforcement learning to prevent reward hacking, while also developing a pipeline for high-quality grounded QA data. Experimental results show that VideoTemp-o3 achieves strong performance in both long-video understanding and temporal grounding tasks.</div>
<div class="mono" style="margin-top:8px">针对长视频理解中均匀帧采样常遗漏关键视觉证据并导致性能下降与幻觉的问题，本文提出了VideoTemp-o3，一个统一的智能视频思考框架，将时序定位与问答联合建模以提升效率与定位能力。方法在监督微调阶段设计了统一的掩码机制以鼓励探索并防止噪声，在强化学习中引入专用奖励以避免奖励黑客行为，同时还构建了高质量的长视频定位问答数据管道。实验结果表明，该方法在长视频理解和时序定位任务上均取得了显著性能。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Reasoning Re-ranker</div>
<div class="meta-line">Authors: Mingfu Liang, Yufei Li, Jay Xu, Kavosh Asadi, Xi Liu, Shuo Gu, Kaushik Rangadurai, Frank Shyu, Shuaiwen Wang, Song Yang, Zhijing Li, Jiang Liu, Mengying Sun, Fei Tian, Xiaohan Wei, Chonglin Sun, Jacob Tao, Shike Mei, Hamed Firooz, Wenlin Chen, Luke Simon</div>
<div class="meta-line">First: 2026-02-08T02:12:24+00:00 · Latest: 2026-02-08T02:12:24+00:00</div>
<div class="meta-line">Comments: 31 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07774v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07774v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2&#x27;s effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式推理重排序器</div>
<div class="mono" style="margin-top:8px">近期研究日益探索将大语言模型（LLMs）作为推荐系统的新范式，因其可扩展性和世界知识。然而，现有工作存在三个关键局限：（1）多数研究聚焦于检索与排序，而对优化最终推荐至关重要的重排序阶段被严重忽视；（2）LLMs通常以零样本或有监督微调方式使用，其推理能力——尤其是通过强化学习（RL）和高质量推理数据增强的能力——尚未充分挖掘；（3）物品常以非语义ID表示，这在拥有数十亿标识符的工业系统中带来严重的可扩展性挑战。为弥补这些不足，我们提出生成式推理重排序器（GR2），这是一个专为重排序设计的三阶段训练端到端框架。首先，通过分词器将非语义ID编码为语义ID（唯一性≥99%），并以此对预训练LLM进行中期训练。接着，通过精心设计的提示与拒绝采样，由更强的大规模LLM生成高质量推理轨迹，用于有监督微调以传授基础推理技能。最后，我们采用解耦裁剪与动态采样策略优化（DAPO），实现可扩展的RL监督，并专门为重排序设计可验证奖励机制。在两个真实数据集上的实验证明GR2的有效性：其在Recall@5和NDCG@5指标上分别超越当前最优方法OneRec-Think达2.4%和1.3%。消融实验证实高级推理轨迹能带来各项指标的显著提升。我们进一步发现RL奖励设计对重排序至关重要：LLMs倾向于通过保持物品顺序进行奖励攻击，这促使我们采用条件可验证奖励机制以抑制该行为并优化重排序性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the under-explored reranking phase in LLM-based recommendation systems, the limited use of LLM reasoning capabilities, and scalability issues from non-semantic item IDs, this paper introduces the Generative Reasoning Reranker (GR2). The method employs a three-stage pipeline: mid-training an LLM on semantic IDs for scalability, generating high-quality reasoning traces for supervised fine-tuning to enhance reasoning, and applying a novel RL approach (DAPO) with verifiable rewards tailored for reranking. Experimental results on real-world datasets show GR2 outperforms the state-of-the-art OneRec-Think, achieving gains of 2.4% in Recall@5 and 1.3% in NDCG@5, with ablations confirming the importance of advanced reasoning and careful RL reward design to prevent reward hacking.</div>
<div class="mono" style="margin-top:8px">本文针对基于大语言模型的推荐系统中重排序阶段研究不足、大语言模型推理能力利用有限以及非语义项目标识带来的可扩展性挑战，提出了生成式推理重排序器（GR2）。该方法采用三阶段训练流程：首先在语义标识上对预训练大语言模型进行中期训练以提高可扩展性；其次通过精心设计的提示和拒绝采样生成高质量推理轨迹，用于监督微调以增强推理能力；最后应用解耦裁剪和动态采样策略优化（DAPO）进行可扩展的强化学习监督，并设计了针对重排序的可验证奖励。在两个真实数据集上的实验表明，GR2在Recall@5和NDCG@5指标上分别比当前最优方法OneRec-Think提升了2.4%和1.3%，消融研究证实了高级推理轨迹的重要作用，并发现精心设计的强化学习奖励对于优化重排序性能和防止奖励黑客行为至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models</div>
<div class="meta-line">Authors: Hao Wang, Hao Gu, Hongming Piao, Kaixiong Gong, Yuxiao Ye, Xiangyu Yue, Sirui Han, Yike Guo, Dapeng Wu</div>
<div class="meta-line">First: 2026-02-02T15:53:55+00:00 · Latest: 2026-02-08T01:59:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02244v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02244v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>保持好奇心的学习：通过自适应自蒸馏实现大推理模型的熵保持监督微调</div>
<div class="mono" style="margin-top:8px">大推理模型的标准后训练流程——监督微调后接强化学习（SFT-then-RL）——可能限制强化学习阶段的收益：监督微调虽模仿专家示范，但常导致模型过度自信并降低生成多样性，使强化学习探索的解决方案空间变窄。在监督微调中添加熵正则化并非万能解药，它倾向于使词元分布趋于均匀，虽增加熵值却未提升有意义的探索能力。本文提出CurioSFT，一种通过内在好奇心增强探索能力的熵保持监督微调方法，包含：（a）自探索蒸馏——将模型蒸馏至自生成的温度缩放教师模型，以激发其能力范围内的探索；（b）熵引导温度选择——自适应调整蒸馏强度，通过增强推理词元的探索并稳定事实词元来缓解知识遗忘。在数学推理任务上的大量实验表明，在监督微调阶段，CurioSFT在分布内任务上超越基线方法2.5分，在分布外任务上提升2.9分。研究同时验证了监督微调阶段保留的探索能力能有效转化为强化学习阶段的具体收益，平均提升达5.0分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a limitation in the standard supervised fine-tuning (SFT) approach for large reasoning models, where SFT reduces generation diversity and leads to overconfidence, thereby hindering subsequent reinforcement learning (RL) exploration. To overcome this, the authors propose CurioSFT, a method that preserves entropy through adaptive self-distillation, combining self-exploratory distillation to encourage exploration within the model&#x27;s capabilities and entropy-guided temperature selection to adjust distillation strength adaptively, focusing exploration on reasoning tokens while stabilizing factual ones. Experimental results on mathematical reasoning tasks show that CurioSFT outperforms vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks during SFT, and it further enhances RL performance with an average improvement of 5.0 points.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型标准监督微调方法中存在的生成多样性降低和过度自信问题，提出了一种名为CurioSFT的熵保持监督微调方法，旨在通过自适应自蒸馏增强探索能力。该方法包括自探索蒸馏，通过温度缩放的自生成教师模型鼓励模型在能力范围内探索；以及熵引导温度选择，自适应调整蒸馏强度，在推理标记上放大探索同时稳定事实标记。在数学推理任务上的实验表明，CurioSFT在监督微调阶段比传统方法在分布内任务上提升2.5分，分布外任务上提升2.9分，并且所保留的探索能力在后续强化学习阶段带来了平均5.0分的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization</div>
<div class="meta-line">Authors: Tanmay Ambadkar, Sourav Panda, Shreyash Kale, Jonathan Dodge, Abhinav Verma</div>
<div class="meta-line">First: 2026-02-08T01:45:01+00:00 · Latest: 2026-02-08T01:45:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07764v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07764v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complete Pareto fronts. We show that this failure stems from two structural issues in current methods: destructive gradient interference caused by premature scalarization and representational collapse across the preference space. We introduce $D^3PO$, a PPO-based framework that reorganizes multi-objective policy optimization to address these issues directly. $D^3PO$ preserves per-objective learning signals through a decomposed optimization pipeline and integrates preferences only after stabilization, enabling reliable credit assignment. In addition, a scaled diversity regularizer enforces sensitivity of policy behavior to preference changes, preventing collapse. Across standard MORL benchmarks, including high-dimensional and many-objective control tasks, $D^3PO$ consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art hypervolume and expected utility while using a single deployable policy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>偏好条件化多目标强化学习：分解式、多样性驱动的策略优化</div>
<div class="mono" style="margin-top:8px">多目标强化学习（MORL）旨在学习平衡多个常相互冲突目标的策略。尽管单一偏好条件化策略是最灵活且可扩展的解决方案，但现有方法在实践中仍显脆弱，常无法恢复完整的帕累托前沿。我们证明该问题源于当前方法的两个结构性缺陷：过早标量化导致的破坏性梯度干扰，以及偏好空间中的表征坍缩。我们提出基于PPO的框架$D^3PO$，通过重组多目标策略优化直接解决这些问题。$D^3PO$通过分解式优化流程保留各目标的学习信号，仅在稳定后集成偏好，实现可靠的信用分配。此外，缩放多样性正则化器强制策略行为对偏好变化保持敏感，防止坍缩。在标准MORL基准测试（包括高维和多目标控制任务）中，$D^3PO$较先前单策略与多策略方法能持续发现更广、更高质量的帕累托前沿，在保持单一可部署策略的同时，在超体积与期望效用指标上达到或超越现有最优水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the practical brittleness of existing preference-conditioned multi-objective reinforcement learning (MORL) methods, which often fail to recover complete Pareto fronts due to destructive gradient interference from premature scalarization and representational collapse across preferences. The authors propose D³PO, a PPO-based framework that reorganizes optimization through a decomposed pipeline preserving per-objective learning signals and integrating preferences only after stabilization, alongside a scaled diversity regularizer to enforce policy sensitivity to preference changes. Experimental results on standard MORL benchmarks, including high-dimensional control tasks, show that D³PO consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art performance in hypervolume and expected utility with a single deployable policy.</div>
<div class="mono" style="margin-top:8px">本文针对现有偏好条件多目标强化学习方法在实践中脆弱、常无法恢复完整帕累托前沿的问题，指出其根源在于过早标量化导致的破坏性梯度干扰和偏好空间中的表征坍缩。作者提出了D³PO，这是一个基于PPO的框架，通过分解的优化流程保留各目标学习信号、在稳定后才整合偏好，并引入缩放多样性正则化器来确保策略对偏好变化的敏感性。在标准多目标强化学习基准测试（包括高维控制任务）上的实验结果表明，D³PO相比先前的单策略和多策略方法，能持续发现更广泛、更高质量的帕累托前沿，在超体积和期望效用指标上达到或超越最先进水平，且仅使用单个可部署策略。</div>
</details>
</div>
<div class="card">
<div class="title">Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control</div>
<div class="meta-line">Authors: Yongjie Fu, Lingyun Zhong, Zifan Li, Xuan Di</div>
<div class="meta-line">First: 2025-04-07T23:02:59+00:00 · Latest: 2026-02-07T23:59:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.05553v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.05553v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent reinforcement learning (MARL) has shown promise for adaptive traffic signal control (ATSC), enabling multiple intersections to coordinate signal timings in real time. However, in large-scale settings, MARL faces constraints due to extensive data sharing and communication requirements. Federated learning (FL) mitigates these challenges by training shared models without directly exchanging raw data, yet traditional FL methods such as FedAvg struggle with highly heterogeneous intersections. Different intersections exhibit varying traffic patterns, demands, and road structures, so performing FedAvg across all agents is inefficient. To address this gap, we propose Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs clustering-based or optimization-based techniques to dynamically group intersections and perform FedAvg independently within groups of intersections with similar characteristics, enabling more effective coordination and scalability than standard FedAvg.Our experiments on synthetic and real-world traffic networks demonstrate that HFRL consistently outperforms decentralized and standard federated RL approaches, and achieves competitive or superior performance compared to centralized RL as network scale and heterogeneity increase, particularly in real-world settings. The method also identifies suitable grouping patterns based on network structure or traffic demand, resulting in a more robust framework for distributed, heterogeneous systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向自适应交通信号控制的联邦分层强化学习</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）在自适应交通信号控制（ATSC）领域展现出潜力，可实现多路口信号配时的实时协同。然而在大规模场景中，MARL面临数据共享与通信需求带来的约束。联邦学习（FL）通过不直接交换原始数据训练共享模型来缓解这些挑战，但传统FL方法（如FedAvg）难以应对高度异质化的路口。不同路口具有差异化的交通模式、流量需求与道路结构，对所有智能体执行FedAvg效率低下。为此，我们提出面向ATSC的分层联邦强化学习（HFRL）。HFRL采用基于聚类或优化的技术动态划分路口群组，在特征相似的群组内独立执行FedAvg，相比标准FedAvg实现了更有效的协同与可扩展性。在合成与真实交通网络上的实验表明：HFRL始终优于分散式及标准联邦强化学习方法；随着网络规模与异质性增加（尤其在真实场景中），其性能达到甚至超越集中式强化学习。该方法还能根据网络结构或交通需求识别合适的群组模式，为分布式异质系统构建了更鲁棒的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying multi-agent reinforcement learning (MARL) to adaptive traffic signal control (ATSC) in large-scale, heterogeneous networks, where traditional federated learning methods like FedAvg are inefficient due to varying intersection characteristics. The authors propose Hierarchical Federated Reinforcement Learning (HFRL), which dynamically groups intersections with similar traffic patterns or structures using clustering or optimization techniques and performs FedAvg independently within each group to improve coordination and scalability. Experimental results on synthetic and real-world networks show that HFRL consistently outperforms decentralized and standard federated RL approaches, matches or exceeds centralized RL performance as scale and heterogeneity increase, and effectively identifies grouping patterns for a more robust distributed framework.</div>
<div class="mono" style="margin-top:8px">本文针对大规模异构网络中应用多智能体强化学习（MARL）进行自适应交通信号控制（ATSC）的挑战，指出传统联邦学习方法如FedAvg因交叉口特征差异而效率低下。作者提出了分层联邦强化学习（HFRL），该方法通过聚类或优化技术动态地将具有相似交通模式或结构的交叉口分组，并在各组内独立执行FedAvg，以提升协调性和可扩展性。在合成和真实交通网络上的实验表明，HFRL持续优于分散式和标准联邦RL方法，随着网络规模和异构性增加，其性能可媲美或超越集中式RL，并能有效识别分组模式，构建更鲁棒的分布式框架。</div>
</details>
</div>
<div class="card">
<div class="title">The Laplacian Keyboard: Beyond the Linear Span</div>
<div class="meta-line">Authors: Siddarth Chandrasekar, Marlos C. Machado</div>
<div class="meta-line">First: 2026-02-07T23:25:29+00:00 · Latest: 2026-02-07T23:25:29+00:00</div>
<div class="meta-line">Comments: 28 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07730v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07730v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拉普拉斯键盘：超越线性张成空间</div>
<div class="mono" style="margin-top:8px">在科学各领域中，拉普拉斯特征向量作为简化复杂系统的基础工具，广泛应用于信号处理至量子力学。在强化学习（RL）中，这些特征向量为逼近奖励函数提供了自然基；然而，其应用通常局限于线性张成空间，限制了在复杂环境中的表达能力。我们提出拉普拉斯键盘（LK）——一种超越线性张成空间的分层框架。LK利用这些特征向量构建任务无关的选项库，形成保证包含线性张成空间内任意奖励对应最优策略的行为基。元策略通过动态组合这些选项，实现对原始线性约束外策略的高效学习。我们建立了零样本逼近误差的理论界，并通过实验证明LK在超越零样本解的同时，相比标准RL方法获得了更高的样本效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation that Laplacian eigenvectors in reinforcement learning are typically confined to linear approximations of reward functions, this paper introduces the Laplacian Keyboard (LK), a hierarchical framework that extends beyond the linear span. The method constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis that theoretically includes optimal policies for rewards in the linear span, and employs a meta-policy to dynamically combine these options for broader expressivity. Experimental results show that LK outperforms zero-shot solutions and achieves better sample efficiency than standard RL methods, with theoretical bounds provided on zero-shot approximation error.</div>
<div class="mono" style="margin-top:8px">本文的动机是强化学习中拉普拉斯特征向量通常局限于奖励函数的线性近似，这限制了在复杂环境中的表达能力。为此，论文提出了拉普拉斯键盘（LK）这一分层框架，其方法基于这些特征向量构建一个与任务无关的选项库，形成行为基，确保包含线性跨度内任意奖励的最优策略，并通过元策略动态组合这些选项以超越线性约束。实验结果表明，LK在零样本近似误差方面有理论保证，实证上超越了零样本解决方案，并相比标准强化学习方法取得了更高的样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs</div>
<div class="meta-line">Authors: Sagnik Mukherjee, Lifan Yuan, Pavan Jayasinha, Dilek Hakkani-Tür, Hao Peng</div>
<div class="meta-line">First: 2026-02-07T23:25:26+00:00 · Latest: 2026-02-07T23:25:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07729v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我们真的需要Adam吗？LLM中SGD实现惊人强大且稀疏的强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL），特别是基于可验证奖励的强化学习（RLVR），已成为训练大型语言模型（LLM）的关键阶段，也是当前规模化研究的重点。然而，尽管近期研究强调了RL与预训练、监督微调等阶段存在本质差异，RL的优化实践仍主要沿用下一词预测阶段的惯例。其中一个惯例是广泛采用AdamW优化器训练大规模Transformer模型，尽管其内存开销较高。我们的分析表明，AdamW中的动量和自适应学习率在RL中的影响远小于在监督微调（SFT）中，这使我们推测RL从Adam风格的逐参数自适应学习率和动量中获益较少。实验证实了这一假设：在LLM的RL任务中，内存效率显著更高的SGD（已知在大规模Transformer的监督学习中表现不佳）达到甚至超越了AdamW的性能。值得注意的是，使用SGD进行全参数微调时，仅更新少于0.02%的模型参数（无需任何稀疏化正则化），比AdamW少更新超过1000倍。我们分析了导致这种更新稀疏性的潜在原因。这些发现为LLM中RL的优化动力学提供了新见解，并表明RL可能比以往认知更具参数效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether the AdamW optimizer, standard in large language model (LLM) training, is necessary for the reinforcement learning (RL) phase, given its high memory cost. The authors hypothesize that RL benefits less from Adam&#x27;s adaptive learning rates and momentum compared to supervised fine-tuning, and they test this by replacing AdamW with the simpler, memory-efficient SGD optimizer. Experimental results show that SGD matches or exceeds AdamW&#x27;s performance in RL for LLMs and, remarkably, achieves this by updating fewer than 0.02% of model parameters without explicit sparsity regularization, indicating RL can be far more parameter-efficient than previously thought.</div>
<div class="mono" style="margin-top:8px">本文探讨了在大语言模型（LLM）训练中，强化学习（RL）阶段是否仍需使用内存开销高的标准AdamW优化器。作者假设相比于监督微调，RL从Adam的自适应学习率和动量中获益较少，因此测试用更简单、内存高效的SGD优化器替代AdamW。实验结果表明，在LLM的RL中，SGD达到甚至超越了AdamW的性能，并且显著的是，它在没有显式稀疏正则化的情况下，仅更新少于0.02%的模型参数就实现了这一效果，这揭示出RL可能具有远超此前认知的参数效率。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Planning in Reinforcement Learning via Model Introspection</div>
<div class="meta-line">Authors: Gabriel Stella</div>
<div class="meta-line">First: 2026-02-07T21:49:21+00:00 · Latest: 2026-02-07T21:49:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07719v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07719v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模型内省的强化学习高效规划方法</div>
<div class="mono" style="margin-top:8px">强化学习与经典规划通常被视为两个不同的问题，其不同的形式化表述需要不同的解决方案。然而，当人类面对任务时，无论任务以何种方式描述，他们往往能推导出高效解决问题所需的额外信息。这种能力的关键在于内省：通过推理自身对问题的内部模型，人类直接合成与任务相关的附加信息。本文提出可将这种内省视为程序分析，并探讨了该方法如何应用于强化学习中各类模型的示例。随后，我们描述了一种算法，能够在关系强化学习所使用的模型类别上实现高效的目标导向规划，从而揭示了强化学习与经典规划之间的新颖联系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the observation that humans use introspection on their internal problem models to efficiently solve tasks, regardless of specification, and it seeks to bridge the gap between reinforcement learning and classical planning. The method proposes framing this introspection as a form of program analysis, which is then applied to various reinforcement learning models, and specifically introduces an algorithm for efficient goal-oriented planning within relational reinforcement learning models. The main experimental results demonstrate the algorithm&#x27;s effectiveness, establishing a novel connection between reinforcement learning and classical planning through this model introspection approach.</div>
<div class="mono" style="margin-top:8px">本文的动机源于观察到人类无论任务如何指定，都能通过对内部问题模型的内省来高效解决问题，旨在弥合强化学习与经典规划之间的差距。其方法将这种内省视为一种程序分析，并将其应用于各类强化学习模型，特别提出了一种在关系强化学习模型中进行高效目标导向规划的算法。主要实验结果证明了该算法的有效性，通过这种模型内省方法，在强化学习与经典规划之间建立了一种新颖的联系。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260210_0359.html">20260210_0359</a>
<a href="archive/20260209_0321.html">20260209_0321</a>
<a href="archive/20260208_0336.html">20260208_0336</a>
<a href="archive/20260207_0400.html">20260207_0400</a>
<a href="archive/20260206_0358.html">20260206_0358</a>
<a href="archive/20260205_0400.html">20260205_0400</a>
<a href="archive/20260204_0411.html">20260204_0411</a>
<a href="archive/20260202_0320.html">20260202_0320</a>
<a href="archive/20260201_0343.html">20260201_0343</a>
<a href="archive/20260131_0357.html">20260131_0357</a>
<a href="archive/20260130_0355.html">20260130_0355</a>
<a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
