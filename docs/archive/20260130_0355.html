<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-30 03:55</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260130_0355</div>
    <div class="row"><div class="card">
<div class="title">LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</div>
<div class="meta-line">Authors: Piyush Jha, Arnav Arora, Vijay Ganesh</div>
<div class="meta-line">Venue: AAAI 2025</div>
<div class="meta-line">First: 2024-11-13T18:44:30+00:00 · Latest: 2026-01-28T18:58:57+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.08862v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.08862v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLMStinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks. Unlike traditional methods, which require complex prompt engineering or white-box access, LLMStinger uses a reinforcement learning (RL) loop to fine-tune an attacker LLM, generating new suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Our method significantly outperforms existing red-teaming approaches (we compared against 15 of the latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for their extensive safety measures. Additionally, we achieved a 94.97% ASR on GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability of LLMStinger across open and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLMStinger：利用强化学习微调的大语言模型实现越狱攻击</div>
<div class="mono" style="margin-top:8px">本文提出LLMStinger创新方法，利用大语言模型自动生成对抗性后缀实施越狱攻击。与传统依赖复杂提示工程或白盒访问的方法不同，LLMStinger采用强化学习循环微调攻击者大语言模型，基于HarmBench基准中的恶意问题及现有攻击生成新型后缀。本方法显著优于现有红队测试方案（已对比15种最新方法），在具备严格安全措施的LLaMA2-7B-chat模型上实现攻击成功率提升57.2%，在Claude 2模型上提升50.3%。此外，在GPT-3.5和Gemma-2B-it模型上分别达到94.97%和99.4%的攻击成功率，证明了该方法在开源与闭源模型间的鲁棒性与适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to efficiently test the safety of large language models (LLMs) against jailbreak attacks without relying on complex manual engineering or white-box access, this paper introduces LLMStinger, a method that uses reinforcement learning to fine-tune an attacker LLM to automatically generate adversarial suffixes. The approach iteratively refines suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Experimental results demonstrate its superior performance, significantly outperforming 15 existing red-teaming methods with a +57.2% Attack Success Rate (ASR) improvement on LLaMA2-7B-chat and a +50.3% increase on Claude 2, while also achieving high ASRs of 94.97% on GPT-3.5 and 99.4% on Gemma-2B-it, showcasing robustness across both open and closed-source models.</div>
<div class="mono" style="margin-top:8px">本文旨在高效测试大语言模型（LLM）针对越狱攻击的安全性，避免依赖复杂的手工工程或白盒访问，为此提出了LLMStinger方法，该方法利用强化学习微调一个攻击者LLM来自动生成对抗性后缀。该方法基于HarmBench基准中的有害问题，利用现有攻击迭代优化后缀生成。实验结果表明，其性能显著优于15种现有红队方法，在LLaMA2-7B-chat上攻击成功率（ASR）提升了57.2%，在Claude 2上提升了50.3%，同时在GPT-3.5和Gemma-2B-it上分别达到94.97%和99.4%的ASR，证明了该方法在开源和闭源模型上的鲁棒性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning</div>
<div class="meta-line">Authors: Minwu Kim, Safal Shrestha, Keith Ross</div>
<div class="meta-line">First: 2026-01-28T18:29:21+00:00 · Latest: 2026-01-28T18:29:21+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20829v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20829v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model&#x27;s robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于失败前缀条件化的饱和问题推理模型训练</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）显著提升了大型语言模型（LLM）的推理能力，但随着问题趋于饱和，训练常陷入停滞。我们发现核心挑战在于信息性失败的可及性差：学习信号存在，但在标准推演中极少出现。为此，我们提出失败前缀条件化——一种从饱和问题中学习的简洁高效方法。该方法不再从原始问题出发，而是通过将训练条件化于罕见错误推理轨迹衍生的前缀，重新分配探索空间，使模型暴露于易失败状态。实验表明，失败前缀条件化带来的性能提升相当于中等难度问题的训练效果，同时保持标记效率。进一步分析模型鲁棒性发现，该方法能降低误导性失败前缀下的性能衰减，尽管对早期正确推理的遵循度略有折衷。最后，我们证明在训练中动态更新失败前缀的迭代方法，能在性能平台期后实现额外增益。总体而言，本研究结果表明失败前缀条件化为RLVR在饱和问题上的持续训练提供了有效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the training stagnation of large language models in Reinforcement Learning with Verifiable Rewards (RLVR) when problems become saturated, where informative failure signals are rarely encountered. The proposed method, failure-prefix conditioning, addresses this by reallocating exploration: instead of starting from the original question, training is conditioned on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. Experimental results show that this approach yields performance gains comparable to training on medium-difficulty problems while maintaining token efficiency, enhances robustness by reducing performance degradation under misleading prefixes, and an iterative variant unlocks further gains after plateaus, offering an effective pathway to extend RLVR training.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型在可验证奖励的强化学习中，当问题趋于饱和时训练停滞的难题，其核心挑战在于难以获取有信息量的失败信号。所提出的方法称为失败前缀条件化，它通过重新分配探索来解决这一问题：训练不是从原始问题开始，而是以从罕见错误推理轨迹中提取的前缀为条件，从而使模型暴露于易失败状态。实验结果表明，该方法在保持令牌效率的同时，取得了与在中等难度问题上训练相当的性能提升，并通过减少在误导性前缀下的性能下降增强了鲁棒性，且迭代变体能在性能平台期后实现进一步增益，为扩展饱和问题的RLVR训练提供了有效途径。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning via Self-Distillation</div>
<div class="meta-line">Authors: Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</div>
<div class="meta-line">First: 2026-01-28T17:45:12+00:00 · Latest: 2026-01-28T17:45:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20802v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#x27;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自蒸馏的强化学习</div>
<div class="mono" style="margin-top:8px">大型语言模型在代码和数学等可验证领域越来越多地通过强化学习进行后训练。然而，当前基于可验证奖励的强化学习方法仅从每次尝试的标量结果奖励中学习，形成了严重的信用分配瓶颈。许多可验证环境实际能提供丰富的文本反馈（如运行时错误或评判评估），用以解释尝试失败的原因。我们将此设定形式化为具有丰富反馈的强化学习，并提出自蒸馏策略优化方法，该方法将标记化反馈转化为密集学习信号，无需外部教师或显式奖励模型。SDPO将基于反馈的当前模型视为自教师，并将其反馈感知的下一标记预测蒸馏回策略中。通过这种方式，SDPO利用了模型在上下文中回溯识别自身错误的能力。在科学推理、工具使用和LiveCodeBench v6的竞技编程任务中，SDPO相比强RLVR基线显著提升了样本效率和最终准确率。值得注意的是，在仅返回标量反馈的标准RLVR环境中，SDPO通过将成功轨迹作为失败尝试的隐式反馈，同样超越了基线方法。最后，在测试时对单个问题应用SDPO可加速困难二元奖励任务的探索，仅需最佳k采样或多轮对话三分之一的尝试次数即可达到相同的发现概率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitation of current reinforcement learning with verifiable rewards (RLVR) methods, which rely solely on scalar outcome rewards and suffer from a credit-assignment bottleneck, despite many verifiable environments offering rich textual feedback like error messages. To address this, the authors formalize reinforcement learning with rich feedback and propose Self-Distillation Policy Optimization (SDPO), a method that converts tokenized feedback into a dense learning signal without external supervision by using the current model conditioned on feedback as a self-teacher to distill feedback-informed next-token predictions back into the policy. Experimental results across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6 show that SDPO improves sample efficiency and final accuracy over strong RLVR baselines, outperforms them even in standard scalar-feedback environments by leveraging successful rollouts as implicit feedback, and accelerates discovery on difficult binary-reward tasks at test time, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with three times fewer attempts.</div>
<div class="mono" style="margin-top:8px">本文的动机在于当前可验证奖励的强化学习方法仅依赖标量结果奖励，存在严重的信用分配瓶颈，而许多可验证环境实际提供丰富的文本反馈（如运行时错误）。为此，作者形式化了具有丰富反馈的强化学习设置，并提出了自蒸馏策略优化方法，该方法无需外部教师或显式奖励模型，通过将当前模型在反馈条件下的输出作为自教师，将其反馈感知的下一个词预测蒸馏回策略中，从而利用模型回顾性识别自身错误的能力。在科学推理、工具使用和LiveCodeBench v6的竞争性编程实验中，SDPO相比强基线在样本效率和最终准确率上均有提升，即使在仅返回标量反馈的标准环境中，通过使用成功轨迹作为失败尝试的隐式反馈也优于基线；此外，在测试时对单个问题应用SDPO可加速困难二元奖励任务的发现，以三分之一的尝试次数达到与最佳k采样或多轮对话相同的发现概率。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-01-28T17:27:08+00:00</div>
<div class="meta-line">Comments: 21 main pages, 7 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效代码库智能体</div>
<div class="mono" style="margin-top:8px">开源权重编程智能体本应具备超越闭源系统的根本优势：可针对私有代码库进行专业化训练，将仓库特定信息直接编码至权重中。然而训练成本与复杂性使这一优势长期停留于理论层面。本研究证明其现已具备可行性。我们提出软验证高效代码库智能体（SERA），这是一种高效的编程智能体训练方法，能够快速低成本创建专用于私有代码库的智能体。仅通过监督微调（SFT），SERA即在完全开源（开放数据、方法、代码）模型中取得最先进成果，同时达到如Devstral-Small-2等前沿开源权重模型的性能水平。创建达到同等性能的SERA模型成本比强化学习降低26倍，比先前合成数据方法降低57倍。我们的软验证生成（SVG）方法能从单个代码库生成数千条轨迹，结合成本效益优势，实现了对私有代码库的专业化适配。除仓库专业化外，我们将SVG应用于更大规模的代码库语料，生成超过20万条合成轨迹，并利用该数据集详细分析了编程智能体训练的缩放规律、消融实验及混杂因素。总体而言，我们相信这项工作将极大加速开源编程智能体的研究进程，并彰显可适配私有代码库的开源模型优势。我们将SERA作为Ai2开源编程智能体系列的首个模型发布，同时开放全部代码、数据及Claude Code集成方案以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SERA is to make the theoretical advantage of open-weight coding agents—specializing them to private codebases through training—practical by overcoming the high cost and complexity of existing methods. The method, Soft-Verified Efficient Repository Agents (SERA), employs an efficient approach using only supervised finetuning (SFT) and introduces Soft Verified Generation (SVG) to generate thousands of training trajectories from a single repository. Main experimental results show that SERA achieves state-of-the-art performance among fully open-source models, matches the performance of frontier open-weight models like Devstral-Small-2, and is significantly cheaper—26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods—to reach equivalent performance, enabling practical specialization to private repositories.</div>
<div class="mono" style="margin-top:8px">SERA的研究动机在于，通过克服现有方法的高成本与复杂性，使开源编码智能体能够实际应用于私有代码库的专业化训练，从而发挥其理论优势。该方法名为软验证高效仓库智能体（SERA），采用了一种高效策略，仅使用监督微调（SFT），并引入了软验证生成（SVG）技术，能够从单个代码仓库生成数千条训练轨迹。主要实验结果表明，SERA在完全开源的模型中取得了最先进的性能，与前沿开源权重模型如Devstral-Small-2表现相当，且成本显著降低——达到同等性能时，比强化学习方法便宜26倍，比之前的合成数据方法便宜57倍，从而实现了对私有代码库的实际专业化应用。</div>
</details>
</div>
<div class="card">
<div class="title">Less is More: Clustered Cross-Covariance Control for Offline RL</div>
<div class="meta-line">Authors: Nan Qiao, Sheng Yue, Shuning Wang, Yongheng Deng, Ju Ren</div>
<div class="meta-line">First: 2026-01-28T16:55:04+00:00 · Latest: 2026-01-28T16:55:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20765v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20765v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少即是多：面向离线强化学习的聚类交叉协方差控制</div>
<div class="mono" style="margin-top:8px">离线强化学习的核心挑战在于分布偏移。数据稀缺或由分布外区域主导的数据集会加剧这一问题。我们的理论分析与实验表明，标准平方误差目标会产生有害的时序差分交叉协方差。该效应在分布外区域被放大，导致优化偏差并损害策略学习。为抵消此机制，我们提出两种互补策略：一是分区缓冲采样，将更新限制在局部回放分区内，以减弱异常协方差效应并校准更新方向，形成易于集成至现有框架的方案——时序差分聚类交叉协方差控制；二是引入显式的基于梯度的校正惩罚项，在每次更新中消除协方差引起的偏差。我们证明缓冲分区能保持最大化目标的下界性质，且这些约束可在不改变策略约束型离线强化学习核心行为的前提下，缓解极端分布外区域的过度保守性。实验表明，本方法较现有方案具有更高稳定性，且在回报率上最高提升30%，尤其在小规模数据集及强调分布外区域的数据划分中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of distributional shift in offline reinforcement learning, particularly exacerbated by scarce or out-of-distribution (OOD) data, where the standard squared error objective induces harmful TD cross-covariance that biases optimization. The method introduces two strategies: partitioned buffer sampling, called Clustered Cross-Covariance Control for TD (C^4), which localizes updates to attenuate irregular covariance effects and align update directions, and an explicit gradient-based corrective penalty to cancel covariance-induced bias. Experimental results demonstrate that this approach maintains stability, reduces excessive conservatism in OOD areas, and achieves up to 30% improvement in returns over prior methods, especially with small datasets and OOD-emphasized splits.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中的分布偏移问题，特别是在数据稀缺或存在分布外区域时，标准平方误差目标会导致有害的TD交叉协方差，从而影响优化。方法提出了两种策略：分区缓冲采样（称为聚类交叉协方差控制C^4），通过局部化更新来减弱不规则协方差效应并调整更新方向，以及一种显式的基于梯度的校正惩罚来消除协方差引起的偏差。实验结果表明，该方法保持了稳定性，减少了在分布外区域的过度保守性，并在小数据集和强调分布外区域的数据划分上，相比先前方法实现了高达30%的回报提升。</div>
</details>
</div>
<div class="card">
<div class="title">Analysis of approximate linear programming solution to Markov decision problem with log barrier function</div>
<div class="meta-line">Authors: Donghwan Lee, Hyukjun Yang, Bum Geun Park</div>
<div class="meta-line">First: 2025-09-24T06:36:11+00:00 · Latest: 2026-01-28T16:42:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19800v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19800v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There are two primary approaches to solving Markov decision problems (MDPs): dynamic programming based on the Bellman equation and linear programming (LP). Dynamic programming methods are the most widely used and form the foundation of both classical and modern reinforcement learning (RL). By contrast, LP-based methods have been less commonly employed, although they have recently gained attention in contexts such as offline RL. The relative underuse of the LP-based methods stems from the fact that it leads to an inequality-constrained optimization problem, which is generally more challenging to solve effectively compared with Bellman-equation-based methods. The purpose of this paper is to establish a theoretical foundation for solving LP-based MDPs in a more effective and practical manner. Our key idea is to leverage the log-barrier function, widely used in inequality-constrained optimization, to transform the LP formulation of the MDP into an unconstrained optimization problem. This reformulation enables approximate solutions to be obtained easily via gradient descent. While the method may appear simple, to the best of our knowledge, a thorough theoretical interpretation of this approach has not yet been developed. This paper aims to bridge this gap.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于对数障碍函数的马尔可夫决策问题近似线性规划解分析</div>
<div class="mono" style="margin-top:8px">求解马尔可夫决策问题（MDP）主要有两种方法：基于贝尔曼方程的动态规划和线性规划（LP）。动态规划方法应用最广泛，是经典及现代强化学习（RL）的基础。相比之下，基于LP的方法较少使用，尽管近期在离线RL等场景中受到关注。这类方法相对未被充分利用，是因为其会转化为不等式约束优化问题，通常比基于贝尔曼方程的方法更难高效求解。本文旨在为更高效、实用地求解基于LP的MDP建立理论基础。核心思路是利用不等式约束优化中广泛使用的对数障碍函数，将MDP的LP形式转化为无约束优化问题，从而可通过梯度下降轻松获得近似解。尽管方法看似简单，但据我们所知，目前尚未有对此方法的完整理论阐释。本文旨在填补这一空白。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underutilization of linear programming (LP) methods for solving Markov decision problems (MDPs), which are typically constrained by inequalities and thus more challenging than dynamic programming approaches. The authors propose transforming the LP formulation into an unconstrained optimization problem using a log-barrier function, enabling approximate solutions via gradient descent. Their main contribution is establishing a theoretical foundation for this method, which, while simple in practice, had lacked rigorous interpretation, thereby making LP-based MDPs more effective and practical for applications like offline reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本文针对解决马尔可夫决策问题时线性规划方法使用不足的问题，该方法因受不等式约束而比基于动态规划的方法更具挑战性。作者提出使用对数障碍函数将线性规划公式转化为无约束优化问题，从而通过梯度下降获得近似解。其主要贡献是为这一方法建立了理论基础，该方法在实践中虽简单，但此前缺乏严格解释，从而使基于线性规划的马尔可夫决策问题在离线强化学习等应用中更加有效和实用。</div>
</details>
</div>
<div class="card">
<div class="title">GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning</div>
<div class="meta-line">Authors: Zhiheng Jiang, Yunzhe Wang, Ryan Marr, Ellen Novoseller, Benjamin T. Files, Volkan Ustun</div>
<div class="meta-line">First: 2026-01-28T16:36:37+00:00 · Latest: 2026-01-28T16:36:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20753v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20753v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphAllocBench：面向偏好条件多目标策略学习的灵活基准</div>
<div class="mono" style="margin-top:8px">多目标强化学习中的偏好条件策略学习旨在通过将策略与用户对目标的指定偏好条件相结合，近似获取多样化的帕累托最优解。这使得单一模型能够在运行时通过生成位于或接近帕累托前沿的策略，灵活适应任意权衡。然而，现有PCPL基准大多局限于简单任务和固定环境，限制了其实用性与可扩展性。为填补这一空白，我们提出了GraphAllocBench——一个基于新型图结构资源分配沙盒环境构建的灵活基准，该环境受城市管理启发而命名为CityPlannerEnv。GraphAllocBench提供涵盖多样化目标函数、可变偏好条件及高维可扩展性的丰富问题集。我们还提出了两个新评估指标——非支配解比例与排序分数，在补充广泛使用的超体积指标的同时，直接捕捉偏好一致性。通过多层感知机与图感知模型的实验，我们证明GraphAllocBench能够揭示现有MORL方法的局限性，并为在复杂高维组合分配任务中应用图神经网络等图学习方法开辟道路。除预设问题集外，GraphAllocBench允许用户灵活调整目标、偏好与分配规则，使其成为推动PCPL发展的通用可扩展基准。代码：https://anonymous.4open.science/r/GraphAllocBench</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GraphAllocBench, a flexible benchmark for Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL), motivated by the limitations of existing benchmarks that are often restricted to toy tasks and lack realism. The method centers on a novel graph-based resource allocation sandbox environment called CityPlannerEnv, which supports diverse objectives, varying preferences, and high-dimensional scalability, and proposes two new evaluation metrics—Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS)—to assess preference consistency alongside hypervolume. Experimental results with Multi-Layer Perceptrons and graph-aware models demonstrate that GraphAllocBench exposes shortcomings in current MORL approaches and highlights the potential of graph-based methods like Graph Neural Networks for complex combinatorial allocation tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了GraphAllocBench，这是一个用于多目标强化学习中偏好条件策略学习的灵活基准，其动机在于现有基准大多局限于玩具任务且缺乏现实性。该方法基于一个名为CityPlannerEnv的新型图资源分配沙盒环境，支持多样化的目标函数、变化的偏好条件和高维可扩展性，并引入了两个新评估指标——非支配解比例和排序分数——以结合超体积指标来评估偏好一致性。通过使用多层感知机和图感知模型进行实验，结果表明GraphAllocBench揭示了当前多目标强化学习方法的局限性，并为在复杂高维组合分配任务中应用图神经网络等图方法铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning</div>
<div class="meta-line">Authors: Silvia Sapora, Devon Hjelm, Alexander Toshev, Omar Attia, Bogdan Mazoure</div>
<div class="meta-line">First: 2025-10-02T16:31:39+00:00 · Latest: 2026-01-28T16:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02180v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.02180v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield black-box models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the MuJoCo, BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GRACE：面向可解释逆向强化学习的语言模型框架</div>
<div class="mono" style="margin-top:8px">逆向强化学习旨在从专家示范中恢复奖励模型，但传统方法产生的黑盒模型难以解释和调试。本文提出GRACE（以代码形式生成奖励），该方法在进化搜索中利用大语言模型，直接从专家轨迹逆向工程出可解释的、基于代码的奖励函数。所得奖励函数为可执行代码，可供检查与验证。我们在MuJoCo、BabyAI和AndroidWorld基准测试中实证验证了GRACE的有效性，该方法即使在复杂的多任务场景中也能高效学习高精度奖励。进一步实验表明，相较于采用真实奖励的竞争性模仿学习与在线强化学习方法，GRACE生成的奖励能产生更优策略。最后，我们证明GRACE能在多任务配置中构建复杂的奖励API。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GRACE is to address the interpretability issues in traditional Inverse Reinforcement Learning, which often produces black-box reward models that are hard to debug. The method involves using Large Language Models within an evolutionary search to reverse-engineer interpretable, code-based reward functions directly from expert trajectories, resulting in executable and verifiable code. Experimental results on MuJoCo, BabyAI, and AndroidWorld benchmarks show that GRACE efficiently learns accurate rewards in complex, multi-task settings, leading to strong policies that compete with both Imitation Learning and online RL approaches using ground-truth rewards, and it can build complex reward APIs in multi-task setups.</div>
<div class="mono" style="margin-top:8px">GRACE的动机是解决传统逆强化学习中奖励模型可解释性差、难以调试的黑盒问题。该方法利用大语言模型结合进化搜索，直接从专家轨迹中逆向生成可解释的、基于代码的奖励函数，产生可执行且可验证的代码。在MuJoCo、BabyAI和AndroidWorld基准测试中的实验结果表明，GRACE能在复杂的多任务设置中高效学习高精度奖励，生成的奖励函数能产生强大的策略，与使用真实奖励的模仿学习和在线强化学习方法相竞争，并能在多任务场景中构建复杂的奖励API。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions</div>
<div class="meta-line">Authors: Raul de la Rosa, Ivana Dusparic, Nicolas Cardozo</div>
<div class="meta-line">Venue: 2025 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C), Tokyo, Japan, 2025, pp. 148-153</div>
<div class="meta-line">First: 2026-01-28T15:46:51+00:00 · Latest: 2026-01-28T15:46:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent&#x27;s action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习智能体对动态动作空间与奖励函数的适应性行为调整</div>
<div class="mono" style="margin-top:8px">强化学习智能体在现实应用中常面临环境非平稳性的挑战，尤其在奖励函数变化或动作空间扩展时表现不佳。本文提出MORPHIN——一种自适应Q学习框架，支持无需完整重训练的实时适应。该框架通过集成概念漂移检测与动态调整学习/探索超参数，使智能体能同时适应奖励函数变化与动作空间的实时扩展，并保留先验策略知识以避免灾难性遗忘。我们在Gridworld基准测试和交通信号控制仿真中验证了该方法。结果表明，相较于标准Q学习基线，MORPHIN在收敛速度与持续适应能力方面表现更优，学习效率最高提升1.7倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of non-stationary environments in reinforcement learning, where agents must adapt to changes in reward functions and expansions of the action space without full retraining. The authors propose MORPHIN, a self-adaptive Q-learning framework that integrates concept drift detection to dynamically adjust learning and exploration hyperparameters, thereby preserving prior knowledge and preventing catastrophic forgetting. Experimental validation using a Gridworld benchmark and a traffic signal control simulation shows that MORPHIN achieves up to 1.7x improvement in learning efficiency and superior convergence speed compared to standard Q-learning, demonstrating effective continuous adaptation.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中环境非平稳性的挑战，即智能体需要在无需完全重新训练的情况下，适应奖励函数变化和动作空间扩展。作者提出了MORPHIN，一种自适应的Q学习框架，它通过集成概念漂移检测来动态调整学习和探索超参数，从而保留先验知识并防止灾难性遗忘。使用Gridworld基准和交通信号控制仿真进行的实验验证表明，与标准Q学习相比，MORPHIN实现了高达1.7倍的学习效率提升和更优的收敛速度，证明了其有效的持续适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models</div>
<div class="meta-line">Authors: Zhiqiang Kou, Junyang Chen, Xin-Qiang Cai, Xiaobo Xia, Ming-Kun Xie, Dong-Dong Wu, Biao Liu, Yuheng Jia, Xin Geng, Masashi Sugiyama, Tat-Seng Chua</div>
<div class="meta-line">First: 2026-01-28T15:14:50+00:00 · Latest: 2026-01-28T15:14:50+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20687v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher&#x27;s preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向本地化小模型的正例-无标签强化学习蒸馏方法</div>
<div class="mono" style="margin-top:8px">受隐私、成本和延迟等因素制约，本地化部署小模型日益普遍。然而多数实际流程仅进行监督微调（SFT），未能进入强化学习（RL）对齐阶段。主要原因是RL对齐通常需要昂贵的人工偏好标注，或严重依赖需大规模API调用和持续工程维护的高质量奖励模型，二者均不适用于本地化场景。为填补这一空白，我们提出面向本地化小模型部署的正例-无标签（PU）强化学习蒸馏方法。该方法无需人工标注偏好或奖励模型，即可将教师模型的黑箱生成能力中蕴含的偏好优化特性蒸馏至可本地训练的学生模型。针对每个提示，我们单次查询教师模型获得锚定响应，在本地采样多个学生候选响应，通过锚定条件自排序生成配对或列表级偏好，从而支持通过直接偏好优化或组相对策略优化实现完全本地化训练循环。理论分析证明本方法诱导的偏好信号具有序一致性且集中于近最优候选，保障了偏好优化的稳定性。实验表明该方法在低成本设置下能持续实现强劲性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for on-premise deployment of small models under privacy, cost, and latency constraints, which often prevents the use of reinforcement learning (RL) alignment due to its reliance on expensive human annotations or large-scale reward models, this paper proposes a positive-unlabeled RL distillation method. The method distills preference-optimization capability from a black-box teacher model into a student by querying the teacher once per prompt for an anchor response, locally sampling multiple student candidates, and performing anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling fully local training via direct preference optimization or group relative policy optimization. Experimental results show that the method achieves consistently strong performance in low-cost settings, with theoretical analysis confirming the order-consistency and concentration of induced preferences on near-optimal candidates.</div>
<div class="mono" style="margin-top:8px">本文的动机是在隐私、成本和延迟约束下，小模型的本地部署日益普遍，但通常难以进行强化学习对齐，因其依赖昂贵的人工标注或大规模奖励模型。为此，提出了一种正未标记强化学习蒸馏方法，该方法从黑盒教师模型中蒸馏偏好优化能力到学生模型，具体通过每个提示查询教师一次获得锚定响应，本地采样多个学生候选，并进行锚定条件自排名以生成成对或列表偏好，从而实现完全本地的直接偏好优化或组相对策略优化训练。实验结果表明，该方法在低成本设置下取得了持续强劲的性能，理论分析证实了诱导偏好的顺序一致性和对近优候选的集中性。</div>
</details>
</div>
<div class="card">
<div class="title">An interpretable data-driven approach to optimizing clinical fall risk assessment</div>
<div class="meta-line">Authors: Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Holley Farley, Kimia Ghobadi</div>
<div class="meta-line">First: 2026-01-08T18:17:31+00:00 · Latest: 2026-01-28T15:05:28+00:00</div>
<div class="meta-line">Comments: This work was intended as a replacement of arXiv:2510.20714 and any subsequent updates will appear there</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05194v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05194v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study&#x27;s risk labels, and without changing the tool&#x27;s form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种可解释的数据驱动方法优化临床跌倒风险评估</div>
<div class="mono" style="margin-top:8px">本研究旨在通过数据驱动建模方法，使约翰霍普金斯跌倒风险评估工具（JHFRAT）的跌倒风险预测与更具临床意义的指标更好契合。我们对2022年3月至2023年10月期间约翰霍普金斯医疗系统三家医院的54,209例住院病例进行了回顾性队列分析，其中20,208例被纳入高风险跌倒病例，13,941例为低风险病例。为融入临床知识并保持可解释性，我们采用约束评分优化（CSO）模型重新调整JHFRAT评分权重，同时保留其累加结构和临床阈值。重新校准指调整项目权重，使生成的评分能更一致地按研究风险标签排序病例，且不改变工具形式或部署流程。模型预测性能较当前JHFRAT显著提升（CSO AUC-ROC=0.91，JHFRAT AUC-ROC=0.86），相当于每周在约翰霍普金斯医疗系统多保护35名高风险患者。约束评分优化模型在包含与不包含电子健康记录变量时表现相似。尽管基准黑盒模型（XGBoost）在性能指标上优于基于知识的约束逻辑回归（AUC-ROC=0.94），但CSO对风险标签变化表现出更强稳健性。这种循证方法为医疗系统通过数据驱动优化技术系统性提升住院患者跌倒预防方案和患者安全提供了坚实基础，有助于改善医疗环境中的风险评估与资源分配。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to better align fall risk prediction with clinically meaningful measures, this study employs a data-driven constrained score optimization (CSO) approach to recalibrate the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) while preserving its interpretable additive structure and clinical thresholds. The method involved a retrospective analysis of over 54,000 inpatient admissions, reweighting the JHFRAT scoring to improve ordering consistency with risk labels. The main experimental results show the CSO model significantly improved predictive performance (AUC-ROC=0.91 vs. 0.86 for the original JHFRAT), which translates to protecting an estimated 35 additional high-risk patients per week across the health system, and demonstrated robustness compared to a higher-performing but less interpretable black-box model.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过数据驱动的方法，使约翰霍普金斯跌倒风险评估工具（JHFRAT）的预测结果与更具临床意义的指标更好地对齐。方法上，采用了一种保持可解释性的约束评分优化（CSO）模型，在保留工具原有可加性结构和临床阈值的前提下，重新调整其评分权重，并对超过5.4万例住院患者进行了回顾性队列分析。主要实验结果表明，优化后的模型预测性能显著提升（AUC-ROC=0.91，原JHFRAT为0.86），预计每周可在全医疗系统内多保护35名高风险患者，且相比性能更优但不可解释的“黑箱”模型，CSO模型对风险标签的变化表现出更强的稳健性。</div>
</details>
</div>
<div class="card">
<div class="title">DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration</div>
<div class="meta-line">Authors: Gilles Eerlings, Brent Zoomers, Jori Liesenborgs, Gustavo Rovelo Ruiz, Kris Luyten</div>
<div class="meta-line">First: 2026-01-28T14:02:28+00:00 · Latest: 2026-01-28T14:02:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20627v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20627v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model&#x27;s accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DIVERSE：诱导分歧的向量演化用于拉什蒙集合探索</div>
<div class="mono" style="margin-top:8px">我们提出DIVERSE框架，用于系统探索深度神经网络的拉什蒙集合——即那些在保持参考模型精度的同时具有不同预测行为的模型集合。DIVERSE通过特征线性调制层增强预训练模型，并采用协方差矩阵自适应进化策略在潜在调制空间中进行搜索，无需重新训练或梯度访问即可生成多样化模型变体。在MNIST、PneumoniaMNIST和CIFAR-10数据集上的实验表明，DIVERSE能发现多个高性能但功能各异的模型。我们的实验证明，DIVERSE能以较低计算成本实现与重新训练相当的多样性，为构建兼具鲁棒性、高性能与均衡模型多样性的集合提供了高效探索方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces DIVERSE, a framework motivated by the need to efficiently explore the Rashomon set—the collection of models that achieve similar accuracy to a reference model but exhibit different predictive behaviors. The method augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and employs Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without requiring retraining or gradient access. Experimental results on MNIST, PneumoniaMNIST, and CIFAR-10 demonstrate that DIVERSE successfully uncovers multiple high-performing yet functionally distinct models, offering a computationally efficient exploration of the Rashomon set that maintains robustness and performance while supporting model multiplicity.</div>
<div class="mono" style="margin-top:8px">本文提出了DIVERSE框架，其动机在于需要高效探索Rashomon集合——即那些与参考模型精度相当但预测行为不同的模型集合。该方法通过向预训练模型添加特征线性调制（FiLM）层，并利用协方差矩阵自适应进化策略（CMA-ES）在潜在调制空间中进行搜索，从而无需重新训练或梯度访问即可生成多样化的模型变体。在MNIST、PneumoniaMNIST和CIFAR-10数据集上的实验结果表明，DIVERSE能够成功发现多个高性能且功能各异的模型，以较低计算成本实现了对Rashomon集合的高效探索，在保持鲁棒性和性能的同时支持了均衡的模型多样性。</div>
</details>
</div>
<div class="card">
<div class="title">Predictor-Free and Hardware-Aware Federated Neural Architecture Search via Pareto-Guided Supernet Training</div>
<div class="meta-line">Authors: Bostan Khan, Masoud Daneshtalab</div>
<div class="meta-line">First: 2026-01-21T16:03:25+00:00 · Latest: 2026-01-28T13:58:23+00:00</div>
<div class="meta-line">Comments: This paper significantly extends the preliminary work accepted at ESANN 2026. Source Code: https://github.com/bostankhan6/DeepFedNAS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15127v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15127v2">PDF</a> · <a href="https://github.com/bostankhan6/DeepFedNAS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于帕累托引导超网训练的无预测器与硬件感知联邦神经架构搜索</div>
<div class="mono" style="margin-top:8px">联邦神经架构搜索旨在为隐私保护的联邦学习自动化模型设计，但当前面临两大瓶颈：无引导的超网训练导致模型次优，以及训练后子网发现流程耗时数小时。本文提出DeepFedNAS，一种基于多目标适应度函数的两阶段框架，融合数学网络设计与架构启发式方法。通过重构超网，DeepFedNAS引入联邦帕累托最优超网训练，利用预计算的高适应度架构帕累托最优缓存作为智能课程优化共享超网权重。随后，其无预测器搜索方法通过将适应度函数作为零成本精度代理，无需昂贵精度替代模型，实现秒级按需子网发现。DeepFedNAS在CIFAR-100上取得最高1.21%的绝对精度提升，具备更优的参数与通信效率，训练后搜索流程整体加速约61倍。通过将流程从超过20小时缩短至约20分钟（含初始缓存生成），并实现20秒级单子网搜索，DeepFedNAS使硬件感知联邦学习部署即时可行。完整源代码与实验脚本已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses inefficiencies in Federated Neural Architecture Search (FedNAS), specifically the unguided supernet training and slow post-training search. It proposes DeepFedNAS, a two-phase framework that first uses a Pareto-optimal cache of high-fitness architectures to guide supernet training via a multi-objective fitness function, and then employs a predictor-free search method using that function as a direct accuracy proxy for rapid subnet discovery. Experiments show state-of-the-art accuracy improvements (e.g., up to 1.21% on CIFAR-100), better efficiency, and a ~61x speedup in the search pipeline, reducing total time from over 20 hours to about 20 minutes and enabling instant subnet searches.</div>
<div class="mono" style="margin-top:8px">该论文针对联邦神经架构搜索中存在的超网训练缺乏引导和训练后搜索缓慢的问题，提出了DeepFedNAS这一两阶段框架。该方法首先利用预计算的帕累托最优架构缓存，通过多目标适应度函数指导超网训练；随后采用无预测器搜索方法，将该函数直接作为精度代理进行快速子网发现。实验结果表明，该方法实现了最先进的精度提升（如在CIFAR-100上最高提升1.21%）、更高的效率，并将搜索流程加速约61倍，总时间从超过20小时缩短至约20分钟，支持即时子网搜索。</div>
</details>
</div>
<div class="card">
<div class="title">Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation</div>
<div class="meta-line">Authors: Yanqi Dai, Yuxiang Ji, Xiao Zhang, Yong Wang, Xiangxiang Chu, Zhiwu Lu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T13:49:23+00:00 · Latest: 2026-01-28T13:49:23+00:00</div>
<div class="meta-line">Comments: Accepted for ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20614v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20614v1">PDF</a> · <a href="https://github.com/AMAP-ML/MathForge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>难度即优势：通过难度感知GRPO与多维度问题重构提升数学推理能力</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）为增强大模型的数学推理能力提供了稳健机制。然而，现有方法在算法和数据层面均存在对高难度问题系统性重视不足的问题，尽管这类问题对提升未充分发展的能力至关重要。算法层面，广泛使用的组相对策略优化（GRPO）存在隐含的不平衡性：对更难问题的策略更新幅度较低。数据层面，现有增强方法主要通过改写问题提升多样性，未能系统性地增加问题内在难度。为解决这些问题，我们提出双轨并行的MathForge框架，从算法和数据双重视角聚焦高难度问题，包含难度感知组策略优化（DGPO）算法与多维度问题重构（MQR）策略。具体而言，DGPO首先通过难度均衡的组优势估计修正GRPO的隐含不平衡，并进一步通过难度感知的问题级加权优先处理高难度问题；MQR则通过多维度重构问题在保持原答案的同时提升难度。整体上，MathForge形成协同循环：MQR拓展数据边界，DGPO有效学习增强数据。大量实验表明，MathForge在多项数学推理任务上显著优于现有方法。代码与增强数据已开源：https://github.com/AMAP-ML/MathForge。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the systematic neglect of challenging questions in existing reinforcement learning methods for mathematical reasoning, which limits the refinement of underdeveloped model capabilities. The authors propose MathForge, a two-part framework comprising a Difficulty-Aware Group Policy Optimization (DGPO) algorithm that corrects implicit update imbalances in GRPO by prioritizing harder questions through balanced advantage estimation and weighting, and a Multi-Aspect Question Reformulation (MQR) strategy that augments data by reformulating questions to increase intrinsic difficulty while preserving correct answers. Experimental results demonstrate that MathForge significantly outperforms prior methods across various mathematical reasoning tasks, with code and data made publicly available.</div>
<div class="mono" style="margin-top:8px">本文针对现有数学推理强化学习方法中系统性忽视难题的问题，指出这限制了模型未充分发展能力的提升。作者提出了MathForge框架，包含两部分：难度感知组策略优化（DGPO）算法，通过平衡优势估计和加权优先处理难题，纠正了GRPO中的隐式更新不平衡；以及多角度问题重构（MQR）策略，通过多角度重构问题来增加数据内在难度，同时保持原正确答案。实验结果表明，MathForge在多种数学推理任务上显著优于现有方法，相关代码和增强数据已公开。</div>
</details>
</div>
<div class="card">
<div class="title">Continuous Evolution Pool: Taming Recurring Concept Drift in Online Time Series Forecasting</div>
<div class="meta-line">Authors: Tianxiang Zhan, Ming Jin, Yuanpeng He, Yuxuan Liang, Yong Deng, Shirui Pan</div>
<div class="meta-line">First: 2025-05-28T03:27:49+00:00 · Latest: 2026-01-28T13:48:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.14790v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.14790v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recurring concept drift poses a dual challenge in online time series forecasting: mitigating catastrophic forgetting while adhering to strict privacy constraints that prevent retaining historical data. Existing approaches predominantly rely on parameter updates or experience replay, which inevitably suffer from knowledge overwriting or privacy risks. To address this, we propose the Continuous Evolution Pool (CEP), a privacy-preserving framework that maintains a dynamic pool of specialized forecasters. Instead of storing raw samples, CEP utilizes lightweight statistical genes to decouple concept identification from forecasting. Specifically, it employs a Retrieval mechanism to identify the nearest concept based on gene similarity, an Evolution strategy to spawn new forecasters upon detecting distribution shifts, and an Elimination policy to prune obsolete models under memory constraints. Experiments on real-world datasets demonstrate that CEP significantly outperforms state-of-the-art baselines, reducing forecasting error by over 20% without accessing historical ground truth.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>持续进化池：在线时间序列预测中驯服周期性概念漂移</div>
<div class="mono" style="margin-top:8px">周期性概念漂移为在线时间序列预测带来双重挑战：既要缓解灾难性遗忘，又需遵循禁止保留历史数据的严格隐私约束。现有方法主要依赖参数更新或经验回放，不可避免地面临知识覆盖或隐私风险。为此，我们提出持续进化池（CEP）——一种维护专用预测器动态池的隐私保护框架。CEP不存储原始样本，而是利用轻量级统计基因将概念识别与预测解耦。具体通过检索机制依据基因相似度识别最近邻概念，通过进化策略在检测到分布漂移时生成新预测器，并通过淘汰策略在内存限制下修剪过时模型。在真实数据集上的实验表明，CEP显著优于现有基线方法，在不访问历史真实值的情况下将预测误差降低超过20%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of recurring concept drift in online time-series forecasting, where models must adapt to shifting data distributions without retaining historical data due to privacy constraints. The authors propose the Continuous Evolution Pool (CEP), a privacy-preserving framework that maintains a dynamic pool of specialized forecasters using lightweight statistical genes to decouple concept identification from forecasting, incorporating retrieval, evolution, and elimination mechanisms. Experimental results on real-world datasets show that CEP reduces forecasting error by over 20% compared to state-of-the-art baselines, effectively mitigating catastrophic forgetting while adhering to strict privacy requirements.</div>
<div class="mono" style="margin-top:8px">本文针对在线时间序列预测中反复出现的概念漂移问题，旨在模型需适应数据分布变化的同时，因隐私约束无法保留历史数据。作者提出了连续进化池（CEP），这是一个隐私保护框架，通过轻量级统计基因解耦概念识别与预测，维护一个动态的专业预测器池，并采用检索、进化和淘汰机制。在真实数据集上的实验表明，CEP相比现有先进基线显著降低了超过20%的预测误差，有效缓解了灾难性遗忘，且严格遵守了隐私要求。</div>
</details>
</div>
<div class="card">
<div class="title">Deep SPI: Safe Policy Improvement via World Models</div>
<div class="meta-line">Authors: Florent Delgrange, Raphael Avalos, Willem Röpke</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-14T09:11:24+00:00 · Latest: 2026-01-28T13:27:55+00:00</div>
<div class="meta-line">Comments: ICLR 2026, 10 pages main text, 21 pages appendix (excluding references)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.12312v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.12312v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe policy improvement (SPI) offers theoretical control over policy updates, yet existing guarantees largely concern offline, tabular reinforcement learning (RL). We study SPI in general online settings, when combined with world model and representation learning. We develop a theoretical framework showing that restricting policy updates to a well-defined neighborhood of the current policy ensures monotonic improvement and convergence. This analysis links transition and reward prediction losses to representation quality, yielding online, &quot;deep&quot; analogues of classical SPI theorems from the offline RL literature. Building on these results, we introduce DeepSPI, a principled on-policy algorithm that couples local transition and reward losses with regularised policy updates. On the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including PPO and DeepMDPs, while retaining theoretical guarantees.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Deep SPI：基于世界模型的安全策略改进</div>
<div class="mono" style="margin-top:8px">安全策略改进（SPI）为策略更新提供了理论控制，但现有保证主要针对离线表格强化学习（RL）。本研究将SPI与世界模型和表征学习结合，探索其在通用在线场景中的应用。我们构建的理论框架表明，将策略更新限制在当前策略的明确定义邻域内，可确保单调改进与收敛。该分析将状态转移与奖励预测损失关联至表征质量，从而在在线场景中构建了离线RL文献经典SPI定理的“深度”类比。基于此，我们提出DeepSPI算法——一种耦合局部转移/奖励损失与正则化策略更新的原则性同策略算法。在ALE-57基准测试中，DeepSPI达到或超越了PPO、DeepMDPs等强基线，同时保持理论保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of ensuring safe policy improvement (SPI) in online reinforcement learning beyond traditional offline, tabular settings, motivated by the need for theoretical guarantees in deep RL with world models. The method introduces a theoretical framework that restricts policy updates to a neighborhood of the current policy to ensure monotonic improvement and convergence, linking transition and reward prediction losses to representation quality, and proposes DeepSPI, an on-policy algorithm combining local losses with regularized updates. Experimental results on the ALE-57 benchmark show that DeepSPI matches or exceeds strong baselines like PPO and DeepMDPs while maintaining theoretical safety guarantees.</div>
<div class="mono" style="margin-top:8px">本文针对在线强化学习中安全策略改进（SPI）的挑战，旨在超越传统离线表格设置，为基于世界模型的深度RL提供理论保证。方法上提出了一个理论框架，通过将策略更新限制在当前策略的邻域内来确保单调改进和收敛，将转移和奖励预测损失与表示质量相关联，并提出了DeepSPI算法，该算法结合局部损失与正则化更新进行在线策略优化。在ALE-57基准测试中，DeepSPI匹配或超越了PPO和DeepMDPs等强基线，同时保持了理论上的安全性保证。</div>
</details>
</div>
<div class="card">
<div class="title">Ranking-aware Reinforcement Learning for Ordinal Ranking</div>
<div class="meta-line">Authors: Aiming Hao, Chen Zhu, Jiashu Zhu, Jiahong Wu, Xiangxiang Chu</div>
<div class="meta-line">First: 2026-01-28T13:22:42+00:00 · Latest: 2026-01-28T13:22:42+00:00</div>
<div class="meta-line">Comments: Accepted to ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20585v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20585v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向序数排序的排序感知强化学习</div>
<div class="mono" style="margin-top:8px">序数回归与排序因固有的序数依赖关系而具有挑战性，传统方法难以有效建模。我们提出排序感知强化学习（RARL），这是一种显式学习此类关系的新型强化学习框架。RARL的核心在于采用统一目标函数，协同整合回归与排序学习（L2R）任务，实现两者相互促进。该框架通过排序感知可验证奖励机制驱动，联合评估回归精度与排序准确性，并借助策略优化实现直接模型更新。为进一步提升训练效果，我们引入响应突变操作（RMO），通过注入受控噪声增强探索能力，避免模型陷入鞍点停滞。在三个独立基准测试上的广泛实验验证了RARL的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of modeling ordinal dependencies in ranking tasks, where conventional methods often fall short. It introduces Ranking-aware Reinforcement Learning (RARL), a novel framework that integrates regression and Learning-to-Rank through a unified objective, using a ranking-aware reward to jointly evaluate precision and accuracy for direct policy optimization. To improve training, Response Mutation Operations inject controlled noise to enhance exploration and avoid stagnation. Experimental results on three benchmarks demonstrate the effectiveness of the proposed approach.</div>
<div class="mono" style="margin-top:8px">本文针对排序任务中传统方法难以建模的序数依赖性问题，提出了一种新颖的排序感知强化学习框架。该方法通过统一目标协同整合回归和排序学习，利用排序感知奖励联合评估回归精度和排序准确性，以驱动策略优化直接更新模型。为进一步提升训练效果，引入了响应突变操作，通过注入受控噪声来增强探索并防止陷入鞍点。在三个不同基准上的广泛实验验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Inequality in Congestion Games with Learning Agents</div>
<div class="meta-line">Authors: Dimitris Michailidis, Sennay Ghebreab, Fernando P. Santos</div>
<div class="meta-line">First: 2026-01-28T13:16:25+00:00 · Latest: 2026-01-28T13:16:25+00:00</div>
<div class="meta-line">Comments: Full version of the extended abstract version appearing in Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20578v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Who benefits from expanding transport networks? While designed to improve mobility, such interventions can also create inequality. In this paper, we show that disparities arise not only from the structure of the network itself but also from differences in how commuters adapt to it. We model commuters as reinforcement learning agents who adapt their travel choices at different learning rates, reflecting unequal access to resources and information. To capture potential efficiency-fairness tradeoffs, we introduce the Price of Learning (PoL), a measure of inefficiency during learning. We analyze both a stylized network -- inspired in the well-known Braess&#x27;s paradox, yet with two-source nodes -- and an abstraction of a real-world metro system (Amsterdam). Our simulations show that network expansions can simultaneously increase efficiency and amplify inequality, especially when faster learners disproportionately benefit from new routes before others adapt. These results highlight that transport policies must account not only for equilibrium outcomes but also for the heterogeneous ways commuters adapt, since both shape the balance between efficiency and fairness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习型智能体拥堵博弈中的不平等问题</div>
<div class="mono" style="margin-top:8px">交通网络扩张使谁受益？此类干预措施旨在提升流动性，却也可能加剧不平等。本文揭示，差异不仅源于网络结构本身，更源于通勤者适应方式的差异。我们将通勤者建模为强化学习智能体，其以不同学习速率调整出行选择，反映资源与信息获取的不平等。为刻画效率与公平的潜在权衡，我们引入“学习代价”指标，用以衡量学习过程中的效率损失。通过分析一个受经典布雷斯悖论启发但具有双源节点的简化网络，以及阿姆斯特丹真实地铁系统的抽象模型，模拟结果表明：网络扩张可能同时提升效率并放大不平等，尤其当快速学习者在新路线开放后、他人尚未适应时获得不成比例的优势。这些发现强调，交通政策不仅需考虑均衡结果，还必须纳入通勤者适应行为的异质性，二者共同塑造效率与公平的平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how network expansions can exacerbate inequality among commuters, motivated by the observation that such interventions may not benefit all travelers equally. The authors model commuters as reinforcement learning agents with heterogeneous learning rates, reflecting disparities in access to information and resources, and introduce the Price of Learning (PoL) to quantify inefficiency during adaptation. Experimental results on both a stylized two-source Braess network and an abstraction of Amsterdam&#x27;s metro system reveal that expansions can simultaneously improve overall efficiency while amplifying inequality, as faster learners capture benefits from new routes before others adapt, underscoring the need for policies that consider heterogeneous adaptation dynamics alongside equilibrium outcomes.</div>
<div class="mono" style="margin-top:8px">本文研究了交通网络扩建如何加剧通勤者间的不平等，其动机在于观察到此类干预措施可能无法平等惠及所有出行者。作者将通勤者建模为具有不同学习速率的强化学习智能体，以反映信息与资源获取的差异，并引入学习代价（PoL）来量化适应过程中的效率损失。在基于双源Braess悖论的简化网络和阿姆斯特丹地铁系统抽象模型上的实验结果表明，网络扩建在提升整体效率的同时可能放大不平等，因为学习更快的通勤者会优先从新路线中获益，这强调了政策制定需同时考虑异质性适应动态与均衡结果。</div>
</details>
</div>
<div class="card">
<div class="title">Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution Simulations for Time-Efficient Fine-Resolution Policy Learning</div>
<div class="meta-line">Authors: Yuki Kadokawa, Hirotaka Tahara, Takamitsu Matsubara</div>
<div class="meta-line">Venue: IEEE Transactions on Automation Science and Engineering 2025</div>
<div class="meta-line">First: 2024-12-10T12:50:25+00:00 · Latest: 2026-01-28T13:14:30+00:00</div>
<div class="meta-line">Comments: accepted for IEEE Transactions on Automation Science and Engineering (T-ASE)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.07477v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.07477v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yuki-kadokawa.github.io/prpd/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In earthwork and construction, excavators often encounter large rocks mixed with various soil conditions, requiring skilled operators. This paper presents a framework for achieving autonomous excavation using reinforcement learning (RL) through a rock excavation simulator. In the simulation, resolution can be defined by the particle size/number in the whole soil space. Fine-resolution simulations closely mimic real-world behavior but demand significant calculation time and challenging sample collection, while coarse-resolution simulations enable faster sample collection but deviate from real-world behavior. To combine the advantages of both resolutions, we explore using policies developed in coarse-resolution simulations for pre-training in fine-resolution simulations. To this end, we propose a novel policy learning framework called Progressive-Resolution Policy Distillation (PRPD), which progressively transfers policies through some middle-resolution simulations with conservative policy transfer to avoid domain gaps that could lead to policy transfer failure. Validation in a rock excavation simulator and nine real-world rock environments demonstrated that PRPD reduced sampling time to less than 1/7 while maintaining task success rates comparable to those achieved through policy learning in a fine-resolution simulation. Additional videos and supplementary results are available on our project page: https://yuki-kadokawa.github.io/prpd/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>渐进分辨率策略蒸馏：利用粗分辨率仿真实现高效细分辨率策略学习</div>
<div class="mono" style="margin-top:8px">在土方工程与建筑施工中，挖掘机常需处理混杂不同土壤条件的大型岩石，这对操作人员技能要求较高。本文提出一种基于强化学习的自主挖掘框架，通过岩石挖掘仿真器实现。仿真中，分辨率可由整个土壤空间中颗粒尺寸/数量定义：细分辨率仿真能精确模拟真实场景，但计算耗时且样本采集困难；粗分辨率仿真虽能快速采集样本，却与真实行为存在偏差。为融合两种分辨率的优势，本研究探索将粗分辨率仿真中训练的策略用于细分辨率仿真的预训练。为此，我们提出名为“渐进分辨率策略蒸馏”的新型策略学习框架，通过中间分辨率仿真逐步迁移策略，并采用保守策略迁移机制以避免因领域差异导致的策略迁移失败。在岩石挖掘仿真器及九种真实岩石环境中的验证表明，该框架在保持与细分辨率仿真相当任务成功率的同时，将采样时间缩短至原时间的1/7以下。更多视频与补充结果详见项目页面：https://yuki-kadokawa.github.io/prpd/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for efficient autonomous excavation using reinforcement learning, where fine-resolution simulations are accurate but computationally expensive, while coarse-resolution ones are fast but less realistic. The method introduces Progressive-Resolution Policy Distillation (PRPD), a framework that progressively transfers policies from coarse to fine resolutions via intermediate steps with conservative transfer to mitigate domain gaps. Experimental results in a rock excavation simulator and real-world environments show that PRPD reduces sampling time to less than one-seventh while maintaining task success rates comparable to fine-resolution-only learning.</div>
<div class="mono" style="margin-top:8px">本文旨在通过强化学习实现高效自主挖掘，其中高分辨率仿真准确但计算成本高，而低分辨率仿真快速但不真实。方法提出了渐进分辨率策略蒸馏（PRPD），该框架通过中间分辨率步骤以保守策略转移方式，逐步将策略从低分辨率迁移到高分辨率，以减少领域差距。在岩石挖掘仿真器和九个真实环境中的验证结果表明，PRPD将采样时间减少至不到七分之一，同时保持了与高分辨率仿真学习相当的任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Smart Exploration in Reinforcement Learning using Bounded Uncertainty Models</div>
<div class="meta-line">Authors: J. S. van Hulst, W. P. M. H. Heemels, D. J. Antunes</div>
<div class="meta-line">Venue: 2025 IEEE 64th Conference on Decision and Control (CDC)</div>
<div class="meta-line">First: 2025-04-08T12:33:38+00:00 · Latest: 2026-01-28T13:09:54+00:00</div>
<div class="meta-line">Comments: Presented at 64th IEEE Conference on Decision and Control, CDC 2025, Rio de Janeiro, Brazil, 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.05978v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.05978v3">PDF</a> · <a href="https://github.com/JvHulst/BUMEX">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a powerful framework for decision-making in uncertain environments, but it often requires large amounts of data to learn an optimal policy. We address this challenge by incorporating prior model knowledge to guide exploration and accelerate the learning process. Specifically, we assume access to a model set that contains the true transition kernel and reward function. We optimize over this model set to obtain upper and lower bounds on the Q-function, which are then used to guide the exploration of the agent. We provide theoretical guarantees on the convergence of the Q-function to the optimal Q-function under the proposed class of exploring policies. Furthermore, we also introduce a data-driven regularized version of the model set optimization problem that ensures the convergence of the class of exploring policies to the optimal policy. Lastly, we show that when the model set has a specific structure, namely the bounded-parameter MDP (BMDP) framework, the regularized model set optimization problem becomes convex and simple to implement. In this setting, we also prove finite-time convergence to the optimal policy under mild assumptions. We demonstrate the effectiveness of the proposed exploration strategy, which we call BUMEX (Bounded Uncertainty Model-based Exploration), in a simulation study. The results indicate that the proposed method can significantly accelerate learning in benchmark examples. A toolbox is available at https://github.com/JvHulst/BUMEX.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于有界不确定性模型的强化学习智能探索方法</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是在不确定环境中进行决策的强大框架，但其通常需要大量数据来学习最优策略。我们通过引入先验模型知识来引导探索并加速学习过程，以应对这一挑战。具体而言，我们假设存在一个包含真实转移核与奖励函数的模型集合，通过在该集合上优化得到Q函数的上界与下界，并以此指导智能体的探索。我们为所提出的探索策略类别提供了Q函数收敛至最优Q函数的理论保证，同时引入一种数据驱动的正则化模型集优化问题，确保探索策略类收敛至最优策略。此外，当模型集具有特定结构（即有界参数马尔可夫决策过程框架）时，正则化模型集优化问题转化为凸优化问题且易于实现；在此设定下，我们进一步证明了在温和假设下策略可在有限时间内收敛至最优策略。通过仿真研究验证了所提出的探索策略（称为BUMEX——基于有界不确定性模型的探索）的有效性，结果表明该方法在基准测试中能显著加速学习。相关工具箱发布于https://github.com/JvHulst/BUMEX。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the data inefficiency of reinforcement learning by incorporating prior model knowledge to guide exploration, proposing a method called BUMEX that optimizes over a model set containing the true environment dynamics to compute Q-function bounds for exploration. The approach provides theoretical convergence guarantees for the Q-function and introduces a regularized, data-driven version that ensures policy optimality, with a convex implementation under the bounded-parameter MDP framework and finite-time convergence proofs. Experimental simulations on benchmarks demonstrate that BUMEX significantly accelerates learning compared to standard methods.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习数据效率低的问题，提出利用先验模型知识引导探索，方法名为BUMEX，通过优化包含真实环境动态的模型集来计算Q函数边界以指导探索。该方法提供了Q函数收敛的理论保证，并引入了正则化的数据驱动版本以确保策略最优性，在有界参数MDP框架下可凸化实现并证明了有限时间收敛性。在基准测试的仿真实验中，BUMEX相比标准方法显著加快了学习速度。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Multi-Agent DRL Based Dynamic Cluster Reconfiguration for UAV Mobility Management</div>
<div class="meta-line">Authors: Irshad A. Meer, Karl-Ludwig Besser, Mustafa Ozger, Dominic Schupke, H. Vincent Poor, Cicek Cavdar</div>
<div class="meta-line">Venue: IEEE Transactions on Cognitive Communications and Networking, vol. 12, pp. 4957-4971, 2026</div>
<div class="meta-line">First: 2024-12-05T19:20:42+00:00 · Latest: 2026-01-28T12:03:52+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.16167v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.16167v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-connectivity involves dynamic cluster formation among distributed access points (APs) and coordinated resource allocation from these APs, highlighting the need for efficient mobility management strategies for users with multi-connectivity. In this paper, we propose a novel mobility management scheme for unmanned aerial vehicles (UAVs) that uses dynamic cluster reconfiguration with energy-efficient power allocation in a wireless interference network. Our objective encompasses meeting stringent reliability demands, minimizing joint power consumption, and reducing the frequency of cluster reconfiguration. To achieve these objectives, we propose a hierarchical multi-agent deep reinforcement learning (H-MADRL) framework, specifically tailored for dynamic clustering and power allocation. The edge cloud connected with a set of APs through low latency optical back-haul links hosts the high-level agent responsible for the optimal clustering policy, while low-level agents reside in the APs and are responsible for the power allocation policy. To further improve the learning efficiency, we propose a novel action-observation transition-driven learning algorithm that allows the low-level agents to use the action space from the high-level agent as part of the local observation space. This allows the lower-level agents to share partial information about the clustering policy and allocate the power more efficiently. The simulation results demonstrate that our proposed distributed algorithm achieves comparable performance to the centralized algorithm. Additionally, it offers better scalability, as the decision time for clustering and power allocation increases by only 10% when doubling the number of APs, compared to a 90% increase observed with the centralized approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于分层多智能体深度强化学习的无人机移动性管理动态集群重构</div>
<div class="mono" style="margin-top:8px">多连接技术涉及分布式接入点间的动态集群形成及协调资源分配，突显了对多连接用户高效移动性管理策略的需求。本文针对无线干扰网络中的无人机，提出一种结合节能功率分配的动态集群重构移动性管理方案。目标包括满足严格可靠性要求、最小化联合功耗并降低集群重构频率。为实现这些目标，我们提出专为动态聚类与功率分配设计的分层多智能体深度强化学习框架：通过低延迟光回程链路连接接入点集的边缘云部署负责最优聚类策略的高层智能体，而低层智能体驻留在接入点中负责功率分配策略。为提升学习效率，我们提出一种新颖的动作-观测转换驱动学习算法，使低层智能体能将高层智能体的动作空间作为局部观测空间的一部分，从而共享聚类策略的部分信息以实现更高效的功率分配。仿真结果表明，所提分布式算法性能与集中式算法相当，且扩展性更优——当接入点数量翻倍时，聚类与功率分配的决策时间仅增加10%，而集中式方法则增加90%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of efficient mobility management for unmanned aerial vehicles (UAVs) in wireless networks requiring multi-connectivity, aiming to meet high reliability, minimize power consumption, and reduce cluster reconfiguration frequency. The authors propose a hierarchical multi-agent deep reinforcement learning (H-MADRL) framework where a high-level agent at the edge cloud handles dynamic cluster formation and low-level agents at access points manage power allocation, enhanced by a novel action-observation transition-driven algorithm that improves learning efficiency through partial information sharing. Experimental simulations show the distributed approach achieves performance comparable to a centralized baseline while offering superior scalability, with decision time increasing only 10% versus 90% when doubling the number of access points.</div>
<div class="mono" style="margin-top:8px">本文针对无人机在需要多连接的无线网络中的高效移动性管理挑战，旨在满足高可靠性、最小化功耗并减少集群重构频率。作者提出了一种分层多智能体深度强化学习框架，其中边缘云的高层智能体负责动态集群形成，接入点的低层智能体负责功率分配，并通过一种新颖的动作-观察转换驱动算法增强，该算法通过部分信息共享提高了学习效率。实验仿真表明，这种分布式方法的性能与集中式基准相当，同时具有更优的可扩展性，当接入点数量翻倍时，其决策时间仅增加10%，而集中式方法则增加90%。</div>
</details>
</div>
<div class="card">
<div class="title">Variational Quantum Circuit-Based Reinforcement Learning for Dynamic Portfolio Optimization</div>
<div class="meta-line">Authors: Vincent Gurgul, Ying Chen, Stefan Lessmann</div>
<div class="meta-line">First: 2026-01-20T15:17:24+00:00 · Latest: 2026-01-28T11:57:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18811v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18811v2">PDF</a> · <a href="https://github.com/VincentGurgul/qrl-dpo-public">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a Quantum Reinforcement Learning (QRL) solution to the dynamic portfolio optimization problem based on Variational Quantum Circuits. The implemented QRL approaches are quantum analogues of the classical neural-network-based Deep Deterministic Policy Gradient and Deep Q-Network algorithms. Through an empirical evaluation on real-world financial data, we show that our quantum agents achieve risk-adjusted performance comparable to, and in some cases exceeding, that of classical Deep RL models with several orders of magnitude more parameters. However, while quantum circuit execution is inherently fast at the hardware level, practical deployment on cloud-based quantum systems introduces substantial latency, making end-to-end runtime currently dominated by infrastructural overhead and limiting practical applicability. Taken together, our results suggest that QRL is theoretically competitive with state-of-the-art classical reinforcement learning and may become practically advantageous as deployment overheads diminish. This positions QRL as a promising paradigm for dynamic decision-making in complex, high-dimensional, and non-stationary environments such as financial markets. The complete codebase is released as open source at: https://github.com/VincentGurgul/qrl-dpo-public</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于变分量子电路的动态投资组合优化强化学习</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于变分量子电路的量子强化学习（QRL）解决方案，用于动态投资组合优化问题。所实现的QRL方法是经典基于神经网络的深度确定性策略梯度与深度Q网络算法的量子类比。通过对真实世界金融数据的实证评估，我们表明量子智能体在风险调整后性能上可与经典深度强化学习模型相媲美，在某些情况下甚至超越后者，而后者参数数量高出数个数量级。然而，尽管量子电路执行在硬件层面本质上是快速的，但在基于云的量子系统上的实际部署引入了显著的延迟，使得端到端运行时间目前主要由基础设施开销主导，限制了实际应用性。综合来看，我们的结果表明QRL在理论上与最先进的经典强化学习具有竞争力，并可能随着部署开销的减少而变得具有实际优势。这使QRL成为金融市场等复杂、高维和非平稳环境中动态决策的有前景范式。完整代码库已在https://github.com/VincentGurgul/qrl-dpo-public开源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient dynamic portfolio optimization in complex financial markets, this paper proposes a Quantum Reinforcement Learning (QRL) approach using Variational Quantum Circuits as quantum analogues of classical Deep RL algorithms like Deep Deterministic Policy Gradient and Deep Q-Network. The method involves training quantum agents on real-world financial data, where they demonstrate risk-adjusted performance comparable to or exceeding classical models despite having far fewer parameters. Experimental results reveal that while quantum circuit execution is inherently fast, practical deployment on cloud-based quantum systems suffers from significant latency due to infrastructural overhead, currently limiting real-world applicability. Overall, the study suggests QRL is theoretically competitive with state-of-the-art classical reinforcement learning and could become practically advantageous as deployment efficiencies improve.</div>
<div class="mono" style="margin-top:8px">本文针对复杂金融市场中动态投资组合优化的需求，提出了一种基于变分量子电路的量子强化学习方法，作为经典深度确定性策略梯度和深度Q网络等算法的量子类比。该方法通过在真实金融数据上训练量子智能体，实验结果表明，尽管参数数量远少于经典模型，量子智能体实现了与之相当甚至更优的风险调整后性能。然而，尽管量子电路执行在硬件层面本征快速，但在基于云的量子系统上的实际部署因基础设施开销而引入显著延迟，目前限制了其实际应用。总体而言，该研究认为量子强化学习在理论上与最先进的经典强化学习具有竞争力，并可能随着部署效率的提升而展现出实用优势。</div>
</details>
</div>
<div class="card">
<div class="title">Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations</div>
<div class="meta-line">Authors: Fatima Ezzeddine, Obaida Ammar, Silvia Giordano, Omran Ayoub</div>
<div class="meta-line">First: 2026-01-28T10:13:12+00:00 · Latest: 2026-01-28T10:13:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20449v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model&#x27;s decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>公平追索权保障：确保反事实解释中的个体与群体公平性</div>
<div class="mono" style="margin-top:8px">可解释人工智能（XAI）对提升机器学习模型透明度日益重要。在各类XAI技术中，反事实解释（CFs）因其能阐明输入特征变化如何改变模型决策、从而为用户提供可操作的追索方案而占据关键地位。确保具有相似属性的个体及不同受保护群体（如人口统计学群体）获得相近且可操作的追索选项，是实现可信赖公平决策的核心。本研究通过聚焦公平CFs的生成直接应对这一挑战：首先从三个维度定义并形式化公平性——1）个体公平性：确保相似个体获得相似CFs；2）群体公平性：确保不同受保护群体获得均衡CFs；3）混合公平性：兼顾个体与更广泛群体层面的公平。我们将该问题构建为优化任务，提出一种新颖的模型无关、基于强化学习的方法，能同时满足个体与群体层面的公平约束——这两个通常被视为正交的目标。在公平性度量方面，我们扩展了常用于审计机器学习模型的现有指标，如个体与群体间的平等追索选择权和平等有效性。通过在三个基准数据集上的评估，表明该方法在保持生成CFs邻近性与合理性质量的同时，能有效确保个体与群体公平性，并分别量化了不同层级公平性的代价。本研究开启了关于混合公平性及其对XAI乃至更广领域作用与影响的深入探讨。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for fairness in counterfactual explanations (CFs) within explainable AI, motivated by ensuring that individuals with similar attributes and those from different protected groups receive comparable and actionable recourse. The authors propose a model-agnostic reinforcement learning method to generate CFs that satisfy both individual fairness (similar individuals get similar CFs) and group fairness (equitable CFs across groups), formulated as an optimization task with extended fairness metrics like equal choice and effectiveness. Experimental results on three benchmark datasets demonstrate that the approach effectively maintains individual and group fairness while preserving CF quality in terms of proximity and plausibility, separately quantifying the fairness costs.</div>
<div class="mono" style="margin-top:8px">本文针对可解释人工智能中反事实解释的公平性需求，旨在确保具有相似属性的个体和不同受保护群体的用户获得可比且可操作的补救措施。作者提出了一种与模型无关的强化学习方法，通过优化任务生成满足个体公平性（相似个体获得相似反事实解释）和群体公平性（跨群体公平解释）的反事实解释，并扩展了公平性指标如平等选择和有效性。在三个基准数据集上的实验结果表明，该方法在保持反事实解释的接近性和合理性质量的同时，有效确保了个体和群体公平性，并分别量化了不同层面的公平性成本。</div>
</details>
</div>
<div class="card">
<div class="title">Robust MAE-Driven NAS: From Mask Reconstruction to Architecture Innovation</div>
<div class="meta-line">Authors: Yiming Hu, Xiangxiang Chu, Yong Wang</div>
<div class="meta-line">First: 2023-11-20T13:45:21+00:00 · Latest: 2026-01-28T10:09:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2311.12086v3">Abs</a> · <a href="https://arxiv.org/pdf/2311.12086v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Architecture Search (NAS) relies heavily on labeled data, which is labor-intensive and time-consuming to obtain. In this paper, we propose a novel NAS method based on an unsupervised paradigm, specifically Masked Autoencoders (MAE), thereby eliminating the need for labeled data. By replacing the supervised learning objective with an image reconstruction task, our approach enables the efficient discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) in the unsupervised setting by designing a hierarchical decoder. Extensive experiments across various datasets demonstrate the effectiveness and robustness of our method, offering empirical evidence of its superiority over the counterparts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒MAE驱动的神经架构搜索：从掩码重建到架构创新</div>
<div class="mono" style="margin-top:8px">神经架构搜索（NAS）严重依赖标注数据，而获取此类数据既耗时又费力。本文提出一种基于无监督范式（特别是掩码自编码器MAE）的新型NAS方法，从而无需标注数据。通过将监督学习目标替换为图像重建任务，我们的方法能够在保持性能和泛化能力的同时，高效探索网络架构。此外，针对无监督场景下广泛使用的可微分架构搜索（DARTS）中出现的性能崩溃问题，我们设计了分层解码器加以解决。跨多个数据集的广泛实验验证了本方法的有效性与鲁棒性，并提供了优于现有方法的实证依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the high cost of labeled data for Neural Architecture Search (NAS) and proposes an unsupervised method based on Masked Autoencoders (MAE) to eliminate dependency on labels. The method replaces supervised objectives with an image reconstruction task and introduces a hierarchical decoder to prevent performance collapse issues common in unsupervised Differentiable Architecture Search (DARTS). Experimental results across multiple datasets show that the approach efficiently discovers high-performing architectures without compromising generalization, demonstrating robustness and superiority over existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对神经架构搜索（NAS）依赖标注数据成本高昂的问题，提出了一种基于掩码自编码器（MAE）的无监督方法以摆脱对标签的依赖。该方法将监督学习目标替换为图像重建任务，并设计了分层解码器来解决无监督可微分架构搜索（DARTS）中常见的性能崩溃问题。在多个数据集上的广泛实验表明，该方法能高效地发现高性能网络架构，且不损害泛化能力，验证了其鲁棒性和相对于现有方法的优越性。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models</div>
<div class="meta-line">Authors: David Bani-Harouni, Chantal Pellegrini, Paul Stangel, Ege Özsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab</div>
<div class="meta-line">First: 2025-03-04T13:48:50+00:00 · Latest: 2026-01-28T08:51:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.02623v5">Abs</a> · <a href="https://arxiv.org/pdf/2503.02623v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We propose a novel Reinforcement Learning approach that allows to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to factual questions. Our method optimizes a reward based on the logarithmic scoring rule, explicitly penalizing both over- and under-confidence. This encourages the model to align its confidence estimates with the actual predictive accuracy. The optimal policy under our reward design would result in perfectly calibrated confidence expressions. Unlike prior approaches that decouple confidence estimation from response generation, our method integrates confidence calibration seamlessly into the generative process of the LLM. Empirically, we demonstrate that models trained with our approach exhibit substantially improved calibration and generalize to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励质疑：基于强化学习的大语言模型校准置信度表达方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的安全可信使用需要对其答案的置信度进行准确表达。我们提出一种新颖的强化学习方法，可直接微调LLMs使其在回答事实性问题时同步输出经过校准的置信度估计。该方法通过基于对数评分规则的奖励函数进行优化，明确惩罚过度自信与自信不足两种偏差，促使模型将置信度估计与实际预测准确率对齐。在我们的奖励设计下，最优策略将产生完全校准的置信度表达。与以往将置信度估计与回答生成分离的方法不同，本方法将置信度校准无缝集成至LLMs的生成过程中。实证研究表明，采用本方法训练的模型显著提升了校准性能，且无需额外微调即可泛化至未见任务，表明其形成了通用的置信度感知能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to enhance the safety and trustworthiness of Large Language Models (LLMs) by improving their ability to express calibrated confidence in answers to factual questions. The method introduces a novel Reinforcement Learning approach that directly fine-tunes LLMs to output confidence estimates alongside responses, optimizing a reward based on the logarithmic scoring rule to penalize both over- and under-confidence and align confidence with predictive accuracy. Experimental results show that models trained this way achieve substantially improved calibration and generalize to unseen tasks without additional fine-tuning, indicating the emergence of general confidence awareness.</div>
<div class="mono" style="margin-top:8px">该研究的动机是通过提升大语言模型在回答事实性问题时表达校准置信度的能力，以增强其安全性和可信度。方法上提出了一种新颖的强化学习方案，直接微调大语言模型，使其在生成答案的同时输出置信度估计，并基于对数评分规则设计奖励函数，明确惩罚过度自信和自信不足，促使模型置信度与实际预测准确性对齐。主要实验结果表明，经此方法训练的模型校准性能显著改善，且无需进一步微调即可泛化至未见任务，这表明模型形成了通用的置信度意识。</div>
</details>
</div>
<div class="card">
<div class="title">Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents</div>
<div class="meta-line">Authors: Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang</div>
<div class="meta-line">First: 2025-09-27T01:36:46+00:00 · Latest: 2026-01-28T07:59:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23040v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23040v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the &quot;memorize while reading&quot; methods. While this approach scales efficiently, it suffers from pruning of latent evidence, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, which integrates the mechanism of memory retrieval into the memory update process, enabling the agent to selectively callback historical memories for non-linear reasoning. To further strengthen training, we propose a multi-level reward design, which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support complex multi-hop reasoning. Extensive experiments demonstrate that ReMemR1 significantly outperforms state-of-the-art baselines on long-context question answering while incurring negligible computational overhead, validating its ability to trade marginal cost for robust long-context reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>回望以向前推理：面向长上下文LLM智能体的可重访记忆机制</div>
<div class="mono" style="margin-top:8px">大语言模型在长上下文问答任务中面临关键证据分散于数百万标记的挑战。现有研究通过线性文档扫描动态更新记忆缓冲区（即“边读边记”方法），虽具扩展效率，但存在潜在证据剪枝、覆盖写入导致信息丢失、以及稀疏强化学习信号等问题。为此，我们提出ReMemR1，将记忆检索机制融入记忆更新过程，使智能体能够选择性回调历史记忆进行非线性推理。为进一步强化训练，我们设计了结合最终答案奖励与密集步骤级信号的多层次奖励机制，以引导有效的记忆使用。这些贡献共同缓解了信息衰减、改进了监督机制，并支持复杂的多跳推理。大量实验表明，ReMemR1在长上下文问答任务上显著优于现有基线方法，且计算开销可忽略，验证了其以边际成本换取稳健长上下文推理的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of long-context question answering in large language models, where relevant evidence is often scattered across extensive documents, by proposing ReMemR1, a method that enhances memory mechanisms. The motivation stems from limitations in existing linear &quot;memorize while reading&quot; approaches, which suffer from information loss and sparse training signals. The method integrates memory retrieval into the update process, allowing selective callback of historical memories for non-linear reasoning, and employs a multi-level reward design combining final-answer and step-level signals to improve supervision. Experimental results show that ReMemR1 significantly outperforms state-of-the-art baselines on long-context QA tasks with minimal computational overhead, demonstrating robust reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在长上下文问答中关键证据分散的挑战，提出了ReMemR1方法以改进记忆机制。其动机源于现有线性“边读边记”方法存在信息丢失和训练信号稀疏的问题。该方法将记忆检索整合到更新过程中，支持选择性回调历史记忆以进行非线性推理，并采用结合最终答案和步骤级信号的多层次奖励设计来加强训练。实验结果表明，ReMemR1在长上下文问答任务上显著优于现有先进基线，且计算开销可忽略，验证了其以边际成本实现稳健推理的能力。</div>
</details>
</div>
<div class="card">
<div class="title">RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation</div>
<div class="meta-line">Authors: Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, Wei Chen</div>
<div class="meta-line">First: 2026-01-13T10:56:39+00:00 · Latest: 2026-01-28T07:50:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08430v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08430v2">PDF</a> · <a href="https://github.com/teqkilla/RubricHub}{">Code1</a> · <a href="https://github.com/teqkilla/RubricHub">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. Our code is available at \href{https://github.com/teqkilla/RubricHub}{ this URL}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RubricHub：通过自动化从粗到细生成构建的全面且高区分度的评分标准数据集</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）在数学等推理密集型领域取得了显著进展。然而，由于缺乏真实基准，优化开放式生成仍具挑战。基于评分标准的评估虽为验证提供了结构化代理，但现有方法存在可扩展性瓶颈和标准粗糙的问题，导致监督天花板效应。为此，我们提出了一种自动化从粗到细的评分标准生成框架。通过结合原则引导合成、多模型聚合与难度演化，该方法能生成全面且高区分度的标准，捕捉细微差异。基于此框架，我们推出了RubricHub——一个大规模（约11万条）多领域数据集。我们通过两阶段后训练流程验证其实用性，包括基于评分标准的拒绝采样微调（RuFT）和强化学习（RuRL）。实验结果表明，RubricHub带来显著性能提升：后训练的Qwen3-14B模型在HealthBench上达到69.3分的最先进水平，超越GPT-5等专有前沿模型。代码发布于\href{https://github.com/teqkilla/RubricHub}{此链接}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of optimizing open-ended text generation in reinforcement learning, where the lack of ground truth and coarse evaluation rubrics limit progress. To overcome this, the authors propose an automated Coarse-to-Fine Rubric Generation framework that integrates principle-guided synthesis, multi-model aggregation, and difficulty evolution to create detailed and discriminative evaluation criteria. They then introduce RubricHub, a large-scale dataset of approximately 110k rubrics across multiple domains, and validate it through a two-stage post-training pipeline involving Rubric-based Rejection Sampling Fine-Tuning and Reinforcement Learning. Experimental results show that models trained with RubricHub achieve state-of-the-art performance, with the Qwen3-14B model surpassing proprietary models like GPT-5 on the HealthBench benchmark with a score of 69.3.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中开放式文本生成的优化难题，指出缺乏真实标注和粗糙评估标准限制了进展。为此，作者提出了一种自动化的从粗到细的评估标准生成框架，结合原则引导合成、多模型聚合和难度演进，以创建细致且区分度高的评估准则。基于此框架，他们推出了RubricHub，一个包含约11万条标准的多领域大规模数据集，并通过包含基于标准的拒绝采样微调和强化学习的两阶段后训练流程进行验证。实验结果表明，使用RubricHub训练的模型实现了最先进的性能，其中Qwen3-14B模型在HealthBench基准测试中以69.3分超越了GPT-5等专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments</div>
<div class="meta-line">Authors: Zhuang Chen, Dazhen Wan, Zhangkai Zheng, Guanqun Bi, Xiyao Xiao, Binghang Li, Minlie Huang</div>
<div class="meta-line">First: 2026-01-28T07:48:39+00:00 · Latest: 2026-01-28T07:48:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20330v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20330v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs&#x27; performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PsychePass：基于轨迹锚定锦标赛的大型语言模型治疗能力校准</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型在心理健康领域展现出潜力，但由于咨询过程具有非结构化和长期性的特点，评估其治疗能力仍面临挑战。我们认为当前评估范式存在锚定缺失缺陷，导致两种不稳定性：过程漂移（无引导的客户模拟偏离特定咨询目标）和标准漂移（静态点式评分缺乏可靠判断的稳定性）。为此，我们提出PsychePass统一框架，通过轨迹锚定锦标赛校准LLM的治疗能力。首先在模拟中锚定交互轨迹，使客户能精确控制动态咨询过程以探测多维能力；随后通过高效的瑞士制锦标赛锚定对战轨迹，利用动态配对对战生成稳健的Elo评分。除排名功能外，我们证明锦标赛轨迹可转化为可信的奖励信号，支持基于策略的强化学习以提升LLM性能。大量实验验证了PsychePass的有效性及其与人类专家评估的高度一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of reliably evaluating large language models (LLMs) for mental healthcare, where current methods suffer from instability due to unanchored client simulations and static scoring, this paper introduces PsychePass, a framework that calibrates therapeutic competence via trajectory-anchored tournaments. The method anchors interactions in controlled client simulations to probe multifaceted capabilities and employs a Swiss-system tournament of pairwise battles to derive robust Elo ratings. Experimental results demonstrate the framework&#x27;s effectiveness and strong consistency with human expert judgments, and further show that the tournament trajectories can serve as credible reward signals for reinforcement learning to enhance LLM performance.</div>
<div class="mono" style="margin-top:8px">针对大语言模型在心理健康领域应用中，现有评估方法因模拟过程漂移和静态评分标准漂移导致不稳定的问题，本文提出了PsychePass框架，通过轨迹锚定的锦标赛来校准模型的治疗能力。该方法首先在模拟中锚定交互轨迹，让客户精确控制咨询过程以探测多方面能力，然后通过高效的瑞士制锦标赛进行动态两两对战，生成稳健的Elo评分。大量实验验证了该框架的有效性及其与人类专家判断的强一致性，并证明锦标赛轨迹可转化为可靠的奖励信号，用于策略强化学习以提升大语言模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning</div>
<div class="meta-line">Authors: Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng</div>
<div class="meta-line">First: 2025-06-04T17:51:08+00:00 · Latest: 2026-01-28T07:36:02+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04207v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.04207v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推进多模态推理：从优化冷启动到分阶段强化学习</div>
<div class="mono" style="margin-top:8px">受Deepseek-R1在复杂文本任务中卓越推理能力的启发，许多研究尝试通过直接应用强化学习（RL）来激发多模态大语言模型（MLLMs）的类似能力。然而，它们仍难以激活复杂推理。本文并非孤立地审视多模态RL，而是深入探究当前训练流程，识别出三个关键现象：1）有效的冷启动初始化对提升MLLM推理至关重要。有趣的是，我们发现仅使用精心筛选的文本数据进行初始化，其性能即可超越许多近期多模态推理模型，甚至在多模态RL之前。2）应用于多模态RL的标准GRPO存在梯度停滞问题，损害训练稳定性与性能。3）在多模态RL阶段后，进行纯文本RL训练能进一步提升多模态推理能力。这种分阶段训练方法有效平衡了感知基础与认知推理的发展。通过整合上述洞见并解决多模态RL问题，我们提出了ReVisual-R1，在MathVerse、MathVision、WeMath、LogicVista、DynaMath等挑战性基准及高难度AIME2024与AIME2025测试中，实现了开源7B参数MLLMs的新最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of activating complex reasoning in Multimodal Large Language Models (MLLMs) beyond direct reinforcement learning (RL) application, this paper analyzes training pipelines and identifies three key phenomena: an optimized text-only cold start initialization can outperform many multimodal models, standard multimodal RL suffers from gradient stagnation, and a subsequent text-only RL stage further boosts reasoning. The method incorporates these insights to address RL issues, proposing a staged training approach named ReVisual-R1. The main experimental results show that ReVisual-R1 achieves state-of-the-art performance among open-source 7B MLLMs on multiple challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, AIME2024, and AIME2025.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决在多模态大语言模型中直接应用强化学习难以激活复杂推理能力的问题。方法上，通过分析训练流程，发现了三个关键现象：优化的纯文本冷启动初始化能超越许多多模态模型，标准多模态强化学习存在梯度停滞，以及后续的纯文本强化学习阶段能进一步提升推理；基于这些发现，研究提出了一种名为ReVisual-R1的分阶段训练方法以解决强化学习问题。主要实验结果表明，ReVisual-R1在包括MathVerse、MathVision、WeMath、LogicVista、DynaMath、AIME2024和AIME2025在内的多个挑战性基准测试中，取得了开源7B多模态大语言模型中最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models</div>
<div class="meta-line">Authors: Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li, Beibei Dong, Jing Dong</div>
<div class="meta-line">First: 2026-01-28T06:54:36+00:00 · Latest: 2026-01-28T06:54:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20305v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20305v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#x27;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#x27;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内生重提示：面向统一多模态模型的自演进认知对齐</div>
<div class="mono" style="margin-top:8px">统一多模态模型虽具备强大的理解能力，却常难以有效指导生成任务。我们将其归因为认知鸿沟：模型缺乏对如何优化自身生成过程的理解。为弥合此鸿沟，我们提出内生重提示机制，通过在生成过程中产生自对齐描述符，将模型理解从被动编码过程转化为显式生成推理步骤。为实现该机制，我们引入SEER（自演进评估与重提示器）训练框架，仅需从紧凑代理任务——视觉指令精细化中抽取300个样本，即可构建两阶段内生循环：首先，通过可验证奖励的强化学习，借助课程学习激活模型的潜在评估能力，产生高保真内生奖励信号；其次，采用模型奖励思维的强化学习，利用该信号优化生成推理策略。实验表明，SEER在评估准确性、重提示效率与生成质量上持续超越前沿基线方法，且未损害通用多模态能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses a cognitive gap in Unified Multimodal Models (UMMs), where strong understanding fails to guide generation effectively. To bridge this, the authors propose Endogenous Reprompting, a mechanism that transforms passive understanding into explicit generative reasoning by having the model generate self-aligned descriptors during generation. They introduce SEER, a training framework that uses only 300 samples from a Visual Instruction Elaboration task to establish a two-stage loop: first, Reinforcement Learning with Verifiable Rewards activates latent evaluation ability to produce an endogenous reward signal, and second, Reinforcement Learning with Model-rewarded Thinking optimizes the generative reasoning policy using this signal. Experiments demonstrate that SEER outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality while preserving general multimodal capabilities.</div>
<div class="mono" style="margin-top:8px">本文针对统一多模态模型（UMMs）中存在的认知鸿沟问题展开研究，即模型强大的理解能力未能有效指导生成过程。为弥合此鸿沟，作者提出了内生重提示机制，通过让模型在生成过程中产生自对齐描述符，将被动理解转化为显式的生成推理步骤。他们引入了SEER训练框架，仅使用视觉指令细化这一紧凑代理任务中的300个样本，建立了一个两阶段内生循环：首先，通过可验证奖励的强化学习激活模型的潜在评估能力，产生高保真的内生奖励信号；其次，利用模型奖励思维的强化学习，基于该信号优化生成推理策略。实验结果表明，SEER在评估准确性、重提示效率和生成质量上均优于现有先进基线，且未损害通用的多模态能力。</div>
</details>
</div>
<div class="card">
<div class="title">AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning</div>
<div class="meta-line">Authors: Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu, Ranjay Krishna, Yu Cheng</div>
<div class="meta-line">First: 2026-01-26T16:04:43+00:00 · Latest: 2026-01-28T05:44:53+00:00</div>
<div class="meta-line">Comments: 28 pages, 10 figures and 13 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18631v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18631v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaReasoner：面向迭代式视觉推理的动态工具编排框架</div>
<div class="mono" style="margin-top:8px">当人类面临超出即时能力的问题时，会借助工具解决，这为提升多模态大语言模型（MLLMs）的视觉推理能力提供了可行范式。有效的推理关键在于：即使面对新工具或新任务，也能准确判断使用何种工具、何时调用工具以及如何在多步骤中组合工具。本文提出 \textbf{AdaReasoner}——一类将工具使用作为通用推理技能（而非针对特定工具或显式监督行为）进行学习的多模态模型系列。AdaReasoner 通过以下技术实现：（i）可扩展的数据构建流程，使模型接触长跨度、多步骤的工具交互；（ii）Tool-GRPO 强化学习算法，基于最终任务成功率优化工具选择与序列编排；（iii）自适应学习机制，动态调节工具使用策略。这些组件共同使模型能够从任务上下文与中间结果推断工具效用，实现多工具协同及对未见工具的泛化。实验表明，AdaReasoner 展现出强大的工具自适应与泛化能力：尽管从未接受相关显式训练，它能自主采用有益工具、抑制无关工具，并根据任务需求动态调整工具使用频率。这些能力使其在多项挑战性基准测试中达到最先进水平：7B 基础模型平均性能提升 24.9%，在 VSP 与 Jigsaw 等多项任务上超越 GPT-5 等强效专有系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces AdaReasoner, a family of multimodal models designed to enhance visual reasoning by enabling dynamic tool orchestration, motivated by the need for models to autonomously select, sequence, and adapt tools in multi-step reasoning tasks without explicit supervision. The method employs a scalable data curation pipeline for long-horizon tool interactions, a reinforcement learning algorithm called Tool-GRPO to optimize tool selection based on end-task success, and an adaptive learning mechanism that regulates tool usage dynamically. Experimental results show that AdaReasoner achieves state-of-the-art performance, improving a 7B base model by an average of 24.9% and surpassing proprietary systems like GPT-5 on tasks such as VSP and Jigsaw, while demonstrating strong generalization to unseen tools and adaptive tool usage behaviors.</div>
<div class="mono" style="margin-top:8px">本文提出了AdaReasoner，这是一个多模态模型系列，旨在通过动态工具编排来增强视觉推理能力，其动机是让模型能够在多步推理任务中自主选择、排序和适应工具，而无需显式监督。该方法采用可扩展的数据处理流程来构建长时程工具交互数据，使用名为Tool-GRPO的强化学习算法根据最终任务成功率优化工具选择，并引入自适应学习机制动态调节工具使用。实验结果表明，AdaReasoner在多个基准测试中取得了最先进的性能，将7B基础模型的平均性能提升了24.9%，并在VSP和Jigsaw等任务上超越了GPT-5等强大专有系统，同时展现出对未见工具的泛化能力和自适应工具使用行为。</div>
</details>
</div>
<div class="card">
<div class="title">HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning</div>
<div class="meta-line">Authors: Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin</div>
<div class="meta-line">First: 2026-01-15T08:45:54+00:00 · Latest: 2026-01-28T04:03:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10187v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10187v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively &quot;tames&quot; the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HOMURA：通过强化学习驯服沙漏模型实现时间受限的大语言模型翻译</div>
<div class="mono" style="margin-top:8px">大语言模型在多语言翻译领域取得显著进展，但受系统性跨语言冗长偏差的制约，使其难以适用于字幕配音等严格时间受限的任务。现有提示工程方法难以平衡语义保真度与刚性时间可行性之间的矛盾。为填补这一空白，我们首先提出沙漏基准——专门用于评估音节级时长约束下的翻译性能。进一步，我们提出HOMURA强化学习框架，显式优化语义保持与时间合规性的权衡。通过采用KL正则化目标及创新的动态音节比例奖励机制，HOMURA有效实现了输出长度的精准调控。实验结果表明，该方法显著优于主流大语言模型基线，在保持语义充分性的同时，实现了尊重语言密度层级的精确长度控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of cross-lingual verbosity bias in Large Language Models (LLMs) for translation, which makes them unsuitable for time-constrained applications like subtitling. To tackle the conflict between semantic fidelity and strict duration limits, the authors introduce the Sand-Glass benchmark for evaluating syllable-level constrained translation and propose HOMURA, a reinforcement learning framework that optimizes the trade-off using a KL-regularized objective with a dynamic syllable-ratio reward. Experimental results show that HOMURA significantly outperforms strong LLM baselines, achieving precise length control while maintaining semantic adequacy across linguistic density hierarchies.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在翻译中存在的跨语言冗长偏差问题展开研究，该问题使其不适用于字幕制作等严格时间约束的任务。为解决语义保真度与固定时长可行性之间的冲突，作者引入了Sand-Glass基准，专门用于评估音节级时长约束下的翻译性能，并提出了HOMURA强化学习框架，该框架通过使用带有动态音节比例奖励的KL正则化目标，明确优化语义保留与时间合规性之间的平衡。实验结果表明，该方法显著优于强大的LLM基线，在尊重语言密度层级的同时实现了精确的长度控制，且未损害语义充分性。</div>
</details>
</div>
<div class="card">
<div class="title">Certificate-Guided Pruning for Stochastic Lipschitz Optimization</div>
<div class="meta-line">Authors: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</div>
<div class="meta-line">First: 2026-01-28T04:02:22+00:00 · Latest: 2026-01-28T04:02:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20231v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20231v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $α$, we prove $\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\tildeO(\varepsilon^{-(2+α)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\log T)$ overhead; CGP-TR scales to $d &gt; 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机Lipschitz优化的证书引导剪枝方法</div>
<div class="mono" style="margin-top:8px">我们研究噪声评估下Lipschitz函数的黑箱优化问题。现有自适应离散化方法虽能隐式避开次优区域，但未提供明确的最优性证书或可量化的进展保证。本文提出**证书引导剪枝方法（CGP）**，通过置信度调整的Lipschitz包络维护显式的潜在最优点**活跃集**$A_t$。任何$A_t$外的点均能以高概率被证明是次优的，且在具有近最优维度$α$的边界条件下，我们证明$\Vol(A_t)$以受控速率收缩，获得$\tildeO(\varepsilon^{-(2+α)})$的样本复杂度。我们开发了三个扩展：CGP-Adaptive通过$O(\log T)$开销在线学习$L$；CGP-TR通过局部证书的信任域方法扩展至$d &gt; 50$维度；CGP-Hybrid在检测到局部平滑性时切换至高斯过程优化。在12个基准测试（$d \in [2, 100]$）上的实验表明，CGP变体在匹配或超越强基准的同时，能通过证书体积提供理论完备的停止准则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of black-box optimization of Lipschitz functions under noisy evaluations, motivated by the lack of explicit optimality certificates and progress guarantees in existing adaptive discretization methods. The authors propose Certificate-Guided Pruning (CGP), a method that maintains an active set of potentially optimal points using confidence-adjusted Lipschitz envelopes to certify suboptimal points with high probability. Under a margin condition, the method provably shrinks the active set volume at a controlled rate, achieving a sample complexity of Õ(ε^{-(2+α)}), and is extended with three variants: CGP-Adaptive for online Lipschitz constant learning, CGP-TR for high-dimensional scaling via trust regions, and CGP-Hybrid for switching to Gaussian process refinement when local smoothness is detected. Experimental results on 12 benchmarks with dimensions from 2 to 100 demonstrate that CGP variants match or exceed strong baselines while providing principled stopping criteria through certificate volume.</div>
<div class="mono" style="margin-top:8px">本文研究了噪声评估下Lipschitz函数的黑盒优化问题，其动机在于现有自适应离散化方法缺乏显式的最优性证明和可测量的进展保证。作者提出了证书引导剪枝（CGP）方法，该方法通过置信度调整的Lipschitz包络维护一个潜在最优点的活动集，以高概率证明次优点的不可行性。在边界条件下，该方法能以受控速率缩小活动集体积，实现Õ(ε^{-(2+α)})的样本复杂度，并扩展出三个变体：CGP-Adaptive用于在线学习Lipschitz常数，CGP-TR通过信任区域扩展到高维空间，CGP-Hybrid在检测到局部平滑性时切换为高斯过程优化。在12个维度从2到100的基准测试中，CGP变体匹配或超越了强基线方法，同时通过证书体积提供了原则性的停止准则。</div>
</details>
</div>
<div class="card">
<div class="title">Proactive SFC Provisioning with Forecast-Driven DRL in Data Centers</div>
<div class="meta-line">Authors: Parisa Fard Moshiri, Poonam Lohan, Burak Kantarci, Emil Janulewicz</div>
<div class="meta-line">First: 2026-01-28T04:00:13+00:00 · Latest: 2026-01-28T04:00:13+00:00</div>
<div class="meta-line">Comments: 6 pages, 3 figures, Accepted to IEEE International Conference on Communications (ICC) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20229v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20229v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Service Function Chaining (SFC) requires efficient placement of Virtual Network Functions (VNFs) to satisfy diverse service requirements while maintaining high resource utilization in Data Centers (DCs). Conventional static resource allocation often leads to overprovisioning or underprovisioning due to the dynamic nature of traffic loads and application demands. To address this challenge, we propose a hybrid forecast-driven Deep reinforcement learning (DRL) framework that combines predictive intelligence with SFC provisioning. Specifically, we leverage DRL to generate datasets capturing DC resource utilization and service demands, which are then used to train deep learning forecasting models. Using Optuna-based hyperparameter optimization, the best-performing models, Spatio-Temporal Graph Neural Network, Temporal Graph Neural Network, and Long Short-Term Memory, are combined into an ensemble to enhance stability and accuracy. The ensemble predictions are integrated into the DC selection process, enabling proactive placement decisions that consider both current and future resource availability. Experimental results demonstrate that the proposed method not only sustains high acceptance ratios for resource-intensive services such as Cloud Gaming and VoIP but also significantly improves acceptance ratios for latency-critical categories such as Augmented Reality increases from 30% to 50%, while Industry 4.0 improves from 30% to 45%. Consequently, the prediction-based model achieves significantly lower E2E latencies of 20.5%, 23.8%, and 34.8% reductions for VoIP, Video Streaming, and Cloud Gaming, respectively. This strategy ensures more balanced resource allocation, and reduces contention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据中心中基于预测驱动深度强化学习的主动式服务功能链配置</div>
<div class="mono" style="margin-top:8px">服务功能链（SFC）需要高效部署虚拟网络功能（VNF）以满足多样化服务需求，同时维持数据中心（DC）的高资源利用率。传统静态资源分配常因流量负载与应用需求的动态特性导致资源过度配置或配置不足。为此，我们提出一种融合预测智能与SFC配置的混合预测驱动深度强化学习（DRL）框架。具体而言，我们利用DRL生成捕获DC资源利用率与服务需求的数据集，进而训练深度学习预测模型。通过基于Optuna的超参数优化，将表现最优的时空图神经网络、时序图神经网络和长短期记忆网络组合成集成模型以提升稳定性与准确性。集成预测结果被纳入DC选择过程，实现兼顾当前与未来资源可用性的主动式部署决策。实验结果表明，该方法不仅维持了云游戏和VoIP等高资源需求服务的高接受率，还将增强现实等时延敏感类别的接受率从30%提升至50%，工业4.0从30%提升至45%。基于预测的模型使VoIP、视频流和云游戏的端到端时延分别显著降低20.5%、23.8%和34.8%。该策略实现了更均衡的资源分配并减少了资源争用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of dynamic Service Function Chaining (SFC) provisioning in data centers, where static resource allocation often leads to inefficiency. The authors propose a hybrid forecast-driven deep reinforcement learning (DRL) framework that first uses DRL to generate datasets on resource utilization and service demands, then trains an ensemble of forecasting models (including Spatio-Temporal Graph Neural Networks and LSTM) optimized with Optuna for proactive SFC placement. Experimental results show the method significantly improves service acceptance ratios, notably increasing Augmented Reality from 30% to 50% and Industry 4.0 from 30% to 45%, while reducing end-to-end latency by 20.5% to 34.8% for services like VoIP and Cloud Gaming, leading to more balanced resource allocation.</div>
<div class="mono" style="margin-top:8px">本文针对数据中心中服务功能链（SFC）的动态配置挑战，传统静态资源分配常导致效率低下。作者提出了一种混合预测驱动的深度强化学习（DRL）框架，首先利用DRL生成资源利用率和服务需求的数据集，然后通过Optuna优化的预测模型集成（包括时空图神经网络和LSTM）进行训练，以实现主动的SFC放置。实验结果表明，该方法显著提高了服务接受率，特别是将增强现实从30%提升至50%，工业4.0从30%提升至45%，同时将VoIP和云游戏等服务的端到端延迟降低了20.5%至34.8%，从而实现更均衡的资源分配。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning</div>
<div class="meta-line">Authors: Hang Zhang, Ruheng Wang, Yuelyu Ji, Mingu Kwak, Xizhi Wu, Chenyu Li, Li Zhang, Wenqi Shi, Yifan Peng, Yanshan Wang</div>
<div class="meta-line">First: 2026-01-28T03:44:20+00:00 · Latest: 2026-01-28T03:44:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20221v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20221v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\method$ demonstrates an $\mathbf{8\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过工具集成强化学习扩展医学推理验证</div>
<div class="mono" style="margin-top:8px">大型语言模型在医学推理基准测试中表现出色，但其在临床环境中的部署需要严格验证以确保事实准确性。虽然奖励模型为推理轨迹验证提供了可扩展的方法，但现有方法存在两个局限：仅生成标量奖励值而缺乏明确依据，且依赖单次检索，无法在验证过程中自适应获取知识。我们提出$\method$，这是一个通过训练医学推理验证器在评估过程中迭代查询外部医学语料库来解决这些问题的智能框架。该方法将工具增强验证与仅需轨迹级监督的迭代强化学习范式相结合，并引入动态调整训练数据分布的自适应课程机制。在四个医学推理基准测试中，$\method$相比现有方法取得显著提升，特别是将MedQA准确率提高23.5%，MedXpertQA提高32.0%。关键的是，$\method$相比先前的奖励模型基线实现了$\mathbf{8倍}$的采样预算降低。这些发现表明，基于动态检索证据的验证为构建更可靠的医学推理系统提供了理论路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need to rigorously verify the factual accuracy of large language models in clinical settings, where existing reward models provide only scalar rewards without justification and rely on static retrieval. The method introduces an agentic framework that trains medical reasoning verifiers to iteratively query external medical corpora during evaluation, combining tool-augmented verification with an iterative reinforcement learning paradigm using trace-level supervision and an adaptive curriculum. Experimental results across four medical benchmarks show substantial gains, including a 23.5% improvement in MedQA accuracy and a 32.0% improvement in MedXpertQA over the base generator, along with an 8x reduction in sampling budget compared to prior baselines, demonstrating that dynamic evidence retrieval enhances reliability.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于在临床环境中需要严格验证大语言模型的事实准确性，而现有奖励模型仅提供无解释的标量奖励且依赖静态检索。方法上，提出了一个代理框架，通过训练医学推理验证器在评估过程中迭代查询外部医学语料库，结合工具增强验证与基于轨迹级监督的迭代强化学习范式及自适应课程机制。在四个医学推理基准上的实验结果表明，该方法取得了显著提升，例如MedQA准确率相对基础生成器提高了23.5%，MedXpertQA提高了32.0%，同时采样预算需求较先前基线降低了8倍，证明动态证据检索为构建更可靠的医学推理系统提供了有效途径。</div>
</details>
</div>
<div class="card">
<div class="title">OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval</div>
<div class="meta-line">Authors: Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-22T14:50:26+00:00 · Latest: 2026-01-28T03:43:07+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.16438v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.16438v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERA&#x27;s Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERA&#x27;s superior performance, validating both the MAPGRPO method and OPERA&#x27;s design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OPERA：一种强化学习增强的编排式规划器-执行器架构，面向推理导向的多跳检索</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）与密集检索器的近期进展推动了检索增强生成（RAG）领域的显著进步。然而，现有方法在复杂的推理导向多跳检索任务中面临重大挑战：1）推理导向规划效果不佳：先前方法难以针对复杂查询生成鲁棒的多步规划，基于规则的分解器在非模板化问题上表现较差。2）推理驱动检索次优：相关方法采用有限的查询重构，导致迭代检索循环常无法定位关键文档。3）推理引导过滤不足：主流方法缺乏细粒度推理能力，难以从噪声结果中有效筛选关键信息，阻碍了检索知识的利用。根本上，这些局限均源于当前RAG架构中检索与推理的弱耦合。我们提出了编排式规划器-执行器推理架构（OPERA），一种新颖的推理驱动检索框架。OPERA的目标规划模块（GPM）将问题分解为子目标，由推理执行模块（REM）通过专精组件执行精确推理与高效检索。为训练OPERA，我们提出了多智能体渐进组相对策略优化（MAPGRPO），一种GRPO的新变体。在复杂多跳基准测试上的实验表明OPERA具有优越性能，验证了MAPGRPO方法及OPERA设计的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing retrieval-augmented generation (RAG) systems in complex multi-hop reasoning tasks—specifically ineffective planning, suboptimal retrieval, and insufficient filtering due to weak coupling between reasoning and retrieval—this paper introduces OPERA, a reinforcement learning-enhanced orchestrated planner-executor architecture. The method features a Goal Planning Module to decompose queries and a Reason-Execute Module for precise reasoning and retrieval, trained with a novel Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO) algorithm. Experimental results on complex multi-hop benchmarks demonstrate OPERA&#x27;s superior performance, validating both the architectural design and the training approach.</div>
<div class="mono" style="margin-top:8px">本文的动机是现有检索增强生成（RAG）系统在复杂多跳推理任务中存在规划低效、检索欠佳和过滤不足等局限，其根源在于推理与检索的耦合较弱。为此，论文提出了OPERA，一种强化学习增强的编排式规划器-执行器架构，其方法包括用于问题分解的目标规划模块和用于精确推理与检索的推理执行模块，并采用新颖的多智能体渐进组相对策略优化（MAPGRPO）算法进行训练。在复杂多跳基准测试上的实验结果表明，OPERA取得了优越的性能，验证了其架构设计和训练方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning</div>
<div class="meta-line">Authors: Jinyang Wu, Shuo Yang, Changpeng Yang, Yuhao Shen, Shuai Zhang, Zhengqi Wen, Jianhua Tao</div>
<div class="meta-line">First: 2026-01-28T03:15:34+00:00 · Latest: 2026-01-28T03:15:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20209v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent&#x27;s intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spark：基于关键状态动态分支的战略策略感知探索用于长时程智能体学习</div>
<div class="mono" style="margin-top:8px">强化学习已赋能大语言模型作为智能体，但在长时程任务训练中，高质量轨迹的稀缺性（尤其在资源有限时）仍构成挑战。现有方法通常扩大轨迹规模并在中间步骤中无差别分配计算资源，这本质上浪费了大量计算预算于次要步骤，且无法保证样本质量。为此，我们提出\textbf{Spark}（基于关键状态动态分支的战略策略感知探索），一种通过在关键决策状态选择性分支以实现资源高效探索的新框架。核心思路是在关键决策点激活自适应分支探索以探测有潜力的轨迹，从而优先采样质量而非盲目覆盖，实现精准资源分配。该设计利用智能体内在决策信号减少对人类先验的依赖，使其能自主扩展探索并实现更强泛化能力。在多类任务（如具身规划）上的实验表明，\textsc{Spark}能以显著更少的训练样本获得更高成功率，并在未见场景中展现稳健泛化性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is the challenge of training large language model agents for long-horizon tasks, where existing methods waste computational resources on trivial steps and fail to ensure high-quality training samples. The proposed method, Spark, introduces a framework for strategic exploration by dynamically branching the agent&#x27;s action search at critical decision states, which allows for precise resource allocation focused on promising trajectories rather than blind coverage. Experimental results across tasks like embodied planning show that Spark achieves higher success rates with significantly fewer training samples and demonstrates robust generalization to unseen scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于训练大型语言模型智能体执行长程任务时面临挑战，现有方法在无关步骤上浪费计算资源且难以保证样本质量。为此，提出的Spark方法通过一种新颖框架，在关键决策状态动态分支进行探索，实现精准资源分配以优先采样高质量轨迹而非盲目覆盖。在具身规划等多样化任务上的实验表明，Spark能以显著更少的训练样本获得更高的成功率，并在未见场景中展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing</div>
<div class="meta-line">Authors: Guanyuan Pan, Shuai Wang, Yugui Lin, Tiansheng Zhou, Pietro Liò, Zhenxin Zhao, Yaqi Wang</div>
<div class="meta-line">First: 2026-01-12T08:37:32+00:00 · Latest: 2026-01-28T02:59:53+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07315v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.07315v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches rely solely on netlists, ignoring the circuit schematic, which hinders the cognitive link between the schematic and its performance. Furthermore, the black-box nature of machine learning methods and hallucination risks in large language models fail to provide the necessary ground-truth explainability required for industrial sign-off. To address these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-start from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance while maintaining physics-based explainability. VLM-CAD meets all specification requirements while maintaining low power consumption in optimizing an amplifier with a complementary input and a class-AB output stage, with a total runtime under 66 minutes across all experiments on two amplifiers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLM-CAD：面向模拟电路尺寸优化的视觉语言模型协同智能体设计流程</div>
<div class="mono" style="margin-top:8px">模拟混合信号电路尺寸优化涉及高维设计空间中的复杂权衡。现有自动优化方法仅依赖网表而忽略电路原理图，阻碍了原理图与性能间的认知关联。机器学习方法的黑箱特性及大语言模型的幻觉风险，均无法满足工业签核所需的可解释性要求。为此，我们提出视觉语言模型优化的协同智能体设计流程（VLM-CAD），通过电路分析、直流工作点优化、基于推理的尺寸确定及外部尺寸优化四个阶段，集成Image2Net工具标注原理图并生成结构化JSON描述供视觉语言模型精准解析。进一步提出可解释信任域贝叶斯优化方法（ExTuRBO），采用智能体生成种子的协同热启动策略，提供双粒度灵敏度分析支持外部尺寸优化，并生成完整设计报告。基于180nm、90nm和45nm预测技术模型的放大器尺寸优化实验表明，VLM-CAD在保持物理可解释性的同时有效平衡功耗与性能。在优化具有互补输入级和AB类输出级的放大器时，VLM-CAD满足全部指标要求且保持低功耗，两个放大器的全实验总运行时间低于66分钟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in existing automatic analog circuit sizing methods, which rely solely on netlists and lack explainability, by proposing VLM-CAD, a Vision Language Model-optimized collaborative agent workflow. The method integrates Image2Net to annotate schematics into structured JSON for VLMs and employs ExTuRBO, an explainable trust region Bayesian optimization approach with collaborative warm-start and sensitivity analysis. Experimental results on amplifier sizing across 180nm, 90nm, and 45nm technology nodes show that VLM-CAD effectively balances power and performance while maintaining physics-based explainability, meeting all specifications with low power consumption and a total runtime under 66 minutes for two amplifiers.</div>
<div class="mono" style="margin-top:8px">本文针对现有自动模拟电路尺寸设计方法仅依赖网表且缺乏可解释性的问题，提出了VLM-CAD，一种视觉语言模型优化的协同智能体工作流程。该方法集成Image2Net将电路原理图标注为结构化JSON供视觉语言模型解析，并采用ExTuRBO，一种基于可解释信任区域的贝叶斯优化方法，具备协同热启动和灵敏度分析功能。在180nm、90nm和45nm工艺节点上的放大器尺寸设计实验表明，VLM-CAD能有效平衡功耗与性能，同时保持基于物理的可解释性，满足所有设计指标且功耗较低，两个放大器的总运行时间在66分钟以内。</div>
</details>
</div>
<div class="card">
<div class="title">Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery</div>
<div class="meta-line">Authors: Zhipeng Zhang, Wenting Ma, Kai Li, Meng Guo, Lei Yang, Wei Yu, Hongji Cui, Yichen Zhang, Mo Zhang, Jinzhe Lin, Zhenjie Yao</div>
<div class="meta-line">First: 2026-01-28T02:43:03+00:00 · Latest: 2026-01-28T02:43:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20193v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20193v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自我怀疑与恢复的元认知强化学习</div>
<div class="mono" style="margin-top:8px">传统鲁棒强化学习方法通常侧重于抑制不可靠经验或受干扰奖励，但缺乏对自身学习过程可靠性的推理能力。这导致此类方法要么因过度保守而对噪声反应过激，要么在不确定性累积时发生灾难性失效。本研究提出一种元认知强化学习框架，使智能体能够基于内部估计的可靠性信号评估、调节并恢复其学习行为。该方法引入由价值预测误差稳定性驱动的元信任变量，通过故障安全调节与渐进信任恢复机制调控学习动态。在奖励受干扰的连续控制基准测试中，实验表明具备恢复能力的元认知控制相比强鲁棒基线方法，实现了更高的平均回报，并显著减少了训练后期的失效情况。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing robust reinforcement learning methods, which often lack introspective reasoning about their own learning reliability and can either overreact to noise or fail catastrophically, this paper proposes a meta-cognitive reinforcement learning framework. The method enables an agent to assess, regulate, and recover its learning behavior using internally estimated reliability signals, specifically through a meta-trust variable driven by Value Prediction Error Stability (VPES) that modulates learning via fail-safe regulation and gradual trust recovery. Experimental results on continuous-control benchmarks with reward corruption show that this recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.</div>
<div class="mono" style="margin-top:8px">针对现有鲁棒强化学习方法缺乏对自身学习过程可靠性的内省推理、容易对噪声反应过度或累积不确定性导致灾难性失败的局限性，本文提出了一种元认知强化学习框架。该方法使智能体能够基于内部估计的可靠性信号来评估、调节和恢复其学习行为，具体通过价值预测误差稳定性驱动的元信任变量，以故障安全调节和渐进信任恢复来调制学习动态。在带有奖励干扰的连续控制基准测试中，实验结果表明，这种支持恢复的元认知控制相比强大的鲁棒性基线方法，实现了更高的平均回报，并显著减少了训练后期的失败。</div>
</details>
</div>
<div class="card">
<div class="title">R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Zhizheng Jiang, Kang Zhao, Weikai Xu, Xinkui Lin, Wei Liu, Jian Luan, Shuo Shang, Peng Han</div>
<div class="meta-line">First: 2026-01-27T13:55:34+00:00 · Latest: 2026-01-28T02:38:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19620v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19620v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>R^3：面向大语言模型强化学习的回放、反思与排序奖励机制</div>
<div class="mono" style="margin-top:8px">大型推理模型旨在通过结构化推理解决多样复杂问题。基于分组的策略优化方法近期取得进展，可在不依赖过程级标注的情况下实现稳定的优势估计。然而，这些方法依赖同批次内高质量样本产生的优势差异，当组内优势在复杂任务中坍缩时，会导致训练过程脆弱且低效。为解决这些问题，我们提出名为R^3的强化学习机制，包含三个方向：（1）跨上下文回放策略：通过回溯同一查询的历史轨迹中有价值的示例，维持组内优势；（2）上下文内自反思机制：使模型能借助过往失败经验优化输出；（3）结构熵排序奖励：通过基于词元级熵模式对响应进行排序，为截断或失败样本分配相对奖励，同时捕捉局部探索与全局稳定性。我们在Deepseek-R1-Distill-Qwen-1.5B上实现该方法，并使用数学领域的DeepscaleR-40k数据集进行训练。实验表明，该方法在多个数学基准测试中达到最先进性能，相比基线模型实现了显著提升且减少了推理词元数量。代码与模型将开源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces R^3, a reinforcement learning mechanism designed to enhance the training stability and efficiency of large reasoning models on complex tasks, addressing the fragility of existing group-based methods that rely on intra-batch advantage gaps. The method combines three components: cross-context replay to preserve advantage by reusing historical trajectories, in-context self-reflection for iterative output refinement using past failures, and a structural entropy ranking reward that assigns relative rewards based on token-level entropy patterns to handle truncated or failed samples. Experimental results on math benchmarks using Deepseek-R1-Distill-Qwen-1.5B trained on DeepscaleR-40k show state-of-the-art performance with significant accuracy improvements and reduced reasoning tokens compared to base models.</div>
<div class="mono" style="margin-top:8px">本文提出了R^3强化学习机制，旨在提升大型推理模型在复杂任务上的训练稳定性和效率，以解决现有基于群体的方法依赖批次内优势差距而导致的脆弱性问题。该方法整合了三个方向：跨上下文回放通过重用历史轨迹来维持优势，上下文内自反思利用过往失败迭代优化输出，以及结构熵排序奖励基于词元级熵模式对截断或失败样本分配相对奖励。在数学领域使用Deepseek-R1-Distill-Qwen-1.5B模型和DeepscaleR-40k数据集进行的实验表明，该方法在多个数学基准测试中实现了最先进的性能，相比基础模型在准确率上有显著提升且推理词元更少。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</div>
<div class="meta-line">Authors: Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Tianle Cai, Wenhao Huang</div>
<div class="meta-line">First: 2025-12-22T22:51:48+00:00 · Latest: 2026-01-28T01:58:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19920v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.19920v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model&#x27;s log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5&#x27;s (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过行为校准强化学习缓解大语言模型幻觉问题</div>
<div class="mono" style="margin-top:8px">大语言模型在关键领域的应用目前受到持续幻觉问题（生成看似合理但事实错误的断言）的阻碍。虽然扩展定律显著提升了模型的通用能力，但理论框架表明幻觉并非随机误差，而是训练目标优先模仿数据分布而非认知诚实度的可预测统计结果。标准的RLVR范式使用二元奖励信号，无意中激励模型成为优秀的应试者而非诚实的交流者，只要正确概率超过零就会鼓励猜测。本文对行为校准进行了全面研究，通过激励模型在不自信时主动弃权来随机承认不确定性，使模型行为与准确性对齐。综合最新进展，我们提出并评估了优化严格适当评分规则的训练干预措施，使模型输出校准后的正确概率。我们的方法使模型能够选择不生成完整回复，或标记存在不确定性的具体主张。基于Qwen3-4B-Instruct的实证分析表明，行为校准强化学习使较小模型在不确定性量化方面超越前沿模型——这是一种可转移的元技能，可与原始预测准确性解耦。在数学推理任务上训练后，我们的模型在挑战性领域内评估（BeyondAIME）中取得的对数尺度准确率-幻觉比率增益（0.806）超过GPT-5（0.207）。此外，在跨领域事实问答（SimpleQA）中，我们的40亿参数大语言模型实现了与Grok-4、Gemini-2.5-Pro等前沿模型相当的零样本校准误差，尽管其事实准确性远低于这些模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the persistent issue of hallucinations in large language models (LLMs), where models generate plausible but incorrect assertions, by proposing that hallucination is a predictable statistical outcome of training objectives that prioritize data distribution mimicry over epistemic honesty. The method introduces behaviorally calibrated reinforcement learning, which optimizes strictly proper scoring rules to incentivize models to output a calibrated probability of correctness, thereby enabling them to either abstain from responding or flag uncertain claims when not confident. Experimental results using Qwen3-4B-Instruct show that this approach allows smaller models to surpass frontier models in uncertainty quantification, achieving a superior log-scale Accuracy-to-Hallucination Ratio gain on math reasoning tasks and competitive zero-shot calibration error on factual QA benchmarks compared to models like GPT-5 and Gemini-2.5-Pro.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型中持续存在的幻觉问题——即模型生成看似合理但事实错误的断言——提出幻觉是训练目标优先模仿数据分布而非认知诚实的可预测统计结果。方法采用行为校准的强化学习，通过优化严格适当评分规则来激励模型输出校准后的正确概率，从而使其在不自信时能够选择弃答或标记不确定主张。基于Qwen3-4B-Instruct的实验结果表明，该方法使较小模型在不确定性量化方面超越前沿模型，在数学推理任务上获得了更优的对数尺度准确率-幻觉比率增益，并在事实问答基准测试中实现了与Grok-4和Gemini-2.5-Pro等模型相当的零样本校准误差。</div>
</details>
</div>
<div class="card">
<div class="title">Governing Strategic Dynamics: Equilibrium Stabilization via Divergence-Driven Control</div>
<div class="meta-line">Authors: Hao Shi, Xi Li, Fangfang Xie</div>
<div class="meta-line">First: 2025-06-30T11:13:36+00:00 · Latest: 2026-01-28T01:08:30+00:00</div>
<div class="meta-line">Comments: 13 pages, 10 figures. Supplementary material included. Revised and extended version; title updated</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.23734v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.23734v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Black-box coevolution in mixed-motive games is often undermined by opponent-drift non-stationarity and noisy rollouts, which distort progress signals and can induce cycling, Red-Queen dynamics, and detachment. We propose the \emph{Marker Gene Method} (MGM), a curriculum-inspired governance mechanism that stabilizes selection by anchoring evaluation to cross-generational marker individuals, together with DWAM and conservative marker-update rules to reduce spurious updates. We also introduce NGD-Div, which adapts the key update threshold using a divergence proxy and natural-gradient optimization. We provide theoretical analysis in strictly competitive settings and evaluate MGM integrated with evolution strategies (MGM-E-NES) on coordination games and a resource-depletion Markov game. MGM-E-NES reliably recovers target coordination in Stag Hunt and Battle of the Sexes, achieving final cooperation probabilities close to $(1,1)$ (e.g., $0.991\pm0.01/1.00\pm0.00$ and $0.97\pm0.00/0.97\pm0.00$ for the two players). In the Markov resource game, it maintains high and stable state-conditioned cooperation across 30 seeds, with final cooperation of $\approx 0.954/0.980/0.916$ in \textsc{Rich}/\textsc{Poor}/\textsc{Collapsed} (both players; small standard deviations), indicating welfare-aligned and state-dependent behavior. Overall, MGM-E-NES transfers across tasks with minimal hyperparameter changes and yields consistently stable training dynamics, showing that top-level governance can substantially improve the robustness of black-box coevolution in dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>战略动态治理：基于发散驱动的均衡稳定控制</div>
<div class="mono" style="margin-top:8px">混合动机博弈中的黑箱协同演化常受对手漂移非平稳性与噪声推演破坏，这些因素会扭曲进展信号并可能引发循环、红皇后动态及解耦现象。本文提出《标记基因方法》——一种受课程学习启发的治理机制，通过将评估锚定于跨代标记个体来稳定选择过程，并结合DWAM与保守标记更新规则以减少伪更新。同时引入NGD-Div方法，利用发散代理与自然梯度优化自适应调整关键更新阈值。我们在严格竞争环境中提供理论分析，并在协调博弈与资源耗竭马尔可夫博弈中评估了融合演化策略的MGM-E-NES方法。实验表明：MGM-E-NES在猎鹿博弈与性别之战中可靠恢复目标协调，最终合作概率接近$(1,1)$（例如两位玩家分别达$0.991\pm0.01/1.00\pm0.00$与$0.97\pm0.00/0.97\pm0.00$）；在马尔可夫资源博弈中，该方法在30次随机种子下均保持高且稳定的状态条件合作，最终在\textsc{富足}/\textsc{贫困}/\textsc{崩溃}状态下的合作率分别为$\approx 0.954/0.980/0.916$（双方玩家；标准差极小），显示出福利对齐与状态依赖行为。总体而言，MGM-E-NES能以最小超参数调整跨任务迁移，并产生持续稳定的训练动态，证明顶层治理能显著提升动态环境中黑箱协同演化的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability in black-box coevolution within mixed-motive games, where opponent drift and noisy rollouts lead to cycling and poor convergence. To stabilize selection, the authors propose the Marker Gene Method (MGM), a governance mechanism that anchors evaluation to cross-generational marker individuals, combined with DWAM and conservative update rules to minimize spurious updates, and introduce NGD-Div for adaptive threshold tuning via divergence proxy and natural-gradient optimization. Experimental results on coordination games and a resource-depletion Markov game show that MGM integrated with evolution strategies (MGM-E-NES) reliably achieves near-perfect cooperation probabilities, such as 0.991/1.00 in Stag Hunt and 0.97/0.97 in Battle of the Sexes, and maintains high, stable state-conditioned cooperation across different game states with minimal hyperparameter adjustments, demonstrating robust performance in dynamic environments.</div>
<div class="mono" style="margin-top:8px">本文针对混合动机博弈中黑箱协同进化因对手漂移和噪声采样导致循环与收敛困难的不稳定性问题，提出了一种名为标记基因方法的治理机制，通过跨代标记个体锚定评估，结合DWAM和保守更新规则减少虚假更新，并引入NGD-Div利用散度代理和自然梯度优化自适应调整阈值。在协调博弈和资源消耗马尔可夫博弈上的实验结果表明，结合进化策略的MGM-E-NES能可靠实现接近完美的合作概率，如在猎鹿博弈中达到0.991/1.00，在性别之战中达到0.97/0.97，并在不同游戏状态下保持高且稳定的条件合作行为，仅需极少超参数调整，展现了在动态环境中的强鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Spectral Representation-based Reinforcement Learning</div>
<div class="meta-line">Authors: Chenxiao Gao, Haotian Sun, Na Li, Dale Schuurmans, Bo Dai</div>
<div class="meta-line">First: 2025-12-17T02:54:42+00:00 · Latest: 2026-01-28T00:45:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15036v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15036v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于谱表示的强化学习</div>
<div class="mono" style="margin-top:8px">在状态空间和动作空间庞大的实际应用中，强化学习通常采用函数逼近来表示策略、价值函数和动态模型等核心组件。尽管神经网络等强大逼近器具有出色的表达能力，但常存在理论模糊性、优化不稳定和探索困难等问题，且在实践中计算成本高昂。本文引入谱表示视角以解决强化学习中的这些难题。该框架源于转移算子的谱分解，可为后续策略优化提供系统动态的有效抽象，同时提供清晰的理论表征。我们揭示了如何为具有隐变量结构或基于能量结构的转移算子构建谱表示，这对应着从数据中提取谱表示的不同学习方法。值得注意的是，每种学习方法在该框架下均可实现有效的强化学习算法。我们还将该谱视角可证明地扩展至部分可观测马尔可夫决策过程。最后，我们在DeepMind控制套件的20余项挑战性任务中验证了这些算法，其性能达到或超越了当前最先进的无模型和基于模型的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the theoretical ambiguities, optimization instability, and high computational costs of powerful function approximations like neural networks in reinforcement learning (RL) for large-scale problems, this paper introduces a spectral representation framework derived from the spectral decomposition of the transition operator to provide an effective abstraction of system dynamics with clear theoretical grounding. The method involves constructing spectral representations for transition operators with latent variable or energy-based structures, leading to distinct learning algorithms that extract these representations from data, each realizing an effective RL algorithm, and the approach is provably extended to partially observable MDPs. Experimental validation on over 20 tasks from the DeepMind Control Suite demonstrates that these algorithms achieve performance comparable or superior to state-of-the-art model-free and model-based baselines.</div>
<div class="mono" style="margin-top:8px">针对强化学习中神经网络等强大函数近似方法在大规模状态和动作空间下存在的理论模糊性、优化不稳定性和高计算成本问题，本文引入了基于转移算子谱分解的谱表示框架，以提供具有清晰理论特征的系统动力学有效抽象。该方法通过为具有隐变量结构或基于能量结构的转移算子构建谱表示，衍生出从数据中提取这些表示的不同学习算法，每种算法均实现了有效的强化学习策略，并可证明地扩展到部分可观测马尔可夫决策过程。在DeepMind控制套件的20多个挑战性任务上的实验验证表明，这些算法的性能达到或超越了当前最先进的基于模型和无模型的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">QUASAR: An Evolutionary Algorithm to Accelerate High-Dimensional Optimization</div>
<div class="meta-line">Authors: Julian Soltes</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2025-11-17T19:02:31+00:00 · Latest: 2026-01-28T00:31:53+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures. Open-source package containing QUASAR is available via &#x27;pip install hdim_opt&#x27;; source code (with experiments) is maintained on GitHub at [https://www.github.com/jgsoltes/hdim-opt]</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13843v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.13843v3">PDF</a> · <a href="https://www.github.com/jgsoltes/hdim-opt">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-dimensional numerical optimization presents a persistent challenge in computational science. This paper introduces Quasi-Adaptive Search with Asymptotic Reinitialization (QUASAR), an evolutionary algorithm to accelerate convergence in complex, non-differentiable problems afflicted by the curse of dimensionality. QUASAR expands upon the core principles of Differential Evolution (DE), introducing quasi-adaptive mechanisms to dynamically balance exploration and exploitation in its search. Inspired by the behavior of quantum particles, the algorithm utilizes three highly stochastic mechanisms that augment standard DE: 1) probabilistic mutation strategies and scaling factors; 2) rank-based crossover rates; 3) asymptotically decaying covariance reinitializations.
  Evaluated on the notoriously difficult CEC2017 benchmark suite of 29 test functions, QUASAR achieved the lowest overall rank sum (367) using the Friedman test, outperforming DE (735) and L-SHADE (452). Geometric mean comparisons show average final solution quality improvements of $3.85 \times$ and $2.07 \times$ compared to DE and L-SHADE, respectively ($p \ll 0.001$), with average optimization speed averaging $1.40 \times$ and $5.16 \times$ faster. QUASAR&#x27;s performance establishes it as an effective, efficient, and user-friendly evolutionary algorithm for complex high-dimensional problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QUASAR：一种加速高维优化的进化算法</div>
<div class="mono" style="margin-top:8px">高维数值优化是计算科学中长期存在的挑战。本文提出具有渐近重初始化特性的准自适应搜索算法（QUASAR），这是一种进化算法，旨在加速受维度灾难影响的复杂、不可微问题的收敛。QUASAR在差分进化（DE）核心原理基础上进行扩展，引入准自适应机制动态平衡搜索过程中的探索与利用。受量子粒子行为启发，该算法采用三种高度随机化机制增强标准DE：1）概率性变异策略与缩放因子；2）基于排序的交叉率；3）渐近衰减的协方差重初始化。在包含29个测试函数的CEC2017高难度基准测试集上，QUASAR通过弗里德曼检验获得最低总秩和（367），优于DE（735）和L-SHADE（452）。几何均值比较显示，与DE和L-SHADE相比，最终解质量平均分别提升$3.85 \times$和$2.07 \times$（$p \ll 0.001$），平均优化速度分别加快$1.40 \times$和$5.16 \times$。QUASAR的性能表明其是处理复杂高维问题高效、实用且用户友好的进化算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces QUASAR, an evolutionary algorithm designed to accelerate high-dimensional optimization for complex, non-differentiable problems suffering from the curse of dimensionality. The method extends Differential Evolution by incorporating quasi-adaptive mechanisms inspired by quantum particle behavior, including probabilistic mutation strategies, rank-based crossover rates, and asymptotically decaying covariance reinitializations to dynamically balance exploration and exploitation. Experimental results on the CEC2017 benchmark suite of 29 functions show QUASAR achieved the best overall rank, with geometric mean comparisons indicating it produced solutions 3.85 times and 2.07 times better than standard DE and L-SHADE, respectively, while also being 1.40 to 5.16 times faster on average.</div>
<div class="mono" style="margin-top:8px">本文提出了QUASAR算法，旨在加速解决受维度灾难影响的复杂、不可微的高维数值优化问题。该方法在差分进化的核心原理上进行了扩展，引入了受量子粒子行为启发的准自适应机制，包括概率性变异策略、基于排名的交叉率以及渐近衰减的协方差重新初始化，以动态平衡搜索过程中的探索与利用。在29个函数的CEC2017基准测试集上的实验结果表明，QUASAR获得了最佳的综合排名，几何平均比较显示其最终解的质量分别比标准差分进化和L-SHADE提高了3.85倍和2.07倍，同时平均优化速度也快了1.40倍至5.16倍。</div>
</details>
</div>
<div class="card">
<div class="title">Generalization in Reinforcement Learning for Radio Access Networks</div>
<div class="meta-line">Authors: Burak Demirel, Yu Wang, Cristian Tatino, Pablo Soldati</div>
<div class="meta-line">First: 2025-07-09T07:22:22+00:00 · Latest: 2026-01-28T00:29:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.06602v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.06602v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern RAN operate in highly dynamic and heterogeneous environments, where hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass such heuristics in constrained settings, the diversity of deployments and unpredictable radio conditions introduce major generalization challenges. Data-driven policies frequently overfit to training conditions, degrading performance in unseen scenarios. To address this, we propose a generalization-centered RL framework for RAN control that: (i) robustly reconstructs dynamically varying states from partial and noisy observations, while encoding static and semi-static information, such as radio nodes, cell attributes, and their topology, through graph representations; (ii) applies domain randomization to broaden the training distribution; and (iii) distributes data generation across multiple actors while centralizing training in a cloud-compatible architecture aligned with O-RAN principles. Although generalization increases computational and data-management complexity, our distributed design mitigates this by scaling data collection and training across diverse network conditions. Applied to downlink link adaptation in five 5G benchmarks, our policy improves average throughput and spectral efficiency by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and by &gt;20% under high mobility. It matches specialized RL in full-buffer traffic and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks, respectively. In nine-cell deployments, GAT models offer 30% higher throughput over MLP baselines. These results, combined with our scalable architecture, offer a path toward AI-native 6G RAN using a single, generalizable RL agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无线接入网络中强化学习的泛化研究</div>
<div class="mono" style="margin-top:8px">现代无线接入网络运行在高度动态和异构的环境中，手动调优的基于规则的无线资源管理算法往往表现不佳。虽然强化学习在受限场景中能超越此类启发式方法，但部署的多样性和不可预测的无线条件带来了重大的泛化挑战。数据驱动的策略常过度拟合训练条件，在未见场景中性能下降。为此，我们提出了一种以泛化为核心的无线接入网络控制强化学习框架，该框架：（i）通过图表示编码静态和半静态信息（如无线节点、小区属性及其拓扑），同时从部分噪声观测中稳健重构动态变化状态；（ii）应用领域随机化以拓宽训练分布；（iii）将数据生成分布至多个执行器，同时将训练集中至符合O-RAN原则的云兼容架构中。尽管泛化增加了计算和数据管理复杂度，我们的分布式设计通过在不同网络条件下扩展数据收集和训练来缓解此问题。在五个5G基准测试的下行链路自适应应用中，我们的策略在满缓冲MIMO/mMIMO场景中比OLLA基线（10% BLER目标）平均吞吐量和频谱效率提升约10%，在高移动性场景中提升超20%。在满缓冲流量场景中与专用强化学习方案相当，在eMBB和混合流量基准测试中分别实现最高4倍和2倍的性能增益。在九小区部署中，图注意力网络模型比多层感知机基线吞吐量提升30%。这些结果结合我们的可扩展架构，为使用单一可泛化强化学习代理实现AI原生的6G无线接入网络提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of hand-tuned, rule-based algorithms in dynamic Radio Access Networks (RAN) and the poor generalization of standard RL policies to unseen conditions, this paper proposes a generalization-focused RL framework. The method robustly reconstructs dynamic states from partial observations using graph representations for network topology, employs domain randomization, and uses a distributed data generation architecture aligned with O-RAN principles. Experimental results on 5G downlink link adaptation show the policy improves average throughput and spectral efficiency by ~10% over an OLLA baseline in full-buffer scenarios and by &gt;20% under high mobility, while graph attention network models yield 30% higher throughput than MLP baselines in nine-cell deployments, demonstrating strong generalization.</div>
<div class="mono" style="margin-top:8px">针对动态无线接入网络中人工调优规则算法性能不足以及标准强化学习策略在未见场景中泛化能力差的问题，本文提出了一种以泛化为核心的强化学习框架。该方法通过图表示对网络拓扑进行编码以从部分观测中鲁棒重建动态状态，采用领域随机化技术，并利用符合O-RAN原则的分布式数据生成架构。在5G下行链路自适应实验中，该策略在满缓冲区场景下相比OLLA基线将平均吞吐量和频谱效率提升了约10%，在高移动性下提升超过20%；在九小区部署中，图注意力网络模型比多层感知机基线实现了30%的吞吐量增益，展现了优异的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Practical Policy Distillation for Reinforcement Learning in Radio Access Networks</div>
<div class="meta-line">Authors: Sara Khosravi, Burak Demirel, Linghui Zhou, Javier Rasines, Pablo Soldati</div>
<div class="meta-line">First: 2025-11-09T22:48:10+00:00 · Latest: 2026-01-28T00:28:56+00:00</div>
<div class="meta-line">Comments: This paper is accepted for publication in IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06563v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.06563v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adopting artificial intelligence (AI) in radio access networks (RANs) presents several challenges, including limited availability of link-level measurements (e.g., CQI reports), stringent real-time processing constraints (e.g., sub-1 ms per TTI), and network heterogeneity (different spectrum bands, cell types, and vendor equipment). A critical yet often overlooked barrier lies in the computational and memory limitations of RAN baseband hardware, particularly in legacy 4th Generation (4G) systems, which typically lack on-chip neural accelerators. As a result, only lightweight AI models (under 1 Mb and sub-100~μs inference time) can be effectively deployed, limiting both their performance and applicability. However, achieving strong generalization across diverse network conditions often requires large-scale models with substantial resource demands. To address this trade-off, this paper investigates policy distillation in the context of a reinforcement learning-based link adaptation task. We explore two strategies: single-policy distillation, where a scenario-agnostic teacher model is compressed into one generalized student model; and multi-policy distillation, where multiple scenario-specific teachers are consolidated into a single generalist student. Experimental evaluations in a high-fidelity, 5th Generation (5G)-compliant simulator demonstrate that both strategies produce compact student models that preserve the teachers&#x27; generalization capabilities while complying with the computational and memory limitations of existing RAN hardware.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向无线接入网强化学习的实用策略蒸馏方法</div>
<div class="mono" style="margin-top:8px">在无线接入网中应用人工智能面临多重挑战：链路级测量数据有限（如CQI报告）、严格的实时处理要求（如每传输时间间隔低于1毫秒）以及网络异构性（不同频段、小区类型和设备供应商）。一个关键但常被忽视的障碍在于RAN基带硬件的算力与内存限制，尤其在缺乏片上神经网络加速器的传统4G系统中。因此，仅能部署轻量化AI模型（小于1Mb且推理时间低于100微秒），这限制了其性能与应用范围。然而，要在多样化网络条件下实现强泛化能力，通常需要资源需求较高的大规模模型。为平衡这一矛盾，本文研究基于强化学习的链路自适应任务中的策略蒸馏技术，探索两种方案：单策略蒸馏——将场景无关的教师模型压缩为单一通用学生模型；多策略蒸馏——将多个场景专用教师模型融合为单一通用学生模型。在高保真5G合规仿真平台上的实验表明，两种方案均能生成紧凑的学生模型，在保持教师模型泛化能力的同时，满足现有RAN硬件的算力与内存限制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of deploying AI in radio access networks (RANs) where hardware constraints, such as limited computational resources and memory in legacy 4G systems, restrict the use of large, high-performing models. The motivation is to enable effective reinforcement learning for tasks like link adaptation under strict real-time and size limits. The method explores policy distillation, specifically comparing single-policy distillation from a scenario-agnostic teacher and multi-policy distillation from multiple scenario-specific teachers into a single compact student model. Experimental results in a 5G-compliant simulator show that both strategies yield lightweight student models (under 1 Mb and sub-100 μs inference) that maintain the generalization performance of the teachers while meeting RAN hardware constraints.</div>
<div class="mono" style="margin-top:8px">本文针对在无线接入网中部署人工智能所面临的挑战展开研究，这些挑战包括传统4G系统硬件计算资源和内存有限，无法支持大型高性能模型。研究动机是在严格的实时处理和模型大小限制下，实现强化学习在链路自适应等任务中的有效应用。方法上探索了策略蒸馏技术，具体比较了从单一场景无关教师模型进行单策略蒸馏，以及从多个场景特定教师模型进行多策略蒸馏，以生成紧凑的学生模型。在5G兼容仿真器中的实验结果表明，两种策略都能产生轻量级学生模型（小于1 Mb且推理时间低于100微秒），在满足RAN硬件限制的同时，保持了教师模型的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models</div>
<div class="meta-line">Authors: Abha Jha, Akanksha Mahajan, Ashwath Vaithinathan Aravindan, Praveen Saravanan, Sai Sailaja Policharla, Sonal Chaturbhuj Gehlot</div>
<div class="meta-line">First: 2026-01-27T23:42:07+00:00 · Latest: 2026-01-27T23:42:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20126v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20126v1">PDF</a> · <a href="https://github.com/Mystic-Slice/rl-abstention">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention (&quot;I don&#x27;t know&quot;) alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励大语言模型中的知识谦逊：学习何时不回答</div>
<div class="mono" style="margin-top:8px">大语言模型常产生幻觉或不可验证内容，削弱了其在事实性领域的可靠性。本研究探讨了基于可验证奖励的强化学习作为一种训练范式，通过同时奖励“我不知道”的弃答行为和正确答案来促进知识谦逊。我们在MedMCQA和Hendrycks数学基准上对Granite-3.3-2B-Instruct和Qwen-3-4B-Instruct模型进行微调与评估，采用三元奖励结构（-1、r_abs、1）并调整弃答奖励参数。进一步研究了将RLVR与监督微调策略结合的效果，即在强化学习前先教授弃答机制。结果表明：适度弃答奖励（r_abs≈-0.25至0.3）能在多项选择题任务中持续减少错误回答且不严重损害准确率，较大模型对弃答激励表现出更强鲁棒性。在开放式问答中，我们发现因探索不足导致的局限性可通过监督式弃答训练部分缓解。总体而言，这些发现证明了可验证奖励设计作为缓解语言模型幻觉的实用方法具有可行性与灵活性。可复现的弃答训练框架代码详见：https://github.com/Mystic-Slice/rl-abstention。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the need to mitigate hallucinations and unverifiable content in Large Language Models (LLMs) to improve their reliability. The method introduces Reinforcement Learning with Verifiable Rewards (RLVR), a training paradigm that uses a ternary reward structure to explicitly reward correct answers, penalize incorrect ones, and provide a separate reward for abstention (i.e., responding &quot;I don&#x27;t know&quot;). The main experimental results, from fine-tuning models like Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on MedMCQA and Hendrycks Math benchmarks, show that moderate abstention rewards consistently reduce incorrect responses without severely degrading accuracy on multiple-choice tasks, with larger models being more robust; however, limitations in open-ended question answering due to insufficient exploration were noted, which can be partially addressed with supervised abstention training prior to reinforcement learning.</div>
<div class="mono" style="margin-top:8px">这项工作的动机是减少大语言模型中的幻觉和不可验证内容，以提高其可靠性。方法上引入了可验证奖励的强化学习（RLVR）训练范式，使用三元奖励结构来明确奖励正确答案、惩罚错误答案，并为弃权（即回答“我不知道”）提供单独奖励。主要实验结果来自在MedMCQA和Hendrycks Math基准上对Granite-3.3-2B-Instruct和Qwen-3-4B-Instruct等模型的微调，表明适度的弃权奖励能在不严重降低多项选择任务准确性的情况下持续减少错误回答，且较大模型表现出更强的鲁棒性；但注意到在开放式问答中因探索不足存在局限性，这可以通过在强化学习前进行监督式弃权训练来部分缓解。</div>
</details>
</div>
<div class="card">
<div class="title">A Reinforcement Learning Based Universal Sequence Design for Polar Codes</div>
<div class="meta-line">Authors: David Kin Wai Ho, Arman Fazeli, Mohamad M. Mansour, Louay M. A. Jalloul</div>
<div class="meta-line">First: 2026-01-27T23:20:09+00:00 · Latest: 2026-01-27T23:20:09+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, ICML2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20118v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To advance Polar code design for 6G applications, we develop a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. Crucially, our method scales to code lengths up to $2048$, making it suitable for use in standardization. Across all $(N,K)$ configurations supported in 5G, our approach achieves competitive performance relative to the NR sequence adopted in 5G and yields up to a 0.2 dB gain over the beta-expansion baseline at $N=2048$. We further highlight the key elements that enabled learning at scale: (i) incorporation of physical law constrained learning grounded in the universal partial order property of Polar codes, (ii) exploitation of the weak long term influence of decisions to limit lookahead evaluation, and (iii) joint multi-configuration optimization to increase learning efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的极化码通用序列设计</div>
<div class="mono" style="margin-top:8px">为推进6G应用中的极化码设计，我们开发了一种基于强化学习的通用序列设计框架，该框架具有可扩展性，并能适应不同信道条件与解码策略。关键突破在于，本方法可扩展至码长2048比特，适用于标准化场景。在5G支持的所有(N,K)配置下，相较于5G采用的NR序列，本方法均展现出具有竞争力的性能；在N=2048时，较β展开基线方案可获得最高0.2 dB增益。我们进一步阐明了实现大规模学习的关键要素：(1) 融入基于极化码通用偏序特性的物理定律约束学习；(2) 利用决策的弱长期影响以限制前瞻评估复杂度；(3) 采用联合多配置优化提升学习效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for advanced Polar code designs for 6G applications, this paper introduces a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. The method scales to code lengths up to 2048, making it suitable for standardization, and incorporates key elements such as physical law constrained learning, limited lookahead evaluation, and joint multi-configuration optimization to enable learning at scale. Experimentally, across all (N,K) configurations supported in 5G, the approach achieves competitive performance relative to the 5G NR sequence and yields up to a 0.2 dB gain over the beta-expansion baseline at N=2048.</div>
<div class="mono" style="margin-top:8px">为推进面向6G应用的极化码设计，本文开发了一种基于强化学习的通用序列设计框架，该框架可扩展并适应不同的信道条件和解码策略。该方法可扩展至码长2048，适用于标准化，并融合了基于极化码通用偏序特性的物理定律约束学习、有限前瞻评估和联合多配置优化等关键要素以实现大规模学习。实验结果表明，在5G支持的所有(N,K)配置下，该方法相对于5G NR序列具有竞争性性能，并在N=2048时相比beta扩展基线实现了高达0.2 dB的增益。</div>
</details>
</div>
<div class="card">
<div class="title">In-Context Reinforcement Learning From Suboptimal Historical Data</div>
<div class="meta-line">Authors: Juncheng Dong, Moyang Guo, Ethan X. Fang, Zhuoran Yang, Vahid Tarokh</div>
<div class="meta-line">First: 2026-01-27T23:13:06+00:00 · Latest: 2026-01-27T23:13:06+00:00</div>
<div class="meta-line">Comments: Accepted to Forty-Second International Conference on Machine Learning (ICML2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20116v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20116v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于次优历史数据的上下文强化学习</div>
<div class="mono" style="margin-top:8px">Transformer模型凭借其上下文学习能力取得了显著的实证成功。受此启发，我们探索训练自回归Transformer进行上下文强化学习。在该设定中，我们首先在包含多任务轨迹的离线数据集上训练Transformer，随后固定该模型并用于新任务的动作策略生成。特别地，我们考虑离线数据来自次优行为策略的情况——此时标准自回归训练等价于模仿学习，将导致次优性能。为此，我们提出决策重要性Transformer框架，以上下文方式模拟演员-评论家算法：先训练基于Transformer的价值函数来估计次优轨迹对应行为策略的优势函数，再通过加权最大似然估计训练策略网络，其权重由已训练的价值函数构建，从而引导次优策略向最优策略演进。我们在赌博机和马尔可夫决策过程问题上进行了大量实验，结果表明该框架（尤其在处理次优历史数据时）具有优越性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the success of transformers in in-context learning, this paper explores in-context reinforcement learning (ICRL) from offline datasets containing suboptimal trajectories, where standard autoregressive training leads to poor imitation. The proposed method, the Decision Importance Transformer (DIT), addresses this by first training a transformer to estimate advantage functions for the suboptimal behavior policies, then training a transformer-based policy using a weighted maximum likelihood loss guided by these estimates to shift policies toward optimality. Experimental results on bandit and Markov Decision Process problems demonstrate that DIT achieves superior performance, especially when the historical data is suboptimal.</div>
<div class="mono" style="margin-top:8px">受Transformer在上下文学习中成功的启发，本文探索了从包含次优轨迹的离线数据中进行上下文强化学习（ICRL），其中标准的自回归训练会导致次优模仿。所提出的方法，即决策重要性Transformer（DIT），通过首先训练一个Transformer来估计次优行为策略的优势函数，然后使用基于这些估计的加权最大似然损失训练一个基于Transformer的策略，从而将策略导向最优性。在赌博机和马尔可夫决策过程问题上的实验结果表明，DIT实现了卓越的性能，尤其是在历史数据为次优的情况下。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis</div>
<div class="meta-line">Authors: Darshan Deshpande, Anand Kannappan, Rebecca Qian</div>
<div class="meta-line">First: 2026-01-27T22:45:43+00:00 · Latest: 2026-01-27T22:45:43+00:00</div>
<div class="meta-line">Comments: Dataset: https://huggingface.co/datasets/PatronusAI/trace-dataset</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20103v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20103v1">PDF</a> · <a href="https://huggingface.co/datasets/PatronusAI/trace-dataset">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于对比分析的代码环境奖励攻击检测基准测试</div>
<div class="mono" style="margin-top:8px">代码生成强化学习的最新进展使得鲁棒环境对防止奖励攻击至关重要。随着大语言模型日益成为基于代码的强化学习评估器，其检测奖励攻击的能力仍未得到充分研究。本文提出了一种涵盖54个类别的奖励利用新型分类法，并引入了TRACE（代码环境奖励异常测试）——一个包含517条测试轨迹的合成整理且经人工验证的基准。与先前在孤立分类场景中评估奖励攻击检测的研究不同，我们在TRACE上将这些评估与更贴近实际的对比异常检测设置进行对比。实验表明，模型在对比设置中比在孤立分类设置中能更有效地捕捉奖励攻击，其中GPT-5.2在最高推理模式下达到63%的最佳检测率，较TRACE上孤立设置的45%显著提升。基于此发现，我们证明最先进模型在处理语义上下文奖励攻击时，相比句法上下文奖励攻击面临更大困难。我们进一步对模型行为进行定性分析，并通过消融研究表明良性轨迹与攻击轨迹的比例及分析聚类规模对检测性能有显著影响。我们公开了基准和评估工具，以支持社区扩展TRACE并评估其模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to prevent reward hacking in reinforcement learning for code generation, this paper introduces a novel taxonomy of reward exploits and the TRACE benchmark, a synthetic, human-verified dataset of 517 testing trajectories across 54 categories. The method contrasts isolated classification with a more realistic contrastive anomaly detection setup for evaluating large language models as evaluators. The main experimental results show that models, particularly GPT-5.2 in its highest reasoning mode, detect reward hacks more effectively in contrastive settings, improving from 45% to 63% detection rate on TRACE, while struggling more with semantically contextualized hacks than syntactic ones, with performance also influenced by trajectory ratios and cluster sizes.</div>
<div class="mono" style="margin-top:8px">本研究旨在防止代码生成强化学习中的奖励攻击，为此提出了一种新颖的奖励攻击分类法，并引入了TRACE基准数据集，该数据集包含54个类别、517条人工验证的合成测试轨迹。方法上，通过对比孤立分类与更贴近实际的对比异常检测设置，来评估作为评判者的大型语言模型。主要实验结果表明，模型在对比设置下能更有效地检测奖励攻击，其中GPT-5.2在最高推理模式下检测率从45%提升至63%，但模型对语义上下文攻击的检测远难于句法上下文攻击，且检测性能受良性轨迹与攻击轨迹比例及分析集群大小的显著影响。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Efficient RL under Episode-Wise Safety in Constrained MDPs with Linear Function Approximation</div>
<div class="meta-line">Authors: Toshinori Kitamura, Arnob Ghosh, Tadashi Kozuno, Wataru Kumagai, Kazumi Kasaura, Kenta Hoshino, Yohei Hosoe, Yutaka Matsuo</div>
<div class="meta-line">First: 2025-02-14T13:07:25+00:00 · Latest: 2026-01-27T22:43:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.10138v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.10138v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the reinforcement learning (RL) problem in a constrained Markov decision process (CMDP), where an agent explores the environment to maximize the expected cumulative reward while satisfying a single constraint on the expected total utility value in every episode. While this problem is well understood in the tabular setting, theoretical results for function approximation remain scarce. This paper closes the gap by proposing an RL algorithm for linear CMDPs that achieves $\tilde{\mathcal{O}}(\sqrt{K})$ regret with an episode-wise zero-violation guarantee. Furthermore, our method is computationally efficient, scaling polynomially with problem-dependent parameters while remaining independent of the state space size. Our results significantly improve upon recent linear CMDP algorithms, which either violate the constraint or incur exponential computational costs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>线性函数逼近约束MDP中基于逐幕安全性的可证明高效强化学习</div>
<div class="mono" style="margin-top:8px">本研究探讨约束马尔可夫决策过程（CMDP）中的强化学习问题，智能体需在满足每幕期望总效用值约束的前提下，通过环境探索最大化期望累积奖励。该问题在表格设定下已有充分研究，但函数逼近的理论成果仍较为匮乏。本文通过提出线性CMDP的强化学习算法填补了这一空白，该算法在实现$\tilde{\mathcal{O}}(\sqrt{K})$遗憾度的同时保证逐幕零约束违反。此外，本方法具有计算高效性，其计算复杂度随问题相关参数呈多项式增长，且与状态空间规模无关。相较于近期线性CMDP算法存在的约束违反或指数级计算成本问题，本研究成果实现了显著改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of safe reinforcement learning in constrained Markov decision processes with linear function approximation, motivated by the scarcity of theoretical guarantees for function approximation despite progress in tabular settings. The method introduces a computationally efficient algorithm for linear CMDPs that ensures episode-wise safety by maintaining a zero constraint violation guarantee. The main experimental results demonstrate that the algorithm achieves sublinear regret of order tilde{mathcal{O}}(sqrt{K}) while scaling polynomially with problem parameters and remaining independent of the state space size, significantly improving upon prior approaches that either violate constraints or incur exponential computational costs.</div>
<div class="mono" style="margin-top:8px">本文针对线性函数近似的约束马尔可夫决策过程中的安全强化学习问题展开研究，其动机在于尽管表格设定下的理论已有进展，但函数近似下的理论保证仍较为缺乏。方法提出了一种用于线性CMDP的计算高效算法，通过维持每回合零约束违反保证来确保安全性。主要实验结果表明，该算法实现了阶为tilde{mathcal{O}}(sqrt{K})的次线性遗憾，同时计算复杂度随问题参数多项式增长且与状态空间大小无关，显著改进了先前要么违反约束要么计算成本呈指数级增长的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery</div>
<div class="meta-line">Authors: Meng Xin, Sweta Priyadarshi, Jingyu Xin, Bilal Kartal, Aditya Vavre, Asma Kuriparambil Thekkumpate, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Ido Shahaf, Akhiad Bercovich, Kinjal Patel, Suguna Varshini Velury, Chenjie Luo, Zhiyu Cheng, Jenny Chen, Chen-Han Yu, Wei Ping, Oleg Rybakov, Nima Tajbakhsh, Oluwatobi Olabiyi, Dusan Stosic, Di Wu, Song Han, Eric Chung, Sharath Turuvekere Sreenivas, Bryan Catanzaro, Yoshi Suhara, Tijmen Blankevoort, Huizi Mao</div>
<div class="meta-line">First: 2026-01-27T22:14:47+00:00 · Latest: 2026-01-27T22:14:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20088v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20088v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This technical report presents quantization-aware distillation (QAD) and our best practices for recovering accuracy of NVFP4-quantized large language models (LLMs) and vision-language models (VLMs). QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss. While applying distillation to quantized models is not a new idea, we observe key advantages of QAD for today&#x27;s LLMs: 1. It shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning (SFT), reinforcement learning (RL), and model merging, where traditional quantization-aware training (QAT) suffers from engineering complexity and training instability; 2. It is robust to data quality and coverage, enabling accuracy recovery without full training data. We evaluate QAD across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1, showing consistent recovery to near-BF16 accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向NVFP4推理精度恢复的量化感知蒸馏方法</div>
<div class="mono" style="margin-top:8px">本技术报告提出量化感知蒸馏（QAD）方法及其最佳实践，用于恢复NVFP4量化大型语言模型（LLM）和视觉语言模型（VLM）的精度。QAD通过KL散度损失将全精度教师模型蒸馏至量化学生模型。尽管对量化模型应用蒸馏并非全新概念，但我们发现QAD对当前LLM具有关键优势：1. 对于通过监督微调（SFT）、强化学习（RL）和模型融合等多阶段后训练流程训练的模型，QAD表现出卓越的效能与稳定性，而传统量化感知训练（QAT）存在工程复杂性和训练不稳定的问题；2. 该方法对数据质量与覆盖范围具有鲁棒性，无需完整训练数据即可实现精度恢复。我们在包括AceReason Nemotron、Nemotron 3 Nano、Nemotron Nano V2、Nemotron Nano V2 VL（VLM）及Llama Nemotron Super v1在内的多个后训练模型上评估QAD，均能稳定恢复至接近BF16的精度水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces quantization-aware distillation (QAD) as a method to recover the inference accuracy of large language and vision-language models quantized to NVFP4 precision, addressing the engineering complexity and instability of traditional quantization-aware training in multi-stage post-training pipelines like supervised fine-tuning and reinforcement learning. The approach distills a full-precision teacher model into a quantized student using KL divergence loss, demonstrating robustness to data quality and eliminating the need for full training datasets. Experimental results on models including AceReason Nemotron and Llama Nemotron Super v1 show that QAD consistently recovers accuracy to near-BF16 levels, proving effective and stable across various post-trained architectures.</div>
<div class="mono" style="margin-top:8px">本文提出了量化感知蒸馏（QAD）方法，用于恢复NVFP4精度量化的大型语言模型和视觉语言模型的推理准确性，解决了传统量化感知训练在监督微调、强化学习等多阶段后训练流程中存在的工程复杂性和训练不稳定问题。该方法通过KL散度损失将全精度教师模型蒸馏到量化学生模型中，对数据质量和覆盖范围具有鲁棒性，无需完整训练数据集。在AceReason Nemotron和Llama Nemotron Super v1等多个后训练模型上的实验结果表明，QAD能一致地将准确性恢复到接近BF16水平，证明了其在各种后训练架构中的有效性和稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis</div>
<div class="meta-line">Authors: Paul Seurin, Dean Price</div>
<div class="meta-line">First: 2026-01-27T21:54:25+00:00 · Latest: 2026-01-27T21:54:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20079v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20079v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Heat-pipe microreactors (HPMRs) are compact and transportable nuclear power systems exhibiting inherent safety, well-suited for deployment in remote regions where access is limited and reliance on costly fossil fuels is prevalent. In prior work, we developed a design optimization framework that incorporates techno-economic considerations through surrogate modeling and reinforcement learning (RL)-based optimization, focusing solely on minimizing the levelized cost of electricity (LCOE) by using a bottom-up cost estimation approach. In this study, we extend that framework to a multi-objective optimization that uses the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm. The objectives include minimizing both the rod-integrated peaking factor ($F_{Δh}$) and LCOE -- subject to safety and operational constraints. We evaluate three cost scenarios: (1) a high-cost axial and drum reflectors, (2) a low-cost axial reflector, and (3) low-cost axial and drum reflectors. Our findings indicate that reducing the solid moderator radius, pin pitch, and drum coating angle -- all while increasing the fuel height -- effectively lowers $F_{Δh}$. Across all three scenarios, four key strategies consistently emerged for optimizing LCOE: (1) minimizing the axial reflector contribution when costly, (2) reducing control drum reliance, (3) substituting expensive tri-structural isotropic (TRISO) fuel with axial reflector material priced at the level of graphite, and (4) maximizing fuel burnup. While PEARL demonstrates promise in navigating trade-offs across diverse design scenarios, discrepancies between surrogate model predictions and full-order simulations remain. Further improvements are anticipated through constraint relaxation and surrogate development, constituting an ongoing area of investigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>热管微堆技术经济优化研究（第二部分）：多目标优化分析</div>
<div class="mono" style="margin-top:8px">热管微堆是一种紧凑、可移动的核能系统，具有固有安全性，适用于交通不便、依赖昂贵化石燃料的偏远地区。先前研究通过代理模型和强化学习优化框架，采用自下而上的成本估算方法，聚焦于最小化平准化度电成本。本研究将该框架扩展为多目标优化，采用强化学习增强帕累托包络算法，在满足安全与运行约束条件下，同步最小化棒积分峰值因子和平准化度电成本。通过评估三种成本情景（高成本轴向/鼓式反射体、低成本轴向反射体、低成本轴向/鼓式反射体），发现减小固体慢化剂半径、栅元间距和鼓涂层角度，同时增加燃料高度可有效降低峰值因子。在所有情景中，优化平准化度电成本均呈现四项关键策略：最小化高成本轴向反射体占比、降低控制鼓依赖度、用石墨价位轴向反射材料替代昂贵的三结构各向同性燃料、最大化燃料燃耗。尽管强化学习增强帕累托包络算法在多场景权衡中展现潜力，但代理模型预测与全阶模拟间仍存在偏差。通过约束松弛与代理模型改进可进一步提升性能，此为持续研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study extends a prior techno-economic optimization framework for heat-pipe microreactors (HPMRs) to perform multi-objective optimization, aiming to balance economic viability with safety by simultaneously minimizing the levelized cost of electricity (LCOE) and the rod-integrated peaking factor (FΔh). The method employs the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm, incorporating safety and operational constraints, and evaluates three cost scenarios for reflector components. Key experimental results show that reducing solid moderator radius, pin pitch, and drum coating angle while increasing fuel height effectively lowers FΔh, and across all scenarios, LCOE optimization consistently involves strategies like minimizing costly axial reflector use, reducing control drum reliance, substituting expensive TRISO fuel with graphite-priced materials, and maximizing fuel burnup. Although PEARL demonstrates potential in navigating design trade-offs, discrepancies between surrogate model predictions and full-order simulations highlight areas for future improvement through constraint relaxation and enhanced surrogate modeling.</div>
<div class="mono" style="margin-top:8px">本研究扩展了先前针对热管微型反应堆（HPMR）的技术经济优化框架，通过多目标优化来平衡经济性与安全性，旨在同时最小化平准化度电成本（LCOE）和棒积分峰值因子（FΔh）。方法上采用基于强化学习的帕累托包络增强（PEARL）算法，结合安全与运行约束，并评估了反射体部件的三种成本情景。主要实验结果表明，减小固体慢化剂半径、栅元间距和鼓涂层角度，同时增加燃料高度，能有效降低FΔh；在所有情景中，LCOE优化策略一致包括：最小化高成本轴向反射体的使用、减少控制鼓依赖、用石墨价格的轴向反射体材料替代昂贵的三结构各向同性（TRISO）燃料，以及最大化燃料燃耗。尽管PEARL在探索设计权衡方面展现出潜力，但代理模型预测与全阶模拟之间的差异仍存在，未来需通过约束松弛和代理模型改进来进一步优化。</div>
</details>
</div>
<div class="card">
<div class="title">Distributional value gradients for stochastic environments</div>
<div class="meta-line">Authors: Baptiste Debes, Tinne Tuytelaars</div>
<div class="meta-line">First: 2026-01-27T21:31:07+00:00 · Latest: 2026-01-27T21:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20071v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20071v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机环境中的分布值梯度方法</div>
<div class="mono" style="margin-top:8px">梯度正则化价值学习方法通过利用习得的转移动态与奖励模型来估计回报梯度，从而提升样本效率。然而，现有方法（如MAGE）在随机或噪声环境中表现欠佳，限制了其适用性。本研究通过将连续状态-动作空间的分布强化学习扩展至不仅建模标量状态-动作价值函数的分布，还建模其梯度分布，以解决这些局限。该方法被称为分布索伯列夫训练。受随机值梯度（SVG）启发，本方法采用通过条件变分自编码器（cVAE）实现的奖励与转移分布单步世界模型。所提框架基于样本，并采用最大切片最大均值差异（MSMMD）实例化分布贝尔曼算子。我们证明索伯列夫增强贝尔曼算子是具有唯一不动点的压缩映射，并揭示了梯度感知强化学习中压缩性所依赖的基本平滑度权衡。为验证方法，我们首先在简单随机强化学习示例问题中展示其有效性，随后在多个MuJoCo环境中进行性能基准测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing gradient-regularized value learning methods, such as MAGE, which struggle in stochastic or noisy environments, thereby improving sample efficiency and applicability. The method extends distributional reinforcement learning to continuous state-action spaces by modeling distributions over both state-action value functions and their gradients, termed Distributional Sobolev Training; it uses a one-step world model via a conditional Variational Autoencoder (cVAE) and instantiates the distributional Bellman operator with Max-sliced Maximum Mean Discrepancy (MSMMD), proving the Sobolev-augmented Bellman operator is a contraction with a unique fixed point. The main experimental results demonstrate effectiveness on a stochastic reinforcement learning toy problem and benchmark performance gains on several MuJoCo environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有梯度正则化价值学习方法（如MAGE）在随机或噪声环境中的局限性，以提高样本效率和适用性。方法通过将分布强化学习扩展到连续状态-动作空间，建模状态-动作价值函数及其梯度的分布，称为分布索伯列夫训练；它使用条件变分自编码器（cVAE）实现一步世界模型，并采用最大切片最大均值差异（MSMMD）实例化分布贝尔曼算子，证明了索伯列夫增强贝尔曼算子是具有唯一不动点的收缩映射。主要实验结果在随机强化学习玩具问题上展示了有效性，并在多个MuJoCo环境中取得了基准性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Regime-Adaptive Bayesian Optimization via Dirichlet Process Mixtures of Gaussian Processes</div>
<div class="meta-line">Authors: Yan Zhang, Xuefeng Liu, Sipeng Chen, Sascha Ranftl, Chong Liu, Shibo Li</div>
<div class="meta-line">First: 2026-01-27T20:45:50+00:00 · Latest: 2026-01-27T20:45:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20043v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard Bayesian Optimization (BO) assumes uniform smoothness across the search space an assumption violated in multi-regime problems such as molecular conformation search through distinct energy basins or drug discovery across heterogeneous molecular scaffolds. A single GP either oversmooths sharp transitions or hallucinates noise in smooth regions, yielding miscalibrated uncertainty. We propose RAMBO, a Dirichlet Process Mixture of Gaussian Processes that automatically discovers latent regimes during optimization, each modeled by an independent GP with locally-optimized hyperparameters. We derive collapsed Gibbs sampling that analytically marginalizes latent functions for efficient inference, and introduce adaptive concentration parameter scheduling for coarse-to-fine regime discovery. Our acquisition functions decompose uncertainty into intra-regime and inter-regime components. Experiments on synthetic benchmarks and real-world applications, including molecular conformer optimization, virtual screening for drug discovery, and fusion reactor design, demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于狄利克雷过程高斯过程混合的自适应机制贝叶斯优化</div>
<div class="mono" style="margin-top:8px">标准贝叶斯优化假设搜索空间具有均匀平滑性，这一假设在多机制问题中往往不成立，例如跨越不同能量势阱的分子构象搜索或异质分子骨架的药物发现。单一高斯过程要么过度平滑剧烈转变，要么在平滑区域产生噪声幻觉，导致不确定性校准失准。我们提出RAMBO方法，采用狄利克雷过程混合高斯过程，在优化过程中自动发现潜在机制，每个机制由具有局部优化超参数的独立高斯过程建模。我们推导出解析边缘化隐函数的折叠吉布斯采样以实现高效推断，并引入自适应集中参数调度实现从粗到细的机制发现。我们的采集函数将不确定性分解为机制内与机制间两个分量。在合成基准测试和实际应用（包括分子构象优化、药物发现虚拟筛选和聚变反应堆设计）中的实验表明，该方法在多机制目标上持续优于现有先进基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is that standard Bayesian Optimization (BO) assumes uniform smoothness, which fails in multi-regime problems like molecular conformation search, leading to miscalibrated uncertainty. The method proposed, RAMBO, uses a Dirichlet Process Mixture of Gaussian Processes to automatically discover latent regimes during optimization, each with a locally-optimized GP, employing collapsed Gibbs sampling for efficient inference and adaptive scheduling for regime discovery. Main experimental results on synthetic benchmarks and real-world tasks, including molecular conformer optimization and drug discovery, show consistent improvements over state-of-the-art baselines in handling multi-regime objectives.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于标准贝叶斯优化假设搜索空间均匀平滑，但在多区域问题（如分子构象搜索）中这一假设不成立，导致不确定性校准错误。所提出的RAMBO方法采用狄利克雷过程混合高斯过程，在优化过程中自动发现潜在区域，每个区域使用局部优化的高斯过程建模，并通过折叠吉布斯采样实现高效推理及自适应调度进行区域发现。在合成基准和实际应用（包括分子构象优化和药物发现）上的主要实验结果表明，该方法在处理多区域目标函数时相比现有先进基线取得了持续改进。</div>
</details>
</div>
<div class="card">
<div class="title">Lipschitz-Regularized Critics Lead to Policy Robustness Against Transition Dynamics Uncertainty</div>
<div class="meta-line">Authors: Xulin Chen, Ruipeng Liu, Zhenyu Gan, Garrett E. Katz</div>
<div class="meta-line">First: 2024-04-22T05:01:29+00:00 · Latest: 2026-01-27T20:38:15+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.13879v4">Abs</a> · <a href="https://arxiv.org/pdf/2404.13879v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncertainties in transition dynamics pose a critical challenge in reinforcement learning (RL), often resulting in performance degradation of trained policies when deployed on hardware. Many robust RL approaches follow two strategies: enforcing smoothness in actor or actor-critic modules with Lipschitz regularization, or learning robust Bellman operators. However, the first strategy does not investigate the impact of critic-only Lipschitz regularization on policy robustness, while the second lacks comprehensive validation in real-world scenarios. Building on this gap and prior work, we propose PPO-PGDLC, an algorithm based on Proximal Policy Optimization (PPO) that integrates Projected Gradient Descent (PGD) with a Lipschitz-regularized critic (LC). The PGD component calculates the adversarial state within an uncertainty set to approximate the robust Bellman operator, and the Lipschitz-regularized critic further improves the smoothness of learned policies. Experimental results on two classic control tasks and one real-world robotic locomotion task demonstrates that, compared to several baseline algorithms, PPO-PGDLC achieves better performance and predicts smoother actions under environmental perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Lipschitz正则化评论家提升策略对状态转移动态不确定性的鲁棒性</div>
<div class="mono" style="margin-top:8px">状态转移动态的不确定性是强化学习（RL）中的关键挑战，常导致训练策略在硬件部署时性能下降。现有鲁棒RL方法主要遵循两种策略：通过Lipschitz正则化强制执行器或执行器-评论家模块的平滑性，或学习鲁棒贝尔曼算子。然而，第一种策略未探究仅对评论家进行Lipschitz正则化对策略鲁棒性的影响，第二种策略缺乏实际场景的全面验证。基于此研究空白与前期工作，我们提出PPO-PGDLC算法——一种基于近端策略优化（PPO）的算法，融合投影梯度下降（PGD）与Lipschitz正则化评论家（LC）。PGD组件在不确定性集合内计算对抗状态以近似鲁棒贝尔曼算子，而Lipschitz正则化评论家进一步提升学习策略的平滑性。在两个经典控制任务和一个真实机器人运动任务上的实验表明，相较于多种基线算法，PPO-PGDLC在环境扰动下实现了更优性能并预测出更平滑的动作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of policy robustness in reinforcement learning when transition dynamics are uncertain, which often degrades real-world performance. The authors propose PPO-PGDLC, an algorithm combining Proximal Policy Optimization with a Projected Gradient Descent component to approximate a robust Bellman operator and a Lipschitz-regularized critic to enhance policy smoothness. Experiments on classic control tasks and a robotic locomotion task show that PPO-PGDLC outperforms baselines by delivering better performance and smoother actions under environmental perturbations.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中转移动态不确定性导致策略鲁棒性不足、影响实际部署性能的问题展开研究。作者提出PPO-PGDLC算法，该算法基于近端策略优化，结合投影梯度下降来近似鲁棒贝尔曼算子，并利用Lipschitz正则化的评论家提升策略平滑性。在经典控制任务和真实机器人运动任务上的实验表明，相比多种基线算法，PPO-PGDLC在环境扰动下能取得更优性能并生成更平滑的动作。</div>
</details>
</div>
<div class="card">
<div class="title">Improving and Accelerating Offline RL in Large Discrete Action Spaces with Structured Policy Initialization</div>
<div class="meta-line">Authors: Matthew Landers, Taylor W. Killian, Thomas Hartvigsen, Afsaneh Doryab</div>
<div class="meta-line">First: 2026-01-07T22:57:21+00:00 · Latest: 2026-01-27T19:55:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04441v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04441v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning in discrete combinatorial action spaces requires searching over exponentially many joint actions to simultaneously select multiple sub-actions that form coherent combinations. Existing approaches either simplify policy learning by assuming independence across sub-actions, which often yields incoherent or invalid actions, or attempt to learn action structure and control jointly, which is slow and unstable. We introduce Structured Policy Initialization (SPIN), a two-stage framework that first pre-trains an Action Structure Model (ASM) to capture the manifold of valid actions, then freezes this representation and trains lightweight policy heads for control. On challenging discrete DM Control benchmarks, SPIN improves average return by up to 39% over the state of the art while reducing time to convergence by up to 12.8$\times$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过结构化策略初始化改进并加速大规模离散动作空间中的离线强化学习</div>
<div class="mono" style="margin-top:8px">在离散组合动作空间中进行强化学习，需要搜索指数级数量的联合动作，以同时选择多个构成连贯组合的子动作。现有方法要么通过假设子动作间相互独立来简化策略学习（这常导致动作不连贯或无效），要么尝试同时学习动作结构和控制（这种方法缓慢且不稳定）。我们提出了结构化策略初始化（SPIN），这是一个两阶段框架：首先预训练动作结构模型（ASM）以捕捉有效动作的流形，随后冻结该表征并训练轻量级策略头进行控制。在具有挑战性的离散DM Control基准测试中，SPIN将平均回报较现有最优方法提升高达39%，同时将收敛时间缩短高达12.8倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge of reinforcement learning in large discrete combinatorial action spaces, where existing methods either make simplifying independence assumptions that lead to invalid actions or jointly learn structure and control in a slow, unstable manner. The proposed method, Structured Policy Initialization (SPIN), addresses this with a two-stage framework: it first pre-trains an Action Structure Model to capture the manifold of valid actions and then freezes this representation to train lightweight policy heads for control. Experimental results on discrete DM Control benchmarks show that SPIN improves average return by up to 39% over state-of-the-art methods and accelerates convergence by up to 12.8 times.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决大规模离散组合动作空间中的强化学习难题，现有方法要么采用独立性简化假设导致动作无效，要么联合学习结构与控制，过程缓慢且不稳定。所提出的方法称为结构化策略初始化（SPIN），采用两阶段框架：首先预训练一个动作结构模型以捕捉有效动作的流形，然后冻结该表示并训练轻量级策略头进行控制。在离散DM Control基准测试中，实验结果表明SPIN将平均回报提高了最多39%，并加速收敛达12.8倍。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring the holographic entropy cone via reinforcement learning</div>
<div class="meta-line">Authors: Temple He, Jaeha Lee, Hirosi Ooguri</div>
<div class="meta-line">First: 2026-01-27T19:00:01+00:00 · Latest: 2026-01-27T19:00:01+00:00</div>
<div class="meta-line">Comments: 38 pages, 10 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19979v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19979v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop a reinforcement learning algorithm to study the holographic entropy cone. Given a target entropy vector, our algorithm searches for a graph realization whose min-cut entropies match the target vector. If the target vector does not admit such a graph realization, it must lie outside the cone, in which case the algorithm finds a graph whose corresponding entropy vector most nearly approximates the target and allows us to probe the location of the facets. For the $\sf N=3$ cone, we confirm that our algorithm successfully rediscovers monogamy of mutual information beginning with a target vector outside the holographic entropy cone. We then apply the algorithm to the $\sf N=6$ cone, analyzing the 6 &quot;mystery&quot; extreme rays of the subadditivity cone from arXiv:2412.15364 that satisfy all known holographic entropy inequalities yet lacked graph realizations. We found realizations for 3 of them, proving they are genuine extreme rays of the holographic entropy cone, while providing evidence that the remaining 3 are not realizable, implying unknown holographic inequalities exist for $\sf N=6$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习探索全息熵锥</div>
<div class="mono" style="margin-top:8px">我们开发了一种强化学习算法来研究全息熵锥。给定目标熵向量，该算法搜索其最小割熵与目标向量匹配的图实现。若目标向量不存在此类图实现，则必位于锥外；此时算法将找到对应熵向量最接近目标的图，从而帮助我们探测锥面的位置。对于 $\sf N=3$ 熵锥，我们验证了算法能从锥外目标向量成功重新发现互信息的单调性。随后将算法应用于 $\sf N=6$ 熵锥，分析了 arXiv:2412.15364 中满足所有已知全息熵不等式但缺乏图实现的6个次可加性锥“神秘”极值射线。我们为其中3个找到了实现，证明它们确是全息熵锥的极值射线；同时为剩余3个不可实现的射线提供了证据，表明 $\sf N=6$ 存在未知的全息熵不等式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to systematically explore the structure of the holographic entropy cone, which characterizes the entanglement entropy patterns possible in holographic quantum gravity, this paper develops a reinforcement learning method to search for graph realizations of target entropy vectors. The algorithm either finds a matching graph, confirming the vector is inside the cone, or approximates it to probe the cone&#x27;s boundaries and facets. Key experimental results include successfully rediscovering the monogamy of mutual information for N=3 and, for N=6, finding graph realizations for 3 of 6 previously mysterious extreme rays—proving they belong to the holographic cone—while evidence suggests the other 3 are not realizable, indicating the existence of unknown holographic entropy inequalities.</div>
<div class="mono" style="margin-top:8px">本研究旨在系统探索全息熵锥的结构，该结构描述了全息量子引力中可能的纠缠熵模式，为此开发了一种强化学习算法来搜索目标熵向量的图实现。该方法要么找到匹配的图以确认向量位于锥内，要么近似逼近以探测锥的边界和面。主要实验结果包括：成功重新发现了N=3情况下的互信息单调性；对于N=6，为先前6个“神秘”极值射线中的3个找到了图实现，证明它们属于全息熵锥，而证据表明其余3个不可实现，这暗示着存在未知的全息熵不等式。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Distillation Enables Continual Learning</div>
<div class="meta-line">Authors: Idan Shenfeld, Mehul Damani, Jonas Hübotter, Pulkit Agrawal</div>
<div class="meta-line">First: 2026-01-27T18:59:08+00:00 · Latest: 2026-01-27T18:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19897v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19897v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自蒸馏实现持续学习</div>
<div class="mono" style="margin-top:8px">持续学习旨在使模型获得新技能与知识的同时不损害现有能力，这仍是基础模型面临的核心挑战。虽然在线策略强化学习可减轻遗忘，但需依赖通常难以获取的显式奖励函数。主流替代方案——基于专家示范的学习——主要采用监督微调（SFT），其本质属于离线策略。本文提出自蒸馏微调（SDFT），一种可直接从示范中实现在线策略学习的简洁方法。SDFT通过将示范条件化模型作为自身教师，利用上下文学习生成在线策略训练信号，从而在掌握新技能时保持原有能力。在技能学习与知识获取任务中，SDFT始终优于SFT，在显著降低灾难性遗忘的同时获得更高的新任务准确率。在序列学习实验中，SDFT使单一模型能随时间累积多项技能且无性能衰退，证实了在线策略蒸馏是实现示范驱动持续学习的可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of continual learning in foundation models, where acquiring new skills often leads to catastrophic forgetting of prior knowledge. The proposed method, Self-Distillation Fine-Tuning (SDFT), enables on-policy learning from demonstrations by using a demonstration-conditioned model as its own teacher to generate training signals, eliminating the need for explicit rewards. Experimental results across skill learning and knowledge acquisition tasks show that SDFT consistently outperforms supervised fine-tuning, achieving higher new-task accuracy while significantly reducing forgetting, and allows a single model to accumulate multiple skills sequentially without performance regression.</div>
<div class="mono" style="margin-top:8px">本文针对基础模型持续学习中的挑战，即学习新技能时常导致对先前知识的灾难性遗忘。提出的方法——自蒸馏微调（SDFT），通过使用演示条件模型作为自身教师来生成训练信号，实现了直接从演示中进行策略上学习，无需显式奖励函数。在技能学习和知识获取任务上的实验结果表明，SDFT持续优于监督微调，在获得更高新任务准确率的同时显著减少遗忘，并使单一模型能够顺序积累多项技能而无需性能回退。</div>
</details>
</div>
<div class="card">
<div class="title">E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning</div>
<div class="meta-line">Authors: Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu, Ziwei Wang</div>
<div class="meta-line">First: 2026-01-27T18:13:22+00:00 · Latest: 2026-01-27T18:13:22+00:00</div>
<div class="meta-line">Comments: Project page: https://e2hil.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19969v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19969v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://e2hil.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>E2HiL：基于熵引导的样本选择实现高效现实世界人机协同强化学习</div>
<div class="mono" style="margin-top:8px">人机协同指导已成为加速复杂现实世界操作任务在线强化学习收敛的有效方法。然而，现有人机协同强化学习框架常面临样本效率低下的问题，需大量人工干预才能收敛，导致人力成本高昂。为此，我们提出一种名为E2HiL的高样本效率现实世界人机协同强化学习框架，通过主动选择信息量丰富的样本减少人工干预。具体而言，策略熵的稳定降低实现了探索与利用间更优的权衡，从而提升样本效率。我们首先构建不同样本对策略熵的影响函数，通过动作概率协方差与策略软优势函数高效估计该影响，进而选择影响函数值适中的样本，同时剔除导致熵值骤降的捷径样本和影响可忽略的噪声样本。在四项现实世界操作任务上的大量实验表明，相比最先进的人机协同强化学习方法，E2HiL在减少10.1%人工干预的同时成功率提升42.1%，验证了其有效性。项目页面（含代码、视频及数学公式）详见：https://e2hil.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the low sample efficiency and high labor costs in existing human-in-the-loop reinforcement learning (HiL-RL) frameworks for real-world manipulation tasks by proposing E2HiL, an entropy-guided sample selection method. The motivation is to reduce human interventions by actively selecting informative samples, achieved through estimating the influence of samples on policy entropy via the covariance of action probabilities and soft advantages, then pruning shortcut and noisy samples to maintain moderate entropy reduction. Experimental results on four real-world tasks show that E2HiL improves success rates by 42.1% while requiring 10.1% fewer human interventions compared to state-of-the-art methods, validating its efficiency and effectiveness.</div>
<div class="mono" style="margin-top:8px">本文针对现有真实世界人机交互强化学习框架在操作任务中样本效率低、人力成本高的问题，提出了E2HiL，一种基于熵引导的样本选择方法。其动机是通过主动选择信息丰富的样本来减少人工干预，具体方法是通过动作概率和软优势的协方差估计样本对策略熵的影响，并剔除导致熵急剧下降的捷径样本和影响可忽略的噪声样本，以保持适度的熵减。在四个真实世界任务上的实验结果表明，与最先进的人机交互强化学习方法相比，E2HiL将成功率提高了42.1%，同时减少了10.1%的人工干预，验证了其高效性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</div>
<div class="meta-line">Authors: Jinyeop Song, Song Wang, Julian Shun, Yada Zhu</div>
<div class="meta-line">First: 2025-09-30T15:14:24+00:00 · Latest: 2026-01-27T17:44:43+00:00</div>
<div class="meta-line">Comments: Wrong numbers are reported for main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26383v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.26383v4">PDF</a> · <a href="https://github.com/Jinyeop3110/KG-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的高效可迁移智能体知识图谱检索增强生成</div>
<div class="mono" style="margin-top:8px">知识图谱检索增强生成（KG-RAG）将大语言模型（LLM）与结构化、可验证的知识图谱（KG）结合，以减少幻觉并展示推理过程。然而，现有KG-RAG系统常由多个LLM模块（如规划、推理、响应）构成，导致推理成本高昂且行为受限于特定知识图谱。为此，我们提出KG-R1——一种基于强化学习（RL）的智能体化KG-RAG框架。KG-R1采用单一智能体，将知识图谱作为交互环境，学习逐步检索并将检索信息融入推理与生成过程，通过端到端强化学习进行优化。在知识图谱问答（KGQA）基准测试中，本方法展现出高效性与可迁移性：使用Qwen-2.5-3B模型时，KG-R1以更少的生成标记数实现了比依赖更大基础模型或微调模型的多模块工作流更高的答案准确率。此外，KG-R1具备即插即用特性：训练后无需调整即可在新知识图谱上保持高准确率。这些特性使KG-R1成为具有实际部署潜力的KG-RAG框架。代码已开源：https://github.com/Jinyeop3110/KG-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the high inference costs and lack of transferability in existing multi-module KG-RAG systems. The method introduces KG-R1, a framework that employs a single reinforcement learning agent to interact with knowledge graphs, learning to retrieve and integrate information for reasoning and generation in an end-to-end optimized process. Main experimental results on KGQA benchmarks show that KG-R1, using a compact 3B model, achieves higher answer accuracy with fewer tokens than prior multi-module methods using larger models, and demonstrates strong transferability to new knowledge graphs without modification.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有多模块知识图谱检索增强生成系统推理成本高和缺乏可迁移性的问题。方法上提出了KG-R1框架，它采用单一的强化学习智能体与知识图谱交互，通过端到端优化学习检索并整合信息进行推理和生成。在知识图谱问答基准上的主要实验结果表明，KG-R1使用紧凑的30亿参数模型，相比之前使用更大模型的多模块方法，能以更少的生成标记数实现更高的答案准确率，并且无需修改即可在新知识图谱上保持强大的性能，展现出良好的可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals</div>
<div class="meta-line">Authors: Octavio Pappalardo</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T17:10:29+00:00 · Latest: 2026-01-27T17:10:29+00:00</div>
<div class="meta-line">Comments: To appear at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19810v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19810v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent&#x27;s post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent&#x27;s capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效探索的无监督学习：通过自设目标预训练自适应策略</div>
<div class="mono" style="margin-top:8px">无监督预训练可为强化学习智能体提供先验知识，并加速下游任务的学习。一个基于人类发展的有前景方向是研究智能体通过设定和追求自身目标进行学习。核心挑战在于如何有效生成、选择并从此类目标中学习。我们关注于广泛分布的下游任务场景，其中零样本解决所有任务不可行。当目标任务超出预训练分布范围或智能体未知任务身份时，此类场景自然出现。本研究中，我们（i）在元学习框架内优化多轮次探索与适应效率，（ii）通过动态评估智能体适应后表现来指导训练课程。我们提出ULEE方法，这是一种结合上下文学习器与对抗性目标生成策略的无监督元学习方法，使训练持续处于智能体能力前沿。在XLand-MiniGrid基准测试中，ULEE预训练展现出可泛化至新目标、环境动态及地图结构的探索与适应能力提升。所得策略实现了更优的零样本与小样本性能，并为长周期微调过程提供强初始化基础，其表现优于从头学习、DIAYN预训练及其他课程设置方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reinforcement learning agents to adapt efficiently to broad downstream task distributions where zero-shot performance is infeasible, this paper introduces ULEE, an unsupervised meta-learning method. The approach combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent&#x27;s capabilities, optimizing for efficient multi-episode exploration and adaptation. Experimental results on XLand-MiniGrid benchmarks demonstrate that ULEE pre-training yields improved exploration and adaptation abilities, generalizing to novel objectives, dynamics, and structures, and outperforming learning from scratch, DIAYN pre-training, and alternative curricula in zero-shot, few-shot, and fine-tuning performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是强化学习智能体需要高效适应广泛的下游任务分布，其中零样本性能难以实现，因此提出了无监督元学习方法ULEE。该方法结合了上下文学习器与对抗性目标生成策略，使训练保持在智能体能力前沿，从而优化多回合探索与适应效率。在XLand-MiniGrid基准测试中，实验结果表明ULEE预训练提升了探索与适应能力，能泛化至新目标、环境动态和地图结构，并在零样本、少样本及微调性能上优于从头学习、DIAYN预训练和其他课程学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Reimagining Peer Review Process Through Multi-Agent Mechanism Design</div>
<div class="meta-line">Authors: Ahmad Farooq, Kamran Iqbal</div>
<div class="meta-line">First: 2026-01-27T16:43:11+00:00 · Latest: 2026-01-27T16:43:11+00:00</div>
<div class="meta-line">Comments: To appear in the Proceedings of the 2026 IEEE/ACM 48th International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE). 4 pages, 1 figure, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as &quot;broken.&quot; This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多智能体机制设计重构同行评审流程</div>
<div class="mono" style="margin-top:8px">软件工程研究界面临系统性危机：随着投稿量激增、激励错位及评审者疲劳，同行评审正逐渐失效。社区调查显示，研究人员普遍认为该流程已“崩溃”。本立场论文指出，这些功能失调本质上是机制设计缺陷，可通过计算方案解决。我们提出将研究社区建模为随机多智能体系统，并应用多智能体强化学习来设计激励相容协议。我们概述了三项干预措施：基于信用的投稿经济体系、MARL优化的评审分配机制，以及评审一致性的混合验证方法。文中还提出了威胁模型、公平性考量及分阶段试点评估指标。这一愿景为构建可持续的同行评审体系规划了研究路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by a perceived systemic crisis in software engineering peer review, characterized by increasing submissions, misaligned incentives, and reviewer fatigue, this position paper proposes a computational solution. The method involves modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design new incentive-compatible protocols, including a credit-based submission economy and optimized reviewer assignment. The main experimental results are not empirical findings but a structured research agenda outlining threat models, equity considerations, and phased pilot metrics to guide future work toward a more sustainable review process.</div>
<div class="mono" style="margin-top:8px">本文的动机是认识到软件工程领域同行评审存在系统性危机，表现为投稿量激增、激励错配和评审疲劳。其方法是将研究社区建模为一个随机多智能体系统，并应用多智能体强化学习来设计激励相容的新协议，例如基于信用的投稿经济和优化评审分配。主要的实验成果并非实证数据，而是一个结构化的研究议程，概述了威胁模型、公平性考量及分阶段试点指标，旨在为构建更可持续的评审流程指引未来研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Activation Function Design Sustains Plasticity in Continual Learning</div>
<div class="meta-line">Authors: Lute Lillo, Nick Cheney</div>
<div class="meta-line">First: 2025-09-26T16:41:47+00:00 · Latest: 2026-01-27T16:19:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22562v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22562v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In independent, identically distributed (i.i.d.) training regimes, activation functions have been benchmarked extensively, and their differences often shrink once model size and optimization are tuned. In continual learning, however, the picture is different: beyond catastrophic forgetting, models can progressively lose the ability to adapt (referred to as loss of plasticity) and the role of the non-linearity in this failure mode remains underexplored. We show that activation choice is a primary, architecture-agnostic lever for mitigating plasticity loss. Building on a property-level analysis of negative-branch shape and saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leaky and Randomized Smooth-Leaky) and evaluate them in two complementary settings: (i) supervised class-incremental benchmarks and (ii) reinforcement learning with non-stationary MuJoCo environments designed to induce controlled distribution and dynamics shifts. We also provide a simple stress protocol and diagnostics that link the shape of the activation to the adaptation under change. The takeaway is straightforward: thoughtful activation design offers a lightweight, domain-general way to sustain plasticity in continual learning without extra capacity or task-specific tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>激活函数设计维持持续学习中的可塑性</div>
<div class="mono" style="margin-top:8px">在独立同分布训练模式下，激活函数已得到广泛基准测试，一旦调整模型规模和优化策略，其差异往往缩小。然而在持续学习中情况不同：除了灾难性遗忘，模型可能逐渐丧失适应能力（称为可塑性丧失），非线性在此失效模式中的作用尚未充分探索。我们证明激活函数选择是缓解可塑性丧失的主要且与架构无关的调控手段。基于对负分支形态和饱和行为的特性分析，我们引入两种即插即用非线性函数（平滑泄漏型与随机平滑泄漏型），并在两个互补场景中评估：1）监督式类增量基准测试；2）采用非平稳MuJoCo环境的强化学习，该环境设计用于引发受控分布与动态偏移。我们还提供简易压力测试协议和诊断方法，将激活函数形态与变化适应能力相关联。核心结论简明：经过深思熟虑的激活函数设计，无需额外容量或任务特定调优，即可为持续学习提供轻量级、跨领域的可塑性维持方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of activation functions in mitigating the loss of plasticity, a key challenge in continual learning where models progressively lose the ability to adapt to new data. Motivated by the observation that activation choice is crucial beyond standard i.i.d. settings, the authors introduce two novel drop-in nonlinearities, Smooth-Leaky and Randomized Smooth-Leaky, designed based on an analysis of negative-branch shape and saturation behavior. Experimental results across supervised class-incremental benchmarks and reinforcement learning with non-stationary MuJoCo environments demonstrate that these activations effectively sustain plasticity, offering a lightweight, architecture-agnostic solution without requiring additional model capacity or task-specific tuning.</div>
<div class="mono" style="margin-top:8px">本文研究了激活函数在缓解持续学习中可塑性丧失问题中的作用，可塑性丧失是指模型逐渐失去适应新数据能力的关键挑战。研究动机源于观察到激活函数的选择在标准独立同分布训练之外至关重要，作者基于对负分支形状和饱和行为的分析，引入了两种新颖的即插即用非线性函数：平滑泄漏型和随机平滑泄漏型。在监督式类增量基准测试以及具有非平稳MuJoCo环境的强化学习实验结果表明，这些激活函数能有效维持可塑性，提供了一种轻量级、与架构无关的解决方案，无需额外模型容量或任务特定调优。</div>
</details>
</div>
<div class="card">
<div class="title">Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning</div>
<div class="meta-line">Authors: David Bani-Harouni, Chantal Pellegrini, Ege Özsoy, Matthias Keicher, Nassir Navab</div>
<div class="meta-line">First: 2025-06-16T13:32:01+00:00 · Latest: 2026-01-27T16:05:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.13474v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.13474v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited &quot;out-of-the-box&quot; capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的假设驱动临床决策语言智能体</div>
<div class="mono" style="margin-top:8px">临床决策是一个动态、交互且循环的过程，医生需反复决定执行何种临床操作，并依据新发现的信息进行诊断与治疗。大型语言模型（LLMs）具备辅助临床医生完成这一过程的潜力，但现有LLMs在临床决策支持中的应用大多存在以下局限：要么假设所有患者信息可即时获取（这一场景不切实际），未对交互式迭代调查过程进行建模；要么仅依赖预训练模型的有限“开箱即用”能力，未进行任务特异性训练。为此，我们提出一种假设驱动且具备不确定性感知的语言智能体LA-CDM，通过循环请求并解读相关检测，逐步收敛至最终诊断。采用监督学习与强化学习结合的混合训练范式，我们围绕临床决策的三个关键维度训练LA-CDM：精准生成假设、评估假设不确定性、优化决策效率。在真实世界数据集MIMIC-CDM（涵盖四种腹部疾病及多样化临床检测）上的实验表明，显式训练临床决策过程能有效提升诊断性能与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current Large Language Model applications in clinical decision support, which either assume full immediate information access or rely solely on pre-trained models without task-specific training. The authors propose LA-CDM, a hypothesis-driven uncertainty-aware language agent that models clinical decision-making as an iterative process of requesting and interpreting tests to converge on a diagnosis. Using a hybrid training approach combining supervised and reinforcement learning with objectives for hypothesis generation, uncertainty estimation, and decision efficiency, the method is evaluated on the real-world MIMIC-CDM dataset for abdominal diseases, demonstrating improved diagnostic performance and efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型语言模型在临床决策支持中的局限性提出改进，这些模型要么假设所有患者信息立即可得，要么仅依赖预训练模型而缺乏任务特定训练。作者提出了LA-CDM，一种基于假设驱动且具有不确定性感知的语言智能体，它将临床决策建模为通过反复请求和解读相关检查以逐步达成诊断的迭代过程。该方法采用结合监督学习和强化学习的混合训练范式，以假设生成、不确定性估计和决策效率为目标，在涵盖四种腹部疾病的真实世界数据集MIMIC-CDM上进行评估，结果显示其能有效提升诊断性能和效率。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action</div>
<div class="meta-line">Authors: Gong Gao, Weidong Zhao, Xianhui Liu, Ning Jia</div>
<div class="meta-line">First: 2026-01-27T15:43:02+00:00 · Latest: 2026-01-27T15:43:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19720v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19720v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing value-based online reinforcement learning (RL) algorithms suffer from slow policy exploitation due to ineffective exploration and delayed policy updates. To address these challenges, we propose an algorithm called Instant Retrospect Action (IRA). Specifically, we propose Q-Representation Discrepancy Evolution (RDE) to facilitate Q-network representation learning, enabling discriminative representations for neighboring state-action pairs. In addition, we adopt an explicit method to policy constraints by enabling Greedy Action Guidance (GAG). This is achieved through backtracking historical actions, which effectively enhances the policy update process. Our proposed method relies on providing the learning algorithm with accurate $k$-nearest-neighbor action value estimates and learning to design a fast-adaptable policy through policy constraints. We further propose the Instant Policy Update (IPU) mechanism, which enhances policy exploitation by systematically increasing the frequency of policy updates. We further discover that the early-stage training conservatism of the IRA method can alleviate the overestimation bias problem in value-based RL. Experimental results show that IRA can significantly improve the learning efficiency and final performance of online RL algorithms on eight MuJoCo continuous control tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于即时回溯动作改进在线强化学习的策略利用效率</div>
<div class="mono" style="margin-top:8px">现有基于价值的在线强化学习算法因探索效率低下与策略更新延迟导致策略利用缓慢。为解决这些问题，本文提出即时回溯动作算法。具体而言，通过Q表示差异演化促进Q网络表示学习，使相邻状态-动作对获得区分性表示；同时采用贪婪动作引导的显式策略约束方法，通过回溯历史动作有效增强策略更新过程。该方法依赖为学习算法提供精确的k近邻动作价值估计，并通过策略约束学习设计快速适应策略。进一步提出即时策略更新机制，通过系统增加策略更新频率提升策略利用效率。研究发现该算法早期训练的保守性能缓解基于价值强化学习中的过高估计偏差问题。实验表明，在八项MuJoCo连续控制任务中，该算法能显著提升在线强化学习算法的学习效率与最终性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the slow policy exploitation in value-based online reinforcement learning, which stems from ineffective exploration and delayed policy updates. To overcome these issues, the authors propose the Instant Retrospect Action (IRA) algorithm, which incorporates Q-Representation Discrepancy Evolution (RDE) to learn discriminative representations for state-action pairs and Greedy Action Guidance (GAG) to impose policy constraints via historical action backtracking, alongside an Instant Policy Update (IPU) mechanism to increase policy update frequency. Experiments on eight MuJoCo continuous control tasks demonstrate that IRA significantly enhances learning efficiency and final performance, while its early-stage conservatism also helps mitigate overestimation bias.</div>
<div class="mono" style="margin-top:8px">本文针对基于价值的在线强化学习中策略利用缓慢的问题，该问题源于探索效率低下和策略更新延迟。为解决这些挑战，作者提出了即时回溯行动（IRA）算法，该方法通过Q表示差异演化（RDE）来学习状态-动作对的区分性表示，并采用贪婪行动指导（GAG）通过回溯历史动作实施策略约束，同时引入即时策略更新（IPU）机制以提高策略更新频率。在八个MuJoCo连续控制任务上的实验结果表明，IRA能显著提升学习效率和最终性能，且其早期训练的保守性有助于缓解价值估计过高偏差问题。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow</div>
<div class="meta-line">Authors: Yunyue Wei, Chenhui Zuo, Yanan Sui</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T15:30:10+00:00 · Latest: 2026-01-27T15:30:10+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于价值引导流的高维连续控制可扩展探索方法</div>
<div class="mono" style="margin-top:8px">在生物与机器人应用中，高维系统的控制因庞大的状态-动作空间而极具挑战，其中有效探索至关重要。强化学习中常用的探索策略大多缺乏方向性，且随动作维度增加性能急剧下降。现有方法多采用降维技术，这会限制策略表达能力并牺牲系统灵活性。本文提出Q引导流探索（Qflex），这是一种可在原生高维动作空间直接进行探索的可扩展强化学习方法。训练过程中，Qflex沿着习得价值函数诱导的概率流，从可学习的源分布中遍历动作，使探索与任务相关梯度而非各向同性噪声对齐。该方法在多种高维连续控制基准测试中显著优于代表性在线强化学习基线。Qflex还成功控制了全身人体肌肉骨骼模型执行敏捷复杂动作，在超高维场景中展现出卓越的可扩展性与样本效率。结果表明，价值引导流为大规模探索提供了原则性且实用的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of effective exploration in high-dimensional continuous control tasks, where traditional reinforcement learning methods suffer from undirected exploration and degraded performance as action dimensionality increases. The authors propose Q-guided Flow Exploration (Qflex), a method that performs exploration directly in the native high-dimensional action space by leveraging a learnable source distribution and a probability flow guided by the learned value function, thereby aligning exploration with task-relevant gradients instead of isotropic noise. Experimental results show that Qflex significantly outperforms baseline online reinforcement learning methods across various high-dimensional benchmarks and successfully controls a full-body human musculoskeletal model to execute agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings.</div>
<div class="mono" style="margin-top:8px">本文针对高维连续控制任务中探索效率低下的挑战，传统强化学习方法因探索方向性差且性能随动作维度增加而下降。作者提出了Q引导流探索（Qflex）方法，该方法通过利用可学习的源分布和由学习到的价值函数引导的概率流，直接在高维动作空间中进行探索，从而使探索与任务相关梯度对齐而非各向同性噪声。实验结果表明，Qflex在多种高维基准测试中显著优于代表性的在线强化学习基线方法，并成功控制了一个全身人体肌肉骨骼模型执行灵活复杂的运动，证明了其在极高维设置下具有卓越的可扩展性和样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion</div>
<div class="meta-line">Authors: Tianyue Jiang, Yanli Wang, Yanlin Wang, Daya Guo, Ensheng Shi, Yuchi Ma, Jiachi Chen, Zibin Zheng</div>
<div class="meta-line">First: 2026-01-27T15:23:14+00:00 · Latest: 2026-01-27T15:23:14+00:00</div>
<div class="meta-line">Comments: To appear at ASE&#x27;25</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19697v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlignCoder：面向仓库级代码补全的检索与目标意图对齐框架</div>
<div class="mono" style="margin-top:8px">仓库级代码补全对现有代码大语言模型仍是挑战，因其对仓库特定上下文和领域知识的理解有限。检索增强生成方法通过检索相关代码片段作为跨文件上下文展现出潜力，但存在两个根本问题：检索过程中查询与目标代码的错位，以及现有检索方法无法有效利用推理信息。为此，我们提出AlignCoder——一个引入查询增强机制和基于强化学习的检索器训练方法的仓库级代码补全框架。该方法通过生成多个候选补全构建增强查询，弥合初始查询与目标代码间的语义鸿沟。同时，我们采用强化学习训练AlignRetriever，使其学会利用增强查询中的推理信息实现更精准检索。我们在两个广泛使用的基准测试（CrossCodeEval和RepoEval）上对五种骨干代码大语言模型进行评估，结果显示在CrossCodeEval基准上EM分数较基线提升18.1%。实验表明该框架不仅性能优越，且在不同代码大语言模型和编程语言间具有高度泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of repository-level code completion, where existing code LLMs struggle with repository-specific context, and retrieval-augmented generation methods often misalign queries with target code or fail to use inference information effectively. To overcome this, the authors propose AlignCoder, a framework that enhances retrieval through a query mechanism generating multiple candidate completions to bridge semantic gaps and a reinforcement learning-trained retriever that leverages inference data for more accurate results. Experimental evaluations on CrossCodeEval and RepoEval benchmarks across five code LLMs show an 18.1% improvement in EM score on CrossCodeEval, demonstrating superior performance and generalizability across models and programming languages.</div>
<div class="mono" style="margin-top:8px">本文针对仓库级代码补全的挑战，现有代码大语言模型在理解仓库特定上下文方面受限，而检索增强生成方法常存在查询与目标代码不对齐或无法有效利用推理信息的问题。为此，作者提出了AlignCoder框架，通过生成多个候选补全来构建增强查询以弥合语义差距，并采用强化学习训练检索器来利用推理信息实现更精准检索。在CrossCodeEval和RepoEval基准测试中，基于五种代码大语言模型的实验结果显示，在CrossCodeEval上EM分数提升了18.1%，表明该框架具有优越性能，并在不同模型和编程语言中展现出高泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Chuanyue Yu, Kuo Zhao, Yuhan Li, Heng Chang, Mingjian Feng, Xiangzhe Jiang, Yufei Sun, Jia Li, Yuzhi Zhang, Jianxin Li, Ziwei Zhang</div>
<div class="meta-line">Venue: the Web Conference 2026</div>
<div class="meta-line">First: 2025-07-31T14:11:16+00:00 · Latest: 2026-01-27T14:24:16+00:00</div>
<div class="meta-line">Comments: Accepted by the Web Conference 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.23581v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.23581v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness in enhancing the reasoning abilities of LLMs by leveraging graph structures for knowledge representation and modeling complex real-world relationships. However, existing GraphRAG methods still face significant bottlenecks when handling complex problems that require multi-hop reasoning, as their query and retrieval phases are largely based on pre-defined heuristics and do not fully utilize the reasoning potentials of LLMs. To address this problem, we propose GraphRAG-R1, an adaptive GraphRAG framework by training LLMs with process-constrained outcome-based reinforcement learning (RL) to enhance the multi-hop reasoning ability. Our method can decompose complex problems, autonomously invoke retrieval tools to acquire necessary information, and perform effective reasoning. Specifically, we utilize a modified version of Group Relative Policy Optimization (GRPO) that supports rollout-with-thinking capability. Next, we design two process-constrained reward functions. To handle the shallow retrieval problem, we design a Progressive Retrieval Attenuation (PRA) reward to encourage essential retrievals. Then, to handle the over-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the model performance with computational costs. We further design a phase-dependent training strategy, containing three training stages corresponding to cold start and these two rewards. Lastly, our method adopts a hybrid graph-textual retrieval to improve the reasoning capacity. Extensive experimental results demonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex reasoning problems compared to state-of-the-art GraphRAG methods on both in-domain and out-of-domain datasets. Furthermore, our framework can be flexibly integrated with various existing retrieval methods, consistently delivering performance improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphRAG-R1：基于过程约束强化学习的图检索增强生成</div>
<div class="mono" style="margin-top:8px">图检索增强生成（GraphRAG）通过利用图结构进行知识表示和建模复杂现实关系，在增强大语言模型推理能力方面展现出显著效果。然而，现有GraphRAG方法在处理需要多跳推理的复杂问题时仍面临瓶颈，因其查询和检索阶段主要基于预定义启发式规则，未能充分利用大语言模型的推理潜力。为此，我们提出GraphRAG-R1——一种通过过程约束结果强化学习训练大语言模型的自适应GraphRAG框架，以增强多跳推理能力。该方法能分解复杂问题、自主调用检索工具获取必要信息并进行有效推理。具体而言，我们采用支持思维推演能力的改进型组相对策略优化算法，设计两种过程约束奖励函数：针对浅层检索问题设计渐进检索衰减奖励以激励必要检索，针对过度思考问题设计成本感知F1奖励以平衡模型性能与计算成本。进一步设计包含冷启动阶段和双奖励阶段的三阶段训练策略，并采用混合图-文本检索提升推理能力。大量实验表明，GraphRAG-R1在领域内外数据集上均优于现有最优GraphRAG方法，能显著提升大语言模型解决复杂推理问题的能力。该框架可灵活集成多种现有检索方法，持续带来性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GraphRAG-R1, a framework designed to overcome the limitations of existing Graph Retrieval-Augmented Generation methods in handling complex multi-hop reasoning tasks, which often rely on pre-defined heuristics and underutilize LLMs&#x27; reasoning potential. The method employs process-constrained reinforcement learning, specifically a modified Group Relative Policy Optimization with rollout-with-thinking, to train LLMs to decompose problems, autonomously invoke retrieval tools, and reason effectively; it incorporates two reward functions—Progressive Retrieval Attenuation to encourage essential retrievals and Cost-Aware F1 to balance performance with computational costs—alongside a phase-dependent training strategy and hybrid graph-textual retrieval. Experimental results show that GraphRAG-R1 significantly enhances LLM performance on complex reasoning problems across in-domain and out-of-domain datasets compared to state-of-the-art GraphRAG methods, while remaining flexible for integration with various retrieval approaches.</div>
<div class="mono" style="margin-top:8px">本文提出了GraphRAG-R1框架，旨在解决现有图检索增强生成方法在处理需要多跳推理的复杂问题时的瓶颈，这些方法通常依赖预定义启发式规则且未充分利用大语言模型的推理潜力。该方法采用过程约束的强化学习，具体为改进的组相对策略优化并支持带思考的推演，以训练大语言模型分解问题、自主调用检索工具并进行有效推理；它设计了两种奖励函数——渐进检索衰减奖励以鼓励必要检索，以及成本感知F1奖励以平衡性能与计算成本——并结合分阶段训练策略和混合图文本检索。实验结果表明，与先进的图检索增强生成方法相比，GraphRAG-R1在领域内和领域外数据集上显著提升了大语言模型解决复杂推理问题的能力，同时能灵活集成多种现有检索方法。</div>
</details>
</div>
<div class="card">
<div class="title">Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement Learning</div>
<div class="meta-line">Authors: Tongxi Wang, Zhuoyang Xia, Xinran Chen, Shan Liu</div>
<div class="meta-line">First: 2026-01-27T13:58:11+00:00 · Latest: 2026-01-27T13:58:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19624v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19624v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world reinforcement learning often faces environment drift, but most existing methods rely on static entropy coefficients/target entropy, causing over-exploration during stable periods and under-exploration after drift (thus slow recovery), and leaving unanswered the principled question of how exploration intensity should scale with drift magnitude. We prove that entropy scheduling under non-stationarity can be reduced to a one-dimensional, round-by-round trade-off, faster tracking of the optimal solution after drift vs. avoiding gratuitous randomness when the environment is stable, so exploration strength can be driven by measurable online drift signals. Building on this, we propose AES (Adaptive Entropy Scheduling), which adaptively adjusts the entropy coefficient/temperature online using observable drift proxies during training, requiring almost no structural changes and incurring minimal overhead. Across 4 algorithm variants, 12 tasks, and 4 drift modes, AES significantly reduces the fraction of performance degradation caused by drift and accelerates recovery after abrupt changes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>追踪漂移：面向非平稳强化学习的变分感知熵调度方法</div>
<div class="mono" style="margin-top:8px">现实世界中的强化学习常面临环境漂移问题，但现有方法大多依赖静态熵系数/目标熵，导致稳定期过度探索、漂移后探索不足（恢复缓慢），且未能从原理上回答探索强度应如何随漂移幅度调整。我们证明非平稳性下的熵调度可简化为逐轮的一维权衡：在漂移后更快追踪最优解 vs 环境稳定时避免无谓随机性，从而使探索强度可由可测量的在线漂移信号驱动。基于此，我们提出AES（自适应熵调度），在训练期间利用可观测的漂移代理指标在线自适应调整熵系数/温度，几乎无需结构调整且开销极小。在4种算法变体、12项任务和4种漂移模式的实验中，AES显著降低了漂移导致的性能退化比例，并加速了突变后的恢复速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of environment drift in real-world reinforcement learning, where static entropy coefficients lead to inefficient exploration—either excessive during stable periods or insufficient after drift, hindering recovery. The authors theoretically reduce entropy scheduling to a one-dimensional trade-off between tracking speed after drift and avoiding unnecessary randomness during stability, enabling exploration strength to be guided by online drift signals. They propose AES (Adaptive Entropy Scheduling), a method that adaptively adjusts entropy coefficients using observable drift proxies with minimal overhead. Experimental results across multiple algorithms, tasks, and drift modes show that AES significantly reduces performance degradation from drift and accelerates recovery after abrupt changes.</div>
<div class="mono" style="margin-top:8px">本文针对现实世界强化学习中环境漂移的挑战，指出静态熵系数会导致探索效率低下——在稳定期过度探索或在漂移后探索不足，从而延缓恢复。作者从理论上将熵调度简化为一个一维权衡问题，即在漂移后快速跟踪最优解与在环境稳定时避免不必要的随机性之间取得平衡，使得探索强度可由在线漂移信号驱动。他们提出了AES（自适应熵调度）方法，该方法利用可观测的漂移代理自适应地在线调整熵系数，几乎无需结构改动且开销极小。在多种算法、任务和漂移模式下的实验结果表明，AES显著降低了由漂移引起的性能下降，并加速了突变后的恢复过程。</div>
</details>
</div>
<div class="card">
<div class="title">Model-free policy gradient for discrete-time mean-field control</div>
<div class="meta-line">Authors: Matthieu Meunier, Huyên Pham, Christoph Reisinger</div>
<div class="meta-line">First: 2026-01-16T11:49:25+00:00 · Latest: 2026-01-27T13:47:58+00:00</div>
<div class="meta-line">Comments: 42 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11217v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11217v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study model-free policy learning for discrete-time mean-field control (MFC) problems with finite state space and compact action space. In contrast to the extensive literature on value-based methods for MFC, policy-based approaches remain largely unexplored due to the intrinsic dependence of transition kernels and rewards on the evolving population state distribution, which prevents the direct use of likelihood-ratio estimators of policy gradients from classical single-agent reinforcement learning. We introduce a novel perturbation scheme on the state-distribution flow and prove that the gradient of the resulting perturbed value function converges to the true policy gradient as the perturbation magnitude vanishes. This construction yields a fully model-free estimator based solely on simulated trajectories and an auxiliary estimate of the sensitivity of the state distribution. Building on this framework, we develop MF-REINFORCE, a model-free policy gradient algorithm for MFC, and establish explicit quantitative bounds on its bias and mean-squared error. Numerical experiments on representative mean-field control tasks demonstrate the effectiveness of the proposed approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离散时间平均场控制的无模型策略梯度方法</div>
<div class="mono" style="margin-top:8px">本文研究具有有限状态空间和紧致动作空间的离散时间平均场控制问题的无模型策略学习方法。与平均场控制领域大量基于价值函数的方法相比，策略梯度方法因转移核与奖励函数对演化群体状态分布的内在依赖性而鲜有探索——这种依赖性阻碍了经典单智能体强化学习中似然比策略梯度估计量的直接应用。我们提出一种针对状态分布流的新型扰动方案，并证明当扰动幅度趋近于零时，所得扰动值函数的梯度收敛至真实策略梯度。该构造产生了一个完全无模型的估计量，仅依赖于模拟轨迹和状态分布敏感度的辅助估计。基于此框架，我们开发了MF-REINFORCE算法——一种适用于平均场控制的无模型策略梯度算法，并建立了其偏差与均方误差的显式定量界。在典型平均场控制任务上的数值实验验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of model-free policy learning in discrete-time mean-field control (MFC) problems, where traditional policy gradient methods fail due to the dependence of system dynamics on the evolving population distribution. The authors propose a novel perturbation scheme on the state-distribution flow, proving that the gradient of the perturbed value function converges to the true policy gradient, which enables a fully model-free estimator using simulated trajectories and an auxiliary sensitivity estimate. They develop the MF-REINFORCE algorithm based on this framework, providing theoretical bounds on its bias and mean-squared error, and demonstrate its effectiveness through numerical experiments on representative MFC tasks.</div>
<div class="mono" style="margin-top:8px">本文针对离散时间平均场控制中的无模型策略学习问题，传统策略梯度方法因系统动态依赖于演化种群分布而失效。作者提出了一种在状态分布流上的新颖扰动方案，证明扰动值函数的梯度收敛于真实策略梯度，从而实现了仅使用模拟轨迹和辅助灵敏度估计的无模型估计器。基于此框架，他们开发了MF-REINFORCE算法，给出了其偏差和均方误差的理论界限，并通过代表性平均场控制任务的数值实验验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation</div>
<div class="meta-line">Authors: Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin</div>
<div class="meta-line">First: 2026-01-13T08:22:28+00:00 · Latest: 2026-01-27T13:47:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08323v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08323v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AtomMem：基于原子内存操作的可学习动态智能体记忆</div>
<div class="mono" style="margin-top:8px">为智能体配备记忆对于解决现实世界中的长程问题至关重要。然而，现有的大多数智能体记忆机制依赖于静态且手工设计的工作流程，这限制了其性能与泛化能力，凸显了对更灵活、基于学习的记忆框架的需求。本文提出AtomMem，将记忆管理重构为一个动态决策问题：通过将高层记忆过程解构为基本的原子CRUD（创建、读取、更新、删除）操作，将记忆工作流转化为可学习的决策过程。结合监督微调与强化学习，AtomMem能够学习一种自主的、任务对齐的策略，以编排适应特定任务需求的记忆行为。在三个长上下文基准测试中的实验结果表明，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法。对训练动态的进一步分析表明，这种基于学习的框架使智能体能够发现结构化、任务对齐的记忆管理策略，凸显了其相对于预定义流程的关键优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of static, hand-crafted memory workflows in agents for long-horizon tasks, which hinder performance and generalization. To address this, the authors propose AtomMem, a learnable memory framework that reframes memory management as a dynamic decision-making problem by decomposing it into fundamental atomic CRUD operations, making the workflow a learnable process. Through a combination of supervised fine-tuning and reinforcement learning, AtomMem learns an autonomous, task-aligned policy for memory orchestration. Experimental results on three long-context benchmarks show that the trained AtomMem-8B model consistently outperforms prior static memory methods, with further analysis revealing that the learning-based approach enables the discovery of structured, task-aligned memory management strategies.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有智能体用于长程任务的记忆机制多依赖静态、手工设计的工作流，这限制了其性能与泛化能力。为此，作者提出了AtomMem，一种可学习的记忆框架，它将记忆管理重构为一个动态决策问题，通过将高层记忆过程解构为基本的原子CRUD操作，使记忆工作流成为可学习的过程。结合监督微调和强化学习，AtomMem学习出一种自主的、与任务对齐的策略来编排记忆行为。在三个长上下文基准测试上的实验结果表明，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法，进一步分析显示，这种基于学习的方法能使智能体发现结构化的、与任务对齐的记忆管理策略，突显了其相对于预定义流程的关键优势。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Exploration via Policy Priors</div>
<div class="meta-line">Authors: Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros, Andreas Krause</div>
<div class="meta-line">First: 2026-01-27T13:45:28+00:00 · Latest: 2026-01-27T13:45:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19612v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19612v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略先验的安全探索</div>
<div class="mono" style="margin-top:8px">安全探索是强化学习智能体在受控环境（如仿真环境）之外进行在线学习与适应的关键需求。本研究通过利用次优但保守的策略（例如从离线数据或模拟器中获得）作为先验来应对这一挑战。我们提出的SOOPER方法采用概率动力学模型进行乐观探索，同时在必要时悲观地回退至保守策略先验。我们证明SOOPER能在整个学习过程中保障安全性，并通过界定其累积遗憾来确保收敛至最优策略。在关键安全强化学习基准和实际硬件上的大量实验表明，SOOPER具备可扩展性，性能优于现有最优方法，并在实践中验证了我们的理论保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to enable safe online reinforcement learning beyond simulated environments by leveraging suboptimal but conservative policies as priors. The method, named SOOPER, employs probabilistic dynamics models to explore optimistically while pessimistically reverting to the conservative policy prior when necessary to ensure safety. Experimental results on safe RL benchmarks and real-world hardware show that SOOPER is scalable, outperforms state-of-the-art methods, and validates theoretical guarantees of safety and convergence to an optimal policy.</div>
<div class="mono" style="margin-top:8px">该研究的动机是利用次优但保守的策略作为先验，使强化学习智能体能在模拟环境之外安全地进行在线学习。方法名为SOOPER，它采用概率动力学模型进行乐观探索，同时在需要时悲观地回退到保守策略先验以确保安全性。在安全强化学习基准和真实硬件上的实验结果表明，SOOPER具有可扩展性，性能优于现有先进方法，并验证了其安全性和收敛到最优策略的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation</div>
<div class="meta-line">Authors: Chongjun Xia, Yanchun Peng, Xianzhi Wang</div>
<div class="meta-line">First: 2026-01-27T13:22:30+00:00 · Latest: 2026-01-27T13:22:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19585v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19585v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive recommender systems can dynamically adapt to user feedback, but often suffer from content homogeneity and filter bubble effects due to overfitting short-term user preferences. While recent efforts aim to improve content diversity, they predominantly operate in static or one-shot settings, neglecting the long-term evolution of user interests. Reinforcement learning provides a principled framework for optimizing long-term user satisfaction by modeling sequential decision-making processes. However, its application in recommendation is hindered by sparse, long-tailed user-item interactions and limited semantic planning capabilities. In this work, we propose LLM-Enhanced Reinforcement Learning (LERL), a novel hierarchical recommendation framework that integrates the semantic planning power of LLM with the fine-grained adaptability of RL. LERL consists of a high-level LLM-based planner that selects semantically diverse content categories, and a low-level RL policy that recommends personalized items within the selected semantic space. This hierarchical design narrows the action space, enhances planning efficiency, and mitigates overexposure to redundant content. Extensive experiments on real-world datasets demonstrate that LERL significantly improves long-term user satisfaction when compared with state-of-the-art baselines. The implementation of LERL is available at https://anonymous.4open.science/r/code3-18D3/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM增强的强化学习在交互式推荐中提升长期用户满意度</div>
<div class="mono" style="margin-top:8px">交互式推荐系统能动态适应用户反馈，但常因过度拟合短期偏好而面临内容同质化和信息茧房效应。现有研究虽致力于提升内容多样性，但多基于静态或单次交互场景，忽略了用户兴趣的长期演化。强化学习通过建模序列决策过程，为优化长期用户满意度提供了理论框架，但其在推荐领域的应用受限于稀疏、长尾的用户-物品交互及语义规划能力不足。本文提出LLM增强的强化学习框架，融合LLM的语义规划能力与RL的细粒度适应性。该框架包含高层LLM规划器（选择语义多样的内容类别）和底层RL策略（在选定语义空间内推荐个性化物品），通过分层设计压缩动作空间、提升规划效率并缓解冗余内容过度曝光。基于真实数据集的实验表明，该框架显著优于现有基线方法。代码已开源：https://anonymous.4open.science/r/code3-18D3/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of interactive recommender systems that overfit short-term preferences and cause content homogeneity, this paper introduces LLM-Enhanced Reinforcement Learning (LERL) to optimize long-term user satisfaction. The method employs a hierarchical framework where a high-level LLM planner selects diverse content categories, and a low-level RL policy recommends personalized items within those categories, thereby narrowing the action space and enhancing semantic planning. Experimental results on real-world datasets show that LERL significantly outperforms state-of-the-art baselines in improving long-term user satisfaction.</div>
<div class="mono" style="margin-top:8px">针对交互式推荐系统因过度拟合短期偏好而导致内容同质化和过滤气泡的问题，本文提出LLM增强的强化学习（LERL）框架以优化长期用户满意度。该方法采用分层结构，其中高层LLM规划器选择语义多样的内容类别，而低层RL策略则在选定类别内推荐个性化项目，从而缩小动作空间并提升规划效率。在真实数据集上的实验结果表明，LERL在提升长期用户满意度方面显著优于现有先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning</div>
<div class="meta-line">Authors: Jiayu Chen, Le Xu, Wentse Chen, Jeff Schneider</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2024-10-15T03:36:43+00:00 · Latest: 2026-01-27T12:54:32+00:00</div>
<div class="meta-line">Comments: This paper is accepted in ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.11234v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.11234v4">PDF</a> · <a href="https://github.com/LucasCJYSDL/Offline-RL-Kit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) is a powerful approach for data-driven decision-making and control. Compared to model-free methods, offline model-based reinforcement learning (MBRL) explicitly learns world models from a static dataset and uses them as surrogate simulators, improving the data efficiency and enabling the learned policy to potentially generalize beyond the dataset support. However, there could be various MDPs that behave identically on the offline dataset and dealing with the uncertainty about the true MDP can be challenging. In this paper, we propose modeling offline MBRL as a Bayes Adaptive Markov Decision Process (BAMDP), which is a principled framework for addressing model uncertainty. We further propose a novel Bayes Adaptive Monte-Carlo planning algorithm capable of solving BAMDPs in continuous state and action spaces with stochastic transitions. This planning process is based on Monte Carlo Tree Search and can be integrated into offline MBRL as a policy improvement operator in policy iteration. Our &quot;RL + Search&quot; framework follows in the footsteps of superhuman AIs like AlphaZero, improving on current offline MBRL methods by incorporating more computation input. The proposed algorithm significantly outperforms state-of-the-art offline RL methods on twelve D4RL MuJoCo tasks and three challenging, stochastic tokamak control tasks. The codebase is available at: https://github.com/LucasCJYSDL/Offline-RL-Kit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯自适应蒙特卡洛树搜索的离线模型强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习是一种数据驱动的决策与控制方法。相较于无模型方法，离线模型强化学习从静态数据集中显式学习世界模型，并将其作为替代模拟器，从而提升数据效率，并使学习策略可能泛化至数据集支持范围之外。然而，离线数据集中可能存在行为一致的不同马尔可夫决策过程，处理真实MDP的不确定性具有挑战性。本文提出将离线MBRL建模为贝叶斯自适应马尔可夫决策过程，这是一个解决模型不确定性的理论框架。我们进一步提出一种新颖的贝叶斯自适应蒙特卡洛规划算法，能够求解具有随机转移的连续状态与动作空间的BAMDP。该规划过程基于蒙特卡洛树搜索，可作为策略改进算子集成到离线MBRL的策略迭代中。我们的“强化学习+搜索”框架遵循AlphaZero等超人类AI的设计思路，通过引入更多计算输入改进现有离线MBRL方法。所提算法在十二项D4RL MuJoCo任务和三项具有挑战性的随机托卡马克控制任务上显著优于当前最先进的离线RL方法。代码库已开源：https://github.com/LucasCJYSDL/Offline-RL-Kit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of model uncertainty in offline model-based reinforcement learning (MBRL), where multiple MDPs can explain the static dataset, by framing the problem as a Bayes Adaptive Markov Decision Process (BAMDP). It introduces a novel Bayes Adaptive Monte-Carlo planning algorithm that leverages Monte Carlo Tree Search to handle continuous state-action spaces and stochastic transitions, integrating this planner as a policy improvement operator within a policy iteration framework. Experimental results demonstrate that this approach significantly outperforms state-of-the-art offline RL methods on twelve D4RL MuJoCo benchmarks and three stochastic tokamak control tasks, showcasing improved data efficiency and generalization.</div>
<div class="mono" style="margin-top:8px">该论文针对离线基于模型的强化学习（MBRL）中的模型不确定性挑战，即静态数据集中可能存在多个解释数据的马尔可夫决策过程，通过将问题建模为贝叶斯自适应马尔可夫决策过程（BAMDP）来解决。研究提出了一种新颖的贝叶斯自适应蒙特卡洛规划算法，利用蒙特卡洛树搜索处理连续状态-动作空间和随机转移，并将该规划器作为策略改进算子集成到策略迭代框架中。实验结果表明，该方法在十二个D4RL MuJoCo基准任务和三个具有挑战性的随机托卡马克控制任务上显著优于当前最先进的离线强化学习方法，体现了更高的数据效率和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">daVinci-Dev: Agent-native Mid-training for Software Engineering</div>
<div class="meta-line">Authors: Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-26T12:20:18+00:00 · Latest: 2026-01-27T12:16:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18418v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18418v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model&#x27;s agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>daVinci-Dev：面向软件工程的智能体原生中期训练</div>
<div class="mono" style="margin-top:8px">近期，大语言模型（LLM）能力前沿已从单轮代码生成转向智能体化软件工程——即模型能自主导航、编辑和测试复杂代码库的范式。尽管后训练方法已成为代码智能体的主流方案，但**智能体中期训练**——在模拟真实智能体工作流程的大规模数据上进行中期训练（MT）——虽比单纯依赖昂贵的强化学习更能规模化地培养基础智能体行为，却因资源需求巨大而尚未得到充分探索。实现有效智能体中期训练的核心挑战在于静态训练数据与真实开发中动态、反馈丰富的环境之间的分布不匹配。为此，我们系统研究了智能体中期训练，建立了大规模智能体开发的数据合成原则与训练方法。我们方法的核心是**智能体原生数据**——包含两种互补轨迹的监督数据：**上下文原生轨迹**完整保留智能体经历的信息流，提供广泛覆盖与多样性；**环境原生轨迹**从可执行代码库收集，其观测源自实际工具调用与测试执行，提供深度与交互真实性。我们在`SWE-Bench Verified`上验证了模型的智能体能力。实验表明，在采用对齐基座模型与智能体框架的两种后训练设置下，我们的方法以不足半数中期训练词元（731亿）超越了先前开源软件工程中期训练方案`Kimi-Dev`。除相对优势外，我们表现最佳的320亿与720亿参数模型分别达到**56.1%**与**58.5%**的问题解决率……</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the shift in LLM capabilities towards agentic software engineering, where models autonomously handle complex repositories, and identifies a gap in scalable training methods; it proposes agentic mid-training using agent-native data, which combines contextually-native trajectories for broad coverage and environmentally-native trajectories from executable repositories for authentic interaction. The method involves synthesizing and training on this data to instill foundational agentic behaviors more efficiently than reinforcement learning. Experimental results on SWE-Bench Verified show that the approach outperforms the prior Kimi-Dev recipe using fewer tokens, with 32B and 72B models achieving resolution rates of 56.1% and 58.5%, respectively.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大语言模型能力前沿正转向智能体式软件工程，即模型自主处理复杂代码库，并发现现有训练方法在可扩展性上存在不足；为此提出了一种智能体中训练方法，采用智能体原生数据，包括保留完整信息流的上下文原生轨迹以提供广泛覆盖，以及来自可执行仓库的环境原生轨迹以确保交互真实性。该方法通过合成并训练此类数据，以比强化学习更高效的方式培养基础智能体行为。在SWE-Bench Verified上的实验结果表明，该方案在使用更少训练令牌的情况下超越了之前的Kimi-Dev方法，其32B和72B模型分别实现了56.1%和58.5%的问题解决率。</div>
</details>
</div>
<div class="card">
<div class="title">Coupled Variational Reinforcement Learning for Language Model General Reasoning</div>
<div class="meta-line">Authors: Xueru Wen, Jie Lou, Yanjiang Liu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Yaojie Lu, Debing Zhang</div>
<div class="meta-line">First: 2025-12-14T07:03:51+00:00 · Latest: 2026-01-27T10:47:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12576v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12576v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While reinforcement learning has achieved impressive progress in language model reasoning, it is constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the probabilities that LLMs generate reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>耦合变分强化学习用于语言模型通用推理</div>
<div class="mono" style="margin-top:8px">尽管强化学习在语言模型推理领域取得了显著进展，但其发展受限于对可验证奖励的需求。近期无需验证器的强化学习方法通过利用大语言模型生成参考答案的概率作为奖励信号，解决了这一局限。然而，这些方法通常仅基于问题条件采样推理轨迹。这种设计使推理轨迹采样与答案信息解耦，导致探索效率低下以及轨迹与最终答案间的不连贯性。本文提出耦合变分强化学习方法，通过混合采样策略耦合先验分布与后验分布，搭建变分推断与强化学习的桥梁。通过构建并优化融合这两种分布的复合分布，该方法在保持强思维-答案连贯性的同时实现高效探索。在数学与通用推理基准上的大量实验表明，该方法较基础模型性能提升12.4%，较当前最优的无验证器强化学习基线额外提升2.3%，为增强语言模型通用推理能力提供了理论框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of existing verifier-free reinforcement learning methods for language model reasoning, which often sample reasoning traces based only on the question, leading to inefficient exploration and poor coherence between traces and final answers. The proposed method, Coupled Variational Reinforcement Learning (CoVRL), integrates variational inference with reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy, optimizing a composite distribution to enhance exploration and maintain thought-answer coherence. Experimental results on mathematical and general reasoning benchmarks demonstrate that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over state-of-the-art verifier-free RL baselines, offering a principled framework to boost language model reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有无验证器强化学习方法在语言模型推理中的局限性，这些方法通常仅基于问题采样推理轨迹，导致探索效率低下且轨迹与最终答案间缺乏连贯性。提出的方法——耦合变分强化学习（CoVRL），通过混合采样策略耦合先验和后验分布，将变分推断与强化学习相结合，并优化复合分布以提升探索效率并保持思维-答案一致性。在数学和通用推理基准上的广泛实验表明，CoVRL相比基础模型性能提升12.4%，并在最先进的无验证器强化学习基线基础上额外提升2.3%，为增强语言模型的通用推理能力提供了一个原则性框架。</div>
</details>
</div>
<div class="card">
<div class="title">APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition</div>
<div class="meta-line">Authors: Finn Rietz, Pedro Zuidberg dos Martires, Johannes Andreas Stork</div>
<div class="meta-line">First: 2026-01-27T10:38:32+00:00 · Latest: 2026-01-27T10:38:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19452v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19452v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating demonstration data into reinforcement learning (RL) can greatly accelerate learning, but existing approaches often assume demonstrations are optimal and fully aligned with the target task. In practice, demonstrations are frequently sparse, suboptimal, or misaligned, which can degrade performance when these demonstrations are integrated into RL. We propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow (NF) priors. Instead of enforcing strict adherence to the priors, APC estimates each prior&#x27;s applicability to the target task while leveraging them for exploration. Moreover, APC either refines useful priors, or sidesteps misaligned ones when necessary to optimize downstream reward. Across diverse benchmarks, APC accelerates learning when demonstrations are aligned, remains robust under severe misalignment, and leverages suboptimal demonstrations to bootstrap exploration while avoiding performance degradation caused by overly strict adherence to suboptimal demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APC-RL：通过自适应策略组合超越数据驱动的行为先验</div>
<div class="mono" style="margin-top:8px">将演示数据融入强化学习（RL）能显著加速学习过程，但现有方法通常假设演示数据是最优且与目标任务完全对齐的。实践中，演示数据往往稀疏、次优或存在偏差，直接整合到RL中可能导致性能下降。我们提出自适应策略组合（APC）——一种分层模型，能自适应地组合多个数据驱动的标准化流（NF）先验。APC不强制严格遵循先验，而是评估每个先验对目标任务的适用性，同时利用它们进行探索。此外，APC会优化有用先验，或在必要时规避偏差先验以最大化下游奖励。在多样化基准测试中，APC在演示数据对齐时加速学习，在严重偏差下保持鲁棒性，并能利用次优演示数据引导探索，同时避免因过度遵循次优演示导致的性能下降。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of integrating imperfect demonstration data into reinforcement learning, where existing methods often assume optimal and fully aligned demonstrations, leading to performance degradation when demonstrations are sparse, suboptimal, or misaligned. The authors propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow priors by estimating each prior&#x27;s applicability to the target task, leveraging them for exploration while refining useful priors or sidesteps misaligned ones to optimize reward. Experimental results across diverse benchmarks show that APC accelerates learning with aligned demonstrations, remains robust under severe misalignment, and effectively leverages suboptimal demonstrations for exploration without performance degradation from strict adherence.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中整合不完美示范数据的问题，现有方法常假设示范数据是最优且完全对齐的，导致在示范数据稀疏、次优或错位时性能下降。作者提出自适应策略组合（APC），这是一种分层模型，通过评估每个数据驱动的归一化流先验对目标任务的适用性，自适应地组合多个先验，利用它们进行探索，同时细化有用先验或在必要时避开错位先验以优化奖励。在多样基准测试中，实验结果表明APC能在示范对齐时加速学习，在严重错位下保持鲁棒性，并能有效利用次优示范引导探索，避免因严格遵循次优示范导致的性能下降。</div>
</details>
</div>
<div class="card">
<div class="title">OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation</div>
<div class="meta-line">Authors: Giuseppe Chiari, Michele Piccoli, Davide Zoni</div>
<div class="meta-line">First: 2026-01-27T10:18:46+00:00 · Latest: 2026-01-27T10:18:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19439v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automation of analog integrated circuit (IC) design remains a longstanding challenge, primarily due to the intricate interdependencies among physical layout, parasitic effects, and circuit-level performance. These interactions impose complex constraints that are difficult to accurately capture and optimize using conventional design methodologies. Although recent advances in machine learning (ML) have shown promise in automating specific stages of the analog design flow, the development of holistic, end-to-end frameworks that integrate these stages and iteratively refine layouts using post-layout, parasitic-aware performance feedback is still in its early stages. Furthermore, progress in this direction is hindered by the limited availability of open, high-quality datasets tailored to the analog domain, restricting both the benchmarking and the generalizability of ML-based techniques. To address these limitations, we present OSIRIS, a scalable dataset generation pipeline for analog IC design. OSIRIS systematically explores the design space of analog circuits while producing comprehensive performance metrics and metadata, thereby enabling ML-driven research in electronic design automation (EDA). In addition, we release a dataset consisting of 87,100 circuit variations generated with OSIRIS, accompanied by a reinforcement learning (RL)-based baseline method that exploits OSIRIS for analog design optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OSIRIS：通过可扩展数据集生成桥接模拟电路设计与机器学习</div>
<div class="mono" style="margin-top:8px">模拟集成电路设计的自动化仍是一项长期挑战，主要源于物理版图、寄生效应与电路性能之间复杂的相互依赖关系。这些相互作用施加了传统设计方法难以精确捕获和优化的复杂约束。尽管机器学习的最新进展在自动化模拟设计流程的特定阶段展现出潜力，但整合这些阶段、并利用后版图寄生效应感知的性能反馈迭代优化版图的端到端整体框架仍处于早期发展阶段。此外，该方向的进展受限于面向模拟领域开放高质量数据集的稀缺，制约了基于机器学习技术的基准测试与泛化能力。为应对这些局限，我们提出OSIRIS——一个用于模拟集成电路设计的可扩展数据集生成流程。OSIRIS系统探索模拟电路设计空间的同时，生成全面的性能指标与元数据，从而推动电子设计自动化领域的机器学习驱动研究。此外，我们发布了包含87,100个由OSIRIS生成电路变体的数据集，并配套提供基于强化学习的基准方法，以利用OSIRIS实现模拟设计优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the longstanding challenge of automating analog integrated circuit design, which suffers from complex interdependencies between layout, parasitics, and performance that are hard to capture with conventional methods, this paper introduces OSIRIS, a scalable dataset generation pipeline for analog IC design. The method systematically explores the analog circuit design space to produce comprehensive performance metrics and metadata, facilitating machine learning research in electronic design automation. The main experimental results include the release of a dataset of 87,100 circuit variations generated using OSIRIS, along with a reinforcement learning-based baseline method that demonstrates its utility for analog design optimization.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决模拟集成电路设计自动化的长期挑战，该领域因物理版图、寄生效应与电路性能间复杂的相互依赖关系而难以用传统方法准确捕捉与优化。为此，作者提出了OSIRIS，一种可扩展的模拟IC设计数据集生成流程，该方法系统探索模拟电路设计空间，生成全面的性能指标和元数据，以支持电子设计自动化中的机器学习研究。主要实验结果包括发布了使用OSIRIS生成的87,100个电路变体数据集，并提供了一个基于强化学习的基准方法，展示了OSIRIS在模拟设计优化中的应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Who Gets Cited Most? Benchmarking Long-Context Reasoning on Scientific Articles</div>
<div class="meta-line">Authors: Miao Li, Alexander Gurung, Irina Saparina, Mirella Lapata</div>
<div class="meta-line">First: 2025-09-25T11:36:09+00:00 · Latest: 2026-01-27T10:01:24+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21028v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21028v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce SciTrek, a novel question-answering benchmark designed to evaluate long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by creating benchmark questions that require information aggregation and synthesis across multiple full-text scientific articles. The questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (i.e., titles, authors, and references). These SQL queries provide explicit, verifiable reasoning processes that enable fine-grained error analysis on model answers, and the data construction scales to contexts of up to 1M tokens with minimal supervision. Experiments on open-weight and proprietary LLMs show that SciTrek poses significant challenges as the context length increases, with supervised fine-tuning and reinforcement learning offering only limited gains. Our analysis reveals systematic shortcomings of frontier LLMs&#x27; ability to effectively perform numerical operations and accurately locate information in long contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>谁被引用最多？基于科学文献的长上下文推理能力基准测试</div>
<div class="mono" style="margin-top:8px">我们提出了SciTrek——一个新颖的问答基准测试集，旨在利用科学文献评估大语言模型（LLMs）的长上下文推理能力。当前的长上下文基准测试多聚焦于简单信息检索任务或采用人工构造的上下文。SciTrek通过创建需要跨多篇全文科学文献进行信息整合与综合的基准问题，解决了这些局限性。问题及其标准答案通过将文献元数据（即标题、作者和参考文献）构建的数据库进行SQL查询自动生成。这些SQL查询提供了明确、可验证的推理过程，支持对模型答案进行细粒度错误分析，且数据构建可扩展至百万级token的上下文规模而仅需极少人工干预。对开源与专有大语言模型的实验表明，随着上下文长度增加，SciTrek会带来显著挑战，监督微调与强化学习仅能带来有限提升。我们的分析揭示了前沿大语言模型在有效执行数值运算和准确定位长上下文信息方面存在系统性缺陷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SciTrek, a new benchmark designed to address the limitations of existing long-context evaluations, which often rely on simple retrieval or artificial contexts. The method automatically generates questions and answers by formulating them as SQL queries over a database of scientific article metadata, enabling scalable creation of tasks requiring information synthesis across articles and providing verifiable reasoning chains for analysis. Experimental results on various LLMs demonstrate that SciTrek presents significant challenges as context length grows, with fine-tuning offering limited improvement, and reveals systematic model weaknesses in numerical reasoning and information location within long documents.</div>
<div class="mono" style="margin-top:8px">本文提出了SciTrek这一新基准，旨在解决现有长上下文评测多关注简单检索或使用人工语境的局限。其方法通过将问题构建为对科学文献元数据数据库的SQL查询，自动生成需要跨文章信息整合的问题与答案，实现了任务的可扩展创建，并提供了可验证的推理链以进行分析。在不同大语言模型上的实验结果表明，随着上下文长度增加，SciTrek带来了显著挑战，微调提升有限，并揭示了前沿模型在长文档中进行数值运算和准确定位信息的系统性不足。</div>
</details>
</div>
<div class="card">
<div class="title">Task-Centric Policy Optimization from Misaligned Motion Priors</div>
<div class="meta-line">Authors: Ziang Zheng, Kai Feng, Yi Nie, Shentao Qin</div>
<div class="meta-line">First: 2026-01-27T09:46:34+00:00 · Latest: 2026-01-27T09:46:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19411v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19411v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing naïve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于错位运动先验的任务中心策略优化</div>
<div class="mono" style="margin-top:8px">人形机器人控制常利用人类演示的运动先验来促进行为自然性。然而，由于本体差异、重定向误差和任务无关变异，此类演示常存在次优性或与机器人任务错位，导致简单模仿会降低任务性能。反之，纯任务强化学习虽能产生多种任务最优解，却常导致不自然或不稳定运动。这揭示了对抗模仿学习中线性奖励混合的根本局限。我们提出\emph{任务中心运动先验}（TCMP），这是一个任务优先的对抗模仿框架，将模仿视为条件正则化器而非对等目标。TCMP在最大化任务改进的同时，仅当模仿信号与任务进展兼容时才纳入，产生自适应、几何感知的更新机制，既能保持任务可行下降方向，又能在错位时抑制有害模仿。我们提供了梯度冲突与任务优先稳定点的理论分析，并通过人形控制实验验证了所提方法能在噪声演示下实现稳健任务性能与一致运动风格。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of using human motion priors for humanoid control, where direct imitation can degrade task performance due to misalignment from embodiment differences or suboptimal demonstrations, while task-only reinforcement learning often yields unnatural motions. The authors propose Task-Centric Motion Priors (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer, adaptively incorporating imitation signals only when they align with task progress to preserve task-feasible optimization and suppress harmful imitation. Experimental results on humanoid control show that TCMP achieves robust task performance while maintaining consistent motion style even with noisy demonstrations, supported by theoretical analysis of gradient conflicts and stationary points.</div>
<div class="mono" style="margin-top:8px">本文针对人形机器人控制中使用人类运动先验的挑战，指出直接模仿会因本体差异或次优演示导致任务性能下降，而仅依赖任务的强化学习常产生不自然运动。作者提出任务中心运动先验（TCMP），这是一个任务优先的对抗模仿框架，将模仿视为条件正则化器，仅在模仿信号与任务进展兼容时自适应地纳入，以保持任务可行的优化并抑制有害模仿。在人形控制实验中，TCMP在噪声演示下实现了鲁棒的任务性能并保持了一致的运动风格，辅以对梯度冲突和驻点的理论分析。</div>
</details>
</div>
<div class="card">
<div class="title">SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling</div>
<div class="meta-line">Authors: Loris Gaven, Clement Romac, Thomas Carta, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer</div>
<div class="meta-line">First: 2024-10-16T11:59:27+00:00 · Latest: 2026-01-27T09:14:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.12481v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.12481v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The past years have seen Large Language Models (LLMs) strive not only as generative models but also as agents solving textual sequential decision-making tasks. When facing complex environments where their zero-shot abilities are insufficient, recent work showed online Reinforcement Learning (RL) could be used for the LLM agent to discover and learn efficient strategies interactively. However, most prior work sticks to on-policy algorithms, which greatly reduces the scope of methods such agents could use for both exploration and exploitation, such as experience replay and hindsight relabeling. Yet, such methods may be key for LLM learning agents, and in particular when designing autonomous intrinsically motivated agents sampling and pursuing their own goals (i.e. autotelic agents). This paper presents and studies an adaptation of Soft Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves the path towards autotelic LLM agents that learn online but can also outperform on-policy methods in more classic multi-goal RL environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAC-GLAM：基于软演员-评论家与后见之明重标注的大语言模型智能体在线强化学习改进方法</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型不仅作为生成模型发展，更成为解决文本序列决策任务的智能体。面对零样本能力不足的复杂环境，近期研究表明可通过在线强化学习使大语言模型智能体以交互方式探索并习得高效策略。然而，现有研究多局限于同策略算法，这限制了智能体在探索与利用时可采用的方法范围（如经验回放与后见之明重标注）。此类方法对大语言模型学习智能体至关重要，特别是在设计能自主采样并追求目标的具内在动机智能体（即自趋智能体）时。本文提出并研究了适用于大语言模型智能体的软演员-评论家与后见之明重标注适配方案。该方法不仅为在线学习的自趋大语言模型智能体开辟了道路，在经典多目标强化学习环境中也展现出超越同策略方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to enhance online reinforcement learning for large language model agents, moving beyond the limitations of on-policy algorithms to incorporate advanced techniques like experience replay and hindsight relabeling, which are crucial for developing autonomous, intrinsically motivated agents. The method introduces an adaptation of Soft Actor-Critic combined with hindsight relabeling specifically tailored for LLM agents, enabling more efficient exploration and exploitation. Experimental results demonstrate that this approach not only facilitates progress toward autotelic LLM agents but also outperforms traditional on-policy methods in standard multi-goal reinforcement learning environments.</div>
<div class="mono" style="margin-top:8px">本文的动机是改进大型语言模型代理的在线强化学习，超越传统策略算法的限制，引入经验回放和事后重标记等关键技术，这对于开发自主、内在动机的代理至关重要。方法上，研究提出了针对LLM代理的软演员-评论家算法与事后重标记的适配方案，以提升探索和利用的效率。实验结果表明，该方法不仅推动了自目标LLM代理的发展，还在经典多目标强化学习环境中超越了传统策略算法的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Behaviors of Evolutionary Algorithms on GPUs: When Does Parallelism Pay Off?</div>
<div class="meta-line">Authors: Xinmeng Yu, Tao Jiang, Ran Cheng, Yaochu Jin, Kay Chen Tan</div>
<div class="meta-line">First: 2026-01-26T12:55:21+00:00 · Latest: 2026-01-27T09:01:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18446v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18446v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evolutionary algorithms (EAs) are increasingly implemented on graphics processing units (GPUs) to leverage parallel processing capabilities for enhanced efficiency. However, existing studies largely emphasize the raw speedup obtained by porting individual algorithms from CPUs to GPUs. Consequently, these studies offer limited insight into when and why GPU parallelism fundamentally benefits EAs. To address this gap, we investigate how GPU parallelism alters the behavior of EAs beyond simple acceleration metrics. We conduct a systematic empirical study of 16 representative EAs on 30 benchmark problems. Specifically, we compare CPU and GPU executions across a wide range of problem dimensionalities and population sizes. Our results reveal that the impact of GPU acceleration is highly heterogeneous and depends strongly on algorithmic structure. We further demonstrate that conventional fixed-budget evaluation based on the number of function evaluations (FEs) is inadequate for GPU execution. In contrast, fixed-time evaluation uncovers performance characteristics that are unobservable under small or practically constrained FE budgets, particularly for adaptive and exploration-oriented algorithms. Moreover, we identify distinct scaling regimes in which GPU parallelism is beneficial, saturates, or degrades as problem dimensionality and population size increase. Crucially, we show that large populations enabled by GPUs not only improve hardware utilization but also reveal algorithm-specific convergence and diversity dynamics that are difficult to observe under CPU-constrained settings. Consequently, our findings indicate that GPU parallelism is not strictly an implementation detail, but a pivotal factor that influences how EAs should be evaluated, compared, and designed for modern computing platforms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化算法在GPU上的扩展行为：并行化何时产生效益？</div>
<div class="mono" style="margin-top:8px">进化算法（EAs）越来越多地在图形处理器（GPU）上实现，以利用并行处理能力提升效率。然而，现有研究主要关注将单个算法从CPU移植到GPU所获得的原始加速比，因此对GPU并行化何时及为何从根本上使EAs受益的见解有限。为填补这一空白，我们研究了GPU并行化如何改变EAs的行为，超越简单的加速指标。我们对30个基准问题上的16种代表性EAs进行了系统性实证研究，特别比较了CPU和GPU在不同问题维度和种群规模下的执行情况。结果表明，GPU加速的影响高度异质，且强烈依赖于算法结构。我们进一步证明，基于函数评估次数（FEs）的传统固定预算评估不适用于GPU执行；相比之下，固定时间评估揭示了在小规模或实际受限的FE预算下无法观察到的性能特征，尤其对于自适应和探索导向的算法。此外，我们识别了不同的扩展机制：随着问题维度和种群规模增加，GPU并行化可能带来效益、趋于饱和或性能下降。关键的是，我们发现GPU支持的大规模种群不仅提高了硬件利用率，还揭示了在CPU受限设置下难以观察到的算法特定收敛与多样性动态。因此，我们的研究结果表明，GPU并行化并非严格的实现细节，而是影响EAs在现代计算平台上如何评估、比较和设计的关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the nuanced impact of GPU parallelism on evolutionary algorithms (EAs), motivated by the observation that prior research focuses narrowly on raw speedup without explaining when and why GPUs fundamentally benefit EAs. The method involves a systematic empirical comparison of 16 EAs on 30 benchmark problems, analyzing CPU versus GPU executions across varying problem dimensions and population sizes. Key experimental results reveal that GPU acceleration effects are highly heterogeneous and algorithm-dependent, demonstrate that fixed-time evaluation uncovers performance traits missed by conventional fixed-budget approaches, and identify distinct scaling regimes where GPU benefits saturate or degrade, showing that large GPU-enabled populations alter convergence and diversity dynamics in ways not observable under CPU constraints.</div>
<div class="mono" style="margin-top:8px">本研究探讨了GPU并行性对进化算法的复杂影响，其动机在于现有研究多局限于原始加速比，未能解释GPU何时及为何从根本上使进化算法受益。方法上，对16种进化算法在30个基准问题上进行了系统性的实证比较，分析了不同问题维度和种群规模下CPU与GPU执行的差异。主要实验结果表明，GPU加速效果高度异质且依赖于算法结构，证明了固定时间评估能揭示传统固定评估次数方法所忽略的性能特征，并识别出GPU效益饱和或下降的不同缩放区间，显示出GPU支持的大规模种群会改变收敛性和多样性动态，这些在CPU受限条件下难以观察到。</div>
</details>
</div>
<div class="card">
<div class="title">CHEHAB RL: Learning to Optimize Fully Homomorphic Encryption Computations</div>
<div class="meta-line">Authors: Bilel Sefsaf, Abderraouf Dandani, Abdessamed Seddiki, Arab Mohammed, Eduardo Chielle, Michail Maniatakos, Riyadh Baghdadi</div>
<div class="meta-line">First: 2026-01-27T08:49:09+00:00 · Latest: 2026-01-27T08:49:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19367v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19367v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fully Homomorphic Encryption (FHE) enables computations directly on encrypted data, but its high computational cost remains a significant barrier. Writing efficient FHE code is a complex task requiring cryptographic expertise, and finding the optimal sequence of program transformations is often intractable. In this paper, we propose CHEHAB RL, a novel framework that leverages deep reinforcement learning (RL) to automate FHE code optimization. Instead of relying on predefined heuristics or combinatorial search, our method trains an RL agent to learn an effective policy for applying a sequence of rewriting rules to automatically vectorize scalar FHE code while reducing instruction latency and noise growth. The proposed approach supports the optimization of both structured and unstructured code. To train the agent, we synthesize a diverse dataset of computations using a large language model (LLM). We integrate our proposed approach into the CHEHAB FHE compiler and evaluate it on a suite of benchmarks, comparing its performance against Coyote, a state-of-the-art vectorizing FHE compiler. The results show that our approach generates code that is $5.3\times$ faster in execution, accumulates $2.54\times$ less noise, while the compilation process itself is $27.9\times$ faster than Coyote (geometric means).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHEHAB RL：学习优化全同态加密计算</div>
<div class="mono" style="margin-top:8px">全同态加密（FHE）支持直接在加密数据上进行计算，但其高昂的计算成本仍是主要障碍。编写高效的FHE代码是一项需要密码学专业知识的复杂任务，寻找最优的程序转换序列通常难以实现。本文提出CHEHAB RL，一种利用深度强化学习（RL）自动化FHE代码优化的新型框架。该方法不依赖预定义启发式规则或组合搜索，而是训练RL智能体学习有效策略，通过应用一系列重写规则自动向量化标量FHE代码，同时降低指令延迟和噪声增长。所提方法支持结构化和非结构化代码的优化。为训练智能体，我们使用大语言模型（LLM）合成了多样化的计算数据集。我们将该方法集成到CHEHAB FHE编译器中，并通过基准测试套件进行评估，与当前最先进的向量化FHE编译器Coyote进行性能对比。结果显示：本方法生成的代码执行速度提升5.3倍，噪声累积减少2.54倍，且编译过程本身比Coyote快27.9倍（几何平均值）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the high computational cost and expert-driven complexity of writing efficient code for Fully Homomorphic Encryption (FHE). The method introduces CHEHAB RL, a framework that uses deep reinforcement learning to automate FHE code optimization by training an agent to apply a sequence of rewriting rules for vectorizing scalar code, thereby reducing latency and noise growth. The main experimental results, integrated into the CHEHAB compiler and evaluated against the state-of-the-art Coyote compiler, demonstrate that the approach generates code that is 5.3 times faster in execution, accumulates 2.54 times less noise, and compiles 27.9 times faster on geometric mean benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决全同态加密（FHE）计算成本高昂且高效代码编写依赖专家经验、过程复杂的问题。方法上提出了CHEHAB RL框架，该框架利用深度强化学习自动化FHE代码优化，通过训练智能体学习应用一系列重写规则以向量化标量代码，从而降低指令延迟和噪声增长。主要实验结果在CHEHAB编译器中实现，并与先进的向量化FHE编译器Coyote进行对比评估，表明该方法生成的代码执行速度提升5.3倍，噪声积累减少2.54倍，且编译过程本身快27.9倍（几何平均值）。</div>
</details>
</div>
<div class="card">
<div class="title">From Observations to Events: Event-Aware World Model for Reinforcement Learning</div>
<div class="meta-line">Authors: Zhao-Han Peng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu, You He</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T08:20:44+00:00 · Latest: 2026-01-27T08:20:44+00:00</div>
<div class="meta-line">Comments: 43 pages, accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19336v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19336v1">PDF</a> · <a href="https://github.com/MarquisDarwin/EAWM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从观察到事件：强化学习的事件感知世界模型</div>
<div class="mono" style="margin-top:8px">尽管基于模型的强化学习（MBRL）通过从原始观察中学习世界模型提高了样本效率，但现有方法难以在结构相似的场景间泛化，且易受纹理或颜色偏移等虚假变化的影响。从认知科学角度看，人类将连续感官流分割为离散事件，并依赖这些关键事件进行决策。受此原理启发，我们提出事件感知世界模型（EAWM），这是一个通用框架，通过学习事件感知表征来简化策略学习，无需人工标注。EAWM采用自动化事件生成器从原始观察中推导事件，并引入通用事件分割器（GES）识别事件边界（即事件片段的起止时间）。通过事件预测，表征空间被塑造以捕捉有意义的时空转换。此外，我们提出了看似不同的世界模型架构的统一形式化表述，并展示了方法的广泛适用性。在Atari 100K、Craftax 1M、DeepMind Control 500K和DMC-GB2 500K上的实验表明，EAWM持续将强MBRL基线的性能提升10%-45%，在各项基准测试中创造了新的最优结果。代码发布于https://github.com/MarquisDarwin/EAWM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the cognitive principle that humans segment continuous sensory streams into discrete events for decision-making, this paper proposes the Event-Aware World Model (EAWM) to address the generalization limitations of model-based reinforcement learning (MBRL) in structurally similar scenes with spurious variations. The method introduces an automated event generator and a Generic Event Segmentor (GES) to derive event-aware representations from raw observations without handcrafted labels, shaping the representation space through event prediction to capture meaningful spatio-temporal transitions. Experimental results on Atari 100K, Craftax 1M, and DeepMind Control benchmarks show that EAWM consistently improves strong MBRL baselines by 10%-45%, achieving new state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">受人类将连续感官流分割为离散事件以辅助决策的认知原理启发，本文提出事件感知世界模型（EAWM），以解决基于模型的强化学习在结构相似场景中因纹理或颜色等伪变化而泛化不足的问题。该方法通过自动化事件生成器和通用事件分割器（GES）从原始观察中学习事件感知表示，无需人工标注，并通过事件预测塑造表示空间以捕获有意义的时空转换。在Atari 100K、Craftax 1M和DeepMind Control等基准测试上的实验表明，EAWM持续将强基线模型的性能提升10%-45%，实现了新的最先进结果。</div>
</details>
</div>
<div class="card">
<div class="title">Innovator-VL: A Multimodal Large Language Model for Scientific Discovery</div>
<div class="meta-line">Authors: Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han, Junlong Ke, Cong Wang, Yicheng Fu, Jiawang Zhao, Jiangchao Yao, Xi Fang, Zhen Wang, Henxing Cai, Lin Yao, Zhifeng Gao, Yanhui Hong, Nang Yuan, Yixuan Li, Guojiang Zhao, Haoyi Tao, Nan Wang, Han Lyu, Guolin Ke, Ning Liao, Xiaoxing Wang, Kai Chen, Zhiyu Li, Feiyu Xiong, Sihan Hu, Kun Chen, Yanfeng Wang, Weinan E, Linfeng Zhang, Linfeng Zhang</div>
<div class="meta-line">First: 2026-01-27T08:12:18+00:00 · Latest: 2026-01-27T08:12:18+00:00</div>
<div class="meta-line">Comments: Innovator-VL tech report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19325v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19325v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Innovator-VL：面向科学发现的多模态大语言模型</div>
<div class="mono" style="margin-top:8px">我们推出Innovator-VL，这是一个面向科学领域的多模态大语言模型，旨在提升跨学科科学领域的理解与推理能力，同时在通用视觉任务上保持优异性能。与依赖海量领域预训练和不透明流程的趋势不同，我们的研究表明：通过原则性训练设计和透明方法论，能以显著降低的数据需求实现强大的科学智能。(i) 首先，我们提供完全透明、端到端可复现的训练流程，涵盖数据收集、清洗、预处理、监督微调、强化学习及评估，并附详细优化方案，便于学界系统性扩展。(ii) 其次，Innovator-VL展现出卓越的数据效率，仅用不足五百万精选样本（无需大规模预训练）即在多项科学任务中取得竞争力表现，证明有效推理可通过原则性数据选择而非盲目扩规模实现。(iii) 再次，该模型具备强泛化能力，在通用视觉、多模态推理及科学基准测试中均表现优异，表明科学对齐能力可融入统一模型且不损害通用性能。我们的实践表明，无需大规模数据亦可构建高效、可复现、高性能的科学多模态模型，为未来研究提供实用基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind Innovator-VL is to develop a scientific multimodal large language model that advances understanding and reasoning across diverse scientific domains while maintaining strong performance on general vision tasks, challenging the trend of relying on massive domain-specific pretraining and opaque pipelines. The method involves a fully transparent, end-to-end reproducible training pipeline covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, with detailed optimization recipes, emphasizing principled training design and data efficiency. The main experimental results show that Innovator-VL achieves competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining, demonstrating remarkable data efficiency, strong generalization on general vision, multimodal reasoning, and scientific benchmarks, and indicating that scientific alignment can be integrated without compromising general-purpose capabilities.</div>
<div class="mono" style="margin-top:8px">Innovator-VL的动机是开发一个科学多模态大语言模型，旨在提升跨多样科学领域的理解和推理能力，同时保持通用视觉任务的优异性能，挑战了依赖大规模领域特定预训练和不透明流程的趋势。方法上，它提供了一个完全透明、端到端可复现的训练流程，涵盖数据收集、清洗、预处理、监督微调、强化学习和评估，并配有详细的优化方案，强调原则性训练设计和数据效率。主要实验结果表明，Innovator-VL在未进行大规模预训练的情况下，仅使用不到五百万精选样本，就在多种科学任务上取得了有竞争力的性能，展现出显著的数据效率、在通用视觉、多模态推理和科学基准上的强大泛化能力，表明科学对齐可以融入统一模型而不损害通用能力。</div>
</details>
</div>
<div class="card">
<div class="title">Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs</div>
<div class="meta-line">Authors: Yanlin Song, Ben Liu, Víctor Gutiérrez-Basulto, Zhiwei Hu, Qianqian Xie, Min Peng, Sophia Ananiadou, Jeff Z. Pan</div>
<div class="meta-line">First: 2025-10-23T16:04:13+00:00 · Latest: 2026-01-27T07:40:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20691v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.20691v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge Graph Question Answering aims to answer natural language questions by reasoning over structured knowledge graphs. While large language models have advanced KGQA through their strong reasoning capabilities, existing methods continue to struggle to fully exploit both the rich knowledge encoded in KGs and the reasoning capabilities of LLMs, particularly in complex scenarios. They often assume complete KG coverage and lack mechanisms to judge when external information is needed, and their reasoning remains locally myopic, failing to maintain coherent multi-step planning, leading to reasoning failures even when relevant knowledge exists. We propose Graph-RFT, a novel two-stage reinforcement fine-tuning KGQA framework with a &#x27;plan-KGsearch-and-Websearch-during-think&#x27; paradigm, that enables LLMs to perform autonomous planning and adaptive retrieval scheduling across KG and web sources under incomplete knowledge conditions. Graph-RFT introduces a chain-of-thought fine-tuning method with a customized plan-retrieval dataset activates structured reasoning and resolves the GRPO cold-start problem. It then introduces a novel plan-retrieval guided reinforcement learning process integrates explicit planning and retrieval actions with a multi-reward design, enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired planning module to decompose complex questions into ordered subquestions, and logical expression to guide tool invocation for globally consistent multi-step reasoning. This reasoning retrieval process is optimized with a multi-reward combining outcome and retrieval specific signals, enabling the model to learn when and how to combine KG and web retrieval effectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>先规划后检索：基于强化学习的知识图谱复杂推理方法</div>
<div class="mono" style="margin-top:8px">知识图谱问答旨在通过对结构化知识图谱进行推理来回答自然语言问题。尽管大语言模型凭借其强大的推理能力推动了KGQA的发展，但现有方法仍难以充分利用知识图谱中编码的丰富知识和大语言模型的推理能力，尤其在复杂场景下。这些方法通常假设知识图谱覆盖完整，缺乏判断何时需要外部信息的机制，且其推理过程存在局部短视问题，无法保持连贯的多步规划，导致即使存在相关知识也会出现推理失败。我们提出了Graph-RFT，一种新颖的两阶段强化微调KGQA框架，采用&#x27;规划-知识图谱搜索与网络搜索-思考&#x27;范式，使大语言模型能在知识不完备条件下跨知识图谱和网络源进行自主规划与自适应检索调度。Graph-RFT引入了一种思维链微调方法，通过定制的规划-检索数据集激活结构化推理并解决GRPO冷启动问题。随后，该框架通过创新的规划-检索引导强化学习流程，将显式规划与检索动作结合多奖励设计，实现覆盖感知的检索调度。它采用笛卡尔启发的规划模块将复杂问题分解为有序子问题，并运用逻辑表达式指导工具调用，以实现全局一致的多步推理。该推理检索过程通过结合结果信号与检索特定信号的多重奖励进行优化，使模型能学习何时及如何有效结合知识图谱与网络检索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing Knowledge Graph Question Answering methods, which struggle to fully leverage both the structured knowledge in knowledge graphs and the reasoning capabilities of large language models, especially under incomplete knowledge conditions and in complex multi-step reasoning scenarios. The proposed method, Graph-RFT, is a two-stage reinforcement fine-tuning framework that introduces a &#x27;plan-KGsearch-and-Websearch-during-think&#x27; paradigm, employing a chain-of-thought fine-tuning stage to activate structured reasoning and a subsequent reinforcement learning stage with a multi-reward design to integrate explicit planning and adaptive retrieval scheduling across knowledge graph and web sources. The main experimental results demonstrate that Graph-RFT enables autonomous planning and coverage-aware retrieval, effectively decomposing complex questions and guiding tool invocation for globally consistent multi-step reasoning, thereby improving performance in complex KGQA tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有知识图谱问答方法的局限性，这些方法难以充分利用知识图谱中的结构化知识和大型语言模型的推理能力，特别是在知识不完整和复杂多步推理的场景下。所提出的方法Graph-RFT是一个两阶段强化微调框架，引入了“规划-知识图谱搜索-网络搜索-思考”范式，通过思维链微调阶段激活结构化推理，并利用多奖励设计的强化学习阶段整合显式规划和跨知识图谱及网络来源的自适应检索调度。主要实验结果表明，Graph-RFT能够实现自主规划和覆盖感知的检索，有效分解复杂问题并指导工具调用以进行全局一致的多步推理，从而提升了复杂知识图谱问答任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Output Feedback Stabilization of Linear Systems via Policy Gradient Methods</div>
<div class="meta-line">Authors: Ankang Zhang, Ming Chi, Xiaoling Wang, Lintao Ye</div>
<div class="meta-line">First: 2026-01-27T07:15:59+00:00 · Latest: 2026-01-27T07:15:59+00:00</div>
<div class="meta-line">Comments: 31 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19284v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19284v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stabilizing a dynamical system is a fundamental problem that serves as a cornerstone for many complex tasks in the field of control systems. The problem becomes challenging when the system model is unknown. Among the Reinforcement Learning (RL) algorithms that have been successfully applied to solve problems pertaining to unknown linear dynamical systems, the policy gradient (PG) method stands out due to its ease of implementation and can solve the problem in a model-free manner. However, most of the existing works on PG methods for unknown linear dynamical systems assume full-state feedback. In this paper, we take a step towards model-free learning for partially observable linear dynamical systems with output feedback and focus on the fundamental stabilization problem of the system. We propose an algorithmic framework that stretches the boundary of PG methods to the problem without global convergence guarantees. We show that by leveraging zeroth-order PG update based on system trajectories and its convergence to stationary points, the proposed algorithms return a stabilizing output feedback policy for discrete-time linear dynamical systems. We also explicitly characterize the sample complexity of our algorithm and verify the effectiveness of the algorithm using numerical examples.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略梯度方法的线性系统输出反馈镇定</div>
<div class="mono" style="margin-top:8px">镇定动态系统是控制领域的基础性问题，为众多复杂任务提供支撑。当系统模型未知时，该问题变得极具挑战性。在已成功应用于未知线性动态系统的强化学习算法中，策略梯度方法因其易于实现且能以无模型方式解决问题而备受关注。然而，现有针对未知线性动态系统的策略梯度研究大多假设全状态反馈。本文针对具有输出反馈的部分可观线性动态系统，向无模型学习迈进一步，聚焦于系统的基础镇定问题。我们提出一种算法框架，将策略梯度方法拓展至缺乏全局收敛保证的问题场景。通过基于系统轨迹的零阶策略梯度更新及其向驻点的收敛性，所提算法能为离散时间线性动态系统返回镇定的输出反馈策略。我们明确量化了算法的样本复杂度，并通过数值算例验证了算法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the stabilization problem for partially observable linear dynamical systems with unknown models, where existing policy gradient methods typically assume full-state feedback. The method introduces a model-free algorithmic framework that extends policy gradient techniques to output feedback scenarios, utilizing zeroth-order updates based on system trajectories to converge to stationary points. The main experimental results demonstrate that the proposed algorithm successfully returns stabilizing output feedback policies for discrete-time linear systems, with explicitly characterized sample complexity and effectiveness validated through numerical examples.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决模型未知的部分可观测线性动态系统的稳定问题，现有策略梯度方法通常假设全状态反馈。方法上提出了一个无模型的算法框架，将策略梯度技术扩展到输出反馈场景，利用基于系统轨迹的零阶更新来收敛到稳定点。主要实验结果表明，所提算法成功为离散时间线性系统返回稳定的输出反馈策略，并通过数值示例验证了其明确表征的样本复杂性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning</div>
<div class="meta-line">Authors: Kishan Panaganti, Zhenwen Liang, Wenhao Yu, Haitao Mi, Dong Yu</div>
<div class="meta-line">First: 2026-01-27T07:10:41+00:00 · Latest: 2026-01-27T07:10:41+00:00</div>
<div class="meta-line">Comments: Keywords: Large Language Models, Reasoning Models, Reinforcement Learning, Distributionally Robust Optimization, GRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19280v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19280v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.
  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model&#x27;s performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大语言模型推理的群组分布鲁棒优化驱动强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型推理能力的近期进展，日益依赖于训练后损失函数与对齐策略的优化。然而，以群组相对策略优化为代表的传统强化学习范式仍受限于静态均匀性：均匀的提示采样与每提示固定次数的推演。对于异构、重尾的推理数据，这会造成结构性低效——在已解决的模式上浪费算力，而对困难问题的长尾部分训练不足。为此，我们提出多对抗者群组分布鲁棒优化，这是一个以优化为先的框架，通过动态调整训练分布，超越了均匀推理模型。我们引入在线难度分类器，将提示动态划分为基于pass@k的难度群组，并设计了两种独立的训练后GDRO博弈：(1) 提示-GDRO，采用EMA去偏乘性权重赌博机采样器，聚焦于密集难度边界，对持续困难群组进行无频率偏见的加权提升；(2) 推演-GDRO，利用影子价格控制器在群组间重新分配推演次数，在固定平均预算下最大化困难任务的梯度方差缩减。我们为两种控制器提供了无悔保证，并对推演-GDRO进行了方差代理分析，推导出平方根最优的推演分配方案。在DAPO 14.1k数据集上使用Qwen3-Base模型的验证表明，相比GRPO基线，提示-GDRO与推演-GDRO在1.7B、4B和8B规模上的pass@8准确率平均相对提升分别达到+10.6%和+10.1%。定性分析揭示了自涌现的课程学习机制：对抗者将资源动态调配至演进的推理前沿，从而提升推理模型的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of standard RL methods like GRPO, which use uniform sampling and fixed rollouts for heterogeneous reasoning tasks, this paper proposes Multi-Adversary Group Distributionally Robust Optimization (GDRO) to dynamically adapt training. The method employs an Online Difficulty Classifier to group prompts by difficulty and introduces two GDRO games: Prompt-GDRO uses a bandit sampler to upweight hard groups, while Rollout-GDRO reallocates rollouts via a controller to maximize gradient variance reduction under a fixed compute budget, backed by theoretical guarantees. Experimental results on the DAPO dataset with Qwen3-Base models show average relative gains of +10.6% and +10.1% in pass@8 accuracy over GRPO across model scales, with qualitative analysis revealing an emergent curriculum that shifts resources to challenging tasks.</div>
<div class="mono" style="margin-top:8px">针对标准强化学习方法如GRPO在异构推理任务中因均匀采样和固定rollout导致的效率低下问题，本文提出了多对抗者组分布鲁棒优化框架，以动态调整训练分布。该方法通过在线难度分类器将提示按难度分组，并引入两个GDRO博弈：Prompt-GDRO使用带偏差修正的赌博机采样器对困难组进行加权，而Rollout-GDRO通过影子价格控制器在固定计算预算下重新分配rollout以最大化梯度方差减少，并提供了理论保证。在DAPO数据集上使用Qwen3-Base模型的实验结果表明，相比GRPO基线，在多个模型规模上pass@8准确率平均相对提升分别达+10.6%和+10.1%，定性分析显示框架能自发形成课程学习，将资源集中于演进中的推理前沿。</div>
</details>
</div>
<div class="card">
<div class="title">Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model</div>
<div class="meta-line">Authors: Qi Si, Xuyang Liu, Penglei Wang, Xin Guo, Yuan Qi, Yuan Cheng</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-27T06:04:02+00:00 · Latest: 2026-01-27T06:04:02+00:00</div>
<div class="meta-line">Comments: 20 pages (7 pages content + 2 pages references + 11 pages appendix), 11 figures, 8 tables. Source code available at https://github.com/darkflash03/SOLD Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19232v1">PDF</a> · <a href="https://github.com/darkflash03/SOLD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">RNA inverse folding, designing sequences to form specific 3D structures, is critical for therapeutics, gene regulation, and synthetic biology. Current methods, focused on sequence recovery, struggle to address structural objectives like secondary structure consistency (SS), minimum free energy (MFE), and local distance difference test (LDDT), leading to suboptimal structural accuracy. To tackle this, we propose a reinforcement learning (RL) framework integrated with a latent diffusion model (LDM). Drawing inspiration from the success of diffusion models in RNA inverse folding, which adeptly model complex sequence-structure interactions, we develop an LDM incorporating pre-trained RNA-FM embeddings from a large-scale RNA model. These embeddings capture co-evolutionary patterns, markedly improving sequence recovery accuracy. However, existing approaches, including diffusion-based methods, cannot effectively handle non-differentiable structural objectives. By contrast, RL excels in this task by using policy-driven reward optimization to navigate complex, non-gradient-based objectives, offering a significant advantage over traditional methods. In summary, we propose the Step-wise Optimization of Latent Diffusion Model (SOLD), a novel RL framework that optimizes single-step noise without sampling the full diffusion trajectory, achieving efficient refinement of multiple structural objectives. Experimental results demonstrate SOLD surpasses its LDM baseline and state-of-the-art methods across all metrics, establishing a robust framework for RNA inverse folding with profound implications for biotechnological and therapeutic applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结构的RNA设计：通过潜扩散模型的分步优化</div>
<div class="mono" style="margin-top:8px">RNA逆折叠旨在设计能形成特定三维结构的序列，对治疗学、基因调控和合成生物学至关重要。现有方法侧重于序列恢复，难以兼顾二级结构一致性（SS）、最小自由能（MFE）和局部距离差异测试（LDDT）等结构目标，导致结构准确性欠佳。为此，我们提出一种与潜扩散模型（LDM）结合的强化学习（RL）框架。受扩散模型在RNA逆折叠中成功建模复杂序列-结构相互作用的启发，我们开发了一种融合大规模RNA模型预训练RNA-FM嵌入的LDM。这些嵌入能捕捉协同进化模式，显著提升序列恢复精度。然而，现有方法（包括基于扩散的方法）难以有效处理不可微分的结构目标。相比之下，RL通过策略驱动的奖励优化处理复杂的非梯度目标，较传统方法具有显著优势。综上，我们提出潜扩散模型分步优化（SOLD）这一新型RL框架，通过优化单步噪声而非采样完整扩散轨迹，实现对多重结构目标的高效优化。实验表明，SOLD在所有指标上均超越其LDM基线和前沿方法，为RNA逆折叠建立了稳健框架，对生物技术和治疗应用具有深远意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing RNA inverse folding methods, which prioritize sequence recovery but often yield suboptimal structural accuracy regarding secondary structure consistency, minimum free energy, and local distance difference test, this paper introduces SOLD, a reinforcement learning framework integrated with a latent diffusion model. The method leverages pre-trained RNA-FM embeddings to capture co-evolutionary patterns and employs RL to optimize non-differentiable structural objectives through step-wise noise optimization without full diffusion trajectory sampling. Experimental results show that SOLD outperforms its latent diffusion model baseline and state-of-the-art methods across all structural metrics, providing a robust framework for RNA design with significant potential for therapeutic and biotechnological applications.</div>
<div class="mono" style="margin-top:8px">针对现有RNA逆折叠方法侧重于序列恢复但结构准确性不足的问题，如二级结构一致性、最小自由能和局部距离差异测试等目标难以优化，本文提出了SOLD，一种结合潜在扩散模型的强化学习框架。该方法利用预训练的RNA-FM嵌入捕捉协同进化模式，并通过强化学习以逐步优化噪声的方式处理不可微的结构目标，无需采样完整扩散轨迹。实验结果表明，SOLD在各项结构指标上均超越了其潜在扩散模型基线和现有最优方法，为RNA设计建立了一个鲁棒的框架，在生物技术和治疗应用方面具有深远意义。</div>
</details>
</div>
<div class="card">
<div class="title">Text2Grad: Reinforcement Learning from Natural Language Feedback</div>
<div class="meta-line">Authors: Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-28T13:23:49+00:00 · Latest: 2026-01-27T04:31:51+00:00</div>
<div class="meta-line">Comments: The code for our method is available at https://github.com/microsoft/Text2Grad</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22338v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.22338v2">PDF</a> · <a href="https://github.com/microsoft/Text2Grad">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model&#x27;s policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answers while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results suggest that natural-language feedback can serve not only as explanations, but also as actionable training signals for fine-grained alignment. The code for our method is available at https://github.com/microsoft/Text2Grad.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Text2Grad：基于自然语言反馈的强化学习</div>
<div class="mono" style="margin-top:8px">传统RLHF使用粗糙的标量奖励优化语言模型，掩盖了成功或失败的细粒度原因，导致学习过程缓慢且不透明。近期研究通过提示或反思用文本批评增强强化学习，提升了可解释性但未改变模型参数。我们提出Text2Grad，一种将自由形式文本反馈转化为片段级梯度的强化学习范式。给定人类（或程序化）批评后，Text2Grad将每个反馈短语与相关词元片段对齐，将这些对齐转化为可微奖励信号，并执行梯度更新以直接修正模型策略中的问题部分。这实现了精确的、基于反馈的调整而非全局微调。Text2Grad通过三个组件实现：(1) 将批评与词元片段配对的高质量反馈标注流程；(2) 预测答案片段级奖励并生成解释性批评的细粒度奖励模型；(3) 反向传播自然语言梯度的片段级策略优化器。在摘要生成、代码生成和问答任务中，Text2Grad持续超越标量奖励强化学习和纯提示基线，既提供更高的任务指标，也带来更丰富的可解释性。结果表明，自然语言反馈不仅能作为解释，还可作为细粒度对齐的可操作训练信号。方法代码发布于https://github.com/microsoft/Text2Grad。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Text2Grad, a reinforcement learning method motivated by the limitations of traditional RLHF, which relies on coarse scalar rewards that obscure fine-grained feedback and slow learning. The method converts free-form textual critiques into span-level gradients by aligning feedback phrases with specific token spans, using a pipeline for feedback annotation, a reward model to predict span-level rewards with critiques, and a policy optimizer to back-propagate these natural-language gradients. Experimental results on summarization, code generation, and question answering show that Text2Grad outperforms scalar-reward RL and prompt-only baselines in both task performance and interpretability, demonstrating that natural-language feedback can serve as actionable training signals for precise model alignment.</div>
<div class="mono" style="margin-top:8px">本文提出了Text2Grad方法，其动机在于传统RLHF使用粗略的标量奖励掩盖了细粒度反馈，导致学习缓慢且不透明。该方法通过将自由形式的文本批评转化为跨度级梯度，利用反馈标注流程将批评与特定词元跨度对齐，结合预测跨度奖励并生成解释性批评的奖励模型，以及回传自然语言梯度的策略优化器。在摘要、代码生成和问答任务上的实验结果表明，Text2Grad在任务指标和可解释性上均优于标量奖励强化学习和仅提示基线，证明自然语言反馈可作为细粒度对齐的有效训练信号。</div>
</details>
</div>
<div class="card">
<div class="title">Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation</div>
<div class="meta-line">Authors: Xiao He, Huangxuan Zhao, Guojia Wan, Wei Zhou, Yanxing Liu, Juhua Liu, Yongchao Xu, Yong Luo, Dacheng Tao, Bo Du</div>
<div class="meta-line">First: 2025-10-14T19:57:03+00:00 · Latest: 2026-01-27T04:30:00+00:00</div>
<div class="meta-line">Comments: This paper contains fundamental errors and will not be replaced</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.12953v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.12953v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hexiao0275.github.io/FetalMind">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent medical vision-language models have shown promise on tasks such as VQA, report generation, and anomaly detection. However, most are adapted to structured adult imaging and underperform in fetal ultrasound, which poses challenges of multi-view image reasoning, numerous diseases, and image diversity. To bridge this gap, we introduce FetalMind, a medical AI system tailored to fetal ultrasound for both report generation and diagnosis. Guided by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which injects an expert-curated bipartite graph into the model to decouple view-disease associations and to steer preference selection along clinically faithful steps via reinforcement learning. This design mitigates variability across diseases and heterogeneity across views, reducing learning bottlenecks while aligning the model&#x27;s inference with obstetric practice. To train FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale fetal ultrasound report corpus, comprising 20K reports from twelve medical centers, addressing the scarcity of domain data. Extensive experiments show that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable. Project Page: https://hexiao0275.github.io/FetalMind.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向胎儿超声解读的认知感知视觉-语言基础模型</div>
<div class="mono" style="margin-top:8px">近期医学视觉-语言模型在视觉问答、报告生成和异常检测等任务中展现出潜力，但多数模型适配结构化成人影像，在胎儿超声领域表现欠佳。胎儿超声面临多视图图像推理、疾病种类繁多和图像多样性等挑战。为填补这一空白，我们推出专为胎儿超声设计的医学人工智能系统FetalMind，兼具报告生成与诊断功能。在临床工作流指导下，我们提出显著认知解耦方法，通过向模型注入专家构建的二部图，解耦视图与疾病的关联，并借助强化学习引导模型沿临床可信步骤进行偏好选择。该设计缓解了疾病间的变异性和视图间的异质性，减少学习瓶颈，同时使模型推理与产科实践保持一致。为大规模训练FetalMind，我们构建了首个大规模胎儿超声报告数据集FetalSigma-1M，包含来自12个医疗中心的2万份报告，缓解了领域数据稀缺问题。大量实验表明，FetalMind在所有孕周阶段均优于开源与闭源基线模型，平均性能提升14%，关键病症准确率提高61.2%，同时保持高效、稳定和可扩展性。项目页面：https://hexiao0275.github.io/FetalMind。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing medical vision-language models, which are primarily designed for structured adult imaging and perform poorly on fetal ultrasound due to challenges like multi-view reasoning, numerous diseases, and image diversity. The method introduces FetalMind, a system tailored for fetal ultrasound interpretation, which employs Salient Epistemic Disentanglement (SED) to inject an expert-curated bipartite graph into the model, decoupling view-disease associations and guiding preference selection via reinforcement learning to align with clinical workflows. Main experimental results demonstrate that FetalMind outperforms both open- and closed-source baselines across all gestational stages, achieving an average performance gain of +14% and a +61.2% higher accuracy on critical conditions, while remaining efficient and scalable, supported by the newly curated FetalSigma-1M dataset.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有医学视觉语言模型的局限性，这些模型主要针对结构化成人影像设计，在胎儿超声领域表现不佳，原因包括多视图推理、疾病种类繁多和图像多样性等挑战。方法上提出了FetalMind系统，专为胎儿超声解读定制，采用显著性认知解耦技术，通过注入专家构建的二部图来分离视图与疾病的关联，并利用强化学习引导偏好选择以符合临床工作流程。主要实验结果表明，FetalMind在所有孕周阶段均优于开源和闭源基线模型，平均性能提升达14%，在关键病症上的准确率提高了61.2%，同时保持高效和可扩展性，并基于新构建的FetalSigma-1M数据集进行训练。</div>
</details>
</div>
<div class="card">
<div class="title">Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind</div>
<div class="meta-line">Authors: Zhitao He, Zongwei Lyu, Yi R Fung</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-22T07:36:48+00:00 · Latest: 2026-01-27T04:09:51+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026, 36 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15715v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15715v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author&#x27;s own critical analysis and response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>镣铐之舞：基于心智理论的学术反驳策略性说服</div>
<div class="mono" style="margin-top:8px">尽管人工智能已深度融入研究流程的各个阶段并取得显著进展，学术反驳仍是一个重要且尚未充分探索的挑战。这是因为反驳是在严重信息不对称下进行的复杂策略性沟通过程，而非简单的技术辩论。因此，当前方法大多仅模仿表层语言特征，难以奏效，缺失了有效说服所需的核心要素——观点采择。本文提出首个基于心智理论的学术反驳框架RebuttalAgent，通过心智理论-策略-响应的三阶段流程实现：建模审稿人心理状态、制定说服策略、生成策略驱动的回应。为训练智能体，我们构建了大规模数据集RebuttalBench，采用创新的批判-精炼方法合成数据。训练过程分为两阶段：首先通过监督微调赋予智能体基于心智理论的分析与策略规划能力，随后利用自奖励机制进行强化学习以实现可扩展的自我改进。为进行可靠高效的自动化评估，我们进一步开发了专用评估器Rebuttal-RM，基于超10万条多源反驳数据训练，其评分与人类偏好的一致性已超越GPT-4.1。大量实验表明，RebuttalAgent在自动化指标上平均超越基线模型18.3%，同时在自动化与人工评估中均优于先进的专有模型。免责声明：生成的反驳内容仅供作者启发思路与辅助草拟，不可替代作者自身的批判性分析与回应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the complex strategic communication and information asymmetry in academic rebuttal, which existing AI approaches fail to address due to a lack of perspective-taking, this paper introduces RebuttalAgent, a framework grounded in Theory of Mind (ToM). The method employs a ToM-Strategy-Response pipeline, trained on a novel synthesized dataset, RebuttalBench, via supervised fine-tuning and reinforcement learning with a self-reward mechanism, and evaluated by a specialized model, Rebuttal-RM. Experimental results demonstrate that RebuttalAgent outperforms the base model by 18.3% on automated metrics and surpasses advanced proprietary models in both automated and human evaluations.</div>
<div class="mono" style="margin-top:8px">本文的动机是学术反驳中复杂的策略性沟通和信息不对称问题，现有AI方法因缺乏观点采择而难以应对。为此，研究提出了基于心智理论（ToM）的RebuttalAgent框架，其方法采用ToM-策略-响应流程，通过监督微调和基于自奖励机制的强化学习在合成数据集RebuttalBench上进行训练，并利用专门评估器Rebuttal-RM进行评测。主要实验结果表明，RebuttalAgent在自动指标上平均超越基础模型18.3%，并在自动和人工评估中均优于先进的专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs</div>
<div class="meta-line">Authors: Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li</div>
<div class="meta-line">First: 2025-09-25T11:51:05+00:00 · Latest: 2026-01-27T03:49:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21044v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21044v2">PDF</a> · <a href="https://github.com/tsinghua-fib-lab/llm_rl_probing_analysis">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families and mathematical datasets shows two robust effects of online RL post-training: (i) an overall increase in average activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in mathematical generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://github.com/tsinghua-fib-lab/llm_rl_probing_analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习微调增强大语言模型内部回路的激活强度与多样性</div>
<div class="mono" style="margin-top:8px">大语言模型通过大规模预训练获得广泛先验知识，并可通过监督微调或基于强化学习的后训练进一步优化。现有研究表明，强化学习微调能带来超越单纯监督微调的性能提升，但其对不同内在特性模型的增强机制尚不明确。本研究受边缘归因分析方法的启发，探究强化学习微调前后模型内部结构的变化。通过对多模型系列与数学数据集的实验分析，发现在线强化学习后训练产生两种稳健效应：（1）平均激活强度整体提升，表明更多内部通路被激活且信号增强；（2）激活模式多样性增加，表现为更高熵值与更分散的边缘分布。这些变化显示强化学习通过重塑信息流路径，使其更具冗余性与灵活性，这或许能解释其在数学泛化任务中的优势。值得注意的是，采用直接偏好优化微调的模型偏离此趋势，其内部变化显著弱于基于近端策略优化与组相对策略优化的训练。本研究系统揭示了强化学习微调如何改变大语言模型内部回路，并凸显了在线强化学习与基于偏好方法之间的方法论差异。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the internal mechanisms by which reinforcement learning (RL) fine-tuning enhances large language model (LLM) capabilities beyond supervised fine-tuning. Motivated by the need to understand why RL improves diverse models, the authors employ edge attribution patching to analyze activation changes before and after RL training across multiple model families on mathematical tasks. The main experimental results reveal two consistent effects of online RL: an overall increase in average activation intensity, indicating stronger engagement of internal pathways, and greater diversity in activation patterns, shown by higher entropy and less concentrated edge distributions. These changes suggest RL reshapes information flow to be more redundant and flexible, potentially explaining improved mathematical generalization, though models fine-tuned with Direct Preference Optimization show weaker or inconsistent internal alterations compared to methods like PPO and GRPO.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究强化学习微调如何从内部机制上增强大语言模型的能力，超越监督微调的效果。研究动机源于需要理解强化学习为何能提升不同模型的性能，作者采用边缘归因修补技术，在数学任务上分析了多个模型家族在强化学习训练前后的激活变化。主要实验结果表明，在线强化学习产生两个一致效应：平均激活强度整体增加，表明内部通路参与更强；激活模式多样性更高，体现为熵值增大和边缘分布更分散。这些变化意味着强化学习重塑了信息流，使其更具冗余性和灵活性，可能解释了数学泛化能力的提升，但使用直接偏好优化微调的模型相比PPO和GRPO等方法，显示出较弱或不一致的内部改变。</div>
</details>
</div>
<div class="card">
<div class="title">RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning</div>
<div class="meta-line">Authors: Qianyue Hao, Sibo Li, Jian Yuan, Yong Li</div>
<div class="meta-line">First: 2025-05-20T09:43:33+00:00 · Latest: 2026-01-27T03:39:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.14140v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.14140v3">PDF</a> · <a href="https://github.com/tsinghua-fib-lab/RL-LLM-Reasoning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs&#x27; parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://github.com/tsinghua-fib-lab/RL-LLM-Reasoning for reproducibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维强化学习：通过推理时强化学习引导大语言模型推理</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLM）发展迅速，但其基于词元的自回归特性限制了复杂推理能力。为提升LLM推理，推理时技术（如思维链/树/图）通过引导复杂逻辑结构而不修改模型参数，以较低成本有效提升了性能。然而，这些手动预定义、任务无关的框架在不同任务中统一应用，缺乏适应性。为此，我们提出思维强化学习（RLoT），利用强化学习训练轻量级导航器模型，在推理时自适应增强LLM推理。具体而言，我们从人类认知视角设计了五种基础逻辑模块。在推理过程中，训练后的RL导航器根据问题特征动态选择合适逻辑模块，组合成任务专属的逻辑结构。在多个推理基准（AIME、MATH、GPQA等）和多种LLM（GPT、Llama、Qwen、DeepSeek）上的实验表明，RLoT相比现有推理时技术最高提升13.4%。值得注意的是，仅用不足3K参数的RL导航器，可使百亿级以下LLM达到千亿级模型的性能。此外，该导航器展现出强泛化能力：基于特定LLM-任务对训练的模型能有效迁移至未见过的LLM和任务。代码已开源：https://github.com/tsinghua-fib-lab/RL-LLM-Reasoning。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of existing inference-time reasoning techniques for large language models (LLMs), which use manually predefined, task-agnostic logical structures that lack adaptability. The proposed method, RL-of-Thoughts (RLoT), addresses this by training a lightweight navigator model with reinforcement learning to dynamically select and combine five basic logic blocks, derived from human cognition, into task-specific reasoning structures during inference. Main experimental results across multiple reasoning benchmarks (AIME, MATH, GPQA) with various LLMs (GPT, Llama, Qwen, DeepSeek) show that RLoT outperforms established techniques by up to 13.4%, enables sub-10B parameter LLMs to match the performance of 100B-scale models, and demonstrates strong transferability to unseen LLMs and tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于现有大语言模型推理时技术（如思维链/树/图）的局限性，这些技术采用手动预定义、与任务无关的逻辑框架，缺乏适应性。为此，本文提出了RL-of-Thoughts方法，通过强化学习训练一个轻量级导航器模型，在推理时根据问题特征，从人类认知角度设计的五个基本逻辑块中动态选择并组合成任务特定的推理结构。在多个推理基准和不同大语言模型上的实验结果表明，该方法性能优于现有技术最高达13.4%，仅用不足3K参数的导航器就能使参数量低于100亿的模型达到千亿级模型的推理水平，并且展现出对未见过的模型和任务的强泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Endless Terminals: Scaling RL Environments for Terminal Agents</div>
<div class="meta-line">Authors: Kanishk Gandhi, Shivam Garg, Noah D. Goodman, Dimitris Papailiopoulos</div>
<div class="meta-line">First: 2026-01-23T04:39:55+00:00 · Latest: 2026-01-27T03:34:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16443v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16443v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无尽终端：面向终端智能体的强化学习环境规模化构建</div>
<div class="mono" style="margin-top:8px">环境是自进化智能体的发展瓶颈。现有终端基准仅为评估设计，而非训练用途；强化学习需要可扩展的流程，而非单纯数据集。我们提出&#x27;无尽终端&#x27;——无需人工标注、通过程序化生成终端任务的自主流程。该流程包含四个阶段：生成多样化任务描述、构建验证容器化环境、生成完成度测试、基于可解性筛选。由此获得3255项涵盖文件操作、日志管理、数据处理、脚本编写及数据库操作的任务。我们采用原始PPO算法配合二元回合奖励与极简交互循环（无检索/多智能体协作/专用工具）训练智能体。尽管设计简约，在无尽终端上训练的模型仍取得显著提升：在预留开发集上，Llama-3.2-3B从4.0%提升至18.2%，Qwen2.5-7B从10.7%提升至53.3%，Qwen3-8B-openthinker-sft从42.6%提升至59.0%。该提升可迁移至人工标注基准：在TerminalBench 2.0中，Llama-3.2-3B从0.0%提升至2.2%，Qwen2.5-7B从2.2%提升至3.4%，Qwen3-8B-openthinker-sft从1.1%提升至6.7%，均优于包含复杂智能体框架在内的其他方案。这些结果表明：当环境实现规模化时，简约的强化学习即可取得成功。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the bottleneck of environment scarcity for training self-improving terminal agents, as existing benchmarks are designed for evaluation rather than scalable reinforcement learning. It introduces Endless Terminals, an autonomous pipeline that procedurally generates diverse terminal tasks—such as file operations and scripting—without human annotation, through stages including task description generation, containerized environment validation, and solvability filtering. Experimental results show that models trained with simple PPO and binary rewards on the resulting 3255 tasks achieve substantial performance gains on both a held-out development set and human-curated benchmarks like TerminalBench 2.0, outperforming more complex agentic approaches and demonstrating that scalable environments enable effective RL training.</div>
<div class="mono" style="margin-top:8px">本文针对自改进终端智能体训练中环境稀缺的瓶颈问题，指出现有基准测试仅适用于评估而非可扩展的强化学习训练。为此，作者提出了Endless Terminals，一个全自动流程，通过生成多样化任务描述、构建验证容器化环境及筛选可解性等步骤，无需人工标注即可程序化生成涵盖文件操作、日志管理等领域的终端任务。实验结果表明，使用简单PPO和二元奖励在生成的3255个任务上训练的模型，在保留开发集和人工标注基准如TerminalBench 2.0上均取得显著性能提升，超越了更复杂的智能体框架，证明环境规模化能使简单强化学习取得成功。</div>
</details>
</div>
<div class="card">
<div class="title">Q-Probe: Scaling Image Quality Assessment to High Resolution via Context-Aware Agentic Probing</div>
<div class="meta-line">Authors: Xiang Li, XueHeng Li, Yu Wang, XuanHua He, ZhangChi Hu, WeiWei Yu, ChengJun Xie</div>
<div class="meta-line">First: 2026-01-21T08:02:32+00:00 · Latest: 2026-01-27T03:25:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15356v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15356v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has empowered Multimodal Large Language Models (MLLMs) to achieve superior human preference alignment in Image Quality Assessment (IQA). However, existing RL-based IQA models typically rely on coarse-grained global views, failing to capture subtle local degradations in high-resolution scenarios. While emerging &quot;Thinking with Images&quot; paradigms enable multi-scale visual perception via zoom-in mechanisms, their direct adaptation to IQA induces spurious &quot;cropping-implies-degradation&quot; biases and misinterprets natural depth-of-field as artifacts. To address these challenges, we propose Q-Probe, the first agentic IQA framework designed to scale IQA to high resolution via context-aware probing. First, we construct Vista-Bench, a pioneering benchmark tailored for fine-grained local degradation analysis in high-resolution IQA settings. Furthermore, we propose a three-stage training paradigm that progressively aligns the model with human preferences, while simultaneously eliminating causal bias through a novel context-aware cropping strategy. Extensive experiments demonstrate that Q-Probe achieves state-of-the-art performance in high-resolution settings while maintaining superior efficacy across resolution scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Q-Probe：通过上下文感知智能探测将图像质量评估扩展至高分辨率</div>
<div class="mono" style="margin-top:8px">强化学习（RL）赋能多模态大语言模型（MLLM）在图像质量评估（IQA）中实现了卓越的人类偏好对齐。然而，现有基于RL的IQA模型通常依赖粗粒度的全局视图，难以捕捉高分辨率场景中细微的局部退化。尽管新兴的“图像思维”范式通过放大机制实现了多尺度视觉感知，但其直接应用于IQA会引发虚假的“裁剪即退化”偏见，并将自然景深误解为伪影。为解决这些挑战，我们提出Q-Probe——首个通过上下文感知探测将IQA扩展至高分辨率的智能代理框架。首先，我们构建了Vista-Bench，这是专为高分辨率IQA场景中细粒度局部退化分析设计的开创性基准。此外，我们提出一种三阶段训练范式，在逐步对齐模型与人类偏好的同时，通过新颖的上下文感知裁剪策略消除因果偏见。大量实验表明，Q-Probe在高分辨率场景中实现了最先进的性能，同时在多分辨率尺度上保持卓越效能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing Reinforcement Learning-based Image Quality Assessment (IQA) models, which rely on coarse global views and fail to capture subtle local degradations in high-resolution images, while also avoiding the biases introduced by direct adaptation of multi-scale &#x27;Thinking with Images&#x27; paradigms. The method introduces Q-Probe, a novel agentic IQA framework that scales assessment to high resolution through context-aware probing, supported by a new benchmark, Vista-Bench, for fine-grained local degradation analysis and a three-stage training paradigm that aligns the model with human preferences and eliminates causal bias via a context-aware cropping strategy. The main experimental results show that Q-Probe achieves state-of-the-art performance in high-resolution IQA settings while maintaining superior efficacy across various resolution scales.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有基于强化学习的图像质量评估模型在高分辨率场景下的局限性，这些模型依赖粗糙的全局视图，无法捕捉细微的局部退化，同时避免直接采用多尺度“图像思维”范式所带来的偏差。方法上提出了Q-Probe，这是一种通过上下文感知探测将图像质量评估扩展到高分辨率的智能体框架，并构建了针对细粒度局部退化分析的新基准Vista-Bench，以及一个三阶段训练范式，该范式通过上下文感知裁剪策略在使模型与人类偏好对齐的同时消除因果偏差。主要实验结果表明，Q-Probe在高分辨率设置下实现了最先进的性能，并在不同分辨率尺度上保持了卓越的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand</div>
<div class="meta-line">Authors: Kiattikun Chobtham</div>
<div class="meta-line">First: 2026-01-15T08:40:01+00:00 · Latest: 2026-01-27T03:13:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10181v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.10181v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习发现东北季风指数以预测泰国月降雨量</div>
<div class="mono" style="margin-top:8px">气候预测因地球系统复杂的时空模式而面临挑战。全球气候指数（如厄尔尼诺-南方涛动）是长期降雨预测的标准输入特征。然而，在泰国特定区域能提升预测精度的局地尺度指数仍存在显著空白。本文提出一种基于海表温度计算的新型东北季风气候指数，以反映北半球冬季季风的气候特征。为优化该指数计算区域，深度Q网络强化学习智能体通过探索并选择与季节性降雨相关性最高的矩形区域来实现。研究将降雨站点划分为12个独立聚类，以区分泰国南部与北部地区的降雨模式。实验结果表明，将优化后的指数纳入长短期记忆模型，能显著提升多数聚类区域的长期月降雨预测能力，有效降低12个月前瞻预测的均方根误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of improving long-term monthly rainfall prediction in Thailand by developing a localized climate index, as existing global indices like ENSO are insufficient for regional accuracy. The method involves creating a novel NorthEast monsoon index derived from sea surface temperature, with its calculation areas optimized using a Deep Q-Network reinforcement learning agent to select rectangles that maximize correlation with seasonal rainfall; rainfall stations are grouped into 12 clusters to capture regional patterns. Experimental results demonstrate that integrating this optimized index into Long Short-Term Memory models significantly enhances prediction skill across most clusters, effectively reducing the Root Mean Square Error for 12-month-ahead forecasts.</div>
<div class="mono" style="margin-top:8px">本文针对泰国长期月降雨量预测的挑战，提出开发本地化气候指数，因为ENSO等全球指数难以满足区域精度需求。方法包括基于海表温度计算新的东北季风指数，并利用深度Q网络强化学习智能体优化计算区域，选择与季节性降雨相关性最高的矩形区域；同时将降雨站分为12个聚类以区分区域模式。实验结果表明，将该优化指数融入长短期记忆模型后，显著提升了大多数聚类区域的预测能力，有效降低了12个月提前预测的均方根误差。</div>
</details>
</div>
<div class="card">
<div class="title">Risk-Sensitive Agent Compositions</div>
<div class="meta-line">Authors: Guruprerana Shabadi, Rajeev Alur</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-05T05:04:44+00:00 · Latest: 2026-01-27T03:02:04+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures. Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04632v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.04632v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">From software development to robot control, modern agentic systems decompose complex objectives into a sequence of subtasks and choose a set of specialized AI agents to complete them. We formalize agentic workflows as directed acyclic graphs, called agent graphs, where edges represent AI agents and paths correspond to feasible compositions of agents. Real-world deployment requires selecting agent compositions that not only maximize task success but also minimize violations of safety, fairness, and privacy requirements which demands a careful analysis of the low-probability (tail) behaviors of compositions of agents. In this work, we consider risk minimization over the set of feasible agent compositions and seek to minimize the value-at-risk and the conditional value-at-risk of the loss distribution of the agent composition where the loss quantifies violations of these requirements. We introduce an efficient algorithm which traverses the agent graph and finds a near-optimal composition of agents. It uses a dynamic programming approach to approximate the value-at-risk of agent compositions by exploiting a union bound. Furthermore, we prove that the approximation is near-optimal asymptotically for a broad class of practical loss functions. We also show how our algorithm can be used to approximate the conditional value-at-risk as a byproduct. To evaluate our framework, we consider a suite of video game-like control benchmarks that require composing several agents trained with reinforcement learning and demonstrate our algorithm&#x27;s effectiveness in approximating the value-at-risk and identifying the optimal agent composition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>风险敏感型智能体组合</div>
<div class="mono" style="margin-top:8px">从软件开发到机器人控制，现代智能体系统将复杂目标分解为一系列子任务，并选择一组专用人工智能智能体来完成它们。我们将智能体工作流形式化为有向无环图（称为智能体图），其中边代表人工智能智能体，路径对应可行的智能体组合。实际部署不仅需要最大化任务成功率，还需最小化对安全、公平和隐私要求的违反，这要求对智能体组合的低概率（尾部）行为进行细致分析。本文考虑在可行智能体组合集合上进行风险最小化，旨在最小化智能体组合损失分布的风险价值和条件风险价值，其中损失量化了对上述要求的违反程度。我们提出一种高效算法，通过遍历智能体图寻找近似最优的智能体组合。该算法采用动态规划方法，利用联合边界近似计算智能体组合的风险价值。进一步证明，对于广泛的实际损失函数类别，该近似具有渐近近优性。我们还展示了如何将算法扩展用于近似条件风险价值。为评估框架性能，我们采用一套类视频游戏控制基准测试（需组合多个强化学习训练的智能体），验证了算法在近似风险价值和识别最优智能体组合方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for AI agent workflows that not only maximize task success but also minimize violations of safety, fairness, and privacy requirements, this paper formalizes agentic systems as directed acyclic graphs and addresses risk minimization over feasible agent compositions. The method introduces an efficient dynamic programming algorithm that traverses the agent graph to approximate the value-at-risk and conditional value-at-risk of loss distributions, using a union bound and proving near-optimal asymptotic approximation for practical loss functions. Experimental results on video game-like control benchmarks with reinforcement learning agents demonstrate the algorithm&#x27;s effectiveness in approximating risk measures and identifying optimal agent compositions.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，现代智能体系统不仅需要最大化任务成功率，还需最小化对安全、公平和隐私要求的违反，因此将智能体工作流形式化为有向无环图，并研究可行智能体组合的风险最小化问题。方法上提出一种高效的动态规划算法，通过遍历智能体图并利用联合界来近似损失分布的风险价值和条件风险价值，证明了对于一类实用损失函数该近似是渐近近优的。在基于强化学习智能体的视频游戏式控制基准测试中，实验结果表明该算法能有效近似风险度量并识别出最优的智能体组合。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach</div>
<div class="meta-line">Authors: Weiran Guo, Bing Bo, Shaoxiang Wu, Jingsheng Yang</div>
<div class="meta-line">First: 2026-01-27T02:49:07+00:00 · Latest: 2026-01-27T02:49:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19122v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19122v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的函数调用模型弱点探索：一种对抗性数据增强方法</div>
<div class="mono" style="margin-top:8px">函数调用能力已成为大语言模型（LLMs）与外部工具及API高效交互的关键。现有提升LLMs函数调用能力的方法依赖于人工标注或模型自动生成的数据，并利用这些数据对LLMs进行微调。然而，这些方法往往缺乏针对性设计，且受限于固定模式和数据分布，制约了函数调用LLMs泛化性与鲁棒性的提升。为突破此局限，我们提出一种新颖的对抗性数据增强方法，采用强化学习系统性地识别并针对函数调用LLMs的弱点。我们的训练框架引入经强化学习（RL）训练的查询模型，专门生成挑战函数调用（FC）模型的对抗性查询。该方法采用零和博弈框架，使查询模型与FC模型进行迭代交替训练。总体而言，本方法推动了更鲁棒FC模型的开发，并为系统化识别与修正LLMs外部工具交互能力的弱点提供了新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing methods for improving the function call capabilities of Large Language Models (LLMs), which rely on manually annotated or automatically generated data that lacks targeted design and restricts generalization. The proposed method introduces an adversarial data augmentation approach using reinforcement learning, where a query model is trained to generate challenging adversarial queries in a zero-sum game framework against the function call model, enabling iterative alternating training. The main experimental results demonstrate that this approach systematically identifies and targets weaknesses, thereby advancing the development of more robust function call models capable of better interaction with external tools.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有提升大语言模型函数调用能力方法的局限性，这些方法依赖于手动标注或自动生成的数据，缺乏针对性设计且限制了泛化性。所提出的方法采用强化学习进行对抗性数据增强，通过训练一个查询模型生成具有挑战性的对抗性查询，与函数调用模型在零和博弈框架中进行迭代交替训练。主要实验结果表明，该方法能系统性地识别并针对弱点，从而推动开发出更鲁棒的函数调用模型，提升其与外部工具交互的能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260129_0353.html">20260129_0353</a>
<a href="archive/20260128_0342.html">20260128_0342</a>
<a href="archive/20260127_0320.html">20260127_0320</a>
<a href="archive/20260126_0313.html">20260126_0313</a>
<a href="archive/20260125_0321.html">20260125_0321</a>
<a href="archive/20260124_0344.html">20260124_0344</a>
<a href="archive/20260123_0345.html">20260123_0345</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
