<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 03:45</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0345</div>
    <div class="row"><div class="card">
<div class="title">QueStER: Query Specification for Generative keyword-based Retrieval</div>
<div class="meta-line">Authors: Arthur Satouf, Yuxuan Zong, Habiboulaye Amadou-Boubacar, Pablo Piantanida, Benjamin Piwowarski</div>
<div class="meta-line">Venue: eACL 2026</div>
<div class="meta-line">First: 2025-11-07T15:01:38+00:00 · Latest: 2026-01-21T17:37:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05301v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05301v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and generating retrieval cues directly from the query, but it can be brittle out of domain and expensive to scale. We introduce QueStER (QUEry SpecificaTion for gEnerative Keyword-Based Retrieval), which bridges GR and query reformulation by learning to generate explicit keyword-based search specifications. Given a user query, a lightweight LLM produces a keyword query that is executed by a standard retriever (BM25), combining the generalization benefits of generative query rewriting with the efficiency and scalability of lexical indexing. We train the rewriting policy with reinforcement learning techniques. Across in- and out-of-domain evaluations, QueStER consistently improves over BM25 and is competitive with neural IR baselines, while maintaining strong efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QueStER：基于生成式关键词检索的查询规范方法</div>
<div class="mono" style="margin-top:8px">生成式检索（GR）通过将相关性存储在模型参数中并直接从查询生成检索线索，与传统“先索引后检索”流程不同，但其存在跨领域脆弱性和扩展成本高的问题。本文提出QueStER（基于生成式关键词检索的查询规范方法），通过学习生成显式的基于关键词的搜索规范，桥接生成式检索与查询重构。给定用户查询时，轻量级大语言模型生成可由标准检索器（BM25）执行的关键词查询，从而融合生成式查询重写的泛化优势与词法索引的效率及可扩展性。我们采用强化学习技术训练重写策略。在领域内外的评估中，QueStER持续优于BM25，并与神经信息检索基线模型表现相当，同时保持高效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the brittleness and scalability challenges of generative retrieval by introducing QueStER, a method that bridges generative retrieval and query reformulation. The motivation is to combine the generalization benefits of generative query rewriting with the efficiency of lexical retrieval. The method employs a lightweight LLM trained with reinforcement learning to generate keyword-based search specifications from user queries, which are then executed by a standard BM25 retriever. Experimental results show that QueStER consistently improves over BM25 and is competitive with neural IR baselines across both in-domain and out-of-domain evaluations, while maintaining strong efficiency and scalability.</div>
<div class="mono" style="margin-top:8px">本文针对生成式检索的脆弱性和可扩展性挑战，提出了QueStER方法，以桥接生成式检索与查询重写。其动机在于结合生成式查询重写的泛化优势与词汇检索的效率。该方法采用经强化学习训练的轻量级大语言模型，从用户查询生成基于关键词的搜索规范，随后由标准BM25检索器执行。实验结果表明，在领域内和领域外评估中，QueStER一致性地优于BM25，并与神经信息检索基线方法竞争，同时保持了良好的效率和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Finding Kissing Numbers with Game-theoretic Reinforcement Learning</div>
<div class="meta-line">Authors: Chengdong Ma, Théo Tao Zhaowei, Pengyu Li, Minghao Liu, Haojun Chen, Zihao Mao, Yuan Cheng, Yuan Qi, Yaodong Yang</div>
<div class="meta-line">First: 2025-11-17T14:02:00+00:00 · Latest: 2026-01-21T16:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13391v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13391v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert&#x27;s 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game that can be fully parallelized at large scale, and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions, discovers over 6000 new structures in 14 and other dimensions, and establishes new records for generalized kissing configurations under various angular constraints. These results demonstrate AI&#x27;s power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于博弈论强化学习的接吻数求解</div>
<div class="mono" style="margin-top:8px">自艾萨克·牛顿于1694年首次研究接吻数问题以来，确定中心球周围非重叠球体的最大数量始终是基础性难题。该问题是希尔伯特第18个球堆积问题的局部对应，连接了几何、数论与信息论。尽管通过格与编码取得重要进展，但8维以上高维几何的不规则性与指数级增长的组合复杂度（超过围棋复杂度）限制了现有方法的可扩展性。本研究将该问题建模为可大规模并行化的双玩家矩阵补全博弈，并训练博弈论强化学习系统PackingStar以高效探索高维空间。矩阵元素表示球心向量间的成对余弦值；一方填充元素，另一方修正次优值，协同最大化矩阵尺寸（对应接吻数）。这种协作机制显著提升样本质量，使极大空间可处理。PackingStar复现了已有构型，并在25至31维超越所有人已知记录——其中25维构型几何对应Leech格并暗示潜在最优性。系统实现了自1971年有理结构以来在13维的首次突破，在14维及其他维度发现超6000个新结构，并在多种角度约束下建立了广义接吻构型的新记录。这些成果证明人工智能具备超越人类直觉探索高维空间的能力，为接吻数问题及更广泛几何问题开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the long-standing challenge of determining the maximum number of non-overlapping spheres that can touch a central sphere, known as the Kissing Number Problem, this paper addresses the limitations of existing methods in high dimensions due to exponential combinatorial complexity. The method introduces PackingStar, a game-theoretic reinforcement learning system that models the problem as a two-player matrix completion game, where one player fills matrix entries representing pairwise cosines of sphere center vectors and another corrects suboptimal ones, enabling efficient parallel exploration of high-dimensional spaces. Key experimental results include PackingStar reproducing known configurations and surpassing all human-known records from dimensions 25 to 31, achieving a breakthrough beyond rational structures in 13 dimensions, discovering over 6000 new structures in 14 and other dimensions, and setting new records for generalized kissing configurations under angular constraints, demonstrating AI&#x27;s capability to explore high-dimensional geometry beyond human intuition.</div>
<div class="mono" style="margin-top:8px">本文针对自牛顿时代以来悬而未决的“接吻数问题”，即确定一个中心球周围可容纳非重叠球的最大数量，旨在解决现有方法在高维空间中因组合复杂性指数增长而受限的挑战。研究方法提出了PackingStar，一种博弈论强化学习系统，将问题建模为双玩家矩阵补全游戏：一名玩家填充表示球心向量对之间余弦的矩阵条目，另一名玩家修正次优条目，从而实现对高维空间的高效并行探索。主要实验结果包括PackingStar复现了已知构型，在25至31维中超越了所有人类已知记录，在13维首次突破了1971年以来的有理结构限制，在14维及其他维度发现了超过6000个新结构，并在不同角度约束下建立了广义接吻构型的新记录，展示了人工智能探索超越人类直觉的高维几何空间的能力。</div>
</details>
</div>
<div class="card">
<div class="title">The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</div>
<div class="meta-line">Authors: Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</div>
<div class="meta-line">First: 2026-01-21T16:41:58+00:00 · Latest: 2026-01-21T16:41:58+00:00</div>
<div class="meta-line">Comments: Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15165v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15165v1">PDF</a> · <a href="https://github.com/LeapLabTHU/JustGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://nzl-thu.github.io/the-flexibility-trap">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵活性陷阱：为何任意顺序生成会限制扩散语言模型的推理潜力</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）突破了传统LLM严格的从左到右约束，实现了按任意顺序生成词元。直观上，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上可为数学和编程等通用任务释放更优的推理潜力。因此，许多研究采用强化学习（RL）来激发dLLMs的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但未扩展dLLMs的推理边界，反而使其收窄。我们发现dLLMs倾向于利用顺序灵活性规避对探索至关重要的高不确定性词元，导致解空间过早坍缩。这一发现挑战了现有dLLMs强化学习方法的前提——这些方法往往为保持顺序灵活性而投入大量复杂度（如处理组合轨迹和难解似然）。我们证明，通过主动放弃任意顺序生成并采用标准组相对策略优化（GRPO），能更有效地激发推理能力。所提方法JustGRPO极简却效果显著（如在GSM8K上达89.1%准确率），同时完整保留了dLLMs的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the assumption that arbitrary token generation order in diffusion language models inherently enhances reasoning, showing it instead leads to a &#x27;flexibility trap&#x27; where models bypass high-uncertainty tokens, prematurely collapsing the solution space. The authors propose JustGRPO, a method that intentionally forgoes arbitrary order and applies standard Group Relative Policy Optimization, which simplifies training by avoiding complex trajectory handling while retaining parallel decoding. Experiments demonstrate its effectiveness, achieving 89.1% accuracy on GSM8K, outperforming existing reinforcement learning approaches that preserve arbitrary order.</div>
<div class="mono" style="margin-top:8px">本文挑战了扩散语言模型中任意令牌生成顺序能增强推理能力的假设，揭示其反而导致&#x27;灵活性陷阱&#x27;，即模型绕过关键的高不确定性令牌，使解空间过早坍缩。作者提出JustGRPO方法，有意放弃任意顺序，采用标准组相对策略优化，从而避免复杂的轨迹处理并简化训练，同时保留并行解码能力。实验证明该方法有效，在GSM8K上达到89.1%的准确率，优于现有保持任意顺序的强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning</div>
<div class="meta-line">Authors: Yuval Kansal, Niraj K. Jha</div>
<div class="meta-line">First: 2026-01-21T16:38:59+00:00 · Latest: 2026-01-21T16:38:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &quot;compositional bridge&quot;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识图谱作为隐式奖励模型：路径衍生信号赋能组合推理</div>
<div class="mono" style="margin-top:8px">大型语言模型在数学与编程等结构化推理领域已接近专家水平，但其在专业科学领域进行组合式多跳推理的能力仍有限。我们提出一种自底向上的学习范式，使模型基于公理化领域事实进行组合以解决复杂未知任务。为此，我们提出一种结合监督微调与强化学习的后训练流程，其中知识图谱充当隐式奖励模型。通过从知识图谱路径中衍生新颖奖励信号，我们提供可验证、可扩展且具基础性的监督，促使模型在强化学习中组合中间公理而非仅优化最终答案。我们在医学领域验证该方法，使用短跳推理路径（1-3跳）训练140亿参数模型，并评估其对复杂多跳查询（4-5跳）的零样本泛化能力。实验表明，路径衍生奖励作为“组合桥梁”，使我们的模型在最困难推理任务上显著超越GPT-5.2和Gemini 3 Pro等更大规模前沿系统。此外，我们证明了该方法在选项重排压力测试中对对抗性扰动的鲁棒性。这项工作表明，将推理过程锚定于结构化知识是实现智能推理的可扩展高效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limited ability of large language models to perform compositional multi-hop reasoning in specialized scientific fields, despite their strong performance in structured domains like mathematics. The method introduces a bottom-up learning paradigm where models are grounded in axiomatic domain facts and composed to solve complex tasks, using a post-training pipeline that combines supervised fine-tuning and reinforcement learning with knowledge graphs as implicit reward models. The main experimental results show that training a 14B model on short-hop reasoning paths enables zero-shot generalization to complex multi-hop queries, significantly outperforming larger models and frontier systems like GPT-5.2 and Gemini 3 Pro on difficult reasoning tasks, while also demonstrating robustness to adversarial perturbations in option-shuffling stress tests.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大语言模型在专业科学领域进行组合式多跳推理能力有限的问题，尽管它们在数学等结构化领域表现优异。方法上提出了一种自底向上的学习范式，将模型基于公理化领域事实进行组合以解决复杂任务，采用结合监督微调和强化学习的后训练流程，并以知识图谱作为隐式奖励模型。主要实验结果表明，通过在短跳推理路径上训练一个140亿参数的模型，能够零样本泛化到复杂的多跳查询，在困难推理任务上显著优于更大模型及前沿系统如GPT-5.2和Gemini 3 Pro，同时在选项重排压力测试中展现出对抗扰动的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</div>
<div class="meta-line">Authors: Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-05-29T04:09:19+00:00 · Latest: 2026-01-21T16:37:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23062v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.23062v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating pre-collected offline data can substantially improve the sample efficiency of reinforcement learning (RL), but its benefits can break down when the transition dynamics in the offline dataset differ from those encountered online. Existing approaches typically mitigate this issue by penalizing or filtering offline transitions in regions with large dynamics gap. However, their dynamics-gap estimators often rely on KL divergence or mutual information, which can be ill-defined when offline and online dynamics have mismatched support. To address this challenge, we propose CompFlow, a principled framework built on the theoretical connection between flow matching and optimal transport. Specifically, we model the online dynamics as a conditional flow built upon the output distribution of a pretrained offline flow, rather than learning it directly from a Gaussian prior. This composite structure provides two advantages: (1) improved generalization when learning online dynamics under limited interaction data, and (2) a well-defined and stable estimate of the dynamics gap via the Wasserstein distance between offline and online transitions. Building on this dynamics-gap estimator, we further develop an optimistic active data collection strategy that prioritizes exploration in high-gap regions, and show theoretically that it reduces the performance gap to the optimal policy. Empirically, CompFlow consistently outperforms strong baselines across a range of RL benchmarks with shifted-dynamics data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向动态偏移数据强化学习的复合流匹配方法</div>
<div class="mono" style="margin-top:8px">利用预收集的离线数据可显著提升强化学习的样本效率，但当离线数据集中的状态转移动态与在线环境存在差异时，其优势可能失效。现有方法通常通过惩罚或过滤动态差异较大区域的离线转移来缓解此问题，但其动态差异估计器常依赖KL散度或互信息，在离线与在线动态支持集不匹配时可能失效。为此，我们提出CompFlow框架，基于流匹配与最优传输的理论关联构建原则性方法：将在线动态建模为基于预训练离线流输出分布的条件流，而非直接从高斯先验学习。该复合结构具有双重优势：(1)在有限交互数据下学习在线动态时提升泛化能力；(2)通过离线与在线转移的Wasserstein距离获得定义明确且稳定的动态差异估计。基于此估计器，我们进一步开发了乐观主动数据收集策略，优先探索高差异区域，并从理论上证明其能缩小与最优策略的性能差距。实验表明，在多种动态偏移的RL基准测试中，CompFlow均稳定优于现有基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge that offline reinforcement learning (RL) can suffer when the transition dynamics of pre-collected data differ from the online environment, a problem often inadequately addressed by existing dynamics-gap estimators that may fail with mismatched support. The proposed method, CompFlow, introduces a principled framework based on flow matching and optimal transport, modeling online dynamics as a conditional flow built upon a pretrained offline flow rather than learning directly from a Gaussian prior, which improves generalization and provides a stable Wasserstein distance-based estimate of the dynamics gap. Experimental results show that CompFlow, combined with an optimistic active data collection strategy that explores high-gap regions, consistently outperforms strong baselines across various RL benchmarks with shifted-dynamics data, and theoretical analysis confirms it reduces the performance gap to the optimal policy.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，当预收集离线数据的转移动态与在线环境不同时，离线强化学习的性能会下降，而现有基于KL散度或互信息的动态差距估计方法在支持集不匹配时可能失效。为此，作者提出了CompFlow方法，这是一个基于流匹配与最优传输的理论框架，通过将在线动态建模为基于预训练离线流输出的条件流，而非直接从高斯先验学习，从而提升了有限交互数据下在线动态学习的泛化能力，并利用Wasserstein距离提供了稳定、定义良好的动态差距估计。主要实验结果表明，结合了优先探索高差距区域的乐观主动数据收集策略后，CompFlow在多个具有动态偏移的强化学习基准测试中均优于现有基线，理论分析也证实其能缩小与最优策略的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-01-21T16:36:19+00:00</div>
<div class="meta-line">Comments: 80 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动梯度下降发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的梯度流动力学来解决这一问题——该任务无思维链无法解决，但存在简单迭代解。我们证明：尽管仅通过最终答案正确性进行训练，梯度流仍会驱动模型收敛至结构化、可解释的逐顶点迭代遍历算法。我们刻画了这种涌现所需的分布特性，指出“简单示例”（需要较少推理步骤的实例）的关键作用。当训练分布为这些简单实例分配足够概率质量时，模型将学习可泛化至更长链的遍历策略；若该质量消失，基于梯度的学习将不可行。我们通过合成数据实验及真实世界语言模型的数学推理任务验证了理论结果，证明理论发现适用于实际场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how transformers trained with outcome-based reinforcement learning can develop chain-of-thought reasoning, a process not well understood. The authors analyze gradient flow dynamics in a synthetic graph traversal task requiring intermediate steps, proving that training on sparse final-answer rewards leads the model to converge to an interpretable, iterative algorithm. Key findings show that the emergence of this reasoning depends on training data containing sufficient &#x27;simple examples&#x27; with fewer steps, which enables generalization to longer chains; without such data, learning fails, a result validated on both synthetic tasks and real-world mathematical reasoning with language models.</div>
<div class="mono" style="margin-top:8px">本文研究了基于结果的强化学习如何使Transformer模型发展出思维链推理能力，这一机制此前尚不明确。作者通过分析一个需要中间步骤的合成图遍历任务中的梯度流动态，证明仅基于最终答案的稀疏奖励进行训练，可使模型收敛到一个可解释的迭代算法。关键发现表明，这种推理能力的涌现依赖于训练数据包含足够多步骤较少的“简单示例”，从而能够泛化到更长的推理链；若缺乏此类数据，则基于梯度的学习会失败，这一结论在合成任务和真实世界语言模型的数学推理实验中得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Tianshi Xu, Yuteng Chen, Meng Li</div>
<div class="meta-line">First: 2026-01-21T16:14:30+00:00 · Latest: 2026-01-21T16:14:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15141v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model&#x27;s intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLEANER：自净化轨迹提升智能体强化学习</div>
<div class="mono" style="margin-top:8px">智能体强化学习（RL）使大语言模型（LLMs）能够利用Python解释器等工具解决复杂问题。然而，对于参数受限模型（如4B-7B规模），探索阶段常因频繁执行失败而产生噪声轨迹，阻碍策略优化。在基于结果的标准奖励设置下，这种噪声会导致严重的信用分配问题——错误动作与成功结果被同时强化。现有缓解方法面临两难：密集奖励易引发奖励破解，而过采样则带来高昂计算成本。为此，我们提出CLEANER框架。区别于外部过滤方法，CLEANER利用模型内在自校正能力，直接在数据收集阶段消除错误污染上下文。其核心相似性感知自适应回滚（SAAR）机制通过追溯性替换失败步骤为成功自校正，自主构建洁净的净化轨迹。SAAR基于语义相似度自适应调节替换粒度，从浅层执行修复到深层推理替换。通过在这些自净化路径上训练，模型内化了正确推理模式而非错误恢复循环。在AIME24/25、GPQA和LiveCodeBench的实证结果显示，相比基线平均准确率分别提升6%、3%和5%。值得注意的是，CLEANER仅用三分之一训练步数即达到最优性能，证明轨迹净化是可扩展的高效智能体RL解决方案。模型与代码已在GitHub开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for CLEANER arises from the challenge that parameter-constrained LLMs in agentic reinforcement learning often generate noisy trajectories due to frequent execution failures during exploration, which hinders policy optimization by causing credit assignment issues where errors are inadvertently reinforced. The method introduces a self-purification approach, specifically the Similarity-Aware Adaptive Rollback (SAAR) mechanism, which leverages the model&#x27;s intrinsic self-correction capability to autonomously construct clean trajectories by retrospectively replacing failed steps with successful self-corrections, adapting the replacement granularity based on semantic similarity to internalize correct reasoning patterns. Main experimental results on benchmarks including AIME24/25, GPQA, and LiveCodeBench demonstrate average accuracy improvements of 6%, 3%, and 5% over baselines, with CLEANER achieving state-of-the-art performance using only one-third of the training steps, highlighting its efficiency and scalability.</div>
<div class="mono" style="margin-top:8px">CLEANER的研究动机源于参数受限的大语言模型在智能体强化学习中，由于探索阶段频繁的执行失败产生噪声轨迹，阻碍策略优化并导致错误行动被错误强化的信用分配问题。该方法提出了一种自净化框架，核心是相似性感知自适应回滚机制，利用模型内在的自我纠正能力，通过回顾性地用成功的自我修正替换失败步骤来自主构建纯净轨迹，并根据语义相似性自适应调整替换粒度，使模型内化正确的推理模式而非错误恢复循环。在AIME24/25、GPQA和LiveCodeBench上的主要实验结果显示，相比基线模型平均准确率分别提升6%、3%和5%，且CLEANER仅用三分之一的训练步数就达到了最先进性能，证明了其高效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding</div>
<div class="meta-line">Authors: Ayan Maity, Sudeshna Sarkar</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-01-21T16:05:04+00:00 · Latest: 2026-01-21T16:05:04+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26 Workshop on AI for Urban Planning</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15131v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于改进网络嵌入的深度强化学习求解有限时间范围车辆路径问题</div>
<div class="mono" style="margin-top:8px">本文研究有限时间范围内的车辆路径问题，目标是在有限时间内最大化服务的客户请求数量。我们提出了一种新颖的路由网络嵌入模块，该模块生成局部节点嵌入向量和上下文感知的全局图表示。针对车辆路径问题构建的马尔可夫决策过程，将节点特征、网络邻接矩阵和边特征作为状态空间的组成部分。我们将剩余有限时间范围整合到网络嵌入模块中，为嵌入模块提供适当的路由上下文。通过将嵌入模块与基于策略梯度的深度强化学习框架相结合，求解有限时间范围的车辆路径问题。我们在真实世界路由网络和人工生成的欧几里得网络上对所提方法进行了训练与验证。实验结果表明，本方法比现有路由方法获得更高的客户服务率，且求解时间显著低于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vehicle routing problem with a finite time horizon, aiming to maximize served customer requests within a limited period. The method introduces a novel deep reinforcement learning framework featuring an improved network embedding module that generates local node embeddings and a context-aware global graph representation, explicitly incorporating the remaining time horizon into the state space. Experiments on real-world and synthetic Euclidean networks demonstrate that the proposed approach achieves a higher customer service rate and significantly reduces solution time compared to existing routing methods.</div>
<div class="mono" style="margin-top:8px">本文研究具有有限时间范围的车辆路径问题，目标是在限定时间内最大化服务的客户请求数量。方法提出了一种新颖的深度强化学习框架，其改进的网络嵌入模块能生成局部节点嵌入和上下文感知的全局图表示，并将剩余时间范围明确纳入状态空间。在真实世界和合成的欧几里得网络上的实验表明，与现有路径规划方法相比，所提方法实现了更高的客户服务率，并显著降低了求解时间。</div>
</details>
</div>
<div class="card">
<div class="title">DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search</div>
<div class="meta-line">Authors: Bostan Khan, Masoud Daneshtalab</div>
<div class="meta-line">First: 2026-01-21T16:03:25+00:00 · Latest: 2026-01-21T16:03:25+00:00</div>
<div class="meta-line">Comments: This paper significantly extends the preliminary work accepted at ESANN 2026. Source Code: https://github.com/bostankhan6/DeepFedNAS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15127v1">PDF</a> · <a href="https://github.com/bostankhan6/DeepFedNAS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepFedNAS：一种面向原则性、硬件感知且无需预测器的联邦神经架构搜索统一框架</div>
<div class="mono" style="margin-top:8px">联邦神经架构搜索旨在为隐私保护的联邦学习自动化模型设计，但当前面临两大瓶颈：无引导的超网络训练导致次优模型，以及训练后子网发现需耗费数小时的高成本流程。本文提出DeepFedNAS——一种基于原则性多目标适应度函数的新型两阶段框架，该函数融合了数学化网络设计与架构启发式规则。通过重构超网络，DeepFedNAS引入联邦帕累托最优超网络训练，利用预计算的高适应度架构帕累托最优缓存作为智能课程来优化共享超网络权重。随后，其无预测器搜索方法通过将该适应度函数作为零成本的精度直接代理，无需昂贵精度替代模型，实现数秒内按需子网发现。DeepFedNAS在CIFAR-100上取得最高1.21%的绝对精度提升，具备更优的参数与通信效率，训练后搜索流程总时间加速约61倍。通过将流程从超过20小时缩减至约20分钟（含初始缓存生成），并实现20秒级单次子网搜索，本框架使硬件感知的联邦学习部署具备即时性与实用性。完整源代码与实验脚本已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces DeepFedNAS, a framework addressing inefficiencies in Federated Neural Architecture Search (FedNAS), where unguided supernet training and slow post-training search hinder practical deployment. The method employs a principled fitness function and a re-engineered supernet to enable Federated Pareto Optimal Supernet Training, using a pre-computed cache of high-fitness architectures as a curriculum, followed by a predictor-free search that uses the fitness function as a zero-cost accuracy proxy for rapid subnet discovery. Experimental results show state-of-the-art accuracy improvements (e.g., up to 1.21% on CIFAR-100), better parameter and communication efficiency, and a ~61x speedup in the search pipeline, reducing total time from over 20 hours to about 20 minutes and enabling instant subnet searches in seconds.</div>
<div class="mono" style="margin-top:8px">本文提出了DeepFedNAS框架，旨在解决联邦神经架构搜索（FedNAS）中因无指导的超网训练和缓慢的训后搜索导致的效率低下问题。该方法采用原则性的适应度函数和重新设计的超网，实现联邦帕累托最优超网训练，利用预计算的高适应度架构缓存作为课程，随后通过无预测器搜索将适应度函数作为零成本精度代理进行快速子网发现。实验结果表明，该框架实现了最先进的精度提升（如在CIFAR-100上最高提升1.21%），具有更优的参数和通信效率，并将搜索流程加速约61倍，总时间从超过20小时缩短至约20分钟，支持秒级即时子网搜索。</div>
</details>
</div>
<div class="card">
<div class="title">Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning</div>
<div class="meta-line">Authors: Oleg Shchendrigin, Egor Cherepanov, Alexey K. Kovalev, Aleksandr I. Panov</div>
<div class="meta-line">First: 2026-01-21T15:27:23+00:00 · Latest: 2026-01-21T15:27:23+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15086v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15086v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://quartz-admirer.github.io/Memory-Rewriting/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中仅凭记忆保持不足以掌握记忆任务</div>
<div class="mono" style="margin-top:8px">现实世界中的有效决策依赖于兼具稳定性与适应性的记忆：环境随时间变化，智能体需长期保留相关信息，同时在情境变化时更新或覆盖过时内容。现有强化学习基准与记忆增强智能体主要关注记忆保持，而对同等重要的记忆重写能力探索不足。为填补这一空白，我们提出一个在部分可观测性下（即智能体必须依赖记忆而非当前观测的自然场景）显式测试持续记忆更新的基准，并借此比较循环网络、基于Transformer及结构化记忆架构。实验表明，经典循环模型尽管结构简单，却在记忆重写任务中展现出比现代结构化记忆（仅在狭窄条件下成功）和基于Transformer的智能体（常无法超越简单保持任务）更强的灵活性与鲁棒性。这些发现揭示了当前方法的根本局限，强调需要平衡稳定保持与适应性更新的记忆机制。本研究突出了这一被忽视的挑战，提出了评估基准，并为设计具有显式可训练遗忘机制的未来强化学习智能体提供了洞见。代码：https://quartz-admirer.github.io/Memory-Rewriting/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of current reinforcement learning (RL) benchmarks and memory-augmented agents, which primarily test memory retention but overlook the critical need for adaptive memory rewriting in dynamic environments. The authors introduce a new benchmark to evaluate continual memory updating under partial observability, comparing recurrent, transformer-based, and structured memory architectures. Experimental results show that classic recurrent models exhibit greater flexibility and robustness in memory rewriting tasks, while structured memories succeed only under narrow conditions and transformer-based agents often fail beyond simple retention, highlighting a fundamental gap in existing approaches and underscoring the necessity for mechanisms that balance stable retention with adaptive updating.</div>
<div class="mono" style="margin-top:8px">本文针对当前强化学习基准和记忆增强智能体的局限性，指出它们主要测试记忆保留能力，却忽视了动态环境中适应性记忆重写的关键需求。作者引入了一个新基准，用于评估部分可观测性下的持续记忆更新，并比较了循环、基于Transformer和结构化记忆架构。实验结果表明，经典循环模型在记忆重写任务中表现出更强的灵活性和鲁棒性，而结构化记忆仅在狭窄条件下成功，基于Transformer的智能体在简单保留任务之外常常失败，这揭示了现有方法的基本缺陷，并强调了平衡稳定保留与适应性更新的机制的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF</div>
<div class="meta-line">Authors: Wang Zixian</div>
<div class="meta-line">First: 2026-01-18T13:57:44+00:00 · Latest: 2026-01-21T14:54:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12415v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12415v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model alignment objectives are often presented as a collection of distinct algorithms, such as PPO, DPO, IPO, and their variants, each motivated by different derivations. In this work, we argue that this diversity obscures a simpler underlying structure. At a fundamental level, alignment objectives involve two independent design choices: (i) how training signals are sampled and weighted, and (ii) how deviations from a reference policy are geometrically penalized. Existing methods typically entangle these choices through a single divergence, most commonly the Kullback-Leibler divergence.
  We show that this entanglement is not merely a modeling convenience but a source of systematic instability. When the same divergence simultaneously determines sample weighting and optimization curvature, adjusting one aspect, such as exploration strength, inevitably alters the other, such as gradient geometry. This coupling is particularly problematic in preference-based reinforcement learning, where advantage signals are unbounded and high-confidence regimes are common.
  We propose a simple but structural remedy by formulating alignment as an orthogonal mirror descent problem, in which sampling geometry enters only as a linear driving force, while optimization geometry is determined independently by a mirror map. This perspective leads to a new alignment objective called Orthogonalized Policy Optimization (OPO), obtained by choosing a Euclidean mirror map in likelihood ratio space. The resulting objective admits a closed-form solution, linear and non-saturating gradient dynamics, and a well-conditioned trust region, while remaining fully compatible with standard large language model training pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正交化策略优化：在RLHF中解耦采样几何与优化几何</div>
<div class="mono" style="margin-top:8px">大语言模型对齐目标常以PPO、DPO、IPO等不同算法形式呈现，各自基于不同推导。本文指出这种多样性掩盖了更简洁的底层结构：对齐目标本质上包含两个独立设计选择：(i)训练信号的采样与加权方式，(ii)对参考策略偏离的几何惩罚机制。现有方法通常通过单一散度（多为KL散度）耦合这两个选择。我们证明这种耦合不仅是建模便利，更是系统性不稳定的根源——当同一散度同时决定样本加权和优化曲率时，调整探索强度等参数必然改变梯度几何特性，这在优势信号无界、高置信度场景常见的基于偏好的强化学习中尤为严重。我们提出结构性解决方案：将对齐问题构建为正交镜像下降问题，使采样几何仅作为线性驱动力，而优化几何由镜像映射独立决定。该视角催生了名为正交化策略优化（OPO）的新目标，通过在似然比空间选择欧几里得镜像映射实现。所得目标具有闭式解、线性非饱和梯度动态、良态信任域，同时完全兼容标准大语言模型训练流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that existing alignment methods entangle sampling and optimization geometries through a single divergence, leading to instability, this work proposes Orthogonalized Policy Optimization (OPO) to decouple these components by formulating alignment as an orthogonal mirror descent problem. The method uses a Euclidean mirror map in likelihood ratio space, resulting in a closed-form objective with linear, non-saturating gradient dynamics and a well-conditioned trust region. Experimental results demonstrate that OPO improves stability in preference-based reinforcement learning, particularly in handling unbounded advantage signals and high-confidence regimes, while remaining compatible with standard training pipelines.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到现有对齐方法通过单一散度纠缠采样与优化几何，导致不稳定性，因此提出正交化策略优化（OPO），通过将对齐表述为正交镜像下降问题来解耦这两个组件。该方法在似然比空间中使用欧几里得镜像映射，得到了一个闭式解的目标函数，具有线性、非饱和的梯度动态和良好条件的信任区域。实验结果表明，OPO在基于偏好的强化学习中提高了稳定性，特别是在处理无界优势信号和高置信度机制时表现良好，同时与标准训练流程保持兼容。</div>
</details>
</div>
<div class="card">
<div class="title">A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem</div>
<div class="meta-line">Authors: Mertcan Daysalilar, Fuat Uyguroglu, Gabriel Nicolosi, Adam Meyers</div>
<div class="meta-line">First: 2026-01-21T14:42:33+00:00 · Latest: 2026-01-21T14:42:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15038v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于课程学习的深度强化学习框架在电动汽车路径规划问题中的应用</div>
<div class="mono" style="margin-top:8px">带时间窗的电动汽车路径规划问题（EVRPTW）是可持续物流中的复杂优化问题，需在满足严格客户时间约束的前提下最小化总行驶距离、车队规模和电池消耗。尽管深度强化学习（DRL）作为传统启发式算法和精确求解器的替代方案展现出巨大潜力，但现有DRL模型常因约束密集而难以保持训练稳定性——无法收敛或泛化。本研究提出一种基于课程学习的深度强化学习（CB-DRL）框架以解决该问题。该框架采用三阶段渐进式课程结构：智能体先学习距离与车队优化（阶段A），再掌握电池管理（阶段B），最终完成完整EVRPTW求解（阶段C）。为保障跨阶段学习稳定性，框架采用改进的近端策略优化算法，配备阶段特异性超参数、价值优势裁剪机制和自适应学习率调度。策略网络基于异构图注意力编码器构建，通过全局-局部注意力机制与特征线性调制增强，能显式捕捉配送中心、客户节点和充电站的异构特性。仅使用N=10的小规模实例训练后，模型在N=5至N=100的未见实例上展现出强泛化能力，在中规模问题上显著优于基准方法。实验证实该课程引导方法在标准DRL基准失效的分布外实例上实现了高可行率与竞争优势，有效弥合了神经求解器的速度优势与运营可靠性之间的鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the training instability of deep reinforcement learning (DRL) models when solving the complex electric vehicle routing problem with time windows (EVRPTW). The authors propose a curriculum-based DRL framework that structures learning into three phases of increasing complexity, starting with distance and fleet optimization, then adding battery management, and finally tackling the full problem. The method employs a modified proximal policy optimization algorithm with phase-specific tuning and a heterogeneous graph attention network to model different node types. Experiments show that the model, trained only on small instances with 10 customers, generalizes robustly to unseen instances of up to 100 customers, achieving high feasibility and competitive solution quality where standard DRL baselines fail.</div>
<div class="mono" style="margin-top:8px">本研究针对深度强化学习模型在求解带时间窗的电动汽车路径规划这一复杂问题时存在的训练不稳定问题，提出了一种课程式深度强化学习框架。该方法将学习过程结构化分为三个难度递增的阶段，从距离和车队优化开始，逐步加入电池管理，最后学习完整问题；其采用改进的近端策略优化算法与异构图注意力网络来区分建模不同节点类型。实验结果表明，该模型仅使用10个客户的小规模实例进行训练，便能泛化到最多100个客户的未见实例上，在标准基线方法失效的情况下，仍能保持高可行性与有竞争力的解质量。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Black-Box Attacks Against Cooperative Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Amine Andam, Jamal Bentahar, Mustapha Hedabou</div>
<div class="meta-line">First: 2025-08-12T18:31:15+00:00 · Latest: 2026-01-21T14:25:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09275v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.09275v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Collaborative multi-agent reinforcement learning has rapidly evolved, offering state-of-the-art algorithms for real-world applications, including sensitive domains. However, a key challenge to its widespread adoption is the lack of a thorough investigation into its vulnerabilities to adversarial attacks. Existing work predominantly focuses on training-time attacks or unrealistic scenarios, such as access to policy weights or the ability to train surrogate policies. In this paper, we investigate new vulnerabilities under more challenging and constrained conditions, assuming an adversary can only collect and perturb the observations of deployed agents. We also consider scenarios where the adversary has no access at all (no observations, actions, or weights). Our main approach is to generate perturbations that intentionally misalign how victim agents see their environment. Our approach is empirically validated on three benchmarks and 22 environments, demonstrating its effectiveness across diverse algorithms and environments. Furthermore, we show that our algorithm is sample-efficient, requiring only 1,000 samples compared to the millions needed by previous methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束条件下针对协作多智能体强化学习的黑盒攻击</div>
<div class="mono" style="margin-top:8px">协作多智能体强化学习快速发展，为包括敏感领域在内的现实应用提供了先进算法。然而，其广泛采用面临的关键挑战在于缺乏对其对抗攻击脆弱性的深入研究。现有工作主要集中于训练时攻击或不切实际的场景，如获取策略权重或训练替代策略。本文在更具挑战性和约束性的条件下研究新的脆弱性，假设攻击者仅能收集并扰动已部署智能体的观测数据。我们还考虑了攻击者完全无法访问任何信息（无观测、动作或权重）的场景。我们的核心方法是生成有意使受害智能体对环境感知失准的扰动。该方法在三个基准测试和22个环境中进行了实证验证，证明了其在多种算法和环境中的有效性。此外，我们的算法具有样本高效性，仅需1,000个样本，而先前方法需要数百万样本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the security vulnerabilities of cooperative multi-agent reinforcement learning (MARL) systems under constrained black-box attack scenarios, motivated by the lack of thorough studies on adversarial threats in realistic deployment settings where attackers cannot access policy weights or train surrogate models. The method involves generating adversarial perturbations to intentionally misalign the observations of deployed agents, assuming the adversary can only collect and perturb observations or, in stricter scenarios, has no access to observations, actions, or weights. Experimental results across three benchmarks and 22 environments demonstrate the approach&#x27;s effectiveness against diverse MARL algorithms, with the algorithm being notably sample-efficient, requiring only 1,000 samples compared to millions needed by prior methods.</div>
<div class="mono" style="margin-top:8px">本文研究了协作式多智能体强化学习系统在受限黑盒攻击场景下的安全漏洞，其动机在于现有研究缺乏对现实部署环境中对抗性威胁的深入分析，攻击者无法访问策略权重或训练替代模型。该方法通过生成对抗性扰动，故意使受害智能体对环境观察产生错位，假设攻击者仅能收集并扰动观察数据，或在更严格场景下无法访问任何观察、动作或权重。在三个基准测试和22个环境中的实验结果表明，该方法对多种多智能体强化学习算法均有效，且具有显著的样本效率，仅需1,000个样本，而先前方法需要数百万个样本。</div>
</details>
</div>
<div class="card">
<div class="title">Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control</div>
<div class="meta-line">Authors: Jannis Becktepe, Aleksandra Franz, Nils Thuerey, Sebastian Peitz</div>
<div class="meta-line">First: 2026-01-21T14:13:44+00:00 · Latest: 2026-01-21T14:13:44+00:00</div>
<div class="meta-line">Comments: Code available at https://github.com/safe-autonomous-systems/fluidgym</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15015v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15015v1">PDF</a> · <a href="https://github.com/safe-autonomous-systems/fluidgym">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模流动控制强化学习算法的即插即用基准测试</div>
<div class="mono" style="margin-top:8px">强化学习在主动流动控制领域已展现出良好前景，但由于现有研究依赖异构的观测与执行方案、数值设置及评估协议，该领域进展仍难以评估。当前AFC基准测试尝试解决这些问题，但严重依赖外部计算流体动力学求解器，不具备完全可微性，且三维与多智能体支持有限。为突破这些限制，我们推出了首个独立、完全可微的AFC强化学习基准套件FluidGym。该套件完全基于GPU加速的PICT求解器在PyTorch中构建，运行于单一Python栈，无需外部CFD软件，并提供标准化评估协议。我们展示了PPO与SAC的基线结果，并将所有环境、数据集及训练模型作为公共资源发布。FluidGym实现了控制方法的系统化比较，为基于学习的流动控制研究建立了可扩展基础，项目地址：https://github.com/safe-autonomous-systems/fluidgym。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the lack of standardized evaluation in reinforcement learning for active flow control, where existing benchmarks rely on heterogeneous setups and external CFD solvers with limited capabilities. The method introduces FluidGym, a fully differentiable benchmark suite built entirely in PyTorch using the GPU-accelerated PICT solver, eliminating the need for external software and providing standardized protocols for 3D and multi-agent scenarios. Main experimental results include baseline performance with PPO and SAC algorithms, demonstrating the framework&#x27;s utility for systematic comparison, with all environments and models released as public resources to establish a scalable foundation for future research.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决强化学习在主动流动控制中缺乏标准化评估的问题，现有基准依赖异构设置和外部计算流体动力学求解器，且功能有限。方法上提出了FluidGym，这是一个完全可微分的基准套件，完全基于PyTorch并使用GPU加速的PICT求解器构建，无需外部软件，并为3D和多智能体场景提供标准化协议。主要实验结果包括使用PPO和SAC算法的基线性能，展示了该框架在系统比较中的实用性，所有环境和模型已作为公共资源发布，为未来基于学习的流动控制研究奠定了可扩展的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Shaping to Mitigate Reward Hacking in RLHF</div>
<div class="meta-line">Authors: Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</div>
<div class="meta-line">First: 2025-02-26T02:57:59+00:00 · Latest: 2026-01-21T13:46:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18770v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.18770v5">PDF</a> · <a href="https://github.com/PorUna-byte/PAR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR&#x27;s superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过奖励塑形缓解RLHF中的奖励黑客问题</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）对于使大语言模型（LLMs）与人类价值观对齐至关重要。然而，RLHF容易受到奖励黑客攻击，即智能体利用奖励函数的缺陷而非学习预期行为，从而降低对齐效果。尽管奖励塑形有助于稳定RLHF并部分缓解奖励黑客问题，但对塑形技术及其基本原理的系统性研究仍显不足。为填补这一空白，我们对主流奖励塑形方法进行了全面研究。分析提出两个关键设计原则：（1）RL奖励应有界；（2）RL奖励宜采用快速初始增长后逐步收敛的模式。基于这些见解，我们提出偏好即奖励（PAR）方法，利用奖励模型中隐含的偏好作为强化学习信号。PAR具有两个关键的方差缩减特性，有助于稳定RLHF训练过程并有效扩展早停容忍窗口。我们在Gemma2-2B基础模型上使用Ultrafeedback-Binarized和HH-RLHF两个数据集评估PAR。实验结果表明PAR优于其他奖励塑形方法：在AlpacaEval 2.0基准测试中，PAR的胜率至少高出其他方法5个百分点；PAR还展现出卓越的数据效率，仅需单个参考奖励即可实现最优性能，并在完整训练两轮后仍能保持对奖励黑客攻击的鲁棒性。代码已开源：https://github.com/PorUna-byte/PAR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward hacking in Reinforcement Learning from Human Feedback (RLHF), where agents exploit flaws in reward functions instead of learning desired behaviors, thereby undermining alignment. The authors conduct a systematic study of existing reward shaping methods, identifying two key design principles: bounded RL rewards and a reward curve with rapid initial growth followed by gradual convergence. Based on these insights, they propose Preference As Reward (PAR), a novel method that uses latent preferences from the reward model as the RL signal, offering variance-reduction properties to stabilize training and extend the early stopping tolerance window. Experiments on Gemma2-2B with Ultrafeedback-Binarized and HH-RLHF datasets show PAR outperforms other shaping methods, achieving at least a 5 percentage point higher win rate on AlpacaEval 2.0, while demonstrating data efficiency with a single reference reward and robustness against reward hacking even after extended training.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习（RLHF）中的奖励破解问题展开研究，即智能体利用奖励函数的缺陷而非学习预期行为，从而损害对齐效果。作者系统分析了现有奖励塑形方法，提出了两个关键设计原则：RL奖励应有界，且奖励曲线应快速初始增长后逐渐收敛。基于这些发现，他们提出了偏好即奖励（PAR）新方法，利用奖励模型中的潜在偏好作为RL信号，具有降低方差特性以稳定训练并延长早停容忍窗口。在Gemma2-2B模型上使用Ultrafeedback-Binarized和HH-RLHF数据集的实验表明，PAR优于其他塑形方法，在AlpacaEval 2.0基准上胜率至少高出5个百分点，同时仅需单一参考奖励即实现高效数据利用，并在完整两轮训练后仍保持对奖励破解的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Regret Approximation for Unsupervised Dynamic Environment Generation</div>
<div class="meta-line">Authors: Harry Mead, Bruno Lacerda, Jakob Foerster, Nick Hawes</div>
<div class="meta-line">First: 2026-01-21T12:58:40+00:00 · Latest: 2026-01-21T12:58:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14957v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14957v1">PDF</a> · <a href="https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进无监督动态环境生成中的遗憾近似方法</div>
<div class="mono" style="margin-top:8px">无监督环境设计旨在为强化学习智能体自动生成训练课程，以提升泛化能力和零样本性能。然而，设计有效课程仍具挑战性，尤其在特定环境参数子集会显著增加策略复杂度的场景中。现有方法面临困难的信用分配问题，且依赖的遗憾近似方法难以识别高难度环境层级，这些问题随环境规模扩大而加剧。我们提出动态环境生成方法，通过增强层级生成器的奖励信号密度来缓解信用分配难题，使无监督环境设计能扩展至更大规模环境。同时，我们提出新型遗憾近似指标——最大化负优势，该指标能更精准识别高难度环境层级。实验表明，最大化负优势指标优于现有遗憾近似方法，与动态环境生成结合后，其性能始终超越现有方法，且优势随环境规模扩大而增强。相关代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to improve Unsupervised Environment Design (UED) for reinforcement learning, which faces challenges in generating effective training curricula due to credit assignment issues and poor regret approximations that fail to identify complex environments. The method introduces Dynamic Environment Generation (DEGen) to provide denser reward signals for level generators and a new regret approximation called Maximised Negative Advantage (MNA) to better pinpoint challenging levels. Experimental results show that MNA outperforms existing regret approximations, and when combined with DEGen, it consistently surpasses prior methods, particularly as environment size increases.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进强化学习中的无监督环境设计（UED），该领域因信用分配困难和现有遗憾近似方法无法有效识别复杂环境而面临课程生成挑战。方法上提出了动态环境生成（DEGen）来为关卡生成器提供更密集的奖励信号，并引入了一种新的遗憾近似方法——最大化负优势（MNA），以更准确地识别困难关卡。实验结果表明，MNA优于现有遗憾近似方法，且与DEGen结合后，尤其在环境规模增大时，持续超越现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study</div>
<div class="meta-line">Authors: Keyu Lv, Manyi Zhang, Xiaobo Xia, Jingchen Ni, Shannan Yan, Xianzhi Yu, Lu Hou, Chun Yuan, Haoli Bai</div>
<div class="meta-line">First: 2026-01-21T11:22:29+00:00 · Latest: 2026-01-21T11:22:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14888v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14888v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低比特量化感知训练为何适用于推理大语言模型？一项系统性研究</div>
<div class="mono" style="margin-top:8px">推理模型在编程和数学等复杂任务上表现出色，但其推理过程通常速度较慢且令牌效率低下。为提升推理效率，训练后量化常伴随显著的精度损失，尤其在低比特设置下的推理任务中。本研究对推理模型的量化感知训练进行了系统性实证分析，主要发现包括：（1）知识蒸馏是通过监督微调或强化学习训练的推理模型的稳健目标；（2）训练后量化为量化感知训练提供了强初始化，能在降低训练成本的同时提升精度；（3）在可行的冷启动条件下，强化学习仍适用于量化模型并能带来额外增益；（4）将训练后量化的校准域与量化感知训练域对齐可加速收敛并常提升最终精度。最终，我们将这些发现整合为优化工作流（Reasoning-QAT），并证明其在多种大语言模型架构和推理数据集上持续优于前沿的训练后量化方法。例如在Qwen3-0.6B模型上，该方法在MATH-500数据集上超越GPTQ达44.53%，并在2比特量化场景中稳定恢复性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study systematically investigates quantization-aware training (QAT) for reasoning large language models (LLMs) to address the inference inefficiency and accuracy drops common with low-bit post-training quantization (PTQ). The method identifies key strategies: using knowledge distillation as a robust training objective, leveraging PTQ as a strong initialization for QAT to reduce cost, applying reinforcement learning to quantized models for gains, and aligning calibration and training domains to improve convergence. Experimental results show that the consolidated workflow, Reasoning-QAT, consistently outperforms state-of-the-art PTQ methods across various LLM backbones and reasoning datasets, notably achieving a 44.53% improvement over GPTQ on MATH-500 for Qwen3-0.6B and recovering performance even at 2-bit precision.</div>
<div class="mono" style="margin-top:8px">本研究系统探讨了面向推理大语言模型的量化感知训练（QAT），旨在解决低比特后训练量化（PTQ）常导致的推理效率低下和精度下降问题。方法上提出了关键策略：使用知识蒸馏作为鲁棒的训练目标，利用PTQ作为QAT的强初始化以降低成本，对量化模型应用强化学习以获得增益，以及对齐校准和训练领域以加速收敛。实验结果表明，整合后的工作流程Reasoning-QAT在多种LLM骨干和推理数据集上持续优于最先进的PTQ方法，特别是在Qwen3-0.6B模型上，在MATH-500数据集上比GPTQ提升了44.53%，并在2比特精度下恢复了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Listwise Direct Preference Optimization with Multi-Dimensional Preference Mixing</div>
<div class="meta-line">Authors: Yuhui Sun, Xiyao Wang, Zixi Li, YiTian Ding, Tianyang Ling, Jialuo Chen, Tianyi Yu, Zhenlong Yuan, Jinman Zhao</div>
<div class="meta-line">First: 2025-06-24T16:47:17+00:00 · Latest: 2026-01-21T10:13:03+00:00</div>
<div class="meta-line">Comments: 13 pages, 1 figures, appendix included</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.19780v6">Abs</a> · <a href="https://arxiv.org/pdf/2506.19780v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent alignment methods based on Direct Preference Optimization (DPO) reformulate preference learning as supervised optimization over pairwise comparisons, offering improved efficiency and stability over reinforcement learning from human feedback (RLHF). However, existing DPO-style methods implicitly assume a single fixed preference objective, which limits their ability to model the structured and sometimes conflicting nature of real-world human judgments that span multiple preference dimensions. In this work, we propose Listwise Direct Preference Optimization ($λ$-DPO), a unified framework that simultaneously improves supervision granularity and preference flexibility. Instead of collapsing multi-dimensional preference signals into a single ranking, $λ$-DPO constructs a mixture of listwise preference distributions weighted by a preference vector $λ$ on the probability simplex, enabling a single model to internalize a continuous spectrum of preference trade-offs. To further improve robustness, we introduce a performance-driven stochastic $λ$ scheduler that adaptively samples preference weights based on empirical downstream performance, explicitly mitigating the risks of misspecification inherent to static weighting schemes. We evaluate our method across multiple model families and scales on six widely used benchmarks. Experimental results show the consistent improvement against baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多维偏好混合的列表式直接偏好优化</div>
<div class="mono" style="margin-top:8px">近期基于直接偏好优化（DPO）的对齐方法将偏好学习重构为基于成对比较的监督优化，相比基于人类反馈的强化学习（RLHF）提升了效率与稳定性。然而现有DPO类方法隐含假设单一固定偏好目标，难以建模现实人类评判中跨越多维度的结构化且可能相互冲突的特性。本研究提出列表式直接偏好优化（$λ$-DPO），该统一框架同步提升监督粒度与偏好灵活性。$λ$-DPO通过概率单纯形上的偏好向量$λ$加权构建列表式偏好分布的混合模型，而非将多维偏好信号压缩为单一排序，使单个模型能够内化连续谱系的偏好权衡。为增强鲁棒性，我们引入性能驱动的随机$λ$调度器，根据下游实证表现自适应采样偏好权重，显式缓解静态加权方案固有的误设风险。我们在六个广泛使用的基准测试中跨越多模型族与规模进行评估，实验结果表明该方法相对基线模型取得持续改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of existing Direct Preference Optimization (DPO) methods, which assume a single fixed preference objective and thus struggle to model the multi-dimensional and potentially conflicting nature of real human judgments, this paper proposes Listwise Direct Preference Optimization (λ-DPO). The method constructs a mixture of listwise preference distributions weighted by a preference vector λ on the probability simplex, allowing a single model to internalize a continuous spectrum of preference trade-offs, and further introduces a performance-driven stochastic λ scheduler to adaptively sample preference weights for improved robustness against misspecification. Main experimental results on six benchmarks across multiple model families and scales demonstrate consistent improvements over baseline methods.</div>
<div class="mono" style="margin-top:8px">本文的动机在于现有直接偏好优化（DPO）方法假设单一的固定偏好目标，难以建模现实世界中多维度且可能冲突的人类判断。为此，论文提出了列表式直接偏好优化（λ-DPO），该方法通过在概率单纯形上构建由偏好向量λ加权的列表式偏好分布混合，使单一模型能够内化连续的偏好权衡谱；并进一步引入了性能驱动的随机λ调度器，基于下游实证表现自适应采样偏好权重，以提升对静态加权方案固有错误设定的鲁棒性。在多个模型系列和规模下的六个广泛使用的基准测试中，实验结果表明该方法相较于基线取得了持续一致的改进。</div>
</details>
</div>
<div class="card">
<div class="title">CI4A: Semantic Component Interfaces for Agents Empowering Web Automation</div>
<div class="meta-line">Authors: Zhi Qiu, Jiazheng Sun, Chenxiao Xia, Jun Zheng, Xin Peng</div>
<div class="meta-line">First: 2026-01-21T09:14:04+00:00 · Latest: 2026-01-21T09:14:04+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14790v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14790v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CI4A：面向智能体的语义化组件接口赋能网页自动化</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在高层语义规划方面展现出卓越能力，但在处理细粒度的底层网页组件操作时仍存在局限。为突破此限制，现有研究多通过强化学习等技术增强模型的基础操作能力。然而，与其迫使智能体适应以人为中心的交互界面，我们提出构建专为智能体优化的交互接口。本文提出面向智能体的组件接口（CI4A），这是一种将UI组件的复杂交互逻辑抽象为智能体可访问的统一工具原语的语义封装机制。我们在工业级前端框架Ant Design中实现了CI4A，覆盖23类常用UI组件。此外，我们开发了具备动态更新动作空间的混合智能体，可根据页面状态灵活调用可用的CI4A工具。基于集成CI4A的Ant Design，我们重构并升级了WebArena基准测试以评估现有前沿方法。实验结果表明，基于CI4A的智能体显著优于现有方案，以86.3%的任务成功率刷新了当前最优水平，同时执行效率也获得大幅提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of Large Language Models in performing fine-grained web component manipulations by proposing a shift from adapting agents to human-centric interfaces to creating agent-optimized interfaces. It introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts complex UI interaction logic into unified tool primitives for agents, implemented in the Ant Design framework covering 23 UI component categories. A hybrid agent with a dynamically updating action space was developed to flexibly invoke CI4A tools, and evaluation on a refactored WebArena benchmark showed that the CI4A-based agent achieves a state-of-the-art task success rate of 86.3% with significant improvements in execution efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在细粒度网页组件操作上的局限性，提出从让智能体适应以人为中心的界面转向创建为智能体优化的界面。它引入了组件接口（CI4A），这是一种语义封装机制，将复杂的用户界面交互逻辑抽象为智能体可访问的统一工具原语，并在Ant Design前端框架中实现，覆盖了23类常用UI组件。研究开发了一个具有动态更新动作空间的混合智能体，以灵活调用CI4A工具，并在重构的WebArena基准测试上进行评估，结果表明基于CI4A的智能体实现了86.3%的最先进任务成功率，并在执行效率上取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Impartial Games: A Challenge for Reinforcement Learning</div>
<div class="meta-line">Authors: Bei Zhou, Søren Riis</div>
<div class="meta-line">First: 2022-05-25T14:02:02+00:00 · Latest: 2026-01-21T08:31:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2205.12787v6">Abs</a> · <a href="https://arxiv.org/pdf/2205.12787v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AlphaZero-style reinforcement learning (RL) algorithms have achieved superhuman performance in many complex board games such as Chess, Shogi, and Go. However, we showcase that these algorithms encounter significant and fundamental challenges when applied to impartial games, a class where players share game pieces and optimal strategy often relies on abstract mathematical principles. Specifically, we utilise the game of Nim as a concrete and illustrative case study to reveal critical limitations of AlphaZero-style and similar self-play RL algorithms. We introduce a novel conceptual framework distinguishing between champion and expert mastery to evaluate RL agent performance. Our findings reveal that while AlphaZero-style agents can achieve champion-level play on very small Nim boards, their learning progression severely degrades as the board size increases. This difficulty stems not merely from complex data distributions or noisy labels, but from a deeper representational bottleneck: the inherent struggle of generic neural networks to implicitly learn abstract, non-associative functions like parity, which are crucial for optimal play in impartial games. This limitation causes a critical breakdown in the positive feedback loop essential for self-play RL, preventing effective learning beyond rote memorisation of frequently observed states. These results align with broader concerns regarding AlphaZero-style algorithms&#x27; vulnerability to adversarial attacks, highlighting their inability to truly master all legal game states. Our work underscores that simple hyperparameter adjustments are insufficient to overcome these challenges, establishing a crucial foundation for the development of fundamentally novel algorithmic approaches, potentially involving neuro-symbolic or meta-learning paradigms, to bridge the gap towards true expert-level AI in combinatorial games.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>公平博弈：强化学习面临的新挑战</div>
<div class="mono" style="margin-top:8px">AlphaZero式强化学习算法已在国际象棋、将棋、围棋等复杂棋盘游戏中实现超越人类的表现。然而，本研究揭示该类算法在应用于公平博弈时遭遇根本性挑战——这类博弈中玩家共享棋子，最优策略常依赖抽象数学原理。我们以尼姆游戏为具体案例，系统揭示AlphaZero式自博弈强化学习算法的关键局限：提出区分冠军级与专家级能力的新评估框架。实验表明，AlphaZero式智能体仅能在极小规模尼姆棋盘实现冠军级表现，其学习能力随棋盘尺寸增加急剧退化。这种困境不仅源于复杂数据分布或噪声标签，更源自表征瓶颈：通用神经网络难以隐式学习奇偶性等抽象非结合函数，而这些函数正是公平博弈最优策略的核心。该缺陷导致自博弈强化学习依赖的正反馈循环崩溃，使学习机制退化为高频状态的机械记忆。此发现与AlphaZero式算法易受对抗攻击的广泛担忧形成呼应，揭示其无法真正掌握所有合法游戏状态。本研究证明简单超参数调整无法解决此类问题，为发展神经符号计算、元学习等新型算法奠定基础，以推动组合博弈领域实现真正专家级人工智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the limitations of AlphaZero-style reinforcement learning algorithms when applied to impartial games like Nim, motivated by their superhuman success in other board games. The method involves using Nim as a case study and introducing a framework to distinguish between champion and expert mastery for evaluating agents. The main experimental results show that while these agents can achieve champion-level play on small boards, their learning severely degrades with larger board sizes due to a representational bottleneck in learning abstract functions like parity, which breaks the self-play feedback loop and prevents true expert mastery.</div>
<div class="mono" style="margin-top:8px">本文研究了AlphaZero风格强化学习算法在应用于尼姆等公平博弈游戏时的局限性，其动机是这些算法在其他棋盘游戏中取得了超人类表现。研究方法以尼姆游戏为具体案例，并引入了一个区分冠军与专家级掌握程度的概念框架来评估智能体。主要实验结果表明，虽然这些智能体能在小棋盘上达到冠军级水平，但随着棋盘尺寸增大，其学习能力严重退化，这是由于神经网络难以隐式学习奇偶性等抽象函数导致的表征瓶颈，破坏了自我对弈的正反馈循环，阻碍了真正的专家级掌握。</div>
</details>
</div>
<div class="card">
<div class="title">Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach</div>
<div class="meta-line">Authors: Wenyun Li, Wenjie Huang, Chen Sun</div>
<div class="meta-line">First: 2025-01-31T13:35:19+00:00 · Latest: 2026-01-21T07:57:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.19128v4">Abs</a> · <a href="https://arxiv.org/pdf/2501.19128v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world scenarios, reward signal for agents are exceedingly sparse, making it challenging to learn an effective reward function for reward shaping. To address this issue, the proposed approach in this paper performs reward shaping not only by utilizing non-zero-reward transitions but also by employing the \emph{Semi-Supervised Learning} (SSL) technique combined with a novel data augmentation to learn trajectory space representations from the majority of transitions, {i.e}., zero-reward transitions, thereby improving the efficacy of reward shaping. Experimental results in Atari and robotic manipulation demonstrate that our method outperforms supervised-based approaches in reward inference, leading to higher agent scores. Notably, in more sparse-reward environments, our method achieves up to twice the peak scores compared to supervised baselines. The proposed double entropy data augmentation enhances performance, showcasing a 15.8\% increase in best score over other augmentation methods</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中稀疏奖励的塑造：一种半监督方法</div>
<div class="mono" style="margin-top:8px">在许多现实场景中，智能体的奖励信号极为稀疏，这为学习有效的奖励函数以进行奖励塑造带来了挑战。为解决此问题，本文提出的方法不仅利用非零奖励转移进行奖励塑造，还结合半监督学习（SSL）技术与一种新颖的数据增强方法，从大多数转移（即零奖励转移）中学习轨迹空间表示，从而提升奖励塑造的效果。在Atari游戏和机器人操作任务中的实验结果表明，本方法在奖励推断方面优于基于监督学习的方法，使智能体获得更高得分。值得注意的是，在奖励更稀疏的环境中，本方法的峰值得分可达监督基线的两倍。所提出的双熵数据增强技术进一步提升了性能，最佳得分相比其他增强方法提高了15.8%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of sparse rewards in reinforcement learning by proposing a semi-supervised approach for reward shaping. The method leverages both non-zero-reward transitions and, innovatively, zero-reward transitions through semi-supervised learning combined with a novel double entropy data augmentation to learn trajectory space representations, thereby improving reward inference. Experimental results in Atari and robotic manipulation environments show that this approach outperforms supervised baselines in agent performance, achieving up to twice the peak scores in more sparse-reward settings, with the data augmentation contributing a 15.8% increase in best score over other methods.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中奖励信号稀疏的挑战，提出了一种用于奖励塑形的半监督方法。该方法通过结合半监督学习和一种新颖的双熵数据增强，不仅利用非零奖励转移，还利用占多数的零奖励转移来学习轨迹空间表示，从而提升奖励推断的效果。在Atari和机器人操作环境中的实验结果表明，该方法在智能体性能上优于基于监督学习的方法，在奖励更稀疏的环境中峰值分数可达基线方法的两倍，且数据增强使最佳分数比其他增强方法提高了15.8%。</div>
</details>
</div>
<div class="card">
<div class="title">PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Yao Lu, Dengdong Fan, Jianzheng Nie, Fan Xu, Jie Chen, Bin Zhou, Yonghong Tian</div>
<div class="meta-line">First: 2026-01-21T07:11:40+00:00 · Latest: 2026-01-21T07:11:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14716v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14716v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PCL-Reasoner-V1.5：基于离线强化学习的数学推理模型进展</div>
<div class="mono" style="margin-top:8px">我们推出PCL-Reasoner-V1.5，这是一个拥有320亿参数、专用于数学推理的大语言模型（LLM）。该模型基于Qwen2.5-32B构建，通过监督微调（SFT）和强化学习（RL）进行优化。核心创新在于我们提出的离线RL方法，相比GRPO等标准在线RL方法，具有更优的训练稳定性和效率。在基于Qwen2.5-32B的后训练模型中，我们的模型取得了最先进的性能，在AIME 2024和AIME 2025上的平均准确率分别达到90.9%和85.6%。本研究表明，离线RL是提升LLM推理能力的稳定高效范式。所有实验均在华为昇腾910C NPU上完成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces PCL-Reasoner-V1.5, a 32-billion-parameter large language model for mathematical reasoning, motivated by the need to enhance reasoning capabilities in LLMs with stable and efficient training. The method builds upon the Qwen2.5-32B base model, applying supervised fine-tuning followed by a novel offline reinforcement learning approach, which offers improved stability and efficiency over online methods like GRPO. Experimental results show state-of-the-art performance, with average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025, demonstrating the effectiveness of offline RL for advancing reasoning in LLMs, as validated on Huawei Ascend 910C NPUs.</div>
<div class="mono" style="margin-top:8px">本文提出了PCL-Reasoner-V1.5，这是一个拥有320亿参数的大型语言模型，旨在通过稳定高效的训练提升数学推理能力。该方法基于Qwen2.5-32B基础模型，先进行监督微调，然后采用一种新颖的离线强化学习方法，相比GRPO等在线方法具有更好的训练稳定性和效率。实验结果表明，该模型在基于Qwen2.5-32B后训练的模型中达到了最先进的性能，在AIME 2024和AIME 2025上的平均准确率分别为90.9%和85.6%，验证了离线强化学习在推进大语言模型推理方面的有效性，所有实验均在华为昇腾910C NPU上完成。</div>
</details>
</div>
<div class="card">
<div class="title">DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs</div>
<div class="meta-line">Authors: Mingxuan Song, Yusen Huo, Bohan Zhou, Shenglin Yin, Zhen Xiao, Jieyi Long, Zhilin Zhang, Chuan Yu</div>
<div class="meta-line">Venue: WWW</div>
<div class="meta-line">First: 2026-01-21T06:58:44+00:00 · Latest: 2026-01-21T06:58:44+00:00</div>
<div class="meta-line">Comments: Accepted at The ACM Web Conference (WWW) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing the advertiser&#x27;s cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs&#x27; in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DARA：基于强化学习微调大语言模型的上下文决策实现在线广告中的少样本预算分配</div>
<div class="mono" style="margin-top:8px">在AI生成竞价（AIGB）范式下，如何在预算约束下优化广告主赢取展示的累积价值是在线广告中的复杂挑战。广告主常有个性化目标但历史交互数据有限，形成传统强化学习方法难以有效应对的少样本场景。大语言模型凭借其上下文学习能力，为AIGB提供了从有限数据泛化的新途径，但其数值精度难以满足细粒度优化需求。为此，我们提出GRPO-Adaptive——一种通过动态更新训练参考策略来同步增强推理与数值精度的高效大语言模型后训练策略。在此基础上，我们进一步提出DARA这一新颖的双阶段框架：第一阶段由少样本推理器通过上下文提示生成初始方案，第二阶段由细粒度优化器通过反馈驱动推理进行方案精调。这种解耦设计使DARA能融合大语言模型的上下文学习优势与AIGB任务所需的精确适应性。在真实场景与合成数据环境中的大量实验表明，本方法在预算约束下的广告主累积价值指标上持续优于现有基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing advertiser value under budget constraints in few-shot online advertising scenarios, where traditional reinforcement learning methods often fail due to limited data. The authors propose DARA, a dual-phase framework that leverages large language models (LLMs) enhanced by a novel post-training strategy called GRPO-Adaptive, which improves reasoning and numerical precision by dynamically updating the reference policy. In DARA, a few-shot reasoner first generates initial plans via in-context prompting, followed by a fine-grained optimizer that refines them through feedback-driven reasoning, effectively combining LLMs&#x27; generalization abilities with precise optimization. Experimental results on real-world and synthetic data show that DARA consistently outperforms existing baselines in cumulative advertiser value under budget constraints.</div>
<div class="mono" style="margin-top:8px">本文针对在线广告中预算约束下广告主价值优化的挑战，在传统强化学习方法因数据有限而效果不佳的少样本场景下展开研究。作者提出了DARA框架，该框架基于一种名为GRPO-Adaptive的新型后训练策略增强大语言模型，通过动态更新参考策略来提高推理和数值精度。DARA将决策过程分解为两个阶段：少样本推理器通过上下文提示生成初始计划，细粒度优化器则利用反馈驱动推理进行精炼，从而结合了大语言模型的泛化能力和精准优化需求。在真实世界和合成数据环境上的大量实验表明，DARA在预算约束下的累计广告主价值方面持续优于现有基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Case-Guided Sequential Assay Planning in Drug Discovery</div>
<div class="meta-line">Authors: Tianchi Chen, Jan Bima, Sean L. Wu, Otto Ritter, Bingjia Yang, Xiang Yu</div>
<div class="meta-line">First: 2026-01-21T06:58:01+00:00 · Latest: 2026-01-21T06:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14710v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s&#x27;)$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>药物发现中的案例引导序贯实验规划</div>
<div class="mono" style="margin-top:8px">在药物发现中，如何在高度不确定性和资源限制下优化排序实验检测是一个关键规划难题。标准强化学习的主要障碍在于缺乏明确的环境模拟器或状态转移数据$(s, a, s&#x27;)$，规划只能依赖静态历史结果数据库。我们提出隐式贝叶斯马尔可夫决策过程，这是一个专为无模拟器场景设计的基于模型的强化学习框架。该框架通过相似历史结果构建非参数置信分布，形成案例引导的隐式状态转移模型。该机制支持基于证据积累的贝叶斯置信更新，并采用集成蒙特卡洛树搜索规划来生成稳定策略，以平衡信息获取与资源效率。通过综合实验验证，在真实世界中枢神经系统药物发现任务中，相比现有启发式方法，该框架在保持决策置信度的同时将资源消耗降低达92%。为严格评估决策质量，我们还在可计算最优策略的合成环境中进行了基准测试。与使用相同基于相似性模型的确定性值迭代方法相比，该框架与最优策略的一致性显著更高，证明了集成规划器的优越性。该框架为数据丰富但模拟器稀缺领域的序贯实验设计提供了实用解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of optimally sequencing experimental assays in drug discovery, a high-stakes planning problem under uncertainty and resource constraints where standard reinforcement learning is hindered by the lack of an environment simulator or transition data. To overcome this, the authors introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based framework that constructs a case-guided implicit model of transition dynamics using nonparametric belief distributions from similar historical outcomes, enabling Bayesian belief updates and ensemble Monte Carlo tree search planning to balance information gain and resource efficiency. Experimental validation on a real-world central nervous system drug discovery task shows IBMDP reduces resource consumption by up to 92% compared to established heuristics while maintaining decision confidence, and in a synthetic environment with a computable optimal policy, it achieves significantly higher alignment with that optimal policy than a deterministic alternative, demonstrating its superiority for sequential experimental design in data-rich but simulator-poor domains.</div>
<div class="mono" style="margin-top:8px">本文针对药物发现中实验检测顺序优化这一高风险规划问题展开研究，该问题在不确定性和资源约束下存在挑战，标准强化学习因缺乏环境模拟器或转移数据而受限。为解决此问题，作者提出了隐式贝叶斯马尔可夫决策过程（IBMDP），这是一个基于模型的框架，通过利用相似历史结果的非参数信念分布构建案例引导的隐式转移动态模型，支持贝叶斯信念更新和集成蒙特卡洛树搜索规划，以平衡信息获取与资源效率。在真实世界中枢神经系统药物发现任务上的实验验证表明，IBMDP相比现有启发式方法将资源消耗降低高达92%，同时保持决策置信度；在一个可计算最优策略的合成环境中，IBMDP比使用相同相似性模型的确定性替代方法显著更接近最优策略，证明了其在数据丰富但模拟器匮乏领域中序列实验设计方面的优越性。</div>
</details>
</div>
<div class="card">
<div class="title">Proximal Policy Optimization with Evolutionary Mutations</div>
<div class="meta-line">Authors: Casimir Czworkowski, Stephen Hornish, Alhassan S. Yasin</div>
<div class="meta-line">First: 2026-01-21T06:34:53+00:00 · Latest: 2026-01-21T06:34:53+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 2 tables, 1 algorithm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14705v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm known for its stability and sample efficiency, but it often suffers from premature convergence due to limited exploration. In this paper, we propose POEM (Proximal Policy Optimization with Evolutionary Mutations), a novel modification to PPO that introduces an adaptive exploration mechanism inspired by evolutionary algorithms. POEM enhances policy diversity by monitoring the Kullback-Leibler (KL) divergence between the current policy and a moving average of previous policies. When policy changes become minimal, indicating stagnation, POEM triggers an adaptive mutation of policy parameters to promote exploration. We evaluate POEM on four OpenAI Gym environments: CarRacing, MountainCar, BipedalWalker, and LunarLander. Through extensive fine-tuning using Bayesian optimization techniques and statistical testing using Welch&#x27;s t-test, we find that POEM significantly outperforms PPO on three of the four tasks (BipedalWalker: t=-2.0642, p=0.0495; CarRacing: t=-6.3987, p=0.0002; MountainCar: t=-6.2431, p&lt;0.0001), while performance on LunarLander is not statistically significant (t=-1.8707, p=0.0778). Our results highlight the potential of integrating evolutionary principles into policy gradient methods to overcome exploration-exploitation tradeoffs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于进化突变的近端策略优化算法</div>
<div class="mono" style="margin-top:8px">近端策略优化（PPO）是一种广泛使用的强化学习算法，以其稳定性和样本效率著称，但常因探索能力有限而陷入早熟收敛。本文提出POEM（基于进化突变的近端策略优化），这是一种受进化算法启发的自适应探索机制改进方法。POEM通过监控当前策略与历史策略移动平均之间的KL散度来增强策略多样性。当策略变化趋于停滞时，POEM会触发策略参数的自适应突变以促进探索。我们在四个OpenAI Gym环境（CarRacing、MountainCar、BipedalWalker和LunarLander）中评估POEM。通过贝叶斯优化技术的精细调参和韦尔奇t检验的统计分析，发现POEM在四项任务中的三项显著优于PPO（BipedalWalker：t=-2.0642，p=0.0495；CarRacing：t=-6.3987，p=0.0002；MountainCar：t=-6.2431，p&lt;0.0001），而LunarLander任务的性能差异未达统计显著性（t=-1.8707，p=0.0778）。本研究揭示了将进化原理融入策略梯度方法以突破探索-利用权衡的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of premature convergence in Proximal Policy Optimization (PPO) due to limited exploration by proposing POEM, a novel algorithm that integrates evolutionary mutation mechanisms. The method enhances policy diversity by monitoring KL divergence between the current policy and a moving average of past policies, triggering adaptive parameter mutations when stagnation is detected to encourage exploration. Experimental results on four OpenAI Gym environments, validated through Bayesian optimization and Welch&#x27;s t-test, show that POEM significantly outperforms standard PPO in three tasks (BipedalWalker, CarRacing, and MountainCar), while performance on LunarLander is not statistically significant, demonstrating the value of evolutionary principles for improving exploration-exploitation tradeoffs in policy gradient methods.</div>
<div class="mono" style="margin-top:8px">本文针对近端策略优化（PPO）算法因探索不足而导致的早熟收敛问题，提出了POEM这一融合进化突变机制的新算法。该方法通过监控当前策略与历史策略移动平均之间的KL散度来增强策略多样性，在检测到策略停滞时触发自适应参数突变以促进探索。在四个OpenAI Gym环境上的实验结果表明，经过贝叶斯优化调参和韦尔奇t检验统计验证，POEM在三个任务（BipedalWalker、CarRacing和MountainCar）上显著优于标准PPO，而在LunarLander任务上未呈现统计显著性，这揭示了将进化原理融入策略梯度方法以优化探索-利用权衡的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization</div>
<div class="meta-line">Authors: Hanyu Li, Jiangshan Duo, Bofei Gao, Hailin Zhang, Sujian Li, Xiaotie Deng, Liang Zhao</div>
<div class="meta-line">First: 2025-12-19T06:30:54+00:00 · Latest: 2026-01-21T06:34:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06052v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06052v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-thought reasoning in large language models can trigger an &quot;overthinking trap&quot;: longer rollouts raise cost and latency yet often yield unreliable accuracy gains. Existing methods use global, static controls that may suppress needed reasoning. We propose mastery-gated, sample-level, soft reinforcement learning compression that penalizes long rollouts only when the model already solves the problem and has produced a shorter rollout. Across benchmarks, it cuts response length by 20-40% with comparable or higher accuracy and generalizes across domains: a model trained on math spontaneously shortens unseen tasks (code, instruction following, general-knowledge QA) without hurting accuracy. We further show two-way transfer between non-agent CoT and tool-use agents: non-agent training reduces SWE-Bench Verified rounds by 13%, while compressing a thinking agent cuts SWE trajectories by 67% tokens and 52% rounds and shortens non-agent outputs by up to 44%. Compression is thus not cosmetic brevity, but an inherent computation policy -- what to keep, and what to forget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的思维链压缩与单领域到全领域泛化方法</div>
<div class="mono" style="margin-top:8px">大语言模型中的思维链推理可能引发&quot;过度思考陷阱&quot;：更长的推理过程会增加计算成本和延迟，却往往无法带来可靠的准确率提升。现有方法采用全局静态控制机制，可能抑制必要的推理步骤。我们提出一种基于掌握度门控的样本级软强化学习压缩方法，仅在模型已解决问题且生成更短推理路径时才对长推理过程进行惩罚。在多个基准测试中，该方法将响应长度缩减20-40%，同时保持相当或更高的准确率，并实现跨领域泛化：在数学领域训练的模型能自发缩短未见任务（代码生成、指令跟随、常识问答）的推理长度而不损害准确率。研究进一步展示了非智能体思维链与工具使用智能体之间的双向迁移：非智能体训练使SWE-Bench验证轮次减少13%，而压缩思考型智能体可使SWE轨迹减少67%的标记量和52%的轮次，并将非智能体输出缩短达44%。这表明压缩并非表面简洁，而是一种内在的计算策略——决定保留什么与遗忘什么。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the &#x27;overthinking trap&#x27; in chain-of-thought reasoning, where longer reasoning sequences increase computational cost without reliably improving accuracy. The authors propose a reinforcement learning method that compresses reasoning by penalizing long rollouts only when the model has already solved the problem and produced a shorter solution. Experimental results show this approach reduces response length by 20-40% while maintaining or improving accuracy across benchmarks, and it generalizes to unseen domains like code and QA without accuracy loss. Additionally, the method enables two-way transfer between non-agent and tool-use agents, significantly reducing token counts and rounds in tasks such as SWE-Bench.</div>
<div class="mono" style="margin-top:8px">本文针对思维链推理中的“过度思考陷阱”问题，即较长的推理序列会增加计算成本却未必提升准确性。作者提出一种强化学习方法，通过仅在模型已解决问题并生成更短推理时惩罚长序列来实现推理压缩。实验结果表明，该方法在多个基准测试中将响应长度减少了20-40%，同时保持或提高了准确性，并能泛化到代码和问答等未见领域而不损失精度。此外，该方法实现了非智能体与工具使用智能体之间的双向迁移，显著降低了SWE-Bench等任务中的令牌数量和轮次。</div>
</details>
</div>
<div class="card">
<div class="title">GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes</div>
<div class="meta-line">Authors: Mohammad Pivezhandi, Mahdi Banisharif, Saeed Bakhshan, Abusayeed Saifullah, Ali Jannesari</div>
<div class="meta-line">First: 2025-12-12T23:46:05+00:00 · Latest: 2026-01-21T06:28:07+00:00</div>
<div class="meta-line">Comments: 49 pages, 4 figures, 19 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12091v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.12091v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous AI agents on embedded platforms require real-time, risk-aware scheduling under resource and thermal constraints. Classical heuristics struggle with workload irregularity, tabular regressors discard structural information, and model-free reinforcement learning (RL) risks overheating. We introduce GraphPerf-RT, a graph neural network surrogate achieving deep learning accuracy at heuristic speeds (2-7ms). GraphPerf-RT is, to our knowledge, the first to unify task DAG topology, CFG-derived code semantics, and runtime context (per-core DVFS, thermal state, utilization) in a heterogeneous graph with typed edges encoding precedence, placement, and contention. Evidential regression with Normal-Inverse-Gamma priors provides calibrated uncertainty; we validate on makespan prediction for risk-aware scheduling. Experiments on three ARM platforms (Jetson TX2, Orin NX, RUBIK Pi) achieve R^2 = 0.81 on log-transformed makespan with Spearman rho = 0.95 and conservative uncertainty calibration (PICP = 99.9% at 95% confidence). Integration with four RL methods demonstrates that multi-agent model-based RL with GraphPerf-RT as the world model achieves 66% makespan reduction and 82% energy reduction versus model-free baselines, with zero thermal violations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphPerf-RT：一种面向OpenMP代码硬件感知调度的图驱动性能模型</div>
<div class="mono" style="margin-top:8px">嵌入式平台上的自主AI智能体需要在资源与热约束下进行实时风险感知调度。传统启发式方法难以应对工作负载不规则性，表格回归器丢弃结构信息，而无模型强化学习（RL）存在过热风险。我们提出GraphPerf-RT——一种图神经网络代理模型，在保持启发式方法速度（2-7毫秒）的同时达到深度学习精度。据我们所知，GraphPerf-RT首次通过类型化边（编码执行顺序、任务放置与资源争用）将任务DAG拓扑、CFG衍生的代码语义及运行时上下文（每核DVFS状态、热状态、利用率）统一至异构图结构中。采用正态-逆伽马先验的证据回归提供校准化不确定性估计，并通过完工时间预测验证风险感知调度能力。在三种ARM平台（Jetson TX2、Orin NX、RUBIK Pi）的实验中，对数转换完工时间预测达到R^2=0.81（斯皮尔曼相关系数ρ=0.95），不确定性校准保守可靠（95%置信水平下PICP=99.9%）。与四种RL方法的集成实验表明：以GraphPerf-RT作为世界模型的多智能体模型化RL，相较无模型基线可实现66%的完工时间缩减与82%的能耗降低，且零热约束违规。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for real-time, risk-aware scheduling of AI agents on resource-constrained embedded platforms, where classical heuristics and tabular models are inadequate, this paper introduces GraphPerf-RT, a graph neural network surrogate model. The method unifies task DAG topology, code semantics from control flow graphs, and runtime context into a heterogeneous graph with typed edges, and employs evidential regression for calibrated uncertainty estimation. Experimental results on three ARM platforms show high prediction accuracy (R²=0.81, Spearman ρ=0.95) and excellent uncertainty calibration; when integrated with reinforcement learning for scheduling, it achieves significant reductions in makespan (66%) and energy (82%) compared to model-free baselines, with zero thermal violations.</div>
<div class="mono" style="margin-top:8px">本文的动机是为资源受限的嵌入式平台上AI智能体的实时、风险感知调度提供解决方案，以克服传统启发式方法和表格模型的不足。方法上提出了GraphPerf-RT，这是一种图神经网络代理模型，它将任务DAG拓扑、控制流图衍生的代码语义和运行时上下文统一到一个具有类型化边的异质图中，并采用证据回归进行校准的不确定性估计。在三个ARM平台上的实验结果表明，该模型具有高预测精度（R²=0.81，Spearman ρ=0.95）和优异的不确定性校准性能；当与强化学习结合用于调度时，相较于无模型基线，能显著降低完工时间（66%）和能耗（82%），且无任何热违规。</div>
</details>
</div>
<div class="card">
<div class="title">CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation</div>
<div class="meta-line">Authors: Yutong Chen, Jiandong Gao, Ji Wu</div>
<div class="meta-line">First: 2026-01-21T06:17:52+00:00 · Latest: 2026-01-21T06:17:52+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14695v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14695v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&#x27;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&#x27;s reasoning ability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoScale-RL：通过协同扩展数据与计算实现高效后训练</div>
<div class="mono" style="margin-top:8px">训练大型推理模型通常不稳定且难以预测，尤其在处理难题或基础模型较弱时。我们发现当前的后训练扩展策略在这些情况下仍有改进空间。为此，我们提出CoScale-RL——一种具有更优数据与计算效率的新型扩展策略。该方法首先通过扩展解决方案使问题可解，其核心在于为每个问题收集多种解法，而非简单扩大数据集。随后，通过扩展推演计算以稳定强化学习过程。我们还采用名为“再蒸馏”的模型融合技术，在扩展规模时维持甚至提升计算效率。实验表明，该方法显著提升了数据与计算效率，在四个基准测试中平均准确率提升达3.76倍。CoScale-RL能够在无需大量监督微调数据集的情况下拓展大型推理模型的能力边界，为提升模型推理能力提供了新的扩展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability and unpredictability in training Large Reasoning Models (LRMs) on hard problems or with weak foundation models, motivating the development of a more efficient post-training scaling strategy. The proposed CoScale-RL method enhances data efficiency by collecting multiple solutions per problem instead of merely enlarging the dataset, and improves computational efficiency by scaling up rollout computation to stabilize Reinforcement Learning, further employing a Re-distillation model merge technique to maintain efficiency during scaling. Experimental results demonstrate significant improvements, with an average 3.76× accuracy gain across four benchmarks, enabling LRMs to advance their reasoning ability boundaries without requiring extensive supervised fine-tuning datasets.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型在困难问题或弱基础模型上训练不稳定和不可预测的问题，提出了一种更高效的训练后扩展策略。所提出的CoScale-RL方法通过为每个问题收集多个解决方案而非简单扩大数据集来提高数据效率，并通过扩展推演计算以稳定强化学习来提升计算效率，同时采用名为再蒸馏的模型融合技术以在扩展时保持效率。实验结果表明，该方法在四个基准测试上平均实现了3.76倍的准确率提升，使大型推理模型能够在无需大量监督微调数据集的情况下突破其推理能力边界。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning</div>
<div class="meta-line">Authors: Jianwen Sun, Xinrui Li, Fuqing Li, Xiaoxuan Shen</div>
<div class="meta-line">First: 2026-01-21T06:08:37+00:00 · Latest: 2026-01-21T06:08:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14693v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14693v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越误差驱动优化：基于目标条件强化学习的经验驱动符号回归</div>
<div class="mono" style="margin-top:8px">符号回归旨在自动识别紧凑且可解释的数学表达式，以建模输入与输出变量间的函数关系。现有基于搜索的符号回归方法大多依赖拟合误差指导搜索过程。然而在庞大的表达式空间中，大量候选表达式可能具有相近的误差值却结构迥异，导致搜索方向模糊并阻碍对潜在真实函数的收敛。为解决这一挑战，我们提出名为EGRL-SR（面向符号回归的经验驱动目标条件强化学习）的新框架。与传统误差驱动方法不同，EGRL-SR引入新视角：利用精确的历史轨迹并优化动作价值网络，以主动引导搜索过程，从而实现更稳健的表达式搜索。具体而言，我们将符号回归构建为目标条件强化学习问题，引入后见经验回放机制，使动作价值网络能从多样化的输入-输出对中泛化通用映射模式。此外，我们设计了全点满足二元奖励函数，促使动作价值网络关注结构模式而非低误差表达式，同时提出结构引导启发式探索策略以增强搜索多样性与空间覆盖。在公开基准测试中，EGRL-SR在恢复率与鲁棒性上持续优于现有先进方法，且能在相同搜索预算下恢复更复杂的表达式。消融实验结果验证了动作价值网络对搜索的有效引导，其中奖励函数与探索策略均发挥关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional error-based symbolic regression methods, which often face ambiguous search directions due to many structurally different expressions having similar fitting errors, this paper proposes EGRL-SR, a novel framework using experience-driven, goal-conditioned reinforcement learning. The method formulates symbolic regression as a goal-conditioned RL problem, incorporates hindsight experience replay to learn common mapping patterns from data, and employs a binary reward function and a structure-guided exploration strategy to focus on structural patterns and enhance search diversity. Experimental results on public benchmarks demonstrate that EGRL-SR outperforms state-of-the-art methods in recovery rate and robustness, successfully recovering more complex expressions under the same computational budget, with ablation studies confirming the critical roles of its core components.</div>
<div class="mono" style="margin-top:8px">针对传统基于误差的符号回归方法因大量结构不同但误差相近的表达式导致搜索方向模糊、难以收敛的问题，本文提出了一种名为EGRL-SR的新框架，采用经验驱动的目标条件强化学习。该方法将符号回归构建为目标条件强化学习问题，结合后见经验回放从数据中学习通用映射模式，并设计了关注结构模式的二元奖励函数和结构引导的启发式探索策略以提升搜索多样性。在公开基准测试上的实验表明，EGRL-SR在恢复率和鲁棒性上 consistently 优于现有先进方法，能在相同搜索预算下恢复更复杂的表达式，消融实验验证了其动作价值网络、奖励函数和探索策略的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks</div>
<div class="meta-line">Authors: Zixuan Ke, Yifei Ming, Austin Xu, Ryan Chin, Xuan-Phi Nguyen, Prathyusha Jwalapuram, Semih Yavuz, Caiming Xiong, Shafiq Joty</div>
<div class="meta-line">First: 2026-01-21T04:57:02+00:00 · Latest: 2026-01-21T04:57:02+00:00</div>
<div class="meta-line">Comments: Preprint; Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14652v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14652v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAS-Orchestra：通过整体编排与受控基准理解并提升多智能体推理能力</div>
<div class="mono" style="margin-top:8px">尽管多智能体系统（MAS）通过智能体协作展现出更高的智能潜力，但当前自动设计MAS的方法未能充分发挥其效能。这一不足主要源于两个关键因素：（1）方法复杂性——现有智能体编排采用顺序化、代码级执行方式，限制了全局系统层面的整体推理能力，且难以随智能体复杂度扩展；（2）效能不确定性——MAS在部署时缺乏与单智能体系统（SAS）相比是否具有实质优势的评估依据。我们提出MAS-Orchestra，一种训练时框架，将MAS编排建模为具有整体编排能力的函数调用强化学习问题，可一次性生成完整MAS系统。该框架将复杂的目标导向子智能体抽象为可调用函数，在隐藏内部执行细节的同时实现系统结构的全局推理。为严谨探究MAS何时及为何有效，我们构建了MASBENCH受控基准，通过深度、时域、广度、并行性和鲁棒性五个维度刻画任务特性。分析表明，MAS的增益并非普适，而关键取决于任务结构、验证协议以及编排器与子智能体的能力。基于这些发现，MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准测试中实现了稳定提升。MAS-Orchestra与MASBENCH共同为多智能体智能的研究提供了更有效的训练与理解框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underperformance of current multi-agent systems (MAS) due to methodological complexity and uncertain efficacy compared to single-agent systems, this paper introduces MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration to generate an entire system at once, abstracting sub-agents as callable functions for global reasoning. To rigorously assess MAS benefits, the authors also propose MASBENCH, a controlled benchmark characterizing tasks along five axes, with analysis showing MAS gains depend on task structure, verification, and agent capabilities rather than being universal. Experimental results demonstrate that MAS-Orchestra achieves consistent improvements on public benchmarks in mathematical reasoning, multi-hop QA, and search-based QA.</div>
<div class="mono" style="margin-top:8px">针对当前多智能体系统（MAS）因方法复杂性和相比单智能体系统效能不确定而表现不佳的问题，本文提出了MAS-Orchestra，这是一个训练时框架，将MAS编排表述为具有整体编排功能的函数调用强化学习问题，一次性生成整个系统，并将子智能体抽象为可调用函数以实现全局推理。为严格评估MAS优势，作者还提出了MASBENCH这一受控基准，从五个维度刻画任务特性，分析表明MAS的增益取决于任务结构、验证协议和智能体能力，而非普遍有效。实验结果显示，MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准上取得了持续改进。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Reasoning for Cold-Start Item Recommendation</div>
<div class="meta-line">Authors: Shijun Li, Yu Wang, Jin Wang, Ying Li, Joydeep Ghosh, Anne Cocos</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2025-11-23T03:22:53+00:00 · Latest: 2026-01-21T04:46:33+00:00</div>
<div class="meta-line">Comments: Published on Proceedings of the ACM on Web Conference 2026 (WWW 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18261v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18261v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix&#x27;s production ranking model by up to 8% in certain cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型推理的冷启动物品推荐研究</div>
<div class="mono" style="margin-top:8px">大语言模型凭借其内在的推理能力和广泛的知识库，在提升推荐系统性能方面展现出巨大潜力。然而现有研究主要集中于用户-物品交互数据丰富的热启动场景，对交互稀疏、传统协同过滤方法受限的冷启动场景探索不足。为突破这一局限，本研究针对Netflix领域提出创新的冷启动物品推荐推理策略。该方法利用大语言模型的先进推理能力，有效推断用户偏好，尤其适用于新引入或交互稀少的物品。我们系统评估了监督微调、基于强化学习的微调以及两者结合的混合方法，以优化推荐性能。基于真实数据的大量实验表明，该方法在冷启动推荐场景中显著提升了方法效能与实践表现。值得注意的是，我们基于推理的微调模型在特定情况下比Netflix生产级排序模型性能提升高达8%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored challenge of cold-start item recommendation where sparse user-item interactions hinder traditional methods, this paper proposes novel reasoning strategies using Large Language Models (LLMs) to infer user preferences for new or rarely interacted items, specifically within the Netflix domain. The method systematically evaluates and employs supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches to optimize LLMs for this task. Experimental results on real-world data show significant improvements in cold-start recommendation performance, with the reasoning-based fine-tuned models outperforming Netflix&#x27;s production ranking model by up to 8% in certain cases.</div>
<div class="mono" style="margin-top:8px">本文的研究动机在于解决冷启动物品推荐这一尚未充分探索的挑战，即稀疏的用户-物品交互数据阻碍了传统协同过滤方法。为此，作者提出了利用大语言模型推理能力的新策略，专门针对Netflix领域，以有效推断用户对新引入或极少交互物品的偏好。方法上，系统评估并采用了监督微调、基于强化学习的微调以及两者结合的混合方法，以优化推荐性能。在真实数据上的大量实验表明，该方法在冷启动推荐场景中取得了显著的效果提升，基于推理的微调模型在特定情况下甚至比Netflix的生产排序模型性能高出8%。</div>
</details>
</div>
<div class="card">
<div class="title">LLM4EO: Large Language Model for Evolutionary Optimization in Flexible Job Shop Scheduling</div>
<div class="meta-line">Authors: Rongjie Liao, Junhao Qiu, Xin Chen, Xiaoping Li</div>
<div class="meta-line">First: 2025-11-20T15:56:09+00:00 · Latest: 2026-01-21T04:44:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16485v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16485v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Customized static operator design has enabled widespread application of Evolutionary Algorithms (EAs), but their search performance is transient during iterations and prone to degradation. Dynamic operators aim to address this but typically rely on predefined designs and localized parameter control during the search process, lacking adaptive optimization throughout evolution. To overcome these limitations, this work leverages Large Language Models (LLMs) to perceive evolutionary dynamics and enable operator-level meta-evolution. The proposed framework, LLMs for Evolutionary Optimization (LLM4EO), comprises three components: knowledge-transfer-based operator design, evolution perception and analysis, and adaptive operator evolution. Firstly, initialization of operators is performed by transferring the strengths of classical operators via LLMs. Then, search preferences and potential limitations of operators are analyzed by integrating fitness performance and evolutionary features, accompanied by corresponding suggestions for improvement. Upon stagnation of population evolution, gene selection priorities of operators are dynamically optimized via improvement prompting strategies. This approach achieves co-evolution of populations and operators in the search, introducing a novel paradigm for enhancing the efficiency and adaptability of EAs. Finally, a series of validations on multiple benchmark datasets of the flexible job shop scheduling problem demonstrate that LLM4EO accelerates population evolution and outperforms both mainstream evolutionary programming and traditional EAs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM4EO：面向柔性作业车间调度进化优化的大语言模型</div>
<div class="mono" style="margin-top:8px">定制化静态算子设计促进了进化算法的广泛应用，但其搜索性能在迭代过程中具有瞬时性且易退化。动态算子虽旨在解决此问题，但通常依赖预定义设计及搜索过程中的局部参数控制，缺乏贯穿进化过程的自适应优化。为突破这些局限，本研究利用大语言模型感知进化动态，实现算子层面的元进化。所提出的进化优化大语言模型框架包含三个组件：基于知识迁移的算子设计、进化感知与分析、自适应算子进化。首先，通过大语言模型迁移经典算子优势完成算子初始化；随后，结合适应度性能与进化特征分析算子的搜索偏好与潜在局限，并给出相应改进建议；当种群进化停滞时，通过改进提示策略动态优化算子的基因选择优先级。该方法实现了搜索中种群与算子的协同进化，为提升进化算法效率与适应性引入了新范式。最后，在柔性作业车间调度问题的多个基准数据集上的验证表明，LLM4EO能加速种群进化，且性能优于主流进化规划与传统进化算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the limitations of static and dynamic operators in Evolutionary Algorithms (EAs), which often lack adaptive optimization throughout the evolutionary search process. To address this, the authors propose LLM4EO, a framework that leverages Large Language Models to enable operator-level meta-evolution; the method involves initializing operators via knowledge transfer from classical designs, analyzing operator performance and limitations using evolutionary features, and dynamically optimizing operators via improvement prompts upon evolutionary stagnation. Experimental results on flexible job shop scheduling benchmarks demonstrate that LLM4EO accelerates population evolution and outperforms both mainstream evolutionary programming and traditional EAs.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决进化算法中静态和动态算子缺乏全程自适应优化能力的局限性。为此，作者提出了LLM4EO框架，利用大语言模型实现算子层面的元进化；其方法包括通过知识迁移初始化算子、结合进化特征分析算子性能与局限，并在种群进化停滞时通过改进提示策略动态优化算子。在柔性作业车间调度基准问题上的实验结果表明，LLM4EO加速了种群进化，其性能优于主流进化编程和传统进化算法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Reinforcement Learning from Neural Feedback: Mapping fNIRS Signals to Agent Performance</div>
<div class="meta-line">Authors: Julia Santaniello, Matthew Russell, Benson Jiang, Donatello Sassaroli, Robert Jacob, Jivko Sinapov</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-17T00:21:46+00:00 · Latest: 2026-01-21T04:22:56+00:00</div>
<div class="meta-line">Comments: Accepted to the Association for the Advancement of Artificial Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12844v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.12844v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating user feedback into the agent&#x27;s training process. This paper introduces a framework that guides agent training through implicit neural signals, with a focus on the neural classification problem. Our work presents and releases a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train multiple classifiers to predict varying levels of agent performance (optimal, suboptimal, or worst-case) from windows of preprocessed fNIRS features, achieving an average F1 score of 67% for binary and 46% for multi-class classification across conditions and domains. We also train multiple regressors to predict the degree of deviation between an agent&#x27;s chosen action and a set of near-optimal policy actions, providing a continuous measure of performance. Finally, we evaluate cross-subject generalization and show that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our results demonstrate that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future Reinforcement Learning from Neural Feedback (RLNF) systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向基于神经反馈的强化学习：将fNIRS信号映射至智能体性能</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）是一种通过将用户反馈整合到智能体训练过程中，使其行为与人类偏好对齐的方法。本文提出了一种通过隐式神经信号指导智能体训练的框架，重点关注神经分类问题。我们构建并发布了一个新颖的功能性近红外光谱（fNIRS）数据集，该数据集采集自25名人类参与者在三个领域（抓放机器人、月球着陆器、Flappy Bird）的实验记录。我们训练了多个分类器，通过预处理后的fNIRS特征窗口预测智能体性能的不同水平（最优、次优或最差），在跨条件与跨领域实验中，二分类与多分类的平均F1分数分别达到67%和46%。同时训练了多个回归器，用于预测智能体所选动作与一组近似最优策略动作之间的偏差程度，从而提供连续的性能度量。最后，我们评估了跨被试泛化能力，结果表明：使用少量被试特定数据对预训练模型进行微调，可使二分类与多分类模型的平均F1分数分别提升17%和41%。我们的研究证明，将隐式fNIRS信号映射至智能体性能具有可行性且可优化，为未来基于神经反馈的强化学习（RLNF）系统奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the potential to advance Reinforcement Learning from Human Feedback (RLHF) by using implicit neural signals instead of explicit human input to guide agent training. The method involves collecting a novel functional near-infrared spectroscopy (fNIRS) dataset from 25 participants across three task domains and training classifiers and regressors to map neural signals to agent performance levels. The main experimental results show that classifiers achieved average F1 scores of 67% for binary and 46% for multi-class performance prediction, while fine-tuning with subject-specific data significantly improved cross-subject generalization, increasing F1 scores by 17% and 41% respectively, demonstrating the feasibility of mapping fNIRS signals to agent performance for future RLNF systems.</div>
<div class="mono" style="margin-top:8px">本文的动机是通过使用隐式神经信号替代显式人类输入来指导智能体训练，从而推进基于人类反馈的强化学习（RLHF）。方法包括从25名参与者在三个任务领域中收集新型功能性近红外光谱（fNIRS）数据集，并训练分类器和回归器将神经信号映射到智能体性能水平。主要实验结果表明，分类器在二元和多类性能预测中分别实现了67%和46%的平均F1分数，而使用特定受试者数据进行微调显著改善了跨受试者泛化能力，使F1分数分别提高了17%和41%，这证明了将fNIRS信号映射到智能体性能以构建未来基于神经反馈的强化学习（RLNF）系统的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning</div>
<div class="meta-line">Authors: Cheol-Hui Lee, Hwa-Yeon Lee, Dong-Joo Kim</div>
<div class="meta-line">First: 2026-01-20T13:38:01+00:00 · Latest: 2026-01-21T03:55:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13964v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13964v2">PDF</a> · <a href="https://github.com/dlcjfgmlnasa/RL-BioAug">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10%) of labeled data to guide the agent&#x27;s policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69% and 8.80% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task--for example, Time Masking with a 62% probability for sleep stage classification and Crop &amp; Resize with a 77% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at https://github.com/dlcjfgmlnasa/RL-BioAug.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RL-BioAug：用于自监督脑电表征学习的标签高效强化学习</div>
<div class="mono" style="margin-top:8px">数据增强的质量是脑电任务中对比学习性能的关键决定因素。尽管该范式在利用未标记数据方面前景广阔，但由于脑电信号的非平稳性（统计特性随时间变化），静态或随机增强策略往往难以保留内在信息。为此，我们提出RL-BioAug框架，利用标签高效强化学习（RL）智能体自主确定最优增强策略。该方法仅使用少量（10%）标记数据指导智能体策略，使编码器能以严格自监督方式学习鲁棒表征。实验结果表明，RL-BioAug显著优于随机选择策略，在Sleep-EDFX和CHB-MIT数据集上的Macro-F1分数分别大幅提升9.69%和8.80%。值得注意的是，该智能体主要针对各任务选择最优策略——例如睡眠分期任务以62%概率选择时间掩蔽，癫痫检测任务以77%概率选择裁剪重缩放。本框架有望替代传统基于启发式的增强方法，建立数据增强的新型自主范式。源代码发布于https://github.com/dlcjfgmlnasa/RL-BioAug。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge that static or random data augmentation strategies often fail to preserve intrinsic information in non-stationary EEG signals, limiting the performance of contrastive learning. To address this, the authors propose RL-BioAug, a framework that uses a label-efficient reinforcement learning agent to autonomously determine optimal augmentation policies, guided by only 10% of labeled data while learning representations in a self-supervised manner. Experimental results show that RL-BioAug significantly outperforms random augmentation, achieving improvements of 9.69% and 8.80% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, with the agent selecting task-specific strategies such as Time Masking for sleep stage classification and Crop &amp; Resize for seizure detection.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，静态或随机的数据增强策略难以保留非平稳脑电图信号的内在信息，从而限制了对比学习的性能。为此，作者提出了RL-BioAug框架，该框架利用标签高效的强化学习代理自主确定最优增强策略，仅使用10%的标注数据进行引导，并以自监督方式学习表征。实验结果表明，RL-BioAug显著优于随机增强策略，在Sleep-EDFX和CHB-MIT数据集上的Macro-F1分数分别提高了9.69%和8.80%，代理选择了任务特定的策略，例如在睡眠阶段分类中以62%概率使用时间掩码，在癫痫检测中以77%概率使用裁剪与调整大小。</div>
</details>
</div>
<div class="card">
<div class="title">SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation</div>
<div class="meta-line">Authors: Xichen Zhang, Ziyi He, Yinghao Zhu, Sitong Wu, Shaozuo Yu, Meng Chu, Wenhu Zhang, Haoru Tan, Jiaya Jia</div>
<div class="meta-line">First: 2026-01-21T03:16:17+00:00 · Latest: 2026-01-21T03:16:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14615v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14615v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SearchGym：通过高性价比与高保真环境模拟引导现实世界搜索智能体</div>
<div class="mono" style="margin-top:8px">搜索智能体已成为解决开放式知识密集型推理任务的关键范式。然而，通过强化学习训练此类智能体面临核心困境：调用实时商业网络API成本过高，而依赖静态数据快照常因数据错位引入噪声。这种错位会产生扭曲的奖励信号，通过惩罚正确推理或奖励幻觉行为破坏训练稳定性。为此，我们提出SearchGym仿真环境，旨在引导鲁棒的搜索智能体。SearchGym采用严谨的生成流程构建可验证知识图谱与对齐文档语料库，确保每个推理任务均基于事实且严格可解。基于此可控环境，我们提出SearchGym-RL课程学习方法，通过纯化反馈逐步优化智能体策略，从基础交互演进至复杂长程规划。在Llama与Qwen系列模型上的大量实验显示出强大的仿真到现实泛化能力。值得注意的是，在SearchGym中训练的Qwen2.5-7B-Base模型在九项多样化基准测试中平均相对超越网络增强型ASearcher基线10.6%。实验结果验证了高保真仿真可作为开发高效搜索智能体的可扩展且高性价比方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training search agents via Reinforcement Learning, where using live web APIs is costly and static data snapshots often yield misaligned, noisy rewards that destabilize training. To overcome this, the authors propose SearchGym, a simulation environment that constructs a verifiable knowledge graph and aligned document corpus to ensure tasks are factually grounded and solvable, alongside SearchGym-RL, a curriculum learning method that progressively optimizes agent policies with purified feedback. Experimental results on models like Llama and Qwen show strong sim-to-real generalization, with the trained Qwen2.5-7B-Base model outperforming the ASearcher baseline by an average of 10.6% across nine benchmarks, validating the approach as a scalable and cost-effective solution.</div>
<div class="mono" style="margin-top:8px">本文针对通过强化学习训练搜索代理的挑战，即使用实时网络API成本高昂，而静态数据快照常导致数据错位，产生噪声奖励信号，从而破坏训练稳定性。为解决此问题，作者提出了SearchGym，这是一个模拟环境，通过构建可验证的知识图谱和对齐的文档语料库，确保任务基于事实且可严格求解，并引入了SearchGym-RL课程学习方法，利用纯化反馈逐步优化代理策略，从基础交互演进到复杂长程规划。在Llama和Qwen系列模型上的广泛实验显示出强大的模拟到现实泛化能力，其中训练的Qwen2.5-7B-Base模型在九个多样化基准测试中平均相对优于ASearcher基线10.6%，验证了该方法作为可扩展且高成本效益解决方案的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GDEPO: Group Dual-dynamic and Equal-right-advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning</div>
<div class="meta-line">Authors: Zhengqing Yan, Xinyang Liu, Yi Zhang, Fan Guo, Yao Liu, Junchen Wan, Kang Song</div>
<div class="meta-line">First: 2026-01-11T07:34:41+00:00 · Latest: 2026-01-21T02:58:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06795v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06795v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDEPO：面向样本受限强化学习的增强训练数据利用率的群组双动态等权优势策略优化方法</div>
<div class="mono" style="margin-top:8px">自动定理证明（ATP）是人工智能（AI）领域的核心挑战，需在Lean等形式化语言中构建机器可验证的证明以评估AI推理能力。强化学习（RL），尤其是高性能的群组相对策略优化（GRPO）算法，已成为该任务的主流方法。然而在ATP场景中，GRPO面临两个关键问题：使用复合奖励时，其相对优势估计可能与形式验证器的二元反馈产生冲突；同时，其静态采样策略若未找到有效证明会丢弃整批数据，导致模型更新零贡献和严重数据浪费。为突破这些局限，我们提出群组双动态等权优势策略优化（GDEPO），该方法包含三项核心机制：1）动态补充采样：对无效批次重新采样直至发现有效证明；2）等权优势：将优势函数的符号（基于正确性）与其幅度（由辅助奖励调节）解耦，确保策略更新的稳定性和正确性；3）动态补充迭代：对初始失败但最终成功的样本施加额外梯度步，加速困难案例的学习。在三个不同难度数据集（MinF2F-test、MathOlympiadBench、PutnamBench）上的实验验证了GDEPO的有效性，消融研究证实了其协同组件的必要性。该方法提升了数据利用率和优化效率，为ATP提供了创新的训练范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing reinforcement learning methods like GRPO in Automated Theorem Proving (ATP), where static sampling wastes data and composite rewards conflict with verifier feedback, this paper introduces GDEPO. The method employs dynamic additional sampling to reuse failed batches, an equal-right advantage mechanism to decouple reward signals for stable policy updates, and dynamic additional iterations to focus on hard cases. Experimental results on datasets including MinF2F-test, MathOlympiadBench, and PutnamBench demonstrate GDEPO&#x27;s effectiveness in improving data utilization and optimization efficiency, with ablation studies confirming the contribution of each component.</div>
<div class="mono" style="margin-top:8px">针对自动定理证明中现有强化学习方法（如GRPO）因静态采样导致数据浪费、复合奖励与验证器反馈冲突的问题，本文提出了GDEPO方法。该方法采用动态补充采样重用失败批次，通过等权优势机制解耦奖励信号以实现稳定策略更新，并利用动态额外迭代专注于困难样本。在MinF2F-test、MathOlympiadBench和PutnamBench等不同难度数据集上的实验结果表明，GDEPO有效提升了数据利用率和优化效率，消融研究验证了各协同组件的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Context-Picker: Dynamic context selection using multi-stage reinforcement learning</div>
<div class="meta-line">Authors: Siyuan Zhu, Chengdong Xu, Kaiqiang Ke, Chao Yu</div>
<div class="meta-line">First: 2025-12-16T14:52:11+00:00 · Latest: 2026-01-21T02:41:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14465v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.14465v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In long-context question answering, selecting the appropriate scope of context for a query remains a key and unresolved challenge. Insufficient context can lead to missing essential information, whereas excessive context often introduces noise and degrades answer quality. Conventional methods, such as retrieving a fixed number of passages or applying reranking, struggle to dynamically determine which context to include. This is especially problematic for factoid questions, which typically depend only on a few precise pieces of evidence. To overcome this limitation, we propose Context-Picker, a reasoning-aware framework that reframes context selection as the task of identifying a minimal sufficient evidence subset, moving beyond conventional similarity-based ranking. Context-Picker uses a human-inspired two-stage reinforcement learning schedule: stage 1 focuses on improving the recall rate of critical passages, and stage 2 prioritizes pruning redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines ``minimal sufficient sets&quot; via a Leave-One-Out (LOO) procedure, providing dense and task-aligned supervision. Experiments on five long-context and multi-hop QA datasets demonstrate that our method outperforms strong RAG baselines and achieved higher answer accuracy. Ablation studies also indicate that our coarse-to-fine optimization schedule, the redundancy-aware reward shaping, along with the rationale generated by the policy, all contribute substantially to these gains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Context-Picker：基于多阶段强化学习的动态上下文选择方法</div>
<div class="mono" style="margin-top:8px">在长上下文问答中，为查询选择合适的上下文范围仍是关键且未解决的挑战。上下文不足可能导致遗漏关键信息，而过量上下文常引入噪声并降低答案质量。传统方法（如检索固定数量段落或重排序）难以动态确定应包含的上下文，这对通常仅依赖少量精确证据的事实型问题尤为突出。为突破此限制，我们提出Context-Picker——一个将上下文选择重构为识别最小充分证据子集任务的推理感知框架，超越了传统基于相似度的排序方法。该框架采用受人类启发的两阶段强化学习方案：第一阶段聚焦提升关键段落召回率，第二阶段优先剪枝冗余以提炼紧凑证据集。为解决奖励稀疏性问题，我们设计了通过留一法挖掘“最小充分集”的离线证据蒸馏流程，提供密集且任务对齐的监督。在五个长上下文与多跳问答数据集上的实验表明，本方法优于强RAG基线且获得更高答案准确率。消融研究证实，从粗到细的优化方案、冗余感知的奖励塑造以及策略生成的推理依据均对性能提升有实质贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of dynamically selecting the optimal context scope for long-context question answering, where fixed retrieval methods often include either insufficient or noisy information, particularly harming factoid questions that rely on precise evidence. The authors propose Context-Picker, a framework that reframes context selection as identifying a minimal sufficient evidence subset, employing a two-stage reinforcement learning approach: the first stage enhances recall of critical passages, and the second stage prunes redundancy to distill a compact set, with an offline evidence distillation pipeline using a Leave-One-Out procedure to provide dense supervision and mitigate reward sparsity. Experimental results on five long-context and multi-hop QA datasets show that Context-Picker outperforms strong RAG baselines in answer accuracy, with ablations confirming the contributions of the coarse-to-fine optimization, redundancy-aware reward shaping, and policy-generated rationales.</div>
<div class="mono" style="margin-top:8px">本文针对长上下文问答中动态选择最佳上下文范围的挑战，传统固定检索方法常导致信息不足或噪声过多，尤其影响依赖精确证据的事实型问题。作者提出Context-Picker框架，将上下文选择重构为识别最小充分证据子集的任务，采用两阶段强化学习：第一阶段提升关键段落的召回率，第二阶段剪枝冗余以提炼紧凑证据集，并通过离线证据蒸馏流程使用留一法提供密集监督，缓解奖励稀疏性。在五个长上下文和多跳问答数据集上的实验表明，该方法优于强RAG基线，获得了更高的答案准确率，消融研究证实了从粗到细的优化调度、冗余感知奖励塑造以及策略生成的理由均对性能提升有重要贡献。</div>
</details>
</div>
<div class="card">
<div class="title">A Finite Expression Method for Solving High-Dimensional Committor Problems</div>
<div class="meta-line">Authors: Zezheng Song, Maria K. Cameron, Haizhao Yang</div>
<div class="meta-line">First: 2023-06-21T13:43:59+00:00 · Latest: 2026-01-21T01:30:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2306.12268v3">Abs</a> · <a href="https://arxiv.org/pdf/2306.12268v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transition path theory (TPT) is a mathematical framework for quantifying rare transition events between a pair of selected metastable states $A$ and $B$. Central to TPT is the committor function, which describes the probability to hit the metastable state $B$ prior to $A$ from any given starting point of the phase space. Once the committor is computed, the transition channels and the transition rate can be readily found. The committor is the solution to the backward Kolmogorov equation with appropriate boundary conditions. However, solving it is a challenging task in high dimensions due to the need to mesh a whole region of the ambient space. In this work, we explore the finite expression method (FEX, Liang and Yang (2022)) as a tool for computing the committor. FEX approximates the committor by an algebraic expression involving a fixed finite number of nonlinear functions and binary arithmetic operations. The optimal nonlinear functions, the binary operations, and the numerical coefficients in the expression template are found via reinforcement learning. The FEX-based committor solver is tested on several high-dimensional benchmark problems. It gives comparable or better results than neural network-based solvers. Most importantly, FEX is capable of correctly identifying the algebraic structure of the solution which allows one to reduce the committor problem to a low-dimensional one and find the committor with any desired accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>求解高维承诺子问题的有限表达式方法</div>
<div class="mono" style="margin-top:8px">过渡路径理论（TPT）是一种量化选定亚稳态A与B之间稀有跃迁事件的数学框架。其核心是承诺子函数，该函数描述了从相空间任意起点出发、在到达A之前首次抵达B的概率。一旦计算出承诺子，即可直接得到跃迁通道和跃迁速率。承诺子是满足特定边界条件的后向柯尔莫哥洛夫方程的解，但在高维空间中求解极具挑战性，因为需要对整个环境空间区域进行网格剖分。本研究探索了有限表达式方法（FEX，Liang与Yang，2022）作为计算承诺子的工具。FEX通过包含固定数量非线性函数和二元算术运算的代数表达式来逼近承诺子，其表达式模板中的最优非线性函数、二元运算及数值系数均通过强化学习确定。基于FEX的承诺子求解器在多个高维基准问题上进行了测试，其结果与基于神经网络的求解器相当或更优。最重要的是，FEX能准确识别解的代数结构，从而将承诺子问题降维处理，并以任意所需精度求解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of solving high-dimensional committor problems in transition path theory, which are essential for quantifying rare transition events but computationally difficult due to the curse of dimensionality, this paper explores the finite expression method (FEX) as an alternative solver. The method approximates the committor function using algebraic expressions with a fixed number of nonlinear functions and binary operations, optimized via reinforcement learning to determine the structure and coefficients. Experimental results on high-dimensional benchmarks show that FEX achieves comparable or superior accuracy to neural network-based solvers and can correctly identify the solution&#x27;s algebraic structure, enabling reduction to low-dimensional problems for arbitrary precision.</div>
<div class="mono" style="margin-top:8px">针对过渡路径理论中高维承诺子问题求解的挑战，该问题因维度灾难而计算困难，本文探索了有限表达式方法作为求解器。该方法通过固定数量的非线性函数和二元运算的代数表达式来近似承诺子函数，并利用强化学习优化表达式结构和系数。在高维基准测试中，实验结果表明该方法取得了与神经网络求解器相当或更优的精度，并能正确识别解的代数结构，从而将问题降至低维以实现任意精度的求解。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforced Inverse Scattering</div>
<div class="meta-line">Authors: Hanyang Jiang, Yuehaw Khoo, Haizhao Yang</div>
<div class="meta-line">First: 2022-06-08T22:56:09+00:00 · Latest: 2026-01-21T01:30:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2206.04186v3">Abs</a> · <a href="https://arxiv.org/pdf/2206.04186v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse wave scattering aims at determining the properties of an object using data on how the object scatters incoming waves. In order to collect information, sensors are put in different locations to send and receive waves from each other. The choice of sensor positions and incident wave frequencies determines the reconstruction quality of scatterer properties. This paper introduces reinforcement learning to develop precision imaging that decides sensor positions and wave frequencies adaptive to different scatterers in an intelligent way, thus obtaining a significant improvement in reconstruction quality with limited imaging resources. Extensive numerical results will be provided to demonstrate the superiority of the proposed method over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化逆散射</div>
<div class="mono" style="margin-top:8px">逆波散射旨在利用物体散射入射波的数据来确定其特性。为收集信息，传感器被置于不同位置以相互发送和接收波。传感器位置与入射波频率的选择决定了散射体特性的重建质量。本文引入强化学习，开发了一种能智能适应不同散射体的自适应传感器位置与波频率决策的精密成像方法，从而在有限成像资源下显著提升重建质量。大量数值结果将验证所提方法相较于现有方法的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge in inverse wave scattering of optimizing limited imaging resources, specifically sensor positions and wave frequencies, to improve the reconstruction quality of scatterer properties. It introduces a reinforcement learning method that adaptively and intelligently selects these parameters based on different scatterers, enabling precision imaging. Experimental results from extensive numerical simulations demonstrate that this approach significantly outperforms existing methods in reconstruction quality.</div>
<div class="mono" style="margin-top:8px">本文的动机是解决逆波散射中如何优化有限的成像资源（如传感器位置和波频率）以提高散射体特性重建质量的挑战。该方法引入强化学习，根据不同散射体自适应地智能选择这些参数，实现精确成像。大量数值实验结果表明，该方法在重建质量上显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability</div>
<div class="meta-line">Authors: Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang</div>
<div class="meta-line">First: 2026-01-14T07:52:14+00:00 · Latest: 2026-01-21T00:48:31+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures. Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09261v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09261v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner&#x27;s own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner&#x27;s internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学会信任经验：不可观测反馈可靠性下的学习监控-信任-调节框架</div>
<div class="mono" style="margin-top:8px">在反馈可靠性不可观测的条件下学习，提出了超越优化鲁棒性的独特挑战：系统必须决定是否从经验中学习，而不仅仅是学习如何稳定学习。我们将此设定研究为不可观测可靠性下的认知可识别性（EIUR），其中每个经验具有潜在可信度，可靠与不可靠反馈在局部可能无法区分，且数据由学习者自身不断演化的信念与行动在闭环中生成。在EIUR中，标准鲁棒学习可能稳定收敛，却形成高置信度但系统错误的信念。
我们提出元认知调节作为实用应对方案：一个内省的二级控制环路，从学习者内部动态的内生证据中推断经验可信度。我们将其形式化为模块化的监控-信任-调节（MTR）分解，并通过自诊断实例化——该方法维护一个缓慢变化的经验信任变量，以软性调节学习更新，无需外部可靠性标签或显式损坏模型。
实证研究表明，在本文探讨的EIUR机制中，自诊断与认知可识别性的提升相关。在强化学习中，它实现了校准后的怀疑态度及在系统损坏奖励下的恢复能力。在监督学习中，它揭示了一个关键分离现象：性能恢复并不等同于认知恢复。准确率可能回升，而内部信念动态仍被早期误导数据锁定，这种失败仅能通过内省诊断检测。MTR与自诊断共同为不可观测可靠性下的自主学习提供了内在可靠性评估的组织抽象与具体设计模板。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of learning when feedback reliability is unobservable, a problem termed Epistemic Identifiability under Unobservable Reliability (EIUR), where standard robust methods can converge to confidently wrong beliefs. The proposed solution is a metacognitive Monitor-Trust-Regulator (MTR) framework, which uses introspective self-diagnosis to infer experience credibility from the learner&#x27;s internal dynamics, softly modulating updates without external labels. Experimental results in reinforcement and supervised learning show that this approach improves epistemic identifiability, enabling calibrated skepticism and revealing that performance recovery can mask persistent erroneous beliefs, detectable only through introspective diagnostics.</div>
<div class="mono" style="margin-top:8px">本文研究了在反馈可靠性不可观测情况下的学习挑战，即不可观测可靠性的认知可识别性问题，其中标准鲁棒方法可能稳定地收敛到错误信念。提出的解决方案是一个元认知的监控-信任-调节器框架，通过内省式自我诊断从学习者的内部动态推断经验可信度，无需外部标签即可柔和调节更新。在强化学习和监督学习中的实验结果表明，该方法提高了认知可识别性，实现了校准的怀疑态度，并揭示性能恢复可能掩盖持续的错误信念，这些只有通过内省诊断才能检测到。</div>
</details>
</div>
<div class="card">
<div class="title">Deceptive Sequential Decision-Making via Regularized Policy Optimization</div>
<div class="meta-line">Authors: Yerin Kim, Alexander Benvenuti, Bo Chen, Mustafa Karabag, Abhishek Kulkarni, Nathaniel D. Bastian, Ufuk Topcu, Matthew Hale</div>
<div class="meta-line">First: 2025-01-30T23:41:40+00:00 · Latest: 2026-01-21T00:03:43+00:00</div>
<div class="meta-line">Comments: 18 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.18803v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.18803v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous systems are increasingly expected to operate in the presence of adversaries, though adversaries may infer sensitive information simply by observing a system. Therefore, present a deceptive sequential decision-making framework that not only conceals sensitive information, but actively misleads adversaries about it. We model autonomous systems as Markov decision processes, with adversaries using inverse reinforcement learning to recover reward functions. To counter them, we present three regularization strategies for policy synthesis problems that actively deceive an adversary about a system&#x27;s reward. ``Diversionary deception&#x27;&#x27; leads an adversary to draw any false conclusion about the system&#x27;s reward function. ``Targeted deception&#x27;&#x27; leads an adversary to draw a specific false conclusion about the system&#x27;s reward function. ``Equivocal deception&#x27;&#x27; leads an adversary to infer that the real reward and a false reward both explain the system&#x27;s behavior. We show how each form of deception can be implemented in policy optimization problems and analytically bound the loss in total accumulated reward induced by deception. Next, we evaluate these developments in a multi-agent setting. We show that diversionary, targeted, and equivocal deception all steer the adversary to false beliefs while still attaining a total accumulated reward that is at least 98% of its optimal, non-deceptive value.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于正则化策略优化的欺骗性序贯决策</div>
<div class="mono" style="margin-top:8px">自主系统日益需要在对抗环境中运行，而对手仅通过观察系统即可推断敏感信息。为此，本文提出一种欺骗性序贯决策框架，不仅能隐藏敏感信息，还能主动误导对手对信息的认知。我们将自主系统建模为马尔可夫决策过程，假设对手使用逆强化学习来恢复奖励函数。为应对此威胁，我们提出三种用于策略合成问题的正则化策略，可主动误导对手对系统奖励的认知：&#x27;转移性欺骗&#x27;使对手对系统奖励函数得出任意错误结论；&#x27;定向欺骗&#x27;使对手对系统奖励函数得出特定错误结论；&#x27;模糊性欺骗&#x27;使对手推断真实奖励与虚假奖励均能解释系统行为。我们展示了每种欺骗形式在策略优化问题中的实现方法，并通过解析界定了欺骗行为导致的总累积奖励损失。随后在多智能体环境中评估这些方法，结果表明转移性、定向性和模糊性欺骗均能引导对手形成错误信念，同时使总累积奖励至少达到最优非欺骗性值的98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for autonomous systems to protect sensitive information from adversarial inference, this paper introduces a deceptive sequential decision-making framework that actively misleads adversaries about a system&#x27;s true objectives. The method models systems as Markov decision processes and counters adversaries using inverse reinforcement learning by incorporating three regularization strategies into policy optimization: diversionary, targeted, and equivocal deception, each designed to manipulate adversary beliefs differently. Experimental results demonstrate that all three deception strategies successfully steer adversaries to false conclusions while maintaining the system&#x27;s total accumulated reward at least 98% of its optimal non-deceptive value, validated in a multi-agent setting.</div>
<div class="mono" style="margin-top:8px">本文针对自主系统需保护敏感信息免受对手推断的问题，提出了一种欺骗性序贯决策框架，旨在主动误导对手对系统真实目标的认知。方法将系统建模为马尔可夫决策过程，并通过在策略优化中引入三种正则化策略（分散性、目标性和模糊性欺骗）来应对使用逆强化学习的对手，以不同方式操纵其信念。实验结果表明，在多智能体环境中，所有三种欺骗策略均能成功引导对手形成错误结论，同时使系统累计奖励保持在至少98%的最优非欺骗性水平。</div>
</details>
</div>
<div class="card">
<div class="title">Report for NSF Workshop on AI for Electronic Design Automation</div>
<div class="meta-line">Authors: Deming Chen, Vijay Ganesh, Weikai Li, Yingyan, Lin, Yong Liu, Subhasish Mitra, David Z. Pan, Ruchir Puri, Jason Cong, Yizhou Sun</div>
<div class="meta-line">First: 2026-01-20T23:45:40+00:00 · Latest: 2026-01-20T23:45:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14541v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14541v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ai4eda-workshop.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>美国国家科学基金会人工智能赋能电子设计自动化研讨会报告</div>
<div class="mono" style="margin-top:8px">本报告提炼了2024年12月10日于温哥华NeurIPS 2024同期举办的美国国家科学基金会人工智能赋能电子设计自动化（EDA）研讨会的讨论与建议。研讨会汇聚机器学习与EDA领域的专家，探讨了人工智能技术——涵盖大语言模型（LLMs）、图神经网络（GNNs）、强化学习（RL）、神经符号方法等——如何促进EDA流程并缩短设计周期。会议围绕四大主题展开：（1）面向物理综合与制造设计（DFM）的人工智能，探讨物理制造工艺的挑战及潜在AI应用；（2）面向高层次与逻辑级综合（HLS/LLS）的人工智能，涵盖编译指示插入、程序转换、RTL代码生成等；（3）用于优化与设计的人工智能工具箱，讨论可能应用于EDA任务的前沿AI进展；（4）面向测试与验证的人工智能，包括LLM辅助验证工具、机器学习增强的SAT求解、安全性与可靠性挑战等。报告建议美国国家科学基金会应促进AI/EDA跨领域合作，投资EDA基础性AI研究，建设稳健的数据基础设施，推广可扩展的计算资源，并加强人才培养以普及硬件设计能力、赋能下一代硬件系统。研讨会详情可通过网站 https://ai4eda-workshop.github.io/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This report summarizes the NSF Workshop on AI for Electronic Design Automation (EDA), motivated by the need to leverage AI technologies like large language models, graph neural networks, and reinforcement learning to accelerate electronic design processes and reduce turnaround time. The method involved expert discussions organized around four key themes: AI for physical synthesis and manufacturing, AI for high-level and logic synthesis, AI toolboxes for optimization, and AI for test and verification. The main experimental results are not applicable as this is a workshop report, but the outcomes include concrete recommendations for NSF to foster AI-EDA collaboration, invest in foundational AI research, develop data and compute infrastructures, and support workforce development to democratize hardware design and enable next-generation systems.</div>
<div class="mono" style="margin-top:8px">本报告总结了美国国家科学基金会（NSF）关于人工智能用于电子设计自动化（EDA）的研讨会，其动机在于利用大型语言模型、图神经网络和强化学习等人工智能技术来加速电子设计流程并缩短周转时间。方法包括围绕四个主题组织专家讨论：用于物理合成和制造的人工智能、用于高层次和逻辑合成的人工智能、用于优化的人工智能工具箱，以及用于测试和验证的人工智能。由于这是一份研讨会报告，未涉及具体实验，但主要成果包括向NSF提出的具体建议，如促进人工智能与EDA的合作、投资于基础人工智能研究、开发数据和计算基础设施，并支持人才培养，以普及硬件设计并推动下一代系统的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Execution-Grounded Automated AI Research</div>
<div class="meta-line">Authors: Chenglei Si, Zitong Yang, Yejin Choi, Emmanuel Candès, Diyi Yang, Tatsunori Hashimoto</div>
<div class="meta-line">First: 2026-01-20T22:35:44+00:00 · Latest: 2026-01-20T22:35:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14525v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14525v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向执行驱动的自动化人工智能研究</div>
<div class="mono" style="margin-top:8px">自动化人工智能研究在加速科学发现方面潜力巨大，但当前大型语言模型常生成看似合理却无效的构想。执行驱动可能有助于解决此问题，但自动化执行的可行性及模型能否从执行反馈中学习尚不明确。为此，我们首先构建了自动化执行器来实施构想，并通过大规模并行GPU实验验证其有效性；随后将LLM预训练与后训练两个现实研究问题转化为执行环境，证明该执行器能实现前沿LLM生成的大部分构想。我们分析了两种从执行反馈中学习的方法：进化搜索与强化学习。执行引导的进化搜索具备样本高效性——仅用十个搜索周期，即在后训练任务中发现显著超越GRPO基线的方法（69.4%对48.0%），在预训练任务中发现优于nanoGPT基线的方案（19.7分钟对35.9分钟）。前沿LLM在搜索中常生成有意义的算法构想，但易早期饱和且仅偶尔呈现扩展趋势。而基于执行奖励的强化学习则遭遇模式坍塌：虽能提升构想模型的平均奖励，却因模型收敛于简单构想而未能突破上限。我们通过对执行构想与训练动态的深入分析，为未来执行驱动的自动化AI研究提供支撑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates automated AI research, motivated by the need to move beyond large language models (LLMs) generating plausible but ineffective ideas by grounding them in actual execution. The method involves building an automated executor to implement AI research ideas—specifically for LLM pre-training and post-tuning problems—and test them via large-scale parallel experiments, then applying evolutionary search and reinforcement learning to learn from the execution feedback. Key experimental results show that execution-guided evolutionary search is sample-efficient, significantly outperforming baselines in both post-training (69.4% vs 48.0%) and pre-training (19.7 vs 35.9 minutes) within ten epochs, while reinforcement learning suffers from mode collapse, improving average reward but not the upper-bound performance as models converge on simpler ideas.</div>
<div class="mono" style="margin-top:8px">本文研究自动化AI科研，其动机是当前大语言模型（LLM）常生成看似合理但无效的想法，需通过实际执行来加以验证。方法上，研究者构建了一个自动化执行器，用于实现针对LLM预训练和后训练的研究想法，并通过大规模并行实验进行测试，进而采用进化搜索和强化学习从执行反馈中学习。主要实验结果表明，执行引导的进化搜索具有样本高效性，在十个搜索周期内，后训练性能显著超越GRPO基线（69.4%对48.0%），预训练配方也优于nanoGPT基线（19.7分钟对35.9分钟）；而基于执行奖励的强化学习则出现模式崩溃，虽提升了想法生成模型的平均奖励，却因模型收敛于简单想法而未能提高性能上限。</div>
</details>
</div>
<div class="card">
<div class="title">Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree</div>
<div class="meta-line">Authors: Leyi Zhao, Weijie Huang, Yitong Guo, Jiang Bian, Chenghong Wang, Xuhong Zhang</div>
<div class="meta-line">First: 2026-01-20T22:32:52+00:00 · Latest: 2026-01-20T22:32:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14523v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14523v1">PDF</a> · <a href="https://github.com/annihi1ation/phylo_evolve">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的系统发育树进化式代码优化</div>
<div class="mono" style="margin-top:8px">为现代GPU优化科学计算算法是一个劳动密集且迭代的过程，涉及在复杂的软硬件栈中反复进行代码修改、基准测试和调优。近期研究探索了基于大语言模型（LLM）辅助的进化式自动代码优化方法，但这些方法主要依赖结果导向的选择和随机突变，未能充分利用迭代优化过程中产生的丰富轨迹信息。我们提出PhyloEvolve系统，这是一个将面向GPU的算法优化重构为上下文强化学习（ICRL）问题的LLM智能体框架。该框架支持基于优化轨迹的条件化经验复用，无需模型重新训练。PhyloEvolve将算法蒸馏与基于提示的决策变换器集成至迭代工作流，将算法修改序列与性能反馈作为首要学习信号。为组织优化历史，我们引入系统发育树表示法，用以捕捉算法变体间的继承、分化和重组，实现回溯追踪、跨谱系迁移与结果复现。该系统融合精英轨迹池、多岛并行探索和容器化执行机制，以在异构硬件上平衡探索与利用效率。我们在偏微分方程求解器、流形学习和谱图算法等科学计算任务上评估PhyloEvolve，其相较于基准方法和传统进化方法，在运行效率、内存利用率和正确性方面均展现出持续改进。代码已发布于：https://github.com/annihi1ation/phylo_evolve</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to automate the labor-intensive process of optimizing scientific computing algorithms for modern GPUs, addressing the limitations of existing LLM-assisted evolutionary methods that underutilize trajectory information. The proposed method, PhyloEvolve, reframes code optimization as an In-Context Reinforcement Learning problem, integrating Algorithm Distillation and prompt-based Decision Transformers to leverage sequences of modifications and performance feedback as learning signals, while a phylogenetic tree organizes optimization history for backtracking and cross-lineage transfer. Experimental results on workloads like PDE solvers and spectral graph algorithms show consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于自动化针对现代GPU的科学计算算法优化过程，该过程通常耗时且依赖人工迭代，现有基于大语言模型的进化方法未能充分利用优化轨迹信息。提出的方法PhyloEvolve将代码优化重构为上下文强化学习问题，结合算法蒸馏和基于提示的决策变换器，将算法修改序列和性能反馈作为学习信号，并引入系统发育树来组织优化历史以支持回溯和跨谱系迁移。在偏微分方程求解器和谱图算法等科学计算任务上的实验结果表明，该方法在运行时间、内存效率和正确性方面均优于基准方法和传统进化方法。</div>
</details>
</div>
<div class="card">
<div class="title">Marvel: Accelerating Safe Online Reinforcement Learning with Finetuned Offline Policy</div>
<div class="meta-line">Authors: Keru Chen, Honghao Wei, Zhigang Deng, Sen Lin</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research (TMLR), 2026</div>
<div class="meta-line">First: 2024-12-05T18:51:18+00:00 · Latest: 2026-01-20T20:06:01+00:00</div>
<div class="meta-line">Comments: Accepted by Transactions on Machine Learning Research (TMLR), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.04426v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.04426v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The high costs and risks involved in extensive environment interactions hinder the practical application of current online safe reinforcement learning (RL) methods. While offline safe RL addresses this by learning policies from static datasets, the performance therein is usually limited due to reliance on data quality and challenges with out-of-distribution (OOD) actions. Inspired by recent successes in offline-to-online (O2O) RL, it is crucial to explore whether offline safe RL can be leveraged to facilitate faster and safer online policy learning, a direction that has yet to be fully investigated. To fill this gap, we first demonstrate that naively applying existing O2O algorithms from standard RL would not work well in the safe RL setting due to two unique challenges: \emph{erroneous Q-estimations}, resulted from offline-online objective mismatch and offline cost sparsity, and \emph{Lagrangian mismatch}, resulted from difficulties in aligning Lagrange multipliers between offline and online policies. To address these challenges, we introduce \textbf{Marvel}, a novel framework for O2O safe RL, comprising two key components that work in concert: \emph{Value Pre-Alignment} to align the Q-functions with the underlying truth before online learning, and \emph{Adaptive PID Control} to effectively adjust the Lagrange multipliers during online finetuning. Extensive experiments demonstrate that Marvel significantly outperforms existing baselines in both reward maximization and safety constraint satisfaction. By introducing the first policy-finetuning based framework for O2O safe RL, which is compatible with many offline and online safe RL methods, our work has the great potential to advance the field towards more efficient and practical safe RL solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Marvel：基于离线策略微调的在线安全强化学习加速框架</div>
<div class="mono" style="margin-top:8px">当前在线安全强化学习方法因需要大量环境交互而面临高成本和高风险，限制了实际应用。离线安全强化学习虽通过静态数据集学习策略规避了交互需求，但其性能受限于数据质量和对分布外动作的处理难题。受离线到在线强化学习近期进展的启发，探索如何利用离线安全强化学习实现更快速、更安全的在线策略学习成为关键方向，而这一方向尚未得到充分研究。为填补空白，本文首先揭示直接套用标准强化学习的离线到在线算法在安全强化学习场景中效果不佳的两大挑战：因离线-在线目标错位和离线成本稀疏性导致的\emph{Q值估计偏差}，以及因离线与在线策略拉格朗日乘子难以对齐导致的\emph{拉格朗日失配}。针对这些挑战，我们提出\textbf{Marvel}——一种创新的离线到在线安全强化学习框架，包含两个协同工作的核心组件：在线学习前对齐Q函数与真实值的\emph{价值预对齐}机制，以及在线微调期间动态调整拉格朗日乘子的\emph{自适应PID控制}模块。大量实验表明，Marvel在奖励最大化和安全约束满足方面均显著优于现有基线方法。作为首个基于策略微调的离线到在线安全强化学习框架，Marvel兼容多种离线与在线安全强化学习方法，有望推动该领域向更高效、更实用的安全强化学习解决方案发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the high cost and risk of online safe reinforcement learning (RL) by proposing a framework that leverages offline safe RL to accelerate safer online policy learning, a direction previously underexplored. The method, named Marvel, introduces two components to overcome challenges in offline-to-online safe RL: Value Pre-Alignment corrects erroneous Q-estimations from offline data, and Adaptive PID Control adjusts Lagrange multipliers to handle mismatches between offline and online policies. Experimental results show that Marvel outperforms existing baselines in both reward maximization and safety constraint satisfaction, offering a compatible framework for efficient and practical safe RL solutions.</div>
<div class="mono" style="margin-top:8px">本文针对在线安全强化学习成本高、风险大的问题，提出利用离线安全强化学习来加速更安全的在线策略学习，这是一个先前未充分探索的方向。该方法名为Marvel，通过两个核心组件解决离线到在线安全强化学习的挑战：价值预对齐纠正离线数据导致的Q值估计错误，自适应PID控制调整拉格朗日乘数以处理离线与在线策略间的失配。实验结果表明，Marvel在奖励最大化和安全约束满足方面均显著优于现有基线，为高效实用的安全强化学习提供了一个兼容性强的框架。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Large Language Models for Black-Box Optimization</div>
<div class="meta-line">Authors: Ye Yuan, Can, Chen, Zipeng Sun, Dinghuai Zhang, Christopher Pal, Xue Liu</div>
<div class="meta-line">First: 2026-01-20T19:59:29+00:00 · Latest: 2026-01-20T19:59:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14446v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14446v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline black-box optimization (BBO) aims to find optimal designs based solely on an offline dataset of designs and their labels. Such scenarios frequently arise in domains like DNA sequence design and robotics, where only a few labeled data points are available. Traditional methods typically rely on task-specific proxy or generative models, overlooking the in-context learning capabilities of pre-trained large language models (LLMs). Recent efforts have adapted autoregressive LLMs to BBO by framing task descriptions and offline datasets as natural language prompts, enabling direct design generation. However, these designs often contain bidirectional dependencies, which left-to-right models struggle to capture. In this paper, we explore diffusion LLMs for BBO, leveraging their bidirectional modeling and iterative refinement capabilities. This motivates our in-context denoising module: we condition the diffusion LLM on the task description and the offline dataset, both formatted in natural language, and prompt it to denoise masked designs into improved candidates. To guide the generation toward high-performing designs, we introduce masked diffusion tree search, which casts the denoising process as a step-wise Monte Carlo Tree Search that dynamically balances exploration and exploitation. Each node represents a partially masked design, each denoising step is an action, and candidates are evaluated via expected improvement under a Gaussian Process trained on the offline dataset. Our method, dLLM, achieves state-of-the-art results in few-shot settings on design-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散大语言模型用于黑盒优化</div>
<div class="mono" style="margin-top:8px">离线黑盒优化（BBO）旨在仅基于设计及其标签的离线数据集寻找最优设计。此类场景常见于DNA序列设计和机器人学等领域，其中仅有少量标注数据可用。传统方法通常依赖任务特定的代理或生成模型，忽视了预训练大语言模型（LLM）的上下文学习能力。近期研究通过将任务描述和离线数据集构建为自然语言提示，将自回归LLM适配至BBO任务，实现直接设计生成。然而，这些设计常包含双向依赖关系，而左至右模型难以有效捕捉。本文探索将扩散LLM应用于BBO，利用其双向建模和迭代优化能力。由此提出上下文去噪模块：将任务描述和离线数据集（均以自然语言格式化）作为扩散LLM的条件输入，引导其将掩码设计去噪为改进候选方案。为引导生成高性能设计，引入掩码扩散树搜索，将去噪过程构建为逐步蒙特卡洛树搜索，动态平衡探索与利用。每个节点表示部分掩码设计，每次去噪步骤视为动作，候选方案通过基于离线数据集训练的高斯过程下的期望改进进行评估。我们的方法dLLM在设计基准测试的少样本场景中取得了最先进的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses offline black-box optimization (BBO) in data-scarce domains like DNA sequence design, where traditional methods overlook the in-context learning of large language models (LLMs). It proposes dLLM, a method that leverages diffusion LLMs for bidirectional modeling and iterative refinement, introducing an in-context denoising module conditioned on natural language task descriptions and datasets, along with a masked diffusion tree search to balance exploration and exploitation via Gaussian Process-based evaluation. Experimental results show that dLLM achieves state-of-the-art performance in few-shot settings on the design-bench benchmark.</div>
<div class="mono" style="margin-top:8px">本文针对DNA序列设计等数据稀缺领域的离线黑盒优化问题，传统方法常忽略大语言模型的上下文学习能力。研究提出dLLM方法，利用扩散大语言模型进行双向建模和迭代优化，引入基于自然语言任务描述和数据集的上下文去噪模块，并结合掩码扩散树搜索，通过高斯过程评估平衡探索与利用。实验结果表明，dLLM在design-bench基准测试的少样本设置中取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow</div>
<div class="meta-line">Authors: Haocheng Xi, Charlie Ruan, Peiyuan Liao, Yujun Lin, Han Cai, Yilong Zhao, Shuo Yang, Kurt Keutzer, Song Han, Ligeng Zhu</div>
<div class="meta-line">First: 2026-01-20T18:54:31+00:00 · Latest: 2026-01-20T18:54:31+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14243v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Jet-RL：通过统一的训练与推演精度流实现基于策略的FP8强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）对于提升大语言模型（LLMs）的复杂推理能力至关重要。然而，现有RL训练流程计算效率低下且资源密集，其中推演阶段占总训练时间的70%以上。量化RL训练，特别是使用FP8精度，为解决这一瓶颈提供了有前景的途径。一种常用策略是在推演阶段采用FP8精度，同时在训练阶段保留BF16精度。本研究首次对FP8 RL训练进行了全面分析，并证明广泛采用的BF16训练+FP8推演策略在长序列推演和复杂任务下存在严重的训练不稳定性和灾难性精度崩溃。分析表明，这些失败源于该方法的离策略特性，导致训练与推理之间存在显著的数值失配。基于此，我们提出Jet-RL——一个能够实现稳健稳定RL优化的FP8训练框架。其核心思想是采用统一的FP8精度流同时处理训练与推演，从而最小化数值差异并消除低效的步间校准需求。大量实验验证了Jet-RL的有效性：相较于BF16训练，本方法在推演阶段最高加速33%，训练阶段最高加速41%，端到端整体加速16%，同时在所有设置下保持稳定收敛，且精度损失可忽略不计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational inefficiency of reinforcement learning (RL) for large language models, where the rollout phase dominates training time. The authors identify that a common strategy of using BF16 precision for training and FP8 for rollout causes instability and accuracy collapse due to numerical mismatches from its off-policy nature. They propose Jet-RL, a framework that employs a unified FP8 precision flow for both training and rollout to minimize discrepancies and eliminate calibration overhead. Experimental results show that Jet-RL achieves up to 33% rollout speedup, 41% training speedup, and 16% end-to-end speedup over BF16 training while maintaining stable convergence with negligible accuracy loss.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型的强化学习训练中计算效率低、推演阶段耗时过长的问题展开研究。作者发现，当前广泛采用的BF16精度训练结合FP8精度推演的策略，由于其离策略性质导致数值不匹配，在长序列和复杂任务中会引起严重的训练不稳定和精度崩溃。为此，他们提出了Jet-RL框架，采用统一的FP8精度流同时进行训练和推演，以最小化数值差异并避免低效的步间校准。实验结果表明，Jet-RL在推演阶段实现了高达33%的加速，训练阶段加速达41%，整体端到端速度比BF16训练提升16%，同时保持了稳定的收敛性且精度损失可忽略不计。</div>
</details>
</div>
<div class="card">
<div class="title">Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression</div>
<div class="meta-line">Authors: Shaurya Mathur, Shreyas Bellary Manjunath, Nitin Kulkarni, Alina Vereshchaka</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-20T18:50:12+00:00 · Latest: 2026-01-20T18:50:12+00:00</div>
<div class="meta-line">Comments: 6 pages, 5 figures (two of them in tables), Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14238v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/firecastrl">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时空野火预测与直升机灭火强化学习抑制策略</div>
<div class="mono" style="margin-top:8px">野火发生频率和强度日益增加，不仅破坏生态系统和社区，每年在美国还造成数十亿美元的灭火成本和经济损失。传统野火管理多为被动响应，仅在火灾被发现后才采取行动。本文提出《FireCastRL》——一种结合野火预测与智能抑制策略的主动式人工智能框架。该框架首先采用深度时空模型预测野火起火点，针对高风险预测结果，部署预训练的强化学习智能体在物理信息三维模拟环境中指挥直升机灭火单元执行实时抑制战术。框架生成的威胁评估报告可协助应急响应人员优化资源调配与规划。此外，我们公开了一个包含950万条环境变量样本的大规模时空数据集用于野火预测。本研究展示了深度学习与强化学习如何协同支持野火预测与战术响应。更多细节详见：https://sites.google.com/view/firecastrl。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing frequency and devastating impact of wildfires, which are often managed reactively, this paper introduces FireCastRL, a proactive AI framework that integrates deep spatiotemporal modeling for wildfire ignition prediction with reinforcement learning (RL) for tactical suppression. The method employs a deep spatiotemporal model to forecast ignitions and, for high-risk cases, deploys a pre-trained RL agent within a physics-informed 3D simulation to execute real-time helitack suppression strategies, generating threat assessment reports to aid emergency responders. Experimental results include the creation of a large-scale dataset with 9.5 million environmental samples and a demonstration that the combined deep learning and RL approach effectively supports both forecasting and tactical response in wildfire management.</div>
<div class="mono" style="margin-top:8px">本文针对日益频繁且破坏性强的野火问题，传统应对方式多为被动响应，提出了FireCastRL这一主动式人工智能框架，将用于野火起火预测的深度时空建模与强化学习的战术扑灭相结合。该方法首先利用深度时空模型预测野火起火点，针对高风险预测，在基于物理的3D模拟中部署预训练的强化学习智能体执行实时直升机灭火战术，并生成威胁评估报告以协助应急人员。主要实验结果包括发布了一个包含950万个环境变量样本的大规模时空数据集，并展示了深度学习与强化学习结合的方法能有效支持野火的预测和战术响应。</div>
</details>
</div>
<div class="card">
<div class="title">Q-learning with Adjoint Matching</div>
<div class="meta-line">Authors: Qiyang Li, Sergey Levine</div>
<div class="meta-line">First: 2026-01-20T18:45:34+00:00 · Latest: 2026-01-20T18:45:34+00:00</div>
<div class="meta-line">Comments: 32 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14234v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14234v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic&#x27;s action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伴随匹配Q学习</div>
<div class="mono" style="margin-top:8px">我们提出伴随匹配Q学习（QAM），这是一种基于时序差分的新型强化学习算法，解决了连续动作强化学习中长期存在的挑战：针对参数化Q函数高效优化表达能力强的扩散或流匹配策略。有效优化需要利用评论家的一阶信息，但对流或扩散策略而言，通过多步去噪过程进行基于梯度的反向传播优化存在数值不稳定性。现有方法要么仅使用价值函数而丢弃梯度信息，要么依赖近似方法牺牲策略表达能力或引入偏差。QAM通过运用生成建模中最新提出的伴随匹配技术，将评论家的动作梯度转化为逐步目标函数，既避免了不稳定的反向传播，又在最优解处提供无偏且表达能力强的策略。结合评论家学习的时序差分更新，QAM在离线和离线转在线强化学习的困难稀疏奖励任务中持续超越现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of efficiently optimizing expressive diffusion or flow-matching policies in continuous-action reinforcement learning, where direct gradient-based optimization through multi-step denoising processes is numerically unstable. The proposed method, Q-learning with Adjoint Matching (QAM), leverages adjoint matching to transform the critic&#x27;s action gradient into a step-wise objective, avoiding unstable backpropagation while maintaining an unbiased and expressive policy. Experimental results demonstrate that QAM consistently outperforms prior methods on hard, sparse reward tasks in both offline and offline-to-online RL settings.</div>
<div class="mono" style="margin-top:8px">本文针对连续动作强化学习中高效优化扩散或流匹配策略的挑战，其中通过多步去噪过程的直接梯度优化存在数值不稳定性。提出的方法——伴随匹配Q学习（QAM）利用伴随匹配技术，将评论者的动作梯度转化为逐步目标函数，避免了不稳定的反向传播，同时保持了无偏且表达性强的策略。实验结果表明，在离线和离线到在线强化学习的困难稀疏奖励任务上，QAM一致优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning</div>
<div class="meta-line">Authors: Egor Cherepanov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov</div>
<div class="meta-line">First: 2026-01-20T18:44:28+00:00 · Latest: 2026-01-20T18:44:28+00:00</div>
<div class="meta-line">Comments: 38 pages, 44 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://avanturist322.github.io/KAGEBench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAGE-Bench：面向强化学习的快速已知轴视觉泛化评估基准</div>
<div class="mono" style="margin-top:8px">基于像素的强化学习智能体常在纯视觉分布偏移下失效，即使潜在动态与奖励机制未变，但现有基准常混杂多种偏移源，阻碍系统分析。我们提出KAGE-Env——一个基于JAX的2D平台环境，将观测过程分解为独立可控的视觉轴，同时保持底层控制问题不变。通过结构设计，改变视觉轴仅通过像素策略引发的状态条件动作分布影响性能，为视觉泛化提供清晰抽象框架。基于此环境，我们构建KAGE-Bench基准，包含6个已知轴测试套件共34组训练-评估配置对，可隔离单一视觉偏移。采用标准PPO-CNN基线实验发现：背景与光度偏移常导致任务完全失败，而智能体外貌偏移影响相对轻微；部分偏移能维持前进动作却破坏任务完成，表明仅凭回报值可能掩盖泛化缺陷。全向量化JAX实现支持单GPU每秒3300万环境步长，实现视觉因素的高效可复现扫描。代码：https://avanturist322.github.io/KAGEBench/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need to isolate and systematically evaluate visual generalization failures in pixel-based reinforcement learning, as existing benchmarks often conflate multiple sources of distribution shift. The method introduces KAGE-Env, a JAX-native 2D platformer that factorizes visual axes to independently control visual shifts while keeping the underlying dynamics fixed, and KAGE-Bench, a benchmark with six suites of 34 configuration pairs to isolate individual shifts. Experimental results using a PPO-CNN baseline reveal strong axis-dependent performance drops, with background and photometric shifts causing severe failures while agent-appearance shifts are less harmful, and show that return metrics can mask generalization issues; the implementation achieves up to 33M steps per second on a single GPU for fast evaluation.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要分离并系统评估基于像素的强化学习中的视觉泛化失败，因为现有基准常混淆多种分布偏移源。方法上提出了KAGE-Env，这是一个JAX原生的2D平台游戏环境，通过分解视觉轴来独立控制视觉变化同时保持底层动态不变，并构建了KAGE-Bench基准，包含六个套件共34个配置对以隔离个体偏移。使用PPO-CNN基线的实验结果表明，性能下降强烈依赖于视觉轴类型，背景和光度偏移常导致严重失败而智能体外观偏移相对温和，同时揭示回报指标可能掩盖泛化问题；该实现可在单GPU上达到每秒3300万步的速度，支持快速评估。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment</div>
<div class="meta-line">Authors: Punit Kumar, Vaibhav Saran, Divyesh Patel, Nitin Kulkarni, Alina Vereshchaka</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-20T18:41:44+00:00 · Latest: 2026-01-20T18:41:44+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14228v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14228v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力机制的离线强化学习与聚类在可解释性脓毒症治疗中的应用</div>
<div class="mono" style="margin-top:8px">脓毒症仍是重症监护病房的主要致死原因之一，及时准确的治疗决策对患者预后至关重要。本研究提出一个可解释的决策支持框架，系统集成四大核心模块：（1）基于聚类的分层模块，通过统计验证的聚类方法在患者入住ICU时将其划分为低、中、高风险组；（2）合成数据增强流程，利用变分自编码器（VAE）和扩散模型扩充液体输注或血管加压药使用等低代表性治疗轨迹；（3）离线强化学习智能体，采用优势加权回归（AWR）算法训练，配备轻量级注意力编码器，并集成保守型安全感知治疗推荐模型；（4）多模态大语言模型驱动的原理生成模块，基于临床语境和检索的专家知识生成自然语言治疗依据。在MIMIC-III和eICU数据集上的评估表明，该方法在实现高治疗准确率的同时，能为临床医生提供可解释且稳健的决策建议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high mortality of sepsis in ICUs and the need for interpretable clinical decision support, this paper proposes a framework integrating patient risk stratification via clustering, synthetic data augmentation using VAE and diffusion models for underrepresented treatments, an offline RL agent with AWR and attention for safety-aware recommendations, and an LLM-based rationale generator. Experimental results on MIMIC-III and eICU datasets demonstrate that the approach achieves high treatment accuracy while delivering interpretable, robust policy recommendations to clinicians.</div>
<div class="mono" style="margin-top:8px">针对ICU中脓毒症高死亡率及可解释临床决策支持的需求，本文提出一个集成框架，包括基于聚类的患者风险分层、使用VAE和扩散模型的合成数据增强以丰富少数治疗轨迹、采用AWR和注意力机制的离线强化学习智能体提供安全感知建议，以及基于多模态大语言模型的理由生成模块。在MIMIC-III和eICU数据集上的实验结果表明，该方法实现了高治疗准确性，同时为临床医生提供可解释且稳健的策略推荐。</div>
</details>
</div>
<div class="card">
<div class="title">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</div>
<div class="meta-line">Authors: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</div>
<div class="meta-line">First: 2025-12-19T17:55:48+00:00 · Latest: 2026-01-20T18:25:48+00:00</div>
<div class="meta-line">Comments: 28 pages, 25 figures. The first four authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17853v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17853v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyTask：面向仿真到现实策略学习进阶的自动化任务与数据生成框架</div>
<div class="mono" style="margin-top:8px">通用机器人学习仍受数据制约：大规模、多样化且高质量的交互数据在现实世界中采集成本高昂。仿真虽已成为扩展数据采集的有效途径，但相关任务——包括仿真任务设计、任务感知场景生成、专家示范合成及仿真到现实迁移——仍需大量人工投入。本文提出AnyTask，一个结合大规模并行GPU仿真与基础模型的自动化框架，用于设计多样化操作任务并合成机器人数据。我们引入三种AnyTask智能体以生成解决尽可能多任务的专家示范：1) ViPR，一种集成视觉语言模型循环并行优化的新型任务与运动规划智能体；2) ViPR-Eureka，一种结合生成密集奖励与大型语言模型引导接触采样的强化学习智能体；3) ViPR-RL，一种联合稀疏奖励下生成高质量示范的混合规划与学习方法。基于生成数据训练行为克隆策略，在仿真中验证后直接部署于真实机器人硬件。该策略能泛化至新物体位姿，在真实世界抓放、抽屉开启、密集接触推动及长时程操作任务套件中平均成功率可达44%。项目网站：https://anytask.rai-inst.com</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the data scarcity problem in generalist robot learning by introducing AnyTask, an automated framework that leverages GPU-accelerated simulation and foundation models to generate diverse manipulation tasks and corresponding robot interaction data. The method employs three specialized agents—ViPR for task and motion planning, ViPR-Eureka for reinforcement learning with dense rewards, and ViPR-RL for hybrid planning with sparse rewards—to synthesize expert demonstrations efficiently. Experimental results show that behavior cloning policies trained on this generated data achieve a 44% average success rate when directly deployed on real robots, generalizing to novel object poses across tasks like pick-and-place, drawer opening, and contact-rich pushing.</div>
<div class="mono" style="margin-top:8px">本文针对通用机器人学习中的数据稀缺问题，提出了AnyTask自动化框架，该框架利用GPU加速仿真和基础模型来生成多样化的操作任务及相应的机器人交互数据。方法采用三种专用智能体——用于任务与运动规划的ViPR、基于密集奖励的强化学习智能体ViPR-Eureka，以及基于稀疏奖励的混合规划学习智能体ViPR-RL——以高效合成专家示范。实验结果表明，基于生成数据训练的行为克隆策略在直接部署到真实机器人上时，平均成功率可达44%，并能泛化至新物体位姿，适用于抓放、开抽屉、接触式推动等任务。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Score-Threshold Optimization for Interpretable Risk Assessment</div>
<div class="meta-line">Authors: Fardin Gankhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Kimia Ghobadi</div>
<div class="meta-line">First: 2025-10-24T18:07:24+00:00 · Latest: 2026-01-20T18:20:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21934v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21934v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Risk assessment tools in healthcare commonly employ point-based scoring systems that map patients to ordinal risk categories via thresholds. While electronic health record (EHR) data presents opportunities for data-driven optimization of these tools, two fundamental challenges impede standard supervised learning: (1) labels are often available only for extreme risk categories due to intervention-censored outcomes, and (2) misclassification cost is asymmetric and increases with ordinal distance. We propose a mixed-integer programming (MIP) framework that jointly optimizes scoring weights and category thresholds in the face of these challenges. Our approach prevents label-scarce category collapse via threshold constraints, and utilizes an asymmetric, distance-aware objective. The MIP framework supports governance constraints, including sign restrictions, sparsity, and minimal modifications to incumbent tools, ensuring practical deployability in clinical workflows. We further develop a continuous relaxation of the MIP problem to provide warm-start solutions for more efficient MIP optimization. We apply the proposed score optimization framework to a case study of inpatient falls risk assessment using the Johns Hopkins Fall Risk Assessment Tool.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可解释风险评估的联合评分-阈值优化</div>
<div class="mono" style="margin-top:8px">医疗健康领域的风险评估工具通常采用基于分数的评分系统，通过阈值将患者映射至有序风险等级。尽管电子健康记录（EHR）数据为这些工具的数据驱动优化提供了契机，但两大根本挑战阻碍了标准监督学习的应用：（1）由于干预审查结果，标签通常仅适用于极端风险等级；（2）误分类成本具有非对称性，且随序数距离增加而上升。针对这些挑战，我们提出一个混合整数规划（MIP）框架，可同步优化评分权重与等级阈值。该方法通过阈值约束防止标签稀缺类别的塌缩，并采用非对称且距离感知的目标函数。该MIP框架支持治理约束，包括符号限制、稀疏性以及对现有工具的最小化修改，从而确保在临床工作流程中的实际可部署性。我们进一步开发了MIP问题的连续松弛形式，为更高效的MIP优化提供预热启动解。我们将所提出的评分优化框架应用于约翰斯·霍普金斯跌倒风险评估工具的住院患者跌倒风险评估案例研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of optimizing healthcare risk assessment tools using electronic health record data, where labels are often only available for extreme risk categories and misclassification costs are asymmetric and increase with ordinal distance. The authors propose a mixed-integer programming framework that jointly optimizes scoring weights and risk category thresholds, incorporating constraints to prevent category collapse and an asymmetric objective to account for misclassification severity. In a case study on inpatient falls risk assessment, the method demonstrates practical deployability by supporting governance constraints like sign restrictions and sparsity, with a continuous relaxation developed to enhance optimization efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对利用电子健康记录数据优化医疗风险评估工具时面临的挑战，即标签通常仅适用于极端风险类别且误分类成本具有不对称性并随序数距离增加。作者提出了一种混合整数规划框架，联合优化评分权重和风险类别阈值，通过阈值约束防止类别崩溃，并采用不对称目标函数考虑误分类严重性。在住院患者跌倒风险评估的案例研究中，该方法通过支持符号限制和稀疏性等治理约束，展示了临床工作流程中的实际可部署性，并开发了连续松弛技术以提高优化效率。</div>
</details>
</div>
<div class="card">
<div class="title">Generalization and Completeness of Stochastic Local Search Algorithms</div>
<div class="meta-line">Authors: Daniel Loscos, Narciso Marti-Oliet, Ismael Rodriguez</div>
<div class="meta-line">First: 2026-01-20T18:17:45+00:00 · Latest: 2026-01-20T18:17:45+00:00</div>
<div class="meta-line">Comments: This paper was published in Swarm and Evolutionary Computation. The present version is the author&#x27;s accepted manuscript</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机局部搜索算法的泛化性与完备性</div>
<div class="mono" style="margin-top:8px">本研究将随机局部搜索（SLS）启发式算法统一归纳为一个形式化模型。该模型包含两个核心组件：一个尽可能大的通用结构，以及一个尽可能小的参数化结构。每种启发式算法通过以不同方式实例化参数部分获得。文中具体展示了遗传算法（GA）、蚁群优化（ACO）和粒子群优化（PSO）的实例。随后，我们运用该模型证明了SLS算法总体上具有图灵完备性。证明过程通过构建能够模拟任意图灵机的遗传算法实现。这一图灵完备性意味着：对于遗传算法乃至广义的SLS方法集（尽管不一定适用于每个具体方法），判定输入与计算输出间关系的任何非平凡性质都是不可判定的。文中还以较非形式化的方式给出了PSO和ACO的类似证明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to unify and formally analyze Stochastic Local Search (SLS) algorithms, this paper proposes a general model consisting of a large common structure and a small parametric part, from which specific heuristics like Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) can be instantiated. The method involves using this framework to construct a GA capable of simulating any Turing machine, thereby proving the Turing-completeness of SLS algorithms in general. The main experimental result is a formal proof that determining non-trivial properties about the input-output relationship is undecidable for GA and, by extension, for the broad class of SLS methods, with similar informal arguments extended to PSO and ACO.</div>
<div class="mono" style="margin-top:8px">本文旨在统一并形式化分析随机局部搜索（SLS）算法，提出了一个通用模型，该模型包含一个尽可能大的公共结构和一个尽可能小的参数化部分，遗传算法（GA）、蚁群优化（ACO）和粒子群优化（PSO）等具体启发式算法均可通过实例化该参数部分得到。方法上，利用该框架构建了一个能够模拟任何图灵机的GA，从而证明了SLS算法整体上的图灵完备性。主要实验结果是形式化证明了对于GA以及广义的SLS方法类（尽管不一定针对每个具体方法），判定其输入输出之间任何非平凡性质都是不可判定的，并对PSO和ACO给出了类似的非正式论证。</div>
</details>
</div>
<div class="card">
<div class="title">InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</div>
<div class="meta-line">Authors: Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-20T18:15:38+00:00 · Latest: 2026-01-20T18:15:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14209v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InT：自提干预实现大语言模型推理中的信用分配</div>
<div class="mono" style="margin-top:8px">结果奖励强化学习（RL）已被证明能有效提升大语言模型（LLM）的推理能力。然而，标准RL仅对最终答案层面分配信用：当结果错误时惩罚整个推理轨迹，结果正确时则均匀强化所有步骤。这导致失败轨迹中的正确中间步骤可能被抑制，而成功轨迹中的无效步骤反而被强化。我们将此失效模式称为信用分配问题。虽然训练过程奖励模型是自然的解决方案，但精准优化此类模型以识别纠正性推理步骤仍具挑战性。本文提出干预训练（InT），该训练范式使模型通过提出简短、有针对性的修正（引导轨迹获得更高奖励）对自身推理轨迹进行细粒度信用分配。利用数学推理数据集中普遍存在的参考答案，并基于验证模型生成解比从头生成正确解更简单的事实，模型首先识别推理中的首个错误，提出单步干预以将轨迹导向正确解，随后对错误发生前的策略轨迹与干预步骤进行监督微调（SFT），从而将错误定位至导致失败的具体步骤。实验表明，所得模型可作为RL训练更优的初始化参数。经过InT及后续RL微调，我们在IMO-AnswerBench上将4B参数基模型的准确率提升近14%，性能超越gpt-oss-20b等更大规模开源模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the credit assignment problem in outcome-reward reinforcement learning (RL) for large language models (LLMs), where standard RL penalizes or reinforces entire reasoning traces uniformly based on final answer correctness, potentially discouraging correct intermediate steps or reinforcing spurious ones. To remedy this, the authors propose Intervention Training (InT), a method where the model self-assigns credit by identifying the first error in its reasoning trace and proposing a targeted single-step correction to steer the trajectory toward the correct solution, using reference solutions from datasets and leveraging the ease of verification over generation. This is followed by supervised fine-tuning (SFT) on the corrected trajectory up to the error point, localizing the error to specific steps. Experimental results show that InT significantly improves model initialization for RL, leading to a nearly 14% accuracy gain over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models like gpt-oss-20b.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型（LLMs）在结果奖励强化学习（RL）中的信用分配问题，即标准RL仅根据最终答案的正确性对整个推理轨迹进行惩罚或强化，可能导致正确中间步骤被抑制或错误步骤被强化。为解决此问题，作者提出了干预训练（InT）方法，通过利用数学推理数据集中常见的参考答案，并基于验证比生成更容易的事实，让模型自我识别其推理轨迹中的首个错误，并提出有针对性的单步修正以引导轨迹朝向正确解。随后，对修正后的轨迹直至错误点进行监督微调（SFT），将错误定位到具体步骤。实验结果表明，InT显著改善了模型作为RL初始化的效果，在IMO-AnswerBench上使4B参数基础模型的准确率提升近14%，超越了如gpt-oss-20b等更大的开源模型。</div>
</details>
</div>
<div class="card">
<div class="title">Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery</div>
<div class="meta-line">Authors: Albina Galiullina, Wouter van Heeswijk, Tom van Woensel</div>
<div class="meta-line">First: 2026-01-20T18:00:42+00:00 · Latest: 2026-01-20T18:00:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14196v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>差异化自提点服务在最后一公里配送中的减排策略</div>
<div class="mono" style="margin-top:8px">自提点作为家庭配送的可持续替代方案被广泛认可，通过在自提点整合订单可缩短配送路线并提高首次投递成功率。然而，当顾客驾车前往取件时，这些优势可能被抵消。本研究提出一种差异化自提点服务（DPO）策略，旨在同步减少配送卡车路线与顾客出行的碳排放。在DPO机制下，系统为每位到达顾客推荐单一自提点，而非提供所有网点的无限制选择，同时保留家庭配送选项。我们在动态随机环境中研究该问题，其中为顾客推荐的自提点取决于历史顾客位置与配送选择。为设计有效的DPO策略，采用基于强化学习的方法，综合考虑顾客与自提点的空间关系及其对未来路线整合的影响。计算实验表明，差异化自提点服务能显著降低总碳排放量：相较于纯家庭配送模式，该策略最高可减少9%排放；与无限制自提点选择、最近自提点分配等对比策略相比，平均减排2%。在自提点密集、站点间距较短的高密度城市区域，差异化策略尤为有效。此外，当顾客对自提点配送的接受度较低时，显式考量顾客到达与选择的动态特性至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the need to mitigate the environmental impact of last-mile delivery, where traditional pickup points can reduce delivery emissions but may inadvertently increase customer travel emissions if customers drive to collect orders. The authors propose a Differentiated Pickup Point Offering (DPO) policy, which uses reinforcement learning to dynamically recommend a single pickup point to each customer based on prior deliveries and spatial relationships, aiming to optimize route consolidation while preserving home delivery as an option. Experimental results demonstrate that DPO reduces total carbon emissions by up to 9% compared to home-only delivery and by an average of 2% over alternative policies like unrestricted choice or nearest-point assignment, with particular effectiveness in dense urban areas with many pickup points and when customer preferences for pickup are low.</div>
<div class="mono" style="margin-top:8px">本研究旨在减少最后一公里配送的环境影响，其动机在于传统取件点虽能降低配送排放，但若顾客驾车取件可能增加出行排放。作者提出差异化取件点提供（DPO）策略，采用强化学习方法，根据历史配送数据和空间关系动态为每位顾客推荐单一取件点，以优化路线整合，同时保留送货上门选项。实验结果表明，DPO策略相比仅提供送货上门可减少高达9%的总碳排放，平均比无限制选择或最近点分配等替代策略降低2%，在取件点密集、距离较短的稠密城市环境中效果尤为显著，且当顾客对取件点偏好较低时，动态考虑顾客到达和选择行为尤为重要。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Efficient Agents: Memory, Tool learning, and Planning</div>
<div class="meta-line">Authors: Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu, Yuchen Fan, Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, Zhiqiang Kou, Daizong Liu, Qi Li, Ning Ding, Siheng Chen, Jing Shao</div>
<div class="meta-line">First: 2026-01-20T17:51:56+00:00 · Latest: 2026-01-20T17:51:56+00:00</div>
<div class="meta-line">Comments: 35 pages, 200 references</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向高效智能体：记忆、工具学习与规划</div>
<div class="mono" style="margin-top:8px">近年来，将大语言模型扩展为智能体系统的研究日益受到关注。尽管智能体的效能持续提升，但对其实际部署至关重要的效率问题却常被忽视。本文从智能体的三个核心组成部分——记忆、工具学习和规划——出发，结合延迟、令牌消耗、步骤数等成本因素，系统探讨效率问题。为全面研究智能体系统自身的效率，我们综述了近期多种实现各异但常遵循共同高层原则的方法，包括但不限于通过压缩与管理限制上下文、设计强化学习奖励以最小化工具调用、采用受控搜索机制提升效率等，并对此展开详细讨论。我们通过两种互补方式界定效率：在固定成本预算下比较效能，以及在可比效能水平下比较成本。这种权衡亦可从效能与成本的帕累托前沿视角理解。基于此，我们还通过总结各组件评估方案、整合基准与方法研究中常用效率指标，审视了以效率为导向的基准测试。此外，本文讨论了关键挑战与未来方向，旨在提供有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to make AI agents practical for real-world deployment, this paper investigates efficiency in three core agent components: memory, tool learning, and planning, considering costs like latency and token usage. The method involves a comprehensive review of recent approaches that share high-level principles such as context compression, reward design to minimize tool use, and controlled search mechanisms. The main experimental results are framed through a dual analysis comparing effectiveness under fixed cost budgets and cost at comparable effectiveness levels, establishing a Pareto frontier perspective, while also summarizing efficiency-oriented benchmarks and metrics from existing studies.</div>
<div class="mono" style="margin-top:8px">本文的动机是使AI智能体在实际部署中更具可行性，因此从记忆、工具学习和规划三个核心组件出发研究效率问题，考虑了延迟和令牌使用等成本。方法上，论文全面综述了近期各种方法，这些方法虽实现不同但共享高层原则，如通过压缩和管理限制上下文、设计强化学习奖励以减少工具调用，以及采用受控搜索机制提升效率。主要实验结果通过双重分析呈现：在固定成本预算下比较有效性，以及在可比有效性水平下比较成本，从而建立有效性-成本的帕累托前沿视角，同时总结了现有研究中的效率导向基准和常用评估指标。</div>
</details>
</div>
<div class="card">
<div class="title">UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms</div>
<div class="meta-line">Authors: Denis Belomestny, Ilya Levin, Alexey Naumov, Sergey Samsonov</div>
<div class="meta-line">First: 2021-05-05T15:38:36+00:00 · Latest: 2026-01-20T15:57:19+00:00</div>
<div class="meta-line">Comments: JOTA camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2105.02135v5">Abs</a> · <a href="https://arxiv.org/pdf/2105.02135v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy evaluation is an important instrument for the comparison of different algorithms in Reinforcement Learning (RL). However, even a precise knowledge of the value function $V^π$ corresponding to a policy $π$ does not provide reliable information on how far the policy $π$ is from the optimal one. We present a novel model-free upper value iteration procedure ({\sf UVIP}) that allows us to estimate the suboptimality gap $V^{\star}(x) - V^π(x)$ from above and to construct confidence intervals for \(V^\star\). Our approach relies on upper bounds to the solution of the Bellman optimality equation via the martingale approach. We provide theoretical guarantees for {\sf UVIP} under general assumptions and illustrate its performance on a number of benchmark RL problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UVIP：评估强化学习算法的无模型方法</div>
<div class="mono" style="margin-top:8px">策略评估是强化学习中比较不同算法的重要工具。然而，即使精确掌握策略π对应的价值函数V^π，仍无法可靠评估该策略与最优策略的差距。本文提出一种新颖的无模型上界值迭代方法（UVIP），能够从上方估计次优性差距V^⋆(x)-V^π(x)，并为V^⋆构建置信区间。该方法通过鞅方法对贝尔曼最优方程的解进行上界估计。我们在通用假设下为UVIP提供理论保证，并在多个基准强化学习问题上展示其性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces UVIP, a model-free method to assess reinforcement learning algorithms by estimating the suboptimality gap between a policy&#x27;s value function and the optimal value, addressing the limitation that knowing only the value function does not indicate proximity to optimality. The approach leverages upper bounds derived from the martingale approach to the Bellman optimality equation to compute these estimates and construct confidence intervals for the optimal value function. Experimental results on various benchmark RL problems demonstrate the method&#x27;s effectiveness in providing reliable performance evaluations.</div>
<div class="mono" style="margin-top:8px">本文提出了UVIP，一种无模型方法，用于评估强化学习算法，通过估计策略价值函数与最优价值之间的次优性差距，解决了仅知道价值函数无法衡量接近最优程度的问题。该方法基于对贝尔曼最优性方程的解应用鞅方法推导上界，从而计算这些估计并为最优价值函数构建置信区间。在多个基准强化学习问题上的实验结果验证了该方法在提供可靠性能评估方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning</div>
<div class="meta-line">Authors: Babacar Toure, Dimitrios Tsilimantos, Omid Esrafilian, Marios Kountouris</div>
<div class="meta-line">First: 2026-01-20T15:55:11+00:00 · Latest: 2026-01-20T15:55:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14092v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14092v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力机制的多目标强化学习优化无人机辅助物联网网络的能量与数据收集</div>
<div class="mono" style="margin-top:8px">凭借其适应性与机动性，无人机在无线网络服务中日益重要，尤其在数据采集任务中。基于人工智能的方法因其能在大规模复杂环境中处理无人机路径规划问题，并弥合与实际部署的差距而备受关注。然而，现有算法常受限于训练数据不足，在高度动态环境中表现欠佳，且往往忽视任务固有的多目标特性，处理方式过于简化。为克服这些局限，本文提出一种基于注意力机制的多目标强化学习架构，能在未知无线信道条件下，显式权衡城市环境中的数据收集与能耗。该方法构建的单一模型无需微调或重新训练，即可适应不同的权衡偏好与动态场景参数。大量仿真表明，本方法在性能、模型紧凑性、样本效率及对未见过场景的泛化能力上均显著优于现有强化学习方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing both energy consumption and data collection in UAV-aided IoT networks, motivated by the limitations of existing AI methods which often rely on limited training data and oversimplify the multi-objective nature of the task. The authors propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly manages the trade-off between data harvesting and energy use in urban settings, without requiring prior knowledge of wireless channel conditions. Experimental results from extensive simulations demonstrate that their method significantly outperforms existing RL solutions in performance, model compactness, sample efficiency, and generalization to unseen scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对无人机辅助物联网网络中能量消耗与数据收集的优化挑战，其动机在于现有人工智能方法常受限于训练数据不足且过度简化任务的多目标特性。作者提出了一种基于注意力的多目标强化学习架构，该架构在无需先验无线信道知识的条件下，明确处理城市环境中数据收集与能耗之间的权衡。大量仿真实验结果表明，该方法在性能、模型紧凑性、样本效率以及最重要的是对未见场景的泛化能力上，均显著优于现有的强化学习解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning</div>
<div class="meta-line">Authors: Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, Yike Guo</div>
<div class="meta-line">First: 2026-01-20T13:18:18+00:00 · Latest: 2026-01-20T13:18:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13942v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13942v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model&#x27;s capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一瞥或凝视：通过强化学习激励大语言模型自适应聚焦搜索</div>
<div class="mono" style="margin-top:8px">大语言模型在视觉理解方面取得了显著成功，但由于静态参数化知识的限制，在处理涉及长尾实体或动态演变信息的知识密集型查询时仍面临困难。近期基于搜索增强的方法试图解决这一局限，但现有方法依赖无差别的全图像检索，引入了大量视觉冗余和噪声，且缺乏深度迭代反思，限制了其在复杂视觉查询上的有效性。为克服这些挑战，我们提出了Glance-or-Gaze（GoG）框架，这是一个从被动感知转向主动视觉规划的完全自主框架。GoG引入了选择性凝视机制，能动态选择是全局一瞥还是聚焦高价值区域，在检索前过滤无关信息。我们设计了双阶段训练策略：通过监督微调实现反思性GoG行为对齐以奠定基础范式，同时采用复杂度自适应强化学习通过迭代推理进一步提升模型处理复杂查询的能力。在六个基准测试上的实验证明了其最先进的性能。消融研究证实选择性凝视机制和复杂度自适应强化学习对实现有效视觉搜索均至关重要。我们将很快发布数据和模型以供进一步探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of Large Multimodal Models (LMMs) in handling knowledge-intensive visual queries due to static parametric knowledge and inefficient search-augmented methods that retrieve entire images, introducing redundancy and noise. The proposed solution, Glance-or-Gaze (GoG), is an autonomous framework that employs a Selective Gaze mechanism to dynamically decide between glancing at global context or gazing into specific high-value regions, thereby filtering irrelevant information before retrieval, and it is trained via a dual-stage strategy involving supervised fine-tuning and complexity-adaptive reinforcement learning to enhance iterative reasoning. Experimental results on six benchmarks show state-of-the-art performance, with ablation studies confirming the necessity of both the Selective Gaze mechanism and the reinforcement learning component for effective visual search.</div>
<div class="mono" style="margin-top:8px">本文针对大型多模态模型在处理知识密集型视觉查询时，因静态参数化知识及现有搜索增强方法检索整个图像导致冗余和噪声的局限性，提出了一种自主框架Glance-or-Gaze（GoG）。该方法引入了选择性凝视机制，动态选择是瞥视全局上下文还是凝视高价值区域，从而在检索前过滤无关信息，并通过包含监督微调和复杂度自适应强化学习的双阶段训练策略来提升迭代推理能力。在六个基准测试上的实验结果表明了其最先进的性能，消融研究证实了选择性凝视机制和强化学习对于有效视觉搜索都是必不可少的。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</div>
<div class="meta-line">Authors: Jianfeng Si, Lin Sun, Zhewen Tan, Xiangzheng Zhang</div>
<div class="meta-line">First: 2025-08-12T02:39:33+00:00 · Latest: 2026-01-20T13:15:53+00:00</div>
<div class="meta-line">Comments: 15 pages,3 figures,5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14904v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.14904v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model&#x27;s safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于魔法令牌引导协同训练的大语言模型高效可切换安全控制</div>
<div class="mono" style="margin-top:8px">当前大语言模型（LLMs）的内容安全方法，如监督微调（SFT）和基于人类反馈的强化学习（RLHF），通常依赖多阶段训练流程，且缺乏细粒度的部署后可控性。为解决这些局限，我们提出一种统一的协同训练框架，能在单一SFT阶段高效整合多种安全行为：积极（合法/亲社会）、消极（未过滤/风险倾向）和拒绝（拒绝导向/保守）。值得注意的是，每种行为均可通过简单的系统级指令（即魔法令牌）动态激活，实现推理时隐蔽高效的行为切换。这种灵活性支持多样化的部署场景，例如积极模式用于安全用户交互，消极模式用于内部红队测试，拒绝模式用于响应上游审核信号的情境感知拒绝。该协同训练策略在输出空间中诱导出独特的安全对齐边界，其特征是与各安全模式对应的、分离良好的响应分布。该边界的存在为模型的安全鲁棒性提供了实证依据，并实现了前所未有的细粒度控制。实验表明，我们的方法在安全对齐质量上媲美SFT+DPO，其中8B模型在安全性能上显著超越DeepSeek-R1（671B），同时大幅降低了训练复杂度和部署成本。本研究为大语言模型内容安全提供了一种可扩展、高效且高度可控的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing LLM safety methods like SFT and RLHF, which involve complex multi-stage training and lack post-deployment controllability. It proposes a unified co-training framework that integrates three safety behaviors—positive, negative, and rejective—within a single SFT stage, activated dynamically at inference via simple system instructions or magic tokens for efficient behavioral switching. Experimental results demonstrate that this method achieves safety alignment comparable to SFT+DPO, with an 8B model notably outperforming DeepSeek-R1 (671B) in safety, while reducing training complexity and deployment costs, thereby offering a scalable and highly controllable solution.</div>
<div class="mono" style="margin-top:8px">本文针对现有大语言模型安全方法（如监督微调和人类反馈强化学习）存在的多阶段训练复杂且部署后缺乏细粒度可控性的问题，提出了一种统一的协同训练框架。该框架在单一监督微调阶段内高效整合了积极、消极和拒绝三种安全行为，并通过简单的系统指令或魔法令牌在推理时动态激活，实现灵活的行为切换。实验结果表明，该方法在安全对齐效果上媲美监督微调加直接偏好优化，其80亿参数模型在安全性能上显著超越6710亿参数的DeepSeek-R1，同时大幅降低了训练复杂度和部署成本，提供了一个可扩展、高效且高度可控的内容安全解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning</div>
<div class="meta-line">Authors: Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya Wang, Jun Wang, Ye Shi</div>
<div class="meta-line">First: 2025-05-24T15:57:07+00:00 · Latest: 2026-01-20T12:33:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18763v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18763v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO&#x27;s superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenPO：生成扩散模型与在线策略强化学习的融合</div>
<div class="mono" style="margin-top:8px">强化学习领域的最新进展展现了基于生成扩散的策略在探索能力和多模态特性方面的强大潜力。尽管离线强化学习和离线策略强化学习已取得显著进步，但将扩散策略整合至PPO等在线策略框架的研究仍显不足。考虑到IsaacLab等大规模并行GPU加速模拟器（专为在线策略强化学习算法优化，可快速训练复杂机器人任务）的广泛应用，这一空白尤为突出。核心挑战在于计算扩散策略下的状态-动作对数似然：高斯策略可直接计算，而基于流的模型因不可逆的前向-反向过程及离散化误差（如欧拉-丸山近似）导致计算困难。为填补此空白，我们提出GenPO——一种利用精确扩散反演构建可逆动作映射的生成式策略优化框架。GenPO通过新颖的双重虚拟动作机制，借助交替更新实现可逆性，突破了对数似然计算障碍。此外，我们利用动作对数似然进行无偏熵和KL散度估计，实现在线策略更新中的KL自适应学习率与熵正则化。在八项IsaacLab基准测试（包括足式运动、灵巧操作、空中控制和机械臂任务）上的大量实验表明，GenPO优于现有强化学习基线。值得注意的是，GenPO是首个成功将扩散策略整合至在线策略强化学习的方法，为大规模并行化训练和现实世界机器人部署释放了其潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces GenPO, a framework that integrates generative diffusion models into on-policy reinforcement learning to address the challenge of computing state-action log-likelihoods, which is intractable in standard diffusion policies due to irreversible forward-reverse processes. The method leverages exact diffusion inversion with a novel doubled dummy action mechanism to construct invertible action mappings, enabling precise log-likelihood computation for unbiased entropy and KL divergence estimation, thus facilitating KL-adaptive learning and entropy regularization. Experimental results across eight IsaacLab benchmarks, including legged locomotion and dexterous manipulation tasks, demonstrate GenPO&#x27;s superiority over existing RL baselines, marking the first successful integration of diffusion policies into on-policy RL for scalable robotic training.</div>
<div class="mono" style="margin-top:8px">本文提出了GenPO框架，将生成扩散模型融入在线策略强化学习，以解决标准扩散策略中因前向-反向过程不可逆而导致的状态-动作对数似然计算难题。该方法利用精确的扩散反转和一种新颖的双重虚拟动作机制，构建可逆的动作映射，从而实现对对数似然的精确计算，支持无偏的熵和KL散度估计，并实现KL自适应学习率与熵正则化。在包括足式运动与灵巧操作在内的八个IsaacLab基准测试上的实验结果表明，GenPO优于现有强化学习基线，首次成功将扩散策略整合到在线策略强化学习中，为大规模并行化机器人训练铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography</div>
<div class="meta-line">Authors: Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam</div>
<div class="meta-line">Venue: ISBI</div>
<div class="meta-line">First: 2026-01-20T12:26:38+00:00 · Latest: 2026-01-20T12:26:38+00:00</div>
<div class="meta-line">Comments: Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13897v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13897v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TractRLFusion：基于GPT的多策略融合纤维束追踪框架</div>
<div class="mono" style="margin-top:8px">纤维束追踪在无创重建白质纤维通路中具有关键作用，为脑连接研究提供重要信息并支持精准神经外科规划。传统方法主要依赖经典确定性与概率性方法，而近期进展通过监督式深度学习与深度强化学习提升了纤维束重建效果。当前核心挑战在于准确重建白质纤维束的同时最小化虚假连接。为此，我们提出TractRLFusion——一种基于GPT的策略融合框架，通过数据驱动的融合策略整合多个强化学习策略。该方法采用两阶段训练数据选择实现高效策略融合，并通过多评价器微调阶段增强鲁棒性与泛化能力。在HCP、ISMRM和TractoInferno数据集上的实验表明，TractRLFusion在准确性与解剖可靠性方面均优于单一强化学习策略及当前最先进的经典方法与深度强化学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve the accuracy and anatomical reliability of white matter fiber tractography while reducing spurious connections, this paper introduces TractRLFusion, a novel framework that integrates multiple reinforcement learning policies using a GPT-based, data-driven fusion strategy. The method employs a two-stage training data selection process for policy fusion and a multi-critic fine-tuning phase to enhance robustness. Experimental results on HCP, ISMRM, and TractoInferno datasets show that TractRLFusion surpasses individual RL policies and state-of-the-art classical and deep reinforcement learning methods in both accuracy and anatomical reliability.</div>
<div class="mono" style="margin-top:8px">本研究旨在提高白质纤维束成像的准确性和解剖学可靠性，同时减少虚假连接，为此提出了TractRLFusion框架，该框架采用基于GPT的数据驱动融合策略整合多个强化学习策略。方法包括两阶段的训练数据选择过程以实现策略融合，以及多批评器微调阶段以增强鲁棒性和泛化能力。在HCP、ISMRM和TractoInferno数据集上的实验表明，TractRLFusion在准确性和解剖学可靠性方面优于单个强化学习策略以及最先进的经典方法和深度强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Hierarchical Optimization with Large Language Models</div>
<div class="meta-line">Authors: Andrej Schwanke, Lyubomir Ivanov, David Salinas, Frank Hutter, Arber Zela</div>
<div class="meta-line">First: 2026-01-20T12:10:13+00:00 · Latest: 2026-01-20T12:10:13+00:00</div>
<div class="meta-line">Comments: 23 pages, 21 figures, 9 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13892v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn&#x27;t have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的多目标分层优化</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLMs）凭借其强大的推理能力在各领域广泛应用，但尚未成为驱动多目标优化的现成选择。传统策略因擅长处理数值输入、通过精细建模平衡探索与帕累托前沿利用、并能处理多重（冲突）目标，在基准测试中表现优异。本文通过将LLMs作为结构化分层搜索策略中的代理模型和候选采样器，弥补了这一差距。通过自适应地将输入空间划分为互不相交的超矩形区域，并用复合评分函数进行排序，我们将LLM的生成过程限制在特定高潜力子空间内，从而简化问题求解——LLM无需推理全局结构，仅需局部处理。在标准正则性假设下，我们证明算法生成的候选解在豪斯多夫距离下收敛于真实帕累托集。实证表明，该方法持续优于基于LLM的全局多目标优化器，并在合成与真实世界基准测试中与经典进化算法及贝叶斯优化算法性能相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the gap in using Large Language Models (LLMs) for multi-objective optimization by proposing a hierarchical search strategy that leverages LLMs as surrogate models and candidate samplers. The method adaptively partitions the input space into hyperrectangular regions, ranking them with a composite score to focus the LLM&#x27;s generative process on high-potential sub-spaces, simplifying reasoning from global to local. Under standard regularity assumptions, the algorithm&#x27;s candidate solutions converge to the true Pareto set in Hausdorff distance, and empirical results show it consistently outperforms global LLM-based optimizers while matching the performance of standard evolutionary and Bayesian optimization methods on synthetic and real-world benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在多目标优化中的应用不足，提出了一种分层搜索策略，将LLM用作代理模型和候选采样器。该方法通过自适应地将输入空间划分为超矩形区域，并使用复合评分函数进行排序，从而将LLM的生成过程限制在具有高潜力的子空间内，使问题从全局推理简化为局部推理。在标准正则性假设下，该算法生成的候选解在豪斯多夫距离下收敛于真实帕累托集，实证结果表明，它在合成和真实世界基准测试中持续优于基于LLM的全局优化器，并与标准进化和贝叶斯优化算法性能相当。</div>
</details>
</div>
<div class="card">
<div class="title">Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning</div>
<div class="meta-line">Authors: Shreyansh Jain, Madhav Singhvi, Shreya Rahul Jain, Pranav S, Dishaa Lokesh, Naren Chittibabu, Akash Anandhan</div>
<div class="meta-line">Venue: ICCCA 2025, pp. 1-6</div>
<div class="meta-line">First: 2025-11-20T06:06:30+00:00 · Latest: 2026-01-20T10:04:41+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures, 2 equations, 3 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16073v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most of the traditional Applicant Tracking Systems (ATS) depend on strict matching using keywords, where candidates that are highly qualified are many times disqualified because of minor semantic differences. In this article, the two-stage process of developing a more comprehensive resume assessment system based on a small language model that is trained with fewer than 600M parameters is introduced and fine-tuned by using GRPO with a uniquely designed reward function. The initial stage is Supervised Fine-Tuning (SFT), which is used to create a strong base model with the ability to perceive resumes beyond superficial overlap of keywords. This SFT model is further optimized in the second step with Reinforcement Learning (RL) via GRPO with the help of multi-component-based rewarding, which will not be considered as a commission of tokens matching. In the initial RL experiments, we found a severe difficulty in the shape of reward hacking: overly aggressive penalty terms resulted in unstable training dynamics and prohibitively negative model behavior. This was solved by trial-and-error refinement of the reward and careful training hyperparameter tuning, which led to a stable and controlled process of gentle polishing. The GRPO-refined model shows high real-life performance, as it shows an accuracy of 91% on unseen data used for testing. It has a high recall of 0.85 on the SELECTED class with a perfect precision of 1.0, which highlights its high reliability for identifying qualified applicants. These findings demonstrate that an appropriately structured two-step fine-tuning pipeline can effectively be used to transfer a small language model into human-like candidate evaluation, surpassing the shortcomings of both traditional ATS systems and unrefined uses of reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的求职申请评估中定制奖励函数的数学框架</div>
<div class="mono" style="margin-top:8px">传统申请人追踪系统（ATS）大多依赖关键词的严格匹配，导致许多高度合格的候选人因细微语义差异而被淘汰。本文介绍了一种基于参数量少于6亿的小型语言模型构建更全面简历评估系统的两阶段流程，该模型采用GRPO算法并通过独特设计的奖励函数进行微调。第一阶段为监督微调（SFT），用于建立能够超越关键词表层重叠感知简历的强基础模型。第二阶段通过GRPO算法结合多组件奖励机制进行强化学习（RL）优化，避免陷入简单的词符匹配陷阱。在初期RL实验中，我们发现了严重的奖励破解问题：过于激进的惩罚项导致训练动态不稳定及模型行为极端负面。通过奖励函数的试错优化与训练超参数精细调整，最终实现了稳定可控的渐进式优化。经GRPO精调的模型在实际测试中表现优异，在未见测试数据上准确率达91%，对“入选”类别的召回率为0.85且精确率达1.0，凸显其在识别合格申请人方面的高可靠性。这些发现表明，结构合理的两阶段微调流程能有效将小型语言模型转化为类人类的候选人评估系统，克服传统ATS系统与未优化强化学习应用的缺陷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional keyword-matching Applicant Tracking Systems (ATS) that often disqualify qualified candidates due to minor semantic differences, this paper introduces a two-stage fine-tuning pipeline using a small language model (under 600M parameters) for resume evaluation. The method first employs Supervised Fine-Tuning (SFT) to build a base model that understands resumes beyond keyword overlap, then refines it via Reinforcement Learning with GRPO using a custom multi-component reward function designed to avoid token-matching bias. Experimental results show that after overcoming initial reward hacking issues through reward and hyperparameter tuning, the GRPO-refined model achieves 91% accuracy on unseen test data, with a recall of 0.85 and perfect precision of 1.0 for the SELECTED class, demonstrating high reliability in identifying qualified applicants and surpassing traditional ATS systems.</div>
<div class="mono" style="margin-top:8px">本文针对传统基于关键词严格匹配的申请人跟踪系统（ATS）常因细微语义差异而淘汰合格候选人的问题，提出了一种使用小型语言模型（参数少于6亿）的两阶段微调流程用于简历评估。方法首先通过监督微调（SFT）构建一个能超越关键词重叠理解简历的基础模型，然后利用GRPO进行强化学习微调，采用自定义的多组件奖励函数以避免令牌匹配偏差。实验结果表明，在通过奖励函数和超参数调试克服了初始的奖励破解问题后，经GRPO优化的模型在未见测试数据上达到91%的准确率，对“选中”类别的召回率为0.85且精确度为1.0，显示出识别合格申请人的高可靠性，超越了传统ATS系统和未优化的强化学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training</div>
<div class="meta-line">Authors: Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, Nikolay Malkin</div>
<div class="meta-line">First: 2025-01-10T18:18:25+00:00 · Latest: 2026-01-20T09:07:11+00:00</div>
<div class="meta-line">Comments: TMLR; code: https://github.com/GFNOrg/gfn-diffusion/tree/stagger</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.06148v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.06148v2">PDF</a> · <a href="https://github.com/GFNOrg/gfn-diffusion/tree/stagger">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of training neural stochastic differential equations, or diffusion models, to sample from a Boltzmann distribution without access to target samples. Existing methods for training such models enforce time-reversal of the generative and noising processes, using either differentiable simulation or off-policy reinforcement learning (RL). We prove equivalences between families of objectives in the limit of infinitesimal discretization steps, linking entropic RL methods (GFlowNets) with continuous-time objects (partial differential equations and path space measures). We further show that an appropriate choice of coarse time discretization during training allows greatly improved sample efficiency and the use of time-local objectives, achieving competitive performance on standard sampling benchmarks with reduced computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从离散时间策略到连续时间扩散采样器：渐近等价性与加速训练</div>
<div class="mono" style="margin-top:8px">本研究针对在无法获取目标样本的情况下，训练神经随机微分方程（即扩散模型）以从玻尔兹曼分布采样的难题。现有训练方法通过可微分模拟或离策略强化学习，强制生成过程与噪声过程的时间可逆性。我们证明了在无穷小离散化步长极限下，不同目标函数族之间的等价关系，从而将熵强化学习方法（GFlowNets）与连续时间对象（偏微分方程与路径空间测度）联系起来。进一步研究表明，训练过程中采用适当的粗粒度时间离散化策略，可显著提升样本效率并实现时间局部化目标函数，在标准采样基准测试中以更低计算成本达到竞争性性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training neural stochastic differential equations (diffusion models) to sample from Boltzmann distributions without requiring target samples, motivated by the computational inefficiencies of existing methods that rely on time-reversal techniques. The authors establish theoretical equivalences between discrete-time objectives, such as those in entropic reinforcement learning (GFlowNets), and continuous-time formulations involving partial differential equations and path space measures, in the limit of infinitesimal discretization steps. Experimentally, they demonstrate that employing a coarse time discretization during training enhances sample efficiency and enables the use of time-local objectives, leading to competitive performance on standard sampling benchmarks while reducing computational costs.</div>
<div class="mono" style="margin-top:8px">本文研究了在无法获取目标样本的情况下，训练神经随机微分方程（扩散模型）以从玻尔兹曼分布中采样的问题，其动机在于现有基于时间反转技术的方法存在计算效率低下的局限。作者证明了在无限小离散化步长极限下，离散时间目标（如熵强化学习中的GFlowNets）与涉及偏微分方程和路径空间测度的连续时间表述之间的理论等价性。实验结果表明，在训练过程中采用粗粒度时间离散化可显著提高样本效率，并允许使用时间局部目标，从而在标准采样基准上实现有竞争力的性能，同时降低了计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering</div>
<div class="meta-line">Authors: Chak Tou Leong, Dingwei Chen, Heming Xia, Qingyu Yin, Sunbowen Lee, Jian Wang, Wenjie Li</div>
<div class="meta-line">First: 2026-01-20T09:07:01+00:00 · Latest: 2026-01-20T09:07:01+00:00</div>
<div class="meta-line">Comments: Working in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13752v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13752v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model&#x27;s self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model&#x27;s reasoning belief effectively shapes its actual behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>寻找RELIEF：通过信念工程塑造推理行为而无需推理监督</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）在复杂问题解决方面取得了显著成功，但常存在计算冗余或推理不忠实的问题。当前塑造LRM行为的方法通常依赖于强化学习或使用黄金标准推理轨迹进行微调，这种范式计算成本高昂且难以扩展。本文揭示LRMs具有潜在的“推理信念”，可内部追踪其自身推理特征，并能通过简单的逻辑探针捕获。基于此发现，我们提出推理信念工程（RELIEF）——一种通过将模型自我认知与目标信念蓝图对齐来塑造LRM行为的简洁有效框架。关键的是，RELIEF完全绕过了对推理轨迹监督的需求，通过微调合成的、自我反思的问答对（这些问答对确认目标信念）来内化期望特征。在效率与忠实性任务上的大量实验表明，RELIEF在降低训练成本的同时，达到或超越了行为监督与基于偏好基线方法的性能。进一步分析证实，改变模型的推理信念能有效塑造其实际行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency and unfaithfulness in large reasoning models (LRMs) by proposing a novel method called Reasoning Belief Engineering (RELIEF), which avoids the high computational cost and scalability issues of traditional supervision-based approaches like reinforcement learning or fine-tuning with gold reasoning traces. The method leverages the insight that LRMs have latent reasoning beliefs about their own reasoning traits, which can be probed via logits, and aligns these beliefs with a target blueprint by fine-tuning on synthesized self-reflective question-answering pairs that affirm desired traits, thereby bypassing reasoning-trace supervision. Experimental results on efficiency and faithfulness tasks show that RELIEF matches or surpasses behavior-supervised and preference-based baselines while reducing training costs, with further analysis confirming that shifting reasoning beliefs effectively shapes model behavior.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型（LRMs）存在的计算冗余和推理不忠实问题，提出了一种名为推理信念工程（RELIEF）的新方法，避免了传统基于监督的方法（如强化学习或使用黄金推理轨迹进行微调）的高计算成本和可扩展性难题。该方法基于LRMs具有关于自身推理特质的潜在信念这一洞见，通过logit探测捕获这些信念，并通过在合成的自反问答对上微调以对齐目标信念蓝图，从而绕开推理轨迹监督。在效率和忠实性任务上的大量实验表明，RELIEF在降低训练成本的同时，匹配或超越了基于行为监督和偏好的基线方法，进一步分析验证了改变模型的推理信念能有效塑造其实际行为。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models</div>
<div class="meta-line">Authors: Jinwu Hu, Dongjin Yang, Langyu Bian, Zhiquan Wen, Yufeng Wang, Yaofo Chen, Bin Xiao, Yuanqing Li, Mingkui Tan</div>
<div class="meta-line">First: 2025-12-17T05:11:58+00:00 · Latest: 2026-01-20T08:55:05+00:00</div>
<div class="meta-line">Comments: under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15089v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15089v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越快与慢：面向大语言模型的认知启发式弹性推理</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在各种语言任务中展现出卓越性能。然而，现有LLM推理策略主要依赖模型自身的快慢模式（如o1思考），难以在不同难度查询间平衡推理效率与准确性。本文提出认知启发式弹性推理（CogER），该框架受人类分层推理机制启发，能动态为每个查询选择最合适的推理策略。具体而言，CogER首先评估输入查询的复杂度，将其分配到预定义的难度层级，每个层级对应定制化处理策略，从而解决查询难度不可观测的挑战。为实现自动策略选择，我们将该过程建模为马尔可夫决策过程，并通过强化学习训练CogER智能体。该智能体由平衡解质量与计算成本的奖励函数引导，确保资源高效推理。此外，针对需要外部工具的查询，我们提出认知工具辅助推理机制，使LLM能在思维链中自主调用外部工具。大量实验表明，CogER优于最先进的测试时扩展方法，在领域内任务上平均精确匹配相对提升至少13%，在领域外任务上相对增益达8%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of balancing reasoning efficiency and accuracy in large language models (LLMs) across queries of varying difficulty, as existing strategies often rely on fixed fast or slow modes. The authors propose Cognitive-Inspired Elastic Reasoning (CogER), a framework that dynamically selects reasoning strategies by first assessing query complexity and assigning queries to predefined levels, modeled as a Markov Decision Process and trained with reinforcement learning to optimize a reward balancing solution quality and computational cost; it also incorporates external tool use when needed. Experimental results show that CogER outperforms state-of-the-art test-time scaling methods, achieving at least a 13% relative improvement in average exact match on in-domain tasks and an 8% gain on out-of-domain tasks.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在处理不同难度查询时难以平衡推理效率与准确性的问题，提出了认知启发的弹性推理框架CogER。该方法受人类分层推理启发，通过评估查询复杂度并将其分配到预定义级别来动态选择推理策略，使用马尔可夫决策过程建模并通过强化学习训练智能体，以优化平衡解质量与计算成本的奖励函数，同时支持在思维链中自主调用外部工具。大量实验表明，CogER在领域内任务上实现了至少13%的相对准确率提升，在领域外任务上也有8%的相对增益，优于现有测试时扩展方法。</div>
</details>
</div>
<div class="card">
<div class="title">Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study</div>
<div class="meta-line">Authors: Chuang Zhang, Geng Sun, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato</div>
<div class="meta-line">First: 2025-08-01T01:53:58+00:00 · Latest: 2026-01-20T08:53:47+00:00</div>
<div class="meta-line">Comments: This paper has been accepted to IEEE Communications Magazine</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.00256v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.00256v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-altitude wireless networks (LAWNs) have the potential to revolutionize communications by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure communications in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure communications in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models (LLMs) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure communication tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低空无线网络中基于大人工智能模型的安全通信：概念、视角与案例研究</div>
<div class="mono" style="margin-top:8px">低空无线网络（LAWNs）通过支持城市包裹配送、空中巡检和空中出租车等应用，具有革新通信的潜力。然而，与传统无线网络相比，LAWNs因低空作业、频繁移动及依赖非授权频谱而面临独特的安全挑战，使其更易遭受恶意攻击。本文研究了LAWNs中基于大人工智能模型（LAM）的安全通信解决方案。具体而言，我们首先探讨了LAWNs中传统AI方法所加剧的安全风险与重要局限，随后介绍了LAM的基本概念，并深入分析了LAM应对这些挑战的作用。为验证LAM在LAWN安全通信中的实际效益，我们提出了一种基于LAM的新型优化框架，该框架利用大语言模型（LLMs）在人工设计表征的基础上生成增强状态特征，并据此设计内在奖励，从而提升安全通信任务中强化学习的性能。通过典型案例研究，仿真结果验证了所提框架的有效性。最后，我们展望了将LAM集成至安全LAWN应用的未来方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the unique security vulnerabilities of low-altitude wireless networks (LAWNs), which arise from their operational environment and mobility, and the limitations of traditional AI methods in addressing them. The proposed method introduces a novel optimization framework that leverages large AI models (LAMs), specifically using large language models to generate enhanced state features and design intrinsic rewards to improve reinforcement learning for secure communication tasks. The main experimental results from a case study validate the effectiveness of this LAM-enabled framework in enhancing security performance for LAWNs.</div>
<div class="mono" style="margin-top:8px">本文的动机在于低空无线网络因其运行环境和移动性带来的独特安全漏洞，以及传统人工智能方法在应对这些漏洞时的局限性。所提出的方法引入了一个新颖的优化框架，该框架利用大型人工智能模型，特别是使用大语言模型来生成增强的状态特征并设计相应的内在奖励，从而改进用于安全通信任务的强化学习性能。案例研究的主要实验结果验证了这种基于大型人工智能模型的框架在提升低空无线网络安全性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Opportunistic Routing in Software-Defined LEO-Terrestrial Systems</div>
<div class="meta-line">Authors: Sivaram Krishnan, Zhouyou Gu, Jihong Park, Sung-Min Oh, Jinho Choi</div>
<div class="meta-line">First: 2026-01-20T07:01:14+00:00 · Latest: 2026-01-20T07:01:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13662v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of large-scale low Earth orbit (LEO) satellite constellations is driving the need for intelligent routing strategies that can effectively deliver data to terrestrial networks under rapidly time-varying topologies and intermittent gateway visibility. Leveraging the global control capabilities of a geostationary (GEO)-resident software-defined networking (SDN) controller, we introduce opportunistic routing, which aims to minimize delivery delay by forwarding packets to any currently available ground gateways rather than fixed destinations. This makes it a promising approach for achieving low-latency and robust data delivery in highly dynamic LEO networks. Specifically, we formulate a constrained stochastic optimization problem and employ a residual reinforcement learning framework to optimize opportunistic routing for reducing transmission delay. Simulation results over multiple days of orbital data demonstrate that our method achieves significant improvements in queue length reduction compared to classical backpressure and other well-known queueing algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>软件定义低轨-地面系统中基于强化学习的机遇路由研究</div>
<div class="mono" style="margin-top:8px">大规模低地球轨道（LEO）卫星星座的激增，迫切需要能在快速时变拓扑和间歇性网关可见性条件下，有效向地面网络传输数据的智能路由策略。利用地球静止轨道（GEO）驻留的软件定义网络（SDN）控制器的全局控制能力，我们提出机遇路由方法，其目标是通过将数据包转发至当前任何可用的地面网关（而非固定目的地）来最小化传输时延。这使其成为在高度动态的低轨网络中实现低时延、鲁棒数据传输的有效途径。具体而言，我们构建了一个约束随机优化问题，并采用残差强化学习框架来优化机遇路由以降低传输时延。基于多日轨道数据的仿真结果表明，相较于经典背压算法及其他知名排队算法，本方法在队列长度缩减方面取得了显著改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need for intelligent routing in large-scale low Earth orbit (LEO) satellite constellations, which face challenges like rapidly changing topologies and intermittent gateway visibility. The method leverages a geostationary (GEO)-resident software-defined networking (SDN) controller to implement opportunistic routing, formulated as a constrained stochastic optimization problem and solved using a residual reinforcement learning framework to minimize delivery delay by forwarding packets to any available ground gateways. Experimental results from multi-day orbital simulations show that this approach significantly reduces queue length compared to classical backpressure and other queueing algorithms, demonstrating its effectiveness for low-latency, robust data delivery in dynamic LEO-terrestrial systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大规模低地球轨道（LEO）卫星星座对智能路由策略的需求，这些星座面临拓扑快速变化和网关可见性间歇性等挑战。方法上，利用地球静止轨道（GEO）驻留的软件定义网络（SDN）控制器实现机会路由，将其建模为一个约束随机优化问题，并采用残差强化学习框架进行优化，通过将数据包转发至任何可用地面网关来最小化传输延迟。基于多日轨道数据的仿真实验结果表明，与经典背压算法及其他知名排队算法相比，该方法在队列长度减少方面取得了显著改进，证明了其在动态LEO-地面系统中实现低延迟、鲁棒数据传输的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn&#x27;t</div>
<div class="meta-line">Authors: Quy-Anh Dang, Chris Ngo</div>
<div class="meta-line">First: 2025-03-20T15:13:23+00:00 · Latest: 2026-01-20T06:53:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.16219v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.16219v2">PDF</a> · <a href="https://github.com/knoveleng/open-rs">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at https://github.com/knoveleng/open-rs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习在小规模语言模型推理中的应用：有效方法与局限分析</div>
<div class="mono" style="margin-top:8px">提升大语言模型（LLMs）的推理能力通常依赖海量计算资源和数据集，这在资源受限场景中难以实现。本研究探索了利用强化学习（RL）提升小规模LLMs推理能力的潜力，聚焦于15亿参数的DeepSeek-R1-Distill-Qwen-1.5B模型，并在严格限制下进行实验：使用4张NVIDIA A40 GPU（每张48GB显存）在24小时内完成训练。通过改进分组相对策略优化（GRPO）算法并构建精炼的高质量数学推理数据集，我们开展了三项实验以探究模型行为与性能。结果表明，仅使用7,000个样本和42美元训练成本（基线模型需数千美元），即可实现推理能力的快速提升——例如AMC23准确率从63%升至80%，AIME24达到46.7%，超越o1-preview模型。但延长训练也暴露出优化不稳定性和生成长度限制等挑战。这些发现凸显了基于RL的微调对小规模LLMs的有效性，为资源有限环境提供了可扩展的推理能力提升方案。我们已开源代码与数据集，为资源受限环境下构建可扩展的推理型LLMs奠定基础。所有资源详见：https://github.com/knoveleng/open-rs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high computational cost of enhancing reasoning in large language models (LLMs), this study explores whether reinforcement learning (RL) can effectively improve reasoning in small LLMs under strict resource constraints. The method adapts the Group Relative Policy Optimization (GRPO) algorithm and uses a compact, high-quality mathematical reasoning dataset to fine-tune a 1.5-billion-parameter model on limited hardware within 24 hours. Experimental results show rapid performance gains, with accuracy on benchmarks like AMC23 increasing from 63% to 80% and AIME24 reaching 46.7% at a training cost of only $42, though challenges like optimization instability arose with longer training, demonstrating RL&#x27;s potential as a cost-effective alternative for resource-limited settings.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决增强大型语言模型（LLM）推理能力通常需要大量计算资源的问题，探索在严格资源限制下，使用强化学习（RL）提升小型LLM推理能力的可行性。方法上，研究采用了分组相对策略优化（GRPO）算法，并利用一个精炼的高质量数学推理数据集，在有限的GPU资源上对15亿参数的模型进行了24小时内的微调。主要实验结果表明，模型推理能力快速提升，例如在AMC23上的准确率从63%提高到80%，在AIME24上达到46.7%，训练成本仅42美元，但更长时间的训练也出现了优化不稳定等挑战，这凸显了RL作为一种经济高效的替代方案在资源有限环境中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Myong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo, Hyondong Oh</div>
<div class="meta-line">First: 2026-01-20T06:46:09+00:00 · Latest: 2026-01-20T06:46:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13657v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13657v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于激光雷达深度强化学习的无人机集群无通信协同导航</div>
<div class="mono" style="margin-top:8px">本文提出一种基于深度强化学习（DRL）的控制器，用于无人机集群在通信受限环境下的协同导航，使其能在复杂多障碍环境中稳定运行。受生物集群中个体无需显式通信即可引导群体的启发，我们采用隐式领航-跟随框架：仅领航无人机掌握目标信息，跟随无人机仅通过机载激光雷达感知学习鲁棒策略，无需个体间通信或识别领航者。系统通过激光雷达点云聚类与扩展卡尔曼滤波实现稳定的邻居跟踪，提供不依赖外部定位系统的可靠感知。核心DRL控制器在GPU加速的Nvidia Isaac Sim中训练，使跟随无人机仅凭局部感知即可学习平衡集群编队与避障的复杂涌现行为，实现隐式跟随领航者并稳健应对遮挡与视场受限等感知挑战。通过大量仿真及五架无人机集群的实地实验验证了方法的鲁棒性与仿真到现实的迁移能力，成功在多样室内外环境中实现无需通信与外部定位的集群导航。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for UAV swarms to operate collectively in communication-denied and obstacle-rich environments, this paper develops a deep reinforcement learning controller inspired by biological swarms. The method employs an implicit leader-follower framework where only the leader knows the goal; followers use only onboard LiDAR sensing, enhanced by point clustering and an extended Kalman filter for neighbor tracking, to learn policies without communication or leader identification. Experimental results from simulations and real-world tests with five UAVs confirm the approach&#x27;s robustness and successful sim-to-real transfer, enabling collective navigation in diverse indoor and outdoor settings.</div>
<div class="mono" style="margin-top:8px">本文的动机是让无人机群能在通信受阻且障碍物密集的环境中实现集体导航，为此受生物集群启发提出了一种深度强化学习控制器。该方法采用隐式领导者-跟随者框架，仅领导者知晓目标位置；跟随者仅依赖机载激光雷达感知，并通过点云聚类和扩展卡尔曼滤波进行邻居跟踪，从而在不依赖通信或识别领导者的情况下学习策略。在仿真和五架无人机的真实世界实验中，该方法展现出鲁棒性并成功实现了从仿真到现实的迁移，能够在多样化的室内外环境中完成集体导航任务。</div>
</details>
</div>
<div class="card">
<div class="title">Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning</div>
<div class="meta-line">Authors: Yuchen Jiao, Jiin Woo, Gen Li, Gauri Joshi, Yuejie Chi</div>
<div class="meta-line">First: 2026-01-20T06:21:54+00:00 · Latest: 2026-01-20T06:21:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13642v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13642v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$, where $\|h^{\star}\|_{\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$, with only $\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平均奖励Q学习的样本复杂度：从单智能体到联邦强化学习</div>
<div class="mono" style="margin-top:8px">平均奖励强化学习通过最大化单位时间步长的平均奖励，为长期决策提供了理论框架。尽管Q学习是一种广泛使用的无模型算法，在折扣和有限时域马尔可夫决策过程（MDP）中已确立样本复杂度，但其在平均奖励设定下的理论保证仍有限。本研究针对弱通信假设下具有有限状态和动作空间的平均奖励MDP，提出了一种简单有效的Q学习算法，涵盖单智能体和联邦两种场景。对于单智能体情况，我们证明参数经过精心选择的Q学习可实现样本复杂度$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$，其中$\|h^{\star}\|_{\mathsf{sp}}$为偏差函数的跨度范数，较先前结果至少提升$\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$倍。在包含$M$个智能体的联邦场景中，我们证明协作可将单智能体样本复杂度降至$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$，且仅需$\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$轮通信。这些结果首次为平均奖励MDP建立了联邦Q学习算法，在样本复杂度和通信复杂度方面均具有可证明的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for theoretical guarantees in average-reward reinforcement learning, where Q-learning&#x27;s sample complexity was not well-understood, this paper proposes a Q-learning algorithm for weakly communicating Markov decision processes in both single-agent and federated settings. The method involves careful parameter selection and, for the federated case, collaboration among multiple agents. Experimentally, the single-agent algorithm achieves a sample complexity of $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$, significantly improving prior bounds, while federated learning reduces per-agent complexity to $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$ with only logarithmic communication rounds, establishing the first provably efficient federated Q-learning for average-reward problems.</div>
<div class="mono" style="margin-top:8px">本文的动机在于平均奖励强化学习中Q学习的样本复杂度缺乏理论保证，为此提出了一种适用于弱通信马尔可夫决策过程的Q学习算法，涵盖单智能体和联邦学习场景。方法上通过精心选择参数，并在联邦设置中实现多智能体协作。实验结果表明，单智能体算法的样本复杂度为$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$，较先前结果有显著提升；而联邦学习将每个智能体的样本复杂度降低至$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$，仅需对数级别的通信轮次，从而首次为平均奖励问题建立了具有可证明效率的联邦Q学习算法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Unified View of Large Language Model Post-Training</div>
<div class="meta-line">Authors: Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, Bowen Zhou</div>
<div class="meta-line">First: 2025-09-04T17:40:33+00:00 · Latest: 2026-01-20T05:05:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04419v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04419v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向大语言模型后训练的统一视角</div>
<div class="mono" style="margin-top:8px">现代语言模型后训练的数据来源主要有两种：在线数据（模型生成的推演轨迹）和离线数据（人类或其他模型的示范数据）。这两类数据通常分别用于强化学习（RL）和监督微调（SFT）等方法。本文论证了这些方法并非相互矛盾，而是同一优化过程的不同实例。我们推导出统一策略梯度估计器，并通过在不同数据分布假设及多种偏差-方差权衡下计算常见目标的梯度，呈现了广泛后训练方法的统一框架。该梯度估计器由四个可互换部分构成：稳定化掩码、参考策略分母、优势估计和似然梯度。基于理论发现，我们提出混合后训练（HPT）算法，能动态选择不同的训练信号。HPT旨在实现对示范数据的有效利用与稳定探索，同时不牺牲已习得的推理模式。我们通过大量实验与消融研究验证了统一理论框架与HPT的有效性。在六个数学推理基准和两个分布外测试集上，HPT在不同规模与架构的模型中均持续超越强基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the observation that two common post-training data sources for large language models—online model-generated rollouts and offline human demonstrations—are typically handled by separate methods like Reinforcement Learning and Supervised Fine-Tuning. The authors propose a unified theoretical view, showing these methods are instances of a single optimization process, and derive a Unified Policy Gradient Estimator composed of four interchangeable components. Their method, Hybrid Post-Training (HPT), dynamically selects training signals to balance exploitation of demonstrations and stable exploration. Experimental results across six mathematical reasoning benchmarks and two out-of-distribution suites demonstrate that HPT consistently outperforms strong baselines across models of varying scales and families.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到大型语言模型后训练中两种常见数据源——在线模型生成数据和离线人类演示数据——通常由强化学习和监督微调等不同方法分别处理。作者提出了一个统一的理论视角，表明这些方法是一个单一优化过程的不同实例，并推导出一个由四个可互换组件构成的统一策略梯度估计器。基于此，他们提出了混合后训练方法，动态选择训练信号以平衡对演示数据的利用和稳定的探索。在六个数学推理基准和两个分布外测试集上的实验结果表明，该方法在不同规模和系列的模型上均一致超越了强基线。</div>
</details>
</div>
<div class="card">
<div class="title">Continual Knowledge Adaptation for Reinforcement Learning</div>
<div class="meta-line">Authors: Jinwu Hu, Zihao Lian, Zhiquan Wen, Chenghao Li, Guohao Chen, Xutao Wen, Bin Xiao, Mingkui Tan</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-22T07:25:41+00:00 · Latest: 2026-01-20T04:47:18+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19314v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19314v2">PDF</a> · <a href="https://github.com/Fhujinwu/CKA-RL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning enables agents to learn optimal behaviors through interactions with environments. However, real-world environments are typically non-stationary, requiring agents to continuously adapt to new tasks and changing conditions. Although Continual Reinforcement Learning facilitates learning across multiple tasks, existing methods often suffer from catastrophic forgetting and inefficient knowledge utilization. To address these challenges, we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL), which enables the accumulation and effective utilization of historical knowledge. Specifically, we introduce a Continual Knowledge Adaptation strategy, which involves maintaining a task-specific knowledge vector pool and dynamically using historical knowledge to adapt the agent to new tasks. This process mitigates catastrophic forgetting and enables efficient knowledge transfer across tasks by preserving and adapting critical model parameters. Additionally, we propose an Adaptive Knowledge Merging mechanism that combines similar knowledge vectors to address scalability challenges, reducing memory requirements while ensuring the retention of essential knowledge. Experiments on three benchmarks demonstrate that the proposed CKA-RL outperforms state-of-the-art methods, achieving an improvement of 4.20% in overall performance and 8.02% in forward transfer. The source code is available at https://github.com/Fhujinwu/CKA-RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习的持续知识适应方法</div>
<div class="mono" style="margin-top:8px">强化学习使智能体能够通过与环境的交互学习最优行为。然而，现实环境通常是非平稳的，要求智能体持续适应新任务和变化的条件。尽管持续强化学习有助于跨多个任务学习，但现有方法常受灾难性遗忘和知识利用效率低下的困扰。为解决这些挑战，我们提出强化学习的持续知识适应方法（CKA-RL），该方法能够积累并有效利用历史知识。具体而言，我们引入一种持续知识适应策略，包括维护任务特定的知识向量池，并动态使用历史知识使智能体适应新任务。该过程通过保留和调整关键模型参数，缓解了灾难性遗忘，并实现了跨任务的高效知识迁移。此外，我们提出一种自适应知识融合机制，通过合并相似知识向量应对可扩展性挑战，在确保保留核心知识的同时降低内存需求。在三个基准测试上的实验表明，所提出的CKA-RL方法优于现有先进方法，整体性能提升4.20%，前向迁移性能提升8.02%。源代码发布于 https://github.com/Fhujinwu/CKA-RL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of catastrophic forgetting and inefficient knowledge transfer in continual reinforcement learning for non-stationary environments. The authors propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL), a method that maintains a pool of task-specific knowledge vectors and dynamically adapts historical knowledge to new tasks, preserving critical parameters to mitigate forgetting. An adaptive knowledge merging mechanism is also introduced to combine similar vectors for scalability. Experimental results on three benchmarks show that CKA-RL outperforms state-of-the-art methods, improving overall performance by 4.20% and forward transfer by 8.02%.</div>
<div class="mono" style="margin-top:8px">本文针对非平稳环境中持续强化学习存在的灾难性遗忘和知识迁移效率低下问题展开研究。作者提出了持续知识适应的强化学习方法（CKA-RL），该方法通过维护任务特定的知识向量池，并动态利用历史知识来适应新任务，从而保留关键参数以缓解遗忘。同时引入了自适应知识合并机制，通过合并相似向量来解决可扩展性挑战。在三个基准测试上的实验结果表明，CKA-RL优于现有最先进方法，整体性能提升4.20%，前向迁移性能提升8.02%。</div>
</details>
</div>
<div class="card">
<div class="title">Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement</div>
<div class="meta-line">Authors: Mingyu Xu, Cheng Fang, Keyue Jiang, Yuqian Zheng, Yanghua Xiao, Baojian Zhou, Qifang Zhao, Suhang Zheng, Xiuwen Zhu, Jiyang Tang, Yongchi Zhao, Yijia Luo, Zhiqi Bai, Yuchi Xu, Wenbo Su, Wei Wang, Bing Zhao, Lin Qu, Xiaoxiao Xu</div>
<div class="meta-line">First: 2026-01-04T15:23:18+00:00 · Latest: 2026-01-20T04:18:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01562v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01562v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Logics-STEM：通过失败驱动的后训练与文档知识增强赋能大语言模型推理</div>
<div class="mono" style="margin-top:8px">我们提出Logics-STEM，这是一种基于Logics-STEM-SFT-Dataset微调的最先进推理模型。该数据集规模达1000万，质量高且多样性丰富，是最大规模的开源长链思维语料库之一。Logics-STEM专注于科学、技术、工程和数学（STEM）领域的推理任务，在STEM相关基准测试中表现卓越，在80亿参数规模上平均优于次优模型4.68%。我们将性能提升归因于数据-算法协同设计引擎，通过联合优化使其适配推理背后的黄金标准分布。数据层面，Logics-STEM-SFT-Dataset通过精心设计的五阶段数据策管引擎构建，包括标注、去重、净化、蒸馏和分层采样，确保质量、多样性和可扩展性。算法层面，我们的失败驱动后训练框架在监督微调阶段，针对模型失败区域进行定向知识检索与数据合成，有效指导第二阶段SFT或强化学习，以更好地拟合目标分布。Logics-STEM的卓越实证性能揭示了大规模开源数据与精心设计合成数据结合的巨大潜力，凸显了数据-算法协同设计在通过后训练增强推理能力中的关键作用。我们公开提供Logics-STEM模型（80亿和320亿参数版本）及Logics-STEM-SFT-Dataset（1000万和降采样220万版本），以支持开源社区的未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Logics-STEM, a reasoning model motivated by the need to enhance large language model (LLM) performance in STEM domains through improved data and training methods. The method involves a data-algorithm co-design engine: data-wise, it constructs a high-quality 10M-scale dataset via a five-stage curation process (annotation, deduplication, decontamination, distillation, and stratified sampling), while algorithm-wise, it employs a failure-driven post-training framework that uses targeted knowledge retrieval and synthetic data around model failures to guide further fine-tuning or reinforcement learning. Experimental results show that Logics-STEM achieves state-of-the-art performance on STEM benchmarks, with an average improvement of 4.68% over the next-best 8B-scale model, demonstrating the effectiveness of the co-design approach in boosting reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文提出了Logics-STEM模型，其动机是通过改进数据和训练方法来增强大语言模型在科学、技术、工程和数学（STEM）领域的推理能力。方法上采用数据与算法协同设计引擎：数据方面，通过精心设计的五阶段数据整理流程（包括标注、去重、去污、蒸馏和分层采样）构建了一个高质量、规模达1000万的数据集；算法方面，采用失败驱动的后训练框架，利用针对模型失败区域的目标知识检索和合成数据来指导第二阶段的监督微调或强化学习。主要实验结果表明，Logics-STEM在STEM相关基准测试中表现出色，相比同规模（80亿参数）的最佳模型平均提升4.68%，验证了协同设计在提升推理能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions</div>
<div class="meta-line">Authors: Xiaorui Wu, Fei Li, Xiaofeng Mao, Xin Zhang, Li Zheng, Yuxiang Peng, Chong Teng, Donghong Ji, Zhuang Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-29T14:26:46+00:00 · Latest: 2026-01-20T03:57:59+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23473v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.23473v3">PDF</a> · <a href="https://github.com/FishT0ucher/EVOREFUSE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 85.34% higher average refusal triggering rate across 9 LLMs without a safety-prior system prompt, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. With supervised fine-tuning on EVOREFUSE-ALIGN, LLAMA3.1-8B-INSTRUCT achieves up to 29.85% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context. Our code and datasets are available at https://github.com/FishT0ucher/EVOREFUSE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EVOREFUSE：基于进化提示优化的LLM过度拒绝伪恶意指令评估与缓解方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）常过度拒绝伪恶意指令：语义无害的查询因保守的安全对齐触发不必要的拒绝，严重影响用户体验。收集此类指令对评估和缓解过度拒绝至关重要，但现有指令构建方法（如人工创建或指令改写）缺乏可扩展性或难以生成足够多样且有效的拒绝诱导提示。为此，我们提出EVOREFUSE——一种通过进化算法生成多样化伪恶意指令的提示优化方法，能持续引发LLMs的明确拒绝。该方法通过变异策略和重组在指令空间进行多方向探索，迭代优化种子指令以最大化LLM拒绝概率的证据下界。基于EVOREFUSE，我们构建了两个新数据集：1）EVOREFUSE-TEST基准集（582条指令），在9个LLMs上实现比次优基准高85.34%的平均拒绝触发率（无安全前置提示）、34.86%的词汇多样性提升及40.03%的LLM响应置信度改进；2）EVOREFUSE-ALIGN对齐集（3000条带响应指令），适用于监督与偏好对齐训练。使用该数据集监督微调后，LLAMA3.1-8B-INSTRUCT的过度拒绝率比次优对齐数据集训练的模型降低29.85%，且不影响安全性。EVOREFUSE-TEST分析表明，模型过度拒绝主要源于对敏感关键词的过度聚焦而忽略整体语境。代码与数据集已开源：https://github.com/FishT0ucher/EVOREFUSE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of large language models (LLMs) over-refusing to respond to pseudo-malicious instructions—harmless queries that trigger unnecessary safety refusals, degrading user experience. To overcome the limitations of existing instruction curation methods, which lack scalability or diversity, the authors propose EVOREFUSE, an evolutionary prompt optimization method that uses mutation and recombination strategies to iteratively evolve seed instructions, maximizing the evidence lower bound on LLM refusal probability. Experimental results show that the created benchmark dataset, EVOREFUSE-TEST, achieves an 85.34% higher average refusal triggering rate across 9 LLMs with greater lexical diversity and improved confidence scores compared to the next-best benchmark, while supervised fine-tuning with the EVOREFUSE-ALIGN dataset reduces over-refusals by up to 29.85% in LLAMA3.1-8B-INSTRUCT without compromising safety.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型对伪恶意指令过度拒绝的问题展开研究，这些指令本质无害但会触发不必要的安全拒绝，损害用户体验。为解决现有指令构建方法在可扩展性和多样性上的不足，作者提出了EVOREFUSE，一种基于进化算法的提示优化方法，通过突变和重组策略迭代演化种子指令，以最大化模型拒绝概率的证据下界。实验结果表明，所构建的基准数据集EVOREFUSE-TEST在9个大语言模型上实现了比次优基准高85.34%的平均拒绝触发率，并具有更高的词汇多样性和置信度得分；同时，使用EVOREFUSE-ALIGN数据集进行监督微调后，LLAMA3.1-8B-INSTRUCT模型的过度拒绝减少了29.85%，且未影响安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Behavior Knowledge Merge in Reinforced Agentic Models</div>
<div class="meta-line">Authors: Xiangchi Yuan, Dachuan Shi, Chunhui Zhang, Zheyuan Liu, Shenglong Yao, Soroush Vosoughi, Wenke Lee</div>
<div class="meta-line">First: 2026-01-20T03:56:53+00:00 · Latest: 2026-01-20T03:56:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13572v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13572v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL&#x27;s non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化智能体模型中的行为知识融合</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在后训练中至关重要，尤其对于需要专业推理行为的智能体模型。在此背景下，模型融合提供了一种实用机制，可将来自不同任务的多个RL训练智能体整合为单一通用模型。然而，现有融合方法专为监督微调（SFT）设计，在保持RL训练智能体模型的任务特定能力方面表现欠佳。其根源在于RL与SFT的任务向量不匹配：策略RL产生的任务向量高度稀疏且异质，而SFT式融合隐含假设任务向量密集且全局可比。当在此不匹配情况下应用标准全局平均时，RL编码关键任务特定行为的非重叠任务向量会被削弱，参数更新被稀释。为解决此问题，我们提出强化智能体融合（RAM），这是一个专为RL训练智能体模型设计的分布感知融合框架。RAM解耦共享与任务特定独有参数更新，平均共享组件的同时选择性保留并重新缩放独有组件以抵消参数更新稀释。跨多智能体领域和模型架构的实验表明，RAM不仅超越融合基线，还能解锁智能体间的协同潜力，实现优于各领域专用智能体的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need to effectively merge multiple reinforcement learning (RL)-trained agentic models into a single generalist model, as existing merging methods designed for supervised fine-tuning are suboptimal due to a mismatch in task vector characteristics—RL induces sparse, heterogeneous vectors while standard methods assume dense, comparable ones, leading to diluted parameter updates and loss of critical behaviors. To address this, the method introduces Reinforced Agent Merging (RAM), a distribution-aware framework that disentangles shared and task-specific parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract dilution. Experimental results across various agent domains and model architectures show that RAM not only outperforms merging baselines but also unlocks synergistic potential, achieving performance superior to that of specialized agents in their respective domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于需要将多个经过强化学习训练的智能体模型有效合并为一个通用模型，因为现有的基于监督微调的合并方法存在不足，这是由于任务向量特征不匹配所致——强化学习产生稀疏、异质的向量，而标准方法假设向量密集且可比，导致参数更新稀释和关键行为丢失。为此，该方法提出了强化智能体合并（RAM），这是一个分布感知的框架，能够解耦共享和任务特定的参数更新，对共享部分进行平均，同时选择性地保留和重新缩放独特部分以抵消稀释效应。在多个智能体领域和模型架构上的实验结果表明，RAM不仅优于合并基线，还能释放智能体间的协同潜力，在各自领域实现优于专用智能体的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework</div>
<div class="meta-line">Authors: Yanheng Li, Zhichen Pu, Lijiang Yang, Zehao Zhou, Yi Qin Gao</div>
<div class="meta-line">First: 2026-01-20T03:41:02+00:00 · Latest: 2026-01-20T03:41:02+00:00</div>
<div class="meta-line">Comments: Total 43 pages: 32 pages Main Text + 11 pages SI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13564v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13564v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据与物理双驱动生成框架下的多目标荧光分子设计</div>
<div class="mono" style="margin-top:8px">设计具有定制光学与理化性质的荧光小分子，需在广阔且未充分探索的化学空间中导航，同时满足多目标与约束条件。传统的生成-评分-筛选方法在实际设计规范下因搜索效率低、机器学习预测泛化性不可靠及量子化学计算成本过高而难以实用。本文提出LUMOS，一种数据与物理双驱动的荧光分子逆向设计框架。LUMOS在共享潜在表征中耦合生成器与预测器，实现从规格到分子的直接设计与高效探索。此外，LUMOS将神经网络与快速含时密度泛函理论（TD-DFT）计算流程结合，构建了一套在速度、精度和泛化性间权衡的互补预测器，确保跨场景的可靠性质预测。最后，LUMOS采用与多目标进化算法集成的性质引导扩散模型，实现多目标与约束下的从头设计与分子优化。在全面基准测试中，LUMOS在荧光性质预测的精度、泛化性与物理合理性上均优于基线模型，并在多目标骨架与片段级分子优化中展现卓越性能。通过TD-DFT和分子动力学（MD）模拟的进一步验证表明，LUMOS能生成满足多种目标规格的有效荧光团。总体而言，这些成果确立了LUMOS作为通用荧光团逆向设计的数据-物理双驱动框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of efficiently designing fluorescent small molecules with specific optical and physicochemical properties, which is hindered by the vast chemical space, low search efficiency of conventional methods, and the high cost of quantum calculations. To overcome this, the authors introduce LUMOS, a data-and-physics dual-driven generative framework that couples a generator and predictor within a shared latent representation, integrates neural networks with fast time-dependent density functional theory (TD-DFT) for complementary property prediction, and employs a property-guided diffusion model with multi-objective evolutionary algorithms for molecular optimization. Experimental results show that LUMOS outperforms baseline models in accuracy, generalizability, and physical plausibility for fluorescence prediction, excels in multi-objective molecular optimization at scaffold and fragment levels, and generates valid fluorophores meeting target specifications as validated by TD-DFT and molecular dynamics simulations.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决高效设计具有特定光学和物理化学性质的荧光小分子的挑战，这一过程因化学空间广阔、传统方法搜索效率低以及量子计算成本高昂而受阻。为此，作者提出了LUMOS，一个数据与物理双驱动的生成框架，它将生成器和预测器耦合在共享潜在表示中，整合神经网络与快速含时密度泛函理论（TD-DFT）以实现互补性质预测，并采用属性引导的扩散模型结合多目标进化算法进行分子优化。实验结果表明，LUMOS在荧光性质预测的准确性、泛化性和物理合理性上优于基线模型，在支架和片段级别的多目标分子优化中表现优异，并通过TD-DFT和分子动力学模拟验证了其能生成符合目标规格的有效荧光团。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models</div>
<div class="meta-line">Authors: Changshuo Zhang</div>
<div class="meta-line">First: 2026-01-20T02:32:39+00:00 · Latest: 2026-01-20T02:32:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13533v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13533v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model&#x27;s decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the &quot;reason first, recommend later&quot; paradigm to achieve &quot;reasoning while recommending&quot;, specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model&#x27;s effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推荐中的推理：生成式重排序模型中的熵引导隐式推理机制</div>
<div class="mono" style="margin-top:8px">强化学习因其探索-利用能力在生成式重排序场景中至关重要，但现有生成方法大多难以适应列表生成过程中模型动态熵变带来的难度变化，导致精准捕捉复杂偏好面临挑战。鉴于语言模型通过融合推理能力已取得显著突破，本研究借鉴该思路引入隐式推理机制，实验验证表明该机制能有效降低模型决策过程的熵值。基于此，我们提出熵引导隐式推理推荐模型，其具备三大核心优势：首先，摒弃“先推理后推荐”范式，实现“边推荐边推理”，针对列表生成的高难度特性设计实时推理机制；其次，通过上下文感知推理令牌与动态温度调节实现熵引导的变长推理，在推理中拓展探索广度，在推荐中提升利用精度，达成更精准的自适应探索-利用平衡；再次，采用轻量级集成设计，无需复杂独立模块或后处理，可便捷适配现有模型。在两个真实数据集上的实验结果验证了模型有效性，其显著优势在于兼容现有生成式重排序模型以提升性能。进一步分析也证明了其实践部署价值与研究潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge that existing generative re-ranking models struggle to adapt to dynamic entropy changes during list generation, which hinders accurate preference capture, this paper introduces an Entropy-Guided Latent Reasoning (EGLR) model. The method integrates a latent reasoning mechanism that enables &quot;reasoning while recommending&quot; through context-aware reasoning tokens and dynamic temperature adjustment, allowing real-time, variable-length reasoning to better balance exploration and exploitation without adding complex modules. Experimental results on two real-world datasets show that EGLR effectively reduces decision entropy and enhances performance, with the advantage of being lightweight and easily adaptable to existing generative re-ranking models to boost their effectiveness.</div>
<div class="mono" style="margin-top:8px">针对现有生成式重排序模型难以适应列表生成过程中动态熵变化、从而无法准确捕捉复杂偏好的问题，本文提出了熵引导潜在推理（EGLR）推荐模型。该方法引入潜在推理机制，通过上下文感知的推理令牌和动态温度调整，实现“边推荐边推理”的实时变长推理，以更精准地平衡探索与利用，且无需复杂独立模块或后处理。在两个真实数据集上的实验结果表明，EGLR有效降低了决策熵并提升了性能，其显著优势在于能够轻量级地适配现有生成式重排序模型，增强其效果，具备实际部署价值与研究潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Optimistic Gradient Learning with Hessian Corrections for High-Dimensional Black-Box Optimization</div>
<div class="meta-line">Authors: Yedidya Kfir, Elad Sarafian, Sarit Kraus, Yoram Louzoun</div>
<div class="meta-line">Venue: AAAI2026</div>
<div class="meta-line">First: 2025-02-07T11:03:50+00:00 · Latest: 2026-01-19T20:14:58+00:00</div>
<div class="meta-line">Comments: We develop a black-box optimization algorithm that learns gradients with neural models and can be applied to solve non-convex high dimensional real-world problems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04829v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.04829v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Black-box algorithms are designed to optimize functions without relying on their underlying analytical structure or gradient information, making them essential when gradients are inaccessible or difficult to compute. Traditional methods for solving black-box optimization (BBO) problems predominantly rely on non-parametric models and struggle to scale to large input spaces. Conversely, parametric methods that model the function with neural estimators and obtain gradient signals via backpropagation may suffer from significant gradient errors. A recent alternative, Explicit Gradient Learning (EGL), which directly learns the gradient using a first-order Taylor approximation, has demonstrated superior performance over both parametric and non-parametric methods. In this work, we propose two novel gradient learning variants to address the robustness challenges posed by high-dimensional, complex, and highly non-linear problems. Optimistic Gradient Learning (OGL) introduces a bias toward lower regions in the function landscape, while Higher-order Gradient Learning (HGL) incorporates second-order Taylor corrections to improve gradient accuracy. We combine these approaches into the unified OHGL algorithm, achieving state-of-the-art (SOTA) performance on the synthetic COCO suite. Additionally, we demonstrate OHGLs applicability to high-dimensional real-world machine learning (ML) tasks such as adversarial training and code generation. Our results highlight OHGLs ability to generate stronger candidates, offering a valuable tool for ML researchers and practitioners tackling high-dimensional, non-linear optimization challenges</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高维黑盒优化的Hessian修正乐观梯度学习</div>
<div class="mono" style="margin-top:8px">黑盒算法旨在不依赖函数解析结构或梯度信息的情况下优化函数，在梯度难以获取或计算时至关重要。传统黑盒优化方法主要依赖非参数模型，难以扩展至高维输入空间。而通过神经估计器建模并利用反向传播获取梯度信号的参数化方法，则可能受显著梯度误差影响。近期提出的显式梯度学习方法通过一阶泰勒近似直接学习梯度，已展现出优于参数化和非参数化方法的性能。本研究提出两种新型梯度学习变体以应对高维、复杂、高度非线性问题带来的鲁棒性挑战：乐观梯度学习引入对函数景观低洼区域的偏置，高阶梯度学习则融入二阶泰勒修正以提升梯度精度。我们将这两种方法整合为统一的OHGL算法，在合成测试集COCO上实现了最先进性能。此外，我们验证了OHGL在高维现实机器学习任务（如对抗训练和代码生成）中的适用性。实验结果表明OHGL能生成更优候选解，为应对高维非线性优化挑战的机器学习研究者与实践者提供了有力工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of scaling black-box optimization to high-dimensional, complex functions where gradients are unavailable. The authors propose two novel gradient learning variants: Optimistic Gradient Learning (OGL), which biases the search toward lower function values, and Higher-order Gradient Learning (HGL), which uses second-order Taylor corrections to improve gradient accuracy. These are unified into the OHGL algorithm, which achieves state-of-the-art performance on the synthetic COCO benchmark and demonstrates strong results in real-world tasks like adversarial training and code generation, producing superior candidate solutions.</div>
<div class="mono" style="margin-top:8px">本文针对高维复杂函数中梯度不可获取的黑盒优化可扩展性挑战，提出了两种新颖的梯度学习方法：乐观梯度学习（OGL）通过偏向函数值较低区域来引导搜索，高阶梯度学习（HGL）则利用二阶泰勒校正提高梯度准确性。两者被统一为OHGL算法，在合成COCO基准测试中取得了最先进的性能，并在对抗训练和代码生成等实际机器学习任务中表现出色，能够生成更优的候选解。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0408.html">20260121_0408</a>
<a href="archive/20260120_0316.html">20260120_0316</a>
<a href="archive/20260119_0312.html">20260119_0312</a>
<a href="archive/20260118_0323.html">20260118_0323</a>
<a href="archive/20260117_0337.html">20260117_0337</a>
<a href="archive/20260116_0346.html">20260116_0346</a>
<a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
