<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-30 05:31</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251230_0531</div>
    <div class="row"><div class="card">
<div class="title">Cost-aware Stopping for Bayesian Optimization</div>
<div class="meta-line">Authors: Qian Xie, Linda Cai, Alexander Terenin, Peter I. Frazier, Ziv Scully</div>
<div class="meta-line">First: 2025-07-16T17:54:14+00:00 · Latest: 2025-12-26T18:48:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12453v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.12453v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In automated machine learning, scientific discovery, and other applications of Bayesian optimization, deciding when to stop evaluating expensive black-box functions in a cost-aware manner is an important but underexplored practical consideration. A natural performance metric for this purpose is the cost-adjusted simple regret, which captures the trade-off between solution quality and cumulative evaluation cost. While several heuristic or adaptive stopping rules have been proposed, they lack guarantees ensuring stopping before incurring excessive function evaluation costs. We propose a principled cost-aware stopping rule for Bayesian optimization that adapts to varying evaluation costs without heuristic tuning. Our rule is grounded in a theoretical connection to state-of-the-art cost-aware acquisition functions, namely the Pandora&#x27;s Box Gittins Index (PBGI) and log expected improvement per cost (LogEIPC). We prove a theoretical guarantee bounding the expected cost-adjusted simple regret incurred by our stopping rule when paired with either acquisition function. Across synthetic and empirical tasks, including hyperparameter optimization and neural architecture size search, pairing our stopping rule with PBGI or LogEIPC usually matches or outperforms other acquisition-function--stopping-rule pairs in terms of cost-adjusted simple regret.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯优化的成本感知停止策略</div>
<div class="mono" style="margin-top:8px">在自动化机器学习、科学发现及其他贝叶斯优化应用中，如何以成本感知的方式决定何时停止评估昂贵的黑盒函数，是一个重要但尚未充分探索的实际问题。为此，成本调整简单遗憾是一种自然的性能度量指标，它捕捉了解决方案质量与累积评估成本之间的权衡。尽管已有多种启发式或自适应停止规则被提出，但它们缺乏确保在产生过高函数评估成本前停止的理论保证。本文提出一种基于原则的成本感知停止规则，适用于贝叶斯优化，能够自适应变化的评估成本而无需启发式调参。该规则的理论基础源于与前沿成本感知采集函数（即潘多拉盒吉廷斯指数和成本对数期望改进）的理论关联。我们证明了当该停止规则与任一采集函数配对时，其预期成本调整简单遗憾的理论上界。在包括超参数优化和神经网络架构规模搜索在内的合成与实证任务中，将本停止规则与潘多拉盒吉廷斯指数或成本对数期望改进配对使用，通常在成本调整简单遗憾方面达到或优于其他采集函数-停止规则组合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the practical need for cost-aware stopping in Bayesian optimization, where evaluating expensive black-box functions requires balancing solution quality with cumulative costs, as measured by cost-adjusted simple regret. The authors propose a principled stopping rule that adapts to varying evaluation costs without heuristic tuning, grounded in theoretical connections to state-of-the-art cost-aware acquisition functions like Pandora&#x27;s Box Gittins Index (PBGI) and log expected improvement per cost (LogEIPC). They provide a theoretical guarantee bounding the expected cost-adjusted simple regret and demonstrate through synthetic and empirical tasks, including hyperparameter optimization and neural architecture search, that their rule paired with PBGI or LogEIPC typically matches or outperforms other acquisition-stopping pairs in terms of cost-adjusted simple regret.</div>
<div class="mono" style="margin-top:8px">本文针对贝叶斯优化中成本感知停止的实际需求，其中评估昂贵的黑盒函数需要在解决方案质量与累积成本之间取得平衡，通过成本调整简单遗憾来衡量。作者提出了一种无需启发式调优、能自适应不同评估成本的原则性停止规则，其理论基础与最先进的成本感知采集函数（如潘多拉盒吉廷斯指数和成本对数期望改进）相关联。他们提供了理论保证，限制了该停止规则在预期成本调整简单遗憾上的上限，并通过合成和实证任务（包括超参数优化和神经架构搜索）证明，该规则与上述采集函数配对时，在成本调整简单遗憾方面通常匹配或优于其他采集-停止组合。</div>
</details>
</div>
<div class="card">
<div class="title">Accelerating Diffusion Planners in Offline RL via Reward-Aware Consistency Trajectory Distillation</div>
<div class="meta-line">Authors: Xintong Duan, Yutong He, Fahim Tajwar, Ruslan Salakhutdinov, J. Zico Kolter, Jeff Schneider</div>
<div class="meta-line">First: 2025-06-09T14:48:19+00:00 · Latest: 2025-12-26T17:50:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07822v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.07822v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although diffusion models have achieved strong results in decision-making tasks, their slow inference speed remains a key limitation. While consistency models offer a potential solution, existing applications to decision-making either struggle with suboptimal demonstrations under behavior cloning or rely on complex concurrent training of multiple networks under the actor-critic framework. In this work, we propose a novel approach to consistency distillation for offline reinforcement learning that directly incorporates reward optimization into the distillation process. Our method achieves single-step sampling while generating higher-reward action trajectories through decoupled training and noise-free reward signals. Empirical evaluations on the Gym MuJoCo, FrankaKitchen, and long horizon planning benchmarks demonstrate that our approach can achieve a 9.7% improvement over previous state-of-the-art while offering up to 142x speedup over diffusion counterparts in inference time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于奖励感知一致性轨迹蒸馏的离线强化学习扩散规划器加速方法</div>
<div class="mono" style="margin-top:8px">尽管扩散模型在决策任务中取得了显著成果，但其缓慢的推理速度仍是关键瓶颈。一致性模型虽提供了潜在解决方案，但现有决策应用要么在行为克隆下受限于次优示范，要么依赖演员-评论家框架中多个网络的复杂并行训练。本研究提出一种新颖的离线强化学习一致性蒸馏方法，将奖励优化直接融入蒸馏过程。该方法通过解耦训练和无噪声奖励信号，在实现单步采样的同时生成更高奖励的动作轨迹。在Gym MuJoCo、FrankaKitchen和长程规划基准测试中的实证评估表明，本方法相比先前最优性能提升9.7%，推理速度较扩散模型最高提升142倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work is motivated by the slow inference speed of diffusion models in decision-making tasks, despite their strong performance. The proposed method introduces a novel consistency distillation approach for offline reinforcement learning that directly incorporates reward optimization into the distillation process, enabling single-step sampling through decoupled training and noise-free reward signals. Experimental results on Gym MuJoCo, FrankaKitchen, and long-horizon planning benchmarks show that the method achieves a 9.7% performance improvement over previous state-of-the-art approaches while offering up to 142x inference speedup compared to diffusion models.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决扩散模型在决策任务中推理速度慢的关键限制。方法提出了一种用于离线强化学习的一致性蒸馏新方法，将奖励优化直接融入蒸馏过程，通过解耦训练和无噪声奖励信号实现单步采样。在Gym MuJoCo、FrankaKitchen和长时程规划基准上的实验结果表明，该方法相比先前最优方法性能提升9.7%，同时推理速度比扩散模型快达142倍。</div>
</details>
</div>
<div class="card">
<div class="title">Periodic Asynchrony: An Effective Method for Accelerating Reinforcement Learning for Large Language Models</div>
<div class="meta-line">Authors: Jian Lu</div>
<div class="meta-line">First: 2025-11-24T08:22:50+00:00 · Latest: 2025-12-26T15:48:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18871v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.18871v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>周期性异步：一种加速大语言模型强化学习的有效方法</div>
<div class="mono" style="margin-top:8px">自GRPO算法提出以来，强化学习（RL）日益受到关注，相关复现与应用尝试不断增多。然而，训练效率仍是关键挑战。主流RL框架通常将推理与训练部署于同一设备，虽通过资源整合降低成本，但其同步执行模式导致计算耦合，阻碍了推理与训练的并发进行。本研究回归推理与训练分离部署的策略，通过改进数据加载器，将传统同步架构转变为周期性异步框架，使各组件可按需独立弹性伸缩，而算法精度与同步方法完全等价，二者均属同策略方法。值得强调的是，我们在训练阶段采用统一的三模型架构，并提出了共享提示注意力掩码以减少重复计算。实践表明，这些工作在NPU平台上实现了至少三倍的RL训练整体性能提升，展现出广泛的应用潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational inefficiency of synchronous inference and training in mainstream reinforcement learning (RL) frameworks for large language models, this paper proposes a periodically asynchronous framework that decouples these components to enable independent, elastic scaling. The method introduces improvements in the data loader and employs a unified tri-model architecture along with a shared-prompt attention mask to reduce repetitive computations, maintaining algorithmic equivalence to synchronous on-policy strategies. Experimental results demonstrate that this approach achieves at least a threefold overall performance improvement in RL training on NPU platforms, highlighting its potential for broad application.</div>
<div class="mono" style="margin-top:8px">针对主流强化学习框架中同步推理与训练导致的计算效率低下问题，本研究提出了一种周期性异步框架，通过解耦这两个组件以实现独立弹性扩展。方法改进了数据加载器，采用统一的三模型架构和共享提示注意力掩码以减少重复计算，同时保持与同步策略在算法精度上的完全等价。实验结果表明，该框架在NPU平台上的强化学习训练中实现了至少三倍的整体性能提升，展现了其广泛应用的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Meta-Learning-Based Handover Management in NextG O-RAN</div>
<div class="meta-line">Authors: Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers</div>
<div class="meta-line">First: 2025-12-26T13:01:46+00:00 · Latest: 2025-12-26T13:01:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22022v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22022v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While traditional handovers (THOs) have served as a backbone for mobile connectivity, they increasingly suffer from failures and delays, especially in dense deployments and high-frequency bands. To address these limitations, 3GPP introduced Conditional Handovers (CHOs) that enable proactive cell reservations and user-driven execution. However, both handover (HO) types present intricate trade-offs in signaling, resource usage, and reliability. This paper presents unique, countrywide mobility management datasets from a top-tier mobile network operator (MNO) that offer fresh insights into these issues and call for adaptive and robust HO control in next-generation networks. Motivated by these findings, we propose CONTRA, a framework that, for the first time, jointly optimizes THOs and CHOs within the O-RAN architecture. We study two variants of CONTRA: one where users are a priori assigned to one of the HO types, reflecting distinct service or user-specific requirements, as well as a more dynamic formulation where the controller decides on-the-fly the HO type, based on system conditions and needs. To this end, it relies on a practical meta-learning algorithm that adapts to runtime observations and guarantees performance comparable to an oracle with perfect future information (universal no-regret). CONTRA is specifically designed for near-real-time deployment as an O-RAN xApp and aligns with the 6G goals of flexible and intelligent control. Extensive evaluations leveraging crowdsourced datasets show that CONTRA improves user throughput and reduces both THO and CHO switching costs, outperforming 3GPP-compliant and Reinforcement Learning (RL) baselines in dynamic and real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于元学习的下一代O-RAN切换管理</div>
<div class="mono" style="margin-top:8px">传统切换（THO）虽作为移动连接的核心支撑，但在密集部署和高频段场景中日益面临失效与延迟问题。为应对这些局限，3GPP引入了条件切换（CHO），支持主动式小区预留和用户驱动执行。然而，两种切换类型在信令、资源利用和可靠性方面存在复杂权衡。本文基于某顶级移动网络运营商（MNO）的全国性移动性管理数据集，揭示了该领域的新见解，并指出下一代网络需具备自适应且鲁棒的切换控制机制。基于此，我们首次提出在O-RAN架构内联合优化THO与CHO的CONTRA框架。我们研究了CONTRA的两种变体：一种是基于先验服务或用户需求分配切换类型的静态方案，另一种是控制器根据实时系统状态动态决策切换类型的弹性方案。该框架采用实用的元学习算法，能自适应运行时观测数据，其性能可逼近具备完美未来信息的理想模型（全局无遗憾）。CONTRA专为近实时部署设计，可作为O-RAN xApp应用，契合6G灵活智能控制目标。基于众包数据集的广泛评估表明，CONTRA在动态真实场景中能提升用户吞吐量，降低THO与CHO切换成本，其性能优于符合3GPP标准的方案及强化学习基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional and conditional handovers in dense mobile networks, this paper introduces CONTRA, a meta-learning-based framework for joint optimization of handover types within the O-RAN architecture. The method employs a practical meta-learning algorithm that dynamically adapts to runtime conditions, enabling either a priori assignment or on-the-fly selection of handover types to balance signaling, resource usage, and reliability. Experimental results using real-world datasets demonstrate that CONTRA enhances user throughput and reduces switching costs, outperforming both 3GPP-compliant and reinforcement learning baselines in dynamic scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对密集移动网络中传统切换和条件切换的局限性，提出了CONTRA框架，这是一种基于元学习的方法，用于在O-RAN架构内联合优化切换类型。该方法采用实用的元学习算法，动态适应运行时条件，支持预先分配或实时选择切换类型，以平衡信令、资源使用和可靠性。利用真实世界数据集的实验结果表明，CONTRA提高了用户吞吐量并降低了切换成本，在动态场景中优于3GPP兼容和强化学习基线。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem</div>
<div class="meta-line">Authors: Xuan Wu, Di Wang, Chunguo Wu, Kaifang Qi, Chunyan Miao, Yubin Xiao, Jian Zhang, You Zhou</div>
<div class="meta-line">First: 2025-07-28T23:38:33+00:00 · Latest: 2025-12-26T07:45:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.21386v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.21386v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed to address Vehicle Routing Problems (VRPs). However, most of these solvers focus exclusively on single-vehicle VRP variants, overlooking the more realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP), which involves multiple vehicles. Existing MMHCVRP solvers typically select a vehicle and its next node to visit at each decoding step, but often make myopic decoding decisions and overlook key properties of MMHCVRP, including local topological relationships, vehicle permutation invariance, and node symmetry, resulting in suboptimal performance. To better address these limitations, we propose ECHO, an efficient NCO solver. First, ECHO exploits the proposed dual-modality node encoder to capture local topological relationships among nodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed Parameter-Free Cross-Attention mechanism to prioritize the vehicle selected in the preceding decoding step. Finally, leveraging vehicle permutation invariance and node symmetry, we introduce a tailored data augment strategy for MMHCVRP to stabilize the Reinforcement Learning training process. To assess the performance of ECHO, we conduct extensive experiments. The experimental results demonstrate that ECHO outperforms state-of-the-art NCO solvers across varying numbers of vehicles and nodes, and exhibits well-performing generalization across both scales and distribution patterns. Finally, ablation studies validate the effectiveness of all proposed methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向最小-最大异构容量车辆路径问题的高效神经组合优化求解器</div>
<div class="mono" style="margin-top:8px">已有众多神经组合优化求解器被提出以应对车辆路径问题，但大多仅关注单车辆变体，忽略了更贴近实际的多车辆最小-最大异构容量车辆路径问题。现有求解器通常在解码步骤中依次选择车辆及访问节点，但往往做出短视决策，且忽视了该问题的关键特性——局部拓扑关系、车辆排列不变性与节点对称性，导致性能欠佳。为此，我们提出高效求解器ECHO：首先通过双模态节点编码器捕捉节点间局部拓扑关系；其次采用无参数交叉注意力机制优先处理前序解码步骤所选车辆，以缓解短视决策；最后利用车辆排列不变性与节点对称性，设计针对性数据增强策略以稳定强化学习训练。大量实验表明，ECHO在不同车辆与节点规模下均优于当前最优神经组合优化求解器，并在规模与分布模式上展现出良好泛化能力。消融实验验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the limitations of existing Neural Combinatorial Optimization (NCO) solvers, which typically focus on single-vehicle routing problems and perform poorly on the more realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP) due to myopic decoding decisions and overlooked problem properties. The proposed method, ECHO, introduces a dual-modality node encoder to capture local topological relationships, a Parameter-Free Cross-Attention mechanism to reduce myopic decisions by prioritizing the previously selected vehicle, and a tailored data augmentation strategy leveraging vehicle permutation invariance and node symmetry to stabilize Reinforcement Learning training. Experimental results show that ECHO outperforms state-of-the-art NCO solvers across different numbers of vehicles and nodes, with strong generalization in scale and distribution, and ablation studies confirm the effectiveness of each component.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有神经组合优化求解器的局限性，这些求解器通常专注于单车辆路径问题，而在更现实的极小-极大异构容量车辆路径问题上表现不佳，原因在于其短视的解码决策和忽略的问题特性。提出的方法ECHO引入了双模态节点编码器以捕捉局部拓扑关系，采用参数无关的交叉注意力机制通过优先选择先前车辆来减少短视决策，并利用车辆排列不变性和节点对称性设计了定制化数据增强策略以稳定强化学习训练。实验结果表明，ECHO在不同车辆和节点数量上均优于最先进的神经组合优化求解器，在规模和分布模式上展现出良好的泛化能力，消融研究验证了所有提出方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">A Comedy of Estimators: On KL Regularization in RL Training of LLMs</div>
<div class="meta-line">Authors: Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville</div>
<div class="meta-line">First: 2025-12-26T04:20:58+00:00 · Latest: 2025-12-26T04:20:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21852v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21852v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>估计量喜剧：论大语言模型强化学习训练中的KL正则化</div>
<div class="mono" style="margin-top:8px">通过强化学习训练可显著提升大语言模型的推理性能。其训练目标包含一个正则化项，即训练策略与参考策略之间的反向KL散度。由于精确计算KL散度不可行，实践中常采用多种估计量基于在线策略样本进行估计。尽管该方法已被广泛采用（包括多个开源库），但尚未有系统研究分析KL估计量在目标函数中的多种整合方式及其对RL训练模型下游性能的影响。近期研究表明，当前主流的KL正则化实践并未为既定目标提供正确的梯度，导致目标与实现之间存在偏差。本文进一步分析这些实践，研究多种估计量配置的梯度特性，揭示设计选择如何影响梯度偏差。我们通过RL微调\texttt{Qwen2.5-7B}、\texttt{Llama-3.1-8B-Instruct}和\texttt{Qwen3-4B-Instruct-2507}模型的不同配置，并在分布内与分布外任务上评估其性能，以实证结果验证上述发现。分析表明，在在线策略设置中：（1）具有偏差梯度的估计量配置可能导致训练不稳定；（2）采用无偏梯度估计量配置能提升分布内及分布外任务的性能。同时，我们探究了离线策略设置中不同KL配置的性能表现，发现KL正则化有助于稳定异步设置产生的离线策略RL训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the practical implementation of KL divergence regularization in reinforcement learning (RL) for fine-tuning large language models (LLMs), motivated by the widespread use of potentially biased estimators that create a discrepancy between the stated objective and its gradients. The method involves a systematic analysis of various KL estimator configurations, examining their gradient properties and empirically testing them by RL fine-tuning models like Qwen2.5-7B and Llama-3.1-8B on both in-distribution and out-of-distribution tasks. The main experimental results show that configurations with biased gradients lead to training instability, whereas those providing unbiased gradients yield better and more stable performance across tasks, with KL regularization also proving beneficial for stabilizing off-policy RL training in asynchronous setups.</div>
<div class="mono" style="margin-top:8px">本文研究了在大型语言模型（LLM）的强化学习（RL）微调中，KL散度正则化的实际实现问题，其动机在于广泛使用的估计器可能存在偏差，导致目标函数与梯度计算不一致。研究方法包括系统分析多种KL估计器配置的梯度特性，并通过RL微调Qwen2.5-7B和Llama-3.1-8B等模型，在分布内和分布外任务上进行实证检验。主要实验结果表明，具有偏差梯度的配置会导致训练不稳定，而提供无偏梯度的配置则在各项任务中表现更优、更稳定，同时KL正则化还有助于稳定异步设置下的离策略RL训练。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</div>
<div class="meta-line">Authors: Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille</div>
<div class="meta-line">First: 2025-12-18T18:59:54+00:00 · Latest: 2025-12-26T02:03:42+00:00</div>
<div class="meta-line">Comments: V2: Added links to the code-generation results and additional details in the appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16917v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16917v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice&#x27;s soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成对抗推理器：通过对抗性强化学习增强大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">具备显式推理能力的大语言模型在数学推理方面表现优异，但仍存在过程性错误，如计算错误、逻辑脆弱以及表面合理但实际无效的步骤。本文提出生成对抗推理器，这是一种基于策略的联合训练框架，旨在通过对抗性强化学习协同进化LLM推理器与基于LLM的判别器，从而增强推理能力。计算高效的审查机制将每条推理链划分为逻辑完整且长度相近的片段，判别器通过简洁、结构化的论证评估每个片段的合理性。学习过程结合互补信号：LLM推理器因产生逻辑一致且答案正确的步骤而获得奖励，判别器则因准确检测错误或区分推理过程中的痕迹而获得奖励。该方法生成密集、校准良好的策略级步骤奖励，补充了稀疏的精确匹配信号，改善了信用分配，提高了样本效率，并提升了LLM的整体推理质量。在多个数学基准测试中，该方法相较于采用标准强化学习后训练的强基线模型均取得稳定提升。具体而言，在AIME24上，我们将DeepSeek-R1-Distill-Qwen-7B从54.0提升至61.3（+7.3），将DeepSeek-R1-Distill-Llama-8B从43.7提升至53.7（+10.0）。模块化判别器还支持针对教师蒸馏、偏好对齐和基于数学证明的推理等目标进行灵活的奖励塑造。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the persistent issue of process errors in large language models (LLMs) during mathematical reasoning, such as incorrect calculations and flawed logic. The authors propose the Generative Adversarial Reasoner, a joint training framework where an LLM reasoner and an LLM-based discriminator are co-evolved through adversarial reinforcement learning. A key innovation is a compute-efficient review schedule that partitions reasoning chains into slices for the discriminator to evaluate, generating dense step-level rewards. Experimental results on mathematical benchmarks show significant improvements; for instance, on AIME24, the method boosts DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7, outperforming standard RL post-training baselines.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在数学推理中持续存在的计算错误、逻辑脆弱等过程性错误问题，提出了生成对抗推理器这一联合训练框架。该方法通过对抗性强化学习协同进化一个LLM推理器和一个基于LLM的判别器，其核心创新在于采用计算高效的审查机制，将推理链分割为逻辑完整的片段供判别器评估，从而生成密集的步骤级奖励信号。在多个数学基准测试上的实验结果表明该方法取得了显著提升，例如在AIME24上，它将DeepSeek-R1-Distill-Qwen-7B从54.0提升至61.3，将DeepSeek-R1-Distill-Llama-8B从43.7提升至53.7，性能优于标准的强化学习后训练基线。</div>
</details>
</div>
<div class="card">
<div class="title">ForestProtector: An IoT Architecture Integrating Machine Vision and Deep Reinforcement Learning for Efficient Wildfire Monitoring</div>
<div class="meta-line">Authors: Kenneth Bonilla-Ormachea, Horacio Cuizaga, Edwin Salcedo, Sebastian Castro, Sergio Fernandez-Testa, Misael Mamani</div>
<div class="meta-line">First: 2025-01-17T02:47:14+00:00 · Latest: 2025-12-25T23:16:17+00:00</div>
<div class="meta-line">Comments: Accepted for publication in the proceedings of the 11th International Conference on Automation, Robotics, and Applications (ICARA 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.09926v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.09926v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early detection of forest fires is crucial to minimizing the environmental and socioeconomic damage they cause. Indeed, a fire&#x27;s duration directly correlates with the difficulty and cost of extinguishing it. For instance, a fire burning for 1 minute might require 1 liter of water to extinguish, while a 2-minute fire could demand 100 liters, and a 10-minute fire might necessitate 1,000 liters. On the other hand, existing fire detection systems based on novel technologies (e.g., remote sensing, PTZ cameras, UAVs) are often expensive and require human intervention, making continuous monitoring of large areas impractical. To address this challenge, this work proposes a low-cost forest fire detection system that utilizes a central gateway device with computer vision capabilities to monitor a 360° field of view for smoke at long distances. A deep reinforcement learning agent enhances surveillance by dynamically controlling the camera&#x27;s orientation, leveraging real-time sensor data (smoke levels, ambient temperature, and humidity) from distributed IoT devices. This approach enables automated wildfire monitoring across expansive areas while reducing false positives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>森林守护者：集成机器视觉与深度强化学习的物联网架构，用于高效野火监测</div>
<div class="mono" style="margin-top:8px">早期发现森林火灾对于最小化其造成的环境和社会经济损失至关重要。实际上，火灾持续时间与扑救难度和成本直接相关。例如，燃烧1分钟的火灾可能仅需1升水即可扑灭，而2分钟的火灾可能需要100升水，10分钟的火灾则可能需要1000升水。另一方面，基于新兴技术（如遥感、PTZ摄像头、无人机）的现有火灾检测系统通常成本高昂且需人工干预，难以实现大面积区域的持续监测。为应对这一挑战，本研究提出了一种低成本的森林火灾检测系统，利用具备计算机视觉能力的中央网关设备，对远距离360°视野内的烟雾进行监控。通过深度强化学习智能体，结合分布式物联网设备提供的实时传感器数据（烟雾浓度、环境温度和湿度），动态控制摄像头方向，从而增强监测能力。该方法实现了广阔区域的自动化野火监测，同时降低了误报率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for cost-effective and automated early wildfire detection to mitigate escalating firefighting resource demands, this paper proposes ForestProtector, an IoT system integrating machine vision and deep reinforcement learning. The method employs a central gateway with computer vision for 360° smoke detection, enhanced by a reinforcement learning agent that dynamically adjusts camera orientation based on real-time sensor data from distributed IoT nodes. Experimental results demonstrate that the system achieves efficient, automated monitoring over large areas while significantly reducing false positive detections compared to existing human-dependent or expensive technologies.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过低成本、自动化的早期森林火灾监测来应对灭火资源需求随火灾时间急剧增长的挑战，提出了名为ForestProtector的物联网架构。该方法结合了机器视觉与深度强化学习，利用具备计算机视觉能力的中央网关进行360度烟雾探测，并通过强化学习智能体依据分布式物联网设备的实时传感器数据动态调整摄像头方向。实验结果表明，该系统能够在大范围区域内实现高效自动化监控，并相较于现有依赖人工或成本高昂的技术，显著降低了误报率。</div>
</details>
</div>
<div class="card">
<div class="title">Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V</div>
<div class="meta-line">Authors: John Chen, Sihan Cheng, Can Gurkan, Ryan Lay, Moez Salahuddin</div>
<div class="meta-line">First: 2025-12-21T02:15:09+00:00 · Latest: 2025-12-25T18:41:26+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18564v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.18564v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models&#x27; capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs&#x27; real-world deployment. Working on a classic 4X strategy game, Sid Meier&#x27;s Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi&#x27;s enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vox Deorum：面向4X/大战略游戏AI的混合LLM架构——基于《文明V》的经验总结</div>
<div class="mono" style="margin-top:8px">大语言模型凭借其自然语言推理能力，在4X与大战略游戏中展现出独特潜力，能实现更自然的人机协作与谈判等交互。然而这类游戏因复杂度高、决策周期长而带来特殊挑战，延迟与成本因素也制约着LLM的实际部署。本研究以经典4X游戏《席德·梅尔的文明V》（搭载Vox Populi模组）为实验平台，提出Vox Deorum混合架构。该分层技术设计使LLM专注于宏观战略推理，将战术执行委派给子系统（如算法AI或未来的强化学习AI）。通过对2,327场完整游戏的测试，我们对比了两个开源LLM（使用简易提示词）与Vox Populi增强AI的表现。结果表明，LLM在实现端到端竞技性游戏的同时，展现出与算法AI及彼此间显著差异的游戏风格。本研究为商业4X游戏整合LLM提供了可行架构，为游戏设计与智能体研究开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to leverage the natural language reasoning capabilities of Large Language Models (LLMs) to enable more intuitive human-AI interactions, such as collaboration and negotiation, within the complex, long-horizon environments of 4X and grand strategy games, while addressing practical deployment concerns like latency and cost. The method introduces Vox Deorum, a hybrid LLM+X architecture for Civilization V with the Vox Populi mod, which uses a layered design where the LLM handles high-level strategic planning and delegates tactical execution to specialized algorithmic subsystems. The main experimental results, based on 2,327 complete game simulations, demonstrate that LLMs achieve gameplay performance competitive with the game&#x27;s enhanced AI while exhibiting distinct and varied play styles, validating the architecture&#x27;s viability for commercial game integration and agentic AI research.</div>
<div class="mono" style="margin-top:8px">本研究的动机是利用大语言模型（LLM）的自然语言推理能力，在复杂、长周期的4X和大战略游戏中实现更直观的人机交互（如协作与谈判），同时解决延迟和成本等实际部署问题。方法上，针对《文明V》及其Vox Populi模组，提出了名为Vox Deorum的混合LLM+X架构，采用分层设计，由LLM负责宏观战略规划，并将战术执行委托给专门的算法子系统。主要实验结果基于2,327场完整游戏模拟，表明LLM在游戏性能上与增强型AI相当，同时展现出截然不同且多样化的游戏风格，验证了该架构在商业游戏集成和智能体AI研究中的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning</div>
<div class="meta-line">Authors: Pei Yang, Ke Zhang, Ji Wang, Xiao Chen, Yuxin Tang, Eric Yang, Lynn Ai, Bill Shi</div>
<div class="meta-line">First: 2025-11-20T10:12:34+00:00 · Latest: 2025-12-25T16:24:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16202v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.16202v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体协同奖励设计用于增强强化学习中的推理能力</div>
<div class="mono" style="margin-top:8px">我们提出了CRM（多智能体协同奖励模型），该框架通过一个协调的专家评估器团队替代单一的黑盒奖励模型，以提升RLHF的鲁棒性和可解释性。传统奖励模型难以同时优化多个可能相互冲突的偏好维度（如事实性、有用性、安全性），且评分依据缺乏透明度。CRM通过将偏好评估分解为多个领域特定的智能体来解决这些问题，每个智能体生成部分信号，并结合基于排序器和嵌入相似度的全局评估器。中央聚合器在每个时间步融合这些信号，平衡逐步正确性、多智能体一致性和重复惩罚等因素，最终生成与标准RL流程兼容的单一训练奖励。策略通过基于优势的更新（如GAE）进行优化，而价值模型回归至聚合奖励，实现在无需额外人工标注的情况下进行多视角奖励塑造。为支持训练与评估，我们引入了rewardBench——一个与CRM协同结构对齐的基准测试与训练套件。CRM与rewardBench共同为更透明的奖励建模和更稳定的优化提供了实用化、模块化的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces the Multi-Agent Collaborative Reward Model (CRM) to address limitations in conventional single reward models used in Reinforcement Learning from Human Feedback (RLHF), which often lack robustness and interpretability when handling multiple, potentially conflicting preference dimensions like factuality and safety. The method decomposes evaluation into specialized agents that generate partial signals, combined by a centralized aggregator balancing factors such as step-wise correctness and multi-agent agreement to produce a unified training reward compatible with standard RL pipelines. Experimental results, supported by the newly introduced rewardBench benchmark, demonstrate that CRM enhances transparency and optimization stability without requiring additional human annotations beyond those used to train the evaluators.</div>
<div class="mono" style="margin-top:8px">本文提出了多智能体协作奖励模型（CRM），以解决强化学习人类反馈中传统单一奖励模型在同时优化多个可能冲突的偏好维度（如事实性、安全性）时缺乏鲁棒性和可解释性的问题。该方法将评估分解为多个专业智能体生成部分信号，并通过中央聚合器平衡逐步正确性、多智能体一致性等因素，生成与标准强化学习流程兼容的统一训练奖励。实验结果表明，结合新引入的rewardBench基准，CRM在无需额外人工标注的情况下，提高了奖励建模的透明度和优化稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">ZIA: A Theoretical Framework for Zero-Input AI</div>
<div class="meta-line">Authors: Aditi De, NeuroBits Labs</div>
<div class="meta-line">First: 2025-02-22T07:42:05+00:00 · Latest: 2025-12-25T16:05:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.16124v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.16124v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Input AI (ZIA) introduces a novel framework for human-computer interaction by enabling proactive intent prediction without explicit user commands. It integrates gaze tracking, bio-signals (EEG, heart rate), and contextual data (time, location, usage history) into a multi-modal model for real-time inference, targeting &lt;100 ms latency. The proposed architecture employs a transformer-based model with cross-modal attention, variational Bayesian inference for uncertainty estimation, and reinforcement learning for adaptive optimization. To support deployment on edge devices (CPUs, TPUs, NPUs), ZIA utilizes quantization, weight pruning, and linear attention to reduce complexity from quadratic to linear with sequence length. Theoretical analysis establishes an information-theoretic bound on prediction error and demonstrates how multi-modal fusion improves accuracy over single-modal approaches. Expected performance suggests 85-90% accuracy with EEG integration and 60-100 ms inference latency. ZIA provides a scalable, privacy-preserving framework for accessibility, healthcare, and consumer applications, advancing AI toward anticipatory intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ZIA：零输入人工智能的理论框架</div>
<div class="mono" style="margin-top:8px">零输入人工智能（ZIA）提出了一种新颖的人机交互框架，通过无需显式用户指令即可主动预测意图。该框架整合了眼动追踪、生物信号（脑电图、心率）和上下文数据（时间、位置、使用历史）到一个多模态模型中，实现实时推断，目标延迟低于100毫秒。所提出的架构采用基于Transformer的模型，具备跨模态注意力机制、用于不确定性估计的变分贝叶斯推断，以及用于自适应优化的强化学习。为支持在边缘设备（CPU、TPU、NPU）上部署，ZIA利用量化、权重剪枝和线性注意力机制，将复杂度从序列长度的二次方降低至线性。理论分析建立了预测误差的信息论界限，并展示了多模态融合如何比单模态方法提高准确性。预期性能显示，结合脑电图时准确率可达85-90%，推断延迟为60-100毫秒。ZIA为无障碍服务、医疗保健和消费应用提供了一个可扩展且保护隐私的框架，推动人工智能向预见性智能迈进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more natural and anticipatory human-computer interaction, this paper introduces the Zero-Input AI (ZIA) framework, which aims to predict user intent proactively without requiring explicit commands. The method integrates multi-modal data—including gaze, bio-signals like EEG and heart rate, and contextual information—using a transformer-based model with cross-modal attention, variational Bayesian inference for uncertainty, and reinforcement learning for optimization; it also employs techniques like quantization and linear attention to enable efficient deployment on edge devices. The main experimental results, supported by theoretical analysis, show that ZIA achieves 85-90% prediction accuracy with EEG integration and maintains a low inference latency of 60-100 ms, outperforming single-modal approaches and providing a scalable, privacy-preserving solution for applications in accessibility and healthcare.</div>
<div class="mono" style="margin-top:8px">本文的动机是追求更自然、更具预见性的人机交互，因此提出了零输入人工智能（ZIA）框架，旨在无需用户明确指令即可主动预测其意图。该方法通过整合凝视、脑电图和心率等生物信号以及上下文信息等多模态数据，采用基于Transformer的模型，结合跨模态注意力、变分贝叶斯推理进行不确定性估计，并使用强化学习进行自适应优化；同时利用量化和线性注意力等技术降低计算复杂度，以实现在边缘设备上的高效部署。主要实验结果及理论分析表明，ZIA在集成脑电图数据时预测准确率达到85-90%，推理延迟保持在60-100毫秒，优于单模态方法，为无障碍辅助和医疗健康等领域提供了一个可扩展且保护隐私的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities</div>
<div class="meta-line">Authors: Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin</div>
<div class="meta-line">First: 2025-12-25T15:40:52+00:00 · Latest: 2025-12-25T15:40:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21717v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21717v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Space-air-ground-integrated network (SAGIN)-enabled multiconnectivity (MC) is emerging as a key enabler for next-generation networks, enabling users to simultaneously utilize multiple links across multi-layer non-terrestrial networks (NTN) and multi-radio access technology (multi-RAT) terrestrial networks (TN). However, the heterogeneity of TN and NTN introduces complex architectural challenges that complicate MC implementation. Specifically, the diversity of link types, spanning air-to-air, air-to-space, space-to-space, space-to-ground, and ground-to-ground communications, renders optimal resource allocation highly complex. Recent advancements in reinforcement learning (RL) and agentic artificial intelligence (AI) have shown remarkable effectiveness in optimal decision-making in complex and dynamic environments. In this paper, we review the current developments in SAGIN-enabled MC and outline the key challenges associated with its implementation. We further highlight the transformative potential of AI-driven approaches for resource optimization in a heterogeneous SAGIN environment. To this end, we present a case study on resource allocation optimization enabled by agentic RL for SAGIN-enabled MC involving diverse radio access technologies (RATs). Results show that learning-based methods can effectively handle complex scenarios and substantially enhance network performance in terms of latency and capacity while incurring a moderate increase in power consumption as an acceptable tradeoff. Finally, open research problems and future directions are presented to realize efficient SAGIN-enabled MC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空天地一体化网络多连接技术：现状、挑战、AI驱动方案与机遇</div>
<div class="mono" style="margin-top:8px">基于空天地一体化网络（SAGIN）的多连接（MC）技术正成为下一代网络的关键赋能手段，使用户能够同时利用多层非地面网络（NTN）与多无线接入技术（多-RAT）地面网络（TN）的多条链路。然而，TN与NTN的异构性带来了复杂的架构挑战，增加了MC实施的难度。具体而言，链路类型的多样性（涵盖空对空、空对天、天对天、天对地及地对地通信）使得最优资源分配高度复杂。强化学习（RL）与智能体人工智能（AI）的最新进展在复杂动态环境的最优决策中展现出显著成效。本文综述了SAGIN赋能的MC技术当前发展，阐述了其实施过程中的关键挑战，并重点探讨了AI驱动方法在异构SAGIN环境中资源优化的变革潜力。为此，我们通过一个案例研究，展示了基于智能体RL的资源分配优化在涉及多种无线接入技术（RAT）的SAGIN赋能MC中的应用。结果表明，基于学习的方法能有效处理复杂场景，在延迟与容量方面显著提升网络性能，同时以适度的功耗增加作为可接受的权衡。最后，本文提出了实现高效SAGIN赋能MC的开放研究问题与未来方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to address the complex architectural challenges of implementing multiconnectivity in heterogeneous space-air-ground-integrated networks (SAGIN), where diverse link types complicate optimal resource allocation. The method involves a comprehensive review of current SAGIN developments and the proposal of AI-driven solutions, specifically highlighting a case study using agentic reinforcement learning for resource optimization across multiple radio access technologies. The main experimental results from the case study demonstrate that such learning-based methods can effectively manage complex scenarios, significantly improving network latency and capacity, albeit with a moderate and acceptable increase in power consumption.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决在异构的空天地一体化网络（SAGIN）中实现多连接所面临的复杂架构挑战，其中多样化的链路类型使得最优资源分配高度复杂。研究方法包括对当前SAGIN发展的全面综述，并提出了人工智能驱动的解决方案，特别通过一个案例研究，展示了利用智能体强化学习在多种无线接入技术中进行资源优化。主要实验结果表明，这类基于学习的方法能有效处理复杂场景，显著提升网络时延和容量性能，尽管会带来适度的、可接受的功耗增加作为权衡。</div>
</details>
</div>
<div class="card">
<div class="title">TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning</div>
<div class="meta-line">Authors: Saisai Yang, Qingyi Huang, Jing Yuan, Liangyu Zha, Kai Tang, Yuhang Yang, Ning Wang, Yucheng Wei, Liyao Li, Wentao Ye, Hao Chen, Tao Zhang, Junlin Zhou, Haobo Wang, Gang Chen, Junbo Zhao</div>
<div class="meta-line">First: 2025-12-23T12:30:37+00:00 · Latest: 2025-12-25T12:35:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20312v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20312v2">PDF</a> · <a href="https://huggingface.co/tablegpt/TableGPT-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TableGPT-R1：通过强化学习推进表格推理能力</div>
<div class="mono" style="margin-top:8px">表格数据是现代数据分析和科学研究的核心。虽然通过监督微调的大型语言模型显著提升了与这类结构化数据的自然语言交互能力，但在处理现实表格任务所需的复杂多步推理和鲁棒代码执行方面仍存在不足。强化学习为增强这些能力提供了可行路径，但其在表格领域的应用面临三大挑战：缺乏包含闭环代码执行和多样化表格结构环境反馈的高质量智能体轨迹、反馈信号极度异质（从严格的SQL执行到开放式数据解读），以及在垂直专业化过程中可能出现的通用知识灾难性遗忘。为克服这些挑战并实现对复杂表格的高级推理，我们提出了基于系统化强化学习框架的专用表格模型——TableGPT-R1。该方法整合了三个核心组件：用于生成难度分层的智能体轨迹以支持监督对齐和强化学习推演的综合数据工程流水线、融合基于规则的验证与标准注入奖励模型的任务自适应奖励系统（包含过程级步骤奖励塑形和行为正则化），以及通过多阶段训练框架在专业化处理表格任务前逐步稳定推理能力。大量实验表明，TableGPT-R1在权威基准测试中达到最先进性能，显著超越基线模型的同时保持了稳健的通用能力。模型已在https://huggingface.co/tablegpt/TableGPT-R1开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for TableGPT-R1 stems from the limitations of current LLMs fine-tuned with SFT, which struggle with the multi-step reasoning and robust code execution needed for complex real-world tabular data tasks. The method introduces a systematic RL framework featuring a data pipeline for generating difficulty-stratified agentic trajectories, a task-adaptive reward system combining rule-based verification with a reward model and step reward shaping, and a multi-stage training process to stabilize reasoning before specialization. The main experimental results show that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while maintaining robust general capabilities.</div>
<div class="mono" style="margin-top:8px">TableGPT-R1的研究动机源于当前基于监督微调的大语言模型在处理复杂表格数据时，难以进行多步推理和鲁棒的代码执行。该方法提出了一个系统的强化学习框架，包括一个用于生成难度分层的智能体轨迹的数据工程管道、一个结合了基于规则的验证与奖励模型及步骤奖励塑造的任务自适应奖励系统，以及一个先稳定推理再进行表格任务专门化的多阶段训练框架。主要实验结果表明，TableGPT-R1在权威基准测试中取得了最先进的性能，显著优于基线模型，同时保持了强大的通用能力。</div>
</details>
</div>
<div class="card">
<div class="title">Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search</div>
<div class="meta-line">Authors: Maximilian Weichart</div>
<div class="meta-line">First: 2025-12-25T12:25:26+00:00 · Latest: 2025-12-25T12:25:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21648v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21648v1">PDF</a> · <a href="http://github.com/Max-We/inverse-rpo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monte Carlo Tree Search (MCTS) has profoundly influenced reinforcement learning (RL) by integrating planning and learning in tasks requiring long-horizon reasoning, exemplified by the AlphaZero family of algorithms. Central to MCTS is the search strategy, governed by a tree policy based on an upper confidence bound (UCB) applied to trees (UCT). A key factor in the success of AlphaZero is the introduction of a prior term in the UCB1-based tree policy PUCT, which improves exploration efficiency and thus accelerates training. While many alternative UCBs with stronger theoretical guarantees than UCB1 exist, extending them to prior-based UCTs has been challenging, since PUCT was derived empirically rather than from first principles. Recent work retrospectively justified PUCT by framing MCTS as a regularized policy optimization (RPO) problem. Building on this perspective, we introduce Inverse-RPO, a general methodology that systematically derives prior-based UCTs from any prior-free UCB. Applying this method to the variance-aware UCB-V, we obtain two new prior-based tree policies that incorporate variance estimates into the search. Experiments indicate that these variance-aware prior-based UCTs outperform PUCT across multiple benchmarks without incurring additional computational cost. We also provide an extension of the mctx library supporting variance-aware UCTs, showing that the required code changes are minimal and intended to facilitate further research on principled prior-based UCTs. Code: github.com/Max-We/inverse-rpo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于方差感知先验的蒙特卡洛树搜索树策略</div>
<div class="mono" style="margin-top:8px">蒙特卡洛树搜索（MCTS）通过将规划与学习相结合，在需要长程推理的任务中深刻影响了强化学习（RL），以AlphaZero系列算法为典型代表。MCTS的核心是搜索策略，由应用于树的置信上界（UCT）树策略控制。AlphaZero成功的关键在于在基于UCB1的树策略PUCT中引入了先验项，从而提高了探索效率并加速了训练。尽管存在许多比UCB1具有更强理论保证的替代UCB，但由于PUCT是经验性推导而非基于第一性原理，将其扩展为基于先验的UCT一直具有挑战性。近期研究通过将MCTS构建为正则化策略优化（RPO）问题，为PUCT提供了回溯性理论依据。基于这一视角，我们提出了Inverse-RPO——一种通用方法，能够从任何无先验UCB中系统推导出基于先验的UCT。将此方法应用于方差感知的UCB-V，我们获得了两种新的基于先验的树策略，将方差估计纳入搜索过程。实验表明，这些方差感知的基于先验的UCT在多个基准测试中均优于PUCT，且未增加额外计算成本。我们还提供了支持方差感知UCT的mctx库扩展，表明所需代码改动极小，旨在促进基于原则的优先UCT的进一步研究。代码：github.com/Max-We/inverse-rpo。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the empirical success of prior-based tree policies like PUCT in Monte Carlo Tree Search (MCTS) and the lack of a principled method to extend theoretically stronger UCB variants to incorporate priors, this paper introduces Inverse-RPO, a general methodology that systematically derives prior-based UCTs from any prior-free upper confidence bound. The method is applied to the variance-aware UCB-V, yielding two new tree policies that integrate variance estimates into the search process. Experimental results across multiple benchmarks demonstrate that these variance-aware prior-based UCTs outperform the standard PUCT without adding computational overhead, and the authors provide an open-source library extension to support further research.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，尽管像PUCT这样的基于先验的树策略在蒙特卡洛树搜索中取得了经验性成功，但缺乏一种系统性的方法将理论更强的UCB变体扩展为包含先验的形式。为此，论文提出了Inverse-RPO这一通用方法，能够从任何无先验的UCB中系统推导出基于先验的UCT。该方法应用于方差感知的UCB-V，产生了两种将方差估计纳入搜索过程的新树策略。在多个基准测试中的实验结果表明，这些方差感知的基于先验的UCT在未增加计算成本的情况下超越了标准PUCT，作者还提供了开源库扩展以支持进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations</div>
<div class="meta-line">Authors: Xin Liu, Haoran Li, Dongbin Zhao</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-25T09:11:14+00:00 · Latest: 2025-12-25T09:11:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21586v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans can efficiently extract knowledge and learn skills from the videos within only a few trials and errors. However, it poses a big challenge to replicate this learning process for autonomous agents, due to the complexity of visual input, the absence of action or reward signals, and the limitations of interaction steps. In this paper, we propose a novel, unsupervised, and sample-efficient framework to achieve imitation learning from videos (ILV), named Behavior Cloning from Videos via Latent Representations (BCV-LR). BCV-LR extracts action-related latent features from high-dimensional video inputs through self-supervised tasks, and then leverages a dynamics-based unsupervised objective to predict latent actions between consecutive frames. The pre-trained latent actions are fine-tuned and efficiently aligned to the real action space online (with collected interactions) for policy behavior cloning. The cloned policy in turn enriches the agent experience for further latent action finetuning, resulting in an iterative policy improvement that is highly sample-efficient.
  We conduct extensive experiments on a set of challenging visual tasks, including both discrete control and continuous control. BCV-LR enables effective (even expert-level on some tasks) policy performance with only a few interactions, surpassing state-of-the-art ILV baselines and reinforcement learning methods (provided with environmental rewards) in terms of sample efficiency across 24/28 tasks. To the best of our knowledge, this work for the first time demonstrates that videos can support extremely sample-efficient visual policy learning, without the need to access any other expert supervision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频作为样本高效的监督：通过潜在表示从视频进行行为克隆</div>
<div class="mono" style="margin-top:8px">人类能够仅通过少量试错就从视频中高效提取知识并学习技能。然而，由于视觉输入的复杂性、动作或奖励信号的缺失以及交互步骤的限制，为自主智能体复现这一学习过程面临巨大挑战。本文提出了一种新颖、无监督且样本高效的框架，用于实现从视频中进行模仿学习（ILV），称为“通过潜在表示从视频进行行为克隆”（BCV-LR）。BCV-LR通过自监督任务从高维视频输入中提取与动作相关的潜在特征，并利用基于动力学的无监督目标来预测连续帧之间的潜在动作。预训练的潜在动作经过在线微调（通过收集的交互）高效对齐到真实动作空间，以进行策略行为克隆。克隆的策略反过来又丰富了智能体经验，用于进一步微调潜在动作，从而形成高度样本高效的迭代策略改进。
我们在包括离散控制和连续控制在内的一系列具有挑战性的视觉任务上进行了广泛实验。BCV-LR仅需少量交互即可实现有效（在某些任务上甚至达到专家级）的策略性能，在24/28个任务的样本效率方面超越了最先进的ILV基线和强化学习方法（提供环境奖励）。据我们所知，这项工作首次证明视频能够支持极度样本高效的视觉策略学习，而无需访问任何其他专家监督。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the human ability to learn skills efficiently from videos without direct interaction, this paper addresses the challenge of sample-efficient imitation learning from visual inputs lacking action or reward signals. The proposed method, Behavior Cloning from Videos via Latent Representations (BCV-LR), employs a self-supervised approach to extract action-related latent features from videos and uses a dynamics-based objective to predict latent actions between frames; these latent actions are then fine-tuned online with limited interactions to align with real actions for policy cloning, enabling iterative improvement. Experimental results on discrete and continuous control tasks show that BCV-LR achieves expert-level performance on some tasks with very few interactions, outperforming state-of-the-art imitation learning and reinforcement learning baselines in sample efficiency across most tested tasks, demonstrating that videos alone can enable highly sample-efficient visual policy learning.</div>
<div class="mono" style="margin-top:8px">受人类能够从视频中高效学习技能的启发，本文针对从缺乏动作或奖励信号的视觉输入中进行样本高效模仿学习的挑战，提出了一种新方法。该方法名为“通过潜在表征从视频进行行为克隆（BCV-LR）”，它通过自监督任务从高维视频中提取与动作相关的潜在特征，并利用基于动力学的无监督目标预测连续帧之间的潜在动作；这些潜在动作随后通过有限的在线交互进行微调，以对齐真实动作空间，从而进行策略克隆，实现迭代改进。在离散和连续控制任务上的实验结果表明，BCV-LR在少量交互下即可在某些任务上达到专家级性能，在大多数测试任务中，其样本效率超越了当前最先进的模仿学习和强化学习方法，证明了仅凭视频就能实现高度样本高效的视觉策略学习。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning</div>
<div class="meta-line">Authors: Wenda Wei, Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Lixin Su, Shuaiqiang Wang, Dawei Yin, Maarten de Rijke, Xueqi Cheng</div>
<div class="meta-line">First: 2025-11-12T08:29:39+00:00 · Latest: 2025-12-25T08:50:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09109v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.09109v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios. Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>前向与后向思考：面向检索增强推理的多目标强化学习</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）已被证明能有效缓解大语言模型的幻觉问题，但在复杂的多步推理场景中其效果仍有限。近期研究将基于搜索的交互机制融入RAG，实现了实时检索的迭代推理。现有方法多依赖结果监督，缺乏对中间步骤的显式指导，易导致奖励破解和响应质量下降。本文提出Bi-RAR——一种新颖的检索增强推理框架，通过前向与后向联合评估每个中间推理步骤。为量化各步骤的信息完备性，我们基于柯尔莫哥洛夫复杂度提出双向信息距离度量，并借助语言模型生成概率进行近似计算。该量化方法同时衡量当前推理与答案的距离及其对问题的回应程度。为在此双向信号下优化推理，我们采用具有级联奖励结构的多目标强化学习框架，强调早期轨迹对齐。在七个问答基准上的实验表明，Bi-RAR优于现有方法，并在训练与推理过程中实现了与搜索引擎的高效交互推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of retrieval-augmented generation in multi-step reasoning, where outcome-based supervision often leads to reward hacking and poor intermediate steps. The authors propose Bi-RAR, a framework that evaluates each reasoning step bidirectionally using a distance metric based on Kolmogorov complexity, approximated via language model probabilities, to measure completeness relative to both the question and answer. This is optimized with multi-objective reinforcement learning featuring a cascading reward structure to prioritize early alignment. Experiments on seven QA benchmarks show Bi-RAR outperforms prior methods and enables efficient search engine interaction during training and inference.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成在多步推理中的局限性，即基于结果的监督常导致奖励破解和中间步骤质量下降。作者提出Bi-RAR框架，通过基于柯尔莫哥洛夫复杂性的距离度量（利用语言模型生成概率近似）双向评估每个推理步骤，以衡量其相对于问题和答案的信息完整性。该方法采用具有级联奖励结构的多目标强化学习进行优化，强调早期轨迹对齐。在七个问答基准上的实验表明，Bi-RAR优于先前方法，并在训练和推理中实现了高效的搜索引擎交互。</div>
</details>
</div>
<div class="card">
<div class="title">Object-Centric World Models for Causality-Aware Reinforcement Learning</div>
<div class="meta-line">Authors: Yosuke Nishimoto, Takashi Matsubara</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-18T08:53:09+00:00 · Latest: 2025-12-25T07:22:11+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14262v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.14262v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向因果感知强化学习的以对象为中心的世界模型</div>
<div class="mono" style="margin-top:8px">世界模型已被开发用于支持样本高效的深度强化学习智能体。然而，由于大多数世界模型学习所有环境组件的整体表示，要准确复现高维、非平稳且由多个具有丰富交互的对象构成的环境仍具挑战。相比之下，人类通过将环境分解为离散对象来感知环境，从而促进高效决策。受此启发，我们提出《基于因果感知强化学习的槽位变换器想象框架》（STICA），这是一个统一框架，其中以对象为中心的变换器作为世界模型及因果感知的策略与价值网络。STICA将每个观测表示为一组以对象为中心的标记，连同表示智能体动作及所得奖励的标记，使世界模型能够预测标记级动态与交互。策略与价值网络随后估计标记级因果关系，并将其用于注意力层，实现因果引导的决策。在对象丰富的基准测试中，STICA在样本效率和最终性能上均持续优于最先进的智能体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the human ability to decompose environments into discrete objects for efficient decision-making, this paper introduces STICA, a framework that integrates object-centric Transformers as a world model with causality-aware policy and value networks. The method represents observations as object-centric tokens alongside action and reward tokens, enabling the world model to predict token-level dynamics and interactions, while the policy networks estimate token-level cause-effect relations for causality-guided attention. Experimental results on object-rich benchmarks show that STICA consistently surpasses state-of-the-art agents in both sample efficiency and final performance.</div>
<div class="mono" style="margin-top:8px">受人类将环境分解为离散对象以进行高效决策的启发，本文提出了STICA框架，该框架将对象为中心的Transformer作为世界模型与因果感知的策略和价值网络相结合。该方法将观察表示为对象为中心的标记，以及动作和奖励标记，使世界模型能够预测标记级动态和交互，同时策略网络估计标记级因果关系以实现因果引导的注意力。在对象丰富的基准测试中，实验结果表明STICA在样本效率和最终性能上均持续优于最先进的智能体。</div>
</details>
</div>
<div class="card">
<div class="title">Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model</div>
<div class="meta-line">Authors: Yanhao Li, Lu Ma, Jiaran Zhang, Lexiang Tang, Wentao Zhang, Guibo Luo</div>
<div class="meta-line">First: 2025-12-25T07:16:26+00:00 · Latest: 2025-12-25T07:16:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21540v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21540v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Leash：面向高效大型推理模型的自适应长度惩罚与奖励塑形</div>
<div class="mono" style="margin-top:8px">现有方法通常依赖固定长度惩罚，但此类惩罚难以调优且无法适应大语言模型推理能力的动态演进，导致准确性与简洁性之间的权衡欠佳。为解决这一问题，我们提出Leash（自适应长度惩罚与奖励塑形），这是一个面向大语言模型高效推理的强化学习框架。我们将长度控制建模为约束优化问题，并采用拉格朗日对偶方法动态调整惩罚系数：当生成内容超过目标长度时增强惩罚，不足时则放宽惩罚。这种自适应机制引导模型在不牺牲任务性能的前提下生成简洁推理。在Deepseek-R1-Distill-Qwen-1.5B和Qwen3-4B-Thinking-2507上的实验表明，Leash在多样化任务（包括分布内数学推理及分布外编码、指令遵循等领域）中将平均推理长度降低60%，同时保持竞争力性能。本研究由此提出了一种实用有效的范式，用于开发平衡推理能力与计算预算的可控高效大语言模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of fixed length penalties in large language models (LLMs) for reasoning tasks, which struggle to adapt to evolving model capabilities and lead to suboptimal trade-offs between accuracy and conciseness. To solve this, the authors propose Leash, a reinforcement learning framework that formulates length control as a constrained optimization problem and uses a Lagrangian primal-dual method to dynamically adjust penalty coefficients—intensifying penalties when generations exceed target lengths and relaxing them when shorter. Experimental results on models like Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 demonstrate that Leash reduces average reasoning length by 60% across diverse tasks, including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following, while maintaining competitive performance.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在推理任务中固定长度惩罚的不足，指出其难以适应模型能力演变，导致准确性与简洁性之间的权衡不佳。为解决这一问题，作者提出了Leash框架，采用强化学习方法将长度控制建模为约束优化问题，并利用拉格朗日对偶方法动态调整惩罚系数——当生成内容超过目标长度时加强惩罚，反之则放松惩罚。在Deepseek-R1-Distill-Qwen-1.5B和Qwen3-4B-Thinking-2507等模型上的实验表明，Leash在包括分布内数学推理和分布外编码、指令遵循等多样任务中，平均推理长度减少了60%，同时保持了竞争力性能。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Actor Critic</div>
<div class="meta-line">Authors: Aoyang Qin, Deqian Kong, Wei Wang, Ying Nian Wu, Song-Chun Zhu, Sirui Xie</div>
<div class="meta-line">First: 2025-12-25T06:31:11+00:00 · Latest: 2025-12-25T06:31:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21527v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21527v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing \textit{policy evaluation} as learning a generative model of the joint distribution over trajectories and returns, $p(τ, y)$, and \textit{policy improvement} as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both \textit{exploitation}, by optimizing latent plans to maximize expected returns, and \textit{exploration}, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC&#x27;s strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式行动者-评论家</div>
<div class="mono" style="margin-top:8px">传统强化学习算法通常聚焦于估计或最大化期望回报，但在利用在线经验优化离线预训练模型时面临挑战。本文提出生成式行动者-评论家这一新颖框架，通过将策略评估重构为学习轨迹与回报联合分布$p(τ, y)$的生成模型，并将策略改进转化为对该学习模型进行灵活推理，从而解耦序列决策过程。为实现GAC，我们引入基于连续潜在规划向量隐变量模型的具体实例化方案，并开发了新型推理策略：在利用阶段通过优化潜在规划以最大化期望回报，在探索阶段通过采样符合动态调整目标回报的潜在规划。在Gym-MuJoCo和Maze2D基准测试中的实验表明，即使缺乏逐步奖励，GAC仍展现出强大的离线性能，且相比前沿方法实现了显著提升的离线至在线改进能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Generative Actor Critic (GAC), motivated by the challenge of refining offline pretrained models with online experiences in reinforcement learning, where conventional methods focused on expected returns struggle. The method decouples sequential decision-making by reframing policy evaluation as learning a generative model of the joint distribution over trajectories and returns, and policy improvement as performing versatile inference on this model, using a latent variable model with continuous latent plan vectors. Experimental results on Gym-MuJoCo and Maze2D benchmarks show that GAC achieves strong offline performance and significantly better offline-to-online improvement compared to state-of-the-art methods, even without step-wise rewards.</div>
<div class="mono" style="margin-top:8px">本文提出了生成式行动者-评论家（GAC）框架，其动机在于解决强化学习中利用在线经验微调离线预训练模型的挑战，传统方法专注于期望回报而面临困难。该方法通过将策略评估重构为学习轨迹与回报的联合分布的生成模型，并将策略改进视为对该模型进行灵活推理，从而解耦序列决策，具体实现基于具有连续潜在计划向量的隐变量模型。在Gym-MuJoCo和Maze2D基准测试上的实验结果表明，GAC在离线性能上表现强劲，且离线到在线的改进显著优于现有先进方法，即使在缺乏逐步奖励的情况下也是如此。</div>
</details>
</div>
<div class="card">
<div class="title">DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO</div>
<div class="meta-line">Authors: Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji</div>
<div class="meta-line">First: 2025-12-25T05:37:37+00:00 · Latest: 2025-12-25T05:37:37+00:00</div>
<div class="meta-line">Comments: Project Page: https://henglin-liu.github.io/DiverseGRPO/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21514v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://henglin-liu.github.io/DiverseGRPO/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL), particularly GRPO, improves image generation quality significantly by comparing the relative performance of images generated within the same group. However, in the later stages of training, the model tends to produce homogenized outputs, lacking creativity and visual diversity, which restricts its application scenarios. This issue can be analyzed from both reward modeling and generation dynamics perspectives. First, traditional GRPO relies on single-sample quality as the reward signal, driving the model to converge toward a few high-reward generation modes while neglecting distribution-level diversity. Second, conventional GRPO regularization neglects the dominant role of early-stage denoising in preserving diversity, causing a misaligned regularization budget that limits the achievable quality--diversity trade-off. Motivated by these insights, we revisit the diversity degradation problem from both reward modeling and generation dynamics. At the reward level, we propose a distributional creativity bonus based on semantic grouping. Specifically, we construct a distribution-level representation via spectral clustering over samples generated from the same caption, and adaptively allocate exploratory rewards according to group sizes to encourage the discovery of novel visual modes. At the generation level, we introduce a structure-aware regularization, which enforces stronger early-stage constraints to preserve diversity without compromising reward optimization efficiency. Experiments demonstrate that our method achieves a 13\%--18\% improvement in semantic diversity under matched quality scores, establishing a new Pareto frontier between image quality and diversity for GRPO-based image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiverseGRPO：通过多样性感知GRPO缓解图像生成中的模式崩溃问题</div>
<div class="mono" style="margin-top:8px">强化学习（特别是GRPO）通过比较同组生成图像的相对性能，显著提升了图像生成质量。然而在训练后期，模型易产生同质化输出，缺乏创造性与视觉多样性，限制了其应用场景。该问题可从奖励建模与生成动态两个维度分析：其一，传统GRPO依赖单样本质量作为奖励信号，驱使模型收敛于少数高奖励生成模式，却忽视了分布层面的多样性；其二，常规GRPO正则化忽略了早期去噪阶段对保持多样性的主导作用，导致正则化预算分配失准，限制了质量-多样性的权衡空间。基于此，我们重新审视奖励建模与生成动态中的多样性退化问题。在奖励层面，提出基于语义分组的分布式创造力奖励：通过对同文本生成样本进行谱聚类构建分布级表征，并依据组规模自适应分配探索性奖励以激励新视觉模式的发现。在生成层面，引入结构感知正则化，通过强化早期阶段约束来保持多样性，同时不影响奖励优化效率。实验表明，本方法在同等质量分数下将语义多样性提升13%–18%，为基于GRPO的图像生成建立了新的质量-多样性帕累托前沿。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs lacking diversity in later training stages. The authors propose a two-pronged approach: at the reward level, they introduce a distributional creativity bonus using spectral clustering to encourage novel visual modes, and at the generation level, they apply structure-aware regularization with stronger early-stage constraints to preserve diversity. Experimental results show that the method improves semantic diversity by 13–18% while maintaining image quality, establishing a new Pareto frontier between quality and diversity.</div>
<div class="mono" style="margin-top:8px">本文针对基于GRPO的图像生成中存在的模式崩溃问题，即模型在训练后期产生同质化输出、缺乏多样性的现象，提出了一种双重解决方案。在奖励层面，作者通过谱聚类引入分布级创造力奖励，以鼓励新颖的视觉模式；在生成层面，采用结构感知的正则化方法，加强早期阶段的约束以保持多样性。实验结果表明，该方法在保持图像质量的同时，将语义多样性提升了13–18%，从而在质量与多样性之间建立了新的帕累托前沿。</div>
</details>
</div>
<div class="card">
<div class="title">One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents</div>
<div class="meta-line">Authors: Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Jiyan He, Yunfang Wu</div>
<div class="meta-line">First: 2025-12-24T05:27:53+00:00 · Latest: 2025-12-25T05:33:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20957v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20957v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一器足矣：面向仓库级大语言模型智能体的强化学习</div>
<div class="mono" style="margin-top:8px">在大型开源软件仓库中定位需要修改的文件和函数具有挑战性，因其规模庞大且结构复杂。现有基于大语言模型的方法通常将其视为仓库级检索任务，并依赖多种辅助工具，这忽视了代码执行逻辑且使模型控制复杂化。我们提出RepoNavigator——一种配备单一执行感知工具（跳转至被调用符号定义）的大语言模型智能体。该统一设计反映了代码执行的实际流程，同时简化了工具操作。RepoNavigator通过强化学习从预训练模型直接端到端训练，无需任何闭源蒸馏。实验表明，经强化学习训练的RepoNavigator实现了最先进的性能：7B模型超越14B基线，14B模型优于32B竞品，32B模型甚至超过Claude-3.7等闭源模型。这些结果证实，将单一结构基础工具与强化学习训练相结合，为仓库级问题定位提供了高效可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of locating relevant code in large open-source repositories, where existing LLM-based methods rely on multiple auxiliary tools, complicating control and overlooking execution logic. The authors propose RepoNavigator, an LLM agent that uses a single tool—jumping to the definition of invoked symbols—to reflect actual code execution flow and simplify tool use. It is trained end-to-end with reinforcement learning from a pretrained model, without closed-source distillation. Experimental results show state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and the 32B model exceeding closed-source models like Claude-3.7, demonstrating the efficiency of this unified, execution-aware approach.</div>
<div class="mono" style="margin-top:8px">本文针对在大型开源代码库中定位相关文件的挑战，现有基于大语言模型的方法依赖多种辅助工具，导致控制复杂且忽略代码执行逻辑。作者提出RepoNavigator，这是一个仅使用单一工具（跳转到被调用符号的定义）的大语言模型智能体，以反映实际代码执行流程并简化工具操作。该模型通过强化学习对预训练模型进行端到端训练，无需闭源蒸馏。实验结果表明其性能达到最先进水平，其中7B模型优于14B基线，14B模型超越32B竞争对手，32B模型甚至超过了Claude-3.7等闭源模型，验证了这种统一且基于执行感知的方法的高效性与可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</div>
<div class="meta-line">Authors: Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Tianle Cai, Wenhao Huang</div>
<div class="meta-line">First: 2025-12-22T22:51:48+00:00 · Latest: 2025-12-25T04:58:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19920v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19920v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model&#x27;s log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5&#x27;s (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过行为校准强化学习缓解大语言模型幻觉问题</div>
<div class="mono" style="margin-top:8px">大语言模型在关键领域的应用目前受到持续幻觉问题（生成看似合理但事实错误的断言）的阻碍。虽然扩展定律显著提升了模型的通用能力，但理论框架表明幻觉并非随机误差，而是训练目标优先模仿数据分布而非认知诚实度的可预测统计结果。标准的RLVR范式使用二元奖励信号，无意中激励模型成为优秀的应试者而非诚实的沟通者，只要正确概率超过零就鼓励猜测。本文对行为校准进行了全面研究，通过激励模型在不自信时主动弃权来随机承认不确定性，使模型行为与准确性保持一致。综合最新进展，我们提出并评估了优化严格适当评分规则的训练干预措施，使模型输出校准后的正确概率。我们的方法使模型能够选择不生成完整回答，或标记存在不确定性的具体主张。基于Qwen3-4B-Instruct的实证分析表明，行为校准强化学习使较小模型在不确定性量化方面超越前沿模型——这是一种可转移的元技能，可与原始预测准确性解耦。在数学推理任务上训练后，我们的模型在具有挑战性的领域内评估（BeyondAIME）中，对数尺度准确率-幻觉比率增益（0.806）超过GPT-5（0.207）。此外，在跨领域事实问答（SimpleQA）中，我们的40亿参数大语言模型实现了与Grok-4和Gemini-2.5-Pro等前沿模型相当的零样本校准误差，尽管其事实准确性远低于这些模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the persistent issue of hallucinations in large language models (LLMs), where models generate plausible but factually incorrect assertions, which hinders deployment in critical domains. The authors argue that standard reinforcement learning from human feedback (RLHF) with binary rewards incentivizes models to guess rather than communicate honestly, and they propose a method of behaviorally calibrated reinforcement learning that trains models to output a calibrated probability of correctness, enabling them to abstain or flag uncertain claims. Experimental results using the Qwen3-4B-Instruct model show that this approach allows smaller models to surpass frontier models in uncertainty quantification on math reasoning and factual QA tasks, achieving significant gains in accuracy-to-hallucination ratio and competitive calibration error.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型中持续存在的幻觉问题展开研究，即模型生成看似合理但事实错误的断言，这阻碍了其在关键领域的部署。作者认为，使用二元奖励的标准人类反馈强化学习会激励模型猜测而非诚实沟通，因此提出了一种行为校准的强化学习方法，训练模型输出校准后的正确概率，使其能够在不确定时弃答或标记可疑声明。基于Qwen3-4B-Instruct模型的实验结果表明，该方法使较小模型在数学推理和事实问答任务的不确定性量化上超越了前沿模型，显著提升了准确率与幻觉比率，并达到了具有竞争力的校准误差。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Is Enough: LLMs Are In-Context Reinforcement Learners</div>
<div class="meta-line">Authors: Kefan Song, Amir Moeini, Peng Wang, Lei Gong, Rohan Chandra, Shangtong Zhang, Yanjun Qi</div>
<div class="meta-line">First: 2025-05-21T16:15:01+00:00 · Latest: 2025-12-25T02:14:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06303v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.06303v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励即足够：大语言模型是上下文强化学习者</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是解决序列决策问题的框架。本研究发现，在大语言模型（LLM）的推理阶段会自发涌现强化学习行为，我们称之为上下文强化学习（ICRL）。为揭示此能力，我们提出了一个简单的多轮提示框架——ICRL提示法，用于实现推理时的自我改进。该方法旨在引导LLM在推理过程中通过强化学习优化特定任务表现：模型每轮生成响应后获得数值标量反馈（即奖励），下一轮提示时将历史响应及其对应奖励拼接为上下文输入。实验表明，随着上下文扩展，响应质量持续提升——LLM能在推理过程中优化标量奖励信号，表现出类强化学习行为。我们在24点游戏、创意写作、ScienceWorld及奥林匹克数学竞赛（AIME与HMMT）上评估ICRL提示法，其性能显著超越Self-Refine、Reflexion等基线方法。值得注意的是，即使奖励信号由同一LLM生成，该方法仍能提升表现，这为测试时扩展提供了新的研究范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the hypothesis that reinforcement learning (RL) capabilities can emerge in large language models (LLMs) during inference, without explicit training. The method introduces in-context RL (ICRL) prompting, a multi-round framework where an LLM receives numerical reward feedback after each response and is then re-prompted with the history of prior responses and rewards to iteratively self-improve. Experimental results across tasks like Game of 24, creative writing, ScienceWorld, and math competitions show that response quality consistently improves with more rounds, outperforming baselines such as Self-Refine and Reflexion, even when rewards are generated by the same LLM, indicating effective inference-time RL behavior.</div>
<div class="mono" style="margin-top:8px">本文的动机是假设大型语言模型（LLM）在推理过程中能够自发涌现强化学习（RL）能力，而无需显式训练。方法上提出了上下文内强化学习（ICRL）提示框架，通过多轮交互让LLM在每次生成响应后接收数值奖励反馈，并在下一轮提示中结合历史响应和奖励进行迭代自我改进。实验结果表明，在24点游戏、创意写作、ScienceWorld和奥林匹克数学竞赛等任务中，响应质量随着轮次增加持续提升，超越了Self-Refine和Reflexion等基线方法，即使奖励由同一LLM生成也能改善性能，这揭示了推理时强化学习的新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Fast Adaptive Anti-Jamming Channel Access via Deep Q Learning and Coarse-Grained Spectrum Prediction</div>
<div class="meta-line">Authors: Jianshu Zhang, Xiaofu Wu, Junquan Hu</div>
<div class="meta-line">First: 2025-02-07T14:25:28+00:00 · Latest: 2025-12-25T01:47:31+00:00</div>
<div class="meta-line">Comments: This paper is accepted by IEEE Transactions on Vehicular Technology (TVT), 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04963v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.04963v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates the anti-jamming channel access problem in complex and unknown jamming environments, where the jammer could dynamically adjust its strategies to target different channels. Traditional channel hopping anti-jamming approaches using fixed patterns are ineffective against such dynamic jamming attacks. Although the emerging deep reinforcement learning (DRL) based dynamic channel access approach could achieve the Nash equilibrium (NE) under fast-changing jamming attacks, it requires extensive training episodes. To address this issue, we propose a fast adaptive anti-jamming channel access approach guided by the intuition of ``learning faster than the jammer&quot;, where a synchronously updated coarse-grained spectrum prediction serves as an auxiliary task for the deep Q network (DQN) based anti-jamming model. This helps the model identify a superior Q-function compared to standard DRL while significantly reducing the number of training episodes. Numerical results indicate that the proposed approach significantly accelerates the rate of convergence in model training, reducing the required training episodes by up to 70\% compared to standard DRL. Additionally, it also achieves a 10\% improvement in throughput over NE strategies, owing to the effective use of coarse-grained spectrum prediction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度Q学习与粗粒度频谱预测的快速自适应抗干扰信道接入方法</div>
<div class="mono" style="margin-top:8px">本文研究了复杂未知干扰环境中的抗干扰信道接入问题，其中干扰机可动态调整策略以针对不同信道。采用固定模式的传统信道跳频抗干扰方法对此类动态干扰攻击效果有限。尽管新兴的基于深度强化学习（DRL）的动态信道接入方法能在快速变化的干扰攻击下达到纳什均衡（NE），但其需要大量训练轮次。为解决该问题，我们提出一种基于“学习速度快于干扰机”直觉的快速自适应抗干扰信道接入方法，其中同步更新的粗粒度频谱预测作为基于深度Q网络（DQN）的抗干扰模型的辅助任务。这有助于模型识别出优于标准DRL的Q函数，同时显著减少训练轮次。数值结果表明，所提方法显著加快了模型训练的收敛速度，与标准DRL相比所需训练轮次减少高达70%。此外，通过有效利用粗粒度频谱预测，其吞吐量较NE策略还提升了10%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient anti-jamming channel access against dynamic jammers that render traditional fixed-pattern hopping ineffective, this paper proposes a fast adaptive approach using deep Q-learning enhanced with coarse-grained spectrum prediction as an auxiliary task. The method leverages the intuition of learning faster than the jammer, integrating prediction to help the DQN-based model identify a superior Q-function, thereby reducing training requirements. Experimental results show that this approach accelerates convergence, cutting training episodes by up to 70% compared to standard deep reinforcement learning, while also improving throughput by 10% over Nash equilibrium strategies through effective spectrum prediction.</div>
<div class="mono" style="margin-top:8px">针对动态干扰环境下传统固定模式信道跳频抗干扰方法失效的问题，本文提出了一种基于深度Q学习和粗粒度频谱预测的快速自适应抗干扰信道接入方法。该方法以“学习速度快于干扰者”为指导思想，将同步更新的粗粒度频谱预测作为深度Q网络的辅助任务，帮助模型识别更优的Q函数，从而显著降低训练需求。数值结果表明，所提方法大幅加快了模型训练的收敛速度，与标准深度强化学习相比，所需训练回合数减少高达70%，同时得益于有效的频谱预测，其吞吐量较纳什均衡策略提升了10%。</div>
</details>
</div>
<div class="card">
<div class="title">dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning</div>
<div class="meta-line">Authors: Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu</div>
<div class="meta-line">First: 2025-12-24T23:31:48+00:00 · Latest: 2025-12-24T23:31:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21446v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21446v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model&#x27;s samples. We propose \texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy&#x27;&#x27; over autoregressive models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>dUltra：基于强化学习的超快速扩散语言模型</div>
<div class="mono" style="margin-top:8px">掩码扩散语言模型（MDLMs）具备并行生成词元的潜力，但即使采用复杂采样策略，多数开源MDLMs单次前向传播的解码词元仍少于5个，导致其采样速度常与自回归+推测解码方案相当，限制了其相对于主流自回归方法的优势。现有基于蒸馏的加速器（dParallel、d3LLM）在基础模型生成的轨迹上微调MDLMs，但微调过程可能偏离策略，且性能受限于基础模型样本质量。本文提出\texttt{dUltra}——一种基于组相对策略优化（GRPO）的同策略强化学习框架，通过学习解掩码策略实现高效并行解码。dUltra引入解掩码规划头，在独立伯努利分布下预测各词元的解掩码概率。我们结合可验证奖励、蒸馏奖励与解掩码步骤数构建奖励信号，联合优化基础扩散大语言模型与解掩码顺序规划器。在数学推理与代码生成任务中，dUltra在精度-效率权衡上优于当前最优启发式与蒸馏基线，推动实现扩散模型对自回归模型的“扩散优势”。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for dUltra stems from the observation that while masked diffusion language models (MDLMs) enable parallel token generation, their practical sampling speed is often limited, as they typically decode fewer than 5 tokens per forward pass, making them comparable to autoregressive models with speculative decoding. To address this, the method introduces dUltra, an on-policy reinforcement learning framework using Group Relative Policy Optimization (GRPO) to learn efficient unmasking strategies; it incorporates an unmasking planner head that predicts per-token unmasking likelihoods and jointly optimizes the diffusion model and planner with rewards for verifiability, distillation, and step count. The main experimental results show that dUltra improves the accuracy-efficiency trade-off over existing heuristic and distillation baselines in mathematical reasoning and code generation tasks, advancing towards &#x27;diffusion supremacy&#x27; over autoregressive models.</div>
<div class="mono" style="margin-top:8px">dUltra的研究动机在于，尽管掩码扩散语言模型（MDLM）支持并行令牌生成，但其实际采样速度通常受限，每次前向传播解码的令牌数常少于5个，性能与采用推测解码的自回归模型相当。为解决此问题，该方法提出了dUltra，这是一个基于组相对策略优化（GRPO）的在线策略强化学习框架，用于学习高效的解掩码策略；它引入了一个解掩码规划头来预测每个令牌在独立伯努利分布下的解掩码概率，并通过结合可验证奖励、蒸馏奖励和解掩码步骤数的奖励信号，联合优化基础扩散大模型和解掩码顺序规划器。主要实验结果表明，在数学推理和代码生成任务中，dUltra相比现有的启发式和蒸馏基线方法，提升了准确性与效率的权衡，推动了扩散模型相对于自回归模型的&#x27;扩散优势&#x27;。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
