<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-08 03:42</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260108_0342</div>
    <div class="row"><div class="card">
<div class="title">Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion</div>
<div class="meta-line">Authors: Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-06T17:52:02+00:00 · Latest: 2026-01-06T17:52:02+00:00</div>
<div class="meta-line">Comments: Preprint. Under review at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03213v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像扩散中的批评家引导强化学习遗忘</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型的机器遗忘旨在移除特定概念，同时保持整体效用。现有扩散遗忘方法通常依赖监督式权重编辑或全局惩罚；强化学习方法虽灵活，但常优化稀疏的轨迹末端奖励，导致高方差更新和弱信用分配。本文提出一种通用的强化学习框架，将去噪视为序列决策过程，并引入具有噪声步奖励的时序感知批评家。具体而言，我们在噪声潜在空间上训练基于CLIP的奖励预测器，利用其每步信号计算优势估计，以更新反向扩散核的策略梯度。该算法实现简单，支持离策略重用，并可集成至标准文本到图像主干网络。在多种概念上的实验表明，本方法在保持图像质量和良性提示保真度的同时，实现了优于或可比拟强基线的遗忘效果；消融研究证实：（i）每步批评家与（ii）噪声条件奖励是稳定性和有效性的关键。我们公开了代码与评估脚本，以促进基于强化学习的扩散遗忘研究的可复现性与未来探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of machine unlearning in text-to-image diffusion models, aiming to remove specific concepts without degrading overall model utility. The authors propose a reinforcement learning framework that treats the denoising process as a sequential decision problem, introducing a timestep-aware critic with per-step rewards conditioned on noisy latents to improve credit assignment and reduce variance. Experimental results demonstrate that this method achieves effective concept forgetting comparable to or better than existing baselines while preserving image quality and fidelity to benign prompts, with ablations confirming the importance of per-step critics and noisy-conditioned rewards for stable performance.</div>
<div class="mono" style="margin-top:8px">本文针对文本到图像扩散模型中的机器遗忘问题，旨在移除特定概念而不损害模型整体性能。研究者提出一个强化学习框架，将去噪过程视为序列决策问题，引入基于时间步的评论家模型，利用噪声潜在空间中的每步奖励来改进信用分配并降低方差。实验结果表明，该方法在多个概念上实现了与现有基线相当或更好的遗忘效果，同时保持了图像质量和对良性提示的保真度，消融研究证实了每步评论家和噪声条件奖励对稳定性和有效性的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward</div>
<div class="meta-line">Authors: Yile Liu, Yixian Liu, Zongwei Li, Yufei Huang, Xinhua Feng, Zhichao Hu, Jinglu Hu, Jianfeng Yan, Fengzong Lian, Yuhong Liu</div>
<div class="meta-line">First: 2026-01-06T17:41:32+00:00 · Latest: 2026-01-06T17:41:32+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UltraLogic：通过大规模数据合成与双极浮动奖励增强大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在自然语言处理领域展现出巨大潜力，但需要多步逻辑、规划与验证的复杂通用推理仍是关键瓶颈。虽然可验证奖励的强化学习在特定领域取得成功，但通用推理领域仍缺乏大规模、高质量且难度分级的数据。为此，我们提出UltraLogic框架，通过基于代码的求解方法将问题的逻辑核心与其自然语言表达解耦，实现高质量数据的自动化生成。该框架包含数百种独特任务类型，并建立了跨越十个难度级别的自动校准流程。此外，为缓解二元奖励稀疏性与非负奖励陷阱，我们引入双极浮动奖励机制，利用分级惩罚有效区分完美响应与存在逻辑缺陷的响应。实验表明，任务多样性是提升推理能力的主要驱动力，而双极浮动奖励结合难度匹配策略能显著提高训练效率，引导模型趋向全局逻辑最优解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind UltraLogic is to overcome the bottleneck in complex general-purpose reasoning of Large Language Models (LLMs), which lack large-scale, high-quality, and difficulty-calibrated data. The method introduces a framework that decouples logical problem cores from natural language via a Code-based Solving approach to automate data synthesis, featuring hundreds of task types and an automated calibration pipeline across ten difficulty levels, along with a Bipolar Float Reward (BFR) mechanism to address binary reward sparsity by applying graded penalties. Experimental results show that task diversity is key to enhancing reasoning, and BFR combined with difficulty matching significantly boosts training efficiency, guiding models toward global logical optima.</div>
<div class="mono" style="margin-top:8px">UltraLogic的动机是解决大型语言模型在复杂通用推理中缺乏大规模、高质量和难度校准数据的瓶颈问题。该方法通过基于代码求解的方法，将问题的逻辑核心与自然语言表达解耦，以自动化合成高质量数据，包含数百种任务类型和跨十个难度级别的自动校准流程，并引入双极浮动奖励机制，通过分级惩罚来区分完美响应与存在逻辑缺陷的响应。实验结果表明，任务多样性是提升推理能力的主要驱动力，而双极浮动奖励结合难度匹配策略显著提高了训练效率，引导模型实现全局逻辑优化。</div>
</details>
</div>
<div class="card">
<div class="title">DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization</div>
<div class="meta-line">Authors: Gang Li, Ming Lin, Tomer Galanti, Zhengzhong Tu, Tianbao Yang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-18T11:08:32+00:00 · Latest: 2026-01-06T16:40:19+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12366v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.12366v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach, yielding long and stable training dynamics; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for a 1.5B model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DisCO：基于判别式约束优化强化大型推理模型</div>
<div class="mono" style="margin-top:8px">DeepSeek-R1 近期的成功与开源使群体相对策略优化（GRPO）作为大型推理模型（LRM）的强化学习方法受到广泛关注。本研究在二元奖励设定下分析了 GRPO 目标函数，揭示了其固有的问题级难度偏差局限，并发现了 GRPO 与监督学习中传统判别式方法的关联。基于这些洞察，我们提出了基于判别学习原理的判别式约束优化（DisCO）框架，用于强化 LRM。DisCO 与 GRPO 及其近期变体的主要区别在于：（1）用评分函数定义的判别目标替代群体相对目标；（2）放弃基于裁剪的替代函数，采用非裁剪强化学习替代目标作为评分函数；（3）采用简洁有效的约束优化方法强制执行 KL 散度约束。因此，DisCO 相比 GRPO 及其变体具有显著优势：（i）通过判别目标完全消除难度偏差；（ii）利用非裁剪评分函数与约束优化方法解决 GRPO 及其变体的熵不稳定问题，实现长周期稳定训练动态；（iii）可融合先进判别学习技术处理数据不平衡问题（训练中大量问题生成的负例答案多于正例）。在增强 SFT 微调模型数学推理能力的实验中，DisCO 显著优于 GRPO 及其改进变体（如 DAPO），在 1.5B 参数的模型上，六项基准任务平均提升达 7%（对比 GRPO）和 6%（对比 DAPO）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces DisCO, a Discriminative Constrained Optimization framework designed to address limitations in Group Relative Policy Optimization (GRPO) for reinforcing large reasoning models. The motivation stems from an analysis revealing GRPO&#x27;s inherent difficulty bias under binary rewards and its connection to discriminative learning, prompting a method that replaces GRPO&#x27;s group relative objective with a discriminative scoring function, employs non-clipping RL surrogates as scoring functions, and uses constrained optimization to enforce KL divergence constraints. Experimental results on enhancing mathematical reasoning in SFT-finetuned models demonstrate that DisCO significantly outperforms GRPO and its variant DAPO, achieving average gains of 7% and 6% respectively across six benchmark tasks for a 1.5B model, while eliminating difficulty bias and ensuring stable training dynamics.</div>
<div class="mono" style="margin-top:8px">本文提出了DisCO（判别式约束优化）框架，旨在解决大型推理模型强化学习中组相对策略优化（GRPO）的局限性。其动机源于对GRPO在二元奖励下固有难度偏差的分析及其与判别式学习的关联，从而设计了一种方法：用判别式评分函数替代GRPO的组相对目标，采用非剪裁的强化学习替代目标作为评分函数，并通过约束优化来强制执行KL散度约束。在增强SFT微调模型数学推理能力的实验中，DisCO显著优于GRPO及其改进变体DAPO，在1.5B模型的六个基准任务上平均分别提升7%和6%，同时消除了难度偏差并确保了稳定的训练动态。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Thinker: A General Reasoning Modular Core for Image Generation</div>
<div class="meta-line">Authors: Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song, Tiezheng Ge, Cheng Yu, Bo Zheng, Zhou Zhao</div>
<div class="meta-line">First: 2026-01-06T15:59:33+00:00 · Latest: 2026-01-06T15:59:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一思考者：面向图像生成的通用推理模块化核心</div>
<div class="mono" style="margin-top:8px">尽管高保真图像合成已取得显著进展，生成模型在遵循逻辑密集型指令方面仍存在困难，暴露出持续的推理-执行鸿沟。与此同时，闭源系统（如Nano Banana）已展现出强大的推理驱动图像生成能力，突显了与当前开源模型间的显著差距。我们认为，弥合这一差距不仅需要更优的视觉生成器，更需可执行的推理能力：将高层意图分解为可直接指导生成过程的、可验证的具象化规划。为此，我们提出统一思考者——一种面向通用图像生成的任务无关推理架构，其设计为可接入多样化生成器与工作流的统一规划核心。该架构将专用思考者模块与图像生成器解耦，实现无需重新训练整个生成模型的推理模块化升级。我们进一步引入两阶段训练范式：首先为思考者构建结构化规划接口，随后应用强化学习使其策略基于像素级反馈进行具象化优化，推动规划更侧重于视觉正确性而非文本合理性。在文本到图像生成与图像编辑任务上的大量实验表明，统一思考者显著提升了图像推理与生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the persistent reasoning-execution gap in generative models and the strong performance of closed-source systems, this paper proposes Unified Thinker, a task-agnostic reasoning architecture designed as a modular core to decompose high-level intents into grounded plans for steering image generation. The method decouples a dedicated Thinker module from the image Generator, enabling modular upgrades, and employs a two-stage training paradigm involving a structured planning interface followed by reinforcement learning with pixel-level feedback to ground plans in visual correctness. Experimental results on text-to-image generation and image editing demonstrate that Unified Thinker substantially improves reasoning and generation quality compared to existing open-source models.</div>
<div class="mono" style="margin-top:8px">本文的动机是生成模型在逻辑密集型指令遵循上存在持续的推理-执行差距，而闭源系统已展现出强大的推理驱动图像生成能力。为此，论文提出了Unified Thinker，一种任务无关的推理架构，作为统一规划核心，可接入不同生成器与工作流，其方法将专用的思考模块与图像生成器解耦以实现模块化升级，并采用两阶段训练范式：先构建结构化规划接口，再通过强化学习利用像素级反馈使策略基于视觉正确性。在文本到图像生成和图像编辑上的大量实验表明，该方法显著提升了图像推理与生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">Gradient Coupling: The Hidden Barrier to Generalization in Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu</div>
<div class="meta-line">First: 2025-09-28T13:24:38+00:00 · Latest: 2026-01-06T15:48:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23870v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23870v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a dominant paradigm for training autonomous agents, yet these agents often exhibit poor generalization, failing to adapt to scenarios not seen during training. In this work, we identify a fundamental cause of this brittleness, a phenomenon which we term &quot;gradient coupling.&quot; We hypothesize that in complex agentic tasks, the high similarity between distinct states leads to destructive interference between gradients. Specifically, a gradient update that reinforces an optimal action in one state can inadvertently increase the likelihood of a suboptimal action in a similar, yet different, state. To solve this, we propose a novel objective where the actor is trained to simultaneously function as a classifier that separates good and bad actions. This auxiliary pressure compels the model to learn disentangled embeddings for positive and negative actions, which mitigates negative gradient interference and improve the generalization performance. Extensive experiments demonstrate the effectiveness of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>梯度耦合：智能体强化学习中泛化能力的隐性障碍</div>
<div class="mono" style="margin-top:8px">强化学习是训练自主智能体的主流范式，但这些智能体常表现出较差的泛化能力，难以适应训练时未见的场景。本研究揭示了导致这种脆弱性的根本原因——我们称之为“梯度耦合”的现象。我们假设在复杂的智能体任务中，不同状态间的高度相似性会导致梯度间的破坏性干扰。具体而言，强化某一状态最优动作的梯度更新，可能无意中增加相似但不同状态下次优动作的选择概率。为解决此问题，我们提出一种新颖的目标函数：将执行器同时训练为区分优劣动作的分类器。这种辅助压力迫使模型学习正负动作的解耦嵌入表示，从而减轻负面梯度干扰并提升泛化性能。大量实验验证了本方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the poor generalization of reinforcement learning agents, attributing it to a phenomenon called &#x27;gradient coupling,&#x27; where high similarity between states causes gradient updates for optimal actions in one state to negatively interfere with action selection in similar states. To address this, the authors propose a novel training objective that compels the actor network to also function as a classifier separating good from bad actions, thereby learning disentangled embeddings that mitigate destructive gradient interference. Experimental results demonstrate that this method effectively improves the agent&#x27;s generalization performance.</div>
<div class="mono" style="margin-top:8px">本文研究了强化学习智能体泛化能力差的问题，将其归因于一种称为&#x27;梯度耦合&#x27;的现象，即状态间的高度相似性导致一个状态中优化动作的梯度更新会干扰相似但不同状态中的动作选择。为解决此问题，作者提出了一种新颖的训练目标，迫使执行器网络同时充当区分好坏动作的分类器，从而学习解耦的表征以减轻破坏性的梯度干扰。大量实验结果表明，该方法有效提升了智能体的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling</div>
<div class="meta-line">Authors: Yiyuan Li, Zhen Huang, Yanan Wu, Weixun Wang, Xuefeng Li, Yijia Luo, Wenbo Su, Bo Zheng, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-06T15:41:35+00:00 · Latest: 2026-01-06T15:41:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03111v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03111v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一统万法：强化学习规模化中的极致数据效率</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的推理能力可通过强化学习（RL）得以释放（OpenAI, 2024；深度求索AI等, 2025a；Zeng等, 2025）。现有LLMs中RL尝试的成功通常依赖于数千乃至更多的高质量样本。本文通过展示单样本学习的显著有效性，挑战了关于LLMs中RL数据需求的基本假设。具体而言，我们提出了博学学习框架，用于设计能引发多学科影响的单一训练样本。我们呈现了三个关键发现：（1）单个经策略性选择的数学推理样本可在物理、化学和生物等多个领域通过RL产生显著的性能提升；（2）对推理至关重要的数学技能揭示了最优博学样本的特征；（3）整合多学科元素的工程化合成样本优于使用自然出现的单个样本进行训练。我们的方法在多种推理基准测试中实现了优于使用更大数据集训练的性能，表明样本质量与设计（而非数量）可能是解锁语言模型增强推理能力的关键。我们的结果预示了一种被称为样本工程的转变，即转向对训练样本进行精准工程化设计，而非单纯增加数据量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high data demands of existing reinforcement learning (RL) approaches for large language models (LLMs), this paper challenges the assumption that thousands of samples are necessary by demonstrating the extreme data efficiency of one-shot learning. The method introduces &#x27;polymath learning&#x27;, a framework for designing a single, strategically engineered training sample intended to elicit multidisciplinary impact. Key experimental results show that a single, well-designed math reasoning sample can produce significant performance improvements across diverse domains like physics, chemistry, and biology via RL, and that such an engineered synthetic sample outperforms training with larger, naturally occurring datasets on various reasoning benchmarks, suggesting sample quality and design are more critical than quantity.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型强化学习通常需要数千高质量样本的高数据需求，挑战了这一基本假设，展示了单样本学习的极高数据效率。其方法引入了&#x27;博学学习&#x27;框架，旨在设计一个能引发多学科影响的单一战略训练样本。主要实验结果表明，一个精心设计的数学推理样本通过强化学习能在物理、化学和生物等多个领域带来显著的性能提升，并且这种工程化的合成样本在多种推理基准测试上优于使用更大规模自然数据集进行训练的效果，表明样本质量与设计可能比数量更能解锁模型的高级推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">A Comedy of Estimators: On KL Regularization in RL Training of LLMs</div>
<div class="meta-line">Authors: Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville</div>
<div class="meta-line">First: 2025-12-26T04:20:58+00:00 · Latest: 2026-01-06T15:07:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21852v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21852v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>估计量的喜剧：论大语言模型强化学习训练中的KL正则化</div>
<div class="mono" style="margin-top:8px">通过强化学习训练可大幅提升大语言模型的推理性能。其训练目标包含一个正则化项，即训练策略与参考策略之间的反向KL散度。由于精确计算KL散度不可行，实践中常使用各类估计器基于在线策略样本进行估计。尽管该方法已被广泛采用（包括多个开源库），但尚未有系统研究分析KL估计器融入目标函数的不同方式及其对RL训练模型下游性能的影响。近期研究表明，当前主流的KL正则化实践并未为既定目标提供正确的梯度，导致目标与实现之间存在偏差。本文进一步分析这些实践，研究多种估计器配置的梯度特性，揭示设计选择如何影响梯度偏差。我们通过RL微调\texttt{Qwen2.5-7B}、\texttt{Llama-3.1-8B-Instruct}和\texttt{Qwen3-4B-Instruct-2507}模型的不同配置，并在分布内与分布外任务上评估其性能，以实证结果验证上述发现。分析表明，在在线策略设置中：（1）具有偏差梯度的估计器配置可能导致训练不稳定；（2）使用产生无偏梯度的估计器配置能在领域内及跨领域任务上获得更优性能。我们还研究了离线策略设置中不同KL配置的性能表现，发现KL正则化有助于稳定异步设置导致的离线策略RL训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the practical implementation of KL regularization in reinforcement learning (RL) training for large language models (LLMs), motivated by the widespread but unanalyzed use of various estimators for the intractable reverse KL divergence in the RL objective. The method involves a systematic analysis of different configurations for incorporating KL estimators, studying their gradient properties to reveal biases, and empirically testing these configurations by RL fine-tuning models like Qwen2.5-7B and Llama-3.1-8B on both in-distribution and out-of-distribution tasks. The main experimental results show that estimator configurations with biased gradients lead to training instabilities, while those with unbiased gradients yield better performance across tasks, and KL regularization also helps stabilize off-policy RL training in asynchronous setups.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLM）强化学习（RL）训练中KL正则化的实际实现，其动机在于实践中广泛使用各种估计器来近似难以精确计算的反向KL散度，但缺乏系统分析。方法包括系统分析整合KL估计器的不同配置，研究其梯度特性以揭示偏差，并通过RL微调Qwen2.5-7B和Llama-3.1-8B等模型在分布内和分布外任务上进行实证测试。主要实验结果表明，具有偏差梯度的估计器配置会导致训练不稳定，而具有无偏梯度的配置则能在各项任务中取得更好性能，且KL正则化还有助于稳定异步设置中的离策略RL训练。</div>
</details>
</div>
<div class="card">
<div class="title">EvoGPT: Leveraging LLM-Driven Seed Diversity to Improve Search-Based Test Suite Generation</div>
<div class="meta-line">Authors: Lior Broide, Roni Stern, Argaman Mordoch</div>
<div class="meta-line">First: 2025-05-18T13:48:53+00:00 · Latest: 2026-01-06T14:41:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12424v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12424v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search-Based Software Testing (SBST) is a well-established approach for automated unit test generation, yet it often suffers from premature convergence and limited diversity in the generated test suites. Recently, Large Language Models (LLMs) have emerged as an alternative technique for unit test generation. We present EvoGPT, a hybrid test generation system that integrates LLM-based test generation with SBST-based test suite optimization. EvoGPT uses LLMs to generate an initial population of test suites, and uses an Evolutionary Algorithm (EA) to further optimize this test suite population. A distinguishing feature of EvoGPT is its explicit enforcement of diversity, achieved through the use of multiple temperatures and prompt instructions during test generation. In addition, each LLM-generated test is refined using a generation-repair loop and coverage-guided assertion generation. To address evolutionary plateaus, EvoGPT also detects stagnation during search and injects additional LLM-generated tests aimed at previously uncovered branches. Here too diversity is enforced using multiple temperatures and prompt instructions. We evaluate EvoGPT on Defects4J, a standard benchmark for test generation. The results show that EvoGPT achieves, on average, a 10\% improvement in both code coverage and mutation score metrics compared to TestART, an LLM-only baseline; and EvoSuite, a standard SBST baseline. An ablation study indicates that explicitly enforcing diversity both at initialization and during the search is key to effectively leveraging LLMs for automated unit test generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvoGPT：利用大语言模型驱动的种子多样性改进基于搜索的测试套件生成</div>
<div class="mono" style="margin-top:8px">基于搜索的软件测试（SBST）是一种成熟的自动化单元测试生成方法，但常面临早熟收敛和生成测试套件多样性不足的问题。近期，大语言模型（LLM）已成为单元测试生成的替代技术。本文提出EvoGPT，一种融合LLM测试生成与SBST测试套件优化的混合测试生成系统。EvoGPT使用LLM生成初始测试套件种群，并采用进化算法（EA）进一步优化该种群。其显著特点是通过在测试生成阶段使用多温度参数和多样化提示指令，显式增强多样性。此外，每个LLM生成的测试均通过生成-修复循环和覆盖率引导的断言生成进行精炼。为应对进化停滞，EvoGPT在搜索过程中检测停滞状态，并注入针对未覆盖分支的额外LLM生成测试，此过程同样采用多温度参数和提示指令以保持多样性。我们在测试生成标准基准Defects4J上评估EvoGPT，结果表明：相较于纯LLM基线TestART和标准SBST基线EvoSuite，EvoGPT在代码覆盖率和变异分数指标上平均提升10%。消融实验证实，在初始化和搜索阶段显式增强多样性，是有效利用LLM进行自动化单元测试生成的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces EvoGPT, a hybrid system that combines Large Language Models (LLMs) with Search-Based Software Testing (SBST) to address issues of premature convergence and limited diversity in automated unit test generation. The method uses LLMs with varied temperatures and prompts to generate a diverse initial population of test suites, which are then optimized via an Evolutionary Algorithm (EA), incorporating a generation-repair loop and coverage-guided assertions; during search stagnation, it injects additional LLM-generated tests targeting uncovered branches while maintaining diversity. Experimental results on the Defects4J benchmark show that EvoGPT improves code coverage and mutation scores by an average of 10% compared to both LLM-only and SBST baselines, with ablation studies confirming the importance of enforced diversity at initialization and during search.</div>
<div class="mono" style="margin-top:8px">本文提出了EvoGPT，一个将大型语言模型（LLMs）与基于搜索的软件测试（SBST）相结合的混合系统，旨在解决自动化单元测试生成中早熟收敛和多样性不足的问题。该方法利用不同温度和提示指令的LLMs生成多样化的初始测试套件种群，然后通过进化算法进行优化，并融入生成-修复循环和覆盖率引导的断言生成；在搜索停滞时，系统注入针对未覆盖分支的额外LLM生成测试，同时保持多样性。在Defects4J基准上的实验结果表明，与纯LLM和SBST基线相比，EvoGPT平均提高了10%的代码覆盖率和变异分数，消融研究证实了在初始化和搜索过程中强制实施多样性的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation</div>
<div class="meta-line">Authors: Yankai Jiang, Qiaoru Li, Binlu Xu, Haoran Sun, Chao Ding, Junting Dong, Yuxiang Cai, Xuhong Zhang, Jianwei Yin</div>
<div class="meta-line">First: 2026-01-06T14:37:50+00:00 · Latest: 2026-01-06T14:37:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03054v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03054v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model&#x27;s robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IBISAgent：强化多模态大语言模型的像素级视觉推理能力，实现通用生物医学对象指代与分割</div>
<div class="mono" style="margin-top:8px">近期医学多模态大语言模型的研究重心正逐渐从图像级理解转向细粒度像素级理解。尽管分割是像素级理解的基础，现有方法面临两大挑战：其一，它们引入隐式分割标记，并需同时微调MLLM与外部像素解码器，这会增加灾难性遗忘风险并限制跨域泛化能力；其二，多数方法依赖单次推理，缺乏迭代优化分割结果的能力，导致性能欠佳。为突破这些局限，我们提出新型智能体化MLLM——IBISAgent，将分割重构为以视觉为中心的多步决策过程。该模型使MLLM无需架构修改即可生成交错推理与基于文本的点击动作、调用分割工具并输出高质量掩码。通过对掩码图像特征进行迭代式多步视觉推理，IBISAgent天然支持掩码优化，并促进像素级视觉推理能力的发展。我们进一步设计包含冷启动监督微调与细粒度奖励强化学习的双阶段训练框架，提升模型在复杂医学指代与推理分割任务中的鲁棒性。大量实验表明，IBISAgent在闭源与开源SOTA方法中均取得持续领先。所有数据集、代码及训练模型将公开释放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to shift medical MLLMs from image-level to fine-grained pixel-level understanding while overcoming limitations of existing segmentation methods—such as catastrophic forgetting from joint fine-tuning and lack of iterative refinement—this paper introduces IBISAgent, an agentic MLLM that reformulates segmentation as a multi-step decision-making process. The method enables MLLMs to perform interleaved reasoning and text-based click actions to invoke segmentation tools without architectural changes, using a two-stage training framework with supervised fine-tuning and agentic reinforcement learning with fine-grained rewards. Experimental results show that IBISAgent consistently outperforms state-of-the-art closed-source and open-source methods in universal biomedical object referring and segmentation tasks.</div>
<div class="mono" style="margin-top:8px">本文的动机是将医学多模态大模型从图像级理解转向细粒度像素级理解，并克服现有分割方法的局限，如联合微调导致的灾难性遗忘和缺乏迭代优化能力。为此，提出了一种名为IBISAgent的智能体多模态大模型，将分割重新定义为多步决策过程，该方法使模型能够通过交错推理和基于文本的点击动作调用分割工具，无需修改架构，并采用包含监督微调和基于细粒度奖励的智能体强化学习的两阶段训练框架。主要实验结果表明，IBISAgent在通用生物医学对象指代和分割任务中持续优于闭源和开源的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis</div>
<div class="meta-line">Authors: Choonghan Kim, Hyunmin Hwang, Hangeol Chang, Jaemin Kim, Jinse Park, Jae-Sung Lim, Jong Chul Ye</div>
<div class="meta-line">First: 2026-01-06T13:44:04+00:00 · Latest: 2026-01-06T13:44:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03018v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03018v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dementia-R1：基于无结构临床记录的强化预训练与推理实现真实世界痴呆症预后预测</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在临床文本理解方面表现出色，但在痴呆症预后等需要跨多次就诊对复杂非单调症状轨迹进行推理的纵向预测任务中仍存在困难。标准监督训练缺乏症状演变的显式标注，而直接强化学习则受限于稀疏的二元奖励信号。为解决这一挑战，我们提出了Dementia-R1——一个基于强化学习的框架，用于从无结构临床记录中实现纵向痴呆症预后预测。该方法采用冷启动强化学习策略，通过预训练模型预测从患者历史中提取的可验证临床指标，从而增强在确定最终临床状态前对疾病进展的推理能力。大量实验表明，Dementia-R1在真实世界无结构临床数据集上取得了77.03%的F1分数。值得注意的是，在ADNI基准测试中，我们的70亿参数模型性能媲美GPT-4o，能有效捕捉波动的认知轨迹。代码已开源：https://anonymous.4open.science/r/dementiar1-CDB5</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of using Large Language Models (LLMs) for longitudinal dementia prognosis, a task requiring reasoning over complex, non-monotonic symptom trajectories across multiple clinical visits, where standard supervised training lacks explicit evolution annotations and direct Reinforcement Learning (RL) suffers from sparse rewards. The proposed method, Dementia-R1, introduces a Cold-Start RL framework that first pre-trains the model to predict verifiable clinical indices extracted from patient histories, thereby enhancing its reasoning capability about disease progression before making a final prognosis. Experimental results on real-world unstructured clinical data show that Dementia-R1 achieves an F1 score of 77.03%, and notably, on the ADNI benchmark, their 7B parameter model performs competitively with GPT-4o in capturing fluctuating cognitive trajectories.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在纵向痴呆症预后任务中的挑战展开研究，该任务需要基于多次临床就诊中复杂、非单调的症状轨迹进行推理，而标准监督训练缺乏明确的演变标注，直接强化学习则受限于稀疏的奖励信号。所提出的方法Dementia-R1采用了一种冷启动强化学习框架，首先预训练模型以预测从患者历史中提取的可验证临床指标，从而在确定最终临床状态前增强其对疾病进展的推理能力。在真实世界非结构化临床数据集上的实验结果表明，Dementia-R1取得了77.03%的F1分数，特别是在ADNI基准测试中，其70亿参数模型在捕捉波动的认知轨迹方面与GPT-4o表现相当。</div>
</details>
</div>
<div class="card">
<div class="title">In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior</div>
<div class="meta-line">Authors: Anaïs Berkes, Vincent Taboga, Donna Vakalis, David Rolnick, Yoshua Bengio</div>
<div class="meta-line">First: 2026-01-06T13:41:31+00:00 · Latest: 2026-01-06T13:41:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03015v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03015v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. We introduce SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, our online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. We prove that SPICE achieves regret-optimal behaviour in both stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. We validate these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯融合上下文与价值先验的上下文强化学习</div>
<div class="mono" style="margin-top:8px">上下文强化学习（ICRL）无需参数更新即可快速适应未知环境，但现有方法要么无法超越训练分布提升性能，要么需要接近最优的数据，限制了实际应用。我们提出SPICE，一种贝叶斯ICRL方法，通过深度集成学习Q值的先验分布，并在测试时利用上下文信息通过贝叶斯更新调整该先验。为克服次优训练数据导致的先验偏差，我们的在线推理采用上置信界规则以促进探索与适应。理论证明，即使在次优轨迹上预训练，SPICE在随机多臂赌博机和有限时域马尔可夫决策过程中均能实现最优遗憾行为。我们在赌博机与控制基准测试中实证验证了这些结论：SPICE在未知任务上做出接近最优的决策，相比现有ICRL与元强化学习方法显著降低遗憾值，同时快速适应新任务并保持分布偏移下的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of in-context reinforcement learning (ICRL), which struggles to improve beyond training distributions or requires optimal data, by proposing SPICE, a Bayesian ICRL method. SPICE learns a prior over Q-values using a deep ensemble and updates it at test-time via Bayesian fusion with in-context information, incorporating an Upper-Confidence Bound rule to enhance exploration and adapt from suboptimal training data. Theoretically, it achieves regret-optimal behavior in stochastic bandits and finite-horizon MDPs, and empirically, it demonstrates near-optimal decisions on unseen tasks, significantly reducing regret compared to prior methods while adapting quickly and remaining robust to distribution shifts.</div>
<div class="mono" style="margin-top:8px">本文针对上下文强化学习（ICRL）难以超越训练分布或需要最优数据的局限性，提出了SPICE这一贝叶斯ICRL方法。SPICE通过深度集成学习Q值的先验，并在测试时利用上下文信息进行贝叶斯更新，结合上置信界规则以增强探索能力，从而从次优训练数据中适应。理论证明，在随机赌博机和有限时域马尔可夫决策过程中，SPICE实现了遗憾最优行为；实验验证表明，它在未见任务上做出接近最优的决策，相比先前ICRL和元强化学习方法显著降低了遗憾，同时快速适应新任务并保持对分布偏移的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal oscillator networks learn to solve a classification problem</div>
<div class="meta-line">Authors: Daan de Bos, Marc Serra-Garcia</div>
<div class="meta-line">First: 2025-02-17T16:54:54+00:00 · Latest: 2026-01-06T13:24:21+00:00</div>
<div class="meta-line">Comments: 14 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.12020v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.12020v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We numerically demonstrate a network of coupled oscillators that can learn to solve a classification task from a set of examples -- performing both training and inference through the nonlinear evolution of the system. We accomplish this by combining three key elements to achieve learning: A long-term memory that stores learned responses, analogous to the synapses in biological brains; a short-term memory that stores the neural activations, similar to the firing patterns of neurons; and an evolution law that updates the synapses in response to novel examples, inspired by synaptic plasticity. Achieving all three elements in wave-based information processors such as metamaterials is a significant challenge. Here, we solve it by leveraging the material multistability to implement long-term memory, and harnessing symmetries and thermal noise to realize the learning rule. Our analysis reveals that the learning mechanism, although inspired by synaptic plasticity, also shares parallelisms with bacterial evolution strategies, where mutation rates increase in the presence of noxious stimuli.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态振荡器网络学习解决分类问题</div>
<div class="mono" style="margin-top:8px">我们通过数值模拟展示了一个耦合振荡器网络，该网络能够从一组示例中学习解决分类任务——通过系统的非线性演化同时完成训练和推理。我们通过结合三个关键要素实现学习：长期记忆（存储学习响应，类似于生物大脑中的突触）、短期记忆（存储神经激活状态，类似于神经元的放电模式），以及受突触可塑性启发的演化规律（根据新示例更新突触）。在超材料等基于波的信息处理器中同时实现这三个要素是一项重大挑战。本文通过利用材料多稳态性实现长期记忆，并借助对称性和热噪声实现学习规则，成功解决了这一难题。我们的分析表明，该学习机制虽受突触可塑性启发，但也与细菌进化策略存在相似性——后者在有害刺激下会提高突变率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the challenge of implementing learning capabilities in wave-based physical systems like metamaterials, which lack conventional computing elements. The method introduces a network of coupled oscillators that combines long-term memory (analogous to synapses), short-term memory (for neural activations), and an evolution law inspired by synaptic plasticity to update connections in response to examples. Experimental results from numerical simulations demonstrate that the system successfully learns and performs classification tasks through its nonlinear dynamics, with the learning mechanism leveraging material multistability for memory and symmetries with thermal noise to realize the update rule, showing parallels to bacterial evolution strategies.</div>
<div class="mono" style="margin-top:8px">本文的动机源于在基于波的物理系统（如超材料）中实现学习能力的挑战，这些系统缺乏传统计算元件。方法提出了一种耦合振荡器网络，结合了长期记忆（类似于突触）、短期记忆（用于神经激活）以及受突触可塑性启发的演化定律，以根据示例更新连接。数值模拟的实验结果表明，该系统通过非线性动力学成功学习并执行分类任务，其学习机制利用材料多稳态实现记忆，并借助对称性和热噪声来实现更新规则，显示出与细菌进化策略的相似性。</div>
</details>
</div>
<div class="card">
<div class="title">Reconsidering Overthinking: Penalizing Internal and External Redundancy in CoT Reasoning</div>
<div class="meta-line">Authors: Jialiang Hong, Taihang Zhen, Kai Chen, Jiaheng Liu, Junlan Feng, Wenpeng Zhu, Jing Huo, Yang Gao, Depeng Wang, Haitao Wan, Xi Yang, Boyan Wang, Fanyu Meng, Yuyao Zhang</div>
<div class="meta-line">First: 2025-08-04T08:22:14+00:00 · Latest: 2026-01-06T13:18:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.02178v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.02178v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) often suffer from overthinking, generating verbose reasoning traces that compromise both computational efficiency and interpretability. Unlike prior efforts that rely on global length-based rewards, we propose a semantic-aware decomposition of redundancy into two distinct forms: internal redundancy (informational stagnation within the reasoning process) and external redundancy (superfluous continuation after the final answer). We introduce a dual-penalty reinforcement learning framework that surgically targets these inefficiencies: a sliding-window semantic analysis is employed to penalize low-gain steps within the reasoning trajectory, while a normalized metric suppresses the post-answer tail. Extensive experiments demonstrate that our method significantly compresses Chain-of-Thought traces with minimal accuracy degradation, while maintaining strong generalization to out-of-domain tasks. Crucially, we reveal an asymmetry in redundancy: external redundancy can be safely eliminated without performance loss, whereas internal redundancy removal requires a calibrated trade-off to maintain reasoning fidelity. Our framework enables fine-grained, implicit control over reasoning length, paving the way for more concise and interpretable LRMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视过度思考：惩罚思维链推理中的内部与外部冗余</div>
<div class="mono" style="margin-top:8px">大型推理模型常因过度思考产生冗长的推理轨迹，损害计算效率与可解释性。区别于以往依赖全局长度奖励的方法，我们提出将冗余语义分解为两种形式：内部冗余（推理过程中的信息停滞）和外部冗余（最终答案后的冗余延续）。我们设计了双惩罚强化学习框架精准针对这些低效环节：通过滑动窗口语义分析惩罚推理轨迹中的低增益步骤，同时采用归一化指标抑制答案后冗余。大量实验表明，该方法能在精度损失最小的情况下显著压缩思维链轨迹，并保持对领域外任务的强泛化能力。关键发现是冗余存在不对称性：外部冗余可安全消除而不影响性能，而内部冗余的消除需权衡校准以保持推理保真度。该框架实现了对推理长度的细粒度隐式控制，为构建更简洁可解释的大型推理模型开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of overthinking in Large Reasoning Models, where verbose reasoning traces reduce efficiency and interpretability. The authors propose a semantic-aware decomposition of redundancy into internal redundancy, which is informational stagnation within the reasoning process, and external redundancy, which involves superfluous continuation after the final answer. Their method introduces a dual-penalty reinforcement learning framework that uses sliding-window semantic analysis to penalize low-gain reasoning steps and a normalized metric to suppress post-answer tails. Experimental results show that this approach significantly compresses Chain-of-Thought traces with minimal accuracy loss, maintains strong generalization to out-of-domain tasks, and reveals an asymmetry: external redundancy can be safely eliminated without performance degradation, while internal redundancy removal requires a calibrated trade-off to preserve reasoning fidelity.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型中存在的过度思考问题展开研究，该问题导致推理轨迹冗长，降低了计算效率和可解释性。作者提出了一种语义感知的冗余分解方法，将冗余分为内部冗余（即推理过程中的信息停滞）和外部冗余（即最终答案后的多余延续）。其方法采用双惩罚强化学习框架，通过滑动窗口语义分析惩罚低增益推理步骤，并使用归一化指标抑制答案后的冗余部分。实验结果表明，该方法能显著压缩思维链轨迹且精度损失极小，在跨域任务上保持强泛化能力，并揭示了一种不对称性：外部冗余可安全消除而不影响性能，而内部冗余的去除则需权衡以保持推理保真度。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning</div>
<div class="meta-line">Authors: Yuankun Xie, Xiaoxuan Guo, Jiayi Zhou, Tao Wang, Jian Liu, Ruibo Fu, Xiaopeng Wang, Haonan Cheng, Long Ye</div>
<div class="meta-line">First: 2026-01-06T12:50:02+00:00 · Latest: 2026-01-06T12:50:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02983v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于音频大语言模型与频时强化学习的可解释全类型音频深度伪造检测</div>
<div class="mono" style="margin-top:8px">音频大语言模型（ALLM）的进展使高质量合成音频更易获取，增加了语音、环境声、歌声及音乐等恶意音频伪造的风险。现实场景的音频伪造检测（ADD）需能泛化至异构音频并提供可解释决策的全类型检测器。基于ALLM强大的多任务泛化能力，本研究首先探究其在监督微调（SFT）与强化微调（RFT）下对全类型ADD的性能。然而，仅使用二元真伪标签的SFT易使模型退化为黑盒分类器，丧失可解释性；而稀疏监督下的原始RFT易出现奖励破解，产生虚假无依据的决策依据。为此，我们提出自动标注与优化流程，构建约34万条频时结构化思维链（CoT）依据作为冷启动示范数据。基于CoT数据，进一步提出频时分组相对策略优化（FT-GRPO）——通过SFT冷启动ALLM，再在规则化频时约束下实施GRPO的两阶段训练范式。实验表明，FT-GRPO在全类型ADD任务中达到最优性能，同时生成可解释且基于频时结构的决策依据。相关数据与代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing accessibility of high-quality synthetic audio and the need for detectors that generalize across diverse audio types while offering interpretable decisions, this paper proposes a novel method for all-type audio deepfake detection. The method introduces an automatic pipeline to construct Frequency-Time structured chain-of-thought rationales, generating approximately 340K demonstrations, and employs a two-stage training paradigm called Frequency Time-Group Relative Policy Optimization (FT-GRPO), which combines supervised fine-tuning with reinforcement fine-tuning under frequency-time constraints. Experimental results show that FT-GRPO achieves state-of-the-art performance on all-type audio deepfake detection tasks while producing interpretable rationales grounded in frequency-time analysis.</div>
<div class="mono" style="margin-top:8px">本文的动机源于高质量合成音频日益普及，需要能够泛化于多种音频类型并提供可解释决策的检测器。方法上，提出了一种自动构建频率-时间结构思维链推理的流程，生成约34万条初始演示数据，并采用名为频率时间-组相对策略优化的两阶段训练范式，结合监督微调和在频率-时间约束下的强化微调。主要实验结果表明，该方法在全类型音频深度伪造检测中达到了最先进的性能，同时产生了基于频率-时间分析的可解释推理。</div>
</details>
</div>
<div class="card">
<div class="title">MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics</div>
<div class="meta-line">Authors: Zhuofan Shi, Hubao A, Yufei Shao, Mengyan Dai, Yadong Yu, Pan Xiang, Dongliang Huang, Hongxu An, Chunxiao Xin, Haiyang Shen, Zhenyu Wang, Yunshan Na, Gang Huang, Xiang Jing</div>
<div class="meta-line">First: 2026-01-05T12:56:51+00:00 · Latest: 2026-01-06T12:33:09+00:00</div>
<div class="meta-line">Comments: 24 pages,4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02075v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02075v2">PDF</a> · <a href="https://github.com/FredericVAN/PKU_MDAgent2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&amp;A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MDAgent2：面向分子动力学代码生成与知识问答的大语言模型</div>
<div class="mono" style="margin-top:8px">分子动力学模拟在材料科学原子尺度行为研究中至关重要，但编写LAMMPS脚本仍属高度专业化且耗时的任务。尽管大语言模型在代码生成和领域问答中展现出潜力，其在分子动力学场景的性能受限于领域数据稀缺、前沿大模型部署成本高昂及代码可执行率低等问题。基于前期MDAgent工作，本文提出首个能在分子动力学领域同时执行知识问答与代码生成的端到端框架MDAgent2。我们构建了领域专用的数据生成流程，产出涵盖分子动力学知识、问答与代码生成的三类高质量数据集。基于这些数据集，采用三阶段后训练策略——持续预训练、监督微调与强化学习——训练出两个领域适配模型MD-Instruct与MD-Code。进一步提出MD-GRPO强化学习方法，以模拟结果为奖励信号并循环利用低奖励轨迹实现持续优化。同时构建可部署的多智能体系统MDAgent2-RUNTIME，集成代码生成、执行、评估与自修正功能。结合本文提出的首个LAMMPS代码生成与问答基准MD-EvalBench，我们的模型与系统在多项指标上超越现有基线。本工作系统论证了大语言模型在工业仿真任务中的适应性与泛化能力，为AI for Science及工业级仿真的自动化代码生成奠定了方法论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for MDAgent2 stems from the specialized and time-consuming nature of writing LAMMPS scripts for molecular dynamics (MD) simulations, coupled with the limitations of existing large language models (LLMs) in this domain due to scarce data, high deployment costs, and low code executability. The method involves constructing high-quality MD-specific datasets and employing a three-stage post-training strategy—continued pre-training, supervised fine-tuning, and reinforcement learning—to train domain-adapted models, MD-Instruct and MD-Code, alongside introducing MD-GRPO, a closed-loop reinforcement learning method that uses simulation outcomes as rewards. Main experimental results, evaluated on the proposed MD-EvalBench benchmark, show that the models and integrated multi-agent system, MDAgent2-RUNTIME, achieve performance surpassing several strong baselines in both knowledge question answering and code generation, demonstrating the adaptability of LLMs to industrial simulation tasks.</div>
<div class="mono" style="margin-top:8px">MDAgent2的研发动机源于分子动力学模拟中编写LAMMPS脚本的专业性和耗时性，以及现有大语言模型在该领域因数据稀缺、部署成本高和代码可执行性低而受限的问题。方法上，通过构建高质量的分子动力学领域专用数据集，并采用持续预训练、监督微调和强化学习三阶段后训练策略来训练领域适应模型MD-Instruct和MD-Code，同时引入了MD-GRPO，这是一种利用模拟结果作为奖励信号的闭环强化学习方法。主要实验结果基于提出的MD-EvalBench基准测试表明，该模型及集成的多智能体系统MDAgent2-RUNTIME在知识问答和代码生成任务上的性能超越了多个强基线，证明了大语言模型在工业模拟任务中的适应性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Learning mirror maps in policy mirror descent</div>
<div class="meta-line">Authors: Carlo Alfano, Sebastian Towers, Silvia Sapora, Chris Lu, Patrick Rebeschini</div>
<div class="meta-line">First: 2024-02-07T19:01:06+00:00 · Latest: 2026-01-06T12:33:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.05187v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.05187v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD&#x27;s full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD&#x27;s efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. Using evolutionary strategies, we identify more efficient mirror maps that enhance the performance of PMD. We first focus on a tabular environment, i.e. Grid-World, where we relate existing theoretical bounds with the performance of PMD for a few standard mirror maps and the learned one. We then show that it is possible to learn a mirror map that outperforms the negative entropy in more complex environments, such as the MinAtar suite. Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略镜像下降中学习镜像映射</div>
<div class="mono" style="margin-top:8px">策略镜像下降（PMD）是强化学习中一个广受欢迎的框架，它作为一个统一视角涵盖了众多算法。这些算法通过选择镜像映射推导而来，并享有有限时间收敛保证。尽管PMD广受欢迎，但其全部潜力的探索仍有限，大多数研究聚焦于特定镜像映射——即负熵，这催生了著名的自然策略梯度（NPG）方法。现有理论研究尚不确定镜像映射的选择是否显著影响PMD的效能。在我们的工作中，我们通过实证研究表明，传统镜像映射选择（NPG）在多个标准基准环境中常产生次优结果。利用进化策略，我们发现了能提升PMD性能的更高效镜像映射。我们首先关注表格环境（如Grid-World），将现有理论界限与PMD在几种标准镜像映射及学习所得映射下的性能相关联。随后我们证明，在更复杂环境（如MinAtar套件）中，可以学习到优于负熵的镜像映射。此外，通过在不同环境中测试各映射，我们证明了所学镜像映射能有效泛化至不同任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the observation that Policy Mirror Descent (PMD), a broad reinforcement learning framework, is predominantly studied with a single mirror map (negative entropy, leading to Natural Policy Gradient), leaving its full potential unexplored and the impact of mirror map choice unclear. The method employs evolutionary strategies to automatically discover more effective mirror maps beyond the conventional choice. Key experimental results demonstrate that the standard negative entropy map often yields suboptimal performance in benchmarks like Grid-World and the MinAtar suite, whereas the learned mirror maps significantly enhance PMD&#x27;s performance and show effective generalization across different tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，策略镜像下降（PMD）这一广泛的强化学习框架主要使用单一镜像映射（负熵，对应自然策略梯度），其潜力未被充分探索，且映射选择的影响尚不明确。方法上采用进化策略自动学习比传统映射更有效的镜像映射。主要实验结果表明，在Grid-World和MinAtar等基准环境中，标准的负熵映射常表现不佳，而学习到的镜像映射能显著提升PMD性能，并在不同任务中展现出良好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning</div>
<div class="meta-line">Authors: Nathanaël Carraz Rakotonirina, Ren Pang, Neha Anna John, Michael Bohlke-Schneider, Momchil Hardalov</div>
<div class="meta-line">First: 2026-01-06T12:31:51+00:00 · Latest: 2026-01-06T12:31:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02972v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02972v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking&#x27;&#x27;. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\% for 8B models and 40\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\text{AUC}_{\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正确、简洁且完整：面向自适应推理的多阶段训练方法</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）的推理能力通过增加测试时计算量（通常以中间标记的形式，即思维链）已显著提升。然而，思维链常变得冗长，增加了计算成本却未提升准确率，甚至可能降低性能，这种现象称为“过度思考”。我们提出一种多阶段高效推理方法，结合监督微调（通过拒绝采样或推理轨迹重构）与采用自适应长度惩罚的强化学习。我们引入一种轻量级奖励函数，惩罚首次正确答案后生成的标记，但仅在有益时鼓励自我验证。我们在七项多样化推理任务中进行全面评估，分析准确率与响应长度的权衡关系。该方法使8B模型的响应长度平均减少28%，32B模型减少40%，而性能仅分别轻微下降1.6和2.5个点。尽管概念简洁，相比更复杂的先进高效推理方法，其在过度思考调整准确率曲线下面积（$\text{AUC}_{\text{OAA}}$）指标上获得76.6分——较基线模型高5分，较次优方法高2.5分，实现了更优的权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of chain-of-thought reasoning in large language models, which often leads to unnecessary computational cost and performance degradation from overthinking, this paper proposes a multi-stage training method to achieve adaptive reasoning. The method combines supervised fine-tuning, using techniques like rejection sampling, with reinforcement learning that incorporates an adaptive length penalty and a lightweight reward function designed to penalize redundant tokens after a correct answer while encouraging beneficial self-verification. Experimental results across seven reasoning tasks show that this approach significantly reduces response length by 28% for 8B models and 40% for 32B models with only minor accuracy drops of 1.6 and 2.5 points, respectively, and achieves a superior trade-off on the Overthinking-Adjusted Accuracy curve compared to other state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">针对大语言模型中思维链推理效率低下、常导致不必要的计算成本和因过度思考引发的性能下降问题，本文提出了一种多阶段训练方法以实现自适应推理。该方法结合了使用拒绝采样等技术的有监督微调，以及融入自适应长度惩罚的强化学习，其轻量级奖励函数旨在惩罚正确答案后的冗余标记，同时鼓励有益的自我验证。在七个推理任务上的实验结果表明，该方法将8B模型的响应长度平均减少28%，32B模型减少40%，而准确率仅分别小幅下降1.6和2.5个点，并在过度思考调整准确率曲线上实现了优于其他先进方法的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models</div>
<div class="meta-line">Authors: Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu</div>
<div class="meta-line">First: 2026-01-06T11:54:47+00:00 · Latest: 2026-01-06T11:54:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing large audio-language models perceive the world as &quot;mono&quot; -- a single stream of audio that ignores the critical spatial dimension (&quot;where&quot;) required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model&#x27;s capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from &quot;mono&quot; semantic recognition to spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界并非单声道：在大型音频语言模型中实现空间理解</div>
<div class="mono" style="margin-top:8px">现有大型音频语言模型将世界感知为“单声道”——即忽略通用声学场景分析所需关键空间维度（“何处”）的单一音频流。为弥合这一差距，我们首先引入听觉场景分析（ASA）的分层框架。在此框架指导下，我们提出一个系统，使Qwen2-Audio等模型能够理解并推理复杂的声学世界。该框架通过三项核心贡献实现目标：首先，构建大规模合成双耳音频数据集以提供丰富空间线索；其次，设计混合特征投影器，利用并行语义编码器与空间编码器提取解耦表征，通过密集融合机制整合这些独立信息流，确保模型获得声学场景的整体视图；最后，采用渐进式训练策略，从监督微调（SFT）推进至基于群体相对策略优化（GRPO）的强化学习，显式提升模型的推理能力。在综合基准测试中，该模型展现出较强的空间理解能力。通过实现空间感知，本研究为利用大模型强大推理能力进行整体声学场景分析提供了清晰路径，推动从“单声道”语义识别向空间智能的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of existing large audio-language models that process audio as a single &#x27;mono&#x27; stream and lack spatial awareness, this paper introduces a hierarchical framework for Auditory Scene Analysis (ASA) to incorporate spatial understanding. The method involves creating a large-scale synthesized binaural audio dataset, designing a hybrid feature projector with parallel semantic and spatial encoders for decoupled representations fused via dense fusion, and employing a progressive training curriculum from supervised fine-tuning to reinforcement learning with GRPO. Experimental results on a comprehensive benchmark show the model achieves strong spatial understanding capabilities, enabling a shift from mere semantic recognition to holistic spatial intelligence in acoustic scene analysis.</div>
<div class="mono" style="margin-top:8px">现有大型音频-语言模型将音频视为单一“单声道”流，缺乏空间感知能力，为此本文引入了听觉场景分析（ASA）的分层框架以融入空间理解。方法包括构建大规模合成双耳音频数据集，设计具有并行语义和空间编码器的混合特征投影器以获取解耦表征并通过密集融合机制整合，以及采用从监督微调到基于GRPO的强化学习的渐进式训练课程。在综合基准测试上的实验结果表明，该模型展现出较强的空间理解能力，推动了从单纯语义识别到整体空间智能的听觉场景分析进步。</div>
</details>
</div>
<div class="card">
<div class="title">Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error</div>
<div class="meta-line">Authors: Chenming Tang, Hsiu-Yuan Huang, Weijie Liu, Clive Bai, Saiyong Yang, Yunfang Wu</div>
<div class="meta-line">First: 2025-10-30T03:36:19+00:00 · Latest: 2026-01-06T11:33:27+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26109v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.26109v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has significantly boosted the reasoning capability of language models (LMs) recently. However, existing RLVR approaches merely train LMs based on their own generated on-policy responses and are constrained by the initial capability of LMs, thus prone to exploration stagnation, in which LMs fail to solve more training problems and cannot further learn from the training data. Some work tries to address this by leveraging off-policy solutions to training problems, but relies on external expert guidance that is limited in availability and scalability. In this work, we propose LTE (Learning to reason from Trial and Error), an approach that hints LMs with their previously self-made mistakes, not requiring any external expert guidance. Experiments validate the effectiveness of LTE, which outperforms the normal group relative policy optimization (GRPO) by 5.02 in Pass@1 and 9.96 in Pass@k on average across six mathematical reasoning benchmarks for Qwen3-8B-Base and even performs better than methods that require external gold solutions as guidance after aligning the experimental setup. Further analysis confirms that LTE successfully mitigates exploration stagnation and enhances both exploitation and exploration during training. Our code is available at https://anonymous.4open.science/r/Learning-from-Trial-and-Error.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不可两次踏入同一条河流：从试错中学习推理</div>
<div class="mono" style="margin-top:8px">近期，基于可验证奖励的强化学习（RLVR）显著提升了语言模型（LM）的推理能力。然而，现有RLVR方法仅基于LM自身生成的同策略响应进行训练，受限于LM的初始能力，容易陷入探索停滞，即LM无法解决更多训练问题，也无法从训练数据中进一步学习。部分研究尝试通过利用训练问题的异策略解决方案来应对此问题，但依赖外部专家指导，其可用性和可扩展性有限。本文提出LTE（从试错中学习推理），该方法通过提示LM其先前自身所犯错误来引导学习，无需任何外部专家指导。实验验证了LTE的有效性：在六个数学推理基准测试中，Qwen3-8B-Base模型使用LTE相比常规组相对策略优化（GRPO）在Pass@1和Pass@k上平均分别提升5.02和9.96；在统一实验设置后，其表现甚至优于需要外部黄金解指导的方法。进一步分析证实，LTE成功缓解了探索停滞，并增强了训练过程中的利用与探索能力。代码发布于https://anonymous.4open.science/r/Learning-from-Trial-and-Error。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of exploration stagnation in reinforcement learning with verifiable rewards (RLVR) for language models, where models are limited by their own on-policy responses and initial capabilities, hindering further learning. The authors propose LTE (Learning to reason from Trial and Error), a method that enables models to learn from their own past mistakes without relying on external expert guidance. Experimental results across six mathematical reasoning benchmarks show that LTE outperforms standard group relative policy optimization (GRPO) by significant margins in Pass@1 and Pass@k metrics for the Qwen3-8B-Base model, even surpassing methods that use external gold solutions, while analysis confirms it mitigates stagnation and enhances both exploitation and exploration during training.</div>
<div class="mono" style="margin-top:8px">本文针对语言模型在可验证奖励强化学习中的探索停滞问题，即模型受限于自身策略内响应和初始能力而无法进一步学习，提出了一种名为LTE（从试错中学习推理）的方法，该方法通过提示模型关注自身先前错误进行学习，无需依赖外部专家指导。在六个数学推理基准测试中，实验结果表明LTE在Qwen3-8B-Base模型上的Pass@1和Pass@k指标均显著优于标准组相对策略优化方法，甚至在实验设置对齐后超越了需要外部黄金解指导的方法，进一步分析证实LTE有效缓解了探索停滞并增强了训练中的利用与探索能力。</div>
</details>
</div>
<div class="card">
<div class="title">ChemBART: A Pre-trained BART Model Assisting Organic Chemistry Analysis</div>
<div class="meta-line">Authors: Kenan Li, Yijian Zhang, Jin Wang, Haipeng Gan, Zeying Sun, Xiaoguang Lei, Hao Dong</div>
<div class="meta-line">First: 2026-01-06T10:55:38+00:00 · Latest: 2026-01-06T10:55:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02915v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02915v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated transformative potential across diverse fields. While LLMs have been applied to molecular simplified molecular input line entry system (SMILES) in computer-aided synthesis planning (CASP), existing methodologies typically address single tasks, such as precursor prediction. We introduce ChemBART, a SMILES-based LLM pre-trained on chemical reactions, which enables a unified model for multiple downstream chemical tasks--achieving the paradigm of &quot;one model, one pre-training, multiple tasks.&quot; By leveraging outputs from a mask-filling pre-training task on reaction expressions, ChemBART effectively solves a variety of chemical problems, including precursor/reagent generation, temperature-yield regression, molecular property classification, and optimizing the policy and value functions within a reinforcement learning framework, integrated with Monte Carlo tree search for multi-step synthesis route design. Unlike single-molecule pre-trained LLMs constrained to specific applications, ChemBART addresses broader chemical challenges and integrates them for comprehensive synthesis planning. Crucially, ChemBART-designed multi-step synthesis routes and reaction conditions directly inspired wet-lab validation, which confirmed shorter pathways with ~30% yield improvement over literature benchmarks. Our work validates the power of reaction-focused pre-training and showcases the broad utility of ChemBART in advancing the complete synthesis planning cycle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ChemBART：一种辅助有机化学分析的预训练BART模型</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型（LLMs）在各领域展现出变革性潜力。尽管LLMs已应用于计算机辅助合成规划（CASP）中的分子简化分子线性输入规范（SMILES），现有方法通常仅针对单一任务（如前体预测）。本文提出ChemBART——一种基于SMILES、通过化学反应预训练的LLM，实现了“一个模型、一次预训练、多项任务”的统一范式。该模型利用掩码填充预训练任务对反应表达式的输出，有效解决了多种化学问题，包括前体/试剂生成、温度-产率回归、分子性质分类，以及结合蒙特卡洛树搜索的多步合成路线设计中强化学习框架的策略与价值函数优化。与局限于特定应用的单分子预训练LLMs不同，ChemBART能应对更广泛的化学挑战，并将其整合为综合性合成规划。关键的是，ChemBART设计的多步合成路线与反应条件直接启发了湿实验验证，实验证实其路径更短，产率较文献基准提升约30%。本研究验证了以反应为核心的预训练效能，并展示了ChemBART在推进完整合成规划周期中的广泛实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for a unified model that can handle multiple chemical tasks beyond single-task approaches like precursor prediction, this paper introduces ChemBART, a SMILES-based large language model pre-trained on chemical reactions. The method leverages a mask-filling pre-training task on reaction expressions to enable a single model for diverse downstream applications, including precursor/reagent generation, temperature-yield regression, molecular property classification, and reinforcement learning-integrated multi-step synthesis route design. Experimental results show that ChemBART-designed synthesis routes, validated through wet-lab experiments, achieved shorter pathways with approximately 30% yield improvement over literature benchmarks, demonstrating its broad utility in advancing comprehensive synthesis planning.</div>
<div class="mono" style="margin-top:8px">本文的动机是开发一个统一模型以处理多种化学任务，超越现有仅针对单一任务（如前体预测）的方法，为此提出了ChemBART，这是一种基于SMILES的预训练大语言模型，专注于化学反应。该方法利用反应表达式的掩码填充预训练任务，使单个模型能够应用于多种下游任务，包括前体/试剂生成、温度-产率回归、分子性质分类以及与强化学习结合的多步合成路线设计。实验结果表明，通过湿实验验证的ChemBART设计合成路线实现了更短的路径，产率较文献基准提升约30%，证明了其在推进完整合成规划周期中的广泛实用性。</div>
</details>
</div>
<div class="card">
<div class="title">SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection</div>
<div class="meta-line">Authors: Zhiyong Cao, Dunqiang Liu, Qi Dai, Haojun Xu, Huaiyan Xu, Huan He, Yafei Liu, Siyuan Liu, XiaoLin Lin, Ke Ma, Ruqian Shi, Sijia Yao, Hao Wang, Sicheng Zhou</div>
<div class="meta-line">First: 2026-01-06T10:00:15+00:00 · Latest: 2026-01-06T10:00:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02871v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02871v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimRPD：基于模拟器的数据评估与选择优化招聘主动对话智能体</div>
<div class="mono" style="margin-top:8px">面向任务的主动对话智能体在招聘场景中具有关键作用，尤其在引导对话实现特定业务目标（如获取社交媒体联系人以进行私域转化）方面。尽管监督微调和强化学习已被证明对此类智能体训练有效，但其性能受限于高质量、目标导向的领域特定训练数据的稀缺性。为解决这一挑战，我们提出SimRPD——一个三阶段框架用于训练招聘主动对话智能体。首先，我们开发高保真用户模拟器，通过多轮在线对话合成大规模对话数据；随后引入基于意图链的多维评估框架，结合全局级与实例级指标，全面评估模拟器并有效筛选高质量数据；最终基于筛选数据集训练招聘主动对话智能体。真实招聘场景实验表明，SimRPD优于现有基于模拟器的数据选择策略，凸显了其在工业部署中的实用价值及面向其他商业对话场景的潜在适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training effective recruitment proactive dialogue agents, which are hindered by the scarcity of high-quality, domain-specific training data. To overcome this, the authors propose SimRPD, a three-stage framework that first uses a high-fidelity user simulator to generate large-scale conversational data, then applies a multi-dimensional evaluation based on Chain-of-Intention to select high-quality data through global and instance-level metrics, and finally trains the dialogue agent on this curated dataset. Experimental results in a real-world recruitment scenario show that SimRPD outperforms existing simulator-based data selection methods, demonstrating its practical utility for industrial applications and potential extension to other business-oriented dialogue systems.</div>
<div class="mono" style="margin-top:8px">本文针对招聘主动对话智能体训练中高质量领域特定数据稀缺的挑战，提出了SimRPD框架。该方法首先通过高保真用户模拟器生成大规模多轮对话数据，然后基于意图链的多维评估框架，结合全局和实例级指标筛选高质量数据，最后利用精选数据训练对话智能体。在真实招聘场景中的实验表明，SimRPD优于现有的基于模拟器的数据选择策略，凸显了其工业部署的实用价值，并具备扩展到其他业务导向对话场景的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Sample-Efficient Neurosymbolic Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Celeste Veronese, Daniele Meli, Alessandro Farinelli</div>
<div class="meta-line">First: 2026-01-06T09:28:53+00:00 · Latest: 2026-01-06T09:28:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02850v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02850v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>样本高效的神经符号深度强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是复杂环境中序列决策的成熟框架。然而，当前最先进的深度强化学习（DRL）算法通常需要大量训练数据，且即使在标准基准测试中，也常难以泛化至小规模训练场景之外。我们提出一种神经符号DRL方法，通过整合背景符号知识来提升样本效率，并增强对更具挑战性、未见任务的泛化能力。针对简单领域实例定义的部分策略（其中易于实现高性能）可作为有用先验迁移，以加速在更复杂环境中的学习，避免从头调整DRL参数。为此，部分策略被表示为逻辑规则，并通过两种机制进行在线推理以指导训练过程：（i）在探索期间偏置动作分布，（ii）在利用期间重新缩放Q值。这种神经符号集成在加速收敛的同时增强了可解释性与可信度，尤其在稀疏奖励环境和长规划跨度任务中。我们在网格世界环境的挑战性变体上（包括完全可观测与部分可观测设置）实证验证了该方法，并展示了其优于当前最先进的奖励机制基线的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the sample inefficiency and limited generalization of deep reinforcement learning (DRL) by proposing a neuro-symbolic approach that integrates background symbolic knowledge to accelerate learning. The method transfers partial policies, represented as logical rules from simple tasks, to guide training in more complex settings through action distribution biasing during exploration and Q-value rescaling during exploitation. Experimental results on challenging gridworld variants, including partially observable settings, demonstrate improved performance over a state-of-the-art baseline, with enhanced sample efficiency, convergence, and interpretability in sparse-reward and long-horizon tasks.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习样本效率低和泛化能力有限的问题，提出了一种融合背景符号知识的神经符号方法以加速学习。该方法将简单任务中定义的局部策略表示为逻辑规则，并通过在探索阶段偏置动作分布、在利用阶段重新缩放Q值来指导复杂场景的训练。在具有挑战性的网格世界变体（包括部分可观测设置）上的实验结果表明，相较于先进的基线方法，该方法在稀疏奖励和长规划视野任务中提高了性能，增强了样本效率、收敛速度和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</div>
<div class="meta-line">Authors: Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen</div>
<div class="meta-line">First: 2025-12-22T03:39:43+00:00 · Latest: 2026-01-06T09:08:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19001v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19001v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI&#x27;s adaptive perception with OR&#x27;s structural rigor. To bridge this gap, we propose a novel OR-Guided &quot;Pretrain-then-Reinforce&quot; framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORPR：一种运筹学引导的预训练-强化学习库存管理模型</div>
<div class="mono" style="margin-top:8px">随着人工智能（AI）与运筹学（OR）在复杂库存系统中协同融合的趋势日益增强，一个关键挑战持续存在：如何有效调和AI的自适应感知能力与OR的结构化严谨性。为弥合这一鸿沟，我们提出了一种新颖的OR引导的“预训练-强化”框架。为提供结构化指导，我们构建了仿真增强的OR模型以生成高质量参考决策，隐式捕捉复杂业务约束与管理偏好。利用这些OR衍生的决策作为基础训练标签，我们设计了领域知识融合的深度学习基础模型以建立核心决策能力，继而进行强化学习（RL）微调阶段。本研究的独特之处在于将RL定位为深度对齐机制，使AI智能体能够内化OR的最优性原理，同时利用探索实现通用策略优化，并允许专家针对特定场景（如促销活动）进行适应性指导。通过大量数值实验及京东集团现场部署（辅以双重差分法分析）的验证，本模型显著优于现行工业实践，实现实际业务指标提升：周转天数减少5.27天、现货率提高2.29%，同时持有成本降低29.95%。与当前主流的暴力模型扩展趋势不同，本研究证明：在结构化OR逻辑引导下，轻量化的领域知识融合模型能够实现顶尖性能与强迁移性。该方法为智能供应链管理提供了可扩展且经济高效的范式，凸显了AI与OR深度对齐的重要价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to reconcile the adaptive perception of AI with the structural rigor of OR in complex inventory systems, this paper proposes an OR-guided &quot;Pretrain-then-Reinforce&quot; learning framework. The method first uses a simulation-augmented OR model to generate high-quality reference decisions, which then train a domain-informed deep learning foundation model; this is followed by a reinforcement learning fine-tuning stage that aligns the AI with OR optimality while allowing for exploration and expert adaptation. Experimental results from numerical tests and a field deployment at JD.com show the model significantly outperforms existing practices, reducing turnover days by 5.27, increasing in-stock rates by 2.29%, and cutting holding costs by 29.95%, demonstrating that a lightweight, domain-informed model guided by OR logic can achieve state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决复杂库存管理中人工智能（AI）的自适应感知与运筹学（OR）的结构严谨性难以有效协同的挑战，提出了一个OR引导的“预训练后强化”学习框架。方法上，首先通过仿真增强的OR模型生成高质量参考决策，用于训练一个领域知识驱动的深度学习基础模型，随后进行强化学习微调，使AI内化OR的最优原则并允许探索和专家针对特定场景（如促销活动）的调整。在大量数值实验和京东的实地部署（辅以双重差分分析）中验证，该模型显著优于现有工业实践，实现了周转天数减少5.27天、现货率提升2.29%以及持有成本降低29.95%的实际收益，表明轻量级、领域感知的模型在OR逻辑指导下能获得先进性能并具备强可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">MiMo-V2-Flash Technical Report</div>
<div class="meta-line">Authors: Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, Gang Xie, Hailin Zhang, Hanglong Lv, Hanyu Li, Heyu Chen, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Lei Li, Liang Zhao, Linghao Zhang, Peidian Li, Qianli Chen, Shaohui Liu, Shihua Yu, Shijie Cao, Shimao Chen, Shouqiu Yu, Shuo Liu, Tianling Zhou, Weijiang Su, Weikun Wang, Wenhan Ma, Xiangwei Deng, Bohan Mao, Bowen Ye, Can Cai, Chenghua Wang, Chengxuan Zhu, Chong Ma, Chun Chen, Chunan Li, Dawei Zhu, Deshan Xiao, Dong Zhang, Duo Zhang, Fangyue Liu, Feiyu Yang, Fengyuan Shi, Guoan Wang, Hao Tian, Hao Wu, Heng Qu, Hongfei Yi, Hongxu An, Hongyi Guan, Xing Zhang, Yifan Song, Yihan Yan, Yihao Zhao, Yingchun Lai, Yizhao Gao, Yu Cheng, Yuanyuan Tian, Yudong Wang, Zhen Tang, Zhengju Tang, Zhengtao Wen, Zhichao Song, Zhixian Zheng, Zihan Jiang, Jian Wen, Jiarui Sun, Jiawei Li, Jinlong Xue, Jun Xia, Kai Fang, Menghang Zhu, Nuo Chen, Qian Tu, Qihao Zhang, Qiying Wang, Rang Li, Rui Ma, Shaolei Zhang, Shengfan Wang, Shicheng Li, Shuhao Gu, Shuhuai Ren, Sirui Deng, Tao Guo, Tianyang Lu, Weiji Zhuang, Weikang Zhang, Weimin Xiong, Wenshan Huang, Wenyu Yang, Xin Zhang, Xing Yong, Xu Wang, Xueyang Xie, Yilin Jiang, Yixin Yang, Yongzhe He, Yu Tu, Yuanliang Dong, Yuchen Liu, Yue Ma, Yue Yu, Yuxing Xiang, Zhaojun Huang, Zhenru Lin, Zhipeng Xu, Zhiyang Chen, Zhonghua Deng, Zihan Zhang, Zihao Yue</div>
<div class="meta-line">First: 2026-01-06T07:31:47+00:00 · Latest: 2026-01-06T07:31:47+00:00</div>
<div class="meta-line">Comments: 31 pages, technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiMo-V2-Flash技术报告</div>
<div class="mono" style="margin-top:8px">我们提出MiMo-V2-Flash，这是一个总参数量为3090亿、激活参数量为150亿的专家混合模型，专为快速、强大的推理与智能体能力而设计。该模型采用混合注意力架构，以5:1的混合比例将滑动窗口注意力与全局注意力交错结合，滑动窗口大小为128个词元。模型通过多词元预测在27万亿词元上进行预训练，使用原生32k上下文长度并后续扩展至256k。为高效扩展训练后计算，MiMo-V2-Flash引入了新颖的多教师同策略蒸馏范式，其中领域专业化教师通过密集的词元级奖励指导学生模型完全掌握其专业知识。尽管总参数量分别仅为DeepSeek-V3.2和Kimi-K2的1/2和1/3，MiMo-V2-Flash仍能媲美这些顶尖开源模型。在推理阶段，通过将多词元预测重用作推测解码的草稿模型，配合三层多词元预测结构可实现最高3.6的接受长度和2.6倍的解码加速。我们开源了模型权重及三层多词元预测权重，以促进开放研究与社区协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient yet powerful large language models, this paper introduces MiMo-V2-Flash, a 309B parameter Mixture-of-Experts model with 15B active parameters, designed for fast reasoning and agentic tasks. The method employs a hybrid attention architecture combining Sliding Window and global attention, pre-trains on 27T tokens with Multi-Token Prediction (MTP), and introduces a novel Multi-Teacher On-Policy Distillation (MTPD) paradigm for efficient post-training scaling. Key experimental results show the model rivals top open-weight models like DeepSeek-V3.2 with far fewer total parameters, and by using MTP as a draft model for speculative decoding, it achieves up to a 3.6 acceptance length and 2.6x decoding speedup.</div>
<div class="mono" style="margin-top:8px">本文旨在开发高效且强大的大语言模型，提出了MiMo-V2-Flash，这是一个总参数量为3090亿、激活参数量为150亿的混合专家模型，专为快速推理和智能体任务设计。其方法采用混合注意力架构，结合滑动窗口与全局注意力，基于2700亿token和多重令牌预测进行预训练，并引入了一种新颖的多教师策略蒸馏范式以实现高效的后训练扩展。主要实验结果表明，该模型在总参数量远少于DeepSeek-V3.2等顶尖开源模型的情况下性能相当，并且通过将多重令牌预测用作推测解码的草稿模型，实现了最高3.6的接受长度和2.6倍的解码加速。</div>
</details>
</div>
<div class="card">
<div class="title">PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism AI Psychological Counselor</div>
<div class="meta-line">Authors: Qianjun Pan, Junyi Wang, Jie Zhou, Yutao Yang, Junsong Li, Kaiyin Xu, Yougen Zhou, Yihan Li, Jingyuan Zhao, Qin Chen, Ningning Zhou, Kai Chen, Liang He</div>
<div class="meta-line">First: 2026-01-05T05:26:57+00:00 · Latest: 2026-01-06T07:31:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01802v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01802v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PsychEval：面向高拟真AI心理咨询师的多会话多疗法基准测试</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估AI，我们推出\texttt{PsychEval}——一个多会话、多疗法、高拟真度的基准测试，旨在解决三大核心挑战：\textbf{1) 能否训练出高拟真AI咨询师？}真实心理咨询是需持续记忆与动态目标追踪的纵向任务。我们构建了跨三个独立阶段（6-10次会话）的多会话基准，要求模型具备记忆连续性、适应性推理与纵向规划等关键能力。数据集标注了涵盖677项元技能与4577项原子技能的完整专业体系。\textbf{2) 如何训练多疗法AI咨询师？}现有模型多聚焦单一疗法，而复杂案例常需跨疗法灵活策略。我们构建了涵盖五大疗法流派（心理动力学、行为主义、认知行为疗法、人本存在主义、后现代主义）的数据集，并基于六大核心心理主题建立了整合三阶段临床框架的整合疗法体系。\textbf{3) 如何系统评估AI咨询师？}我们建立了包含来访者维度与咨询师维度的18项疗法专用/通用指标的评估框架，并构建了2000余个多样化来访者画像支持评估。大量实验分析充分验证了数据集的高质量与临床保真度。\texttt{PsychEval}更突破静态基准测试，可作为高保真强化学习环境，支持具备临床责任意识与适应性的AI咨询师的自主进化训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable AI in psychological assessment, this paper introduces PsychEval, a benchmark designed to train and evaluate highly realistic, multi-therapy AI counselors. The method involves constructing a multi-session dataset spanning 6-10 sessions across three clinical stages, annotated with over 677 meta-skills, and covering five therapeutic modalities within a unified framework to enable adaptive, longitudinal counseling. The main experimental results validate the dataset&#x27;s superior quality and clinical fidelity, and the benchmark serves as a reinforcement learning environment for training self-evolutionary AI counselors, supported by a holistic evaluation framework with 18 metrics and over 2,000 diverse client profiles.</div>
<div class="mono" style="margin-top:8px">为开发可靠的心理评估人工智能，本文提出了PsychEval基准，旨在训练和评估高真实性、多疗法的AI心理咨询师。其方法在于构建一个多会话数据集，涵盖6-10次会话和三个临床阶段，标注了超过677项元技能，并整合了五种治疗模式于统一框架中，以支持适应性、纵向的咨询过程。主要实验结果验证了数据集的高质量和临床保真度，该基准作为一个强化学习环境可用于训练自我进化的AI咨询师，并辅以一个包含18项指标和超过2000个多样化客户档案的整体评估框架。</div>
</details>
</div>
<div class="card">
<div class="title">Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation</div>
<div class="meta-line">Authors: Haoyu Dong, Zhengmao He, Yang Li, Zhibin Li, Xinyu Yi, Zhe Zhao</div>
<div class="meta-line">First: 2026-01-06T07:26:39+00:00 · Latest: 2026-01-06T07:26:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-like dexterous hands with multiple fingers offer human-level manipulation capabilities, but training control policies that can directly deploy on real hardware remains difficult due to contact-rich physics and imperfect actuation. We close this gap with a practical sim-to-real reinforcement learning (RL) framework that utilizes dense tactile feedback combined with joint torque sensing to explicitly regulate physical interactions. To enable effective sim-to-real transfer, we introduce (i) a computationally fast tactile simulation that computes distances between dense virtual tactile units and the object via parallel forward kinematics, providing high-rate, high-resolution touch signals needed by RL; (ii) a current-to-torque calibration that eliminates the need for torque sensors on dexterous hands by mapping motor current to joint torque; and (iii) actuator dynamics modeling to bridge the actuation gaps with randomization of non-ideal effects such as backlash, torque-speed saturation. Using an asymmetric actor-critic PPO pipeline trained entirely in simulation, our policies deploy directly to a five-finger hand. The resulting policies demonstrated two essential skills: (1) command-based, controllable grasp force tracking, and (2) reorientation of objects in the hand, both of which were robustly executed without fine-tuning on the robot. By combining tactile and torque in the observation space with effective sensing/actuation modeling, our system provides a practical solution to achieve reliable dexterous manipulation. To our knowledge, this is the first demonstration of controllable grasping on a multi-finger dexterous hand trained entirely in simulation and transferred zero-shot on real hardware.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弥合现实鸿沟：基于力控的灵巧抓取与操作零样本仿真到真实部署</div>
<div class="mono" style="margin-top:8px">多指仿人灵巧手具备人类水平的操作能力，但由于密集接触物理特性与不完美的驱动机制，训练能直接部署于真实硬件的控制策略仍具挑战。本研究提出一种实用的仿真到真实强化学习框架，通过融合密集触觉反馈与关节力矩传感来显式调控物理交互。为实现有效的仿真到真实迁移，我们引入：（1）基于并行正向运动学的快速触觉仿真，通过计算密集虚拟触觉单元与物体的距离，提供强化学习所需的高频高分辨率触觉信号；（2）电流-力矩标定方法，通过映射电机电流至关节力矩，消除灵巧手对力矩传感器的依赖；（3）驱动器动力学建模，通过随机化背隙、力矩-转速饱和等非理想效应来弥合驱动差异。采用完全在仿真中训练的非对称执行者-评论者PPO框架，所得策略可直接部署于五指灵巧手。实验证明该策略具备两项核心能力：（1）基于指令的可控抓握力跟踪；（2）手内物体重定向，二者均无需在真实机器人上微调即可鲁棒执行。通过将触觉与力矩感知融入观测空间，并结合有效的传感/驱动建模，本系统为实现可靠灵巧操作提供了实用方案。据我们所知，这是首个完全在仿真中训练、并零样本迁移至真实硬件的多指灵巧手可控抓取实证研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of transferring dexterous manipulation policies from simulation to real hardware without fine-tuning, motivated by the difficulty of training contact-rich, force-based control for multi-fingered hands. The method introduces a sim-to-real reinforcement learning framework that integrates dense tactile feedback and joint torque sensing, supported by a fast tactile simulator, a current-to-torque calibration to avoid physical torque sensors, and actuator dynamics modeling with randomization of non-ideal effects. Experimental results on a five-finger hand show that policies trained entirely in simulation achieve zero-shot deployment, successfully demonstrating controllable grasp force tracking and object reorientation robustly in the real world.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决灵巧操作策略从仿真到真实硬件零次迁移的难题，其动机在于多指手在接触密集、基于力的控制训练上存在困难。方法上提出了一个仿真到现实的强化学习框架，整合了密集触觉反馈和关节扭矩感知，并辅以快速触觉模拟器、免物理扭矩传感器的电流-扭矩标定，以及对非理想效应进行随机化的执行器动力学建模。在五指手上的实验结果表明，完全在仿真中训练的策略实现了零次部署，在真实世界中鲁棒地完成了可控抓握力跟踪和物体重定向任务。</div>
</details>
</div>
<div class="card">
<div class="title">DiRL: An Efficient Post-Training Framework for Diffusion Language Models</div>
<div class="meta-line">Authors: Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyang He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu</div>
<div class="meta-line">First: 2025-12-23T08:33:19+00:00 · Latest: 2026-01-06T06:47:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22234v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22234v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiRL：一种高效的扩散语言模型后训练框架</div>
<div class="mono" style="margin-top:8px">扩散语言模型已成为自回归模型的有力替代方案。尽管近期研究验证了其预训练潜力并提升了推理速度，但扩散语言模型的后训练体系仍不成熟。现有方法存在计算效率低下、训练与推理目标不匹配等问题，严重限制了在数学等复杂推理任务上的性能。为此，我们提出DiRL框架，通过紧密整合FlexAttention加速的分块训练与LMDeploy优化的推理架构，构建了高效的在线模型更新循环，支持监督微调与强化学习两阶段后训练。基于此框架，我们提出首个面向扩散语言模型的无偏分组相对策略优化算法DiPO。通过在高质量数学数据上训练DiRL-8B-Instruct模型，实验表明该模型在扩散语言模型中取得最先进的数学性能，并在多项基准测试中超越Qwen2.5系列同规模模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the underdeveloped post-training landscape for Diffusion Language Models (dLLMs), which suffer from computational inefficiency and objective mismatches that limit performance on complex reasoning tasks like mathematics. The method introduces DiRL, an efficient post-training framework that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference to enable a streamlined online update loop for two-stage post-training (Supervised Fine-Tuning and Reinforcement Learning), and proposes DiPO, an unbiased Group Relative Policy Optimization tailored for dLLMs. The main experimental results show that the trained DiRL-8B-Instruct model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于扩散语言模型（dLLMs）的后训练领域尚不成熟，现有方法存在计算效率低下及训练与推理目标不匹配的问题，严重限制了其在数学等复杂推理任务上的性能。方法上提出了DiRL，一个高效的后训练框架，通过集成FlexAttention加速的块状训练与LMDeploy优化的推理，实现了简化的在线模型更新循环，以支持高效的两阶段后训练（监督微调与强化学习），并提出了首个为dLLMs定制的无偏分组相对策略优化方法DiPO。主要实验结果表明，基于高质量数学数据训练的DiRL-8B-Instruct模型在dLLMs中取得了最先进的数学性能，并在多个基准测试中超越了Qwen2.5系列的可比模型。</div>
</details>
</div>
<div class="card">
<div class="title">Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies</div>
<div class="meta-line">Authors: Mingming Zhang, Na Li, Zhuang Feiqing, Hongyang Zheng, Jiangbing Zhou, Wang Wuyin, Sheng-jie Sun, XiaoWei Chen, Junxiong Zhu, Lixin Zou, Chenliang Li</div>
<div class="meta-line">First: 2026-01-06T06:42:25+00:00 · Latest: 2026-01-06T06:42:25+00:00</div>
<div class="meta-line">Comments: 11pages, 5figures, In Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02754v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02754v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning.
  To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Q正则化生成式自动出价：从次优轨迹到最优策略</div>
<div class="mono" style="margin-top:8px">随着电子商务的快速发展，自动出价已成为优化多样化广告主环境下广告效果的关键技术。现有方法主要聚焦于强化学习与生成模型，通过复杂结构和昂贵的超参数调优来模仿离线历史行为，而次优轨迹进一步加剧了策略学习的难度。为应对这些挑战，本文提出QGA——一种新颖的Q值正则化生成式自动出价方法。该方法将基于双重Q学习策略的Q值正则化模块嵌入决策变换器主干网络，实现策略模仿与动作价值最大化的联合优化，使习得的出价策略既能利用数据集经验，又能缓解次优轨迹的负面影响。此外，为安全探索数据分布之外的策略空间，我们提出Q值引导的双重探索机制：通过多目标回报条件约束与局部扰动动作对决策变换器进行调控，整个探索过程由前述Q值模块动态引导，为候选动作提供原则性评估。在公开基准与仿真环境中的实验表明，QGA相较现有方法始终取得更优或极具竞争力的结果。值得注意的是，在大型真实场景A/B测试中，QGA实现了广告交易总额3.27%的提升与广告投资回报率2.49%的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges in auto-bidding where existing reinforcement learning and generative models rely on suboptimal historical data and require costly tuning, this paper proposes QGA, a Q-value regularized generative auto-bidding method. The method integrates a Q-value regularization with double Q-learning into a Decision Transformer backbone to jointly optimize policy imitation and action-value maximization, mitigating the impact of poor trajectories, and employs a Q-guided dual-exploration mechanism for safe policy space exploration. Experimental results on benchmarks and simulations show QGA outperforms alternatives, with real-world A/B testing achieving a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.</div>
<div class="mono" style="margin-top:8px">针对自动出价中现有强化学习和生成模型依赖次优历史数据且调优成本高的问题，本文提出了QGA，一种Q值正则化的生成式自动出价方法。该方法将Q值正则化与双重Q学习集成到决策变换器主干中，以联合优化策略模仿和动作价值最大化，减轻不良轨迹的影响，并采用Q引导的双重探索机制进行安全的策略空间探索。在基准测试和模拟环境中的实验结果表明，QGA优于现有方法，实际A/B测试实现了广告总交易额提升3.27%和广告投资回报率改善2.49%。</div>
</details>
</div>
<div class="card">
<div class="title">Time-Scaling Is What Agents Need Now</div>
<div class="meta-line">Authors: Zhi Liu, Guangzhi Wang</div>
<div class="meta-line">First: 2026-01-06T05:01:17+00:00 · Latest: 2026-01-06T05:01:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on &quot;perception-representation,&quot; Reinforcement Learning on &quot;decision-making-behavior,&quot; and Symbolic AI on &quot;knowledge-reasoning.&quot; With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop &quot;perception-decision-action&quot; capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for &quot;Time-Scaling&quot;--the systematic extension and optimization of an agent&#x27;s ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时间扩展是智能体当前所需的核心能力</div>
<div class="mono" style="margin-top:8px">早期人工智能范式呈现认知功能分离：神经网络侧重“感知-表征”，强化学习专注“决策-行为”，符号人工智能聚焦“知识-推理”。随着基于Transformer的大模型与世界模型的发展，这些范式正融合为具备闭环“感知-决策-行动”能力的认知智能体。人类通过时序化递进推理在有限认知资源下解决复杂问题，语言则依赖问题空间搜索实现深层语义推理。早期大语言模型虽能生成流畅文本，却缺乏稳健的语义推理能力。思维链、思维树等提示技术通过显式化中间步骤扩展推理路径，而DeepSeek-R1等近期模型通过显式推理轨迹提升性能，但这些方法在搜索完备性与效率上存在局限。这凸显了“时间扩展”的必要性——即系统化扩展与优化智能体随时间展开推理的能力。时间扩展指利用延伸时序路径的架构设计，实现更深层问题空间探索、动态策略调整与强化元认知控制，模拟人类在认知约束下的序列推理机制。它代表着在不线性增加静态模型参数的前提下增强深度推理与问题解决能力的关键前沿。推进智能体能力需将时间扩展原则置于核心，确立显式时序推理管理的基础性地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the convergence of previously separate AI paradigms into cognitive agents with closed-loop perception-decision-action capabilities, highlighting that while prompting techniques have improved reasoning, they remain limited in search completeness and efficiency. It proposes &#x27;Time-Scaling&#x27; as a method to systematically extend an agent&#x27;s ability to unfold reasoning over time through architectural designs with extended temporal pathways, enabling deeper exploration and dynamic strategy adjustment. The main experimental insight, drawn from analyzing recent models like DeepSeek-R1, is that this approach enhances deep reasoning and problem-solving without requiring proportional increases in static model parameters, positioning explicit temporal reasoning management as a critical frontier for advancing intelligent agents.</div>
<div class="mono" style="margin-top:8px">本文的动机源于早期分离的人工智能范式（如神经网络、强化学习和符号AI）正汇聚成具有感知-决策-行动闭环能力的认知智能体，但现有提示技术在搜索完备性和效率上存在局限。为此，论文提出“时间缩放”方法，即通过利用扩展时间路径的架构设计，系统化提升智能体随时间展开推理的能力，从而实现更深层的问题空间探索和动态策略调整。基于对DeepSeek-R1等近期模型的分析，主要实验结果表明，该方法能在不显著增加静态模型参数的前提下，增强深度推理和问题解决能力，从而将显式时间推理管理确立为推进智能体能力的关键前沿。</div>
</details>
</div>
<div class="card">
<div class="title">TextBO: Bayesian Optimization in Language Space for Eval-Efficient Self-Improving AI</div>
<div class="meta-line">Authors: Enoch Hyunwook Kang, Hema Yoganarasimhan</div>
<div class="meta-line">First: 2025-11-15T07:04:44+00:00 · Latest: 2026-01-06T03:54:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12063v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.12063v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have enabled self-improving AI systems that iteratively generate, evaluate, and refine their outcomes. Recent studies show that prompt-optimization-based self-improvement can outperform state-of-the-art reinforcement-learning fine-tuning of LLMs, but performance is typically measured by generation efficiency. However, in many applications, the constraint is evaluation efficiency: obtaining reliable feedback is far more costly than generating candidates. To optimize for evaluation efficiency, we extend Upper Confidence Bound-Bayesian Optimization (UCB-BO), a framework known for optimal evaluation-efficiency guarantees, to the language domain. Doing so is challenging for two reasons: (i) gradients needed for UCB-BO are ill-defined in discrete prompt space; and (ii) UCB-style exploration relies on a surrogate model and acquisition function, which only live implicitly in the LLM. We overcome these challenges by proving that combining simple textual gradients (LLM-proposed local edits) with the Best-of-N selection strategy statistically emulates ascent along the gradient of the canonical UCB acquisition function. Based on this result, we propose TextBO, a simple, evaluation-efficient self-improving algorithm that operates purely in language space without explicit surrogates or calibrated uncertainty models. We empirically validate TextBO on automated ad-alignment tasks using a persona-induced preference distribution, demonstrating superior performance per evaluation compared to strong baselines such as Best-of-N and GEPA. We also evaluate TextBO&#x27;s Best-of-N multi-step textual-gradient mechanism on agentic AI benchmarks by augmenting GEPA with it and show that it significantly outperforms standard GEPA. In sum, TextBO is a simple and principled framework for AI self-improving system design that bridges prompt optimization with classical Bayesian optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TextBO：面向评估高效自改进AI的语言空间贝叶斯优化</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）催生了能够迭代生成、评估与优化输出的自改进AI系统。近期研究表明，基于提示优化的自改进方法可超越最先进的LLM强化学习微调技术，但现有研究多关注生成效率的评估。然而在实际应用中，评估效率往往成为关键瓶颈：获取可靠反馈的成本远高于生成候选方案。为优化评估效率，本研究将具有最优评估效率保证的理论框架——上置信界贝叶斯优化（UCB-BO）拓展至语言领域。该拓展面临双重挑战：（1）离散提示空间中无法明确定义UCB-BO所需的梯度；（2）UCB式探索依赖的代理模型与采集函数仅隐式存在于LLM中。我们通过理论证明：将简单文本梯度（LLM提出的局部编辑）与N选最优策略相结合，可在统计意义上模拟经典UCB采集函数梯度的上升轨迹。基于此，提出TextBO算法——一种完全在语言空间运行、无需显式代理模型或校准不确定性模型的评估高效自改进算法。在基于人格诱导偏好分布的自动广告对齐任务中，TextBO在单次评估性能上显著超越N选最优、GEPA等基线方法。通过将TextBO的多步文本梯度机制集成至GEPA框架，在智能体基准测试中展现出对标准GEPA的显著性能提升。综上，TextBO为AI自改进系统设计提供了连接提示优化与经典贝叶斯优化的简洁理论框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of evaluation efficiency in self-improving AI systems, where obtaining reliable feedback is costly compared to generating candidates. To optimize for evaluation efficiency, the authors propose TextBO, a method that extends Upper Confidence Bound-Bayesian Optimization (UCB-BO) to the language domain by using simple textual gradients from LLMs and a Best-of-N selection strategy, which statistically emulates gradient ascent on the UCB acquisition function. Experimental results on automated ad-alignment tasks show that TextBO outperforms baselines like Best-of-N and GEPA in performance per evaluation, and when augmented with its multi-step textual-gradient mechanism, it significantly improves upon standard GEPA on agentic AI benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对自改进AI系统中评估效率的挑战，即获取可靠反馈比生成候选方案成本更高。为优化评估效率，作者提出了TextBO方法，通过使用大语言模型生成的简单文本梯度和Best-of-N选择策略，将上置信界贝叶斯优化扩展到语言领域，从而在统计上模拟了UCB采集函数的梯度上升。在自动广告对齐任务上的实验结果表明，TextBO在每次评估的性能上优于Best-of-N和GEPA等基线方法，并且当其多步文本梯度机制应用于GEPA时，在智能体AI基准测试中显著超越了标准GEPA。</div>
</details>
</div>
<div class="card">
<div class="title">SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</div>
<div class="meta-line">Authors: Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun</div>
<div class="meta-line">First: 2025-12-26T14:51:39+00:00 · Latest: 2026-01-06T03:31:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22322v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22322v2">PDF</a> · <a href="https://github.com/TencentYoutuResearch/SmartSnap">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent&#x27;s entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B. Code is available at: https://github.com/TencentYoutuResearch/SmartSnap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SmartSnap：自验证智能体的主动证据搜寻机制</div>
<div class="mono" style="margin-top:8px">智能体强化学习在复杂GUI任务中展现出构建自主智能体的巨大潜力，但其可扩展性始终受限于任务完成验证环节。现有验证方法属于被动的事后处理模式：验证器（如基于规则的评分脚本、奖励/评判模型、LLM-as-a-Judge）通过分析智能体完整交互轨迹判断任务成败。这种处理包含无关噪声信息的冗长上下文方式，给验证机制带来挑战，导致高昂成本与低可靠性。为突破此瓶颈，我们提出SmartSnap范式，将被动事后验证转变为智能体主动的现场自验证。我们设计的新型自验证智能体具备双重使命：既完成任务，又通过精选快照证据证明完成度。遵循我们提出的3C原则（完整性、简洁性、创造性），智能体利用在线环境访问权限，在最小化决定性快照集上执行自验证。这些证据将作为通用LLM-as-a-Judge验证器的唯一判定材料。跨模型系列与规模的移动端任务实验表明，SmartSnap范式能以可扩展方式训练LLM驱动智能体，为80亿和300亿参数模型分别带来26.08%和16.66%的性能提升。解决方案寻找与证据搜寻的协同作用，培育出性能可对标DeepSeek V3.1与Qwen3-235B-A22B的高效自验证智能体。代码已开源：https://github.com/TencentYoutuResearch/SmartSnap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the scalability bottleneck in agentic reinforcement learning caused by inefficient post-hoc task verification, which processes verbose and noisy interaction histories, leading to high costs and low reliability. To overcome this, the authors propose SmartSnap, a paradigm shift to proactive self-verification where agents are designed with dual missions: completing tasks and proving accomplishment by curating minimal, decisive snapshot evidences guided by Completeness, Conciseness, and Creativity principles. Experimental results on mobile tasks show that SmartSnap enables scalable training of LLM-driven agents, achieving performance gains of up to 26.08% and 16.66% for 8B and 30B models respectively, and facilitates competitive performance against larger models like DeepSeek V3.1 and Qwen3-235B-A22B.</div>
<div class="mono" style="margin-top:8px">本文针对智能体强化学习中因低效的事后任务验证导致的扩展性瓶颈问题，该验证需处理冗长嘈杂的交互历史，成本高昂且可靠性低。为此，作者提出SmartSnap范式，将被动验证转变为主动自验证，设计具有双重使命的自验证智能体：完成任务并通过遵循完整性、简洁性和创造性原则，在环境中主动收集最小化、决定性的快照证据来证明完成情况。在移动任务上的实验表明，SmartSnap实现了大语言模型驱动智能体的可扩展训练，使8B和30B模型性能分别提升高达26.08%和16.66%，并与DeepSeek V3.1和Qwen3-235B-A22B等更大模型相比展现出竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?</div>
<div class="meta-line">Authors: Mingyuan Wu, Meitang Li, Jingcheng Yang, Jize Jiang, Kaizhuo Yan, Zhaoheng Li, Hanchao Yu, Minjia Zhang, Klara Nahrstedt</div>
<div class="meta-line">Venue: Neurips 2025 Oral</div>
<div class="meta-line">First: 2025-06-20T18:23:48+00:00 · Latest: 2026-01-06T03:04:42+00:00</div>
<div class="meta-line">Comments: Neurips 2025 Multimodal Algorithmic Reasoning Workshop Oral. In submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.17417v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.17417v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inference time techniques such as decoding time scaling and self refinement have been shown to substantially improve mathematical reasoning in large language models (LLMs), largely attributed to emergent self correction and self verification behaviors often elicited through reinforcement learning (RL). In this work, we ask whether the same recipe transfers to vision language models (VLMs), especially RL finetuned variants that claim strong visual mathematical reasoning.
  Through extensive evaluation, we reach three main findings that differ markedly from text only models. First, generation time capability matters more than verification and refinement: simple majority voting consistently and substantially outperforms verification centric strategies such as best of N with self verification. Second, behaviors often associated with RL tuned models at inference time, such as the &#x27;Aha moment,&#x27; do not yield reliable reasoning performance improvements. Third, visual information is not effectively integrated into the model&#x27;s self verification process.
  Overall, our analysis highlights a key limitation: current RL trained VLMs derive limited benefit from self verification in the visual modality, which constrains the effectiveness of inference time scaling for visual mathematical reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>再探‘顿悟时刻’：视觉语言模型在推理时扩展中是否真正具备自我验证能力？</div>
<div class="mono" style="margin-top:8px">解码时扩展与自我精炼等推理时技术已被证明能显著提升大语言模型的数学推理能力，这主要归因于通过强化学习引发的涌现性自我校正与自我验证行为。本研究探讨该机制是否适用于视觉语言模型，尤其是声称具备强大视觉数学推理能力的强化学习微调变体。通过广泛评估，我们得出与纯文本模型截然不同的三个主要发现：第一，生成时能力比验证与精炼更重要——简单的多数投票法持续且显著优于以自我验证为核心的‘最佳N选一’等策略；第二，推理时常与强化学习调优模型关联的行为（如‘顿悟时刻’）并未带来可靠的推理性能提升；第三，视觉信息未能有效融入模型的自我验证过程。总体而言，我们的分析揭示了一个关键局限：当前基于强化学习训练的视觉语言模型在视觉模态中从自我验证获得的收益有限，这制约了推理时扩展对视觉数学推理的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether inference-time scaling techniques like self-verification, which significantly boost mathematical reasoning in text-only LLMs, are similarly effective for vision-language models (VLMs), particularly those fine-tuned with reinforcement learning. The authors conduct extensive evaluations and find three key divergences: generation-time capability, such as simple majority voting, consistently outperforms verification-centric strategies; behaviors like the &#x27;Aha moment&#x27; do not reliably improve reasoning; and visual information is poorly integrated into self-verification. Overall, the results reveal a major limitation, showing that current RL-trained VLMs gain minimal benefit from self-verification in visual tasks, thereby constraining inference-time scaling for visual mathematical reasoning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了推理时缩放技术（如自我验证）在提升纯文本大语言模型数学推理能力方面的有效性是否同样适用于视觉语言模型，特别是经过强化学习微调的模型。通过广泛评估，作者发现了三个关键差异：生成时能力（如简单多数投票）持续优于以验证为中心的策略；与“顿悟时刻”相关的行为并未可靠提升推理性能；且视觉信息未能有效融入模型的自我验证过程。总体而言，分析突出了一个主要局限：当前经过强化学习训练的视觉语言模型在视觉模态中从自我验证获得的益处有限，这制约了推理时缩放技术在视觉数学推理中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Offline Model-Based Optimization: Comprehensive Review</div>
<div class="meta-line">Authors: Minsu Kim, Jiayao Gu, Ye Yuan, Taeyoung Yun, Zixuan Liu, Yoshua Bengio, Can Chen</div>
<div class="meta-line">First: 2025-03-21T16:35:02+00:00 · Latest: 2026-01-06T02:53:28+00:00</div>
<div class="meta-line">Comments: Accepted to TMLR 2026 (Survey Certification)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.17286v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.17286v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline optimization is a fundamental challenge in science and engineering, where the goal is to optimize black-box functions using only offline datasets. This setting is particularly relevant when querying the objective function is prohibitively expensive or infeasible, with applications spanning protein engineering, material discovery, neural architecture search, and beyond. The main difficulty lies in accurately estimating the objective landscape beyond the available data, where extrapolations are fraught with significant epistemic uncertainty. This uncertainty can lead to objective hacking(reward hacking), exploiting model inaccuracies in unseen regions, or other spurious optimizations that yield misleadingly high performance estimates outside the training distribution. Recent advances in model-based optimization(MBO) have harnessed the generalization capabilities of deep neural networks to develop offline-specific surrogate and generative models. Trained with carefully designed strategies, these models are more robust against out-of-distribution issues, facilitating the discovery of improved designs. Despite its growing impact in accelerating scientific discovery, the field lacks a comprehensive review. To bridge this gap, we present the first thorough review of offline MBO. We begin by formalizing the problem for both single-objective and multi-objective settings and by reviewing recent benchmarks and evaluation metrics. We then categorize existing approaches into two key areas: surrogate modeling, which emphasizes accurate function approximation in out-of-distribution regions, and generative modeling, which explores high-dimensional design spaces to identify high-performing designs. Finally, we examine the key challenges and propose promising directions for advancement in this rapidly evolving field including safe control of superintelligent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线基于模型的优化：全面综述</div>
<div class="mono" style="margin-top:8px">离线优化是科学与工程中的基础性挑战，其目标在于仅利用离线数据集优化黑箱函数。当目标函数的查询成本极高或不可行时，这一设定尤为重要，其应用涵盖蛋白质工程、材料发现、神经架构搜索等领域。主要困难在于准确估计可用数据之外的目标函数形态，其中外推过程充满显著的认知不确定性。这种不确定性可能导致目标函数被操纵（奖励黑客行为）、利用模型在未见区域的误差，或产生其他虚假优化，从而在训练分布之外得出误导性的高性能估计。基于模型的优化（MBO）领域的最新进展利用深度神经网络的泛化能力，开发了专门针对离线场景的代理模型和生成模型。这些模型通过精心设计的策略进行训练，对分布外问题更具鲁棒性，有助于发现改进的设计方案。尽管该领域在加速科学发现方面影响日益增长，但尚缺乏全面综述。为填补这一空白，本文首次对离线MBO进行全面回顾。我们首先形式化了单目标和多目标场景下的问题定义，并回顾了近期基准测试与评估指标。随后将现有方法归纳为两个关键方向：强调在分布外区域实现精确函数逼近的代理建模，以及探索高维设计空间以识别高性能设计的生成建模。最后，我们探讨了该快速演进领域的关键挑战，并提出了包括超智能系统安全控制在内的前沿发展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to optimize black-box functions using only offline datasets, a common challenge in fields like protein engineering and material discovery where direct queries are expensive or infeasible. The method involves a comprehensive review of offline model-based optimization (MBO), categorizing approaches into surrogate modeling for accurate out-of-distribution function approximation and generative modeling for exploring high-dimensional design spaces. The main experimental results highlight that recent advances, particularly using deep neural networks with carefully designed training strategies, have improved robustness against epistemic uncertainty and out-of-distribution issues, facilitating the discovery of better designs while mitigating risks like objective hacking.</div>
<div class="mono" style="margin-top:8px">本文的动机在于仅使用离线数据集优化黑盒函数的需求，这在蛋白质工程和材料发现等领域中很常见，因为直接查询成本高昂或不可行。方法上，该文对离线基于模型的优化进行了全面综述，将现有方法分为两类：强调在分布外区域准确函数逼近的代理建模，以及探索高维设计空间以识别高性能设计的生成建模。主要实验结果指出，近期进展特别是利用深度神经网络结合精心设计的训练策略，已提升了对认知不确定性和分布外问题的鲁棒性，促进了更优设计的发现，同时减轻了目标黑客等风险。</div>
</details>
</div>
<div class="card">
<div class="title">Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks</div>
<div class="meta-line">Authors: Hadi Partovi Aria, Zhe Xu</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-01-06T02:25:26+00:00 · Latest: 2026-01-06T02:25:26+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02666v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于因果图时序逻辑公式推断加速时序扩展任务中的强化学习</div>
<div class="mono" style="margin-top:8px">决策任务常在具有时空动态的图结构上展开。黑盒强化学习常忽略局部变化如何通过网络结构传播，限制了样本效率与可解释性。本文提出GTL-CIRL闭环框架，同步学习策略并挖掘因果图时序逻辑规范。该方法通过鲁棒性塑造奖励函数，在效应失效时收集反例，并采用高斯过程驱动的贝叶斯优化精调参数化因果模板。高斯过程模型捕捉系统动态中的时空相关性，实现对复杂参数空间的高效探索。基因网络与电力网络的案例研究表明，相较于标准强化学习基线，本方法能实现更快的学习速度与更清晰、可验证的行为模式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the sample inefficiency and lack of interpretability in black-box reinforcement learning for tasks with spatial-temporal graph dynamics, where local changes propagate through network structures. To overcome this, the authors propose GTL-CIRL, a closed-loop framework that concurrently learns policies and mines interpretable Causal Graph Temporal Logic (Causal GTL) specifications; it shapes rewards using robustness measures, collects counterexamples when expected effects fail, and refines parameterized cause templates via Gaussian Process-driven Bayesian optimization, which captures spatial and temporal correlations for efficient exploration. Experimental results in gene regulatory and power network case studies demonstrate that the method achieves faster learning and yields clearer, verifiable behavior compared to standard RL baselines.</div>
<div class="mono" style="margin-top:8px">本文针对具有时空图动态的任务中，黑盒强化学习样本效率低且缺乏可解释性的问题，其中局部变化会通过网络结构传播。为此，作者提出了GTL-CIRL这一闭环框架，能同时学习策略并挖掘可解释的因果图时序逻辑规范；该方法通过鲁棒性度量塑造奖励，在预期效应失败时收集反例，并利用高斯过程驱动的贝叶斯优化来细化参数化的原因模板，从而捕获时空相关性以实现高效探索。在基因调控和电力网络的案例研究中，实验结果表明，相较于标准强化学习基线，该方法实现了更快的学习速度，并产生了更清晰、可验证的行为。</div>
</details>
</div>
<div class="card">
<div class="title">Effective Online 3D Bin Packing with Lookahead Parcels Using Monte Carlo Tree Search</div>
<div class="meta-line">Authors: Jiangyi Fang, Bowen Zhou, Haotian Wang, Xin Zhu, Leye Wang</div>
<div class="meta-line">First: 2026-01-06T01:51:11+00:00 · Latest: 2026-01-06T01:51:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02649v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02649v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online 3D Bin Packing (3D-BP) with robotic arms is crucial for reducing transportation and labor costs in modern logistics. While Deep Reinforcement Learning (DRL) has shown strong performance, it often fails to adapt to real-world short-term distribution shifts, which arise as different batches of goods arrive sequentially, causing performance drops. We argue that the short-term lookahead information available in modern logistics systems is key to mitigating this issue, especially during distribution shifts. We formulate online 3D-BP with lookahead parcels as a Model Predictive Control (MPC) problem and adapt the Monte Carlo Tree Search (MCTS) framework to solve it. Our framework employs a dynamic exploration prior that automatically balances a learned RL policy and a robust random policy based on the lookahead characteristics. Additionally, we design an auxiliary reward to penalize long-term spatial waste from individual placements. Extensive experiments on real-world datasets show that our method consistently outperforms state-of-the-art baselines, achieving over 10\% gains under distributional shifts, 4\% average improvement in online deployment, and up to more than 8\% in the best case--demonstrating the effectiveness of our framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于蒙特卡洛树搜索的带前瞻包裹在线三维装箱优化方法</div>
<div class="mono" style="margin-top:8px">在线三维装箱问题在机器人化现代物流中对降低运输与人力成本至关重要。尽管深度强化学习方法已展现优异性能，但其常难以适应现实场景中因货物批次连续到达导致的短期分布偏移，从而导致性能下降。我们认为，现代物流系统中可获取的短期前瞻信息是缓解此问题的关键，尤其在分布偏移期间。本文将带前瞻包裹的在线三维装箱问题建模为模型预测控制问题，并采用蒙特卡洛树搜索框架进行求解。该框架通过动态探索先验机制，基于前瞻特征自动平衡学习到的强化学习策略与鲁棒随机策略。此外，我们设计了辅助奖励函数以惩罚单次放置导致的长期空间浪费。在真实数据集上的大量实验表明，本方法始终优于现有先进基线：在分布偏移下获得超过10%的性能提升，在线部署平均提升4%，最佳情况下可达8%以上，验证了框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of online 3D bin packing in logistics, where deep reinforcement learning methods often struggle with short-term distribution shifts as new batches of goods arrive. To leverage available lookahead parcel information, the authors formulate the problem as a Model Predictive Control task and solve it using a Monte Carlo Tree Search framework enhanced with a dynamic exploration prior that balances a learned policy and a random one, plus an auxiliary reward to reduce spatial waste. Experimental results on real-world datasets demonstrate that this approach consistently outperforms existing baselines, achieving over 10% improvement under distribution shifts and an average 4% gain in online deployment.</div>
<div class="mono" style="margin-top:8px">本文针对物流中的在线三维装箱问题展开研究，该问题中深度强化学习方法常因货物批次顺序到达导致的短期分布偏移而性能下降。为利用可预见的包裹信息，作者将问题建模为模型预测控制任务，并采用蒙特卡洛树搜索框架求解，其中引入了动态探索先验以平衡学习策略与随机策略，同时设计辅助奖励以减少空间浪费。在真实数据集上的大量实验表明，该方法始终优于现有基线，在分布偏移下获得超过10%的性能提升，在线部署平均改善4%，验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SWaRL: Safeguard Code Watermarking via Reinforcement Learning</div>
<div class="meta-line">Authors: Neusha Javidnia, Ruisi Zhang, Ashish Kundu, Farinaz Koushanfar</div>
<div class="meta-line">First: 2026-01-05T23:35:39+00:00 · Latest: 2026-01-05T23:35:39+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SWaRL, a robust and fidelity-preserving watermarking framework designed to protect the intellectual property of code LLM owners by embedding unique and verifiable signatures in the generated output. Existing approaches rely on manually crafted transformation rules to preserve watermarked code functionality or manipulate token-generation probabilities at inference time, which are prone to compilation errors. To address these challenges, SWaRL employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward signal to maintain watermark detectability. Furthermore, SWaRL employs low-rank adaptation (LoRA) during fine-tuning, allowing the learned watermark information to be transferable across model updates. Extensive experiments show that SWaRL achieves higher watermark detection accuracy compared to prior methods while fully maintaining watermarked code functionality. The LoRA-based signature embedding steers the base model to generate and solve code in a watermark-specific manner without significant computational overhead. Moreover, SWaRL exhibits strong resilience against refactoring and adversarial transformation attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWaRL：基于强化学习的代码水印保护框架</div>
<div class="mono" style="margin-top:8px">本文提出SWaRL，一种鲁棒且保真度的水印框架，旨在通过向生成代码中嵌入唯一可验证签名来保护代码大语言模型所有者的知识产权。现有方法依赖人工设计的转换规则来保持水印代码功能，或在推理时操纵词元生成概率，易导致编译错误。为解决这些问题，SWaRL采用基于强化学习的协同训练框架：利用编译器反馈确保功能正确性，并联合训练保密验证器作为奖励信号以维持水印可检测性。此外，SWaRL在微调阶段采用低秩自适应技术，使习得的水印信息可跨模型更新迁移。大量实验表明，SWaRL在完全保持水印代码功能的同时，比现有方法实现更高的水印检测准确率。基于低秩自适应的签名嵌入引导基础模型以水印定制方式生成和求解代码，且无显著计算开销。该框架对代码重构和对抗性转换攻击亦表现出强鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWaRL is to protect the intellectual property of code LLM owners by embedding verifiable watermarks in generated code, addressing limitations of existing methods that often cause compilation errors. The method employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward to maintain watermark detectability, along with low-rank adaptation (LoRA) for transferable watermarking across model updates. Experimental results demonstrate that SWaRL achieves higher watermark detection accuracy than prior methods while fully preserving code functionality, exhibits strong resilience against refactoring and adversarial attacks, and operates without significant computational overhead.</div>
<div class="mono" style="margin-top:8px">SWaRL的动机是通过在生成的代码中嵌入可验证的水印来保护代码大语言模型所有者的知识产权，以解决现有方法常导致编译错误的问题。该方法采用基于强化学习的协同训练框架，利用编译器反馈确保功能正确性，并通过联合训练的保密验证器作为奖励信号来维持水印可检测性，同时使用低秩适应（LoRA）实现水印信息在模型更新间的可迁移性。实验结果表明，SWaRL相比先前方法实现了更高的水印检测准确率，完全保持了水印代码的功能性，对重构和对抗性攻击表现出强韧性，且没有显著的计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">Solving the Paint Shop Problem with Flexible Management of Multi-Lane Buffers Using Reinforcement Learning and Action Masking</div>
<div class="meta-line">Authors: Mirko Stappert, Bernhard Lutz, Janis Brammer, Dirk Neumann</div>
<div class="meta-line">First: 2025-04-03T14:37:40+00:00 · Latest: 2026-01-05T21:51:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.02644v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.02644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the paint shop problem, an unordered incoming sequence of cars assigned to different colors has to be reshuffled with the objective of minimizing the number of color changes. To reshuffle the incoming sequence, manufacturers can employ a first-in-first-out multi-lane buffer system allowing store and retrieve operations. So far, prior studies primarily focused on simple decision heuristics like greedy or simplified problem variants that do not allow full flexibility when performing store and retrieve operations. In this study, we propose a reinforcement learning approach to minimize color changes for the flexible problem variant, where store and retrieve operations can be performed in an arbitrary order. After proving that greedy retrieval is optimal, we incorporate this finding into the model using action masking. Our evaluation, based on 170 problem instances with 2-8 buffer lanes and 5-15 colors, shows that our approach reduces color changes compared to existing methods by considerable margins depending on the problem size. Furthermore, we demonstrate the robustness of our approach towards different buffer sizes and imbalanced color distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>运用强化学习与动作掩码实现多车道缓冲区灵活管理的涂装车间问题求解</div>
<div class="mono" style="margin-top:8px">涂装车间问题中，需对分配不同颜色的无序进站车辆序列进行重排，以最小化颜色切换次数。制造商可采用先进先出多车道缓冲区系统执行存储与检索操作实现序列重排。现有研究多聚焦于贪心等简单决策启发式或限制存储/检索操作灵活性的简化问题变体。本研究提出强化学习方法，针对可任意顺序执行存储/检索操作的灵活问题变体最小化颜色切换。在证明贪心检索策略最优性后，通过动作掩码将该结论融入模型。基于170个含2-8个缓冲车道及5-15种颜色的问题实例评估表明，本方法较现有方案显著降低颜色切换次数（降幅随问题规模变化）。此外，研究验证了该方法对不同缓冲区尺寸及非均衡颜色分布的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to minimize costly color changes in automotive paint shops, where an unordered sequence of cars must be efficiently reshuffled using a multi-lane buffer, this study addresses limitations of prior heuristic methods that restrict operational flexibility. The proposed method employs reinforcement learning enhanced with action masking, which incorporates a proven optimal strategy for greedy retrieval to intelligently manage arbitrary store and retrieve operations. Experimental results on 170 instances with varying buffer lanes and colors demonstrate that this approach significantly reduces color changes compared to existing methods and shows robustness across different buffer sizes and imbalanced color distributions.</div>
<div class="mono" style="margin-top:8px">本研究针对汽车涂装车间中需通过多车道缓冲区对无序车辆序列进行重排以最小化颜色切换成本的问题，其动机在于克服现有启发式方法在操作灵活性上的局限。所提出的方法采用强化学习并结合动作掩码技术，整合了已证明最优的贪心检索策略，以智能管理任意的存储和取出操作。基于170个不同缓冲区车道和颜色数量的实例评估表明，该方法相比现有方法显著减少了颜色切换次数，并在不同缓冲区大小和不平衡颜色分布下展现了良好的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Active operator learning with predictive uncertainty quantification for partial differential equations</div>
<div class="meta-line">Authors: Nick Winovich, Mitchell Daneker, Lu Lu, Guang Lin</div>
<div class="meta-line">First: 2025-03-05T04:48:14+00:00 · Latest: 2026-01-05T21:21:15+00:00</div>
<div class="meta-line">Comments: Submitted to the Journal of Computational Physics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03178v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.03178v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework&#x27;s uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>偏微分方程主动算子学习与预测不确定性量化</div>
<div class="mono" style="margin-top:8px">随着神经算子被广泛用于快速求解偏微分方程，理解模型预测的准确性及相关误差水平对于在科学应用中部署可靠的代理模型至关重要。现有不确定性量化框架采用集成或贝叶斯方法，在训练和推断阶段均会产生高昂计算成本。本文提出一种专为深度算子网络设计的轻量级预测不确定性量化方法，该方法也可推广至其他算子网络。在线性与非线性偏微分方程上的数值实验表明，在训练数据集足够大的情况下，该框架的不确定性估计具有无偏性，并能提供准确的分布外不确定性预测。本框架提供快速推断与不确定性估计，可高效驱动传统求解器难以承担的外循环分析。我们展示了如何将预测不确定性应用于贝叶斯优化和主动学习问题，以提升外循环优化过程的精度与数据效率。在主动学习场景中，我们将框架扩展至傅里叶神经算子，并描述了适用于其他算子网络的通用方法。为实现实时部署，我们引入基于预计算主干输出和稀疏放置矩阵的推断策略，将评估时间缩短至五分之一以下。该方法为时间敏感场景下的不确定性感知算子学习提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable and computationally efficient uncertainty quantification (UQ) in neural operator surrogates for solving partial differential equations (PDEs), this paper proposes a lightweight predictive UQ method tailored for DeepONets and generalizable to other operator networks. The method avoids costly ensemble or Bayesian approaches by introducing a framework that provides fast inference and unbiased uncertainty estimates, including an efficient inference strategy using precomputed trunk outputs and a sparse placement matrix to reduce evaluation time by over fivefold. Experimental results on linear and nonlinear PDEs demonstrate that the framework delivers accurate out-of-distribution uncertainty predictions with sufficient training data and effectively improves accuracy and data-efficiency in applications like Bayesian optimization and active learning.</div>
<div class="mono" style="margin-top:8px">本文的动机是为求解偏微分方程的神经算子代理模型提供可靠且计算高效的预测不确定性量化方法，以克服现有集成或贝叶斯方法训练和推理成本高的问题。该方法提出了一种轻量级的预测不确定性量化框架，专为DeepONet设计并可推广至其他算子网络，通过引入基于预计算主干输出和稀疏放置矩阵的推理策略，将评估时间减少五倍以上。在线性和非线性偏微分方程上的数值实验表明，该框架在足够训练数据下能提供无偏的不确定性估计和准确的分布外不确定性预测，并在贝叶斯优化和主动学习等应用中有效提升了精度和数据效率。</div>
</details>
</div>
<div class="card">
<div class="title">LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware Layout Generation</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen, Naren Ramakrishnan</div>
<div class="meta-line">First: 2025-09-21T03:02:59+00:00 · Latest: 2026-01-05T20:28:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16891v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.16891v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型作为布局设计师：增强内容感知布局生成的空间推理能力</div>
<div class="mono" style="margin-top:8px">尽管大语言模型在文本领域展现出卓越的推理与规划能力，并能有效执行复杂任务指令，但其理解和操纵空间关系的能力仍存在局限。这种能力对于内容感知的图形布局设计至关重要——该任务需在画布上协调排列异构元素，使最终设计保持视觉平衡与结构可行性。此问题要求在有限视觉空间内精确协调多元素的位置、对齐与结构组织。为突破此限制，我们提出LaySPA：一个基于强化学习的框架，通过显式空间推理能力增强基于大语言模型的布局设计智能体。LaySPA采用混合奖励信号，同步捕捉几何约束、结构保真度与视觉质量，使智能体能够导航画布、建模元素间关系并优化空间排布。通过组相对策略优化，智能体生成的内容感知布局能反映显著区域、遵循空间约束，同时生成可解释的推理轨迹以说明布局决策，并输出结构化布局规范。实验结果表明，LaySPA显著提升了结构有效且视觉吸引力布局的生成质量，其性能超越规模更大的通用大语言模型，并与专业布局模型的最新成果相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limited spatial reasoning capabilities of Large Language Models (LLMs), which are crucial for content-aware graphic layout design requiring the balanced arrangement of heterogeneous elements on a canvas. To address this, the method introduces LaySPA, a reinforcement learning framework that augments LLM-based agents with explicit spatial reasoning, using hybrid reward signals for geometric constraints and visual quality, along with group-relative policy optimization to generate interpretable layout specifications. The main experimental results demonstrate that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于大型语言模型在空间推理能力上的局限性，而这对于需要将异构元素平衡排列在画布上的内容感知图形布局设计至关重要。为解决这一问题，方法上提出了LaySPA，这是一个基于强化学习的框架，通过混合奖励信号捕捉几何约束和视觉质量，并利用组相对策略优化来增强基于LLM的智能体的显式空间推理能力，从而生成可解释的布局规范。主要实验结果表明，LaySPA显著改善了结构有效且视觉吸引人的布局生成效果，其性能超越了更大的通用大型语言模型，并与最先进的专用布局模型相当。</div>
</details>
</div>
<div class="card">
<div class="title">Textual Explanations and Their Evaluations for Reinforcement Learning Policy</div>
<div class="meta-line">Authors: Ahmad Terra, Mohit Ahmed, Rafia Inam, Elena Fersman, Martin Törngren</div>
<div class="meta-line">First: 2026-01-05T19:38:07+00:00 · Latest: 2026-01-05T19:38:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02514v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert&#x27;s knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习策略的文本解释及其评估</div>
<div class="mono" style="margin-top:8px">理解强化学习（RL）策略对于确保自主智能体行为符合人类预期至关重要。这一目标可通过可解释强化学习（XRL）技术实现。尽管文本解释易于人类理解，但其正确性保障仍具挑战，且现有评估方法有限。本文提出一种新颖的XRL框架，用于生成文本解释、将其转化为透明规则集、提升解释质量并进行评估。该框架可融入专家知识，并设计了自动谓词生成器以解析状态语义信息。通过大型语言模型（LLM）和聚类技术识别高频条件生成文本解释，进而将条件转化为规则以评估其属性、保真度及部署环境中的性能。提出两种优化技术以提升解释质量并减少信息冲突。在三个开源环境中进行实验以确保可复现性，并通过电信用例验证所提XRL框架的工业适用性。该框架解决了现有自主策略解释方法的局限性，生成的透明规则在特定任务中表现良好，同时实现了对文本解释的系统化定量评估，为XRL领域提供了重要洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to ensure that Reinforcement Learning (RL) policies align with human expectations and the challenge of evaluating textual explanations, this paper introduces a novel Explainable RL (XRL) framework. The method generates textual explanations using a Large Language Model and clustering to identify frequent conditions, converts these into transparent rules, and refines them to improve quality and reduce conflicts. Experimental results in three open-source environments and a telecom use case demonstrate that the framework addresses limitations of prior methods, enables systematic evaluation, and produces rules that achieve satisfactory task performance.</div>
<div class="mono" style="margin-top:8px">本文的动机是确保强化学习策略符合人类预期，并解决文本解释评估的挑战，提出了一种新颖的可解释强化学习框架。该方法利用大语言模型和聚类技术生成文本解释，识别频繁条件并将其转换为透明规则，再通过精炼技术提升质量并减少冲突。在三个开源环境和电信用例中的实验表明，该框架克服了现有方法的局限，实现了系统化评估，且生成的规则在特定任务中取得了令人满意的性能。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Enhanced Reinforcement Learning for Time Series Anomaly Detection</div>
<div class="meta-line">Authors: Bahareh Golchin, Banafsheh Rekabdar, Danielle Justo</div>
<div class="meta-line">First: 2026-01-05T19:33:30+00:00 · Latest: 2026-01-05T19:33:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02511v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02511v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting anomalies in time series data is crucial for finance, healthcare, sensor networks, and industrial monitoring applications. However, time series anomaly detection often suffers from sparse labels, complex temporal patterns, and costly expert annotation. We propose a unified framework that integrates Large Language Model (LLM)-based potential functions for reward shaping with Reinforcement Learning (RL), Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation. An LSTM-based RL agent leverages LLM-derived semantic rewards to guide exploration, while VAE reconstruction errors add unsupervised anomaly signals. Active learning selects the most uncertain samples, and label propagation efficiently expands labeled data. Evaluations on Yahoo-A1 and SMD benchmarks demonstrate that our method achieves state-of-the-art detection accuracy under limited labeling budgets and operates effectively in data-constrained settings. This study highlights the promise of combining LLMs with RL and advanced unsupervised techniques for robust, scalable anomaly detection in real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型增强的强化学习在时间序列异常检测中的应用</div>
<div class="mono" style="margin-top:8px">时间序列异常检测在金融、医疗、传感器网络和工业监测等领域至关重要，但常面临标注稀疏、时序模式复杂和专家标注成本高昂的挑战。本文提出一种统一框架，整合了基于大语言模型的奖励塑形潜力函数与强化学习、变分自编码器增强的动态奖励缩放机制，以及结合标签传播的主动学习方法。基于LSTM的强化学习智能体利用大语言模型衍生的语义奖励引导探索，同时变分自编码器的重构误差提供无监督异常信号。主动学习筛选最不确定样本，标签传播高效扩展标注数据。在Yahoo-A1和SMD基准测试上的评估表明，该方法在有限标注预算下实现了最优检测精度，并在数据受限场景中保持高效运行。本研究彰显了将大语言模型与强化学习及先进无监督技术相结合，为实际应用提供鲁棒、可扩展异常检测方案的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of time series anomaly detection, such as sparse labels and complex temporal patterns, by proposing a unified framework that integrates Large Language Models (LLMs) with Reinforcement Learning (RL). The method uses LLM-based potential functions for reward shaping, Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation, where an LSTM-based RL agent leverages semantic rewards from LLMs and unsupervised signals from VAE reconstruction errors. Experimental results on Yahoo-A1 and SMD benchmarks show that the approach achieves state-of-the-art detection accuracy under limited labeling budgets, demonstrating effectiveness in data-constrained settings.</div>
<div class="mono" style="margin-top:8px">本文针对时间序列异常检测中标签稀疏、时序模式复杂等挑战，提出了一种将大语言模型（LLM）与强化学习（RL）相结合的统一框架。该方法利用基于LLM的势函数进行奖励塑造，结合变分自编码器（VAE）增强的动态奖励缩放以及主动学习与标签传播技术，通过LSTM驱动的RL智能体融合LLM的语义奖励和VAE重建误差的无监督信号。在Yahoo-A1和SMD基准测试上的实验结果表明，该方法在有限标注预算下实现了最先进的检测精度，有效适用于数据受限的实际场景。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Open-Ended Reasoning to Predict the Future</div>
<div class="meta-line">Authors: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</div>
<div class="meta-line">First: 2025-12-31T18:59:51+00:00 · Latest: 2026-01-05T18:45:47+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25070v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.25070v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展开放式推理以预测未来</div>
<div class="mono" style="margin-top:8px">高风险决策涉及对未来不确定性的推理。本研究训练语言模型对开放式预测问题进行预测。为扩大训练数据规模，我们通过全自动精细筛选方法，从每日新闻报道的全球事件中合成新颖的预测问题。基于自建数据集OpenForesight训练Qwen3思维模型。为防止训练与评估期间未来信息泄露，预测系统全程采用离线新闻语料库进行数据生成与检索。通过小型验证集指导，我们验证了检索机制的优势及改进的强化学习奖励函数。最终预测系统在2025年5月至8月期间进行隔离测试。专用模型OpenForecaster 8B性能媲美规模更大的专有模型，其训练显著提升了预测的准确性、校准度和一致性。研究发现预测训练带来的校准改进可泛化至主流基准测试。我们开源全部模型、代码与数据，以推动语言模型预测研究的广泛普及。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for high-stakes decision-making under uncertainty, this work develops language models for open-ended future prediction. The method involves automatically synthesizing forecasting questions from daily news to create the OpenForesight dataset, training Qwen3 thinking models while preventing data leakage via an offline corpus, and enhancing performance through retrieval and an improved RL reward function. Experimental results on held-out tests from May to August 2025 show that the specialized OpenForecaster 8B model matches larger proprietary models in accuracy, calibration, and consistency, with calibration gains generalizing to other benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究旨在应对高风险决策中的不确定性，开发用于开放式未来预测的语言模型。方法包括从每日新闻中自动合成预测问题以构建OpenForesight数据集，使用离线语料库训练Qwen3思维模型以防止数据泄露，并通过检索和改进的强化学习奖励函数提升性能。在2025年5月至8月的保留测试中，实验结果表明，专门的OpenForecaster 8B模型在准确性、校准性和一致性上媲美更大的专有模型，且校准改进可推广到其他基准测试。</div>
</details>
</div>
<div class="card">
<div class="title">VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation</div>
<div class="meta-line">Authors: Shikun Sun, Liao Qu, Huichao Zhang, Yiheng Liu, Yangyang Song, Xian Li, Xu Wang, Yi Jiang, Daniel K. Du, Xinglong Wu, Jia Jia</div>
<div class="meta-line">First: 2026-01-05T16:36:40+00:00 · Latest: 2026-01-05T16:36:40+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/ByteVisionLab/NextFlow</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02256v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02256v1">PDF</a> · <a href="https://github.com/ByteVisionLab/NextFlow">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VAR强化学习优化方案：解决视觉自回归生成中的异步策略冲突</div>
<div class="mono" style="margin-top:8px">视觉生成领域主要由三大范式主导：自回归模型、扩散模型和视觉自回归模型。与自回归和扩散模型不同，VAR模型在生成步骤中处理异构输入结构，这导致了严重的异步策略冲突。该问题在强化学习场景中尤为突出，造成训练不稳定和优化对齐不足。为解决此问题，我们提出一种新颖框架，通过显式管理这些冲突来增强组相对策略优化。我们的方法整合了三个协同组件：1）用于引导早期生成的稳定中间奖励机制；2）实现精确信用分配的动态时间步重加权方案；3）基于奖励反馈学习原理设计的新型掩码传播算法，可在空间和时间维度隔离优化效应。相较于原始GRPO基线，本方法在样本质量和目标对齐方面均取得显著提升，为VAR模型实现了稳健有效的优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the asynchronous policy conflicts inherent in Visual AutoRegressive (VAR) models due to their heterogeneous input structures across generation steps, which cause unstable training and suboptimal alignment in reinforcement learning (RL) scenarios, this paper proposes an enhanced Group Relative Policy Optimization (GRPO) framework. The method integrates three components: a stabilizing intermediate reward for early-stage guidance, a dynamic time-step reweighting scheme for precise credit assignment, and a novel mask propagation algorithm derived from Reward Feedback Learning (ReFL) to isolate optimization effects spatially and temporally. Experimental results show significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust optimization for VAR models.</div>
<div class="mono" style="margin-top:8px">本文针对视觉自回归（VAR）模型在生成步骤中因异构输入结构导致的异步策略冲突问题，该问题在强化学习场景中引发训练不稳定和对齐不佳，提出了一种增强的组相对策略优化（GRPO）框架。方法整合了三个协同组件：用于早期引导的稳定中间奖励、用于精确信用分配的动态时间步重加权方案，以及源自奖励反馈学习（ReFL）的新型掩码传播算法，以在空间和时间上隔离优化效应。实验结果表明，相较于原始GRPO基线，该方法在样本质量和目标对齐方面取得显著提升，实现了对VAR模型的稳健优化。</div>
</details>
</div>
<div class="card">
<div class="title">Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training</div>
<div class="meta-line">Authors: Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud</div>
<div class="meta-line">First: 2025-07-02T14:29:30+00:00 · Latest: 2026-01-05T16:10:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01752v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.01752v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privacy and security concerns such as susceptibility to data poisoning attacks. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide non-vacuous generalization bounds and strong theoretical guarantees for privacy, robustness to data poisoning attacks, and extraction attacks. In experiments with LLMs, we demonstrate empirically that black-box optimization methods, despite the scalability and computational challenges inherent to black-box approaches, are able to learn, showing how a few iterations of BBoxER improve performance, generalize well on a benchmark of reasoning datasets, and are robust to membership inference attacks. This positions BBoxER as an attractive add-on on top of gradient-based optimization, offering suitability for deployment in restricted or privacy-sensitive environments while also providing non-vacuous generalization guarantees.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需窥探的调优：可证明的泛化边界与鲁棒性大语言模型后训练</div>
<div class="mono" style="margin-top:8px">基于梯度的优化是深度学习的核心方法，通过反向传播实现高效可扩展的训练。然而，训练过程中暴露梯度可能泄露底层数据的敏感信息，引发隐私与安全隐患（如数据投毒攻击的脆弱性）。相比之下，将模型视为黑箱函数的黑箱优化方法仅依赖函数评估指导优化，在数据访问受限、对抗风险高或需避免过拟合的场景中展现出潜力。本文提出BBoxER——一种用于大语言模型后训练的进化黑箱方法，通过对训练数据的隐式压缩构建信息瓶颈。利用信息流的可追踪性，我们为隐私保护、数据投毒攻击鲁棒性和提取攻击防御提供了非平凡的泛化边界与强理论保证。在大语言模型实验中，我们实证表明：尽管黑箱方法存在可扩展性与计算挑战，BBoxER仅需数次迭代即可提升模型性能，在推理数据集基准测试中展现良好泛化能力，并对成员推断攻击保持鲁棒性。这使BBoxER成为基于梯度优化的理想补充方案，适用于受限或隐私敏感环境，同时提供非平凡的泛化保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by privacy and security concerns in gradient-based deep learning, which can leak sensitive data and be vulnerable to attacks, this paper introduces BBoxER, an evolutionary black-box optimization method for LLM post-training that treats the model as an opaque function to create an information bottleneck. The method provides theoretical guarantees for generalization, privacy, and robustness against data poisoning and extraction attacks. Experimental results with LLMs show that BBoxER improves performance, generalizes well on reasoning benchmarks, and demonstrates robustness to membership inference attacks, positioning it as a suitable add-on for privacy-sensitive deployments.</div>
<div class="mono" style="margin-top:8px">针对基于梯度的深度学习可能泄露敏感数据且易受攻击的隐私与安全问题，本文提出了BBoxER，一种用于大语言模型后训练的进化黑盒优化方法，将模型视为不透明函数以创建信息瓶颈。该方法为泛化、隐私以及对抗数据投毒和提取攻击的鲁棒性提供了理论保证。在大语言模型上的实验结果表明，BBoxER能提升性能，在推理基准测试上泛化良好，并对成员推理攻击表现出鲁棒性，使其成为适用于隐私敏感部署的有效附加工具。</div>
</details>
</div>
<div class="card">
<div class="title">FaithLens: Detecting and Explaining Faithfulness Hallucination</div>
<div class="meta-line">Authors: Shuzheng Si, Qingyi Wang, Haozhe Zhao, Yuzhuo Bai, Guanqiao Chen, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun</div>
<div class="meta-line">First: 2025-12-23T09:20:32+00:00 · Latest: 2026-01-05T15:43:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20182v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20182v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FaithLens：检测与解释忠实性幻觉</div>
<div class="mono" style="margin-top:8px">识别大语言模型输出是否包含忠实性幻觉对实际应用至关重要，例如检索增强生成与摘要任务。本文提出FaithLens，一种高效低成本的忠实性幻觉检测模型，可同时提供二元预测及相应解释以增强可信度。我们首先通过先进大语言模型合成含解释的训练数据，并采用严格的数据过滤策略确保标签准确性、解释质量与数据多样性。随后基于这些高质量数据对模型进行冷启动微调，并通过基于规则的强化学习进一步优化，同时奖励预测正确性与解释质量。在12项多样化任务上的实验表明，仅80亿参数的FaithLens在性能上超越GPT-4.1及o3等先进模型，且能生成高质量解释，在可信度、效率与效能间实现独特平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces FaithLens, a model designed to detect and explain faithfulness hallucinations in large language model outputs, motivated by the need for reliability in applications like retrieval-augmented generation and summarization. The method involves synthesizing training data with explanations using advanced LLMs, applying data filtering for quality and diversity, fine-tuning an 8B-parameter model, and optimizing it with rule-based reinforcement learning that rewards both prediction accuracy and explanation quality. Experimental results across 12 diverse tasks demonstrate that FaithLens outperforms models such as GPT-4.1 and o3 in detection performance while producing high-quality explanations, achieving a balance of trustworthiness, efficiency, and effectiveness.</div>
<div class="mono" style="margin-top:8px">本文提出了FaithLens模型，旨在检测和解释大语言模型输出中的忠实性幻觉，其动机源于检索增强生成和摘要等实际应用中对可靠性的需求。该方法通过使用先进大语言模型合成带解释的训练数据，应用数据过滤策略确保标签正确性、解释质量和数据多样性，对8B参数模型进行微调作为冷启动，并采用基于规则的强化学习进行优化，以预测正确性和解释质量为奖励。在12个多样化任务上的实验结果表明，FaithLens在检测性能上超越了GPT-4.1和o3等先进模型，并能生成高质量的解释，实现了可信性、效率和有效性的独特平衡。</div>
</details>
</div>
<div class="card">
<div class="title">NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation</div>
<div class="meta-line">Authors: Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song, Yongsheng Dong, Shikun Sun, Xian Li, Xu Wang, Yi Jiang, Hu Ye, Bo Chen, Yiming Gao, Peng Liu, Akide Liu, Zhipeng Yang, Qili Deng, Linjie Xing, Jiyang Liu, Zhao Wang, Yang Zhou, Mingcong Liu, Yi Zhang, Qian He, Xiwei Hu, Zhongqi Qi, Jie Shao, Zhiye Fu, Shuai Wang, Fangmin Chen, Xuezhi Chai, Zhihua Wu, Yitong Wang, Zehuan Yuan, Daniel K. Du, Xinglong Wu</div>
<div class="meta-line">First: 2026-01-05T15:27:04+00:00 · Latest: 2026-01-05T15:27:04+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/ByteVisionLab/NextFlow</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02204v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02204v1">PDF</a> · <a href="https://github.com/ByteVisionLab/NextFlow">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NextFlow：统一序列建模激活多模态理解与生成</div>
<div class="mono" style="margin-top:8px">我们提出NextFlow——一个基于6万亿交错文本-图像离散标记训练的统一仅解码器自回归Transformer。通过在统一自回归架构中利用统一的视觉表示，NextFlow原生激活了多模态理解与生成能力，解锁了图像编辑、交错内容生成和视频生成等功能。受模态本质差异的启发（文本严格序列化，图像本质层级化），我们保留文本的下一标记预测，但对视觉生成采用下一尺度预测。这突破了传统光栅扫描方法，仅需5秒即可生成1024x1024图像，比同类自回归模型快数个数量级。我们通过稳健的训练方案解决了多尺度生成的不稳定性，并引入了强化学习的前缀调优策略。实验表明，NextFlow在统一模型中达到最先进性能，并在视觉质量上媲美专业扩散模型基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to unify sequential modeling for multimodal tasks, NextFlow introduces a decoder-only autoregressive transformer trained on 6 trillion interleaved text-image tokens. The method employs a unified architecture that retains next-token prediction for text but adopts next-scale prediction for visual generation, diverging from raster-scan approaches to better handle the hierarchical nature of images, and incorporates a robust training recipe and prefix-tuning for reinforcement learning to address instability. Experimental results show that NextFlow generates 1024x1024 images in just 5 seconds, significantly faster than comparable autoregressive models, achieves state-of-the-art performance among unified models, and rivals specialized diffusion baselines in visual quality.</div>
<div class="mono" style="margin-top:8px">NextFlow的动机在于统一多模态任务的序列建模，提出了一种基于6万亿交错文本-图像标记训练的仅解码器自回归Transformer。该方法采用统一架构，对文本保留下一标记预测，但对视觉生成采用下一尺度预测，以更好地处理图像的分层特性，区别于传统光栅扫描方法，并通过稳健的训练方案和用于强化学习的前缀调优来解决不稳定性。实验结果表明，NextFlow能在5秒内生成1024x1024图像，速度远超同类自回归模型，在统一模型中达到最先进性能，并在视觉质量上可与专业扩散基线相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models</div>
<div class="meta-line">Authors: Shouang Wei, Min Zhang, Xin Lin, Bo Jiang, Kun Kuang, Zhongxiang Dai</div>
<div class="meta-line">First: 2025-11-12T01:27:02+00:00 · Latest: 2026-01-05T15:26:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08873v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08873v2">PDF</a> · <a href="https://github.com/Mind-Lab-ECNU/UCO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students&#x27; evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students&#x27; cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students&#x27; cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student&#x27;s Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UCO：一种基于大语言模型的自适应教学多轮交互式强化学习方法</div>
<div class="mono" style="margin-top:8px">在教育场景中，大语言模型正从答案提供者转变为智能导师，但当前监督微调方法仅习得表层教学模式，缺乏动态适应能力。现有强化学习方法虽能缓解此局限，却面临两大挑战：其一，仅依据学生能否输出正确答案评估教学效果，无法区分学生是真正理解还是复述教师答案；其二，无法通过交互对话实时感知学生认知状态的变化，因而难以动态调整教学策略以适应学生认知水平。为此，我们提出单向认知优化方法以应对这些挑战。该方法采用多轮交互式强化学习范式，其创新在于两个协同奖励函数：进展奖励捕捉学生的认知提升，评估学生是否真正从困惑转向理解；支架奖励动态识别每位学生的最近发展区，激励教师在该区域内维持有效教学。通过在BigMath和MathTutorBench基准上与11个基线模型对比实验表明，UCO模型在同等规模模型中表现最优，且性能媲美先进闭源模型。代码与数据已开源：https://github.com/Mind-Lab-ECNU/UCO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the shift of large language models (LLMs) from answer providers to intelligent tutors, this paper addresses the limitations of current methods that lack dynamic adaptation and genuine student understanding assessment. The proposed Unidirectional Cognitive Optimization (UCO) method employs a multi-turn interactive reinforcement learning paradigm with two synergistic reward functions: a Progress Reward to capture cognitive advancement from confusion to comprehension, and a Scaffold Reward to dynamically identify each student&#x27;s Zone of Proximal Development (ZPD) for adaptive teaching. Experimental results on BigMath and MathTutorBench benchmarks show that UCO outperforms 11 baseline models of equivalent scale and achieves performance comparable to advanced closed-source models.</div>
<div class="mono" style="margin-top:8px">本文的动机源于大型语言模型（LLMs）正从答案提供者转向智能导师，但现有方法缺乏动态适应能力且无法评估学生的真实理解水平。为此，提出了单向认知优化（UCO）方法，采用多轮交互式强化学习范式，包含两个协同奖励函数：进展奖励用于捕捉学生从困惑到理解的认知进步，支架奖励则动态识别每个学生的最近发展区（ZPD）以实现自适应教学。在BigMath和MathTutorBench基准上的实验结果表明，UCO优于11个同等规模的基线模型，并达到了与先进闭源模型相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents</div>
<div class="meta-line">Authors: Keyu Wang, Bingchen Miao, Wendong Bu, Yu Wu, Juncheng Li, Shengyu Zhang, Wenqiao Zhang, Siliang Tang, Jun Xiao, Yueting Zhuang</div>
<div class="meta-line">First: 2026-01-05T15:24:05+00:00 · Latest: 2026-01-05T15:24:05+00:00</div>
<div class="meta-line">Comments: 19 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02201v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CORE：面向虚拟代理的基于代码的逆向自训练框架与图扩展</div>
<div class="mono" style="margin-top:8px">通过整合多模态大语言模型，多模态虚拟代理的发展取得了显著进展。然而，主流训练范式面临关键挑战：行为克隆通过模仿实现简单有效，但行为多样性不足；强化学习能够通过探索发现新策略，但严重依赖人工设计的奖励函数。为解决这两种方法间的冲突，我们提出CORE——一种基于代码的逆向自训练框架与图扩展方法，它桥接了模仿与探索，提供了一种既能提升行为多样性、又无需依赖人工奖励设计的新型训练框架。具体而言，我们引入语义代码抽象技术，可从专家示范中自动推断奖励函数而无需人工设计。推断出的奖励函数（称为标签函数）是可执行代码，用于验证任务中的关键步骤。在此基础上，我们提出策略图扩展以增强领域内行为多样性，该方法构建称为策略图的多路径图，捕捉超越专家示范的多样化有效解决方案。此外，我们引入轨迹引导外推技术，通过利用成功与失败轨迹来扩展任务空间，从而丰富跨领域行为多样性。在Web和Android平台上的实验表明，CORE显著提升了整体性能与泛化能力，凸显其作为构建强大虚拟代理的鲁棒且可泛化训练范式的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing training paradigms for Multimodal Virtual Agents, where Behavior Cloning lacks diversity and Reinforcement Learning depends on manual reward design. The authors propose CORE, a framework that bridges imitation and exploration by introducing Semantic Code Abstraction to automatically infer executable reward functions from expert demonstrations, Strategy Graph Expansion to capture diverse in-domain solutions via a multi-path graph, and Trajectory-Guided Extrapolation to enhance out-of-domain diversity using both successful and failed trajectories. Experimental results on Web and Android platforms show that CORE significantly improves overall performance and generalization, demonstrating its robustness as a training paradigm for virtual agents.</div>
<div class="mono" style="margin-top:8px">本文针对多模态虚拟代理现有训练范式的局限性展开研究，其中行为克隆缺乏行为多样性，而强化学习依赖人工设计奖励函数。作者提出了CORE框架，通过语义代码抽象从专家演示中自动推断可执行的奖励函数，策略图扩展构建多路径图以捕捉领域内多样化的有效解决方案，以及轨迹引导外推利用成功和失败轨迹来增强领域外行为多样性，从而弥合模仿与探索之间的差距。在Web和Android平台上的实验表明，CORE显著提升了整体性能和泛化能力，凸显了其作为构建强大虚拟代理的鲁棒且可泛化训练范式的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense</div>
<div class="meta-line">Authors: Yu Li, Sizhe Tang, Rongqian Chen, Fei Xu Yu, Guangyu Jiang, Mahdi Imani, Nathaniel D. Bastian, Tian Lan</div>
<div class="meta-line">First: 2026-01-05T15:18:54+00:00 · Latest: 2026-01-05T15:18:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02196v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ACDZero：基于图嵌入的树搜索技术，实现自动化网络防御的掌控</div>
<div class="mono" style="margin-top:8px">自动化网络防御（ACD）旨在以最少或无需人工干预的方式保护计算机网络，通过采取隔离主机、重置服务、部署诱饵或更新访问控制等纠正措施应对入侵。然而，现有ACD方法（如深度强化学习）在决策/状态空间庞大的复杂网络中常面临探索困难，需耗费大量样本。受学习样本高效防御策略的需求启发，我们将CAGE挑战赛4中的ACD问题建模为基于上下文的部分可观测马尔可夫决策过程，并提出一种以蒙特卡洛树搜索为核心的防御策略。该方法显式建模ACD中的探索-利用权衡，利用统计抽样指导探索与决策。我们创新性地运用图神经网络将网络观测嵌入为属性图，实现对主机及其关系的置换不变推理。为在复杂搜索空间中实现实用化，我们通过学习的图嵌入和图编辑动作先验引导MCTS，将无模型泛化、策略蒸馏与前向规划相结合。在涵盖多样化网络结构和对抗行为的CC4场景中评估表明，相较于前沿强化学习基线，这种基于图嵌入的搜索引导规划能提升防御奖励与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sample inefficiency of deep reinforcement learning in complex automated cyber defense (ACD) scenarios with large state spaces, this paper frames ACD as a partially observable Markov decision process and proposes a planning-centric defense policy using Monte Carlo Tree Search (MCTS). The method innovatively employs graph neural networks (GNNs) to embed network observations as attributed graphs for permutation-invariant reasoning, guiding MCTS with learned graph embeddings and priors over graph-edit actions to combine model-free generalization with look-ahead planning. Experimental evaluation on CAGE Challenge 4 scenarios demonstrates that this graph-embedding-based tree search approach improves defense reward and robustness compared to state-of-the-art RL baselines.</div>
<div class="mono" style="margin-top:8px">针对深度强化学习在复杂自动化网络防御场景中样本效率低下的问题，本文将其建模为部分可观测马尔可夫决策过程，并提出一种基于蒙特卡洛树搜索的规划式防御策略。该方法创新性地使用图神经网络将网络观测嵌入为属性图以实现置换不变推理，并利用学习到的图嵌入和图编辑动作先验来引导树搜索，从而结合无模型泛化与前瞻规划。在CAGE Challenge 4场景上的实验评估表明，这种基于图嵌入的树搜索方法相比最先进的强化学习基线，能提升防御奖励和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting</div>
<div class="meta-line">Authors: Muxi Diao, Lele Yang, Wuxuan Gong, Yutong Zhang, Zhonghao Yan, Yufei Han, Kongming Liang, Weiran Xu, Zhanyu Ma</div>
<div class="meta-line">First: 2026-01-05T14:28:17+00:00 · Latest: 2026-01-05T14:28:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02151v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model&#x27;s internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as &quot;Confident Conflicts&quot; tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>熵自适应微调：解决置信冲突以缓解遗忘</div>
<div class="mono" style="margin-top:8px">监督微调（SFT）是领域适应的标准范式，但常导致灾难性遗忘。与之形成鲜明对比的是，策略强化学习（RL）能有效保留通用能力。我们探究这一差异，发现根本性的分布差距：RL与模型内部信念保持一致，而SFT强制模型拟合外部监督。这种不匹配常表现为“置信冲突”标记，其特征是低概率但低熵。在此类情况下，模型对其自身预测高度自信，却被强制学习相悖的真实标注，从而引发破坏性梯度更新。为此，我们提出熵自适应微调（EAFT）。与仅依赖预测概率的方法不同，EAFT利用标记级熵作为门控机制，以区分认知不确定性与知识冲突。这使得模型能够从不确定样本中学习，同时抑制冲突数据的梯度。在数学、医疗和智能体领域对Qwen与GLM系列（参数量4B至32B）的广泛实验验证了我们的假设。EAFT在保持标准SFT下游性能的同时，显著缓解了通用能力的退化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that Supervised Fine-Tuning (SFT) often causes catastrophic forgetting, whereas on-policy Reinforcement Learning (RL) preserves general capabilities, a discrepancy attributed to a distributional gap where SFT forces the model to fit external supervision against its internal beliefs, leading to &quot;Confident Conflicts&quot;—tokens with low probability but low entropy that trigger destructive updates. To address this, the method proposes Entropy-Adaptive Fine-Tuning (EAFT), which uses token-level entropy as a gating mechanism to differentiate epistemic uncertainty from knowledge conflict, enabling learning from uncertain samples while suppressing gradients on conflicting data. Experimental results on Qwen and GLM models (4B to 32B parameters) across mathematical, medical, and agentic domains show that EAFT matches standard SFT&#x27;s downstream performance while significantly reducing degradation of general capabilities.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到监督微调（SFT）常导致灾难性遗忘，而基于策略的强化学习（RL）能保持通用能力，这种差异源于分布差距：SFT迫使模型拟合外部监督，与其内部信念冲突，产生“自信冲突”——即概率低但熵低的标记，引发破坏性更新。为解决此问题，方法提出熵自适应微调（EAFT），利用标记级熵作为门控机制，区分认知不确定性与知识冲突，从而从不确定样本中学习，同时抑制冲突数据的梯度。在Qwen和GLM系列模型（4B至32B参数）上，于数学、医疗和代理领域的实验结果表明，EAFT在匹配标准SFT下游性能的同时，显著减轻了通用能力的退化。</div>
</details>
</div>
<div class="card">
<div class="title">Gibbs randomness-compression proposition: An efficient deep learning</div>
<div class="meta-line">Authors: M. Süzen</div>
<div class="meta-line">First: 2025-05-29T10:48:35+00:00 · Latest: 2026-01-05T14:07:39+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures, 1 table, 1 algorithm, 1 theorem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23869v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.23869v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A proposition that connects randomness and compression is put forward via Gibbs entropy over set of measurement vectors associated with a compression process. The proposition states that a lossy compression process is equivalent to {\it directed randomness} that preserves information content. The proposition originated from the observed behavior in newly proposed {\it Dual Tomographic Compression} (DTC) compress-train framework. This is akin to tomographic reconstruction of layer weight matrices via building compressed sensed projections, via so-called {\it weight rays}. This tomographic approach is applied to previous and next layers in a dual fashion, that triggers neuronal-level pruning. This novel model compress-train scheme appears in iterative fashion and acts as a smart neural architecture search: also called {\it compression aware training}. The experiments demonstrated the utility of this dual-tomography during training: method accelerates and supports lottery ticket hypothesis. However, random compress-train iterations having similar performance demonstrated the connection between randomness and compression from statistical physics perspective, we formulated the so-called {\it Gibbs randomness-compression proposition}, signifying randomness-compression relationship via Gibbs entropy. The proposition is supported with the experimental evidence, resulting in very high correlation between learning performance vs. Gibbs entropy over compression ratios. Practically, the DTC framework provides a promising approach for massively energy- and resource-efficient deep learning training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>吉布斯随机性-压缩命题：一种高效深度学习框架</div>
<div class="mono" style="margin-top:8px">本研究通过压缩过程相关测量向量集合的吉布斯熵，提出了连接随机性与压缩的命题。该命题指出：有损压缩过程等价于保持信息内容的定向随机性。该命题源于新提出的双层断层扫描压缩训练框架中观察到的现象，该框架通过构建压缩感知投影（即权重射线）对层权重矩阵进行断层扫描重建。这种断层扫描方法以前后层双向方式实施，触发神经元级剪枝。这种新颖的压缩训练模型以迭代方式运行，可视为智能神经架构搜索（亦称压缩感知训练）。实验证明双断层扫描在训练中的效用：该方法加速并支持彩票假设。随机压缩训练迭代具有相似性能的现象，从统计物理视角揭示了随机性与压缩的关联，由此我们提出吉布斯随机性-压缩命题，通过吉布斯熵表征随机性-压缩关系。实验证据支持该命题，显示学习性能与压缩比吉布斯熵间存在高度相关性。实践中，DTC框架为大规模节能高效的深度学习训练提供了可行方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for energy- and resource-efficient deep learning, this paper introduces the Gibbs randomness-compression proposition, which links lossy compression to directed randomness that preserves information, inspired by observations from a novel Dual Tomographic Compression (DTC) framework. The method employs a compress-train scheme using tomographic reconstruction of layer weights via compressed sensing projections, called weight rays, applied dually to adjacent layers to enable neuronal pruning and compression-aware training. Experimental results show that DTC accelerates training, supports the lottery ticket hypothesis, and demonstrates a high correlation between learning performance and Gibbs entropy across compression ratios, validating the proposition&#x27;s practical utility for efficient model training.</div>
<div class="mono" style="margin-top:8px">本文旨在实现高能效和资源高效的深度学习，提出了吉布斯随机性-压缩命题，该命题通过吉布斯熵将损失压缩与保留信息内容的定向随机性联系起来，灵感来源于新型双断层压缩框架的观察。该方法采用压缩训练方案，通过称为权重射线的压缩感知投影对层权重进行断层重建，以双重方式应用于相邻层，实现神经元剪枝和压缩感知训练。实验结果表明，双断层压缩框架加速了训练，支持彩票假设，并在学习性能与压缩比的吉布斯熵之间显示出高度相关性，验证了该命题在实际高效模型训练中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</div>
<div class="meta-line">Authors: Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Zhenbo Xu, Lechen Ning, Huijia Wu, Zhaofeng He</div>
<div class="meta-line">First: 2025-12-29T07:39:49+00:00 · Latest: 2026-01-05T13:39:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23260v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23260v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety alignment -- training large language models (LLMs) to refuse harmful requests while remaining helpful -- is critical for responsible deployment. Prior work established that safety behaviors are governed by low-rank structures, suggesting parameter-efficient fine-tuning (PEFT) should be well-suited for alignment. However, Low-Rank Adaptation (LoRA) consistently underperforms full fine-tuning and reinforcement learning on safety benchmarks. We attribute this gap to semantic entanglement: safety-relevant directions are intertwined with unrelated concepts due to polysemanticity, impeding implicit subspace identification. To address this, we propose SAILS (Safety Alignment via Interpretable Low-rank Subspace), which leverages Sparse Autoencoders (SAEs) to disentangle representations into monosemantic features, constructs an interpretable safety subspace from SAE decoder directions, and uses it to initialize LoRA adapters. Theoretically, we prove that SAE-based identification achieves arbitrarily small recovery error under monosemanticity assumptions, while direct identification suffers an irreducible error floor. Empirically, SAILS achieves up to 99.6% safety rate on Gemma-2-9B -- exceeding full fine-tuning by 7.4 points and matching RLHF-based models -- while updating only 0.19% of parameters and providing interpretability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于稀疏自编码器构建低秩子空间适配的可解释安全对齐方法</div>
<div class="mono" style="margin-top:8px">安全对齐——训练大语言模型在拒绝有害请求的同时保持有用性——对负责任部署至关重要。先前研究证实安全行为受低秩结构调控，表明参数高效微调应适用于对齐任务。然而，低秩适配在安全基准上持续弱于全参数微调和强化学习。我们将此差距归因于语义纠缠：由于多义性，安全相关方向与无关概念相互交织，阻碍了隐式子空间识别。为此，我们提出SAILS（基于可解释低秩子空间的安全对齐），利用稀疏自编码器将表征解耦为单义特征，从SAE解码器方向构建可解释安全子空间，并用于初始化LoRA适配器。理论上，我们证明在单义性假设下基于SAE的识别可实现任意小的恢复误差，而直接识别存在不可约的误差下限。实证中，SAILS在Gemma-2-9B上实现99.6%的安全率——超越全参数微调7.4个百分点，媲美基于RLHF的模型——仅更新0.19%参数且具备可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to improve the safety alignment of large language models (LLMs) through a more interpretable and parameter-efficient method, as existing low-rank adaptation techniques like LoRA underperform due to semantic entanglement where safety concepts are mixed with unrelated features. The proposed method, SAILS, addresses this by using Sparse Autoencoders (SAEs) to disentangle representations into monosemantic features, constructing an interpretable safety subspace from these features to initialize LoRA adapters for fine-tuning. Experimental results show that SAILS achieves a high safety rate of up to 99.6% on the Gemma-2-9B model, surpassing full fine-tuning by 7.4 percentage points and matching reinforcement learning-based models, while updating only 0.19% of parameters and offering enhanced interpretability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过一种更可解释且参数高效的方法改进大语言模型（LLM）的安全对齐，因为现有的低秩适应技术（如LoRA）由于安全概念与无关特征语义纠缠而表现不佳。所提出的SAILS方法利用稀疏自编码器（SAE）将表示解耦为单语义特征，并基于这些特征构建可解释的安全子空间来初始化LoRA适配器进行微调。实验结果表明，SAILS在Gemma-2-9B模型上实现了高达99.6%的安全率，超过全参数微调7.4个百分点，与基于强化学习的模型性能相当，同时仅更新0.19%的参数并提供了更好的可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents</div>
<div class="meta-line">Authors: Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Jiyan He, Yunfang Wu</div>
<div class="meta-line">First: 2025-12-24T05:27:53+00:00 · Latest: 2026-01-05T13:23:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20957v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.20957v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一器足矣：面向仓库级大语言模型智能体的强化学习</div>
<div class="mono" style="margin-top:8px">在大型开源软件仓库中，由于规模庞大和结构复杂，准确定位需要修改的文件和函数极具挑战。现有基于大语言模型的方法通常将其视为仓库级检索任务，并依赖多种辅助工具，这忽视了代码执行逻辑且使模型控制复杂化。我们提出RepoNavigator——一种配备单一执行感知工具（跳转至被调用符号定义）的大语言模型智能体。该统一设计既反映了代码执行的实际流程，又简化了工具操作。RepoNavigator通过强化学习对预训练模型进行端到端训练，无需任何闭源蒸馏。实验表明，经强化学习训练的RepoNavigator实现了最先进的性能：7B模型超越14B基线，14B模型优于32B竞品，32B模型甚至超过Claude-3.7等闭源模型。这些结果证实，将单一结构基础工具与强化学习训练相结合，为仓库级问题定位提供了高效且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of locating relevant code in large open-source repositories, where existing LLM-based methods rely on multiple auxiliary tools, complicating control and overlooking execution logic. The authors propose RepoNavigator, an LLM agent that uses a single tool—jumping to the definition of invoked symbols—to mirror actual code execution flow and simplify tool use. It is trained end-to-end with reinforcement learning from a pretrained model, without closed-source distillation. Experimental results show state-of-the-art performance: the 7B model outperforms 14B baselines, the 14B model surpasses 32B competitors, and the 32B model exceeds closed-source models like Claude-3.7, demonstrating that a single, execution-aware tool combined with RL offers an efficient, scalable solution for repository-level issue localization.</div>
<div class="mono" style="margin-top:8px">本文针对大型开源代码库中定位相关文件的挑战，指出现有基于大语言模型的方法依赖多种辅助工具，导致控制复杂且忽略代码执行逻辑。作者提出RepoNavigator智能体，仅使用单一工具——跳转到被调用符号的定义——以反映实际执行流程并简化工具操作。该模型通过强化学习对预训练模型进行端到端训练，无需闭源蒸馏。实验结果表明其达到最先进性能：7B模型优于14B基线，14B模型超越32B竞争对手，32B模型甚至超过Claude-3.7等闭源模型，证实了结合单一结构感知工具与强化学习能为仓库级问题定位提供高效、可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">SIP-BMM: Constructing the Capability--Efficiency Pareto Set for LLMs via Structural Importance Prior Bayesian Model Merging</div>
<div class="meta-line">Authors: Kesheng Chen, Yamin Hu, Zhenqian Zhu, Wenjian Luo, Yiya Diao</div>
<div class="meta-line">First: 2025-12-10T15:32:56+00:00 · Latest: 2026-01-05T12:45:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09972v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.09972v3">PDF</a> · <a href="https://github.com/MiLab-HITSZ/2026-SIPBMM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a Pareto set is pivotal for navigating the capability--efficiency trade-offs in Large Language Models (LLMs). However, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the curse of dimensionality, rendering the search space computationally intractable. To resolve this dichotomy, we propose Structural Importance Prior Bayesian Model Merging (SIP-BMM), a framework that automatically constructs the LLM Pareto set. SIP-BMM renders high-dimensional layer-wise search tractable by introducing an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy. By leveraging a structural importance prior derived from task-vector differences, our method guides SAASBO to automatically identify critical layers, thereby dramatically reducing the effective dimensionality without sacrificing the granularity of full-model control. The entire process is automated within an evolutionary loop driven by the Log-Noisy Expected Hypervolume Improvement ($q$NEHVI) acquisition function. Experiments demonstrate that SIP-BMM discovers a stronger and denser Pareto front than competitive baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/MiLab-HITSZ/2026-SIPBMM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIP-BMM：基于结构重要性先验的贝叶斯模型融合构建大语言模型能力-效率帕累托前沿</div>
<div class="mono" style="margin-top:8px">构建帕累托前沿对于权衡大语言模型（LLM）的能力与效率至关重要，但现有模型融合技术仍存在不足。粗粒度的模型级方法仅能生成稀疏的次优解集，而细粒度的分层方法则受维度灾难困扰，导致搜索空间在计算上不可行。为解决这一矛盾，本文提出结构重要性先验贝叶斯模型融合（SIP-BMM），该框架可自动构建LLM的帕累托解集。SIP-BMM通过引入重要性感知的稀疏轴对齐子空间贝叶斯优化（SAASBO）策略，使高维分层搜索变得可行。该方法利用从任务向量差异中推导的结构重要性先验，引导SAASBO自动识别关键层，从而在保持全模型控制粒度的同时显著降低有效维度。整个流程在由对数噪声期望超体积改进（$q$NEHVI）采集函数驱动的进化循环中自动执行。实验表明，SIP-BMM能比基线方法发现更强且更密集的帕累托前沿，支持根据多样化运行约束进行敏捷模型选择。代码已开源：https://github.com/MiLab-HITSZ/2026-SIPBMM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the inadequacy of existing model merging techniques in constructing a dense Pareto set that effectively navigates the trade-off between capability and efficiency in Large Language Models (LLMs). The proposed method, Structural Importance Prior Bayesian Model Merging (SIP-BMM), introduces an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy guided by a structural importance prior derived from task-vector differences; this automates the identification of critical layers, making high-dimensional layer-wise search tractable within an evolutionary loop driven by a Log-Noisy Expected Hypervolume Improvement acquisition function. The main experimental results demonstrate that SIP-BMM discovers a stronger and denser Pareto front compared to competitive baselines, enabling agile model selection tailored to diverse operational constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有模型融合技术在构建大语言模型能力与效率权衡的密集帕累托前沿方面的不足。所提出的方法——基于结构重要性先验的贝叶斯模型融合（SIP-BMM），引入了一种由任务向量差异导出的结构重要性先验引导的、重要性感知的稀疏轴对齐子空间贝叶斯优化策略；该方法能自动识别关键层，从而在由对数噪声期望超体积改进获取函数驱动的进化循环中，使高维分层搜索变得可行。主要实验结果表明，与竞争基线相比，SIP-BMM能发现更强、更密集的帕累托前沿，从而支持根据多样化操作约束进行敏捷的模型选择。</div>
</details>
</div>
<div class="card">
<div class="title">Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management</div>
<div class="meta-line">Authors: Faizan Ahmed, Aniket Dixit, James Brusey</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2026-01-05T12:35:33+00:00 · Latest: 2026-01-05T12:35:33+00:00</div>
<div class="meta-line">Comments: 6 pages, accepted at NeurIPS workshop 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02061v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02061v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度强化学习中的高阶动作正则化：从连续控制到建筑能源管理</div>
<div class="mono" style="margin-top:8px">深度强化学习智能体常表现出不稳定、高频的控制行为，因能耗过高与机械磨损阻碍实际部署。我们通过高阶导数惩罚系统研究动作平滑性正则化，从连续控制基准的理论理解推进至建筑能源管理的实践验证。在四个连续控制环境中的综合评估表明，三阶导数惩罚（加加速度最小化）能持续实现更优平滑性，同时保持竞争力性能。我们将这些发现延伸至暖通空调控制系统，其中平滑策略使设备切换减少60%，转化为显著运营效益。本研究确立了高阶动作正则化作为强化学习优化与能源关键应用中运行约束间的有效桥梁。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the erratic, high-frequency control behaviors common in deep reinforcement learning agents, which cause excessive energy use and mechanical wear, hindering real-world deployment. The method involves systematically applying higher-order action derivative penalties, particularly focusing on third-order penalties (jerk minimization), to regularize and smooth the agent&#x27;s control policies. The main experimental results, validated across four continuous control benchmarks and a building HVAC system, show that this approach consistently achieves superior smoothness while maintaining competitive task performance, and in the HVAC application, it reduces equipment switching by 60%, yielding significant operational benefits.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决深度强化学习智能体常见的、阻碍实际部署的、导致过度能耗和机械磨损的、不稳定且高频的控制行为。其方法是通过系统性地应用高阶动作导数惩罚，特别是三阶惩罚（急动度最小化），来正则化和平滑智能体的控制策略。主要实验结果在四个连续控制基准测试和一个建筑暖通空调系统中得到验证，表明该方法在保持竞争力的任务性能的同时，始终能实现更优的平滑性，并且在暖通空调应用中，将设备切换次数减少了60%，带来了显著的操作效益。</div>
</details>
</div>
<div class="card">
<div class="title">GDRO: Group-level Reward Post-training Suitable for Diffusion Models</div>
<div class="meta-line">Authors: Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, Hengshuang Zhao</div>
<div class="meta-line">First: 2026-01-05T11:47:18+00:00 · Latest: 2026-01-05T11:47:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02036v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02036v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDRO：适用于扩散模型的群体级奖励后训练方法</div>
<div class="mono" style="margin-top:8px">近期研究将大语言模型的在线强化学习应用于文本到图像的整流流扩散模型以实现奖励对齐，群体级奖励成功使模型与目标奖励对齐，但面临效率低下、依赖随机采样器和奖励破解等挑战。问题在于整流流模型与大语言模型存在本质差异：1）效率方面，在线图像采样耗时显著且主导训练时间；2）随机性方面，整流流在初始噪声固定后具有确定性。针对这些问题并受大语言模型群体级奖励启发，我们设计了群体级直接奖励优化（GDRO）。GDRO是一种结合整流流模型特性的群体级奖励对齐后训练新范式，理论分析表明其支持完全离线训练，节省图像采样的大量时间成本，且独立于扩散采样器，无需通过ODE到SDE近似获取随机性。实验还研究了可能误导评估的奖励破解陷阱，在评估中引入修正分数以同时考虑原始评估奖励与奖励破解趋势。大量实验证明，GDRO通过OCR和GenEval任务的群体离线优化，有效提升扩散模型的奖励分数，同时在缓解奖励破解方面展现出强稳定性和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the inefficiency, stochastic sampler dependency, and reward hacking issues that arise when applying online reinforcement learning from large language models to reward alignment in text-to-image rectified flow diffusion models. The method introduces Group-level Direct Reward Optimization (GDRO), a novel post-training paradigm designed specifically for the deterministic nature of rectified flow models, enabling fully offline training and eliminating the need for stochastic approximations. The main experimental results demonstrate that GDRO effectively and efficiently improves reward scores on OCR and GenEval tasks through group-wise offline optimization, while showing strong stability and robustness in mitigating reward hacking, as validated by a corrected evaluation metric.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决将大型语言模型的在线强化学习应用于文本到图像整流流扩散模型的奖励对齐时，所面临的效率低下、依赖随机采样器和奖励黑客攻击等问题。方法上提出了组级直接奖励优化（GDRO），这是一种专为整流流模型的确定性特性设计的新型后训练范式，支持完全离线训练，无需随机近似。主要实验结果表明，GDRO通过组级离线优化，在OCR和GenEval任务上有效且高效地提升了扩散模型的奖励分数，同时在使用修正评估指标验证下，展现出缓解奖励黑客攻击的强大稳定性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Improved Runtime Guarantees for the SPEA2 Multi-Objective Optimizer</div>
<div class="meta-line">Authors: Benjamin Doerr, Martin S. Krejca, Milan Stanković</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-11-10T14:43:02+00:00 · Latest: 2026-01-05T10:10:07+00:00</div>
<div class="meta-line">Comments: Accepted for AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07150v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.07150v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Together with the NSGA-II, the SPEA2 is one of the most widely used domination-based multi-objective evolutionary algorithms. For both algorithms, the known runtime guarantees are linear in the population size; for the NSGA-II, matching lower bounds exist. With a careful study of the more complex selection mechanism of the SPEA2, we show that it has very different population dynamics. From these, we prove runtime guarantees for the OneMinMax, LeadingOnesTrailingZeros, and OneJumpZeroJump benchmarks that depend less on the population size. For example, we show that the SPEA2 with parent population size $μ\ge n - 2k + 3$ and offspring population size $λ$ computes the Pareto front of the OneJumpZeroJump benchmark with gap size $k$ in an expected number of $O( (λ+μ)n + n^{k+1})$ function evaluations. This shows that the best runtime guarantee of $O(n^{k+1})$ is not only achieved for $μ= Θ(n)$ and $λ= O(n)$ but for arbitrary $μ, λ= O(n^k)$. Thus, choosing suitable parameters -- a key challenge in using heuristic algorithms -- is much easier for the SPEA2 than the NSGA-II.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPEA2多目标优化器改进的运行时保证</div>
<div class="mono" style="margin-top:8px">SPEA2与NSGA-II同为最广泛使用的基于支配关系的多目标进化算法。对于这两种算法，已知的运行时保证与种群规模呈线性关系；NSGA-II存在匹配的下界。通过对SPEA2更复杂的选择机制进行细致研究，我们发现其种群动态存在显著差异。基于此，我们证明了SPEA2在OneMinMax、LeadingOnesTrailingZeros和OneJumpZeroJump基准测试上的运行时保证对种群规模的依赖性更低。例如，我们证明当父代种群规模$μ\ge n - 2k + 3$且子代种群规模为$λ$时，SPEA2计算间隙大小为$k$的OneJumpZeroJump基准帕累托前沿的期望函数评估次数为$O( (λ+μ)n + n^{k+1})$。这表明最佳运行时保证$O(n^{k+1})$不仅能在$μ= Θ(n)$和$λ= O(n)$时实现，对于任意$μ, λ= O(n^k)$同样成立。因此，对于SPEA2而言，选择合适参数（启发式算法应用中的关键挑战）比NSGA-II更为容易。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to provide improved theoretical runtime guarantees for the SPEA2, a prominent multi-objective evolutionary algorithm, motivated by the fact that existing guarantees for it and the NSGA-II are linear in population size, with matching lower bounds known for NSGA-II. The method involves a careful analysis of SPEA2&#x27;s more complex selection mechanism to reveal its distinct population dynamics. The main experimental results are theoretical runtime bounds for benchmarks like OneMinMax and OneJumpZeroJump; for example, on OneJumpZeroJump with gap size k, SPEA2 with parent population size μ ≥ n - 2k + 3 and offspring size λ achieves an expected runtime of O((λ+μ)n + n^{k+1}), demonstrating that the optimal O(n^{k+1}) guarantee holds for a much wider range of parameters (μ, λ = O(n^k)) than for NSGA-II, making parameter selection easier.</div>
<div class="mono" style="margin-top:8px">本文旨在为著名的多目标进化算法SPEA2提供改进的理论运行时间保证，其动机在于现有对SPEA2和NSGA-II的运行时间保证均与种群大小线性相关，且NSGA-II已知存在匹配的下界。研究方法包括仔细分析SPEA2更复杂的选择机制，以揭示其独特的种群动态。主要实验结果是对OneMinMax和OneJumpZeroJump等基准问题的理论运行时间界；例如，在间隙大小为k的OneJumpZeroJump问题上，当父代种群大小μ ≥ n - 2k + 3且子代大小为λ时，SPEA2的期望运行时间为O((λ+μ)n + n^{k+1})，这表明最优的O(n^{k+1})保证在比NSGA-II更广泛的参数范围（μ, λ = O(n^k)）内成立，从而使得参数选择更加容易。</div>
</details>
</div>
<div class="card">
<div class="title">WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</div>
<div class="meta-line">Authors: Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead</div>
<div class="meta-line">First: 2026-01-05T09:35:11+00:00 · Latest: 2026-01-05T09:35:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02439v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent&#x27;s own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebGym：面向视觉网页代理的现实任务规模化训练环境</div>
<div class="mono" style="margin-top:8px">我们推出WebGym，这是迄今最大规模的开源环境，用于训练现实视觉网页代理。真实网站具有非稳态和多样性特征，使得人工或小规模任务集难以支撑稳健的策略学习。WebGym包含近30万个任务，通过标准化评估体系覆盖多样化的真实网站与难度层级。我们采用简洁的强化学习方案训练代理：利用代理自身交互轨迹进行训练，以任务奖励作为学习反馈。为实现强化学习的规模化，我们专门为网页代理开发了高吞吐异步轨迹采样系统，使WebGym的轨迹采样速度较原始实现提升4-5倍。其次，我们通过拓展任务集的广度、深度与规模实现持续性能提升。基于Qwen-3-VL-8B-Instruct强基视觉语言模型在WebGym上进行微调后，在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的代理。该提升具有实质性意义，因为我们的测试集完全由训练阶段未见的网站任务构成，这与多数现有视觉网页代理训练研究形成鲜明对比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for WebGym is to address the insufficiency of artificial or small-scale task sets for training robust visual web agents, given the non-stationary and diverse nature of real websites. The method involves creating a large-scale open-source environment with nearly 300,000 tasks across real-world websites, using a reinforcement learning recipe that trains on agent interaction traces with task rewards as feedback, and implementing a high-throughput asynchronous rollout system to speed up trajectory sampling. The main experimental results show that this system achieves a 4-5x rollout speedup compared to naive implementations, and fine-tuning the Qwen-3-VL-8B-Instruct model on WebGym improves the success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models like GPT-4o and GPT-5-Thinking.</div>
<div class="mono" style="margin-top:8px">WebGym的动机在于解决真实网站的非平稳性和多样性导致人工或小规模任务集不足以训练鲁棒视觉网络代理的问题。其方法包括创建一个大规模开源环境，包含近30万个基于真实网站的任务，采用强化学习方案，利用代理交互轨迹和任务奖励作为反馈进行训练，并开发了一个高吞吐量的异步轨迹采样系统以加速训练。主要实验结果表明，该系统相比简单实现实现了4-5倍的轨迹采样加速，且在WebGym上微调Qwen-3-VL-8B-Instruct模型后，在未见网站任务测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o和GPT-5-Thinking等专有模型的代理。</div>
</details>
</div>
<div class="card">
<div class="title">Distorted Distributional Policy Evaluation for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Ryo Iwaki, Takayuki Osogami</div>
<div class="meta-line">First: 2026-01-05T09:04:10+00:00 · Latest: 2026-01-05T09:04:10+00:00</div>
<div class="meta-line">Comments: The preprint version of the paper accepted to ICONIP2025. The Version of Record is available online at https://link.springer.com/chapter/10.1007/978-981-95-4091-4_35</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01917v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线强化学习中基于扭曲分布的策略评估方法</div>
<div class="mono" style="margin-top:8px">尽管分布强化学习方法在在线场景中表现出色，但在离线场景中的成功仍有限。我们假设现有离线分布强化学习方法的一个关键局限在于其统一低估回报分位数的处理方式。这种统一的悲观性可能导致过于保守的价值估计，最终阻碍泛化能力和性能。为解决这一问题，我们引入了一种称为分位数扭曲的新概念，通过根据支持数据的可用性调整保守程度，实现非均匀的悲观性。该方法基于理论分析并经过实证验证，显示出相较于统一悲观性的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of distributional reinforcement learning (DRL) in offline settings, where existing methods often uniformly underestimate return quantiles, leading to overly conservative value estimates that hinder generalization. To overcome this, the authors propose a novel quantile distortion technique that introduces non-uniform pessimism, adjusting conservatism based on data availability to provide more nuanced value estimates. Experimental results validate the approach, showing improved performance compared to methods relying on uniform pessimism.</div>
<div class="mono" style="margin-top:8px">本文针对分布强化学习在离线场景中的局限性展开研究，指出现有方法通常对回报分位数进行均匀低估，导致价值估计过于保守并阻碍泛化能力。为此，作者提出了一种新颖的分位数扭曲技术，通过基于数据可用性调整保守程度，实现非均匀的悲观估计，从而提供更精细的价值评估。实验结果表明，该方法相比均匀悲观方法取得了更好的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning</div>
<div class="meta-line">Authors: Yuxuan Li, Harshith Reddy Kethireddy, Srijita Das</div>
<div class="meta-line">First: 2026-01-05T08:49:30+00:00 · Latest: 2026-01-05T08:49:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01904v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01904v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.
  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method&#x27;s learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.
  We also find language model&#x27;s noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估基于偏好的强化学习中特征相关噪声</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PbRL）因其适用于奖励函数难以获取的复杂任务而受到关注。然而，若非来自完美示教者，偏好数据常伴随不确定性与噪声。现有研究多聚焦噪声检测，但噪声类型有限且多与观测无关的均匀分布。本研究形式化了目标特征相关噪声的概念，提出了轨迹特征噪声、轨迹相似性噪声、不确定性感知噪声及语言模型噪声等变体。我们在DMControl和Meta-world的复杂连续控制任务中评估了与特定特征相关的噪声。实验表明：在某些特征相关噪声场景下，当前最先进的抗噪PbRL方法性能显著下降；而无显式去噪的PbRL方法在多数场景中反超抗噪方法。我们还发现语言模型噪声与特征相关噪声具有相似特性，可模拟真实人类行为，这呼吁未来需开展针对特征相关噪声的鲁棒性学习研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of noisy preferences in Preference-based Reinforcement Learning (PbRL), motivated by the fact that real-world preferences often contain uncertainty not captured by prior noise models, which typically assume uniform, observation-independent noise. The authors formalize the concept of targeted feature-dependent noise, proposing several variants such as trajectory feature noise and uncertainty-aware noise, and evaluate these in complex continuous control tasks from DMControl and Meta-world. Their experimental results reveal that state-of-the-art noise-robust PbRL methods can suffer significant performance degradation under feature-dependent noise, while simpler methods without explicit denoising surprisingly outperform them in most settings; additionally, they find that noise from language models mimics feature-dependent noise, highlighting the need for more robust approaches.</div>
<div class="mono" style="margin-top:8px">本文针对基于偏好的强化学习（PbRL）中偏好噪声的挑战展开研究，其动机在于现实世界中的偏好常包含不确定性，而先前噪声模型多假设均匀且与观测无关的噪声，未能捕捉此类特征。作者形式化了目标特征依赖噪声的概念，提出了轨迹特征噪声、不确定性感知噪声等多种变体，并在DMControl和Meta-world的复杂连续控制任务中进行了评估。实验结果表明，在特征依赖噪声设置下，先进的抗噪声PbRL方法学习性能显著下降，而无显式去噪的PbRL方法在多数设置中反而表现更优；同时，研究发现语言模型产生的噪声与特征依赖噪声相似，这模拟了真实人类偏好，呼吁未来需开发更鲁棒的学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Convergence of a L2 regularized Policy Gradient Algorithm for the Multi Armed Bandit</div>
<div class="meta-line">Authors: Stefana Anita, Gabriel Turinici</div>
<div class="meta-line">Venue: In: Antonacopoulos, A., et al. (eds) Pattern Recognition. ICPR 2024. Lecture Notes in Computer Science, vol 15326. Springer, Cham (2025)</div>
<div class="meta-line">First: 2024-02-09T13:10:04+00:00 · Latest: 2026-01-05T08:44:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.06388v4">Abs</a> · <a href="https://arxiv.org/pdf/2402.06388v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although Multi Armed Bandit (MAB) on one hand and the policy gradient approach on the other hand are among the most used frameworks of Reinforcement Learning, the theoretical properties of the policy gradient algorithm used for MAB have not been given enough attention. We investigate in this work the convergence of such a procedure for the situation when a $L2$ regularization term is present jointly with the &#x27;softmax&#x27; parametrization. We prove convergence under appropriate technical hypotheses and test numerically the procedure including situations beyond the theoretical setting. The tests show that a time dependent regularized procedure can improve over the canonical approach especially when the initial guess is far from the solution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>L2正则化策略梯度算法在多臂老虎机问题中的收敛性分析</div>
<div class="mono" style="margin-top:8px">尽管多臂老虎机（MAB）与策略梯度方法是强化学习中最常用的两大框架，但针对MAB的策略梯度算法的理论性质尚未得到充分关注。本研究探讨了在采用&#x27;Softmax&#x27;参数化并引入L2正则化项的情况下，该算法的收敛性。我们在适当的技术假设下证明了其收敛性，并对包括超出理论设定范围的情形进行了数值测试。实验表明，时间依赖的正则化方法相较于经典方法具有优势，尤其在初始猜测远离最优解时效果更为显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of theoretical analysis for policy gradient methods applied to the Multi-Armed Bandit (MAB) problem, particularly when using softmax parameterization with L2 regularization. The authors propose a regularized policy gradient algorithm and rigorously prove its convergence under specific technical conditions. Numerical experiments, including tests beyond the theoretical assumptions, demonstrate that a time-dependent regularization schedule can significantly enhance performance compared to the standard approach, especially when initial parameters are poorly chosen.</div>
<div class="mono" style="margin-top:8px">本文针对多臂老虎机问题中策略梯度方法缺乏理论分析的问题展开研究，重点探讨了使用softmax参数化和L2正则化时的算法性质。作者提出了一种正则化策略梯度算法，并在特定技术假设下严格证明了其收敛性。数值实验（包括超出理论设定的测试）表明，采用时间依赖的正则化方案相比标准方法能有效提升性能，尤其在初始参数远离最优解时效果更为显著。</div>
</details>
</div>
<div class="card">
<div class="title">A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning</div>
<div class="meta-line">Authors: Wencheng Cai, Xuchao Gao, Congying Han, Mingqiang Li, Tiande Guo</div>
<div class="meta-line">First: 2025-12-05T14:39:50+00:00 · Latest: 2026-01-05T08:40:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05753v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.05753v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的快速抗干扰认知雷达部署算法</div>
<div class="mono" style="margin-top:8px">认知雷达的快速部署以应对干扰仍是现代战争中的关键挑战，更高效的部署能更快探测目标。现有方法主要基于进化算法，耗时且易陷入局部最优。我们通过神经网络的高效推理克服这些缺陷，提出全新框架：快速抗干扰雷达部署算法（FARDA）。首先将雷达部署问题建模为端到端任务，设计深度强化学习算法求解，开发集成神经模块感知热图信息并设计新型奖励机制。实验表明，本方法在覆盖效果上与进化算法相当，但部署速度提升约7,000倍。消融实验进一步验证了FARDA各组成部分的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for rapid and efficient cognitive radar deployment to counter jamming in modern warfare, where existing evolutionary methods are slow and prone to local optima, this paper proposes the Fast Anti-Jamming Radar Deployment Algorithm (FARDA). The method frames deployment as an end-to-end task, employing deep reinforcement learning with integrated neural modules to perceive heatmap information and a novel reward structure. Experimental results show that FARDA achieves coverage performance similar to evolutionary algorithms while operating approximately 7,000 times faster, with ablation studies validating the contribution of each component.</div>
<div class="mono" style="margin-top:8px">针对现代战争中快速高效部署认知雷达以对抗干扰的需求，现有进化算法耗时且易陷入局部最优，本文提出了快速抗干扰雷达部署算法（FARDA）。该方法将部署问题建模为端到端任务，采用深度强化学习，设计了集成神经模块来感知热图信息并引入新的奖励格式。实验结果表明，FARDA在达到与进化算法相当覆盖性能的同时，部署速度提升了约7,000倍，消融实验进一步证实了算法各组成部分的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation</div>
<div class="meta-line">Authors: Shuta Kikuchi, Shu Tanaka</div>
<div class="meta-line">First: 2026-01-05T07:41:34+00:00 · Latest: 2026-01-05T07:41:34+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01860v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01860v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于二次优化退火因子分解机与MDR评估的高阶上位性检测</div>
<div class="mono" style="margin-top:8px">检测高阶上位性是遗传关联研究中的核心挑战，源于候选位点组合的组合爆炸问题。尽管多因子降维（MDR）是评估上位性的常用方法，但随着位点数量或交互阶数的增加，基于MDR的穷举搜索在计算上变得不可行。本文将该问题定义为黑盒优化问题，并采用二次优化退火因子分解机（FMQA）进行求解。我们提出一种基于FMQA的高效上位性检测方法，其中以MDR计算的分类错误率（CER）作为黑盒目标函数。通过使用预设高阶上位性的模拟病例对照数据集进行实验评估，结果表明所提方法能在有限迭代次数内成功识别不同交互阶数和遗传位点数量的真实上位性。这些结果证明该方法对高阶上位性检测具有高效性和计算可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the computational infeasibility of exhaustive multifactor dimensionality reduction (MDR) searches when detecting high-order genetic epistasis due to combinatorial explosion. The method defines epistasis detection as a black-box optimization problem and solves it using a factorization machine with quadratic optimization annealing (FMQA), where MDR&#x27;s classification error rate serves as the objective function. Experimental results on simulated case-control datasets with predefined high-order epistasis show that the proposed method successfully identifies ground-truth interactions across various orders and locus numbers within limited iterations, demonstrating both effectiveness and computational efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决检测高阶基因上位性时，因组合爆炸导致基于多因子降维（MDR）的穷举搜索计算不可行的问题。方法将上位性检测定义为黑盒优化问题，并采用带二次优化退火的因子分解机（FMQA）进行求解，其中以MDR计算的分类错误率作为目标函数。在具有预定义高阶上位性的模拟病例对照数据集上的实验结果表明，所提方法在有限迭代次数内成功识别了不同交互阶数和基因位点数量的真实上位性，证明了其有效性和计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization</div>
<div class="meta-line">Authors: SB Danush Vikraman, Hannah Abagail, Prasanna Kesavraj, Gajanan V Honnavar</div>
<div class="meta-line">First: 2026-01-05T06:51:08+00:00 · Latest: 2026-01-05T06:51:08+00:00</div>
<div class="meta-line">Comments: 22 pages, 9 figures, includes extensive ablation studies and benchmark comparisons</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01832v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Yukthi Opus (YO), a multi-chain hybrid metaheuristic designed for NP-hard optimization under explicit evaluation budget constraints. YO integrates three complementary mechanisms in a structured two-phase architecture: Markov Chain Monte Carlo (MCMC) for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to enable controlled escape from local minima. A dedicated burn-in phase allocates evaluations to probabilistic exploration, after which a hybrid optimization loop refines promising candidates. YO further incorporates a spatial blacklist mechanism to avoid repeated evaluation of poor regions and a multi-chain execution strategy to improve robustness and reduce sensitivity to initialization.
  We evaluate YO on three benchmarks: the Rastrigin function (5D) with ablation studies, the Traveling Salesman Problem with 50 to 200 cities, and the Rosenbrock function (5D) with comparisons against established optimizers including CMA-ES, Bayesian optimization, and accelerated particle swarm optimization. Results show that MCMC exploration and greedy refinement are critical for solution quality, while simulated annealing and multi-chain execution primarily improve stability and variance reduction. Overall, YO achieves competitive performance on large and multimodal problems while maintaining predictable evaluation budgets, making it suitable for expensive black-box optimization settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Yukthi Opus：一种面向大规模NP难优化问题的多链混合元启发式算法</div>
<div class="mono" style="margin-top:8px">本文提出Yukthi Opus（YO）——一种在显式评估预算约束下针对NP难优化问题设计的多链混合元启发式算法。YO采用结构化双阶段架构集成三种互补机制：马尔可夫链蒙特卡洛（MCMC）实现全局探索，贪婪局部搜索进行开发，以及带自适应重加热的模拟退火实现可控的局部极小值逃逸。专用预热阶段将评估资源分配给概率性探索，随后混合优化循环对潜力解进行精细化。YO进一步引入空间黑名单机制避免重复评估劣质区域，并采用多链执行策略提升鲁棒性、降低对初始化的敏感性。
我们在三个基准测试中评估YO：含消融实验的5维Rastrigin函数、50至200城市的旅行商问题，以及5维Rosenbrock函数（与CMA-ES、贝叶斯优化、加速粒子群优化等经典优化器对比）。结果表明：MCMC探索与贪婪精细化对解质量至关重要，而模拟退火与多链执行主要提升稳定性并降低方差。总体而言，YO在保持可预测评估预算的同时，于大规模多模态问题上取得具有竞争力的性能，适用于高成本黑箱优化场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Yukthi Opus (YO), a multi-chain hybrid metaheuristic motivated by the need to solve large-scale NP-hard optimization problems under strict evaluation budget constraints, such as in expensive black-box settings. The method employs a structured two-phase architecture integrating Markov Chain Monte Carlo for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to escape local minima, alongside a spatial blacklist and multi-chain execution for robustness. Experimental results on benchmarks including the Rastrigin function, Traveling Salesman Problem, and Rosenbrock function demonstrate that YO achieves competitive performance, with MCMC and greedy refinement crucial for solution quality, while simulated annealing and multi-chain strategies enhance stability and reduce variance compared to optimizers like CMA-ES and Bayesian optimization.</div>
<div class="mono" style="margin-top:8px">本文提出了Yukthi Opus（YO），一种多链混合元启发式方法，其动机在于在严格评估预算约束下解决大规模NP难优化问题，适用于昂贵的黑盒优化场景。该方法采用结构化的两阶段架构，整合了马尔可夫链蒙特卡洛进行全局探索、贪婪局部搜索进行利用，以及带自适应重加热的模拟退火以逃离局部极小值，同时结合空间黑名单和多链执行策略以提高鲁棒性。在Rastrigin函数、旅行商问题和Rosenbrock函数等基准测试上的实验结果表明，YO取得了有竞争力的性能，其中MCMC和贪婪细化对解的质量至关重要，而模拟退火和多链策略则增强了稳定性并降低了方差，优于CMA-ES和贝叶斯优化等现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">VAR-MATH: Probing True Mathematical Reasoning in LLMS via Symbolic Multi-Instance Benchmarks</div>
<div class="meta-line">Authors: Jian Yao, Ran Cheng, Kay Chen Tan</div>
<div class="meta-line">First: 2025-07-17T08:10:55+00:00 · Latest: 2026-01-05T05:39:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12885v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.12885v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of LLMs, as measured by standard benchmarks. Yet these gains often persist even when models are trained with flawed signals, such as random or inverted rewards. This raises a fundamental question: do such improvements reflect genuine reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To answer this question, we adopt an evaluation-centric perspective and highlight two critical shortcomings in existing protocols. First, benchmark contamination arises because test problems are publicly available, thereby increasing the risk of data leakage. Second, evaluation fragility results from reliance on single-instance assessments, which are sensitive to stochastic outputs and fail to capture reasoning consistency. These limitations suggest the need for a new evaluation paradigm that can probe reasoning ability beyond memorization and one-off success. As response, we propose VAR-MATH, a symbolic evaluation framework that converts fixed numerical problems into parameterized templates and requires models to solve multiple instantiations of each. This design enforces consistency across structurally equivalent variants, mitigates contamination, and enhances robustness through bootstrapped metrics. We apply VAR-MATH to transform three popular benchmarks, AMC23, AIME24, and AIME25, into their symbolic counterparts, VAR-AMC23, VAR-AIME24, and VAR-AIME25. Experimental results show substantial performance drops for RL-trained models on these variabilized benchmarks, especially for smaller models, with average declines of 47.9\% on AMC23, 58.8\% on AIME24, and 72.9\% on AIME25. These findings indicate that some existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VAR-MATH：通过符号化多实例基准测试探究大语言模型的真实数学推理能力</div>
<div class="mono" style="margin-top:8px">强化学习的最新进展显著提升了大语言模型在标准基准测试中的数学推理能力。然而，即使模型使用随机或反向奖励等有缺陷的信号进行训练，这些提升依然存在。这引发了一个根本性问题：此类改进是否反映了真实的推理能力，抑或仅仅是模型对基准特定模式过拟合的假象？为解答此问题，我们采用评估中心视角，指出现有评估方案存在两个关键缺陷：一是因测试题目公开可用导致的基准污染风险，二是依赖对随机输出敏感且无法捕捉推理一致性的单实例评估所导致的评估脆弱性。这些局限表明需要建立超越记忆与单次成功的新评估范式。为此，我们提出VAR-MATH——一个将固定数值问题转化为参数化模板、要求模型求解每个模板多个实例的符号化评估框架。该设计通过结构等价变体强制一致性检验，缓解基准污染，并采用自助法指标增强鲁棒性。我们将VAR-MATH应用于三个主流基准（AMC23、AIME24、AIME25），生成其符号化版本VAR-AMC23、VAR-AIME24、VAR-AIME25。实验结果显示，经强化学习训练的模型在这些变量化基准上性能大幅下降（小型模型尤为明显），在AMC23、AIME24、AIME25上的平均降幅分别为47.9%、58.8%、72.9%。这表明现有部分强化学习方法依赖表面启发式规则，无法泛化至特定数值形式之外。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by concerns that improvements in large language models&#x27; mathematical reasoning from reinforcement learning may reflect overfitting to benchmark patterns rather than genuine reasoning, exacerbated by data leakage and fragile single-instance evaluations. To address this, the authors propose VAR-MATH, a symbolic evaluation framework that converts fixed numerical problems into parameterized templates, requiring models to solve multiple instantiations to test consistency and mitigate contamination. Experimental results on transformed benchmarks show substantial performance drops for RL-trained models, with average declines of 47.9% to 72.9%, indicating reliance on superficial heuristics rather than robust generalization.</div>
<div class="mono" style="margin-top:8px">本文的动机是担忧大型语言模型通过强化学习在数学推理上的提升可能仅反映了对基准测试模式的过拟合，而非真正的推理能力，这一问题因数据泄露和脆弱的单实例评估而加剧。为此，作者提出了VAR-MATH，一个符号化评估框架，将固定数值问题转换为参数化模板，要求模型解决多个实例以测试一致性并减少污染。在转换后的基准测试上的实验结果显示，经过强化学习训练的模型性能显著下降，平均降幅达47.9%至72.9%，表明这些模型依赖表面启发式方法，而非稳健的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Moments Matter:Stabilizing Policy Optimization using Return Distributions</div>
<div class="meta-line">Authors: Dennis Jabs, Aditya Mohan, Marius Lindauer</div>
<div class="meta-line">First: 2026-01-05T05:27:11+00:00 · Latest: 2026-01-05T05:27:11+00:00</div>
<div class="meta-line">Comments: Workshop paper at RLDM&#x27;25</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01803v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01803v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>矩之要义：利用回报分布稳定策略优化</div>
<div class="mono" style="margin-top:8px">深度强化学习智能体常因环境因素（随机状态转移、初始条件、奖励噪声）与算法因素（小批量采样、探索噪声）的共同作用，习得虽能获得相同回合总回报但行为迥异的策略。在连续控制任务中，微小的参数偏移即可导致步态失稳，这既增加了算法对比的难度，也阻碍了策略向现实世界的迁移。已有研究表明，当策略更新穿越噪声邻域时会产生此类不稳定性，而通过重复采样小批量数据、更新参数θ并测量最终回报所得的更新后回报分布R(θ)的离散程度，正是衡量此类噪声的有效指标。虽然显式约束策略以保持狭窄的R(θ)能提升稳定性，但在高维场景中直接估计R(θ)的计算代价高昂。我们提出一种利用环境随机性来缓解更新引发变异性的替代方案：通过分布评论家建模状态-动作回报分布，进而使用该分布的高阶矩（偏度与峰度）修正PPO的优势函数。通过惩罚极端尾部行为，本方法能阻止策略进入易失稳的参数区间。我们假设，在更新后评论家估值与更新后实际回报严重失配的环境中，标准PPO难以生成狭窄的R(θ)。实验表明，在此类场景中，基于矩的修正方法能使R(θ)收窄，在Walker2D环境中将稳定性提升最高达75%，同时保持相当的评估回报。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the instability in deep reinforcement learning policies, where agents with similar episodic returns can exhibit vastly different behaviors due to environmental and algorithmic noise, complicating algorithm comparison and real-world deployment. To address this, the method introduces a distributional critic to model state-action return distributions and modifies the Proximal Policy Optimization (PPO) advantage function by incorporating higher-order moments (skewness and kurtosis), thereby penalizing extreme tail behaviors to discourage unstable policy updates. Experimental results demonstrate that this approach narrows the post-update return distribution, improving stability by up to 75% in the Walker2D environment while maintaining comparable evaluation returns.</div>
<div class="mono" style="margin-top:8px">本文的动机源于深度强化学习中策略的不稳定性问题，即由于环境和算法噪声，具有相似回合回报的智能体可能表现出截然不同的行为，这给算法比较和实际应用带来了困难。为解决此问题，方法引入了一个分布评论家来建模状态-动作回报分布，并通过结合高阶矩（偏度和峰度）来修改近端策略优化（PPO）的优势函数，从而惩罚极端尾部行为以避免不稳定的策略更新。实验结果表明，该方法能缩小更新后的回报分布，在Walker2D环境中将稳定性提升高达75%，同时保持可比的评估回报。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving</div>
<div class="meta-line">Authors: Qi Wei, Junchao Fan, Zhao Yang, Jianhua Wang, Jingkai Mao, Xiaolin Chang</div>
<div class="meta-line">First: 2026-01-05T05:20:16+00:00 · Latest: 2026-01-05T05:20:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01800v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01800v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏威胁与聚焦防御：面向安全自动驾驶的临界感知鲁棒强化学习</div>
<div class="mono" style="margin-top:8px">强化学习在自动驾驶领域展现出巨大潜力，但其对扰动的脆弱性仍是实际部署的关键障碍。作为主要应对策略，对抗训练通过让自动驾驶智能体在故意引入扰动的对抗环境中学习来提升策略鲁棒性。现有方法通常将交互建模为连续攻击的零和博弈，但此类设计忽略了智能体与对抗者之间的固有不对称性，且未能反映安全关键风险的稀疏特性，导致所获鲁棒性难以满足实际自动驾驶场景需求。为突破这些局限，我们提出临界感知鲁棒强化学习——一种处理自动驾驶中稀疏安全关键风险的新型对抗训练方法。该方法包含两个交互组件：风险暴露对抗器与风险靶向鲁棒智能体。我们将二者交互建模为一般和博弈，使风险暴露对抗器专注于暴露安全关键故障（如碰撞），而风险靶向鲁棒智能体学习在安全与驾驶效率间取得平衡。风险暴露对抗器采用解耦优化机制，在受限预算下更有效识别并利用稀疏的安全关键时刻。这种聚焦攻击必然导致对抗数据稀缺，风险靶向鲁棒智能体通过双经验回放池联合利用正常与对抗经验，并在扰动下强制策略一致性以稳定行为。实验表明，相较于最先进的基线方法，本方法在所有案例中至少降低22.66%的碰撞率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the vulnerability of reinforcement learning (RL) in autonomous driving to adversarial perturbations, noting that existing adversarial training methods often model interactions as zero-sum games with continuous attacks, which overlooks the sparsity of safety-critical risks and the asymmetry between agent and adversary. To overcome this, the authors propose Criticality-Aware Robust RL (CARRL), a novel adversarial training framework consisting of a risk exposure adversary (REA) that focuses on exploiting sparse, safety-critical moments via a decoupled optimization under a constrained budget, and a risk-targeted robust agent (RTRA) that learns to balance safety and efficiency using a dual replay buffer to handle scarce adversarial data and enforce policy consistency. Experimental results show that CARRL reduces collision rates by at least 22.66% compared to state-of-the-art baselines, demonstrating improved robustness in safe autonomous driving scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习在自动驾驶中对抗扰动的脆弱性问题展开研究，指出现有的对抗训练方法通常将交互建模为连续攻击的零和博弈，忽略了安全关键风险的稀疏性以及智能体与对手之间的不对称性。为此，作者提出了关键性感知的鲁棒强化学习（CARRL），这是一种新颖的对抗训练框架，包含一个风险暴露对手（REA）和一个风险目标鲁棒智能体（RTRA）；REA通过解耦优化在受限预算下专注于利用稀疏的安全关键时刻，而RTRA则利用双经验回放池结合良性及对抗性经验来处理数据稀缺问题，并强制策略在扰动下的一致性以稳定行为。实验结果表明，与最先进的基线方法相比，CARRL在所有案例中至少降低了22.66%的碰撞率，证明了其在安全自动驾驶场景中具有更强的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">ScRPO: From Errors to Insights</div>
<div class="meta-line">Authors: Lianrui Li, Dakuan Lu, Jiawei Shao, Xuelong Li</div>
<div class="meta-line">First: 2025-11-08T16:30:44+00:00 · Latest: 2026-01-05T05:19:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06065v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.06065v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to empower large language models with advanced mathematical reasoning capabilities through iterative self-reflection and error correction. The ScRPO framework operates in two distinct phases: (1) Trial-and-error learning stage, where the model is trained via GRPO, and incorrect responses are collected to form an &quot;error pool&quot;; and (2) Self-correction learning stage, which guides the model to introspectively analyze and rectify the reasoning flaws behind its previous errors. Extensive evaluations across challenging mathematical benchmarks, including AIME, AMC, Olympiad, MATH-500, and GSM8k, validate the efficacy of our approach. Using DeepSeek-R1-Distill-Qwen-1.5B and 7B as backbones, ScRPO achieves average accuracies of 64.8% and 77.8%, respectively. This represents a significant improvement of 6.0% and 3.2% over vanilla baselines, consistently outperforming strong post-training methods such as DAPO and GRPO. These findings establish ScRPO as a robust paradigm for enabling autonomous self-improvement in AI systems, particularly in tasks with limited external feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ScRPO：从错误到洞见</div>
<div class="mono" style="margin-top:8px">本文提出自校正相对策略优化（ScRPO），这是一种新颖的强化学习框架，旨在通过迭代式自我反思与错误校正，赋予大语言模型高级数学推理能力。ScRPO框架包含两个独立阶段：（1）试错学习阶段：通过GRPO训练模型，并收集错误响应构建“错误池”；（2）自校正学习阶段：引导模型内省分析并修正先前错误背后的推理缺陷。在AIME、AMC、奥林匹克、MATH-500及GSM8k等挑战性数学基准上的广泛评估验证了本方法的有效性。以DeepSeek-R1-Distill-Qwen-1.5B和7B为骨干模型时，ScRPO分别实现了64.8%和77.8%的平均准确率，较原始基线显著提升6.0%和3.2%，且持续优于DAPO、GRPO等强后训练方法。这些发现确立了ScRPO作为实现AI系统自主自我改进的稳健范式，尤其在外部反馈有限的任务中具有突出价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to enhance large language models&#x27; mathematical reasoning through autonomous self-improvement, this paper introduces Self-correction Relative Policy Optimization (ScRPO), a two-phase reinforcement learning framework. The method first employs a trial-and-error stage using GRPO to collect incorrect responses into an error pool, followed by a self-correction stage where the model learns to introspectively analyze and rectify its reasoning flaws. Experimental results on benchmarks like AIME, AMC, and GSM8k show that ScRPO, using DeepSeek-R1-Distill-Qwen models of 1.5B and 7B parameters, achieves average accuracies of 64.8% and 77.8%, respectively, outperforming vanilla baselines by 6.0% and 3.2% and surpassing methods like DAPO and GRPO, demonstrating its efficacy in tasks with limited external feedback.</div>
<div class="mono" style="margin-top:8px">为了通过自主自我改进来增强大语言模型的数学推理能力，本文提出了自校正相对策略优化（ScRPO），这是一个两阶段的强化学习框架。该方法首先使用GRPO进行试错学习，收集错误响应形成错误池，随后在自校正阶段引导模型内省分析并纠正其推理缺陷。在AIME、AMC和GSM8k等基准测试上的实验结果表明，ScRPO使用1.5B和7B参数的DeepSeek-R1-Distill-Qwen模型，分别实现了64.8%和77.8%的平均准确率，比原始基线提高了6.0%和3.2%，并超越了DAPO和GRPO等方法，证明了其在外部反馈有限任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines</div>
<div class="meta-line">Authors: Rajiv Chaitanya Muttur</div>
<div class="meta-line">First: 2026-01-05T04:39:31+00:00 · Latest: 2026-01-05T04:39:31+00:00</div>
<div class="meta-line">Comments: Presented at ICEdge 2025; nominated for Best Paper Award</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01785v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining &lt;1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SRAS：面向边缘原生RAG管道的轻量级强化学习文档选择器</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）系统通常依赖固定的top-k文档选择机制，这种机制忽略了下游生成质量并带来计算开销。我们提出SRAS（稀疏奖励感知选择器），一种通过强化学习（RL）训练的轻量级文档选择器，专为边缘原生RAG部署设计。与先前假设大内存和高延迟预算的基于RL的检索器不同，SRAS使用近端策略优化（PPO）学习紧凑策略（约0.76MB），并通过结合松弛F1和BERTScore的混合奖励信号进行指导。我们的方法在严格的令牌和计算约束下运行，在CPU上保持&lt;1秒的延迟。SRAS在合成QA基准测试中优于监督和随机选择器，并能泛化到真实世界数据，在SQuAD v2数据集上无需领域特定调优即达到0.8546的BERTScore F1值。此项工作首次证明基于RL的文档选择可实现超轻量化、延迟感知，并能有效用于设备端RAG管道。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of fixed top-k document selection in Retrieval-Augmented Generation (RAG) systems, which neglects generation quality and incurs computational costs, by proposing SRAS, a lightweight reinforcement learning-based document selector designed for edge-native deployment. The method employs Proximal Policy Optimization (PPO) to train a compact policy (~0.76MB) using a hybrid reward signal combining Relaxed F1 and BERTScore, operating under strict token and compute constraints to maintain under 1-second latency on CPU. Experimental results show that SRAS outperforms supervised and random selectors on a synthetic QA benchmark and generalizes to real-world data, achieving a BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning, demonstrating its effectiveness for on-device RAG pipelines.</div>
<div class="mono" style="margin-top:8px">本文针对检索增强生成（RAG）系统中固定 top-k 文档选择机制忽略生成质量并带来计算开销的问题，提出了 SRAS，一种基于强化学习的轻量级文档选择器，专为边缘原生部署设计。该方法使用近端策略优化（PPO）训练一个紧凑的策略（约0.76MB），通过结合 Relaxed F1 和 BERTScore 的混合奖励信号进行指导，在严格的令牌和计算约束下运行，在 CPU 上保持低于1秒的延迟。实验结果表明，SRAS 在合成 QA 基准测试中优于监督和随机选择器，并能泛化到真实世界数据，在 SQuAD v2 上实现了 0.8546 的 BERTScore F1 分数，无需领域特定调优，证明了其在设备端 RAG 管道中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism</div>
<div class="meta-line">Authors: Tianwei Ni, Esther Derman, Vineet Jain, Vincent Taboga, Siamak Ravanbakhsh, Pierre-Luc Bacon</div>
<div class="meta-line">First: 2025-12-04T00:07:08+00:00 · Latest: 2026-01-05T04:22:12+00:00</div>
<div class="meta-line">Comments: Preprint (52 pages, 15 figures) and code is available at https://github.com/twni2016/neubay</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04341v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04341v2">PDF</a> · <a href="https://github.com/twni2016/neubay">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Popular offline reinforcement learning (RL) methods rely on conservatism, either by penalizing out-of-dataset actions or by restricting rollout horizons. In this work, we question the universality of this principle and instead revisit a complementary one: a Bayesian perspective. Rather than enforcing conservatism, the Bayesian approach tackles epistemic uncertainty in offline data by modeling a posterior distribution over plausible world models and training a history-dependent agent to maximize expected rewards, enabling test-time generalization. We first illustrate, in a bandit setting, that Bayesianism excels on low-quality datasets where conservatism fails. We then scale this principle to realistic tasks and show that long-horizon planning is critical for reducing value overestimation once conservatism is removed. To make this feasible, we introduce key design choices for performing and learning from long-horizon rollouts while controlling compounding errors. These yield our algorithm, NEUBAY, grounded in the neutral Bayesian principle. On D4RL and NeoRL benchmarks, NEUBAY generally matches or surpasses leading conservative algorithms, achieving new state-of-the-art on 7 datasets. Notably, it succeeds with rollout horizons of several hundred steps, contrary to dominant practice. Finally, we characterize datasets by quality and coverage, showing when NEUBAY is preferable to conservative methods. Together, we argue NEUBAY lays the foundation for a new practical direction in offline and model-based RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需保守性的长时域模型离线强化学习</div>
<div class="mono" style="margin-top:8px">主流离线强化学习方法依赖保守性原则，或通过惩罚数据集外动作，或通过限制推演时域。本研究质疑该原则的普适性，转而重新审视一种互补视角：贝叶斯方法。贝叶斯方法不强制保守性，而是通过对可能世界模型的后验分布建模，并训练一个依赖历史的智能体以最大化期望奖励，从而处理离线数据中的认知不确定性，实现测试时泛化。我们首先在赌博机场景中证明，贝叶斯方法在保守性方法失效的低质量数据集上表现优异。随后将该原则扩展至实际任务，表明移除保守性后，长时域规划对减少价值高估至关重要。为实现此目标，我们提出了关键设计选择，以执行长时域推演并从中学习，同时控制误差累积。这些设计形成了基于中性贝叶斯原则的算法NEUBAY。在D4RL和NeoRL基准测试中，NEUBAY普遍匹配或超越主流保守算法，在7个数据集上达到新的最优性能。值得注意的是，该算法成功实现了数百步的长时域推演，与主流实践相反。最后，我们通过数据质量和覆盖度对数据集进行表征，阐明NEUBAY优于保守方法的适用场景。综合而言，我们认为NEUBAY为离线与模型强化学习的新实用方向奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the prevailing reliance on conservatism in offline reinforcement learning, proposing instead a Bayesian approach that models epistemic uncertainty through a posterior distribution over world models and trains a history-dependent agent for reward maximization. The method, named NEUBAY, emphasizes long-horizon planning to mitigate value overestimation without conservatism, incorporating design choices to manage compounding errors during rollouts. Experimental results on D4RL and NeoRL benchmarks demonstrate that NEUBAY matches or exceeds state-of-the-art conservative algorithms, achieving top performance on 7 datasets with rollout horizons of hundreds of steps, and analysis identifies dataset conditions where this approach outperforms conservative methods.</div>
<div class="mono" style="margin-top:8px">本文挑战了离线强化学习中普遍依赖保守主义的做法，转而提出一种贝叶斯方法，通过对世界模型的后验分布建模来应对认知不确定性，并训练一个依赖历史的智能体以最大化奖励。该方法名为NEUBAY，强调无需保守主义的长时程规划以减少价值高估，并通过关键设计控制滚动过程中的复合误差。在D4RL和NeoRL基准测试中，NEUBAY达到或超越了领先的保守算法，在7个数据集上取得了最优性能，且能支持数百步的滚动时程，分析还揭示了该方法在特定数据集质量与覆盖条件下优于保守方法。</div>
</details>
</div>
<div class="card">
<div class="title">AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing</div>
<div class="meta-line">Authors: Jiacheng Li, Jianchao Tan, Zhidong Yang, Feiye Huo, Yerui Sun, Yuchen Xie, Xunliang Cai</div>
<div class="meta-line">First: 2025-12-27T04:12:40+00:00 · Latest: 2026-01-05T04:00:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22455v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22455v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AFA-LoRA：通过激活函数退火在LoRA中实现非线性适配</div>
<div class="mono" style="margin-top:8px">低秩适配（LoRA）是一种广泛采用的参数高效微调方法，但其线性适配过程限制了表达能力，导致线性训练与非线性训练之间存在性能差距。为弥合这一差距，我们提出AFA-LoRA，这是一种新颖的训练策略，在保持无缝可合并性的同时为LoRA引入非线性表达能力。其核心创新在于采用退火激活函数，在训练过程中从非线性变换逐步过渡到线性变换，使适配器先获得更强的表征能力，最终收敛为可合并的线性形式。我们在监督微调、强化学习和推测解码任务中验证了该方法，结果表明AFA-LoRA显著缩小了与全参数训练的性能差距，为参数高效适配提供了更强大实用的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of Low-Rank Adaptation (LoRA), a popular parameter-efficient fine-tuning method, whose linear adaptation process restricts its expressive power compared to non-linear training. To bridge this gap, the authors propose AFA-LoRA, a training strategy that introduces non-linear expressivity into LoRA while preserving its seamless mergeability through an annealed activation function that transitions from non-linear to linear during training. Experimental results on supervised fine-tuning, reinforcement learning, and speculative decoding demonstrate that AFA-LoRA reduces the performance gap between LoRA and full-parameter training, enabling a more powerful and practical paradigm for parameter-efficient adaptation.</div>
<div class="mono" style="margin-top:8px">本文针对广泛使用的参数高效微调方法低秩适应（LoRA）的局限性展开研究，其线性适应过程限制了表达能力，与非线性训练存在差距。为弥补这一差距，作者提出了AFA-LoRA，这是一种通过退火激活函数在训练中从非线性过渡到线性的策略，在保持LoRA无缝可合并性的同时引入了非线性表达能力。在监督微调、强化学习和推测解码上的实验结果表明，AFA-LoRA缩小了LoRA与全参数训练之间的性能差距，从而实现了更强大且实用的参数高效适应范式。</div>
</details>
</div>
<div class="card">
<div class="title">Generation of Geodesics with Actor-Critic Reinforcement Learning to Predict Midpoints</div>
<div class="meta-line">Authors: Kazumi Kasaura</div>
<div class="meta-line">Venue: Journal of Machine Learning Research, 26(212):1-36, 2025</div>
<div class="meta-line">First: 2024-07-02T07:06:49+00:00 · Latest: 2026-01-05T03:59:23+00:00</div>
<div class="meta-line">Comments: 17 pages with 8 pages of appendices and references, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.01991v5">Abs</a> · <a href="https://arxiv.org/pdf/2407.01991v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To find the shortest paths for all pairs on manifolds with infinitesimally defined metrics, we introduce a framework to generate them by predicting midpoints recursively. To learn midpoint prediction, we propose an actor-critic approach. We prove the soundness of our approach and show experimentally that the proposed method outperforms existing methods on several planning tasks, including path planning for agents with complex kinematics and motion planning for multi-degree-of-freedom robot arms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于演员-评论家强化学习生成测地线以预测中点</div>
<div class="mono" style="margin-top:8px">为求解无穷小度量流形上所有点对之间的最短路径，我们提出一种通过递归预测中点生成路径的框架。针对中点预测学习，我们采用演员-评论家方法。理论证明了该方法的有效性，实验表明在多项规划任务中（包括复杂运动学智能体路径规划与多自由度机械臂运动规划），本方法性能优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to compute shortest paths (geodesics) on manifolds with infinitesimally defined metrics for all point pairs, this paper introduces a framework that recursively predicts path midpoints to generate these geodesics. The method employs an actor-critic reinforcement learning approach to learn the midpoint prediction function, with theoretical soundness established. Experimental results demonstrate that the proposed method outperforms existing techniques on several planning tasks, including path planning for agents with complex kinematics and motion planning for multi-degree-of-freedom robot arms.</div>
<div class="mono" style="margin-top:8px">本文旨在为具有无穷小定义度量的流形上所有点对计算最短路径（测地线），其动机是提出一个通过递归预测中点来生成这些测地线的框架。方法上，采用行动者-评论家强化学习来学习中点预测函数，并证明了其理论正确性。主要实验结果表明，该方法在多个规划任务上优于现有方法，包括具有复杂运动学的智能体路径规划以及多自由度机械臂的运动规划。</div>
</details>
</div>
<div class="card">
<div class="title">Precision Autotuning for Linear Solvers via Reinforcement Learning</div>
<div class="meta-line">Authors: Erin Carson, Xinye Chen</div>
<div class="meta-line">First: 2026-01-02T15:59:42+00:00 · Latest: 2026-01-05T03:25:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00728v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00728v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的线性求解器精度自动调优</div>
<div class="mono" style="margin-top:8px">我们提出了一种用于线性求解器自适应精度调优的强化学习框架，并可扩展至通用算法。该框架被建模为上下文赌博机问题，通过离散化状态空间下的增量动作价值估计进行求解，以选择计算步骤的最优精度配置，平衡精度与计算效率。为验证其有效性，我们将该框架应用于求解线性方程组 $Ax = b$ 的迭代求精过程。在此应用中，我们的方法基于系统计算特征动态选择精度。具体而言，通过Q表将离散化特征（如近似条件数与矩阵范数）映射至动作（特定步骤的精度配置），并采用ε-贪婪策略进行优化，以最大化平衡精度与计算成本的多目标奖励。实验结果表明，该方法能有效选择精度，在保持与双精度基准相当精度的同时降低计算成本。该框架可泛化至多样化的样本外数据，并为其他数值算法中应用强化学习精度选择提供思路，推动了科学计算中混合精度数值方法的发展。据我们所知，这是首个基于强化学习的精度自动调优研究，并在未见数据集上得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a reinforcement learning framework for autotuning numerical precision in linear solvers, motivated by the need to balance computational efficiency with solution accuracy in scientific computing. The method formulates the problem as a contextual bandit, using a Q-table with discretized state features—such as approximate condition number and matrix norm—to select optimal precision configurations via an epsilon-greedy strategy, maximizing a reward that trades off accuracy and cost. Experimental application to iterative refinement for linear systems shows the framework effectively reduces computational cost while maintaining accuracy comparable to double-precision baselines, and it generalizes well to unseen datasets, offering a novel RL-based approach for mixed-precision methods.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于强化学习的线性求解器数值精度自动调优框架，旨在平衡科学计算中的计算效率与求解精度。该方法将问题构建为上下文赌博机模型，通过离散化状态特征（如近似条件数和矩阵范数）的Q表，利用ε-贪婪策略选择最优精度配置，以优化兼顾精度与计算成本的多目标奖励函数。在线性系统迭代求精的实验应用中，该框架在保持与双精度基准相当精度的同时有效降低了计算成本，并能泛化至未见数据集，为混合精度数值方法提供了创新的强化学习解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA</div>
<div class="meta-line">Authors: Zhichao Wang</div>
<div class="meta-line">First: 2025-10-27T21:18:19+00:00 · Latest: 2026-01-05T03:09:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23868v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23868v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine \textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT minimizes the discrepancy between implicit and explicit reward models. It combines three key ideas: (1) the online multi-response generation and normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the implicit-explicit reward alignment principle of UNA. By jointly normalizing the implicit and explicit rewards, GIFT eliminates an otherwise intractable term that prevents effective use of implicit rewards. This normalization transforms the complex reward maximization objective into a simple mean squared error (MSE) loss between the normalized reward functions, converting a non-convex optimization problem into a convex, stable, and analytically differentiable formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy and thus retains exploration capability. Compared to GRPO, it requires fewer hyperparameters, converges faster, and generalizes better with significantly reduced training overfitting. Empirically, GIFT achieves superior reasoning and alignment performance on mathematical benchmarks while remaining computationally efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GIFT：基于群体相对隐式微调整合GRPO、DPO与UNA</div>
<div class="mono" style="margin-top:8px">本文提出一种新颖的强化学习框架——\textbf{群体相对隐式微调}（GIFT），用于大语言模型对齐。与PPO或GRPO直接最大化累积奖励不同，GIFT通过最小化隐式与显式奖励模型间的差异实现优化。该框架融合三项核心思想：（1）GRPO的在线多响应生成与归一化机制；（2）DPO的隐式奖励建模方法；（3）UNA的隐式-显式奖励对齐原则。通过对隐式与显式奖励进行联合归一化，GIFT消除了阻碍隐式奖励有效使用的复杂项，将原本非凸的奖励最大化目标转化为归一化奖励函数间的均方误差损失，从而构建出凸优化、稳定且可解析求导的数学形式。相较于DPO、UNA等离线方法，GIFT保持在线策略特性并保留探索能力；相比GRPO，其超参数更少、收敛更快、泛化性更强且显著降低训练过拟合。实验表明，GIFT在数学推理基准测试中取得优越的对齐性能，同时保持计算高效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces GIFT, a reinforcement learning framework designed to align large language models by addressing the limitations of existing methods like PPO, GRPO, DPO, and UNA. Its motivation is to avoid directly maximizing cumulative rewards, instead minimizing the discrepancy between implicit and explicit reward models through a combination of GRPO&#x27;s online multi-response generation, DPO&#x27;s implicit reward formulation, and UNA&#x27;s alignment principle. The method jointly normalizes implicit and explicit rewards, transforming the optimization into a convex mean squared error loss, which simplifies training and enhances stability. Experimental results show that GIFT outperforms baselines in reasoning and alignment on mathematical benchmarks, with faster convergence, better generalization, reduced overfitting, and maintained computational efficiency while retaining on-policy exploration.</div>
<div class="mono" style="margin-top:8px">本文提出了GIFT，一种用于对齐大语言模型的强化学习框架，旨在克服PPO、GRPO、DPO和UNA等现有方法的局限性。其动机是通过结合GRPO的在线多响应生成、DPO的隐式奖励公式和UNA的对齐原则，避免直接最大化累积奖励，转而最小化隐式与显式奖励模型之间的差异。该方法对隐式和显式奖励进行联合归一化，将优化问题转化为凸的均方误差损失，从而简化训练并提高稳定性。实验结果表明，GIFT在数学基准测试中实现了更优的推理和对齐性能，具有更快的收敛速度、更好的泛化能力、减少的训练过拟合，并在保持计算效率的同时保留了在线策略探索能力。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-05T02:49:33+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但这些模型仍易受对抗性越狱攻击的影响——攻击者通过精心设计的细微扰动绕过安全机制，触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等问题，难以应用于现实黑盒场景。为此，我们提出基于零阶优化与同步扰动随机逼近（ZO-SPSA）的黑盒越狱攻击方法。ZO-SPSA具备三大优势：（1）通过输入输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低资源需求与GPU内存消耗。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，且扰动不可感知性与白盒方法相当。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱的现实可行性，并暴露了当前LVLMs安全机制的关键缺陷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods that require full model access, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free approach approximates gradients through input-output interactions without needing internal model knowledge, making it model-agnostic and resource-efficient. Experimental results on models like InstructBLIP, LLaVA, and MiniGPT-4 show a high jailbreak success rate of up to 83.0% on InstructBLIP, with imperceptible perturbations, and adversarial examples from MiniGPT-4 demonstrate strong transferability to other LVLMs, achieving an attack success rate of 64.18%, highlighting critical safety weaknesses in LVLMs.</div>
<div class="mono" style="margin-top:8px">本文的动机在于大型视觉语言模型（LVLMs）易受对抗性越狱攻击，而现有需要完整模型访问权限的白盒方法因计算成本高和对抗样本迁移性不足而不切实际。为此，研究提出了一种基于零阶优化和同步扰动随机逼近（ZO-SPSA）的黑盒攻击方法，该方法通过输入输出交互进行梯度近似，无需模型内部知识，具有模型无关性和低资源消耗的优势。在InstructBLIP、LLaVA和MiniGPT-4等模型上的实验结果表明，在InstructBLIP上实现了最高83.0%的越狱成功率，扰动难以察觉，且从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率可达64.18%，揭示了当前LVLMs安全机制的重大缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning</div>
<div class="meta-line">Authors: Pei Yang, Ke Zhang, Ji Wang, Xiao Chen, Yuxin Tang, Eric Yang, Lynn Ai, Bill Shi</div>
<div class="meta-line">First: 2025-11-20T10:12:34+00:00 · Latest: 2026-01-05T01:31:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16202v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.16202v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体协同奖励设计用于增强强化学习中的推理能力</div>
<div class="mono" style="margin-top:8px">我们提出了CRM（多智能体协同奖励模型），该框架通过一个协调的专家评估团队替代单一的黑盒奖励模型，以提升RLHF的鲁棒性和可解释性。传统奖励模型难以同时优化多个可能相互冲突的偏好维度（如事实性、帮助性、安全性），且评分依据缺乏透明度。CRM通过将偏好评估分解为多个领域特定的智能体来解决这些问题，每个智能体生成部分信号，并结合基于排序和嵌入相似性的全局评估器。中央聚合器在每个时间步融合这些信号，平衡逐步正确性、多智能体一致性和重复惩罚等因素，最终生成与标准RL流程兼容的单一训练奖励。策略通过基于优势的更新（如GAE）进行优化，而价值模型回归至聚合奖励，实现在无需额外人工标注的情况下进行多视角奖励塑造。为支持训练与评估，我们引入了rewardBench，这是一个与CRM协同结构对齐的基准测试和训练套件。CRM与rewardBench共同为更透明的奖励建模和更稳定的优化提供了实用化、模块化的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces the Multi-Agent Collaborative Reward Model (CRM) to address limitations in conventional single reward models for reinforcement learning from human feedback (RLHF), which often lack robustness and interpretability when handling multiple, potentially conflicting preference dimensions like factuality and safety. CRM decomposes evaluation into specialized agents that generate partial signals, combined by a centralized aggregator balancing factors such as step-wise correctness and multi-agent agreement to produce a unified training reward compatible with standard RL pipelines. Experimental results, supported by the newly introduced rewardBench benchmark, demonstrate that CRM enhances reward modeling transparency and optimization stability without requiring additional human annotations beyond those used to train the evaluators.</div>
<div class="mono" style="margin-top:8px">本文提出多智能体协作奖励模型（CRM），以解决强化学习人类反馈中传统单一奖励模型在同时优化多个可能冲突的偏好维度（如事实性、安全性）时缺乏鲁棒性和可解释性的问题。该方法将评估分解为多个专业智能体生成部分信号，并通过中央聚合器平衡逐步正确性、多智能体一致性等因素，生成与标准强化学习流程兼容的统一训练奖励。实验结果表明，结合新引入的rewardBench基准，CRM在无需额外人工标注的情况下，提高了奖励建模的透明度和优化稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance</div>
<div class="meta-line">Authors: Ziheng Chen, Minxuan Hu, Jiayu Yi, Wenxi Sun</div>
<div class="meta-line">First: 2026-01-05T01:02:41+00:00 · Latest: 2026-01-05T01:02:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01709v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01709v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>期权对冲的强化学习：静态隐含波动率拟合与缺口感知性能对比</div>
<div class="mono" style="margin-top:8px">我们通过引入风险厌恶与交易成本扩展了Black-Scholes框架下的Q学习器（QLBS），并提出了一种新颖的期权定价复制学习（RLOP）方法。两种方法均与标准强化学习算法完全兼容，并在市场摩擦条件下运行。基于SPY与XOP期权数据的实验表明：自适应QLBS在隐含波动率空间实现了更高的静态定价精度，而RLOP通过降低缺口概率展现出更优的动态对冲性能。这些结果凸显了超越静态拟合评估期权定价模型的重要性，并强调了实际对冲结果的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for option pricing and hedging models that perform well under real-world market frictions, this paper extends the QLBS framework with risk aversion and costs and introduces a new RLOP method, both compatible with standard reinforcement learning. The method involves applying these algorithms to SPY and XOP option data to evaluate static pricing accuracy and dynamic hedging performance. The main experimental results show that Adaptive-QLBS achieves better static fit in implied volatility, while RLOP provides superior dynamic hedging by effectively reducing shortfall probability, underscoring that realized hedging outcomes are a critical metric beyond static model fit.</div>
<div class="mono" style="margin-top:8px">本文的动机是开发能在现实市场摩擦下表现良好的期权定价与对冲模型，为此扩展了QLBS框架并引入了风险规避和交易成本，同时提出了一种新的RLOP方法，两种方法均与标准强化学习算法兼容。研究方法是将这些算法应用于SPY和XOP期权数据，以评估静态定价准确性和动态对冲性能。主要实验结果表明，Adaptive-QLBS在隐含波动率空间实现了更高的静态定价精度，而RLOP通过有效降低短缺概率提供了更优的动态对冲表现，这强调了除静态拟合外，实际对冲结果也是评估期权模型的重要指标。</div>
</details>
</div>
<div class="card">
<div class="title">DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors</div>
<div class="meta-line">Authors: Yash Thesia, Meera Suthar</div>
<div class="meta-line">First: 2026-01-04T22:58:34+00:00 · Latest: 2026-01-04T22:58:34+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01688v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01688v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the &quot;Cold Start&quot; problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator&#x27;s latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique &quot;optimization trajectory&quot; of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiMEx：基于潜在扩散先验突破数据无关模型窃取冷启动壁垒</div>
<div class="mono" style="margin-top:8px">模型窃取攻击对机器学习即服务（MLaaS）构成根本性威胁，攻击者能以极低的训练成本复现专有模型。虽然数据无关模型窃取（DFME）已成为隐蔽的攻击途径，但其始终受限于“冷启动”问题：基于生成对抗网络（GAN）的攻击者需耗费数千次查询才能从随机噪声收敛至有效数据。本文提出DiMEx框架，通过利用预训练潜在扩散模型的丰富语义先验，彻底绕过该初始化壁垒。通过在生成器潜在空间中采用随机嵌入贝叶斯优化（REMBO），DiMEx能即时合成高保真查询，仅用2000次查询即在SVHN数据集上达到52.1%的模型一致性，较当前最优GAN基线提升超16%。为应对这种高语义威胁，我们提出混合状态集成（HSE）防御机制，通过识别潜在空间攻击特有的“优化轨迹”实现检测。实验表明：DiMEx虽能规避静态分布检测器，但HSE可利用其时序特征将攻击成功率压制至21.6%，且延迟可忽略不计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to overcome the cold start problem in data-free model extraction, which hinders GAN-based methods from efficiently generating useful queries, this paper introduces DiMEx, a framework that leverages pre-trained latent diffusion models to provide rich semantic priors. The method employs Random Embedding Bayesian Optimization in the latent space to synthesize high-fidelity queries immediately, bypassing the slow convergence from random noise. Experimental results show that DiMEx achieves 52.1% agreement on SVHN with only 2,000 queries, outperforming state-of-the-art GAN baselines by over 16%, and while it evades static detectors, a proposed Hybrid Stateful Ensemble defense exploits its temporal signature to reduce attack success to 21.6%.</div>
<div class="mono" style="margin-top:8px">本文旨在解决数据自由模型提取中的冷启动问题，该问题阻碍了基于GAN的方法高效生成有效查询，为此提出了DiMEx框架，利用预训练的潜在扩散模型提供丰富的语义先验。该方法在潜在空间中采用随机嵌入贝叶斯优化来立即合成高保真查询，避免了从随机噪声缓慢收敛的过程。实验结果表明，DiMEx仅用2000次查询就在SVHN上实现了52.1%的一致性，比最先进的GAN基线高出16%以上，并且尽管它能规避静态检测器，但提出的混合状态集成防御通过利用其时间特征将攻击成功率降低至21.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives</div>
<div class="meta-line">Authors: Wei Liu, Yaoxin Wu, Yingqian Zhang, Thomas Bäck, Yingjie Fan</div>
<div class="meta-line">First: 2026-01-04T20:57:43+00:00 · Latest: 2026-01-04T20:57:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01665v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多目标神经组合优化的对抗性实例生成与鲁棒训练</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）在解决多目标组合优化问题（MOCOPs）方面展现出巨大潜力，但基于学习的求解器鲁棒性仍未得到充分探索，尤其是在多样且复杂的问题分布上。本文提出一个面向鲁棒性的统一框架，用于处理偏好条件化的MOCOPs DRL求解器。该框架包含：基于偏好的对抗攻击方法，用于生成暴露求解器弱点的困难实例，并通过帕累托前沿质量退化程度量化攻击效果；以及一种防御策略，将难度感知的偏好选择融入对抗训练，以减少对受限偏好区域的过拟合并提升分布外性能。在多目标旅行商问题（MOTSP）、多目标容量约束车辆路径问题（MOCVRP）和多目标背包问题（MOKP）上的实验表明：攻击方法能成功为不同求解器生成困难实例；防御方法显著增强了神经求解器的鲁棒性与泛化能力，在困难及分布外实例上取得更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underexplored robustness of deep reinforcement learning (DRL) solvers for multi-objective combinatorial optimization problems (MOCOPs) across diverse problem distributions. The authors propose a unified robustness framework that includes a preference-based adversarial attack to generate hard instances, which exposes solver weaknesses by degrading Pareto-front quality, and a defense strategy integrating hardness-aware preference selection into adversarial training to reduce overfitting and improve out-of-distribution performance. Experiments on MOTSP, MOCVRP, and MOKP demonstrate that the attack successfully generates hard instances for various solvers, while the defense significantly enhances solver robustness and generalizability on challenging or out-of-distribution cases.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习求解多目标组合优化问题时，在不同问题分布下鲁棒性研究不足的现状，提出一个统一的鲁棒性框架。该框架包含一种基于偏好的对抗攻击方法，用于生成暴露求解器弱点的困难实例，并通过帕累托前沿质量下降量化攻击效果；同时引入一种防御策略，将难度感知的偏好选择融入对抗训练，以减少对受限偏好区域的过拟合并提升分布外性能。在多目标旅行商问题、多目标容量约束车辆路径问题和多目标背包问题上的实验表明，该攻击方法能成功为不同求解器生成困难实例，而防御方法则显著增强了神经求解器的鲁棒性和泛化能力，在困难或分布外实例上表现优异。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
