<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-31 03:26</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251231_0326</div>
    <div class="row"><div class="card">
<div class="title">Training AI Co-Scientists Using Rubric Rewards</div>
<div class="meta-line">Authors: Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse</div>
<div class="meta-line">First: 2025-12-29T18:59:33+00:00 · Latest: 2025-12-29T18:59:33+00:00</div>
<div class="meta-line">Comments: 11 pages in the main paper, total 119 including sample outputs in the Appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23707v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于评分准则奖励训练AI科研助手</div>
<div class="mono" style="margin-top:8px">AI科研助手正逐渐成为协助人类研究者实现科研目标的工具，其核心功能是根据既定目标与约束条件生成研究方案。这些方案可供研究者进行头脑风暴，或经进一步优化后实施。然而，当前语言模型在生成完全符合约束条件与隐性需求的研究方案方面仍存在困难。本研究探索如何利用海量现有科研文献训练语言模型以生成更优质的研究方案。我们通过自动提取多领域论文中的研究目标及目标特异性评分准则，构建了可扩展的多样化训练语料库。随后采用基于自评分的强化学习方法训练研究方案生成模型：训练过程中由初始策略的冻结副本担任评分器，评分准则形成的生成器-验证器差异使模型能在无需外部人工监督的情况下持续优化。为验证该方法，我们组织机器学习领域专家开展了225小时的人工评估。专家在70%的研究目标中更倾向于我们微调的Qwen3-30B-A3B模型生成的方案，并对84%的自动提取目标特异性评分准则表示认可。为评估泛化能力，我们将该方法扩展至医学论文及arXiv预印本的研究目标，并采用前沿模型评审团进行评估。微调后模型获得12-22%的相对性能提升，并展现出显著的跨领域泛化能力，即使在医学研究等难以获取执行反馈的问题场景中同样有效。这些发现共同证明，这种可扩展的自动化训练方法具有提升通用AI科研助手能力的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for AI co-scientists that can generate high-quality research plans adhering to constraints, this work develops a method to train language models using rubric rewards derived from existing research papers. The approach automatically extracts research goals and grading rubrics from papers across domains, then employs reinforcement learning with self-grading, where a frozen initial policy acts as a grader to create a generator-verifier gap for unsupervised improvement. Experimental results from a human expert study on machine learning goals show that the finetuned Qwen3-30B-A3B model&#x27;s plans are preferred 70% of the time over the initial model, with 84% of extracted rubrics approved, and the method generalizes to medical and arXiv domains, yielding 12-22% relative improvements in model evaluations.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升AI科研助手生成符合约束的研究计划的能力，其动机源于当前语言模型在此任务上的不足。方法上，通过从多领域论文中自动提取研究目标和评分标准构建训练语料，并采用基于自我评分的强化学习进行训练，其中冻结的初始策略作为评分者，形成生成器-验证器差距以实现无监督改进。实验结果表明，在机器学习目标的人类专家评估中，微调后的Qwen3-30B-A3B模型生成的研究计划在70%的情况下优于初始模型，84%的自动提取评分标准获得认可，且该方法能推广到医学和arXiv领域，在模型评估中取得12-22%的相对提升，证明了其跨域有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization</div>
<div class="meta-line">Authors: Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</div>
<div class="meta-line">First: 2025-10-09T17:58:07+00:00 · Latest: 2025-12-29T18:55:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08554v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08554v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce Group Diffusion Policy Optimization (GDPO), a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过群组扩散策略优化提升扩散语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）通过迭代优化实现并行、顺序无关的文本生成，为自回归大语言模型（LLMs）提供了灵活的替代方案。然而，由于似然函数难以处理，将强化学习（RL）微调应用于DLMs仍是一个开放挑战。开创性工作如diffu-GRPO通过单步解掩码估计词元级似然，虽计算高效但存在严重偏差。更理论完备的基础在于序列级似然，其中证据下界（ELBO）可作为替代目标。然而，尽管存在清晰的数学关联，基于ELBO的方法因似然评估成本过高而应用有限。本研究重新审视ELBO估计，解析其方差来源。该分解启发了通过沿关键维度进行快速确定性积分近似来降低方差。基于此，我们提出群组扩散策略优化（GDPO）——一种专为DLMs设计的RL算法。GDPO采用简洁高效的半确定性蒙特卡洛方案，缓解原始双重蒙特卡洛采样下ELBO估计器的方差爆炸问题，在严格评估预算下构建了可证明更低方差的估计器。实验表明，GDPO在多数数学、推理和代码基准测试中，相比预训练检查点持续提升性能，并优于当前先进基线方法diffu-GRPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fine-tuning diffusion language models (DLMs) with reinforcement learning, which is difficult due to their intractable likelihood. The authors propose Group Diffusion Policy Optimization (GDPO), a method that reduces variance in estimating the evidence lower bound (ELBO) by using semi-deterministic Monte Carlo schemes for more efficient integral approximations. Experimental results show that GDPO consistently improves pretrained checkpoints and outperforms the baseline diffu-GRPO on most math, reasoning, and coding benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对扩散语言模型因似然函数难以处理而难以进行强化学习微调的问题，提出了组扩散策略优化方法。该方法通过半确定性蒙特卡洛方案降低证据下界估计的方差，实现了更高效的积分近似。实验结果表明，该方法在多数数学、推理和代码基准测试中优于现有基线，并持续提升了预训练模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Bellman Calibration for V-Learning in Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Lars van der Laan, Nathan Kallus</div>
<div class="meta-line">First: 2025-12-29T18:52:18+00:00 · Latest: 2025-12-29T18:52:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23694v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23694v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model&#x27;s predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线强化学习中V学习的贝尔曼校准</div>
<div class="mono" style="margin-top:8px">我们提出迭代贝尔曼校准——一种简单、模型无关、后处理的校准方法，用于无限时域马尔可夫决策过程中的离策略价值预测校准。贝尔曼校准要求具有相似预测长期回报的状态，在目标策略下展现出符合贝尔曼方程的单步回报。通过将拟合的贝尔曼目标重复回归到模型预测上，并采用双重稳健伪结果处理离策略数据，我们将经典直方图校准与等渗校准适配到动态反事实场景中。这产生了一种可应用于任意价值估计器的一维拟合价值迭代方案。我们的分析在弱假设条件下为校准和预测提供了有限样本保证，且关键地无需贝尔曼完备性或可实现性假设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable off-policy value estimation in infinite-horizon settings without stringent model assumptions, this paper introduces Iterated Bellman Calibration, a model-agnostic post-hoc method. The approach calibrates value predictions by ensuring states with similar predicted long-term returns have one-step returns consistent with the Bellman equation under a target policy, adapting histogram and isotonic calibration techniques to dynamic, counterfactual data via repeated regression with doubly robust pseudo-outcomes. Experimental results, supported by finite-sample guarantees, demonstrate that the method effectively improves calibration and prediction accuracy without requiring Bellman completeness or realizability, making it applicable to any value estimator.</div>
<div class="mono" style="margin-top:8px">本文针对无限时域场景中无需严格模型假设的可靠离策略价值估计需求，提出了迭代贝尔曼校准这一模型无关的后处理方法。该方法通过确保具有相似预测长期回报的状态在目标策略下符合贝尔曼方程的一步回报，来校准价值预测，并利用双稳健伪结果进行重复回归，将直方图和等渗校准技术适配于动态反事实数据。实验结果表明，该方法在不需要贝尔曼完备性或可实现性的前提下，有效提升了校准和预测准确性，且适用于任何价值估计器，并提供了有限样本的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</div>
<div class="meta-line">Authors: Deniz Akdemir</div>
<div class="meta-line">First: 2025-12-29T17:21:44+00:00 · Latest: 2025-12-29T17:21:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23617v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23617v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing &quot;negative transfer&quot; that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam&#x27;s theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Le Cam畸变：一种基于决策理论的鲁棒迁移学习框架</div>
<div class="mono" style="margin-top:8px">分布偏移是现实世界机器学习中的核心挑战。主流范式——无监督域适应（UDA）——通过对称散度最小化强制特征不变性以对齐源域和目标域表示[Ganin et al., 2016]。我们证明这种方法存在根本缺陷：当域信息量不均等时（例如高质量传感器与低质量传感器），严格不变性必然导致信息破坏，引发‘负迁移’现象，在安全关键应用中可能造成灾难性后果[Wang et al., 2019]。
我们提出基于Le Cam统计实验理论[Le Cam, 1986]的决策理论框架，通过构造性近似用定向可模拟性替代对称不变性。我们引入以缺陷距离$δ(E_1, E_2)$量化的Le Cam畸变，作为可模拟性条件下迁移风险的严格上界。该框架通过学习从源域模拟目标域的内核，实现不损害源域性能的迁移。在五项实验（基因组学、视觉、强化学习）中，Le Cam畸变实现：（1）HLA基因组学中接近完美的频率估计（相关性$r=0.999$，媲美经典方法），（2）CIFAR-10图像分类中源域效用零损失（准确率保持81.2%，而CycleGAN下降34.7%），（3）强化学习控制中安全策略迁移（基于不变性的方法出现灾难性崩溃）。Le Cam畸变为医疗影像、自主系统和精准医疗等无法承受负迁移的领域，提供了首个风险可控的迁移学习理论框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of unsupervised domain adaptation (UDA) which can cause catastrophic negative transfer when enforcing strict feature invariance between unequally informative domains, this paper proposes a decision-theoretic framework based on Le Cam&#x27;s theory. The method replaces symmetric invariance with directional simulability, introducing Le Cam Distortion quantified by the Deficiency Distance to provide a rigorous upper bound for transfer risk. Experimental results across genomics, vision, and reinforcement learning demonstrate near-perfect frequency estimation in HLA genomics, zero source utility loss in CIFAR-10 classification compared to significant drops in invariance-based methods, and safe policy transfer in RL where previous methods fail catastrophically.</div>
<div class="mono" style="margin-top:8px">本文的动机在于无监督域适应（UDA）在强制不同信息量的域之间进行严格特征对齐时，可能导致灾难性的负迁移。为此，研究基于Le Cam统计实验理论提出了一个决策理论框架，其方法是用方向可模拟性取代对称不变性，并引入以缺陷距离量化的Le Cam失真作为迁移风险的严格上界。在基因组学、视觉和强化学习等五个实验中的主要结果表明：该方法在HLA基因组学中实现了近乎完美的频率估计，在CIFAR-10图像分类中保持了源域效用零损失，并在强化学习控制中实现了安全策略迁移，而基于不变性的方法则出现了灾难性失效。</div>
</details>
</div>
<div class="card">
<div class="title">PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</div>
<div class="meta-line">Authors: Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang</div>
<div class="meta-line">First: 2025-12-29T15:34:27+00:00 · Latest: 2025-12-29T15:34:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23545v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23545v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PathFound：一种激活循证病理诊断的智能体多模态模型</div>
<div class="mono" style="margin-top:8px">近期的病理基础模型在视觉表征学习和多模态交互方面取得了显著进展。然而，大多数模型仍依赖静态推理范式，即对全切片图像进行一次处理以生成预测，在诊断不明确时缺乏重新评估或针对性证据获取。这与临床诊断流程形成对比，后者通过重复切片观察和进一步检查请求来完善假设。我们提出PathFound，一种旨在支持病理诊断中循证推理的智能体多模态模型。PathFound整合了病理视觉基础模型、视觉语言模型和强化学习训练推理模型的能力，通过初始诊断、证据搜寻和最终决策三个阶段，执行主动信息获取和诊断优化。在多个大型多模态模型中，采用此策略持续提升了诊断准确性，表明循证工作流程在计算病理学中的有效性。其中，PathFound在多样化临床场景中实现了最先进的诊断性能，并展现出发现细微特征（如核特征与局部浸润）的强大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the gap between static inference in existing pathological foundation models and the iterative, evidence-seeking workflow of clinical diagnosis, this paper introduces PathFound, an agentic multimodal model designed to activate evidence-seeking pathological diagnosis. The method integrates pathological visual foundation models, vision-language models, and reinforcement learning-trained reasoning models to proactively acquire information and refine diagnoses through stages of initial assessment, evidence-seeking, and final decision. Experimental results show that adopting this agentic strategy consistently improves diagnostic accuracy across several large multimodal models, with PathFound achieving state-of-the-art performance in diverse clinical scenarios and demonstrating strong capability in identifying subtle pathological details like nuclear features and local invasions.</div>
<div class="mono" style="margin-top:8px">本文的动机源于现有病理基础模型的静态推理范式与临床诊断中迭代式、寻求证据的工作流程之间存在差距，因此提出了PathFound，一种旨在激活寻求证据的病理诊断的智能体多模态模型。该方法整合了病理视觉基础模型、视觉语言模型和通过强化学习训练得到的推理模型，通过初始诊断、证据寻求和最终决策三个阶段主动获取信息并优化诊断。实验结果表明，采用这种智能体策略在多个大型多模态模型中持续提升了诊断准确性，其中PathFound在多样化的临床场景中实现了最先进的诊断性能，并展现出在识别细胞核特征和局部浸润等细微病理细节方面的强大潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Expressive Temporal Specifications for Reward Monitoring</div>
<div class="meta-line">Authors: Omar Adalat, Francesco Belardinelli</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-16T22:28:30+00:00 · Latest: 2025-12-29T15:04:16+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12808v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.12808v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于奖励监控的表达性时序规约</div>
<div class="mono" style="margin-top:8px">在强化学习中，如何设计信息丰富且密集的奖励函数仍是核心挑战，因其直接影响智能体训练效率。本研究利用有限迹上定量线性时序逻辑（$\text{LTL}_f[\mathcal{F}]$）的表达能力，合成为运行时可观测状态轨迹生成密集奖励流的监控器。通过在训练期间提供细粒度反馈，这些监控器能引导智能体达成最优行为，并缓解长时序决策中因当前主流布尔语义导致的奖励稀疏问题。本框架与算法无关，仅依赖状态标注函数，天然支持非马尔可夫性质的规约。实验表明，定量监控器始终优于布尔监控器，且在不同环境中能进一步提升任务完成的量化指标并缩短收敛时间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of sparse rewards in Reinforcement Learning by proposing a method to generate dense, informative reward signals using quantitative Linear Temporal Logic on finite traces (LTL_f[F]). This approach synthesizes reward monitors that evaluate state trajectories at runtime, providing nuanced feedback to guide agents, especially in long-horizon tasks where Boolean reward specifications often lead to sparse rewards. The framework is algorithm-agnostic, relying only on a state labeling function, and can specify non-Markovian properties. Experimental results demonstrate that these quantitative monitors consistently outperform or subsume Boolean monitors, improving task completion metrics and reducing convergence time across various environments.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中奖励稀疏的挑战，提出了一种利用有限轨迹上的定量线性时序逻辑（LTL_f[F]）来生成密集、信息丰富奖励信号的方法。该方法通过合成奖励监控器，在运行时评估状态轨迹，为智能体提供细致反馈，以指导其在长周期决策任务中的行为，克服了当前主流布尔语义下奖励稀疏的问题。该框架与算法无关，仅依赖于状态标记函数，并能自然指定非马尔可夫性质。实验结果表明，这种定量监控器在多种环境中均能涵盖并优于布尔监控器，有效提升了任务完成度的量化指标并减少了收敛时间。</div>
</details>
</div>
<div class="card">
<div class="title">Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua</div>
<div class="meta-line">First: 2025-12-29T14:50:23+00:00 · Latest: 2025-12-29T14:50:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23515v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23515v1">PDF</a> · <a href="https://github.com/FinStep-AI/Alpha-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alpha-R1：基于强化学习与大语言模型推理的阿尔法因子筛选</div>
<div class="mono" style="margin-top:8px">在非平稳市场中，信号衰减与机制转换对数据驱动的投资策略构成持续挑战。传统时间序列与机器学习方法主要依赖历史相关性，当经济环境变化时往往难以泛化。尽管大语言模型在处理非结构化信息方面具备强大能力，但其通过显式经济推理支持量化因子筛选的潜力尚未得到充分探索。现有基于因子的方法通常将阿尔法因子简化为数值时间序列，忽略了决定因子经济相关性的语义逻辑。我们提出Alpha-R1——一个通过强化学习训练的80亿参数推理模型，用于情境感知的阿尔法因子筛选。该模型通过分析因子逻辑与实时新闻，评估变化市场条件下的阿尔法相关性，并依据情境一致性选择性地激活或停用因子。跨多资产池的实证结果表明，Alpha-R1持续超越基准策略，并对阿尔法衰减表现出更强的鲁棒性。完整实现与资源详见https://github.com/FinStep-AI/Alpha-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of alpha decay and regime shifts in non-stationary financial markets, where traditional data-driven strategies relying on historical correlations often fail to generalize. To overcome this, the authors propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning that explicitly incorporates economic reasoning by processing factor logic and real-time news to contextually evaluate and selectively activate or deactivate alphas. Experimental results across multiple asset pools demonstrate that Alpha-R1 consistently outperforms benchmark strategies and shows enhanced robustness against alpha decay.</div>
<div class="mono" style="margin-top:8px">本文针对非平稳金融市场中阿尔法衰减和机制转换的挑战，传统依赖历史相关性的数据驱动策略往往难以泛化。为此，作者提出了Alpha-R1，这是一个通过强化学习训练的80亿参数推理模型，它通过处理因子逻辑和实时新闻，结合经济推理来上下文评估并选择性激活或停用阿尔法因子。在多个资产池中的实验结果表明，Alpha-R1持续优于基准策略，并展现出对阿尔法衰减更强的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization</div>
<div class="meta-line">Authors: Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink</div>
<div class="meta-line">First: 2025-12-29T14:32:34+00:00 · Latest: 2025-12-29T14:32:34+00:00</div>
<div class="meta-line">Comments: 16 page,10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23493v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23493v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this article, we consider an industrial internet of things (IIoT) network supporting multi-device dynamic ultra-reliable low-latency communication (URLLC) while the channel state information (CSI) is imperfect. A joint link adaptation (LA) and device scheduling (including the order) design is provided, aiming at maximizing the total transmission rate under strict block error rate (BLER) constraints. In particular, a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method is proposed, which determines the device served order sequence and the corresponding modulation and coding scheme (MCS) adaptively based on the imperfect CSI. Note that the imperfection of CSI, error sample imbalance in URLLC networks, as well as the parameter sensitivity nature of the TD3 algorithm likely diminish the algorithm&#x27;s convergence speed and reliability. To address such an issue, we proposed a BO based training mechanism for the convergence speed improvement, which provides a more reliable learning direction and sample selection method to track the imbalance sample problem. Via extensive simulations, we show that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向URLLC工业物联网网络的联合链路自适应与设备调度方法：一种基于深度强化学习及贝叶斯优化的策略</div>
<div class="mono" style="margin-top:8px">本文研究支持多设备动态超可靠低时延通信的工业物联网网络，在信道状态信息不完美的条件下，提出一种联合链路自适应与设备调度（含服务顺序）的设计方案，旨在严格误块率约束下最大化总传输速率。特别地，提出一种基于贝叶斯优化的双延迟深度确定性策略梯度方法，该方法依据不完美的信道状态信息自适应确定设备服务顺序序列及相应的调制编码方案。需注意，信道状态信息的不完美性、URLLC网络中的误差样本不平衡问题，以及TD3算法固有的参数敏感性，均可能降低算法的收敛速度与可靠性。为此，我们提出一种基于贝叶斯优化的训练机制以提升收敛速度，通过提供更可靠的学习方向与样本选择方法来追踪不平衡样本问题。大量仿真结果表明，相较于现有方案，所提算法实现了更快的收敛速度与更高的总速率性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing multi-device dynamic ultra-reliable low-latency communication (URLLC) in industrial IoT networks under imperfect channel state information (CSI). The authors propose a joint link adaptation and device scheduling design, employing a Bayesian optimization-driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method to adaptively determine device serving order and modulation and coding schemes, aiming to maximize total transmission rate under strict block error rate constraints. Experimental results from extensive simulations demonstrate that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.</div>
<div class="mono" style="margin-top:8px">本文针对工业物联网网络中信道状态信息不完善条件下的多设备动态超可靠低时延通信优化问题展开研究。作者提出了一种联合链路自适应和设备调度的设计方案，采用基于贝叶斯优化的双延迟深度确定性策略梯度方法，自适应地确定设备服务顺序及调制编码方案，以在严格误块率约束下最大化总传输速率。大量仿真实验结果表明，与现有解决方案相比，所提算法实现了更快的收敛速度和更高的总速率性能。</div>
</details>
</div>
<div class="card">
<div class="title">RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation</div>
<div class="meta-line">Authors: Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Jiakun Du, Xiangyuan Wang, Xu Fu, Letong Shi, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang</div>
<div class="meta-line">First: 2025-09-19T13:24:17+00:00 · Latest: 2025-12-29T14:13:21+00:00</div>
<div class="meta-line">Comments: GitHub Repo: https://github.com/RLinf/RLinf</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15965v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.15965v2">PDF</a> · <a href="https://github.com/RLinf/RLinf">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker&#x27;s adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving $1.07\times-2.43\times$ speedup in end-to-end training throughput.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLinf：基于宏微观流程转换的灵活高效大规模强化学习系统</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在推动通用人工智能、智能体与具身智能发展方面展现出巨大潜力。然而，RL工作流固有的异构性与动态性常导致现有系统硬件利用率低、训练速度慢。本文提出RLinf——一个基于关键观察的高性能RL训练系统：高效RL训练的主要瓶颈在于系统灵活性。为最大化灵活性与效率，RLinf构建于名为“宏微观流程转换（M2Flow）”的新型RL系统设计范式之上，该范式能在时空维度自动解构易于编排的高层RL工作流，并将其重组为优化执行流。依托RLinf工作节点的自适应通信能力，我们设计了上下文切换与弹性流水线技术实现M2Flow转换，并采用性能剖析驱动的调度策略生成最优执行计划。在推理RL与具身RL任务上的广泛实验表明，RLinf持续优于现有先进系统，端到端训练吞吐量提升达1.07倍至2.43倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for RLinf stems from the inefficiencies in reinforcement learning (RL) training caused by heterogeneous and dynamic workflows, which lead to low hardware utilization and slow training speeds. The method introduces a novel system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically decomposes high-level RL workflows temporally and spatially and recomposes them into optimized execution flows, enhanced by adaptive communication, context switching, elastic pipelining, and a profiling-guided scheduler. Experimental results on reasoning and embodied RL tasks show that RLinf consistently outperforms state-of-the-art systems, achieving a 1.07× to 2.43× speedup in end-to-end training throughput.</div>
<div class="mono" style="margin-top:8px">RLinf的动机源于强化学习训练中因工作流异构和动态性导致的硬件利用率低和训练速度慢的问题。该方法提出了一种名为宏到微流转换的新系统设计范式，能自动在时间和空间维度分解高级RL工作流并重组为优化执行流，辅以自适应通信、上下文切换、弹性流水线和基于性能分析的调度策略。在推理和具身RL任务上的实验结果表明，RLinf持续优于现有先进系统，实现了端到端训练吞吐量1.07倍至2.43倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation</div>
<div class="meta-line">Authors: Toqeer Ali Syed, Mohammad Riyaz Belgaum, Salman Jan, Asadullah Abdullah Khan, Saad Said Alqahtani</div>
<div class="meta-line">First: 2025-12-29T14:06:09+00:00 · Latest: 2025-12-29T14:06:09+00:00</div>
<div class="meta-line">Comments: Conference paper, accept in ACCA IEEE Bahrain</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23480v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23480v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, reinforcement learning (RL), and multi-agent coordination. The suggested system utilizes specialized security agents coordinated with the help of LangChain and LangGraph, communicates with actual CI/CD environments with the Model Context Protocol (MCP), and documents all the observations and actions in a blockchain security ledger to ensure integrity and auditing. Reinforcement learning can be used to achieve adaptive mitigation strategies that consider the balance between security effectiveness and the operational overhead, and LLMs can be used to achieve semantic vulnerability analysis, as well as explainable decisions. This framework is tested based on simulated pipelines, as well as, actual world CI/CD integrations on GitHub Actions and Jenkins, including injection attacks, insecure deserialization, access control violations, and configuration errors. Experimental outcomes indicate better detection accuracy, shorter mitigation latency and reasonable build-time overhead than rule-based, provenance only and RL only baselines. These results show that agentic AI can facilitate the transition to self defending, proactive software supply chains rather than reactive verification ones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向软件供应链安全自主防御的智能体人工智能：从溯源到漏洞缓解的超越</div>
<div class="mono" style="margin-top:8px">软件供应链攻击日益聚焦于可信开发与交付流程，传统构建后完整性机制已不适用。现有框架如SLSA、SBOM和in toto主要提供溯源与可追溯性，但缺乏主动识别和消除软件生产漏洞的能力。本文提出一种基于自主软件供应链安全的智能体人工智能（AI）框架，融合大型语言模型（LLM）推理、强化学习（RL）与多智能体协同。该系统利用LangChain和LangGraph协调专业化安全智能体，通过模型上下文协议（MCP）与实际CI/CD环境交互，并将所有观测与行动记录于区块链安全账本以确保完整性与可审计性。强化学习用于实现兼顾安全效能与运维开销的自适应缓解策略，LLM则支持语义漏洞分析与可解释决策。该框架在模拟流水线及GitHub Actions与Jenkins的真实CI/CD环境中进行测试，涵盖注入攻击、不安全反序列化、访问控制违规及配置错误等场景。实验结果表明，相较于基于规则、仅溯源及仅强化学习的基线方法，本框架在检测精度、缓解延迟及构建开销方面均表现更优。这些结果证明智能体AI可推动软件供应链从被动验证向主动自主防御转型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing sophistication of software supply chain attacks that bypass traditional post-build integrity checks, this paper proposes an agentic AI framework for autonomous defense that moves beyond passive provenance tracking to active vulnerability mitigation. The method integrates large language models for semantic vulnerability analysis, reinforcement learning for adaptive mitigation strategies that balance security and operational costs, and multi-agent coordination via LangChain/LangGraph, with actions logged on a blockchain ledger for auditability and interfaced with real CI/CD pipelines using the Model Context Protocol. Experimental evaluation on simulated and real-world CI/CD environments (GitHub Actions, Jenkins) against attacks like code injection and insecure deserialization shows the framework achieves higher detection accuracy, lower mitigation latency, and acceptable build-time overhead compared to rule-based, provenance-only, and RL-only baselines, demonstrating a shift toward proactive, self-defending software supply chains.</div>
<div class="mono" style="margin-top:8px">针对日益复杂、绕过传统构建后完整性检查的软件供应链攻击，本文提出了一种智能体AI框架，旨在实现从被动溯源到主动漏洞缓解的自主防御。该方法融合了大语言模型进行语义漏洞分析，利用强化学习制定兼顾安全性与操作开销的自适应缓解策略，并通过LangChain/LangGraph实现多智能体协同，所有观察与行动均记录于区块链安全账本以确保可审计性，同时使用模型上下文协议与真实CI/CD管道交互。在模拟及真实CI/CD环境（如GitHub Actions、Jenkins）中针对代码注入、不安全反序列化等攻击的实验表明，该框架相比基于规则、仅溯源及仅强化学习的基线方法，具有更高的检测准确率、更低的缓解延迟以及可接受的构建时间开销，展现了向主动自防御软件供应链的转变潜力。</div>
</details>
</div>
<div class="card">
<div class="title">HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation</div>
<div class="meta-line">Authors: Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao</div>
<div class="meta-line">First: 2025-12-29T13:46:24+00:00 · Latest: 2025-12-29T13:46:24+00:00</div>
<div class="meta-line">Comments: Github: see https://github.com/Tencent-Hunyuan/HY-Motion-1.0</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23464v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23464v1">PDF</a> · <a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HY-Motion 1.0：面向文本到动作生成的流匹配模型规模化研究</div>
<div class="mono" style="margin-top:8px">我们推出HY-Motion 1.0系列模型，这是一套先进的大规模动作生成模型，能够根据文本描述生成3D人体动作。该研究首次成功将基于扩散Transformer（DiT）的流匹配模型扩展至十亿参数规模，在动作生成领域实现了显著超越当前开源基准的指令跟随能力。我们独创性地采用全阶段训练范式——包括基于3000多小时动作数据的大规模预训练、400小时精选数据的高质量微调，以及结合人类反馈与奖励模型的强化学习——确保模型精准对齐文本指令并保持高动作质量。该框架依托我们精心设计的数据处理流程，执行严格的动作清洗与标注。最终，我们的模型实现了最广泛的动作覆盖，涵盖6大类超过200种动作类别。我们将HY-Motion 1.0开源给社区，以推动未来研究并加速3D人体动作生成模型向商业化成熟阶段迈进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for high-quality, instruction-following 3D human motion generation from text, this work introduces HY-Motion 1.0, a series of large-scale models. The method scales Diffusion Transformer-based flow matching to billions of parameters and employs a full-stage training paradigm involving large-scale pretraining, high-quality fine-tuning, and reinforcement learning from human feedback, supported by a meticulous data processing pipeline. The main experimental results demonstrate that the model significantly outperforms existing open-source benchmarks, achieving extensive coverage of over 200 motion categories across 6 major classes with precise text-motion alignment.</div>
<div class="mono" style="margin-top:8px">为从文本生成高质量、可遵循指令的3D人体运动，本研究提出了HY-Motion 1.0系列大规模模型。其方法将基于扩散变换器的流匹配模型扩展至数十亿参数规模，并采用包含大规模预训练、高质量微调以及基于人类反馈的强化学习的全阶段训练范式，辅以精细的数据处理流程。主要实验结果表明，该模型显著超越了现有开源基准，实现了对6大类、超过200种运动类别的广泛覆盖，并确保了文本与运动之间的精确对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs</div>
<div class="meta-line">Authors: Daniel Furelos-Blanco, Charles Pert, Frederik Kelbel, Alex F. Spies, Alessandra Russo, Michael Dennis</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-16T17:48:09+00:00 · Latest: 2025-12-29T13:42:46+00:00</div>
<div class="meta-line">Comments: Extended version of paper accepted for publication at the 40th AAAI Conference on Artificial Intelligence (AAAI)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12706v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12706v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越固定任务：面向任务-关卡组合的无监督环境设计</div>
<div class="mono" style="margin-top:8px">在复杂环境（关卡）中训练通用智能体执行复杂指令（任务）仍是强化学习的核心挑战。随机采样任务-关卡组合常产生无解配对，凸显了任务与关卡协同设计的必要性。虽然无监督环境设计（UED）已被证明能有效自动生成关卡课程，但现有研究仅针对固定任务。本文提出ATLAS（面向规范自课程的任務与关卡对齐方法），这是一种在任务与关卡上生成联合自课程的新方法。该方法基于UED框架，能自动生成可解且具挑战性的任务-关卡组合用于策略训练。为评估ATLAS并推动领域发展，我们构建了在Minigrid关卡中以奖励机建模任务的评估套件。实验表明ATLAS显著优于随机采样方法，尤其在可解组合采样概率较低时优势更明显。研究进一步证明，利用任务与关卡结构的变异操作能加速高性能策略的收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training general reinforcement learning agents to follow complex instructions in intricate environments, where random sampling of task-level pairs often yields unsolvable combinations. To overcome this, the authors propose ATLAS, a novel unsupervised environment design method that generates joint autocurricula over both tasks and levels, automatically producing solvable yet challenging pairs for policy training. Experimental results on a Minigrid-based evaluation suite with tasks modeled as reward machines show that ATLAS significantly outperforms random sampling, especially when solvable pairs are rare, and that mutations leveraging task and level structure accelerate policy convergence.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中训练通用智能体在复杂环境中执行复杂指令的挑战，其中随机采样任务-关卡组合常产生不可解配对。为解决此问题，作者提出了ATLAS方法，这是一种新颖的无监督环境设计方法，能生成任务与关卡的联合自动课程，自动为策略训练产生可解且具挑战性的配对。在基于Minigrid、以奖励机建模任务的评估套件上的实验表明，ATLAS显著优于随机采样方法，尤其在可解配对稀缺时效果更明显，且利用任务和关卡结构的变异能加速策略收敛。</div>
</details>
</div>
<div class="card">
<div class="title">Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance</div>
<div class="meta-line">Authors: Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang</div>
<div class="meta-line">First: 2025-12-29T13:39:41+00:00 · Latest: 2025-12-29T13:39:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23461v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23461v1">PDF</a> · <a href="https://github.com/Qwen-Applications/DIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信息论指导消除奖励模型中的归纳偏差</div>
<div class="mono" style="margin-top:8px">奖励模型在基于人类反馈的强化学习中至关重要，用于使大语言模型与人类价值观对齐。然而，奖励模型训练数据普遍存在质量较低的问题，包含易导致过拟合和奖励攻击的归纳偏差。例如，更详尽全面的回答通常更受人类青睐，但往往伴随更多字数，导致回答长度成为不可避免的偏差之一。现有少数去偏差方法仅针对单一特定偏差类型，或仅通过简单线性相关性建模问题。为缓解奖励建模中更复杂多样的归纳偏差，我们提出一种新颖的信息论去偏差方法DIR。受信息瓶颈理论启发，该方法最大化奖励模型评分与人类偏好对之间的互信息，同时最小化奖励模型输出与偏好输入中偏差属性的互信息。基于信息论的理论支撑，DIR能处理具有非线性关联的更复杂偏差类型，大幅扩展了奖励模型去偏差方法的实际应用场景。实验中，我们通过三类归纳偏差验证DIR的有效性：回答长度偏差、迎合性偏差和格式偏差。研究发现DIR不仅能有效缓解目标偏差，还能提升基于人类反馈的强化学习在多样化基准测试中的性能，获得更优的泛化能力。代码与训练方案已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of inductive biases in reward models (RMs) used for aligning large language models with human feedback, where biases like a preference for longer responses can lead to overfitting and reward hacking. The authors propose DIR, an information-theoretic debiasing method that maximizes mutual information between RM scores and human preferences while minimizing it between RM outputs and biased input attributes, thereby handling complex, non-linear biases. Experimental results demonstrate that DIR effectively mitigates biases related to response length, sycophancy, and format, and improves the generalization and performance of reinforcement learning from human feedback across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习中奖励模型存在的归纳偏差问题展开研究，这类偏差（如对较长回复的偏好）容易导致过拟合和奖励攻击。作者提出了DIR这一基于信息论的去偏差方法，通过最大化奖励模型分数与人类偏好之间的互信息，同时最小化其输出与带偏差输入属性之间的互信息，从而处理复杂的非线性偏差。实验结果表明，DIR能有效减轻与回复长度、迎合性及格式相关的偏差，并在多种基准测试中提升了强化学习从人类反馈中学习的泛化能力和性能。</div>
</details>
</div>
<div class="card">
<div class="title">Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following</div>
<div class="meta-line">Authors: Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song</div>
<div class="meta-line">First: 2025-12-29T13:31:08+00:00 · Latest: 2025-12-29T13:31:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23457v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23457v1">PDF</a> · <a href="https://github.com/sastpg/HIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将失败重演为成功：面向指令跟随的样本高效强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在使大语言模型（LLM）遵循多样化约束的指令方面展现出潜力。尽管成果令人鼓舞，但RL的改进必然依赖于采样成功的高质量响应；然而，初始模型常因能力有限而难以生成满足所有约束的响应，导致奖励稀疏或难以区分，从而阻碍学习。本研究提出后见指令重演（HiR），一种面向复杂指令跟随任务的新型样本高效RL框架，采用“选择-重写”策略，基于事后已满足的约束将失败尝试重演为成功样本。我们对这些重演样本及原始样本进行RL训练，从理论上将目标构建为指令级与响应级的双重偏好学习，从而仅用二元奖励信号即可实现高效优化。大量实验表明，所提HiR在不同指令跟随任务中均取得显著效果，同时所需计算资源更少。代码与数据集已开源：https://github.com/sastpg/HIR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the challenge that reinforcement learning for aligning large language models to follow complex instructions often suffers from sparse rewards, as initial models struggle to generate responses that meet all constraints, hindering learning efficiency. The proposed method, Hindsight instruction Replay (HiR), addresses this by employing a select-then-rewrite strategy to retroactively treat failed attempts as successful based on partially satisfied constraints, and performs RL on both original and replayed samples through a dual-preference learning framework optimized with binary rewards. Experimental results show that HiR achieves promising performance across various instruction-following tasks while being more sample-efficient and computationally economical than baseline approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，使用强化学习对齐大语言模型以遵循复杂指令时，常因初始模型难以生成满足所有约束的响应而导致奖励稀疏，阻碍学习效率。所提出的方法称为后见指令回放（HiR），采用选择-重写策略，根据事后已满足的约束将失败尝试重放为成功样本，并在原始和重放样本上通过指令与响应层面的双重偏好学习框架进行强化学习，仅需二元奖励信号即可高效优化。实验结果表明，HiR在多种指令跟随任务中取得了良好性能，同时相比基线方法更具样本效率和计算经济性。</div>
</details>
</div>
<div class="card">
<div class="title">The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis</div>
<div class="meta-line">Authors: Alex Lewandowski, Adtiya A. Ramesh, Edan Meyer, Dale Schuurmans, Marlos C. Machado</div>
<div class="meta-line">Venue: NeurIPS 2025 spotlight</div>
<div class="meta-line">First: 2025-12-29T12:31:46+00:00 · Latest: 2025-12-29T12:31:46+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 (spotlight)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23419v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning is often motivated by the idea, known as the big world hypothesis, that &quot;the world is bigger&quot; than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent&#x27;s capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent&#x27;s ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界更广阔！大世界假说的计算嵌入视角</div>
<div class="mono" style="margin-top:8px">持续学习常基于&#x27;大世界假说&#x27;——即&#x27;世界比智能体更广阔&#x27;。现有问题框架通过显式约束智能体与环境的关系来体现这一思想，这类约束促使智能体持续调整以最优利用有限能力，而非收敛至固定解。但显式约束往往具有随意性、难以整合，且可能限制智能体能力扩展的有效性。本文提出一种问题设定：无论智能体能力如何，其始终受限于环境嵌入的约束。具体而言，我们引入计算嵌入视角，将嵌入智能体表征为在通用（形式化）计算机中模拟的自动机。此类自动机始终受约束；我们证明其等价于在可数无限状态空间的部分可观测马尔可夫决策过程中交互的智能体。针对该设定，我们提出名为&#x27;交互性&#x27;的目标函数，用于衡量智能体通过学习新预测持续调整行为的能力。随后开发了基于模型的强化学习算法以寻求交互性，并构建合成问题评估持续学习能力。实验表明：深度非线性网络难以维持交互性，而深度线性网络能随容量提升维持更高交互性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges conventional continual learning formulations that rely on explicit constraints to model the &#x27;big world hypothesis,&#x27; arguing they can be ad hoc and limit scalability. Instead, it proposes a computationally-embedded perspective, modeling the agent as an automaton within a universal computer, which inherently constrains it and is proven equivalent to interacting with a partially observable Markov decision process over an infinite state-space. The authors introduce &#x27;interactivity&#x27; as an objective to measure continual adaptation and develop a model-based reinforcement learning algorithm for it; experimental results on a synthetic problem reveal that deep nonlinear networks struggle to sustain interactivity, while deep linear networks achieve higher interactivity with increased capacity.</div>
<div class="mono" style="margin-top:8px">本文对依赖显式约束来建模&#x27;大世界假设&#x27;的传统持续学习框架提出挑战，认为这些约束可能具有随意性并限制可扩展性。为此，研究提出一种计算嵌入的视角，将智能体建模为通用计算机内的自动机，从而内在地约束其能力，并被证明等价于在无限状态空间的部分可观测马尔可夫决策过程中进行交互。作者引入&#x27;交互性&#x27;作为衡量持续适应能力的指标，并开发了基于模型的强化学习算法进行优化；在合成问题上的实验结果表明，深度非线性网络难以维持交互性，而深度线性网络随着容量增加能实现更高的交互性。</div>
</details>
</div>
<div class="card">
<div class="title">OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</div>
<div class="meta-line">Authors: Zhenguo Zhang, Haohan Zheng, Yishen Wang, Le Xu, Tianchen Deng, Xuefeng Chen, Qu Chen, Bo Zhang, Wuxiong Huang</div>
<div class="meta-line">First: 2025-12-16T03:19:28+00:00 · Latest: 2025-12-29T12:27:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14044v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.14044v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning. While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels. Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and &quot;zoom in&quot; on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model&#x27;s significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniDrive-R1：基于强化驱动的交错式多模态思维链实现可信视觉语言自动驾驶</div>
<div class="mono" style="margin-top:8px">视觉语言模型在自动驾驶等安全关键领域的应用，因可靠性缺陷（尤其是物体幻觉问题）而严重受限。该缺陷源于模型依赖未接地的文本思维链推理。现有多模态思维链方法虽尝试缓解，但存在两个根本缺陷：（1）解耦的感知与推理阶段阻碍端到端联合优化；（2）依赖昂贵密集的定位标注。为此，我们提出OmniDrive-R1——专为自动驾驶设计的端到端视觉语言模型框架，通过交错式多模态思维链机制统一感知与推理。其核心创新是强化驱动的视觉接地能力，使模型能自主聚焦关键区域进行细粒度分析。该能力由纯两阶段强化学习训练流程与Clip-GRPO算法实现。Clip-GRPO引入无需标注的基于过程的接地奖励，不仅消除对密集标注的依赖，还通过强制视觉焦点与文本推理的实时跨模态一致性，规避外部工具调用的不稳定性。在DriveLMM-o1数据集上的大量实验表明，相比基线模型Qwen2.5VL-7B，OmniDrive-R1将整体推理分数从51.77%提升至80.35%，最终答案准确率从37.81%提升至73.62%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the reliability failures, particularly object hallucination, in Vision-Language Models (VLMs) for autonomous driving, which stem from ungrounded text-based reasoning. The authors propose OmniDrive-R1, an end-to-end framework that unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism, enhanced by a reinforcement-driven visual grounding capability that allows the model to autonomously focus on critical regions. Using a two-stage reinforcement learning pipeline and the novel Clip-GRPO algorithm with an annotation-free, process-based reward, the model achieves significant improvements on the DriveLMM-o1 benchmark, raising the overall reasoning score from 51.77% to 80.35% and final answer accuracy from 37.81% to 73.62% compared to the baseline.</div>
<div class="mono" style="margin-top:8px">本文针对自动驾驶中视觉语言模型因依赖无根据的文本推理而产生的可靠性故障（特别是物体幻觉）问题展开研究。作者提出了OmniDrive-R1，这是一个端到端框架，通过交错多模态思维链机制统一感知与推理，并利用强化学习驱动的视觉定位能力使模型能自主聚焦关键区域。该方法采用两阶段强化学习流程和创新的Clip-GRPO算法，通过无需标注的过程奖励机制，在DriveLMM-o1基准测试中取得了显著提升：与基线模型相比，整体推理分数从51.77%提高至80.35%，最终答案准确率从37.81%提升至73.62%。</div>
</details>
</div>
<div class="card">
<div class="title">AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis</div>
<div class="meta-line">Authors: Cehua Yang, Dongyu Xiao, Junming Lin, Yuyang Song, Hanxu Yan, Shawn Guo, Wei Zhang, Jian Yang, Mingjie Tang, Bryan Dai</div>
<div class="meta-line">First: 2025-12-29T10:49:35+00:00 · Latest: 2025-12-29T10:49:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23366v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23366v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advancement of Text-to-SQL systems is currently hindered by the scarcity of high-quality training data and the limited reasoning capabilities of models in complex scenarios. In this paper, we propose a holistic framework that addresses these issues through a dual-centric approach. From a Data-Centric perspective, we construct an iterative data factory that synthesizes RL-ready data characterized by high correctness and precise semantic-logic alignment, ensured by strict verification. From a Model-Centric perspective, we introduce a novel Agentic Reinforcement Learning framework. This framework employs a Diversity-Aware Cold Start stage to initialize a robust policy, followed by Group Relative Policy Optimization (GRPO) to refine the agent&#x27;s reasoning via environmental feedback. Extensive experiments on BIRD and Spider benchmarks demonstrate that our synergistic approach achieves state-of-the-art performance among single-model methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AGRO-SQL：基于高保真数据合成的智能体群组相对优化方法</div>
<div class="mono" style="margin-top:8px">当前，文本到SQL系统的发展受限于高质量训练数据的稀缺性以及模型在复杂场景下的推理能力不足。本文提出一种整体性框架，通过双中心路径解决这些问题。在数据中心层面，我们构建了迭代式数据工厂，通过严格验证机制合成具有高正确率与精准语义逻辑对齐的强化学习就绪数据。在模型中心层面，我们引入了新型智能体强化学习框架，该框架采用多样性感知冷启动阶段初始化稳健策略，继而通过群组相对策略优化（GRPO）利用环境反馈精化智能体推理能力。在BIRD和Spider基准上的大量实验表明，我们的协同方法在单模型方法中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of Text-to-SQL systems, namely the scarcity of high-quality training data and models&#x27; insufficient reasoning in complex scenarios. The proposed holistic framework adopts a dual approach: a Data-Centric component creates an iterative data factory to synthesize verified, high-fidelity data with strong semantic-logic alignment, and a Model-Centric component introduces an Agentic Reinforcement Learning framework that uses a Diversity-Aware Cold Start and Group Relative Policy Optimization (GRPO) to refine the agent&#x27;s reasoning through environmental feedback. Experimental results on the BIRD and Spider benchmarks show that this synergistic method achieves state-of-the-art performance among single-model systems.</div>
<div class="mono" style="margin-top:8px">本文针对文本到SQL系统面临的高质量训练数据稀缺和模型在复杂场景下推理能力有限的问题，提出了一个整体解决方案。该框架采用双中心策略：从数据中心视角，构建了一个迭代式数据工厂，通过严格验证合成具有高正确性和精准语义逻辑对齐的训练数据；从模型中心视角，引入了一种新型智能体强化学习框架，通过多样性感知冷启动阶段初始化策略，并利用组相对策略优化（GRPO）依据环境反馈优化智能体推理。在BIRD和Spider基准测试上的广泛实验表明，这种协同方法在单模型方法中取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning</div>
<div class="meta-line">Authors: Yilun Liu, Ziang Chen, Song Xu, Minggui He, Shimin Tao, Weibin Meng, Yuming Xie, Tao Han, Chunguang Zhao, Jingzhou Du, Daimeng Wei, Shenglin Zhang, Yongqian Sun</div>
<div class="meta-line">First: 2025-09-30T09:19:31+00:00 · Latest: 2025-12-29T10:05:56+00:00</div>
<div class="meta-line">Comments: Accepted by ICSE 2026 (SEIP Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25987v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25987v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing complexity of log data in modern software systems has prompted the use of Large Language Models (LLMs) for automated log analysis. Current approaches typically rely on direct supervised fine-tuning (SFT) on log-label pairs. However, this exacerbates the domain discrepancy between general-purpose LLMs and specialized log data, causing overfitting. Furthermore, SFT&#x27;s imbalanced loss computation often allows lengthy contexts to overwhelm critical, concise details in model answers, leading to hallucinations. To address these limitations, we propose R-Log, a novel reasoning-based paradigm that mirrors the structured, step-by-step analytical process of human engineers. This approach enhances generalizability by learning the underlying rules behind conclusions. We further employ Reinforcement Learning (RL) to optimize the model within a simulated O&amp;M environment, thereby reducing hallucinations by directly rewarding correct outcomes. R-Log is first cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13 strategies from manual O&amp;M practices, to establish an initial reasoning capability. This ability is then refined via RL using a joint reward function. Empirical evaluations on real-world logs show that R-Log outperforms existing methods across five log analysis tasks, particularly in unseen scenarios (by 228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the efficacy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>R-Log：基于推理的强化学习激励大语言模型的日志分析能力</div>
<div class="mono" style="margin-top:8px">现代软件系统中日益复杂的日志数据推动了大语言模型（LLMs）在自动化日志分析中的应用。现有方法通常依赖对日志-标签对的直接监督微调（SFT），但这加剧了通用大语言模型与专业日志数据之间的领域差异，导致过拟合。此外，SFT的不均衡损失计算常使冗长上下文淹没模型答案中的关键简明细节，引发幻觉。为应对这些局限，我们提出R-Log——一种模拟人类工程师结构化、分步分析过程的新型推理范式。该方法通过学习结论背后的潜在规则增强泛化能力。我们进一步采用强化学习（RL）在模拟运维环境中优化模型，通过直接奖励正确结果减少幻觉。R-Log首先基于2000余条人工运维实践中的13种策略指导构建的推理轨迹数据集进行冷启动，建立初始推理能力，随后通过联合奖励函数进行RL精调。真实日志的实证评估表明，R-Log在五项日志分析任务中均优于现有方法，尤其在未见场景中（提升228.05%）。我们还设计了提速5倍且保持93%效能的R-Log-fast版本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of supervised fine-tuning for log analysis with LLMs, which suffers from domain discrepancy and hallucinations due to imbalanced loss, this paper introduces R-Log, a method that employs a reasoning-based paradigm to mimic human analytical steps and uses reinforcement learning within a simulated environment to optimize outcomes. The approach is cold-started on a dataset of reasoning trajectories guided by operational strategies and refined via a joint reward function. Experimental results on real-world logs demonstrate that R-Log outperforms existing methods across five tasks, especially in unseen scenarios by 228.05%, with a faster variant maintaining 93% efficacy at 5x speed.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型在日志分析中因监督微调导致的领域差异和幻觉问题，本文提出了R-Log方法，其动机是模仿人类工程师的结构化推理步骤，并通过强化学习在模拟运维环境中优化模型以减少错误。该方法基于人工运维策略构建推理轨迹数据集进行冷启动，并利用联合奖励函数进行强化学习精炼。在真实日志上的实验结果表明，R-Log在五项日志分析任务中优于现有方法，尤其在未见场景下性能提升228.05%，其快速变体在保持93%效能的同时实现了5倍加速。</div>
</details>
</div>
<div class="card">
<div class="title">Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL</div>
<div class="meta-line">Authors: Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer</div>
<div class="meta-line">First: 2025-12-29T08:57:58+00:00 · Latest: 2025-12-29T08:57:58+00:00</div>
<div class="meta-line">Comments: 11 pages, 9 figures. Accepted by ACM for presentation at UCC &#x27;25 (18th International Conference on Utility and Cloud Computing), December 1-4, 2025, France. Proceedings publication pending</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23310v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23310v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying large language models (LLMs) on edge devices is challenging due to their limited memory and power resources. Cloud-only inference reduces device burden but introduces high latency and cost. Static edge-cloud partitions optimize a single metric and struggle when bandwidth fluctuates. We propose Splitwise, a novel Lyapunov-assisted deep reinforcement learning (DRL) framework for fine-grained, adaptive partitioning of LLMs across edge and cloud environments. Splitwise decomposes transformer layers into attention heads and feed-forward sub-blocks, exposing more partition choices than layer-wise schemes. A hierarchical DRL policy, guided by Lyapunov optimization, jointly minimizes latency, energy consumption, and accuracy degradation while guaranteeing queue stability under stochastic workloads and variable network bandwidth. Splitwise also guarantees robustness via partition checkpoints with exponential backoff recovery in case of communication failures. Experiments on Jetson Orin NX, Galaxy S23, and Raspberry Pi 5 with GPT-2 (1.5B), LLaMA-7B, and LLaMA-13B show that Splitwise reduces end-to-end latency by 1.4x-2.8x and cuts energy consumption by up to 41% compared with existing partitioners. It lowers the 95th-percentile latency by 53-61% relative to cloud-only execution, while maintaining accuracy and modest memory requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Splitwise：基于李雅普诺夫辅助深度强化学习的LLM协同边缘-云端推理框架</div>
<div class="mono" style="margin-top:8px">在边缘设备上部署大型语言模型（LLM）面临内存和功耗资源有限的挑战。纯云端推理虽减轻设备负担，但会引入高延迟与成本。静态边缘-云端划分方案仅优化单一指标，难以适应带宽波动。本文提出Splitwise——一种基于李雅普诺夫辅助深度强化学习（DRL）的新型框架，实现LLM在边缘与云端环境的细粒度自适应划分。该框架将Transformer层解耦为注意力头与前馈子模块，提供比分层方案更丰富的划分选择。通过李雅普诺夫优化指导的层次化DRL策略，在随机工作负载和动态网络带宽下联合优化延迟、能耗与精度损失，同时保障队列稳定性。Splitwise还通过划分检查点与指数退避恢复机制保障通信故障时的系统鲁棒性。在Jetson Orin NX、Galaxy S23和树莓派5设备上，使用GPT-2（1.5B）、LLaMA-7B和LLaMA-13B模型的实验表明：相比现有划分器，Splitwise将端到端延迟降低1.4-2.8倍，能耗最高减少41%；相较于纯云端执行，其第95百分位延迟降低53-61%，同时保持精度并仅需适度内存开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for Splitwise stems from the difficulty of deploying large language models (LLMs) on resource-constrained edge devices, where cloud-only inference incurs high latency and static edge-cloud partitions fail under fluctuating network conditions. The method introduces a Lyapunov-assisted deep reinforcement learning (DRL) framework that performs fine-grained, adaptive partitioning by decomposing transformer layers into attention heads and feed-forward sub-blocks, enabling a hierarchical policy to jointly optimize latency, energy, and accuracy while ensuring queue stability and robustness via checkpoint recovery. Experimental results on devices like Jetson Orin NX and Raspberry Pi 5 with models including GPT-2 and LLaMA-7B/13B show that Splitwise reduces end-to-end latency by 1.4x-2.8x, cuts energy consumption by up to 41%, and lowers the 95th-percentile latency by 53-61% compared to cloud-only execution, all while maintaining accuracy and modest memory overhead.</div>
<div class="mono" style="margin-top:8px">Splitwise 的研究动机源于在内存和功耗资源有限的边缘设备上部署大语言模型（LLM）的挑战，纯云端推理会带来高延迟和高成本，而静态的边缘-云划分方案在带宽波动时效果不佳。该方法提出了一种基于李雅普诺夫优化的深度强化学习（DRL）框架，通过将Transformer层分解为注意力头和前馈子块来实现细粒度自适应划分，利用分层策略在保证队列稳定性和通过检查点恢复实现鲁棒性的同时，联合优化延迟、能耗和精度损失。在Jetson Orin NX、Galaxy S23和树莓派5等设备上使用GPT-2、LLaMA-7B和LLaMA-13B模型的实验结果表明，与现有划分方案相比，Splitwise将端到端延迟降低了1.4至2.8倍，能耗最高减少41%，相较于纯云端执行，其第95百分位延迟降低了53-61%，同时保持了模型精度和适中的内存需求。</div>
</details>
</div>
<div class="card">
<div class="title">ZIA: A Theoretical Framework for Zero-Input AI</div>
<div class="meta-line">Authors: Aditi De</div>
<div class="meta-line">First: 2025-02-22T07:42:05+00:00 · Latest: 2025-12-29T07:12:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.16124v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.16124v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Input AI (ZIA) introduces a novel framework for human-computer interaction by enabling proactive intent prediction without explicit user commands. It integrates gaze tracking, bio-signals (EEG, heart rate), and contextual data (time, location, usage history) into a multi-modal model for real-time inference, targeting &lt;100 ms latency. The proposed architecture employs a transformer-based model with cross-modal attention, variational Bayesian inference for uncertainty estimation, and reinforcement learning for adaptive optimization. To support deployment on edge devices (CPUs, TPUs, NPUs), ZIA utilizes quantization, weight pruning, and linear attention to reduce complexity from quadratic to linear with sequence length. Theoretical analysis establishes an information-theoretic bound on prediction error and demonstrates how multi-modal fusion improves accuracy over single-modal approaches. Expected performance suggests 85-90% accuracy with EEG integration and 60-100 ms inference latency. ZIA provides a scalable, privacy-preserving framework for accessibility, healthcare, and consumer applications, advancing AI toward anticipatory intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ZIA：零输入人工智能的理论框架</div>
<div class="mono" style="margin-top:8px">零输入人工智能（ZIA）提出了一种新颖的人机交互框架，通过无需显式用户指令即可主动预测意图。该框架整合了眼动追踪、生物信号（脑电图、心率）和上下文数据（时间、位置、使用历史）到一个多模态模型中，实现实时推断，目标延迟低于100毫秒。所提出的架构采用基于Transformer的模型，具备跨模态注意力机制、用于不确定性估计的变分贝叶斯推断，以及用于自适应优化的强化学习。为支持在边缘设备（CPU、TPU、NPU）上部署，ZIA利用量化、权重剪枝和线性注意力机制，将复杂度从序列长度的二次方降低至线性。理论分析建立了预测误差的信息论界限，并展示了多模态融合如何比单模态方法提高准确性。预期性能显示，结合脑电图时准确率可达85-90%，推断延迟为60-100毫秒。ZIA为无障碍服务、医疗保健和消费应用提供了一个可扩展且保护隐私的框架，推动人工智能向预见性智能迈进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more natural and anticipatory human-computer interaction, this paper introduces the Zero-Input AI (ZIA) framework, which aims to predict user intent proactively without requiring explicit commands. The method integrates multi-modal data—including gaze, bio-signals like EEG and heart rate, and contextual information—using a transformer-based model with cross-modal attention, variational Bayesian inference for uncertainty, and reinforcement learning for optimization; it also employs techniques like quantization and linear attention to enable efficient deployment on edge devices. The main experimental results, supported by theoretical analysis, show that ZIA achieves 85-90% prediction accuracy with EEG integration and maintains a low inference latency of 60-100 ms, outperforming single-modal approaches and providing a scalable, privacy-preserving solution for applications in accessibility and healthcare.</div>
<div class="mono" style="margin-top:8px">本文的动机是追求更自然、更具预见性的人机交互，为此提出了零输入人工智能（ZIA）框架，旨在无需用户明确指令即可主动预测其意图。该方法通过整合凝视、脑电图和心率等生物信号以及上下文数据，采用基于Transformer的跨模态注意力模型、用于不确定性估计的变分贝叶斯推理以及自适应优化的强化学习；同时利用量化和线性注意力等技术降低计算复杂度，以支持在边缘设备上的高效部署。主要实验结果及理论分析表明，ZIA在集成脑电图数据时实现了85-90%的预测准确率，推理延迟保持在60-100毫秒，优于单模态方法，为无障碍辅助和医疗健康等领域提供了一个可扩展且保护隐私的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing</div>
<div class="meta-line">Authors: Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang</div>
<div class="meta-line">First: 2025-12-29T06:58:46+00:00 · Latest: 2025-12-29T06:58:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23244v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23244v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViLaCD-R1：面向遥感语义变化检测的视觉-语言框架</div>
<div class="mono" style="margin-top:8px">遥感变化检测（RSCD）是一项复杂的多图像推理任务，传统方法采用基于像素的算子或编码器-解码器网络，这些方法难以充分捕捉高层语义信息，且易受非语义干扰影响。尽管近期基于多模态和视觉语言模型（VLM）的方法通过引入文本描述增强了对变化区域的语义理解，但仍面临空间定位不准确、像素级边界划分不精确、可解释性有限等挑战。为解决这些问题，我们提出ViLaCD-R1——一个包含多图像推理器（MIR）与掩码引导解码器（MGD）的两阶段框架。具体而言，该框架通过监督微调（SFT）和强化学习（RL）在区块级双时相推理任务上训练VLM，以双时相图像块作为输入，输出粗粒度变化掩码。随后，解码器融合双时相图像特征与该粗掩码，生成精确的二元变化图。在多个RSCD基准测试上的综合评估表明，ViLaCD-R1显著提升了真实语义变化的识别与定位能力，有效抑制非语义变化干扰，并在复杂现实场景中实现了最先进的检测精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional remote sensing change detection methods, which struggle with high-level semantics and non-semantic noise, and the shortcomings of recent vision-language models in spatial localization and boundary precision, this paper introduces ViLaCD-R1, a two-stage vision-language framework. The method employs a Multi-Image Reasoner, fine-tuned with supervised and reinforcement learning on block-level tasks to generate a coarse change mask from dual-temporal image patches, followed by a Mask-Guided Decoder that refines this mask with image features to produce a precise binary change map. Experimental results on multiple benchmarks show that the framework significantly enhances the recognition and localization of true semantic changes, effectively suppresses irrelevant variations, and achieves state-of-the-art accuracy in complex real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本文的动机在于传统遥感变化检测方法难以捕捉高层语义且易受非语义干扰，而近期基于视觉-语言模型的方法又存在空间定位不准和边界划分不精确的问题。为此，论文提出了ViLaCD-R1这一两阶段视觉-语言框架，其方法包含一个多图像推理器和一个掩码引导解码器：推理器通过监督微调和强化学习在块级双时相推理任务上进行训练，以生成粗略变化掩码；解码器则融合双时相图像特征与该掩码来预测精确的二进制变化图。在多个基准测试上的实验结果表明，该框架显著提升了真实语义变化的识别与定位能力，鲁棒地抑制了非语义变化，并在复杂的真实场景中取得了最先进的准确度。</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search</div>
<div class="meta-line">Authors: Ziyang Zeng, Heming Jing, Jindong Chen, Xiangli Li, Hongyu Liu, Yixuan He, Zhengyu Li, Yige Sun, Zheyong Xie, Yuqing Yang, Shaosheng Cao, Jun Fan, Yi Wu, Yao Hu</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-11-30T16:31:16+00:00 · Latest: 2025-12-29T06:38:38+00:00</div>
<div class="meta-line">Comments: Accepted to the ADS Track at KDD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00968v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive offline evaluations and online A/B tests demonstrate that our approach consistently delivers significant improvements across key relevance and business metrics, validating its effectiveness, robustness, and practicality for large-scale industrial search systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小红书搜索中基于强化学习的生成式排序相关性优化研究</div>
<div class="mono" style="margin-top:8px">排序相关性是搜索引擎的核心任务，旨在识别与用户查询最相关的条目。传统相关性模型通常输出标量分数或直接预测相关性标签，这限制了模型的可解释性及对复杂相关性信号的建模能力。受近期思维链推理在复杂任务中进展的启发，本研究探讨显式推理能否提升相关性建模的可解释性与性能。然而，现有基于推理的生成式相关性模型主要依赖大量人工标注或合成思维链数据进行监督微调，其泛化能力往往受限。此外，领域无关的自由形式推理通常过于泛化且缺乏事实依据，难以处理开放域搜索中普遍存在的多样性与模糊性案例。本研究将小红书搜索中的相关性建模构建为推理任务，并提出基于强化学习的训练框架以增强生成式相关性模型的具身推理能力。具体而言，我们将实际业务特定的相关性准则融入多步推理提示设计，并提出逐步优势掩码——一种轻量化的过程监督策略，通过改进信用分配促进对这些准则的有效学习。为实现工业部署，我们进一步将大规模强化学习调优模型蒸馏为适用于实际搜索系统的轻量版本。大量离线评估与在线A/B测试表明，该方法在关键相关性指标和业务指标上均取得显著提升，验证了其在大规模工业搜索系统中的有效性、鲁棒性与实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional scalar-based relevance models and the poor generalization of supervised generative relevance models, this work introduces a reinforcement learning framework to enhance grounded reasoning for search relevance in Xiaohongshu. The method formulates relevance as a reasoning task, incorporating business-specific criteria into multi-step prompts and employing a Stepwise Advantage Masking strategy for improved process supervision and credit assignment, followed by model distillation for deployment. Experimental results from offline evaluations and online A/B tests show significant improvements in key relevance and business metrics, demonstrating the approach&#x27;s effectiveness and robustness for industrial search systems.</div>
<div class="mono" style="margin-top:8px">针对传统标量相关性模型的局限性以及监督式生成相关性模型泛化能力不足的问题，本研究提出了一种强化学习框架，以增强小红书搜索中相关性任务的 grounded 推理能力。该方法将相关性建模为推理任务，将业务特定标准融入多步提示设计，并采用逐步优势掩码策略以改进过程监督和信用分配，随后通过模型蒸馏实现部署。离线和在线A/B测试的实验结果表明，该方法在关键相关性和业务指标上均取得显著提升，验证了其在大规模工业搜索系统中的有效性、鲁棒性和实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Parameter Efficient Methods for RLVR</div>
<div class="meta-line">Authors: Qingyu Yin, Yulun Wu, Zhennan Shen, Sunbowen Li, Zhilin Wang, Yanshu Li, Chak Tou Leong, Jiale Kang, Jinjin Gu</div>
<div class="meta-line">First: 2025-12-29T03:13:08+00:00 · Latest: 2025-12-29T03:13:08+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23165v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估RLVR的参数高效方法</div>
<div class="mono" style="margin-top:8px">我们在可验证奖励强化学习（RLVR）范式下，系统评估了参数高效微调（PEFT）方法。RLVR通过可验证反馈激励语言模型提升推理能力；然而，尽管LoRA等方法被广泛使用，但适用于RLVR的最优PEFT架构仍未明确。本研究首次在数学推理基准上，对DeepSeek-R1-Distill系列模型中的12种PEFT方法进行全面评估。实证结果对默认采用标准LoRA的做法提出挑战，主要发现有三点：首先，结构变体（如DoRA、AdaLoRA和MiSS）持续优于LoRA；其次，我们揭示了基于SVD的初始化策略（如PiSSA、MiLoRA）存在谱崩溃现象，其失败归因于主成分更新与RL优化的根本性错配；此外，消融实验表明极端参数削减（如VeRA、Rank-1）会严重制约推理能力。我们通过消融研究与扩展实验进一步验证了这些发现。本研究为倡导参数高效RL方法的深入探索提供了权威指南。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to identify optimal Parameter-Efficient Fine-Tuning (PEFT) architectures for Reinforcement Learning with Verifiable Rewards (RLVR), a paradigm that enhances language model reasoning through verifiable feedback, as the default use of methods like LoRA lacks empirical justification. The method involves a systematic evaluation of over 12 PEFT techniques, including structural variants and SVD-informed strategies, on the DeepSeek-R1-Distill model families using mathematical reasoning benchmarks. The main experimental results reveal that structural variants such as DoRA, AdaLoRA, and MiSS consistently outperform standard LoRA, while SVD-based methods like PiSSA fail due to spectral collapse from misalignment with RL optimization, and extreme parameter reduction methods severely impair reasoning capacity, providing a definitive guide for future PEFT exploration in RL.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，在通过可验证反馈增强语言模型推理能力的可验证奖励强化学习范式下，需要确定最优的参数高效微调架构，因为像LoRA这样的默认方法缺乏实证依据。研究方法包括在DeepSeek-R1-Distill模型系列上，使用数学推理基准对超过12种参数高效微调技术进行系统评估，涵盖结构变体和基于奇异值分解的策略。主要实验结果表明，DoRA、AdaLoRA和MiSS等结构变体持续优于标准LoRA，而基于奇异值分解的方法如PiSSA则因与强化学习优化不匹配导致谱崩溃而失败，且极端参数削减方法严重损害推理能力，这为未来强化学习中参数高效方法的探索提供了明确指导。</div>
</details>
</div>
<div class="card">
<div class="title">Scaled-Dot-Product Attention as One-Sided Entropic Optimal Transport</div>
<div class="meta-line">Authors: Elon Litman</div>
<div class="meta-line">First: 2025-08-11T18:00:17+00:00 · Latest: 2025-12-29T01:41:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08369v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.08369v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The scaled-dot-product attention (SDPA) mechanism is a core component of modern deep learning, but its mathematical form is often motivated by heuristics. This work provides a first-principles justification for SDPA. We first show that the attention forward pass is the exact solution to a degenerate, one-sided Entropic Optimal Transport (EOT) problem, which seeks a distribution that maximizes similarity while being maximally entropic. This optimization perspective has a direct consequence for the backward pass. We prove that the standard gradient computed via backpropagation is mathematically identical to an advantage-based policy gradient, a variance-reduced update rule from reinforcement learning. Crucially, we demonstrate that the EOT formulation of the forward pass induces a specific information geometry on the space of attention distributions. It is this geometry, characterized by the Fisher Information Matrix, that dictates the precise form of the learning gradient, revealing the advantage-based update as a natural consequence of the optimization problem being solved. This unified view reveals SDPA as a principled mechanism where the forward pass performs optimal inference and the backward pass implements a rational, manifold-aware learning update.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缩放点积注意力作为单侧熵最优传输</div>
<div class="mono" style="margin-top:8px">缩放点积注意力机制是现代深度学习的核心组件，但其数学形式常基于启发式推导。本研究为SDPA提供了第一性原理的论证。我们首先证明注意力前向传播是一个退化单侧熵最优传输问题的精确解，该问题寻求在最大化熵的同时最大化相似性的分布。这一优化视角对反向传播具有直接推论：我们证明通过反向传播计算的标准梯度在数学上等同于基于优势的策略梯度——一种强化学习中方差缩减的更新规则。关键在于，我们证明了前向传播的EOT公式在注意力分布空间上诱导出特定的信息几何结构。正是这种以费舍尔信息矩阵为特征的几何结构决定了学习梯度的精确形式，揭示出基于优势的更新是所求解优化问题的自然结果。这一统一视角表明，SDPA是一种原理性机制：前向传播执行最优推断，反向传播则实现理性的流形感知学习更新。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper provides a first-principles justification for the scaled-dot-product attention (SDPA) mechanism by reframing it within an optimization framework. The motivation is to move beyond heuristic explanations and establish a rigorous mathematical foundation for SDPA. The method demonstrates that the attention forward pass is the exact solution to a one-sided entropic optimal transport problem, which maximizes similarity under an entropy constraint. The main experimental results are theoretical proofs showing this formulation induces a specific information geometry, and that the standard backpropagation gradient is mathematically identical to a variance-reduced, advantage-based policy gradient from reinforcement learning, thereby unifying the forward inference and backward learning passes as a principled, geometry-aware mechanism.</div>
<div class="mono" style="margin-top:8px">本文通过将缩放点积注意力机制重新置于优化框架内，为其提供了第一性原理的论证。其动机是超越启发式解释，为SDPA建立严格的数学基础。该方法证明注意力前向传播是单侧熵最优传输问题的精确解，该问题在熵约束下最大化相似性。主要的实验结果是理论证明，表明该公式会诱导出一种特定的信息几何结构，并且标准的反向传播梯度在数学上等同于强化学习中一种方差缩减的、基于优势的策略梯度，从而将前向推断和后向学习过程统一为一个有原则的、几何感知的机制。</div>
</details>
</div>
<div class="card">
<div class="title">A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms</div>
<div class="meta-line">Authors: Yingru Li, Ziniu Li, Jiacai Liu</div>
<div class="meta-line">First: 2025-12-28T22:25:27+00:00 · Latest: 2025-12-28T22:25:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23097v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23097v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于大语言模型混合在线强化与模仿学习的注记：形式化与算法</div>
<div class="mono" style="margin-top:8px">本文提出一个统一的大语言模型微调框架，融合模仿学习与强化学习。通过分析结合轨迹级KL散度与任务奖励的复合目标梯度，我们推导出两个自然分解的组成部分：(1) 用于词元级模仿的解析可计算稠密梯度，(2) 用于长程奖励优化的蒙特卡洛估计稀疏梯度。稠密梯度具有闭式的逻辑值级计算公式，可实现高效的GPU并行计算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for a unified fine-tuning framework for Large Language Models that effectively combines the strengths of Imitation Learning and Reinforcement Learning. The method involves analyzing the gradient of a composite objective function, which decomposes into a Dense Gradient for token-level imitation—computable in closed form for efficient GPU use—and a Sparse Gradient estimated via Monte Carlo methods for long-term reward optimization. The main experimental result, implied by the formulation, is that this decomposition enables a practical and efficient algorithm for fine-tuning LLMs by simultaneously leveraging precise imitation signals and sparse reward guidance.</div>
<div class="mono" style="margin-top:8px">本文的动机是需要一个统一的大语言模型微调框架，以有效结合模仿学习和强化学习的优势。其方法基于分析一个组合目标函数的梯度，该梯度可分解为用于词元级模仿的稠密梯度（可通过闭式解高效计算，利于GPU实现）和用于长期奖励优化的稀疏梯度（通过蒙特卡洛方法估计）。主要的实验成果，由该公式推导得出，是这种分解能够实现一种实用高效的算法，通过同时利用精确的模仿信号和稀疏的奖励指导来微调大语言模型。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients</div>
<div class="meta-line">Authors: Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa</div>
<div class="meta-line">First: 2025-12-28T21:57:42+00:00 · Latest: 2025-12-28T21:57:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23090v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基准测试成功，临床应用失败：当强化学习为基准而非患者优化时</div>
<div class="mono" style="margin-top:8px">近期针对大语言模型的强化学习进展提升了推理任务性能，但其在资源受限的医学影像领域的应用仍待探索。我们提出ChexReason——一个通过R1风格方法（先监督微调后GRPO）训练的视觉语言模型，仅使用2000个SFT样本、1000个RL样本和单张A100 GPU。在CheXpert和NIH基准上的评估揭示了根本性矛盾：GRPO恢复了分布内性能（CheXpert提升23%，宏观F1=0.346），但削弱了跨数据集迁移能力（NIH下降19%）。这与NV-Reason-CXR-3B等高资源模型表现一致，表明问题源于RL范式而非模型规模。我们发现泛化悖论：SFT检查点在优化前能独特提升NIH性能，表明教师引导的推理能捕获更多机构无关特征。跨模型比较显示，结构化推理框架对通用视觉语言模型有益，但对医学预训练模型增益有限。因此，对于需要跨人群鲁棒性的临床部署，精细化的监督微调可能优于激进的强化学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the application of reinforcement learning (RL) to vision-language models for medical imaging, motivated by the need to adapt recent RL advances for large language models to resource-constrained clinical settings. The method involves training a model named ChexReason using a two-stage RL approach (supervised fine-tuning followed by GRPO) with limited data and compute. Experimental results on CheXpert and NIH benchmarks reveal a tension: while RL improves in-distribution performance significantly, it severely degrades cross-dataset generalization, indicating that aggressive RL optimization may harm clinical robustness, and suggesting that careful supervised fine-tuning could be more effective for deployment across diverse populations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了将强化学习应用于医学影像的视觉语言模型，其动机在于将大语言模型的强化学习进展适配到资源受限的临床环境中。方法上，研究者使用有限的数据和计算资源，通过一个两阶段强化学习流程（监督微调后接GRPO）训练了一个名为ChexReason的模型。在CheXpert和NIH基准测试上的实验结果表明了一个矛盾：强化学习虽能显著提升模型在分布内数据上的性能，却严重损害了其跨数据集的泛化能力，这表明激进的强化学习优化可能不利于临床鲁棒性，并暗示精心设计的监督微调对于需要在多样人群中部署的模型可能更为有效。</div>
</details>
</div>
<div class="card">
<div class="title">Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning</div>
<div class="meta-line">Authors: Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang</div>
<div class="meta-line">First: 2025-12-28T21:44:07+00:00 · Latest: 2025-12-28T21:44:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23087v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23087v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe&#x27;&#x27; vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>驯服长尾：通过动态词汇剪枝实现稳定的大语言模型强化学习</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的强化学习面临一个根本性矛盾：高吞吐量推理引擎与数值精确的训练系统会从相同参数产生不同的概率分布，导致训练-推理失配。我们证明这种失配具有不对称效应：对数概率失配的边界按$(1-p)$缩放，其中$p$为词元概率。对于高概率词元，该边界趋近于零，对序列级失配的贡献可忽略；而对于长尾中的低概率词元，边界保持较大值，且这些词元在被采样时会呈现系统性偏差失配，随序列累积导致梯度估计失稳。我们提出通过动态剪枝构建排除极端长尾的“安全”词汇表来约束强化学习目标，而非采用事后修正。通过剪枝此类词元，我们将大规模系统性偏差失配转化为有界的小幅优化偏差。实验表明，该方法能实现稳定训练；理论上，我们界定了词汇剪枝引入的优化偏差范围。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the training-inference mismatch in reinforcement learning for large language models, where differences in probability distributions between training and inference systems cause instability, particularly due to systematic biases from low-probability tail tokens. The method introduces dynamic vocabulary pruning to constrain the RL objective by excluding extreme tail tokens, thereby trading large, biased mismatches for a small, bounded optimization bias. Experimental results demonstrate that this approach achieves stable training, with theoretical bounds provided on the optimization bias introduced by pruning.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大语言模型强化学习中训练与推理不匹配的问题，这种不匹配由概率分布差异引起，尤其因低概率尾部令牌的系统性偏差而导致训练不稳定。方法采用动态词汇剪枝，通过排除极端尾部令牌来约束强化学习目标，从而将大的系统性偏差替换为小的有界优化偏差。实验结果表明，该方法实现了稳定的训练，并从理论上界定了剪枝引入的优化偏差。</div>
</details>
</div>
<div class="card">
<div class="title">Trust Region Masking for Long-Horizon LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang</div>
<div class="meta-line">First: 2025-12-28T20:41:59+00:00 · Latest: 2025-12-28T20:41:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23075v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长视野大语言模型强化学习中的信任区域掩码技术</div>
<div class="mono" style="margin-top:8px">针对大语言模型的策略梯度方法通过采样滚动策略$π_{\text{roll}}$计算代理目标进行优化。当$π_{\text{roll}} \ne π_θ$时，代理目标与真实目标间存在近似误差。先前研究表明，由于实现差异、专家混合路由不连续性及分布式训练滞后性，这种离策略失配在现代LLM-RL中不可避免。经典信任区域误差界随序列长度$T$呈$O(T^2)$增长，导致其在长视野任务中失效。本文推导出两个更紧致的误差界：按$O(T^{3/2})$缩放的Pinsker-边际界和按$O(T)$缩放的混合界。关键的是，这两个误差界均依赖于$D_{kl}^{tok,max}$——序列中所有位置的最大词元级KL散度。这是本质上的序列级度量：需要检查完整轨迹才能计算，因此无法通过PPO裁剪等词元无关方法控制。我们提出信任区域掩码技术，若序列中任意词元违反信任区域则将该序列完全排除在梯度计算之外，从而为长视野LLM-RL首次提供非空泛的单调改进保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of off-policy approximation error in reinforcement learning for large language models (LLMs), where classical trust region bounds become vacuous for long sequences due to O(T^2) scaling with sequence length T. To tackle this, the authors derive tighter bounds—a Pinsker-Marginal bound scaling as O(T^{3/2}) and a Mixed bound scaling as O(T)—that depend on the maximum token-level KL divergence across a sequence, a sequence-level quantity not controllable by token-independent methods like PPO clipping. They propose Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates the trust region, and demonstrate that this approach provides non-vacuous monotonic improvement guarantees for long-horizon LLM-RL tasks.</div>
<div class="mono" style="margin-top:8px">该论文针对大语言模型强化学习中的离策略近似误差问题，传统信任区域边界因随序列长度T呈O(T^2)增长而在长序列任务中失效。为此，作者推导出更紧的边界——一个按O(T^{3/2})缩放的Pinsker-Marginal边界和一个按O(T)缩放的混合边界，这些边界依赖于序列中所有位置的最大令牌级KL散度，这是一个序列级量值，无法通过PPO裁剪等令牌无关方法控制。他们提出了信任区域掩码方法，该方法在任一令牌违反信任区域时排除整个序列的梯度计算，实验表明这为长视野LLM-RL任务提供了首个非空泛的单调改进保证。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Offline Reinforcement Learning: First Imitate, then Improve</div>
<div class="meta-line">Authors: Adam Jelley, Trevor McInroe, Sam Devlin, Amos Storkey</div>
<div class="meta-line">First: 2024-06-19T09:16:38+00:00 · Latest: 2025-12-28T15:40:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.13376v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.13376v2">PDF</a> · <a href="https://github.com/AdamJelley/EfficientOfflineRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised imitation-based approaches are often favored over off-policy reinforcement learning approaches for learning policies offline, since their straightforward optimization objective makes them computationally efficient and stable to train. However, their performance is fundamentally limited by the behavior policy that collected the dataset. Off-policy reinforcement learning provides a promising approach for improving on the behavior policy, but training is often computationally inefficient and unstable due to temporal-difference bootstrapping. In this paper, we propose a best-of-both approach by pre-training with supervised learning before improving performance with off-policy reinforcement learning. Specifically, we demonstrate improved efficiency by pre-training an actor with behavior cloning and a critic with a supervised Monte-Carlo value error. We find that we are able to substantially improve the training time of popular off-policy algorithms on standard benchmarks, and also achieve greater stability. Code is available at: https://github.com/AdamJelley/EfficientOfflineRL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效离线强化学习：先模仿，后优化</div>
<div class="mono" style="margin-top:8px">基于监督模仿的方法因其优化目标直接、计算效率高且训练稳定，在离线策略学习中常优于离轨强化学习方法。但其性能受限于数据集采集时的行为策略。离轨强化学习虽能改进行为策略，却因时序差分自举导致计算效率低下且训练不稳定。本文提出一种融合方案：先通过监督学习预训练，再用离轨强化学习优化性能。具体而言，我们通过行为克隆预训练行动者、用监督蒙特卡洛价值误差预训练评价者，显著提升了标准基准测试中主流离轨算法的训练效率与稳定性。代码已开源：https://github.com/AdamJelley/EfficientOfflineRL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of offline reinforcement learning by combining the stability of supervised imitation learning with the performance potential of off-policy RL. The proposed method first pre-trains a policy actor via behavior cloning and a value critic using a supervised Monte-Carlo error, establishing a strong initialization. Experimental results on standard benchmarks show this hybrid approach substantially reduces the training time and increases the stability of subsequent off-policy RL algorithms compared to training them from scratch.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中模仿学习性能受限与离策略方法训练低效不稳定的问题，提出了一种结合两者优势的方法。该方法首先通过行为克隆预训练策略执行器，并使用监督式蒙特卡洛价值误差预训练价值评论家，以此为基础进行后续的离策略强化学习优化。在标准基准测试上的实验结果表明，该方法能显著缩短主流离策略算法的训练时间，并提升训练过程的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Agents like Large Language Models</div>
<div class="meta-line">Authors: Adam Jelley, Yuhan Cao, Dave Bignell, Amos Storkey, Sam Devlin, Tabish Rashid</div>
<div class="meta-line">First: 2024-06-06T16:05:45+00:00 · Latest: 2025-12-28T15:24:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.04208v2">Abs</a> · <a href="https://arxiv.org/pdf/2406.04208v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://adamjelley.github.io/aligning-agents-like-llms">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training agents to act competently in complex 3D environments from high-dimensional visual information is challenging. Reinforcement learning is conventionally used to train such agents, but requires a carefully designed reward function, and is difficult to scale to obtain robust agents that generalize to new tasks. In contrast, Large Language Models (LLMs) demonstrate impressively general capabilities resulting from large-scale pre-training and post-training alignment, but struggle to act in complex environments. This position paper draws explicit analogies between decision-making agents and LLMs, and argues that agents should be trained like LLMs to achieve more general, robust, and aligned behaviors. We provide a proof-of-concept to demonstrate how the procedure for training LLMs can be used to train an agent in a 3D video game environment from pixels. We investigate the importance of each stage of the LLM training pipeline, while providing guidance and insights for successfully applying this approach to agents. Our paper provides an alternative perspective to contemporary LLM Agents on how recent progress in LLMs can be leveraged for decision-making agents, and we hope will illuminate a path towards developing more generally capable agents for video games and beyond. Project summary and videos: https://adamjelley.github.io/aligning-agents-like-llms .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像大语言模型一样对齐智能体</div>
<div class="mono" style="margin-top:8px">训练智能体基于高维视觉信息在复杂3D环境中熟练行动具有挑战性。传统上使用强化学习训练此类智能体，但需精心设计奖励函数，且难以扩展至能泛化到新任务的鲁棒智能体。相比之下，大语言模型通过大规模预训练与对齐后训练展现出令人印象深刻的通用能力，却难以在复杂环境中行动。本文明确类比决策智能体与大语言模型，主张应像训练大语言模型一样训练智能体，以实现更通用、鲁棒且对齐的行为。我们通过概念验证演示了如何利用大语言模型训练流程，在3D视频游戏环境中基于像素训练智能体，探究该流程各阶段的重要性，并为成功应用此方法提供指导与见解。本文为当代大语言模型智能体研究提供了新视角，说明如何利用大语言模型的最新进展赋能决策智能体，有望为开发更通用的视频游戏及其他领域智能体指明方向。项目摘要与视频：https://adamjelley.github.io/aligning-agents-like-llms。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This position paper is motivated by the challenge of training robust and generalizable decision-making agents in complex 3D environments, where conventional reinforcement learning often requires meticulous reward engineering and struggles with generalization. The method proposes an alternative by drawing an explicit analogy to Large Language Models (LLMs), suggesting that agents should be trained using a similar pipeline of large-scale pre-training and post-training alignment to acquire more general capabilities. As a proof of concept, the authors successfully apply this LLM-inspired training procedure to train an agent directly from pixels in a 3D video game environment, experimentally validating the importance of each stage in the pipeline and providing practical guidance for its application.</div>
<div class="mono" style="margin-top:8px">本文的动机源于在复杂3D环境中训练稳健且可泛化的决策智能体所面临的挑战，传统的强化学习方法通常需要精心设计奖励函数且难以实现良好的泛化。为此，该方法提出一种新视角，明确类比大语言模型的训练范式，主张智能体应通过类似的大规模预训练和对齐后训练流程来获得更通用的能力。作为概念验证，作者成功将这一受大语言模型启发的训练流程应用于一个3D视频游戏环境中，直接从像素训练智能体，并通过实验验证了流程中各阶段的重要性，为该方法的应用提供了实践指导。</div>
</details>
</div>
<div class="card">
<div class="title">APO: Alpha-Divergence Preference Optimization</div>
<div class="meta-line">Authors: Wang Zixian</div>
<div class="meta-line">First: 2025-12-28T14:51:03+00:00 · Latest: 2025-12-28T14:51:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22953v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APO：Alpha散度偏好优化</div>
<div class="mono" style="margin-top:8px">现代对齐实践主要受两种散度机制主导。监督微调和多数蒸馏式目标隐式地最小化前向KL散度KL(q || π_θ)，虽能产生稳定的模式覆盖更新，但常未能充分利用高奖励模式。相比之下，基于人类反馈的PPO式在线强化学习更接近反向KL散度KL(π_θ || q)，可实现模式寻求式改进，但存在模式坍塌风险。近期锚定方法（如ADPO）表明，在锚定坐标中进行投影能显著提升稳定性，但通常仅采用单一散度。本文提出Alpha散度偏好优化（APO），这是一种基于Csiszar α散度的锚定框架，可在同一锚定几何中连续插值前向与反向KL行为。我们推导出以α参数化的统一梯度动态，分析梯度方差特性，并提出实用的奖励-置信度引导α调度策略——仅当策略同时满足改进条件与置信校准时，才从覆盖阶段过渡至利用阶段。基于Qwen3-1.7B模型在math-level3任务上的实验表明，APO在保持训练稳定性的同时，其性能与GRPO、GSPO基线模型具有竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Alpha-Divergence Preference Optimization (APO), motivated by the limitations of existing alignment methods that rely on either forward KL divergence for stable but under-exploitative updates or reverse KL divergence for mode-seeking but potentially unstable improvements. APO proposes an anchored framework using Csiszar alpha-divergence to continuously interpolate between these two regimes within a unified geometry, with a method that includes deriving gradient dynamics parameterized by alpha and implementing a reward-and-confidence-guarded schedule to transition from coverage to exploitation. Experimental results on Qwen3-1.7B with math-level3 show that APO achieves competitive performance compared to GRPO and GSPO baselines while maintaining training stability.</div>
<div class="mono" style="margin-top:8px">本文提出了Alpha-Divergence Preference Optimization（APO），其动机在于现有对齐方法存在局限：前向KL散度方法更新稳定但易欠利用高奖励模式，而反向KL散度方法虽能寻求模式改进却可能导致模式崩溃。APO采用一种锚定框架，利用Csiszar alpha散度在统一几何结构中连续插值这两种行为，方法包括推导以alpha参数化的梯度动态，并设计一种基于奖励和置信度的调度策略，仅在策略改进且校准置信时从覆盖转向利用。在Qwen3-1.7B模型和math-level3任务上的实验结果表明，APO在保持训练稳定性的同时，性能与GRPO和GSPO基线相当。</div>
</details>
</div>
<div class="card">
<div class="title">A Survey of Reinforcement Learning from Human Feedback</div>
<div class="meta-line">Authors: Timo Kaufmann, Paul Weng, Viktor Bengs, Eyke Hüllermeier</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, 2025</div>
<div class="meta-line">First: 2023-12-22T18:58:06+00:00 · Latest: 2025-12-28T14:30:12+00:00</div>
<div class="meta-line">Comments: Published version (TMLR): https://openreview.net/pdf?id=f7OkIurx4b</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.14925v3">Abs</a> · <a href="https://arxiv.org/pdf/2312.14925v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning provides a promising approach to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The success in training large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF has played a decisive role in directing the model&#x27;s capabilities towards human objectives. This article provides an overview of the fundamentals of RLHF, exploring how RL agents interact with human feedback. While recent focus has been on RLHF for LLMs, our survey covers the technique across multiple domains. We provide our most comprehensive coverage in control and robotics, where many fundamental techniques originate, alongside a dedicated LLM section. We examine the core principles that underpin RLHF, how algorithms and human feedback work together, and the main research trends in the field. Our goal is to give researchers and practitioners a clear understanding of this rapidly growing field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人类反馈的强化学习综述</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）是强化学习（RL）的一种变体，通过人类反馈而非预设奖励函数进行学习。它建立在偏好强化学习（PbRL）相关研究基础上，处于人工智能与人机交互的交叉领域。这一方向为提升智能系统的性能与适应性、促进其目标与人类价值观对齐提供了可行路径。近年来大规模语言模型（LLM）训练的成功显著展示了该潜力，其中RLHF在引导模型能力契合人类目标方面发挥了决定性作用。本文系统梳理RLHF的基础原理，探讨RL智能体如何与人类反馈交互。尽管近期研究聚焦于LLM领域的RLHF，本综述涵盖多领域技术应用，其中对控制与机器人领域（诸多基础技术发源地）进行了最全面的阐述，并设有LLM专题章节。我们深入剖析RLHF的核心原则、算法与人类反馈的协同机制，以及该领域的主要研究趋势，旨在为研究人员与实践者提供对这一快速发展领域的清晰认知。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey paper is motivated by the need to align AI systems with human values, focusing on Reinforcement Learning from Human Feedback (RLHF) as a method that replaces engineered reward functions with human input to enhance performance and adaptability. The method involves RL agents interacting with human feedback, with the survey covering fundamentals, algorithmic integration, and research trends across domains, notably in control/robotics and large language models (LLMs). The main experimental results highlighted include RLHF&#x27;s decisive role in successfully training LLMs to align with human objectives, demonstrating its broad applicability and effectiveness in directing model capabilities.</div>
<div class="mono" style="margin-top:8px">本综述论文的动机是需要使人工智能系统与人类价值观对齐，重点关注从人类反馈中强化学习（RLHF）这一方法，它用人类输入替代工程化的奖励函数，以提升系统性能和适应性。该方法涉及强化学习智能体与人类反馈的交互，综述涵盖了基本原理、算法集成及跨领域研究趋势，特别是在控制/机器人学和大型语言模型（LLM）中的应用。主要实验结果强调RLHF在成功训练LLM以对齐人类目标中的决定性作用，展示了其在引导模型能力方面的广泛适用性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Heterogeneity in Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Tianyi Hu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu, Min Chen, Xin Yu</div>
<div class="meta-line">First: 2025-12-28T14:07:31+00:00 · Latest: 2025-12-28T14:07:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22941v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22941v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Heterogeneity is a fundamental property in multi-agent reinforcement learning (MARL), which is closely related not only to the functional differences of agents, but also to policy diversity and environmental interactions. However, the MARL field currently lacks a rigorous definition and deeper understanding of heterogeneity. This paper systematically discusses heterogeneity in MARL from the perspectives of definition, quantification, and utilization. First, based on an agent-level modeling of MARL, we categorize heterogeneity into five types and provide mathematical definitions. Second, we define the concept of heterogeneity distance and propose a practical quantification method. Third, we design a heterogeneity-based multi-agent dynamic parameter sharing algorithm as an example of the application of our methodology. Case studies demonstrate that our method can effectively identify and quantify various types of agent heterogeneity. Experimental results show that the proposed algorithm, compared to other parameter sharing baselines, has better interpretability and stronger adaptability. The proposed methodology will help the MARL community gain a more comprehensive and profound understanding of heterogeneity, and further promote the development of practical algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体强化学习中的异质性研究</div>
<div class="mono" style="margin-top:8px">异质性是强化学习多智能体系统（MARL）的基本属性，不仅与智能体的功能差异密切相关，还涉及策略多样性与环境交互。然而，当前MARL领域对异质性缺乏严格定义和深入理解。本文从定义、量化和应用三个维度系统探讨了MARL中的异质性：首先基于智能体层面对MARL建模，将异质性划分为五类并给出数学定义；其次提出异质性距离概念及实用量化方法；最后设计基于异质性的多智能体动态参数共享算法作为方法应用示例。案例研究表明，本方法能有效识别并量化各类智能体异质性。实验结果表明，相较于其他参数共享基线算法，所提算法具有更好的可解释性与更强的环境适应性。该研究框架将助力MARL领域更全面深入地理解异质性，并推动实用算法的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of a rigorous definition and understanding of heterogeneity in multi-agent reinforcement learning (MARL), which is crucial for agent functional differences, policy diversity, and environmental interactions. The authors systematically approach the issue by first categorizing heterogeneity into five types with mathematical definitions, then proposing a quantification method based on a defined heterogeneity distance, and finally demonstrating its utility by designing a heterogeneity-based dynamic parameter sharing algorithm. Experimental case studies confirm the method&#x27;s effectiveness in identifying and quantifying agent heterogeneity, and results show that the proposed algorithm offers better interpretability and stronger adaptability compared to other parameter-sharing baselines, thereby advancing the MARL field&#x27;s comprehension and practical algorithmic development.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体强化学习（MARL）中异质性缺乏严格定义和理解的问题展开研究，异质性与智能体的功能差异、策略多样性和环境交互密切相关。作者从定义、量化和利用三个角度系统性地探讨了异质性：首先基于智能体层面的建模，将异质性分为五类并给出数学定义；其次定义了异质性距离的概念并提出了一种实用的量化方法；最后以设计一种基于异质性的多智能体动态参数共享算法为例，展示了该方法的应用。案例研究表明，该方法能有效识别和量化各类智能体异质性；实验结果表明，所提算法相较于其他参数共享基线方法，具有更好的可解释性和更强的适应性，这将有助于MARL领域更全面、深刻地理解异质性，并进一步推动实用算法的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Ünver Çiftçi</div>
<div class="meta-line">First: 2025-12-28T12:41:09+00:00 · Latest: 2025-12-28T12:41:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22910v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22910v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough&#x27;&#x27; before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sat-EnQ：基于满意化弱Q学习器集成的高可靠性与低计算成本强化学习框架</div>
<div class="mono" style="margin-top:8px">深度Q学习算法在早期训练阶段因最大化算子放大估计误差而存在显著不稳定性。受有限理性理论与发展学习启发，本文提出Sat-EnQ两阶段框架：先实现“足够好”的基准性能，再进行激进优化。第一阶段通过动态基线约束早期价值增长，以满意化目标训练轻量级Q网络集成，在避免灾难性高估的同时生成低方差多样性估计；第二阶段将集成知识蒸馏至大型网络，并用标准双DQN微调。理论证明表明满意化机制能约束更新幅度且不增加目标方差，推论量化了方差显著降低的条件。实验显示Sat-EnQ实现3.8倍方差降低、完全消除灾难性失效（DQN失败率50%）、在环境噪声下保持79%性能，且计算需求比自助集成降低2.5倍。该研究为通过“先满意化后优化”实现鲁棒强化学习提供了理论路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the notorious instability of deep Q-learning, particularly the early training overestimation problem, this paper introduces Sat-EnQ, a two-phase framework inspired by bounded rationality. The method first trains an ensemble of lightweight Q-networks under a satisficing objective that limits value growth using a dynamic baseline to produce diverse, low-variance estimates, then distills the ensemble into a larger network for fine-tuning with standard Double DQN. The main experimental results demonstrate that Sat-EnQ achieves a 3.8x variance reduction, eliminates catastrophic failures (0% vs. 50% for DQN), maintains 79% performance under environmental noise, and requires 2.5x less compute than bootstrapped ensembles.</div>
<div class="mono" style="margin-top:8px">本文针对深度Q学习算法，尤其是早期训练中因估计误差放大导致的不稳定问题，受有限理性理论启发，提出了Sat-EnQ这一两阶段框架。该方法首先在满足性目标下训练一个轻量级Q网络集成，利用动态基线限制早期价值增长以产生多样化的低方差估计，随后将集成知识蒸馏到更大的网络中并用标准双DQN进行微调。主要实验结果表明，Sat-EnQ实现了3.8倍的方差降低，完全避免了灾难性故障（DQN为50%），在环境噪声下保持79%的性能，且所需计算量比自举集成减少2.5倍。</div>
</details>
</div>
<div class="card">
<div class="title">SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Xiaotian Ren, Nuerxiati Abudurexiti, Zhengyong Jiang, Angelos Stefanidis, Hongbin Liu, Jionglong Su</div>
<div class="meta-line">First: 2025-12-28T11:56:39+00:00 · Latest: 2025-12-28T11:56:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22895v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated&#x27;&#x27; mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAMP-HDRL：基于分层深度强化学习的多智能体投资组合管理——分段配置与动量调整效用</div>
<div class="mono" style="margin-top:8px">非平稳市场中的投资组合优化因机制转换、动态相关性及深度强化学习（DRL）策略可解释性有限而面临挑战。本文提出一种基于分层深度强化学习的多智能体投资组合管理方法——分段配置与动量调整效用（SAMP-HDRL）。该框架首先通过动态资产分组将市场划分为优质与普通子集：上层智能体提取全局市场信号，下层智能体在掩码约束下执行组内配置；基于效用的资金分配机制整合风险资产与无风险资产，确保全局与局部决策的协调一致。在三种市场机制（2019-2021年）的回测中，SAMP-HDRL在波动与震荡环境下持续优于九种传统基线方法与九种DRL基准模型。相较于最强基线，本方法实现至少5%的收益率提升、5%的夏普比率提升、5%的索提诺比率提升及2%的欧米茄比率提升，且在动荡市场中收益增幅更为显著。消融实验证实上下层协调、动态聚类与资金分配对模型鲁棒性不可或缺。基于SHAP的可解释性分析进一步揭示了智能体间“分散+集中”的互补机制，为决策过程提供透明洞见。总体而言，SAMP-HDRL将结构性市场约束直接嵌入DRL流程，在复杂金融环境中展现出更强的适应性、鲁棒性与可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenges of portfolio optimization in non-stationary markets, such as regime shifts and the limited interpretability of deep reinforcement learning policies, this paper proposes SAMP-HDRL, a hierarchical deep reinforcement learning framework. The method first dynamically groups assets into high-quality and ordinary subsets, employs an upper-level agent for global market signals and lower-level agents for intra-group allocation under mask constraints, and integrates a utility-based capital allocation mechanism. Experimental backtests across three market regimes (2019-2021) show that SAMP-HDRL consistently outperforms traditional and DRL baselines, achieving at least 5% higher Return, Sharpe ratio, and Sortino ratio, with larger gains in turbulent markets, while ablation studies confirm the importance of its key components and SHAP analysis reveals a complementary decision-making mechanism.</div>
<div class="mono" style="margin-top:8px">针对非平稳市场中投资组合优化的挑战，如市场状态转换和深度强化学习策略可解释性有限，本文提出了SAMP-HDRL，一种分层深度强化学习框架。该方法首先动态地将资产分组为高质量和普通子集，采用上层智能体提取全局市场信号，下层智能体在掩码约束下进行组内资产配置，并集成了基于效用的资本分配机制。在三个市场状态（2019-2021年）的回测实验表明，SAMP-HDRL持续优于传统和深度强化学习基准方法，实现了至少5%的更高收益率、夏普比率和索提诺比率，在动荡市场中收益更大，消融研究确认了其关键组件的重要性，SHAP分析则揭示了互补的决策机制。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks</div>
<div class="meta-line">Authors: Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov</div>
<div class="meta-line">First: 2025-12-28T10:56:20+00:00 · Latest: 2025-12-28T10:56:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22876v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22876v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern AI systems often comprise multiple learnable components that can be naturally organized as graphs. A central challenge is the end-to-end training of such systems without restrictive architectural or training assumptions. Such tasks fit the theory and approaches of the collaborative Multi-Agent Reinforcement Learning (MARL) field. We introduce Reinforcement Networks, a general framework for MARL that organizes agents as vertices in a directed acyclic graph (DAG). This structure extends hierarchical RL to arbitrary DAGs, enabling flexible credit assignment and scalable coordination while avoiding strict topologies, fully centralized training, and other limitations of current approaches. We formalize training and inference methods for the Reinforcement Networks framework and connect it to the LevelEnv concept to support reproducible construction, training, and evaluation. We demonstrate the effectiveness of our approach on several collaborative MARL setups by developing several Reinforcement Networks models that achieve improved performance over standard MARL baselines. Beyond empirical gains, Reinforcement Networks unify hierarchical, modular, and graph-structured views of MARL, opening a principled path toward designing and training complex multi-agent systems. We conclude with theoretical and practical directions - richer graph morphologies, compositional curricula, and graph-aware exploration. That positions Reinforcement Networks as a foundation for a new line of research in scalable, structured MARL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化网络：面向协作式多智能体强化学习任务的新型框架</div>
<div class="mono" style="margin-top:8px">现代人工智能系统常包含多个可学习组件，这些组件可自然组织为图结构。核心挑战在于如何在不施加严格架构或训练假设的前提下实现此类系统的端到端训练。此类任务契合协作式多智能体强化学习（MARL）领域的理论与方法。本文提出强化网络——一种将智能体组织为有向无环图（DAG）顶点的通用MARL框架。该结构将分层强化学习扩展至任意DAG，在避免严格拓扑结构、完全集中式训练等现有方法局限性的同时，实现了灵活的信用分配与可扩展的协同机制。我们形式化定义了强化网络框架的训练与推理方法，并将其与LevelEnv概念关联以支持可复现的构建、训练与评估流程。通过开发多个强化网络模型并在多种协作式MARL场景中验证，本方法在性能上超越了标准MARL基线。除实证优势外，强化网络统一了MARL的分层化、模块化与图结构化视角，为复杂多智能体系统的设计与训练开辟了理论路径。文末探讨了理论与应用方向——更丰富的图形态、组合式课程学习及图感知探索机制，这使强化网络成为可扩展结构化MARL新研究方向的奠基性框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Reinforcement Networks, a novel framework for collaborative Multi-Agent Reinforcement Learning (MARL) motivated by the need to train modern AI systems with multiple learnable components organized as graphs without restrictive architectural or training assumptions. The method organizes agents as vertices in a directed acyclic graph (DAG), extending hierarchical RL to arbitrary DAGs to enable flexible credit assignment and scalable coordination while avoiding limitations like strict topologies or fully centralized training. Experimental results on several collaborative MARL setups show that Reinforcement Networks models achieve improved performance over standard MARL baselines, unifying hierarchical, modular, and graph-structured views of MARL to open a principled path for designing complex multi-agent systems.</div>
<div class="mono" style="margin-top:8px">本文提出了强化网络，这是一种用于协作多智能体强化学习的新框架，其动机在于需要训练由多个可学习组件以图结构组织的现代AI系统，而无需限制性架构或训练假设。该方法将智能体组织为有向无环图中的顶点，将分层强化学习扩展到任意有向无环图，以实现灵活的信用分配和可扩展的协调，同时避免了严格拓扑或完全集中训练等限制。在多个协作多智能体强化学习设置上的实验结果表明，强化网络模型相比标准基线实现了性能提升，统一了分层、模块化和图结构化的多智能体强化学习视角，为设计复杂多智能体系统开辟了原则性路径。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks</div>
<div class="meta-line">Authors: Soham Padia, Dhananjay Vaidya, Ramchandra Mangrulkar</div>
<div class="meta-line">First: 2025-12-28T10:11:32+00:00 · Latest: 2025-12-28T10:11:32+00:00</div>
<div class="meta-line">Comments: 34 pages, 19 figures, 10 tables. Code available at https://github.com/soham-padia/blockchain-iot-trust</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22860v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22860v1">PDF</a> · <a href="https://github.com/soham-padia/blockchain-iot-trust">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Securing blockchain-enabled IoT networks against sophisticated adversarial attacks remains a critical challenge. This paper presents a trust-based delegated consensus framework integrating Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation, combined with learning-based defense mechanisms. We systematically compare three reinforcement learning approaches -- tabular Q-learning (RL), Deep RL with Dueling Double DQN (DRL), and Multi-Agent RL (MARL) -- against five distinct attack families: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI), and Time-Delayed Poisoning (TDP). Experimental results on a 16-node simulated IoT network reveal significant performance variations: MARL achieves superior detection under collusive attacks (F1=0.85 vs. DRL&#x27;s 0.68 and RL&#x27;s 0.50), while DRL and MARL both attain perfect detection (F1=1.00) against adaptive attacks where RL fails (F1=0.50). All agents successfully defend against Byzantine attacks (F1=1.00). Most critically, the Time-Delayed Poisoning attack proves catastrophic for all agents, with F1 scores dropping to 0.11-0.16 after sleeper activation, demonstrating the severe threat posed by trust-building adversaries. Our findings indicate that coordinated multi-agent learning provides measurable advantages for defending against sophisticated trust manipulation attacks in blockchain IoT environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向区块链物联网的自适应信任共识：对比RL、DRL与MARL在应对简单恶意、共谋谣言、自适应、拜占庭及潜伏攻击中的表现</div>
<div class="mono" style="margin-top:8px">保障区块链赋能的物联网网络抵御复杂对抗攻击仍是一项关键挑战。本文提出一种基于信任的委托共识框架，将全同态加密与基于属性的访问控制相结合，实现隐私保护策略评估，并融合基于学习的防御机制。我们系统比较了三种强化学习方法——表格Q学习、基于决斗双深度Q网络的深度强化学习以及多智能体强化学习——在应对五类攻击家族时的表现：简单恶意攻击、共谋谣言攻击、自适应对抗攻击、拜占庭故障注入攻击和时延投毒攻击。在16节点模拟物联网网络上的实验结果显示显著性能差异：多智能体强化学习在共谋攻击下实现最优检测（F1=0.85，深度强化学习为0.68，强化学习为0.50）；深度强化学习与多智能体强化学习对自适应攻击均达到完美检测（F1=1.00），而强化学习失效（F1=0.50）。所有智能体均成功防御拜占庭攻击（F1=1.00）。最关键的是，时延投毒攻击对所有智能体均造成灾难性影响，潜伏激活后F1分数骤降至0.11-0.16，揭示了信任构建型攻击者构成的严重威胁。研究结果表明，在区块链物联网环境中，协同多智能体学习为防御复杂信任操纵攻击提供了可量化的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of securing blockchain-enabled IoT networks against advanced adversarial attacks by proposing a trust-based delegated consensus framework that integrates Fully Homomorphic Encryption and Attribute-Based Access Control for privacy-preserving policy evaluation, combined with learning-based defense mechanisms. The study systematically compares three reinforcement learning approaches—tabular Q-learning (RL), Deep RL with Dueling Double DQN (DRL), and Multi-Agent RL (MARL)—against five distinct attack families: Naive Malicious, Collusive Rumor, Adaptive Adversarial, Byzantine Fault Injection, and Time-Delayed Poisoning. Experimental results on a 16-node simulated IoT network show significant performance variations: MARL achieves superior detection under collusive attacks (F1=0.85 vs. DRL&#x27;s 0.68 and RL&#x27;s 0.50), while DRL and MARL both attain perfect detection (F1=1.00) against adaptive attacks where RL fails (F1=0.50); all agents successfully defend against Byzantine attacks (F1=1.00). However, the Time-Delayed Poisoning attack proves catastrophic, causing F1 scores to drop to 0.11-0.16 after sleeper activation, highlighting the severe threat of trust-building adversaries, with findings indicating that coordinated multi-agent learning offers measurable advantages for defending against sophisticated trust manipulation attacks.</div>
<div class="mono" style="margin-top:8px">本文针对区块链物联网网络面临的高级对抗性攻击安全挑战，提出了一种基于信任的委托共识框架，该框架集成了全同态加密和基于属性的访问控制以实现隐私保护策略评估，并结合了基于学习的防御机制。研究系统比较了三种强化学习方法——表格Q学习（RL）、采用决斗双DQN的深度强化学习（DRL）和多智能体强化学习（MARL）——对抗五种不同的攻击类型：简单恶意攻击、共谋谣言攻击、自适应对抗攻击、拜占庭故障注入攻击和时间延迟投毒攻击。在16个节点的模拟物联网网络上的实验结果显示显著的性能差异：MARL在共谋攻击下实现了更优的检测（F1=0.85，而DRL为0.68，RL为0.50），而DRL和MARL在自适应攻击中均达到完美检测（F1=1.00），而RL则失败（F1=0.50）；所有智能体均成功防御了拜占庭攻击（F1=1.00）。然而，时间延迟投毒攻击被证明是灾难性的，导致休眠激活后F1分数降至0.11-0.16，突显了信任构建对手的严重威胁，研究结果表明协调的多智能体学习为防御复杂的信任操纵攻击提供了可衡量的优势。</div>
</details>
</div>
<div class="card">
<div class="title">AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Shihao Cai, Runnan Fang, Jialong Wu, Baixuan Li, Xinyu Wang, Yong Jiang, Liangcai Su, Liwen Zhang, Wenbiao Yin, Zhen Zhang, Fuli Feng, Pengjun Xie, Xiaobin Wang</div>
<div class="meta-line">First: 2025-12-28T09:43:11+00:00 · Latest: 2025-12-28T09:43:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22857v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22857v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoForge：面向智能体强化学习的自动化环境合成</div>
<div class="mono" style="margin-top:8px">在模拟环境中进行强化学习（RL）为提升基于语言的智能体提供了一种成本效益高且可扩展性强的途径。然而，先前的研究多局限于半自动化的环境合成或难度不足的任务，缺乏广度与深度。此外，环境中集成的模拟用户的不稳定性，以及模拟环境间的异质性，进一步加剧了智能体强化学习的挑战。本研究提出：（1）一个统一流程，用于自动化、可扩展地合成与高难度但易于验证任务相关的模拟环境；（2）一种环境层级的强化学习算法，不仅能有效缓解用户不稳定性，还能在环境层面进行优势估计，从而提升训练效率与稳定性。在tau-bench、tau2-Bench和VitaBench等智能体基准测试上的全面评估验证了所提方法的有效性。进一步的深入分析突显了其出色的跨领域泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of prior reinforcement learning (RL) approaches for language agents, which relied on semi-automated environment synthesis or insufficiently difficult tasks, and were challenged by user instability and environmental heterogeneity. The method introduces AutoForge, featuring a unified pipeline for automated, scalable synthesis of simulated environments with high-difficulty yet easily verifiable tasks, and an environment-level RL algorithm that mitigates user instability and performs advantage estimation at the environment level to enhance training efficiency and stability. The main experimental results, from comprehensive evaluations on benchmarks like tau-bench and VitaBench, validate the method&#x27;s effectiveness and demonstrate its strong out-of-domain generalization capability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是克服先前面向语言智能体的强化学习方法存在的局限，这些方法依赖于半自动化的环境合成或难度不足的任务，并受模拟用户不稳定性和环境异质性挑战。方法提出了AutoForge，包含一个用于自动化、可扩展合成高难度但易于验证任务的模拟环境的统一流程，以及一种环境级强化学习算法，该算法不仅能有效缓解用户不稳定性，还在环境层面进行优势估计，从而提升训练效率和稳定性。主要实验结果通过对tau-bench和VitaBench等基准的综合评估，验证了该方法的有效性，并强调了其出色的领域外泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning</div>
<div class="meta-line">Authors: Gaurav Chaudhary, Laxmidhar Behera</div>
<div class="meta-line">First: 2025-12-28T07:29:29+00:00 · Latest: 2025-12-28T07:29:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22824v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22824v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy&#x27;s confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method&#x27;s underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TEACH：基于时序方差驱动的强化学习课程学习框架</div>
<div class="mono" style="margin-top:8px">强化学习在解决单目标任务方面已取得显著成功，但在多目标场景中，智能体需学习通用目标条件策略，均匀目标选择常导致样本效率低下。受生物系统自适应结构化学习过程的启发，我们提出一种新颖的师生学习范式，通过时序方差驱动课程加速目标条件强化学习。该框架中，教师模块依据策略置信度（以状态-动作价值函数参数化）的时序方差动态优先选择方差最高的目标，通过聚焦这些高不确定性目标提供自适应学习信号，促进持续高效进展。我们建立了Q值时序方差与策略演化的理论关联，揭示了方法的内在机理。本方法具有算法无关性，可无缝集成现有强化学习框架，并在11类机器人操作与迷宫导航任务中验证了其相对于前沿课程学习与目标选择方法的持续显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the sample inefficiency of uniform goal selection in multi-goal reinforcement learning, where agents must learn a universal goal-conditioned policy. To address this, the authors propose TEACH, a student-teacher paradigm that uses a temporal variance-driven curriculum, where the teacher dynamically prioritizes goals with the highest temporal variance in the policy&#x27;s Q-value confidence scores to provide adaptive learning signals targeting high-uncertainty areas. Experimental results across 11 robotic manipulation and maze navigation tasks demonstrate consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods, with the approach being algorithm-agnostic and easily integrated into existing RL frameworks.</div>
<div class="mono" style="margin-top:8px">本文的动机在于多目标强化学习中均匀目标选择导致的样本效率低下问题，其中智能体需学习通用的目标条件策略。为解决此问题，作者提出了TEACH，一种基于时序方差驱动课程的学生-教师范式，教师模块通过动态优先选择策略Q值置信度时序方差最高的目标，以针对高不确定性区域提供自适应学习信号。在11个机器人操作和迷宫导航任务上的实验结果表明，该方法相较于最先进的课程学习和目标选择方法取得了持续且显著的性能提升，且该框架与算法无关，可无缝集成到现有强化学习框架中。</div>
</details>
</div>
<div class="card">
<div class="title">ReDiF: Reinforced Distillation for Few Step Diffusion</div>
<div class="meta-line">Authors: Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee</div>
<div class="meta-line">First: 2025-12-28T06:27:24+00:00 · Latest: 2025-12-28T06:27:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22802v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher&#x27;s outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReDiF：基于强化学习的扩散模型少步蒸馏</div>
<div class="mono" style="margin-top:8px">蒸馏技术通过构建规模更小或步数更少的模型来近似高步数教师模型的行为，以解决扩散模型采样速度慢的问题。本研究提出一种基于强化学习的扩散模型蒸馏框架。该方法不依赖固定的重构损失或一致性损失，而是将蒸馏过程视为策略优化问题，通过从教师模型输出对齐度导出的奖励信号训练学生模型。这种强化学习驱动的框架动态引导学生模型探索多种去噪路径，使其能够朝数据分布的高概率区域采取更长且优化的步骤，而非依赖渐进式细化。该框架利用扩散模型处理更大步长的固有能力，有效管理生成过程。实验结果表明，与现有蒸馏技术相比，该方法能以显著更少的推理步骤和计算资源实现更优性能。此外，该框架与模型无关，可适用于任何具有合适奖励函数的扩散模型，为高效扩散学习提供了通用优化范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the slow sampling of diffusion models by introducing a reinforcement learning-based distillation framework called ReDiF, which treats distillation as a policy optimization problem rather than relying on fixed losses. The method dynamically trains a student model using reward signals from teacher alignment, enabling exploration of multiple denoising paths to take longer, optimized steps toward high-probability data regions. Experiments demonstrate that ReDiF achieves superior performance with significantly fewer inference steps and computational resources compared to existing techniques, while remaining model-agnostic and applicable to various diffusion models with suitable rewards.</div>
<div class="mono" style="margin-top:8px">本文针对扩散模型采样速度慢的问题，提出了一种基于强化学习的蒸馏框架ReDiF，将蒸馏过程视为策略优化问题，而非依赖固定损失函数。该方法通过教师对齐的奖励信号动态训练学生模型，探索多种去噪路径，以采取更长的优化步骤逼近数据分布的高概率区域。实验结果表明，与现有蒸馏技术相比，ReDiF在显著减少推理步骤和计算资源的同时实现了更优性能，且框架与模型无关，适用于具有合适奖励函数的各类扩散模型。</div>
</details>
</div>
<div class="card">
<div class="title">FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents</div>
<div class="meta-line">Authors: Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo</div>
<div class="meta-line">First: 2025-12-28T00:24:01+00:00 · Latest: 2025-12-28T00:24:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22733v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22733v1">PDF</a> · <a href="https://github.com/SHAO-Jiaqi757/FoldAct">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent&#x27;s future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \textbf{FoldAct}\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\times$ speedup.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FoldAct：面向长视野搜索智能体的高效稳定上下文折叠方法</div>
<div class="mono" style="margin-top:8px">面向大语言模型的长视野强化学习面临因上下文无限增长带来的可扩展性挑战，催生了在任务执行期间压缩交互历史的上下文折叠方法。然而，现有方法将摘要动作视为标准动作，忽略了摘要本质上会改变智能体未来的观测空间，形成依赖策略的非平稳观测分布，这违背了强化学习的核心假设。由此引发三个根本性挑战：（1）摘要标记因训练信号不足导致的梯度稀释；（2）策略更新改变摘要分布形成的自我条件循环，引发训练崩溃的恶性循环；（3）每轮处理独特上下文产生的计算成本。我们提出\textbf{FoldAct}框架，通过三项关键创新应对这些挑战：为摘要与动作标记提供独立梯度信号的分离损失计算、降低分布偏移的全上下文一致性损失、以及减少计算成本的选择性片段训练。该方法实现了带上下文折叠的长视野搜索智能体的稳定训练，在解决非平稳观测问题的同时，以5.19倍的加速比提升了训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the scalability challenges in long-horizon reinforcement learning for large language models, where unbounded context growth during task execution leads to inefficiency. Existing context folding methods compress interaction history but treat summaries as standard actions, creating a policy-dependent, non-stationary observation distribution that violates RL assumptions and causes issues like gradient dilution, self-conditioning cycles, and high computational costs. To overcome these, the authors propose FoldAct, a framework featuring separated loss computation for independent gradient signals on summary and action tokens, a full context consistency loss to mitigate distribution shift, and selective segment training to reduce computational overhead. Experimental results demonstrate that FoldAct enables stable training of long-horizon search agents with context folding, effectively addressing the non-stationary observation problem while achieving a 5.19× training speedup.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在长程强化学习中因任务执行时上下文无限增长而导致的扩展性挑战，现有上下文折叠方法虽能压缩交互历史，但将摘要视为标准动作，造成了策略依赖的非平稳观测分布，违背了强化学习假设，并引发梯度稀释、自条件循环和高计算成本等问题。为解决这些挑战，作者提出了FoldAct框架，其核心创新包括：为摘要和动作令牌提供独立梯度信号的分离损失计算、减少分布偏移的完整上下文一致性损失，以及降低计算开销的选择性片段训练。实验结果表明，FoldAct能够稳定训练具有上下文折叠功能的长程搜索智能体，有效解决了非平稳观测问题，同时实现了5.19倍的训练加速。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Context Generalization in Reinforcement Learning from Few Training Contexts</div>
<div class="meta-line">Authors: James Chapman, Kedar Karhadkar, Guido Montufar</div>
<div class="meta-line">Venue: Neurips 2025</div>
<div class="meta-line">First: 2025-07-10T00:23:13+00:00 · Latest: 2025-12-27T22:41:39+00:00</div>
<div class="meta-line">Comments: 10 pages, 8 figures, 3 tables, publushed at Neurips 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.07348v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.07348v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning (DRL) has achieved remarkable success across multiple domains, including competitive games, natural language processing, and robotics. Despite these advancements, policies trained via DRL often struggle to generalize to evaluation environments with different parameters. This challenge is typically addressed by training with multiple contexts and/or by leveraging additional structure in the problem. However, obtaining sufficient training data across diverse contexts can be impractical in real-world applications. In this work, we consider contextual Markov decision processes (CMDPs) with transition and reward functions that exhibit regularity in context parameters. We introduce the context-enhanced Bellman equation (CEBE) to improve generalization when training on a single context. We prove both analytically and empirically that the CEBE yields a first-order approximation to the Q-function trained across multiple contexts. We then derive context sample enhancement (CSE) as an efficient data augmentation method for approximating the CEBE in deterministic control environments. We numerically validate the performance of CSE in simulation environments, showcasing its potential to improve generalization in DRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于少量训练情境的强化学习零样本情境泛化</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）在竞争性游戏、自然语言处理和机器人等多个领域取得了显著成就。然而，通过DRL训练的策略往往难以泛化到参数不同的评估环境。通常的解决方案是通过多情境训练和/或利用问题的附加结构来应对这一挑战，但在实际应用中获取足够多样化的训练数据可能不切实际。本研究针对具有情境参数规律性的转移函数和奖励函数的情境马尔可夫决策过程（CMDP），提出了情境增强贝尔曼方程（CEBE），以提升在单一情境训练时的泛化能力。我们通过理论分析和实验证明，CEBE可对多情境训练的Q函数提供一阶近似。进而推导出情境样本增强（CSE）作为确定性控制环境中近似CEBE的高效数据增强方法。通过仿真环境的数值验证，展示了CSE在提升DRL泛化性能方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of poor generalization in deep reinforcement learning (DRL) policies when evaluated in environments with different parameters, a common issue as obtaining diverse training contexts can be impractical. The method introduces a context-enhanced Bellman equation (CEBE) for contextual Markov decision processes, which analytically provides a first-order approximation to a multi-context Q-function, and derives context sample enhancement (CSE) as a practical data augmentation technique to approximate it. Experimental results from numerical simulations demonstrate that CSE effectively improves the generalization performance of DRL agents trained on only a few contexts.</div>
<div class="mono" style="margin-top:8px">本文针对深度强化学习策略在参数不同的评估环境中泛化能力差的问题展开研究，因为在现实应用中获取多样化的训练环境往往不切实际。该方法为上下文马尔可夫决策过程引入了上下文增强贝尔曼方程，从分析上提供了对多上下文Q函数的一阶近似，并推导出上下文样本增强作为一种实用的数据增强技术来实现该近似。数值模拟实验结果表明，上下文样本增强能够有效提升仅基于少数上下文训练的深度强化学习智能体的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Memento-II: Learning by Stateful Reflective Memory</div>
<div class="meta-line">Authors: Jun Wang</div>
<div class="meta-line">First: 2025-12-27T22:15:03+00:00 · Latest: 2025-12-27T22:15:03+00:00</div>
<div class="meta-line">Comments: 32 pages, three figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22716v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22716v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Memento-II：基于状态化反思记忆的学习</div>
<div class="mono" style="margin-top:8px">本文提出了一种用于大型语言模型智能体持续与经验学习的理论框架，该框架将情景记忆与强化学习相结合。框架将反思确立为关键机制，使智能体能够通过交互实现自适应，无需反向传播或模型微调，从而弱化了传统训练与部署阶段的界限。为形式化这一过程，我们引入了状态化反思决策过程，将反思学习建模为与情景记忆的两阶段读写交互：写入存储交互结果并对应策略评估，读取检索相关历史案例并对应策略改进。研究表明，该过程可在增强的状态-记忆表示上诱导出等价的马尔可夫决策过程，从而允许运用动态规划和强化学习的经典工具。我们进一步通过熵正则化策略迭代实例化该框架，并建立收敛性保证。随着情景记忆增长并充分覆盖状态空间，所得策略将收敛至最优解。这项工作为基于记忆增强与检索的语言模型智能体提供了无需参数更新即可持续自适应的理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for continual and experiential learning in large language model agents without relying on backpropagation or fine-tuning, this paper introduces Memento-II, a theoretical framework that integrates episodic memory with reinforcement learning through a mechanism called reflection. The method formalizes reflective learning as a Stateful Reflective Decision Process, where writing to memory stores interaction outcomes for policy evaluation and reading retrieves past cases for policy improvement, inducing an equivalent Markov decision process over augmented state-memory representations. Experimental results, supported by theoretical analysis, demonstrate that with entropy-regularized policy iteration, the framework guarantees convergence to an optimal policy as episodic memory expands to cover the state space, providing a principled foundation for memory-augmented agents capable of continual adaptation without parameter updates.</div>
<div class="mono" style="margin-top:8px">本文旨在使大型语言模型智能体能够在不依赖反向传播或微调的情况下进行持续和经验学习，为此提出了Memento-II理论框架，通过反思机制将情景记忆与强化学习相结合。该方法将反思学习形式化为状态化反思决策过程，其中向记忆写入存储交互结果以进行策略评估，而读取则检索相关过往案例以进行策略改进，从而在增强的状态-记忆表示上诱导出等价的马尔可夫决策过程。实验与理论分析结果表明，采用熵正则化策略迭代时，随着情景记忆增长并充分覆盖状态空间，所得策略能保证收敛至最优解，这为无需参数更新的记忆增强型智能体提供了持续适应的理论基础。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
