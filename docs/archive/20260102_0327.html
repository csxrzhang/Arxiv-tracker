<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-02 03:27</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260102_0327</div>
    <div class="row"><div class="card">
<div class="title">Scaling Open-Ended Reasoning to Predict the Future</div>
<div class="meta-line">Authors: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</div>
<div class="meta-line">First: 2025-12-31T18:59:51+00:00 · Latest: 2025-12-31T18:59:51+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25070v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25070v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展开放式推理以预测未来</div>
<div class="mono" style="margin-top:8px">高风险决策涉及对未来不确定性的推理。本研究训练语言模型对开放式预测问题进行预测。为扩大训练数据规模，我们基于每日新闻报道的全球事件，采用全自动精细筛选方法合成新的预测问题。我们在自建数据集OpenForesight上训练Qwen3思维模型。为防止训练和评估过程中的未来信息泄露，预测系统全程采用离线新闻语料库进行数据生成与检索。通过小型验证集的指导，我们证明了检索机制的优势以及改进的强化学习奖励函数。最终预测系统在2025年5月至8月期间进行封闭测试。我们的专用模型OpenForecaster 8B性能媲美规模更大的专有模型，其训练显著提升了预测的准确性、校准度和一致性。研究发现预测训练带来的校准改进可泛化至主流基准测试。我们开源全部模型、代码与数据，以推动语言模型预测研究的广泛普及。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for high-stakes decision-making under uncertainty, this work develops language models for open-ended future prediction. The method involves automatically synthesizing forecasting questions from daily news to create the OpenForesight dataset, training Qwen3 thinking models while preventing data leakage via an offline news corpus, and enhancing performance through retrieval and an improved RL reward function. Experimental results on held-out tests from May to August 2025 show that the specialized OpenForecaster 8B model matches larger proprietary models in accuracy, calibration, and consistency, with calibration gains generalizing to other benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究旨在支持高风险不确定性决策，开发用于开放式未来预测的语言模型。方法上，通过从每日新闻中自动合成预测问题构建OpenForesight数据集，使用离线新闻语料防止数据泄露来训练Qwen3思维模型，并利用检索和改进的强化学习奖励函数提升性能。实验结果表明，在2025年5月至8月的保留测试中，专门的OpenForecaster 8B模型在准确性、校准性和一致性上媲美更大的专有模型，且校准改进可推广到其他基准测试。</div>
</details>
</div>
<div class="card">
<div class="title">Many Minds from One Model: Bayesian Transformers for Population Intelligence</div>
<div class="meta-line">Authors: Diji Yang, Yi Zhang</div>
<div class="meta-line">First: 2025-12-31T18:56:02+00:00 · Latest: 2025-12-31T18:56:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25063v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25063v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.
  B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一模型生众智：面向群体智能的贝叶斯Transformer</div>
<div class="mono" style="margin-top:8px">尽管规模庞大且成效显著，现代Transformer几乎都被训练为单一思维系统：优化过程产生一组确定性参数，仅代表对数据的单一功能假设。受&#x27;智能源于众智&#x27;理念启发，我们提出群体贝叶斯Transformer（B-Trans），将标准大语言模型转化为贝叶斯Transformer模型，支持从单组预训练权重中采样生成多样且连贯的模型实例。B-Trans通过将归一化层中类偏置偏移量视为具有高斯变分近似的随机变量，引入贝叶斯启发的后验代理，在不训练完整贝叶斯神经网络的前提下诱导模型行为分布。从该代理采样可获得行为多样且保持通用能力的模型实例集合。为维持单次生成内的连贯性，我们在序列层级冻结采样噪声，确保跨词元的时间一致性。B-Trans支持群体级决策，通过聚合采样个体的预测显著增强探索能力。在零样本生成、可验证奖励强化学习（RLVR）及无显式标签强化学习的实验中，B-Trans有效利用群体智慧，在获得更优任务性能的同时，相比确定性基线实现了更卓越的语义多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the idea that intelligence emerges from many minds rather than a single deterministic model, this paper proposes Population Bayesian Transformers (B-Trans), a method to convert a standard Large Language Model into a Bayesian Transformer. The method introduces a Bayesian posterior proxy by treating bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, enabling efficient sampling of diverse model instances from a single set of pre-trained weights while maintaining coherence through sequence-level noise freezing. Experimental results in zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels show that B-Trans leverages population-level decision-making to achieve superior semantic diversity and better task performance compared to deterministic baselines.</div>
<div class="mono" style="margin-top:8px">受智能源于众多思维而非单一确定性模型这一理念的启发，本文提出了群体贝叶斯变换器（B-Trans），这是一种将标准大语言模型转换为贝叶斯变换器的方法。该方法通过将归一化层中的偏置类偏移视为具有高斯变分近似的随机变量，引入贝叶斯后验代理，从而能够从一组预训练权重中高效采样出多样化的模型实例，同时通过序列级噪声冻结保持生成一致性。在零样本生成、带可验证奖励的强化学习（RLVR）以及无显式标签的强化学习等实验结果表明，B-Trans利用群体层面的决策机制，相比确定性基线模型，实现了更优的语义多样性和任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning</div>
<div class="meta-line">Authors: Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke Hüllermeier</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-31T18:21:52+00:00 · Latest: 2025-12-31T18:21:52+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25023v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResponseRank：通过偏好强度学习实现数据高效奖励建模</div>
<div class="mono" style="margin-top:8px">强化学习人类反馈（RLHF）常用的二元选择仅能传递偏好方向。人们可能选择苹果而非橙子、香蕉而非葡萄，但哪种偏好更强？强度对于不确定性下的决策和偏好模型泛化至关重要，却难以可靠测量。响应时间、标注者间一致性等元数据可作为强度代理指标，但常存在噪声和混杂因素。我们提出ResponseRank以应对从噪声强度信号中学习的挑战。该方法利用代理信号的相对差异，通过推断的偏好强度对成对比较的响应进行排序。为控制系统性变异，我们仅在精心构建的分层内进行局部信号比较。这能在对强度信号做最少假设的前提下，稳健学习与强度排序一致的效用差异。我们的贡献有三方面：（1）ResponseRank：一种通过局部有效相对强度信号稳健学习偏好强度的新方法；（2）在合成偏好学习（模拟响应时间）、语言建模（标注者一致性）、强化学习控制任务（模拟回合回报）等多样化任务中提升样本效率与鲁棒性的实证证据；（3）皮尔逊距离相关性（PDC）：一种将基数效用学习与序数准确性分离的新度量指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of binary preference data in reinforcement learning from human feedback (RLHF), which only indicates direction but not the strength of preferences, crucial for decision-making under uncertainty and model generalization. The authors propose ResponseRank, a method that infers preference strength by leveraging noisy proxy signals like response times and annotator agreement; it ranks responses within carefully constructed strata to control for systemic variation, enabling robust learning of utility differences with minimal assumptions about the strength signal. Experimental results across synthetic preference learning, language modeling, and RL control tasks demonstrate improved sample efficiency and robustness, supported by a novel metric, Pearson Distance Correlation (PDC), for evaluating cardinal utility learning.</div>
<div class="mono" style="margin-top:8px">本文针对基于人类反馈的强化学习（RLHF）中二元偏好数据仅能指示方向而无法反映偏好强度的问题展开研究，偏好强度对于不确定性下的决策和模型泛化至关重要。作者提出了ResponseRank方法，通过利用响应时间和标注者一致性等噪声代理信号来推断偏好强度；该方法在精心构建的层内对响应进行排序以控制系统性变异，从而在最小化对强度信号假设的前提下，稳健地学习效用差异。在合成偏好学习、语言建模和强化学习控制任务上的实验结果表明，该方法提高了样本效率和鲁棒性，并引入了一种新指标——皮尔逊距离相关性（PDC），用于评估基数效用学习。</div>
</details>
</div>
<div class="card">
<div class="title">MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control</div>
<div class="meta-line">Authors: Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She</div>
<div class="meta-line">First: 2025-12-31T16:36:44+00:00 · Latest: 2025-12-31T16:36:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24955v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MSACL：基于李雅普诺夫证书的指数稳定控制多步执行者-评论者学习框架</div>
<div class="mono" style="margin-top:8px">在无模型强化学习中实现可证明的稳定性仍具挑战性，尤其在探索与严格安全性之间的平衡方面。本文提出MSACL框架，通过多步李雅普诺夫证书学习将指数稳定性理论与最大熵强化学习相结合。与依赖复杂奖励工程的方法不同，MSACL利用离策略多步数据学习满足理论稳定性条件的李雅普诺夫证书。通过引入指数稳定性标签（ESL）和λ加权聚合机制，该框架有效平衡多步学习中的偏差-方差权衡。策略优化由稳定性感知优势函数引导，确保习得策略促进李雅普诺夫函数快速下降。我们在六个基准测试（包括镇定和非线性跟踪任务）中评估MSACL，证明其优于现有基于李雅普诺夫的强化学习算法。MSACL在简单奖励条件下实现指数稳定性和快速收敛，同时对不确定性具有显著鲁棒性，并能泛化至未见轨迹。敏感性分析确定多步视界n=20可作为跨不同系统的鲁棒默认值。通过将李雅普诺夫理论与离策略执行者-评论者框架结合，MSACL为可验证的安全学习控制奠定基础。源代码与基准环境将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of achieving provable stability in model-free reinforcement learning by introducing MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. The method utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions, employing Exponential Stability Labels and a λ-weighted aggregation mechanism to balance bias-variance trade-offs, with policy optimization guided by a stability-aware advantage function. Experimental results across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrate MSACL&#x27;s superiority over state-of-the-art Lyapunov-based RL algorithms, achieving exponential stability, rapid convergence under simple rewards, and robustness to uncertainties with generalization to unseen trajectories, while sensitivity analysis identifies a multi-step horizon of n=20 as a robust default.</div>
<div class="mono" style="margin-top:8px">本文针对无模型强化学习中实现可证明稳定性的挑战，提出了MSACL框架，该框架通过多步李雅普诺夫证书学习，将指数稳定性理论与最大熵强化学习相结合。该方法利用离策略多步数据学习满足理论稳定性条件的李雅普诺夫证书，通过引入指数稳定性标签和λ加权聚合机制来平衡偏差-方差权衡，并采用稳定性感知的优势函数指导策略优化。在包括稳定性和非线性跟踪任务在内的六个基准测试上的实验结果表明，MSACL优于现有的基于李雅普诺夫的强化学习算法，在简单奖励下实现了指数稳定性、快速收敛，以及对不确定性的强鲁棒性和对未见轨迹的泛化能力，敏感性分析确定多步视野n=20为跨不同系统的鲁棒默认值。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Deployment Improves Planning Skills in LLMs</div>
<div class="meta-line">Authors: Augusto B. Corrêa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, André G. Pereira, Yarin Gal</div>
<div class="meta-line">First: 2025-12-31T16:03:14+00:00 · Latest: 2025-12-31T16:03:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24940v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models&#x27; deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代部署提升大语言模型的规划能力</div>
<div class="mono" style="margin-top:8px">研究表明，通过迭代部署大语言模型（LLMs），并基于用户从前一轮模型部署中精心筛选的数据对后续模型进行微调，能够显著改变最终模型的特性。在多种规划任务中测试该机制后，我们观察到规划能力得到大幅提升，后续模型展现出涌现的泛化能力，能够生成比初始模型更长的规划方案。理论分析进一步表明，迭代部署本质上是在外层循环中实现了强化学习（RL）训练（而非作为有意识的模型训练环节），并隐含了奖励函数。这种与RL的关联具有两方面重要意义：其一，对AI安全领域而言，由于迭代部署所隐含的奖励函数未明确定义，可能对未来模型部署的特性产生意外影响；其二，该机制可视为替代显式RL的训练范式，其依赖数据筛选而非显式奖励信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study is motivated by the need to understand how iterative deployment of large language models (LLMs), where each iteration is fine-tuned on user-curated data from prior deployments, can alter model properties. The method involves testing this iterative process across various planning domains, revealing that it significantly enhances planning skills, with later models demonstrating emergent generalization by generating much longer plans than initial versions. Experimental results show substantial improvements, and theoretical analysis indicates that iterative deployment effectively functions as an outer-loop reinforcement learning (RL) mechanism with an implicit reward function, which has implications for AI safety due to the undefined nature of this reward and offers an alternative training regime to explicit RL through data curation.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究大语言模型（LLMs）的迭代部署——即每次迭代基于用户从前次部署中精心筛选的数据进行微调——如何改变模型特性。方法是在多个规划领域测试这一迭代过程，发现它显著提升了规划能力，后续模型展现出涌现的泛化能力，能生成比初始模型长得多的计划。实验结果表明了实质性改进，理论分析则揭示迭代部署有效地实现了外层循环的强化学习（RL）机制，具有隐式奖励函数，这对AI安全领域具有影响，因为该奖励函数未明确定义可能带来意外后果，同时它提供了一种通过数据筛选而非显式奖励的替代训练方案。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks</div>
<div class="meta-line">Authors: Shota Suzuki, Satoshi Ono</div>
<div class="meta-line">Venue: IEICE Transactions on Information and Systems, Vol.E108.D, No. 6, pp. 640-643, 2025</div>
<div class="meta-line">First: 2025-12-31T11:30:28+00:00 · Latest: 2025-12-31T11:30:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24793v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24793v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural architecture search (NAS), which automates the architectural design process of deep neural networks (DNN), has attracted increasing attention. Multimodal DNNs that necessitate feature fusion from multiple modalities benefit from NAS due to their structural complexity; however, constructing an architecture for multimodal DNNs through NAS requires a substantial amount of labeled training data. Thus, this paper proposes a self-supervised learning (SSL) method for architecture search of multimodal DNNs. The proposed method applies SSL comprehensively for both the architecture search and model pretraining processes. Experimental results demonstrated that the proposed method successfully designed architectures for DNNs from unlabeled training data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多模态深度神经网络的自监督神经架构搜索</div>
<div class="mono" style="margin-top:8px">神经架构搜索（NAS）通过自动化深度神经网络（DNN）的架构设计过程，已引起广泛关注。多模态DNN因其结构复杂性需融合多模态特征，从而受益于NAS；然而，通过NAS为多模态DNN构建架构需要大量标注训练数据。为此，本文提出一种面向多模态DNN架构搜索的自监督学习（SSL）方法。该方法将SSL全面应用于架构搜索与模型预训练过程。实验结果表明，所提方法成功利用未标注训练数据为DNN设计了有效架构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying neural architecture search (NAS) to multimodal deep neural networks, which typically require large labeled datasets for effective architecture design. The authors propose a self-supervised learning method that integrates SSL into both the architecture search and model pretraining phases, enabling the discovery of effective multimodal network structures without relying on labeled data. Experimental results confirm that the approach successfully designs architectures using only unlabeled training data, demonstrating its potential for data-efficient multimodal model development.</div>
<div class="mono" style="margin-top:8px">本文针对多模态深度神经网络应用神经架构搜索时通常需要大量标注数据的问题，提出了一种自监督学习方法。该方法将自监督学习全面应用于架构搜索和模型预训练过程，从而无需标注数据即可设计网络结构。实验结果表明，所提方法仅使用未标注训练数据就成功设计了网络架构，验证了其在数据高效的多模态模型开发中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation</div>
<div class="meta-line">Authors: Takeru Kusakabe, Yudai Hirose, Mashiho Mukaida, Satoshi Ono</div>
<div class="meta-line">First: 2025-12-31T11:30:03+00:00 · Latest: 2025-12-31T11:30:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24792v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24792v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks (DNNs) remain vulnerable to adversarial attacks that cause misclassification when specific perturbations are added to input images. This vulnerability also threatens the reliability of DNN-based monocular depth estimation (MDE) models, making robustness enhancement a critical need in practical applications. To validate the vulnerability of DNN-based MDE models, this study proposes a projection-based adversarial attack method that projects perturbation light onto a target object. The proposed method employs physics-in-the-loop (PITL) optimization -- evaluating candidate solutions in actual environments to account for device specifications and disturbances -- and utilizes a distributed covariance matrix adaptation evolution strategy. Experiments confirmed that the proposed method successfully created adversarial examples that lead to depth misestimations, resulting in parts of objects disappearing from the target scene.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于投影的对抗攻击：采用物理在环优化针对单目深度估计</div>
<div class="mono" style="margin-top:8px">深度神经网络（DNNs）在面对输入图像添加特定扰动时仍易受对抗攻击而导致误分类，这一脆弱性同样威胁着基于DNN的单目深度估计（MDE）模型的可靠性，使得鲁棒性增强成为实际应用中的关键需求。为验证基于DNN的MDE模型的脆弱性，本研究提出一种基于投影的对抗攻击方法，将扰动光线投射至目标物体。该方法采用物理在环（PITL）优化——通过在真实环境中评估候选解以考虑设备规格和干扰——并利用分布式协方差矩阵自适应进化策略。实验证实，所提方法成功生成了导致深度估计错误的对抗样本，使得目标场景中物体的部分区域消失。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of deep neural networks to adversarial perturbations and the critical need to enhance the robustness of monocular depth estimation models, this paper proposes a projection-based adversarial attack method that physically projects perturbation light onto a target object. The method employs physics-in-the-loop optimization, which evaluates candidate adversarial patterns in real-world environments to account for device specifications and disturbances, utilizing a distributed covariance matrix adaptation evolution strategy for optimization. Experimental results confirmed that the proposed method successfully generated adversarial examples causing significant depth misestimations, such as making parts of objects disappear from the estimated depth scene.</div>
<div class="mono" style="margin-top:8px">本研究针对深度神经网络易受对抗性攻击的脆弱性，以及提升单目深度估计模型鲁棒性的实际需求，提出了一种基于投影的对抗攻击方法，该方法将扰动光线物理投射到目标物体上。该方法采用物理在环优化，通过在真实环境中评估候选对抗模式来考虑设备规格和干扰，并利用分布式协方差矩阵自适应进化策略进行优化。实验结果表明，所提方法成功生成了导致深度估计错误的对抗样本，使得目标场景中物体的部分区域在深度估计中消失。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification</div>
<div class="meta-line">Authors: Mikhael Djajapermana, Moritz Reiber, Daniel Mueller-Gritschneder, Ulf Schlichtmann</div>
<div class="meta-line">First: 2025-11-04T20:52:56+00:00 · Latest: 2025-12-31T10:37:03+00:00</div>
<div class="meta-line">Comments: Presented at ITEM workshop co-located with ECML PKDD 2024, Vilnius LT</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02992v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.02992v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向微型机器学习图像分类的混合卷积与视觉Transformer神经架构搜索空间</div>
<div class="mono" style="margin-top:8px">卷积神经网络与视觉Transformer的混合架构已超越纯卷积或纯Transformer架构。然而，由于这些架构参数量大、计算成本高，难以部署于微型机器学习场景。本文提出一种面向神经架构搜索的新型混合卷积-Transformer搜索空间，旨在为图像分类任务寻找高效混合架构。该搜索空间涵盖可学习局部与全局信息的混合卷积-Transformer模块，以及创新的可搜索池化层模块，以实现高效特征图降维。在CIFAR10数据集上的实验表明，在严格模型尺寸限制下，本搜索空间生成的混合卷积-Transformer架构在精度与推理速度上均优于基于ResNet的微型机器学习模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient yet accurate image classification models suitable for tinyML deployment, where hybrid CNN-ViT architectures typically suffer from high computational costs, this paper introduces a novel Neural Architecture Search (NAS) search space that combines CNN and ViT blocks to capture both local and global features, along with a searchable pooling block for optimized feature map reduction. The method enables automated discovery of compact hybrid architectures. Experimental results on CIFAR10 demonstrate that the discovered models achieve superior accuracy and faster inference speeds compared to ResNet-based tinyML models under strict size constraints.</div>
<div class="mono" style="margin-top:8px">本文的动机是开发适用于微型机器学习部署的高效图像分类模型，因为现有混合CNN-ViT架构通常计算成本过高。方法上，提出了一种新颖的神经架构搜索空间，结合CNN与ViT模块以学习局部和全局信息，并引入可搜索池化模块来优化特征图降维。在CIFAR10数据集上的实验结果表明，在严格模型大小限制下，通过该搜索空间发现的混合架构在准确性和推理速度上均优于基于ResNet的微型机器学习模型。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Offline Reinforcement Learning with Corruption Robustness</div>
<div class="meta-line">Authors: Nam Phuong Tran, Andi Nika, Goran Radanovic, Long Tran-Thanh, Debmalya Mandal</div>
<div class="meta-line">First: 2025-12-31T10:28:25+00:00 · Latest: 2025-12-31T10:28:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24768v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24768v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate robustness to strong data corruption in offline sparse reinforcement learning (RL). In our setting, an adversary may arbitrarily perturb a fraction of the collected trajectories from a high-dimensional but sparse Markov decision process, and our goal is to estimate a near optimal policy. The main challenge is that, in the high-dimensional regime where the number of samples $N$ is smaller than the feature dimension $d$, exploiting sparsity is essential for obtaining non-vacuous guarantees but has not been systematically studied in offline RL. We analyse the problem under uniform coverage and sparse single-concentrability assumptions. While Least Square Value Iteration (LSVI), a standard approach for robust offline RL, performs well under uniform coverage, we show that integrating sparsity into LSVI is unnatural, and its analysis may break down due to overly pessimistic bonuses. To overcome this, we propose actor-critic methods with sparse robust estimator oracles, which avoid the use of pointwise pessimistic bonuses and provide the first non-vacuous guarantees for sparse offline RL under single-policy concentrability coverage. Moreover, we extend our results to the contaminated setting and show that our algorithm remains robust under strong contamination. Our results provide the first non-vacuous guarantees in high-dimensional sparse MDPs with single-policy concentrability coverage and corruption, showing that learning a near-optimal policy remains possible in regimes where traditional robust offline RL techniques may fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏离线强化学习的抗干扰鲁棒性研究</div>
<div class="mono" style="margin-top:8px">本文研究离线稀疏强化学习（RL）对强数据干扰的鲁棒性。在我们的设定中，对手可能任意扰动从高维稀疏马尔可夫决策过程中收集的部分轨迹，目标是估计接近最优的策略。主要挑战在于，在样本数$N$小于特征维度$d$的高维场景下，利用稀疏性对于获得非平凡保证至关重要，但离线RL中尚未系统研究此问题。我们在均匀覆盖和稀疏单集中性假设下分析该问题。虽然鲁棒离线RL的标准方法——最小二乘值迭代（LSVI）在均匀覆盖下表现良好，但我们发现将稀疏性融入LSVI存在固有局限，其分析可能因过于悲观的奖励修正项而失效。为此，我们提出基于稀疏鲁棒估计器oracle的演员-评论家方法，避免使用逐点悲观奖励修正项，首次为单策略集中性覆盖下的稀疏离线RL提供非平凡保证。此外，我们将结果扩展至污染场景，证明算法在强污染下仍保持鲁棒性。本研究首次为具有单策略集中性覆盖与干扰的高维稀疏MDP提供了非平凡保证，表明在传统鲁棒离线RL技术可能失效的场景中，学习接近最优策略仍然可行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robust offline reinforcement learning in high-dimensional sparse Markov decision processes where data may be partially corrupted by an adversary. The motivation stems from the need to estimate near-optimal policies despite adversarial perturbations to a fraction of trajectories, particularly in high-dimensional regimes where sample size is smaller than feature dimension, making sparsity exploitation critical. The method introduces actor-critic approaches that incorporate sparse robust estimator oracles, avoiding the overly pessimistic bonuses associated with traditional Least Square Value Iteration (LSVI) and enabling analysis under sparse single-concentrability assumptions rather than uniform coverage. Experimental results demonstrate that the proposed algorithms achieve non-vacuous guarantees for learning near-optimal policies under single-policy concentrability and strong contamination, outperforming standard robust offline RL techniques that may fail in such sparse, high-dimensional settings.</div>
<div class="mono" style="margin-top:8px">本文研究了在高维稀疏马尔可夫决策过程中，面对数据可能被对手部分破坏的情况下，实现鲁棒的离线强化学习。其动机在于，尽管部分轨迹受到对抗性扰动，仍需估计接近最优的策略，尤其是在样本量小于特征维度的高维场景中，利用稀疏性至关重要。方法上提出了结合稀疏鲁棒估计器oracle的actor-critic方法，避免了传统最小二乘值迭代（LSVI）中过于悲观的奖励机制，并能在稀疏单策略集中性假设下进行分析，而非依赖均匀覆盖。实验结果表明，所提算法在单策略集中性和强污染条件下，能够获得非空泛的保证来学习接近最优的策略，优于传统鲁棒离线强化学习技术，后者在稀疏高维环境中可能失效。</div>
</details>
</div>
<div class="card">
<div class="title">Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow</div>
<div class="meta-line">Authors: Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei, Ruohan Zhang</div>
<div class="meta-line">First: 2025-12-31T10:25:24+00:00 · Latest: 2025-12-31T10:25:24+00:00</div>
<div class="meta-line">Comments: Project website: https://dream2flow.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24766v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24766v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dream2flow.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dream2Flow：通过三维物体流桥接视频生成与开放世界操控</div>
<div class="mono" style="margin-top:8px">生成式视频建模已成为零样本推理开放世界操控中合理物理交互的有力工具。然而，将此类人为引导的运动转化为机器人系统所需的底层动作仍具挑战。我们观察到，给定初始图像和任务指令，这些模型擅长合成合理的物体运动。因此，我们提出Dream2Flow框架，通过三维物体流作为中间表示，桥接视频生成与机器人控制。该方法从生成视频中重建三维物体运动，并将操控任务表述为物体轨迹跟踪。通过将状态变化与实现变化的执行器分离，Dream2Flow克服了具身鸿沟，使预训练视频模型能够零样本指导多种类别物体的操控——包括刚性、铰接式、可变形及颗粒状物体。通过轨迹优化或强化学习，Dream2Flow将重建的三维物体流转化为可执行的底层指令，无需任务特定演示。仿真与真实实验表明，三维物体流是适配视频生成模型至开放世界机器人操控的通用可扩展接口。视频与可视化内容详见项目网站。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for Dream2Flow is to bridge the gap between generative video models, which can reason about plausible physical interactions from instructions, and the low-level actions required for robotic manipulation. The method introduces a framework that uses 3D object flow as an intermediate representation; it reconstructs 3D object motions from videos generated by pre-trained models and formulates manipulation as object trajectory tracking, enabling zero-shot adaptation without task-specific demonstrations. Main experimental results demonstrate that Dream2Flow successfully manipulates diverse object categories—including rigid, articulated, deformable, and granular types—in both simulation and real-world settings, converting 3D flow into executable commands via trajectory optimization or reinforcement learning.</div>
<div class="mono" style="margin-top:8px">Dream2Flow的动机在于弥合生成视频模型（能够根据指令推理合理的物理交互）与机器人操作所需的低级动作之间的差距。该方法提出了一个以3D物体流作为中间表示的框架；它从预训练模型生成的视频中重建3D物体运动，并将操作任务转化为物体轨迹跟踪，从而实现无需任务特定演示的零样本适应。主要实验结果表明，Dream2Flow在仿真和现实环境中成功操作了多种物体类别——包括刚性、铰接、可变形和颗粒状物体，并通过轨迹优化或强化学习将3D流转换为可执行的命令。</div>
</details>
</div>
<div class="card">
<div class="title">AINav: Large Language Model-Based Adaptive Interactive Navigation</div>
<div class="meta-line">Authors: Kangjie Zhou, Yao Mu, Haoyang Song, Yi Zeng, Pengying Wu, Han Gao, Chang Liu</div>
<div class="meta-line">First: 2025-03-29T02:17:52+00:00 · Latest: 2025-12-31T09:40:34+00:00</div>
<div class="meta-line">Comments: 13 pages, 12 figures, accepted to IEEE Robotics &amp; Automation Magazine</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.22942v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.22942v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic navigation in complex environments remains a critical research challenge. Traditional navigation methods focus on optimal trajectory generation within fixed free workspace, therefore struggling in environments lacking viable paths to the goal, such as disaster zones or cluttered warehouses. To address this problem, we propose AINav, an adaptive interactive navigation approach that proactively interacts with environments to create feasible paths to achieve originally unreachable goals. Specifically, we present a primitive skill tree for task planning with large language models (LLMs), facilitating effective reasoning to determine interaction objects and sequences. To ensure robust subtask execution, we adopt reinforcement learning to pre-train a comprehensive skill library containing versatile locomotion and interaction behaviors for motion planning. Furthermore, we introduce an adaptive replanning approach featuring two LLM-based modules: an advisor serving as a flexible replanning trigger and an arborist for autonomous plan adjustment. Integrated with the tree structure, the replanning mechanism allows for convenient node addition and pruning, enabling rapid plan adaptation in a priori unknown environments. Comprehensive simulations and experiments have demonstrated AINav&#x27;s effectiveness and adaptivity in diverse scenarios. The supplementary video is available at: https://youtu.be/CjXm5KFx9AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AINav：基于大语言模型的自适应交互式导航</div>
<div class="mono" style="margin-top:8px">复杂环境下的机器人导航仍是一个关键研究挑战。传统导航方法侧重于在固定自由工作空间内生成最优轨迹，因此在缺乏可行路径到达目标的环境中（如灾区或杂乱仓库）表现不佳。为解决此问题，我们提出AINav，一种自适应交互式导航方法，通过主动与环境交互来创建可行路径，以实现原本无法到达的目标。具体而言，我们构建了一个用于大语言模型任务规划的基元技能树，以促进有效推理来确定交互对象与序列。为确保子任务执行的鲁棒性，我们采用强化学习预训练一个包含多样化运动与交互行为的综合技能库，用于运动规划。此外，我们引入一种自适应重规划方法，包含两个基于大语言模型的模块：作为灵活重规划触发器的顾问模块，以及用于自主调整规划的决策树优化模块。该重规划机制与树结构集成，支持便捷的节点增删，从而在先验未知环境中实现快速规划调整。综合仿真与实验验证了AINav在多种场景下的有效性与适应性。补充视频见：https://youtu.be/CjXm5KFx9AI。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of traditional robotic navigation methods in environments lacking clear paths, such as disaster zones, this paper introduces AINav, an adaptive interactive navigation system that proactively interacts with obstacles to create feasible routes. The method employs a large language model (LLM) for high-level task planning via a primitive skill tree, alongside a reinforcement learning-trained skill library for robust motion execution, and features an adaptive replanning mechanism with LLM-based modules for dynamic plan adjustment. Experimental results from comprehensive simulations demonstrate the system&#x27;s effectiveness and adaptability across diverse scenarios.</div>
<div class="mono" style="margin-top:8px">针对传统机器人导航方法在缺乏可行路径的复杂环境（如灾区）中的局限性，本文提出了AINav，一种自适应交互式导航系统，通过主动与环境交互来创建可行路径。该方法利用大语言模型进行基于原始技能树的高层任务规划，结合强化学习预训练的技能库实现鲁棒的运动执行，并采用基于大语言模块的自适应重规划机制进行动态调整。综合仿真实验结果表明，该系统在多种场景中均表现出有效性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting</div>
<div class="meta-line">Authors: Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao</div>
<div class="meta-line">First: 2025-12-31T08:10:03+00:00 · Latest: 2025-12-31T08:10:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24702v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24702v1">PDF</a> · <a href="https://github.com/AHideoKuzeA/Evol-SAM3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &quot;generate-then-segment&quot; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &quot;Generate-Evaluate-Evolve&quot; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>演进而非训练：基于进化提示的零样本推理分割</div>
<div class="mono" style="margin-top:8px">推理分割要求模型解析复杂且依赖上下文的语言查询以实现像素级定位。当前主流方法严重依赖监督微调（SFT）或强化学习（RL），但SFT存在灾难性遗忘和领域依赖性问题，RL则常受训练不稳定性和对预定义奖励函数僵化依赖的制约。尽管近期免训练方法规避了这些训练负担，但其本质上受限于静态推理范式——通常依赖单次“生成-分割”链式流程，存在推理深度不足、无法自我纠正语言幻觉或空间误判的缺陷。本文突破这些限制，提出EVOL-SAM3这一新型零样本框架，将推理分割重构为推理时进化搜索过程。该框架通过“生成-评估-进化”循环迭代优化提示假设种群，引入视觉竞技场进行无参考成对竞赛评估提示适应性，设计语义变异算子注入多样性并修正语义错误，同时采用异构竞技场模块融合几何先验与语义推理以确保最终选择的鲁棒性。大量实验表明，EVOL-SAM3在零样本设定下不仅显著超越静态基线方法，更在挑战性ReasonSeg基准上大幅优于全监督前沿方法。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of supervised fine-tuning and reinforcement learning in reasoning segmentation, which suffer from issues like catastrophic forgetting and training instability, as well as the static inference and insufficient reasoning depth of training-free methods, this paper proposes EVOL-SAM3, a zero-shot framework that reformulates the task as an evolutionary search process. The method employs a &quot;Generate-Evaluate-Evolve&quot; loop where a population of prompt hypotheses is iteratively refined using a Visual Arena for fitness evaluation via pairwise tournaments and a Semantic Mutation operator to inject diversity and correct errors, with a Heterogeneous Arena integrating geometric priors for final selection. Experimental results show that EVOL-SAM3 substantially outperforms static baselines and surpasses fully supervised state-of-the-art methods on the ReasonSeg benchmark in a zero-shot setting.</div>
<div class="mono" style="margin-top:8px">本文的动机是针对推理分割任务中监督微调和强化学习方法存在的灾难性遗忘、训练不稳定等问题，以及免训练方法静态推理和深度不足的局限，提出了EVOL-SAM3这一零样本框架，将任务重新定义为进化搜索过程。该方法采用“生成-评估-进化”循环，通过视觉竞技场进行基于成对竞争的适应性评估，语义突变算子注入多样性和纠正错误，并结合异构竞技场整合几何先验进行最终选择，迭代优化提示假设种群。实验结果表明，EVOL-SAM3在零样本设置下显著优于静态基线方法，并在ReasonSeg基准测试中超越了完全监督的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation</div>
<div class="meta-line">Authors: Yury Kolomeytsev, Dmitry Golembiovsky</div>
<div class="meta-line">First: 2025-12-31T05:58:57+00:00 · Latest: 2025-12-31T05:58:57+00:00</div>
<div class="meta-line">Comments: 22 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24651v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24651v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous mobile robots operating in complex, dynamic environments face the dual challenge of navigating large-scale, structurally diverse spaces with static obstacles while safely interacting with various moving agents. Traditional graph-based planners excel at long-range pathfinding but lack reactivity, while Deep Reinforcement Learning (DRL) methods demonstrate strong collision avoidance but often fail to reach distant goals due to a lack of global context. We propose Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), a hybrid framework that bridges this gap. Our approach utilizes a graph-based global planner to generate a path, which is integrated into a local DRL policy via a sequence of checkpoints encoded in both the state space and reward function. To ensure social compliance, the local planner employs an entity-aware reward structure that dynamically adjusts safety margins and penalties based on the semantic type of surrounding agents. We validate the proposed method through extensive testing in a realistic simulation environment derived from real-world map data. Comprehensive experiments demonstrate that HMP-DRL consistently outperforms other methods, including state-of-the-art approaches, in terms of key metrics of robot navigation: success rate, collision rate, and time to reach the goal. Overall, these findings confirm that integrating long-term path guidance with semantically-aware local control significantly enhances both the safety and reliability of autonomous navigation in complex human-centric settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的混合运动规划用于移动机器人导航</div>
<div class="mono" style="margin-top:8px">在复杂动态环境中运行的自主移动机器人面临双重挑战：既要在大规模、结构多样的静态障碍空间中导航，又要安全地与各类移动智能体交互。传统基于图的规划器擅长长距离路径规划但缺乏实时反应能力，而深度强化学习方法虽展现出优异的避障能力，却常因缺乏全局信息而难以抵达远距离目标。本文提出基于深度强化学习的混合运动规划框架，通过将基于图的全局规划器生成的路径，以状态空间和奖励函数中编码的检查点序列形式融入局部DRL策略，有效弥合了上述鸿沟。为保障社会合规性，局部规划器采用实体感知的奖励结构，根据周围智能体的语义类型动态调整安全边界与惩罚机制。基于真实地图数据构建的仿真环境测试表明，HMP-DRL在机器人导航关键指标（成功率、碰撞率、抵达时间）上均优于现有先进方法。研究证实，将长期路径引导与语义感知的局部控制相结合，能显著提升复杂人本环境中自主导航的安全性与可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling autonomous mobile robots to navigate complex, dynamic environments by combining the strengths of global pathfinding and reactive local control. The proposed Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL) method integrates a graph-based global planner, which provides long-range path guidance via checkpoints, with a local Deep Reinforcement Learning policy that uses an entity-aware reward structure to ensure safe and socially compliant interactions with various moving agents. Experimental validation in a realistic simulation environment shows that HMP-DRL outperforms state-of-the-art methods, achieving higher success rates, lower collision rates, and faster goal completion times, thereby confirming the efficacy of the hybrid approach for safe and reliable navigation in human-centric settings.</div>
<div class="mono" style="margin-top:8px">本文旨在解决自主移动机器人在复杂动态环境中导航的挑战，通过结合全局路径规划与反应式局部控制的优势。所提出的混合运动规划与深度强化学习方法整合了基于图的全局规划器（通过检查点提供长距离路径引导）和局部深度强化学习策略（该策略采用实体感知的奖励结构，以确保与各类移动智能体进行安全且符合社会规范的交互）。在基于真实地图数据的仿真环境中的综合实验表明，该方法在导航成功率、碰撞率和抵达目标时间等关键指标上均优于现有先进方法，从而验证了这种融合长期路径引导与语义感知局部控制的混合框架能显著提升在复杂人本环境中的导航安全性与可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison</div>
<div class="meta-line">Authors: Yoonho Lee, Joseph Boen, Chelsea Finn</div>
<div class="meta-line">First: 2025-11-11T07:14:13+00:00 · Latest: 2025-12-31T05:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07919v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.07919v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反馈下降：基于成对比较的开放式文本优化框架</div>
<div class="mono" style="margin-top:8px">本文提出《反馈下降》框架，通过结构化文本反馈（而非仅依赖标量奖励）优化文本产物——包括提示词、代码和分子结构。该方法保留详细评析而非压缩为二元偏好，拓宽了偏好学习中的信息瓶颈，实现在文本空间而非权重空间的定向优化。研究表明，上下文学习可将结构化反馈转化为类梯度的方向性信息，从而实现针对性编辑。与先前将判断压缩为单比特的方法不同，本框架的评估器为每次比较配对文本反馈，形成高带宽监督。迭代循环完全在推理阶段完成，无需修改模型权重，且具有任务无关性。我们在三个不同领域评估反馈下降框架，发现其性能优于最先进的提示优化方法（GEPA）、强化学习方法（GRPO、REINVENT），甚至超越专用图基分子优化器。在DOCKSTRING分子发现基准测试中，该框架识别出的新型类药分子超越包含26万种化合物的数据库中99.9%的分子，覆盖六种蛋白质靶点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Feedback Descent, a framework motivated by the need to optimize text artifacts like prompts, code, and molecules using detailed textual feedback rather than limited scalar rewards, thereby addressing the information bottleneck in preference learning. The method leverages in-context learning to convert structured pairwise feedback into gradient-like directional signals, enabling iterative text edits purely at inference time without weight updates, making it task-agnostic. Experimental results across three domains show it outperforms state-of-the-art methods, including in molecule discovery where it identifies novel drug-like compounds exceeding the 99.9th percentile of a large database for multiple protein targets.</div>
<div class="mono" style="margin-top:8px">本文提出了反馈下降框架，其动机在于通过详细的文本反馈而非有限的标量奖励来优化提示、代码和分子等文本产物，从而解决偏好学习中的信息瓶颈问题。该方法利用上下文学习将结构化的成对反馈转化为类似梯度的方向信息，实现在推理时进行纯文本迭代编辑而无需更新模型权重，具有任务无关性。在三个不同领域的实验结果表明，其性能优于现有先进方法，如在分子发现中，针对多个蛋白质靶点识别出的新型类药物分子超越了包含超过26万种化合物数据库的99.9百分位。</div>
</details>
</div>
<div class="card">
<div class="title">Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?</div>
<div class="meta-line">Authors: Zijian Zhao, Sen Li</div>
<div class="meta-line">First: 2025-09-26T13:15:18+00:00 · Latest: 2025-12-31T05:05:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03257v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03257v2">PDF</a> · <a href="https://github.com/RS2002/Triple-BERT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate real-time challenge of bundling and matching passengers-each with distinct origins and destinations-to available vehicles, all while navigating significant system uncertainties. Due to the extensive observation space arising from the large number of drivers and orders, order dispatching, though fundamentally a centralized task, is often addressed using Multi-Agent Reinforcement Learning (MARL). However, independent MARL methods fail to capture global information and exhibit poor cooperation among workers, while Centralized Training Decentralized Execution (CTDE) MARL methods suffer from the curse of dimensionality. To overcome these challenges, we propose Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method designed specifically for large-scale order dispatching on ride-sharing platforms. Built on a variant TD3, our approach addresses the vast action space through an action decomposition strategy that breaks down the joint action probability into individual driver action probabilities. To handle the extensive observation space, we introduce a novel BERT-based network, where parameter reuse mitigates parameter growth as the number of drivers and orders increases, and the attention mechanism effectively captures the complex relationships among the large pool of driver and orders. We validate our method using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves approximately an 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our code, trained model parameters, and processed data are publicly available at the repository https://github.com/RS2002/Triple-BERT .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Triple-BERT：网约车平台订单调度真的需要多智能体强化学习吗？</div>
<div class="mono" style="margin-top:8px">以Uber和Lyft为代表的按需网约车平台面临实时捆绑匹配乘客与车辆的复杂挑战，需在系统高度不确定性的环境下处理海量订单与司机的动态匹配。由于司机与订单规模庞大导致观测空间巨大，订单调度虽本质是集中式任务，却常采用多智能体强化学习（MARL）方法。然而，独立MARL方法难以获取全局信息且智能体协作性差，而集中训练分散执行（CTDE）类MARL方法又受维度灾难制约。为此，我们提出Triple-BERT——一种专为大规模网约车订单调度设计的集中式单智能体强化学习方法。该方法基于TD3算法变体，通过动作分解策略将联合动作概率拆分为独立司机动作概率以应对巨大动作空间；针对庞大观测空间，我们设计了基于BERT的新型网络架构：参数复用机制抑制了司机与订单数量增长带来的参数量膨胀，注意力机制则能有效捕捉海量司机与订单间的复杂关联。基于曼哈顿真实网约车数据的实验表明，Triple-BERT相较当前最优方法实现约11.95%的综合性能提升，其中订单响应量增加4.26%，接驾时间降低22.25%。代码、训练模型及处理数据已开源：https://github.com/RS2002/Triple-BERT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the complex order dispatch problem in ride-sharing platforms, where the large scale of drivers and orders makes traditional Multi-Agent Reinforcement Learning (MARL) methods inefficient due to poor cooperation or high dimensionality. The authors propose Triple-BERT, a centralized single-agent reinforcement learning method based on TD3, which decomposes joint actions into individual driver probabilities and employs a BERT-based network with parameter sharing and attention mechanisms to manage the vast observation space and capture intricate driver-order relationships. Experimental validation on a real-world Manhattan dataset shows that Triple-BERT outperforms state-of-the-art methods, increasing served orders by 4.26%, reducing pickup times by 22.25%, and achieving an overall 11.95% performance improvement.</div>
<div class="mono" style="margin-top:8px">本文针对网约车平台中复杂的订单调度问题展开研究，由于司机和订单数量庞大，传统的多智能体强化学习方法因协作性差或维度灾难而效率低下。作者提出了Triple-BERT，一种基于TD3的集中式单智能体强化学习方法，通过将联合动作分解为单个司机的动作概率，并采用基于BERT的网络结构，利用参数共享和注意力机制来管理庞大的观察空间并捕捉司机与订单间的复杂关系。在纽约曼哈顿的真实数据集上的实验表明，Triple-BERT优于现有最优方法，使成交订单数增加4.26%，接驾时间减少22.25%，整体性能提升约11.95%。</div>
</details>
</div>
<div class="card">
<div class="title">One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms</div>
<div class="meta-line">Authors: Zijian Zhao, Sen Li</div>
<div class="meta-line">First: 2025-07-21T08:04:31+00:00 · Latest: 2025-12-31T05:00:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.15351v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.15351v2">PDF</a> · <a href="https://github.com/RS2002/OSPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Order dispatch is a critical task in ride-sharing systems with Autonomous Vehicles (AVs), directly influencing efficiency and profits. Recently, Multi-Agent Reinforcement Learning (MARL) has emerged as a promising solution to this problem by decomposing the large state and action spaces among individual agents, effectively addressing the Curse of Dimensionality (CoD) in transportation market, which is caused by the substantial number of vehicles, passengers, and orders. However, conventional MARL-based approaches heavily rely on accurate estimation of the value function, which becomes problematic in large-scale, highly uncertain environments. To address this issue, we propose two novel methods that bypass value function estimation, leveraging the homogeneous property of AV fleets. First, we draw an analogy between AV fleets and groups in Group Relative Policy Optimization (GRPO), adapting it to the order dispatch task. By replacing the Proximal Policy Optimization (PPO) baseline with the group average reward-to-go, GRPO eliminates critic estimation errors and reduces training bias. Inspired by this baseline replacement, we further propose One-Step Policy Optimization (OSPO), demonstrating that the optimal policy can be trained using only one-step group rewards under a homogeneous fleet. Experiments on a real-world ride-hailing dataset show that both GRPO and OSPO achieve promising performance across all scenarios, efficiently optimizing pickup times and the number of served orders using simple Multilayer Perceptron (MLP) networks. Furthermore, OSPO outperforms GRPO in all scenarios, attributed to its elimination of bias caused by the bounded time horizon of GRPO. Our code, trained models, and processed data are provided at https://github.com/RS2002/OSPO .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步足矣：基于一步策略优化的多智能体强化学习在网约车平台订单调度中的应用</div>
<div class="mono" style="margin-top:8px">订单调度是自动驾驶车辆（AV）网约车系统中的关键任务，直接影响效率和利润。近年来，多智能体强化学习（MARL）通过将庞大的状态和动作空间分解至个体智能体，有效应对了由大量车辆、乘客和订单引发的交通市场维度灾难（CoD），成为该问题的潜力解决方案。然而，传统基于MARL的方法严重依赖价值函数的精确估计，这在规模庞大、高度不确定的环境中存在问题。为解决此问题，我们提出两种绕过价值函数估计的新方法，利用自动驾驶车队的同质性特性。首先，我们将自动驾驶车队类比为组相对策略优化（GRPO）中的群组，并将其适配至订单调度任务。通过用群组平均未来奖励替代近端策略优化（PPO）基线，GRPO消除了评论家估计误差并减少了训练偏差。受此基线替换启发，我们进一步提出一步策略优化（OSPO），证明在同质车队条件下仅使用单步群组奖励即可训练最优策略。在真实网约车数据集上的实验表明，GRPO和OSPO在所有场景中均取得优异性能，仅使用简单的多层感知机（MLP）网络即可高效优化接载时间和完成订单数。此外，OSPO因消除了GRPO有限时间范围导致的偏差，在所有场景中均优于GRPO。我们的代码、训练模型和处理数据已发布于 https://github.com/RS2002/OSPO。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of order dispatch in ride-sharing systems with autonomous vehicles, where traditional multi-agent reinforcement learning methods suffer from value function estimation errors in large-scale uncertain environments. The authors propose two novel methods that bypass value function estimation by leveraging fleet homogeneity: Group Relative Policy Optimization (GRPO), which adapts group-based policy optimization to replace the critic with group average rewards, and One-Step Policy Optimization (OSPO), which demonstrates that optimal policies can be trained using only one-step group rewards. Experiments on a real-world ride-hailing dataset show that both methods achieve promising performance in optimizing pickup times and served orders, with OSPO outperforming GRPO by eliminating bias from bounded time horizons.</div>
<div class="mono" style="margin-top:8px">本文针对自动驾驶车辆共享出行系统中的订单调度问题，传统多智能体强化学习方法在大规模不确定环境中存在价值函数估计误差的挑战。作者提出了两种绕过价值函数估计的新方法，利用车队同质性：群体相对策略优化（GRPO）将基于群体的策略优化应用于订单调度，用群体平均奖励替代评论家；以及一步策略优化（OSPO），证明仅使用一步群体奖励即可训练最优策略。在真实网约车数据集上的实验表明，两种方法在优化接载时间和完成订单数方面均表现优异，其中OSPO通过消除GRPO有限时间范围带来的偏差，在所有场景中表现更优。</div>
</details>
</div>
<div class="card">
<div class="title">Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</div>
<div class="meta-line">Authors: Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun</div>
<div class="meta-line">First: 2025-12-31T04:17:36+00:00 · Latest: 2025-12-31T04:17:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24615v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24615v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \textbf{Workflow} mode for standard tasks and a \textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Youtu-Agent：通过自动化生成与混合策略优化提升智能体生产力</div>
<div class="mono" style="margin-top:8px">现有大语言模型（LLM）智能体框架面临两大挑战：高配置成本与静态能力局限。构建高质量智能体通常需在工具集成与提示工程上投入大量人工，而已部署的智能体难以适应动态环境，且微调成本高昂。为此，我们提出Youtu-Agent——一个面向LLM智能体自动化生成与持续演进的模块化框架。该框架采用结构化配置系统，将执行环境、工具集与上下文管理解耦，支持灵活复用与自动化合成。我们引入两种生成范式：面向标准任务的“工作流”模式，以及面向复杂非标需求的“元智能体”模式，可自动生成工具代码、提示词与配置。此外，Youtu-Agent建立了混合策略优化系统：（1）“智能体实践”模块支持智能体通过上下文优化积累经验、提升性能，无需参数更新；（2）“智能体强化学习”模块与分布式训练框架集成，支持对任意Youtu-Agent进行端到端、大规模、可扩展的稳定强化学习。实验表明，Youtu-Agent在WebWalkerQA（71.47%）和GAIA（72.8%）基准上使用开源权重模型达到最优性能。自动化生成流程工具合成成功率超81%，实践模块在AIME 2024/2025上分别提升性能2.7%与5.4%。智能体强化学习训练在7B参数LLM上实现40%加速且性能稳定提升，在数学与通用/多跳问答基准上分别将代码推理与搜索能力提升35%和21%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high manual configuration costs and static capabilities of existing LLM agent frameworks, this paper introduces Youtu-Agent, a modular framework for automated agent generation and continuous evolution. The method features a structured configuration system and two generation paradigms—Workflow for standard tasks and Meta-Agent for complex ones—to automatically synthesize tools and prompts, alongside a hybrid policy optimization system combining in-context experience accumulation (Agent Practice) and scalable reinforcement learning (Agent RL). Experimental results show state-of-the-art performance on WebWalkerQA (71.47%) and GAIA (72.8%) with open-weight models, an over 81% tool synthesis success rate, performance gains on AIME benchmarks (+2.7% and +5.4%), and Agent RL achieving a 40% speedup with up to 35% and 21% improvements on coding/reasoning and searching tasks.</div>
<div class="mono" style="margin-top:8px">针对现有大语言模型智能体框架配置成本高、能力静态的问题，本文提出了Youtu-Agent，一个用于自动化生成和持续演进的模块化框架。该方法采用结构化配置系统，提供工作流和元智能体两种生成范式，以自动合成工具和提示，并建立了混合策略优化系统，结合基于上下文经验积累的智能体实践模块和可扩展的强化学习模块。实验结果表明，该框架在WebWalkerQA（71.47%）和GAIA（72.8%）上使用开源模型达到了先进性能，工具合成成功率超过81%，在AIME基准上性能分别提升2.7%和5.4%，且智能体强化学习实现了40%的加速，在数学和通用/多跳问答基准上的编码/推理与搜索能力分别提升高达35%和21%。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization</div>
<div class="meta-line">Authors: Dong Qiu, Duo Xu, Limengxi Yue</div>
<div class="meta-line">First: 2025-12-31T03:59:18+00:00 · Latest: 2025-12-31T03:59:18+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICFTIC 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24609v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习增强型LLM智能体在协同决策与性能优化中的应用</div>
<div class="mono" style="margin-top:8px">大语言模型在语言任务中表现优异，但在多智能体环境中常缺乏协作意识，难以优化全局性能。本文提出一种强化学习增强的LLM智能体框架，将协作建模为去中心化部分可观测马尔可夫决策过程，采用集中训练与分散执行机制。通过引入群体相对策略优化算法，在训练期间利用全局信号联合优化智能体策略，并设计平衡任务质量、速度与协调成本的简化联合奖励函数。在协同写作与编程基准测试中，该框架相比单智能体基线实现任务处理速度提升3倍，写作结构/风格一致性达98.7%，编程测试通过率达74.6%。该方法持续超越主流多智能体LLM基线，为复杂工作流中的可靠协作提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of Large Language Models (LLMs) in multi-agent collaboration, where they often lack cooperative awareness and struggle to optimize global performance. To overcome this, the authors propose a reinforcement learning-augmented LLM agent framework that models cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and employs centralized training with decentralized execution (CTDE). They introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies using global signals during training, along with a simplified joint reward balancing task quality, speed, and coordination costs. Experimental results on collaborative writing and coding benchmarks show that the framework achieves a 3x increase in task processing speed compared to single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding, consistently outperforming strong multi-agent LLM baselines.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在多智能体协作中缺乏合作意识、难以优化全局性能的问题，提出了一种强化学习增强的LLM智能体框架。该方法将协作建模为分散式部分可观测马尔可夫决策过程，并采用集中训练与分散执行的策略，引入了群体相对策略优化算法，在训练期间利用全局信号联合优化智能体策略，同时使用平衡任务质量、速度和协调成本的简化联合奖励。在协作写作和编码基准测试中，该框架相比单智能体基线实现了任务处理速度3倍的提升，写作结构/风格一致性达到98.7%，编码测试通过率为74.6%，持续优于现有的强大多智能体LLM基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning</div>
<div class="meta-line">Authors: Shanyu Han, Yangbo He, Yang Liu</div>
<div class="meta-line">First: 2025-12-31T03:13:22+00:00 · Latest: 2025-12-31T03:13:22+00:00</div>
<div class="meta-line">Comments: 63 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24580v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24580v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a novel framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty. We define two distinct yet coupled risk measures: an inner risk measure addressing state and cost randomness and an outer risk measure capturing transition dynamics uncertainty. Our framework unifies and generalizes most existing RL frameworks by permitting general coherent risk measures for both inner and outer risk measures. Within this framework, we construct a risk-sensitive robust Markov decision process (RSRMDP), derive its Bellman equation, and provide error analysis under a given posterior distribution. We further develop a Bayesian Dynamic Programming (Bayesian DP) algorithm that alternates between posterior updates and value iteration. The approach employs an estimator for the risk-based Bellman operator that combines Monte Carlo sampling with convex optimization, for which we prove strong consistency guarantees. Furthermore, we demonstrate that the algorithm converges to a near-optimal policy in the training environment and analyze both the sample complexity and the computational complexity under the Dirichlet posterior and CVaR. Finally, we validate our approach through two numerical experiments. The results exhibit excellent convergence properties while providing intuitive demonstrations of its advantages in both risk-sensitivity and robustness. Empirically, we further demonstrate the advantages of the proposed algorithm through an application on option hedging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向策略风险敏感强化学习的鲁棒贝叶斯动态规划</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的风险敏感强化学习框架，该框架融合了对状态转移不确定性的鲁棒性。我们定义了两个相互关联的风险度量：内部风险度量处理状态与成本的随机性，外部风险度量捕捉转移动态的不确定性。通过允许内部与外部风险度量采用一般的一致性风险度量，本框架统一并推广了现有大多数强化学习框架。在此框架下，我们构建了风险敏感鲁棒马尔可夫决策过程，推导出其贝尔曼方程，并在给定后验分布下进行了误差分析。进一步，我们开发了一种贝叶斯动态规划算法，该算法在后验更新与值迭代之间交替进行。该方法采用结合蒙特卡洛采样与凸优化的风险贝尔曼算子估计器，并证明了其强一致性保证。此外，我们展示了算法在训练环境中收敛至接近最优策略，并分析了在狄利克雷后验与条件风险价值下的样本复杂度与计算复杂度。最后，通过两个数值实验验证了所提方法。结果展示了优异的收敛特性，并直观地证明了其在风险敏感性与鲁棒性方面的优势。实证中，我们进一步通过期权对冲应用展示了所提算法的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the need for reinforcement learning frameworks that are both risk-sensitive and robust to uncertainty in transition dynamics. The method introduces a novel risk-sensitive robust Markov decision process (RSRMDP) that employs two coupled coherent risk measures: an inner measure for state and cost randomness and an outer measure for transition uncertainty, unifying existing approaches. It develops a Bayesian Dynamic Programming algorithm that alternates posterior updates with value iteration, using a Monte Carlo and convex optimization-based estimator for the Bellman operator with proven consistency. Experimental results from two numerical studies, including an option hedging application, demonstrate the algorithm&#x27;s strong convergence and advantages in balancing risk sensitivity and robustness.</div>
<div class="mono" style="margin-top:8px">本文的动机是强化学习框架需要同时具备风险敏感性以及对状态转移不确定性的鲁棒性。方法上，提出了一种新颖的风险敏感鲁棒马尔可夫决策过程（RSRMDP），采用两个耦合的相干风险度量：内部度量处理状态和成本随机性，外部度量捕捉转移动态不确定性，从而统一了现有方法。开发了一种贝叶斯动态规划算法，交替进行后验更新与值迭代，并使用基于蒙特卡洛采样和凸优化的贝尔曼算子估计器，具有可证明的一致性保证。两个数值实验（包括一个期权对冲应用）的结果表明，该算法收敛性良好，并在平衡风险敏感性与鲁棒性方面展现出优势。</div>
</details>
</div>
<div class="card">
<div class="title">From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme</div>
<div class="meta-line">Authors: Xueyan Li, Yingyi Xue, Mengjie Jiang, Qingzi Zhu, Yazhe Niu</div>
<div class="meta-line">First: 2025-12-31T01:35:49+00:00 · Latest: 2025-12-31T01:35:49+00:00</div>
<div class="meta-line">Comments: 46 pages, 20 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24555v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24555v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating humorous memes is a challenging multimodal task that moves beyond direct image-to-caption supervision. It requires a nuanced reasoning over visual content, contextual cues, and subjective humor. To bridge this gap between visual perception and humorous punchline creation, we propose HUMOR}, a novel framework that guides VLMs through hierarchical reasoning and aligns them with group-wise human preferences. First, HUMOR employs a hierarchical, multi-path Chain-of-Thought (CoT): the model begins by identifying a template-level intent, then explores diverse reasoning paths under different contexts, and finally anchors onto a high-quality, context-specific path. This CoT supervision, which traces back from ground-truth captions, enhances reasoning diversity. We further analyze that this multi-path exploration with anchoring maintains a high expected humor quality, under the practical condition that high-quality paths retain significant probability mass. Second, to capture subjective humor, we train a pairwise reward model that operates within groups of memes sharing the same template. Following established theory, this approach ensures a consistent and robust proxy for human preference, even with subjective and noisy labels. The reward model then enables a group-wise reinforcement learning optimization, guaranteeing providing a theoretical guarantee for monotonic improvement within the trust region. Extensive experiments show that HUMOR empowers various VLMs with superior reasoning diversity, more reliable preference alignment, and higher overall meme quality. Beyond memes, our work presents a general training paradigm for open-ended, human-aligned multimodal generation, where success is guided by comparative judgment within coherent output group.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从感知到笑点：赋能视觉语言模型掌握开放场景下的表情包艺术</div>
<div class="mono" style="margin-top:8px">生成幽默表情包是一项超越直接图像-标题监督的复杂多模态任务，需要对视觉内容、上下文线索和主观幽默进行细致推理。为弥合视觉感知与幽默笑点创作之间的鸿沟，我们提出HUMOR框架，通过分层推理引导视觉语言模型，并将其与群体化人类偏好对齐。首先，HUMOR采用分层多路径思维链：模型先识别模板级意图，随后在不同语境下探索多样化推理路径，最终锚定高质量的情境特定路径。这种从真实标注回溯的思维链监督增强了推理多样性。我们进一步分析表明，在高质量路径保持显著概率质量的现实条件下，这种带锚定的多路径探索能维持较高的预期幽默质量。其次，为捕捉主观幽默，我们训练了在共享同一模板的表情包群组内运作的成对奖励模型。依据成熟理论，该方法即使面对主观且含噪的标注，也能确保获得一致稳健的人类偏好代理。该奖励模型进而支持群组化强化学习优化，为信任域内的单调改进提供理论保证。大量实验表明，HUMOR能赋予各类视觉语言模型更卓越的推理多样性、更可靠的偏好对齐能力及更高的整体表情包质量。除表情包外，本研究为开放式、人类对齐的多模态生成提供了通用训练范式，其成功关键在于对连贯输出群组内的比较性判断。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating humorous memes, which requires nuanced multimodal reasoning beyond simple captioning, by proposing the HUMOR framework to bridge visual perception and punchline creation. The method employs a hierarchical multi-path Chain-of-Thought approach to enhance reasoning diversity through template intent identification and context-specific path anchoring, alongside a pairwise reward model trained on meme template groups to align with subjective human preferences via group-wise reinforcement learning. Experimental results demonstrate that HUMOR empowers various vision-language models with improved reasoning diversity, better preference alignment, and higher overall meme quality, offering a general paradigm for human-aligned multimodal generation.</div>
<div class="mono" style="margin-top:8px">本文针对生成幽默表情包这一超越简单配图的多模态推理挑战，提出了HUMOR框架以连接视觉感知与笑点创作。该方法采用分层多路径思维链，通过模板意图识别和上下文特定路径锚定来增强推理多样性，并结合基于表情包模板组训练的成对奖励模型，利用分组强化学习对齐主观人类偏好。实验结果表明，HUMOR能赋能多种视觉语言模型，提升推理多样性、偏好对齐可靠性和整体表情包质量，为开放式、人类对齐的多模态生成提供了通用训练范式。</div>
</details>
</div>
<div class="card">
<div class="title">From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning</div>
<div class="meta-line">Authors: Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera</div>
<div class="meta-line">First: 2025-12-31T00:36:03+00:00 · Latest: 2025-12-31T00:36:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24532v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24532v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从基础模块到规划：基于强化学习的大语言模型多步空间推理</div>
<div class="mono" style="margin-top:8px">大语言模型的空间推理能力因在导航与规划中的应用而日益受到关注。尽管具备强大的通用语言能力，大语言模型在结构化环境中的空间变换与多步规划方面仍存在困难。本文提出一种两阶段方法，将空间推理分解为原子级基础模块及其组合：首先通过监督微调学习旋转、平移、缩放等基础空间变换，使模型掌握空间物理规律；随后冻结该物理感知模型，在GRPO框架内训练轻量级LoRA适配器，以闭环方式学习组合这些基础模块的策略，实现谜题环境中的多步规划。为支持该流程，我们构建了ASCII艺术数据集及对应的ASCII强化学习环境。实验表明，在具有显式状态更新的动态环境和需依赖内部状态跨步推理的静态环境中，本方法均稳定优于基线模型（包括通用主干模型、物理感知模型及端到端强化学习模型）。此外，相较于从零开始的端到端强化学习，该方法收敛更快、训练更稳定。最后，我们通过注意力模式分析验证了微调对空间理解能力的实质性提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling large language models (LLMs) to perform multi-step spatial reasoning, which is crucial for navigation and planning tasks, as current models often struggle with spatial transformations and sequential planning. The proposed method employs a two-stage approach: first, supervised fine-tuning teaches the model basic spatial physics through atomic transformations like rotation and translation; second, with the physics-aware model frozen, lightweight LoRA adapters are trained using reinforcement learning (GRPO framework) to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, supported by a synthesized ASCII-art dataset and environment. Experimental results show that this approach consistently outperforms baselines, including the generic backbone and end-to-end RL models, in both dynamic and static environments, with faster convergence and more stable training, while attention pattern analysis indicates meaningful improvements in spatial understanding.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在空间推理中难以处理空间变换和多步规划的问题展开研究，旨在提升其在导航和规划等任务中的应用能力。方法采用两阶段策略：首先通过监督微调让模型学习旋转、平移等基本空间物理变换；随后冻结该物理感知模型，使用强化学习框架训练轻量级LoRA适配器，以组合这些基础模块在基于谜题的环境中进行多步规划，并构建了ASCII艺术数据集和环境支持。实验结果表明，该方法在动态和静态环境中均优于基线模型，包括通用骨干模型和端到端强化学习模型，且训练收敛更快、更稳定，注意力模式分析也证实了空间理解能力的有效提升。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-fidelity Bayesian Optimization: A Review</div>
<div class="meta-line">Authors: Bach Do, Ruda Zhang</div>
<div class="meta-line">Venue: AIAA Journal 63:6 (2025) 2286-2322</div>
<div class="meta-line">First: 2023-11-21T23:22:11+00:00 · Latest: 2025-12-31T00:31:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2311.13050v3">Abs</a> · <a href="https://arxiv.org/pdf/2311.13050v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resided at the intersection of multi-fidelity optimization (MFO) and Bayesian optimization (BO), MF BO has found a niche in solving expensive engineering design optimization problems, thanks to its advantages in incorporating physical and mathematical understandings of the problems, saving resources, addressing exploitation-exploration trade-off, considering uncertainty, and processing parallel computing. The increasing number of works dedicated to MF BO suggests the need for a comprehensive review of this advanced optimization technique. In this paper, we survey recent developments of two essential ingredients of MF BO: Gaussian process (GP) based MF surrogates and acquisition functions. We first categorize the existing MF modeling methods and MFO strategies to locate MF BO in a large family of surrogate-based optimization and MFO algorithms. We then exploit the common properties shared between the methods from each ingredient of MF BO to describe important GP-based MF surrogate models and review various acquisition functions. By doing so, we expect to provide a structured understanding of MF BO. Finally, we attempt to reveal important aspects that require further research for applications of MF BO in solving intricate yet important design optimization problems, including constrained optimization, high-dimensional optimization, optimization under uncertainty, and multi-objective optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多保真度贝叶斯优化综述</div>
<div class="mono" style="margin-top:8px">多保真度贝叶斯优化（MF BO）融合了多保真度优化（MFO）与贝叶斯优化（BO），凭借其在整合问题物理与数学理解、节约资源、平衡探索与利用、处理不确定性以及并行计算等方面的优势，已成为解决昂贵工程设计优化问题的有效方法。随着相关研究日益增多，对这一先进优化技术进行系统性综述显得尤为必要。本文聚焦MF BO的两大核心要素：基于高斯过程（GP）的多保真度代理模型与采集函数。我们首先对现有MF建模方法与MFO策略进行分类，将MF BO置于基于代理的优化及MFO算法体系中进行定位；进而通过分析各要素方法的共性，阐述重要的GP基多保真度代理模型，并综述各类采集函数，以构建对MF BO的结构化认知。最后，本文探讨了MF BO在求解复杂关键设计优化问题（包括约束优化、高维优化、不确定性优化及多目标优化）时仍需深入研究的重要方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to efficiently solve expensive engineering design problems by leveraging multiple information sources of varying cost and accuracy, this review paper surveys the field of multi-fidelity Bayesian optimization (MF BO), which combines multi-fidelity optimization with Bayesian optimization. The method focuses on analyzing two core components: Gaussian process-based multi-fidelity surrogate models for approximating the objective function and acquisition functions for guiding the optimization search. The main experimental results, drawn from synthesizing the literature, demonstrate that MF BO effectively balances exploration and exploitation, saves computational resources, and handles uncertainty, though the review identifies open challenges in areas like constrained, high-dimensional, and multi-objective optimization for future research.</div>
<div class="mono" style="margin-top:8px">本文旨在通过利用不同成本和精度的多源信息来高效解决昂贵的工程设计问题，综述了融合多保真度优化与贝叶斯优化的多保真度贝叶斯优化领域。方法上重点分析了两个核心组成部分：用于近似目标函数的基于高斯过程的多保真度代理模型，以及用于指导优化搜索的采集函数。通过综合文献，主要实验结果表明，该方法能有效平衡探索与利用、节省计算资源并处理不确定性，同时综述指出了在约束优化、高维优化和多目标优化等方向仍需进一步研究的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics</div>
<div class="meta-line">Authors: Akash Samanta, Sheldon Williamson</div>
<div class="meta-line">First: 2025-12-30T19:57:52+00:00 · Latest: 2025-12-30T19:57:52+00:00</div>
<div class="meta-line">Comments: This preprint focuses on the theoretical framework and diagnostic behavior. Comprehensive experimental validation in application-specific settings is deferred to a companion experimental study</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24445v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Building on this framework, we derive diagnostic-driven instantiations including a stabilized supervised optimizer, a diagnostic-regulated actor-critic scheme, and a diagnostic-conditioned learned optimizer. Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to temporal-difference error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏差-噪声-对齐诊断的自适应学习框架</div>
<div class="mono" style="margin-top:8px">部署于非平稳安全关键环境的学习系统常面临学习动态演化时的不稳定性、收敛缓慢或脆弱适应等问题。现代优化、强化学习和元学习方法虽能适应梯度统计特性，却大多忽略误差信号本身的时间结构。本文提出一种诊断驱动的自适应学习框架，通过将误差演化分解为三个原则性分量进行显式建模：捕捉持续漂移的偏差、捕捉随机波动的噪声、以及捕捉导致超调的重复杂向激励的对齐。这些诊断指标通过损失函数或时序差分误差轨迹的轻量级统计量在线计算，且独立于模型架构与任务领域。研究表明，所提出的偏差-噪声-对齐分解为监督优化、演员-评论家强化学习及可学习优化器提供了统一的控制框架。基于此框架，我们推导出诊断驱动的具体实现方案：包括稳定化监督优化器、诊断调节型演员-评论家机制以及诊断条件化可学习优化器。在标准平滑性假设下，我们为所有案例建立了有效更新的有界性与稳定性证明。演员-评论家学习中的典型诊断示例揭示了所提信号如何根据时序差分误差结构调节适应过程。总体而言，本研究将误差演化提升为自适应学习中的核心对象，为动态环境下的可靠学习提供了可解释、轻量化的理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the instability and slow convergence of learning systems in nonstationary environments, this paper introduces a diagnostic-driven adaptive learning framework that models error evolution through a decomposition into bias, noise, and alignment components. The method computes these diagnostics online from lightweight statistics of error trajectories, independent of model architecture, and applies them as a control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers, leading to instantiations like a stabilized optimizer and a diagnostic-regulated actor-critic scheme. Under smoothness assumptions, the approach ensures bounded updates and stability, with experimental illustrations in actor-critic learning demonstrating how the diagnostics modulate adaptation based on temporal-difference error structure, providing an interpretable foundation for reliable learning in dynamic settings.</div>
<div class="mono" style="margin-top:8px">针对非平稳环境中学习系统的不稳定性和收敛缓慢问题，本文提出了一种诊断驱动的自适应学习框架，通过将误差分解为偏差、噪声和对齐分量来建模误差演化。该方法从误差轨迹的轻量级统计中在线计算这些诊断指标，独立于模型架构，并将其作为监督优化、演员-评论家强化学习和学习优化器的控制主干，衍生出稳定优化器和诊断调节的演员-评论家方案等实例。在平滑性假设下，该方法确保了更新有界和稳定性，演员-批评学习中的实验示例展示了诊断如何根据时序差分误差结构调节适应过程，为动态环境中的可靠学习提供了可解释的轻量级基础。</div>
</details>
</div>
<div class="card">
<div class="title">Lagrangian Index Policy for Restless Bandits with Average Reward</div>
<div class="meta-line">Authors: Konstantin Avrachenkov, Vivek S. Borkar, Pratik Shah</div>
<div class="meta-line">First: 2024-12-17T08:03:53+00:00 · Latest: 2025-12-30T19:29:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.12641v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.12641v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the Lagrangian Index Policy (LIP) for restless multi-armed bandits with long-run average reward. In particular, we compare the performance of LIP with the performance of the Whittle Index Policy (WIP), both heuristic policies known to be asymptotically optimal under certain natural conditions. Even though in most cases their performances are very similar, in the cases when WIP shows bad performance, LIP continues to perform very well. We then propose reinforcement learning algorithms, both tabular and NN-based, to obtain online learning schemes for LIP in the model-free setting. The proposed reinforcement learning schemes for LIP require significantly less memory than the analogous schemes for WIP. We calculate analytically the Lagrangian index for the restart model, which applies to the optimal web crawling and the minimization of the weighted age of information. We also give a new proof of asymptotic optimality in case of homogeneous arms as the number of arms goes to infinity, based on exchangeability and de Finetti&#x27;s theorem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平均奖励约束下不安定老虎机问题的拉格朗日指数策略</div>
<div class="mono" style="margin-top:8px">本研究针对长期平均奖励约束下的不安定多臂老虎机问题，探讨拉格朗日指数策略（LIP）的性能。通过与Whittle指数策略（WIP）的系统对比发现：在多数场景下两者性能相近，但在WIP表现欠佳的特殊案例中，LIP仍能保持优异性能。我们进一步提出基于表格方法和神经网络的强化学习算法，实现了模型无关环境下LIP的在线学习方案。相较于WIP的对应方案，所提出的LIP强化学习方案显著降低了内存需求。针对重启模型（适用于最优网络爬虫和加权信息年龄最小化问题），我们解析推导了拉格朗日指数计算公式。此外，基于可交换性和德菲内蒂定理，我们为同质臂情形提供了当臂数量趋于无穷时渐进最优性的新证明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the Lagrangian Index Policy (LIP) for restless multi-armed bandits under an average reward criterion, motivated by the need for effective heuristics that remain robust where the well-known Whittle Index Policy (WIP) fails. The method involves analytically deriving the Lagrangian index for specific models like the restart model and proposing both tabular and neural network-based reinforcement learning algorithms to implement LIP in a model-free setting, which notably reduces memory requirements compared to analogous WIP schemes. Experimental results demonstrate that LIP performs similarly to WIP in many cases but maintains strong performance in scenarios where WIP deteriorates, and the paper also provides a new proof of LIP&#x27;s asymptotic optimality for homogeneous arms using exchangeability and de Finetti&#x27;s theorem.</div>
<div class="mono" style="margin-top:8px">本文研究了平均奖励准则下的拉格朗日指数策略（LIP）用于 restless 多臂老虎机问题，其动机在于当著名的 Whittle 指数策略（WIP）表现不佳时，仍需一种保持鲁棒性的有效启发式方法。该方法包括为重启模型等特定模型解析推导拉格朗日指数，并提出了基于表格和神经网络的强化学习算法，以在无模型设置中实现 LIP，与类似的 WIP 方案相比显著降低了内存需求。实验结果表明，LIP 在许多情况下与 WIP 表现相似，但在 WIP 性能恶化的场景中仍保持良好性能，同时论文还利用可交换性和 de Finetti 定理为同质臂情况提供了 LIP 渐近最优性的新证明。</div>
</details>
</div>
<div class="card">
<div class="title">Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue Settings</div>
<div class="meta-line">Authors: Liangqi Yuan, Dong-Jun Han, Shiqiang Wang, Christopher G. Brinton</div>
<div class="meta-line">First: 2025-02-16T06:18:28+00:00 · Latest: 2025-12-30T19:01:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.11007v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.11007v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compared to traditional machine learning models, recent large language models (LLMs) can exhibit multi-task-solving capabilities through multiple dialogues and multi-modal data sources. These unique characteristics of LLMs, together with their large model size, make their deployment more challenging. Specifically, (i) deploying LLMs on local devices faces computational, memory, and energy resource issues, while (ii) deploying them in the cloud cannot guarantee real-time service and incurs communication/usage costs. In this paper, we design TMO, a local-cloud LLM inference system with Three-M Offloading: Multi-modal, Multi-task, and Multi-dialogue. TMO incorporates (i) a lightweight local LLM that can process simple tasks at high speed and (ii) a large-scale cloud LLM that can handle multi-modal data sources. We develop a resource-constrained reinforcement learning (RCRL) strategy for TMO that optimizes the inference location (i.e., local vs. cloud) and multi-modal data sources to use for each task/dialogue, aiming to maximize the long-term reward (response quality, latency, and usage cost) while adhering to resource constraints. We also contribute M4A1, a new dataset we curated that contains reward and cost metrics across multiple modality, task, dialogue, and LLM configurations, enabling evaluation of offloading decisions. We demonstrate the effectiveness of TMO compared to several exploration-decision and LLM-as-Agent baselines, showing significant improvements in latency, cost, and response quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态、多任务、多对话场景下大语言模型的本地-云端推理卸载</div>
<div class="mono" style="margin-top:8px">相比传统机器学习模型，近期的大语言模型（LLM）可通过多轮对话和多模态数据源展现多任务处理能力。LLM的这些独特特性及其庞大的模型规模，使其部署面临更大挑战：一方面，在本地设备部署会受计算、内存和能源资源限制；另一方面，云端部署则难以保证实时性且会产生通信/使用成本。本文设计了TMO——一种具备三模态卸载（多模态、多任务、多对话）的本地-云端LLM推理系统。TMO包含：（1）可高速处理简单任务的轻量级本地LLM；（2）能处理多模态数据源的大规模云端LLM。我们为TMO开发了资源约束强化学习策略，通过优化每个任务/对话的推理位置（本地或云端）及多模态数据源选择，在满足资源约束的同时最大化长期收益（响应质量、延迟与使用成本）。我们还构建了M4A1数据集，该数据集涵盖多模态、多任务、多对话及多LLM配置下的收益与成本指标，可用于评估卸载决策。实验表明，相较于多种探索决策和LLM智能体基线方法，TMO在延迟、成本和响应质量方面均有显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the deployment challenges of large language models (LLMs), which face computational and memory constraints on local devices and latency or cost issues in the cloud, particularly in multi-modal, multi-task, and multi-dialogue scenarios. To tackle this, the authors propose TMO, a local-cloud inference system that employs a lightweight local LLM for simple tasks and a large-scale cloud LLM for complex multi-modal data, using a resource-constrained reinforcement learning strategy to optimize offloading decisions by balancing response quality, latency, and cost. Experimental results on the newly curated M4A1 dataset show that TMO significantly outperforms baseline methods in latency, cost, and response quality.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在部署中面临的挑战展开研究，即在本地设备上存在计算、内存和能源限制，而在云端部署则无法保证实时性并产生通信或使用成本，尤其是在多模态、多任务和多对话场景下。为此，作者提出了TMO系统，该系统结合轻量级本地大语言模型处理简单任务和大型云端大语言模型处理复杂多模态数据，并采用资源约束强化学习策略来优化卸载决策，以权衡响应质量、延迟和成本。在新构建的M4A1数据集上的实验结果表明，TMO在延迟、成本和响应质量方面显著优于多种基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models</div>
<div class="meta-line">Authors: Lars van der Laan, Aurelien Bibaut, Nathan Kallus</div>
<div class="meta-line">First: 2025-12-30T18:41:05+00:00 · Latest: 2025-12-30T18:41:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24407v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24407v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse reinforcement learning (IRL) and dynamic discrete choice (DDC) models explain sequential decision-making by recovering reward functions that rationalize observed behavior. Flexible IRL methods typically rely on machine learning but provide no guarantees for valid inference, while classical DDC approaches impose restrictive parametric specifications and often require repeated dynamic programming. We develop a semiparametric framework for debiased inverse reinforcement learning that yields statistically efficient inference for a broad class of reward-dependent functionals in maximum entropy IRL and Gumbel-shock DDC models. We show that the log-behavior policy acts as a pseudo-reward that point-identifies policy value differences and, under a simple normalization, the reward itself. We then formalize these targets, including policy values under known and counterfactual softmax policies and functionals of the normalized reward, as smooth functionals of the behavior policy and transition kernel, establish pathwise differentiability, and derive their efficient influence functions. Building on this characterization, we construct automatic debiased machine-learning estimators that allow flexible nonparametric estimation of nuisance components while achieving $\sqrt{n}$-consistency, asymptotic normality, and semiparametric efficiency. Our framework extends classical inference for DDC models to nonparametric rewards and modern machine-learning tools, providing a unified and computationally tractable approach to statistical inference in IRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逆强化学习与动态离散选择模型的高效推断方法</div>
<div class="mono" style="margin-top:8px">逆强化学习（IRL）与动态离散选择（DDC）模型通过还原解释观测行为的奖励函数来阐释序列决策过程。灵活的IRL方法通常依赖机器学习但无法保证有效推断，而经典DDC方法采用限制性参数设定且常需重复动态规划。本文提出一种半参数化的去偏逆强化学习框架，为最大熵IRL与Gumbel冲击DDC模型中广泛奖励依赖泛函提供统计高效推断。研究表明：对数行为策略可作为伪奖励函数，在简单归一化条件下可点识别策略价值差异及奖励本身。我们将这些目标（包括已知与反事实softmax策略下的策略价值、归一化奖励泛函）形式化为行为策略与转移核的光滑泛函，建立路径可微性并推导其高效影响函数。基于此特征，构建自动去偏机器学习估计量，支持对干扰成分进行灵活非参数估计，同时实现√n一致性、渐近正态性与半参数有效性。本框架将经典DDC模型推断扩展至非参数化奖励与现代机器学习工具，为IRL统计推断提供统一且计算可行的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing methods in inverse reinforcement learning (IRL) and dynamic discrete choice (DDC) models, where flexible machine learning approaches lack inference guarantees and classical parametric methods are restrictive and computationally intensive. The authors propose a semiparametric framework for debiased IRL, showing that the log-behavior policy serves as a pseudo-reward to identify policy value differences and, with normalization, the reward itself. They formalize targets like policy values and reward functionals as smooth functionals of the behavior policy and transition kernel, derive efficient influence functions, and construct debiased machine-learning estimators. Experimental results demonstrate that these estimators achieve √n-consistency, asymptotic normality, and semiparametric efficiency, extending classical DDC inference to nonparametric rewards and enabling computationally tractable statistical inference in IRL.</div>
<div class="mono" style="margin-top:8px">本文针对逆强化学习（IRL）和动态离散选择（DDC）模型中现有方法的局限性，即灵活的机器学习方法缺乏统计推断保证，而经典参数化方法限制性强且计算成本高。作者提出了一种去偏逆强化学习的半参数框架，证明对数行为策略可作为伪奖励来识别策略价值差异，并在归一化后识别奖励本身。他们将策略价值和奖励函数等目标形式化为行为策略和转移核的光滑泛函，推导了有效影响函数，并构建了去偏机器学习估计器。实验结果表明，这些估计器实现了√n一致性、渐近正态性和半参数效率，将经典DDC推断扩展到非参数奖励，为IRL提供了统一且计算可行的统计推断方法。</div>
</details>
</div>
<div class="card">
<div class="title">MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems</div>
<div class="meta-line">Authors: Wan Jiang, Xinyi Zang, Yudong Zhao, Yusi Zou, Yunfei Lu, Junbo Tong, Yang Liu, Ming Li, Jiani Shi, Xin Yang</div>
<div class="meta-line">First: 2025-12-30T16:27:41+00:00 · Latest: 2025-12-30T16:27:41+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24325v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24325v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern recommender systems face significant computational challenges due to growing model complexity and traffic scale, making efficient computation allocation critical for maximizing business revenue. Existing approaches typically simplify multi-stage computation resource allocation, neglecting inter-stage dependencies, thus limiting global optimality. In this paper, we propose MaRCA, a multi-agent reinforcement learning framework for end-to-end computation resource allocation in large-scale recommender systems. MaRCA models the stages of a recommender system as cooperative agents, using Centralized Training with Decentralized Execution (CTDE) to optimize revenue under computation resource constraints. We introduce an AutoBucket TestBench for accurate computation cost estimation, and a Model Predictive Control (MPC)-based Revenue-Cost Balancer to proactively forecast traffic loads and adjust the revenue-cost trade-off accordingly. Since its end-to-end deployment in the advertising pipeline of a leading global e-commerce platform in November 2024, MaRCA has consistently handled hundreds of billions of ad requests per day and has delivered a 16.67% revenue uplift using existing computation resources.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MaRCA：面向大规模推荐系统动态计算分配的多智能体强化学习框架</div>
<div class="mono" style="margin-top:8px">现代推荐系统因模型复杂度与流量规模持续增长而面临显著计算挑战，高效的计算分配对最大化商业收益至关重要。现有方法通常简化多阶段计算资源分配，忽略阶段间依赖关系，从而限制了全局最优性。本文提出MaRCA——一种面向大规模推荐系统端到端计算资源分配的多智能体强化学习框架。MaRCA将推荐系统各阶段建模为协作智能体，采用集中训练分散执行（CTDE）范式，在计算资源约束下优化收益。我们引入AutoBucket测试平台以实现精确计算成本估算，并设计基于模型预测控制（MPC）的收益-成本平衡器，主动预测流量负载并动态调整收益-成本权衡。自2024年11月在头部全球电商平台的广告流水线完成端到端部署以来，MaRCA每日稳定处理数千亿广告请求，在维持现有计算资源条件下实现16.67%的收益提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the computational inefficiencies in large-scale recommender systems by proposing MaRCA, a multi-agent reinforcement learning framework that optimizes dynamic computation allocation across interdependent stages to maximize revenue under resource constraints. The method employs Centralized Training with Decentralized Execution (CTDE) to coordinate cooperative agents representing system stages, alongside an AutoBucket TestBench for precise cost estimation and a Model Predictive Control (MPC)-based balancer to forecast traffic and adjust revenue-cost trade-offs. Experimental results from deployment on a global e-commerce platform show that MaRCA handles hundreds of billions of daily ad requests and achieves a 16.67% revenue increase using existing computational resources.</div>
<div class="mono" style="margin-top:8px">本文针对大规模推荐系统中因模型复杂性和流量增长导致的计算效率问题，提出了MaRCA框架，该框架采用多智能体强化学习来优化动态计算资源分配，以在资源约束下最大化收入。方法上，它通过集中训练与分散执行（CTDE）协调代表系统各阶段的合作智能体，并引入AutoBucket测试平台进行精确成本估算，以及基于模型预测控制（MPC）的收入-成本平衡器来预测流量并调整权衡。实验结果表明，自2024年11月在全球领先电商平台的广告管道中端到端部署以来，MaRCA每日处理数千亿广告请求，并利用现有计算资源实现了16.67%的收入提升。</div>
</details>
</div>
<div class="card">
<div class="title">DRL-TH: Jointly Utilizing Temporal Graph Attention and Hierarchical Fusion for UGV Navigation in Crowded Environments</div>
<div class="meta-line">Authors: Ruitong Li, Lin Zhang, Yuenan Zhao, Chengxin Liu, Ran Song, Wei Zhang</div>
<div class="meta-line">First: 2025-12-30T15:17:07+00:00 · Latest: 2025-12-30T15:17:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24284v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24284v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep reinforcement learning (DRL) methods have demonstrated potential for autonomous navigation and obstacle avoidance of unmanned ground vehicles (UGVs) in crowded environments. Most existing approaches rely on single-frame observation and employ simple concatenation for multi-modal fusion, which limits their ability to capture temporal context and hinders dynamic adaptability. To address these challenges, we propose a DRL-based navigation framework, DRL-TH, which leverages temporal graph attention and hierarchical graph pooling to integrate historical observations and adaptively fuse multi-modal information. Specifically, we introduce a temporal-guided graph attention network (TG-GAT) that incorporates temporal weights into attention scores to capture correlations between consecutive frames, thereby enabling the implicit estimation of scene evolution. In addition, we design a graph hierarchical abstraction module (GHAM) that applies hierarchical pooling and learnable weighted fusion to dynamically integrate RGB and LiDAR features, achieving balanced representation across multiple scales. Extensive experiments demonstrate that our DRL-TH outperforms existing methods in various crowded environments. We also implemented DRL-TH control policy on a real UGV and showed that it performed well in real world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DRL-TH：联合利用时序图注意力与层次融合实现拥挤环境下无人地面车辆导航</div>
<div class="mono" style="margin-top:8px">深度强化学习方法在拥挤环境中无人地面车辆的自主导航与避障方面展现出潜力。现有方法多依赖单帧观测，并采用简单拼接进行多模态融合，限制了时序上下文捕捉能力与动态适应性。为解决这些问题，本文提出基于深度强化学习的导航框架DRL-TH，通过时序图注意力与层次图池化整合历史观测并自适应融合多模态信息。具体而言，我们提出时序引导图注意力网络，将时序权重融入注意力评分以捕捉连续帧间关联，从而隐式估计场景演化；同时设计图层次抽象模块，通过层次池化与可学习加权融合动态整合RGB与激光雷达特征，实现多尺度平衡表征。大量实验表明，DRL-TH在多种拥挤环境中优于现有方法。我们在真实无人地面车辆上部署了DRL-TH控制策略，验证了其在现实场景中的良好性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of existing deep reinforcement learning (DRL) methods for unmanned ground vehicle (UGV) navigation, which often rely on single-frame observations and simplistic multi-modal fusion, thereby lacking temporal context and dynamic adaptability in crowded settings. The proposed method, DRL-TH, introduces a temporal-guided graph attention network (TG-GAT) to incorporate historical observations with temporal weights for capturing scene evolution, alongside a graph hierarchical abstraction module (GHAM) that uses hierarchical pooling and learnable fusion to dynamically balance RGB and LiDAR features across scales. Experimental results show that DRL-TH outperforms existing methods in simulated crowded environments and performs effectively when deployed on a real UGV in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有深度强化学习方法在无人地面车辆导航中的局限性，这些方法通常依赖单帧观测和简单的多模态融合，导致在拥挤环境中缺乏时序上下文和动态适应性。所提出的DRL-TH方法采用时序引导图注意力网络来整合带有时序权重的历史观测以捕捉场景演化，并结合图层次抽象模块通过分层池化和可学习融合动态平衡RGB与激光雷达的多尺度特征。实验结果表明，DRL-TH在多种模拟拥挤环境中优于现有方法，并在真实无人地面车辆的实际场景中表现良好。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem</div>
<div class="meta-line">Authors: Pengfu Wan, Jiawei Chen, Gangyan Xu</div>
<div class="meta-line">First: 2025-12-30T14:26:33+00:00 · Latest: 2025-12-30T14:26:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24251v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Fleet Size and Mix Vehicle Routing Problem (FSMVRP) is a prominent variant of the Vehicle Routing Problem (VRP), extensively studied in operations research and computational science. FSMVRP requires simultaneous decisions on fleet composition and routing, making it highly applicable to real-world scenarios such as short-term vehicle rental and on-demand logistics. However, these requirements also increase the complexity of FSMVRP, posing significant challenges, particularly in large-scale and time-constrained environments. In this paper, we propose a deep reinforcement learning (DRL)-based approach for solving FSMVRP, capable of generating near-optimal solutions within a few seconds. Specifically, we formulate the problem as a Markov Decision Process (MDP) and develop a novel policy network, termed FRIPN, that seamlessly integrates fleet composition and routing decisions. Our method incorporates specialized input embeddings designed for distinctdecision objectives, including a remaining graph embedding to facilitate effective vehicle employment decisions. Comprehensive experiments are conducted on both randomly generated instances and benchmark datasets. The experimental results demonstrate that our method exhibits notable advantages in terms of computational efficiency and scalability, particularly in large-scale and time-constrained scenarios. These strengths highlight the potential of our approach for practical applications and provide valuable inspiration for extending DRL-based techniques to other variants of VRP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的车队规模与混合车辆路径问题求解</div>
<div class="mono" style="margin-top:8px">车队规模与混合车辆路径问题（FSMVRP）是车辆路径问题（VRP）的一个重要变体，在运筹学和计算科学领域得到广泛研究。FSMVRP需同时决策车队构成与路径规划，使其在短期车辆租赁、按需物流等现实场景中具有高度适用性。然而，这些要求也增加了FSMVRP的复杂性，尤其在规模庞大且时间受限的环境中构成显著挑战。本文提出一种基于深度强化学习（DRL）的FSMVRP求解方法，能在数秒内生成近似最优解。具体而言，我们将问题建模为马尔可夫决策过程（MDP），并开发了一种名为FRIPN的新型策略网络，无缝整合车队构成与路径决策。该方法采用针对不同决策目标设计的专用输入嵌入，包括用于辅助车辆调度决策的剩余图嵌入。我们在随机生成实例和基准数据集上进行了全面实验，结果表明：该方法在计算效率和可扩展性方面表现突出，尤其在大规模与时间受限场景中。这些优势凸显了本方法在实际应用中的潜力，并为将基于DRL的技术扩展至其他VRP变体提供了重要启示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the complex Fleet Size and Mix Vehicle Routing Problem (FSMVRP), which involves simultaneous decisions on vehicle fleet composition and delivery routes, a challenge critical for applications like short-term rentals and on-demand logistics. The authors propose a deep reinforcement learning (DRL) approach, formulating FSMVRP as a Markov Decision Process and introducing a novel policy network called FRIPN that integrates fleet and routing decisions with specialized input embeddings, including a remaining graph embedding. Experimental results on random and benchmark datasets show that the method generates near-optimal solutions rapidly, offering superior computational efficiency and scalability, especially in large-scale, time-sensitive scenarios, thereby demonstrating practical potential and inspiring extensions to other VRP variants.</div>
<div class="mono" style="margin-top:8px">本文针对车队规模与混合车辆路径问题（FSMVRP）这一复杂挑战展开研究，该问题需要同时决定车队构成和配送路线，在短期租赁和按需物流等实际应用中至关重要。作者提出了一种基于深度强化学习的方法，将FSMVRP建模为马尔可夫决策过程，并设计了一种名为FRIPN的新型策略网络，该网络通过专门的输入嵌入（包括剩余图嵌入）来整合车队和路径决策。在随机生成实例和基准数据集上的实验结果表明，该方法能快速生成近似最优解，尤其在大规模和时间受限的场景中展现出卓越的计算效率和可扩展性，凸显了其实际应用潜力，并为将深度强化学习技术扩展到其他车辆路径问题变体提供了有益启示。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Parameter Efficient Methods for RLVR</div>
<div class="meta-line">Authors: Qingyu Yin, Yulun Wu, Zhennan Shen, Sunbowen Li, Zhilin Wang, Yanshu Li, Chak Tou Leong, Jiale Kang, Jinjin Gu</div>
<div class="meta-line">First: 2025-12-29T03:13:08+00:00 · Latest: 2025-12-30T13:11:05+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23165v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23165v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估RLVR的参数高效方法</div>
<div class="mono" style="margin-top:8px">我们在可验证奖励强化学习（RLVR）范式下，系统评估了参数高效微调（PEFT）方法。RLVR通过可验证反馈激励语言模型提升推理能力；然而，尽管LoRA等方法被广泛使用，但适用于RLVR的最佳PEFT架构仍未明确。本研究首次在数学推理基准上对DeepSeek-R1-Distill系列模型进行了超过12种PEFT方法的全面评估。实证结果对默认采用标准LoRA的做法提出了挑战，主要发现有三点：首先，结构变体（如DoRA、AdaLoRA、MiSS）持续优于LoRA；其次，我们揭示了基于SVD的初始化策略（如PiSSA、MiLoRA）存在谱崩溃现象，其失败归因于主成分更新与RL优化的根本性错配；此外，消融实验表明极端参数削减（如VeRA、Rank-1）会严重制约推理能力。我们通过消融研究与扩展实验进一步验证了结论。本工作为倡导参数高效RL方法的深入探索提供了权威指南。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to identify optimal Parameter-Efficient Fine-Tuning (PEFT) methods for Reinforcement Learning with Verifiable Rewards (RLVR), a paradigm that enhances language model reasoning through verifiable feedback, as the default use of methods like LoRA lacks empirical justification. The method involves a systematic evaluation of over 12 PEFT techniques, including structural variants and SVD-informed strategies, on the DeepSeek-R1-Distill model families using mathematical reasoning benchmarks. The main experimental results reveal that structural variants such as DoRA, AdaLoRA, and MiSS consistently outperform standard LoRA, while SVD-based methods like PiSSA and MiLoRA fail due to spectral collapse from misalignment with RL optimization, and extreme parameter reduction methods severely impair reasoning capacity, providing a definitive guide for future PEFT exploration in RL.</div>
<div class="mono" style="margin-top:8px">本文的研究动机在于，强化学习与可验证奖励范式通过可验证反馈增强语言模型推理能力，但如LoRA等参数高效微调方法的默认使用缺乏实证依据，因此需要确定其最优方法。研究方法包括在DeepSeek-R1-Distill模型系列上，使用数学推理基准对超过12种参数高效微调技术进行系统评估，涵盖结构变体和SVD初始化策略。主要实验结果表明，DoRA、AdaLoRA和MiSS等结构变体持续优于标准LoRA，而PiSSA和MiLoRA等SVD方法因与强化学习优化的根本错配导致谱崩溃而失败，且极端参数削减方法严重制约推理能力，这为未来参数高效强化学习方法的探索提供了明确指导。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming</div>
<div class="meta-line">Authors: Ningwei Bai, Chi Pui Chan, Qichen Yin, Tengyang Gong, Yunda Yan, Zezhi Tang</div>
<div class="meta-line">First: 2025-12-05T22:52:22+00:00 · Latest: 2025-12-30T13:01:10+00:00</div>
<div class="meta-line">Comments: 9 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15735v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.15735v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于事件触发鲁棒自适应动态规划的不确定非线性系统深度强化学习优化</div>
<div class="mono" style="margin-top:8px">本研究提出一种统一控制架构，将强化学习驱动的控制器与抗扰扩展状态观测器相结合，并辅以事件触发机制以限制不必要的计算。扩展状态观测器用于实时估计系统状态与集总扰动，为有效扰动补偿奠定基础。为在缺乏精确系统描述的情况下获得近似最优行为，采用基于值迭代的自适应动态规划方法进行策略逼近。事件触发机制的引入确保学习模块的参数更新仅在状态偏差超过预设阈值时执行，从而避免过度学习活动并显著降低计算负荷。通过李雅普诺夫稳定性分析表征了闭环系统的稳定性。数值实验进一步证实，所提方法在保持强控制性能与扰动容忍度的同时，相比标准时间触发自适应动态规划方案显著降低了采样与处理开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient and robust control of uncertain nonlinear systems, this paper proposes a unified architecture combining reinforcement learning with an extended state observer and an event-triggered mechanism. The method uses the observer to estimate states and disturbances in real time for compensation, employs adaptive dynamic programming for near-optimal policy approximation without precise system models, and integrates an event-triggered mechanism to update learning parameters only when necessary, reducing computational load. Experimental results demonstrate that the approach maintains strong control performance and disturbance rejection while significantly lowering sampling and processing efforts compared to standard time-triggered methods, with stability validated via Lyapunov analysis.</div>
<div class="mono" style="margin-top:8px">本文针对不确定非线性系统的高效鲁棒控制需求，提出了一种结合强化学习、扩张状态观测器和事件触发机制的统一架构。该方法利用观测器实时估计系统状态和集总扰动以进行补偿，采用自适应动态规划在无需精确系统模型的情况下近似最优策略，并通过事件触发机制仅在状态偏差超过预设界限时更新学习参数，从而降低计算负担。实验结果表明，与标准时间触发方案相比，该方法在保持强大控制性能和扰动容忍度的同时，显著减少了采样和处理开销，其闭环系统的稳定性通过李雅普诺夫分析得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">GARDO: Reinforcing Diffusion Models without Reward Hacking</div>
<div class="meta-line">Authors: Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan</div>
<div class="meta-line">First: 2025-12-30T10:55:45+00:00 · Latest: 2025-12-30T10:55:45+00:00</div>
<div class="meta-line">Comments: 17 pages. Project: https://tinnerhrhe.github.io/gardo_project</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24138v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24138v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tinnerhrhe.github.io/gardo_project">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GARDO：规避奖励破解的扩散模型强化框架</div>
<div class="mono" style="margin-top:8px">通过在线强化学习微调扩散模型已展现出提升文本-图像对齐能力的巨大潜力。然而，由于视觉任务中难以精确指定真实目标，模型常通过仅部分反映真实目标的代理奖励进行优化，这种错配易导致奖励破解现象——代理分数上升的同时真实图像质量下降、生成多样性坍缩。现有方案通常通过参考策略正则化来防止奖励破解，但参考策略往往次优，这会牺牲样本效率并阻碍对高奖励新区域的探索。为平衡样本效率、有效探索与奖励破解缓解这三重需求，我们提出具有多样性感知优化的门控自适应正则化框架GARDO。其核心洞见在于：正则化无需全局应用，而应选择性惩罚高不确定性样本子集。针对探索难题，GARDO引入自适应正则化机制，定期更新参考模型以匹配在线策略能力；针对强化学习中的模式坍缩问题，GARDO对兼具高质量与高多样性的样本实施奖励放大，在稳定优化过程中促进模式覆盖。跨多样代理奖励与未知评估指标的实验表明，GARDO能在不牺牲样本效率或探索能力的前提下持续缓解奖励破解并提升生成多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the problem of reward hacking in fine-tuning diffusion models via online reinforcement learning, where optimizing for proxy rewards can degrade real image quality and diversity. The authors propose GARDO, a framework that selectively applies regularization to high-uncertainty samples, adaptively updates the reference policy to aid exploration, and boosts rewards for diverse high-quality outputs to prevent mode collapse. Experiments across various proxy rewards and unseen metrics demonstrate that GARDO effectively mitigates reward hacking, enhances diversity, and maintains sample efficiency without compromising exploration.</div>
<div class="mono" style="margin-top:8px">本文针对扩散模型通过在线强化学习微调时出现的奖励黑客问题展开研究，即优化代理奖励可能导致真实图像质量下降和多样性丧失。作者提出了GARDO框架，其核心在于选择性地对高不确定性样本进行正则化，自适应更新参考策略以促进探索，并奖励多样化的高质量输出以防止模式崩溃。在不同代理奖励和未见指标上的实验表明，GARDO能有效缓解奖励黑客问题，提升生成多样性，同时保持样本效率和探索能力。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design</div>
<div class="meta-line">Authors: Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte</div>
<div class="meta-line">First: 2025-12-30T10:01:55+00:00 · Latest: 2025-12-30T10:01:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24120v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24120v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated neural network architecture design remains a significant challenge in computer vision. Task diversity and computational constraints require both effective architectures and efficient search methods. Large Language Models (LLMs) present a promising alternative to computationally intensive Neural Architecture Search (NAS), but their application to architecture generation in computer vision has not been systematically studied, particularly regarding prompt engineering and validation strategies. Building on the task-agnostic NNGPT/LEMUR framework, this work introduces and validates two key contributions for computer vision. First, we present Few-Shot Architecture Prompting (FSAP), the first systematic study of the number of supporting examples (n = 1, 2, 3, 4, 5, 6) for LLM-based architecture generation. We find that using n = 3 examples best balances architectural diversity and context focus for vision tasks. Second, we introduce Whitespace-Normalized Hash Validation, a lightweight deduplication method (less than 1 ms) that provides a 100x speedup over AST parsing and prevents redundant training of duplicate computer vision architectures. In large-scale experiments across seven computer vision benchmarks (MNIST, CIFAR-10, CIFAR-100, CelebA, ImageNette, SVHN, Places365), we generated 1,900 unique architectures. We also introduce a dataset-balanced evaluation methodology to address the challenge of comparing architectures across heterogeneous vision tasks. These contributions provide actionable guidelines for LLM-based architecture search in computer vision and establish rigorous evaluation practices, making automated design more accessible to researchers with limited computational resources.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强基于LLM的神经网络生成：面向自动化架构设计的少样本提示与高效验证方法</div>
<div class="mono" style="margin-top:8px">自动化神经网络架构设计仍是计算机视觉领域的重大挑战。任务多样性与计算资源限制要求同时具备有效的架构与高效的搜索方法。大语言模型为替代计算密集的神经架构搜索提供了新思路，但其在计算机视觉架构生成中的应用尚未得到系统研究，尤其在提示工程与验证策略方面。本研究基于任务无关的NNGPT/LEMUR框架，针对计算机视觉领域提出并验证了两项核心贡献：首先，我们提出少样本架构提示方法，首次系统研究了支持样本数量对LLM架构生成的影响，发现采用3个示例能在视觉任务中最佳平衡架构多样性与上下文聚焦；其次，我们提出空白标准化哈希验证方法，这种轻量级去重技术仅需不足1毫秒，比AST解析提速百倍，能有效避免重复视觉架构的冗余训练。通过在七个计算机视觉基准数据集的大规模实验中生成1900个独立架构，我们同时提出数据集平衡评估方法以解决异构视觉任务的架构比较难题。这些成果为计算机视觉领域的LLM架构搜索提供了可操作的指导原则，并建立了严谨的评估体系，使计算资源有限的研究者能更便捷地开展自动化设计研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of automated neural network design for computer vision by leveraging Large Language Models (LLMs) as an efficient alternative to traditional Neural Architecture Search. The primary motivation is to systematically study LLM-based generation, focusing on prompt engineering and validation to overcome computational constraints. The method introduces Few-Shot Architecture Prompting (FSAP) to determine the optimal number of supporting examples, finding n=3 best balances diversity and focus, and proposes a fast Whitespace-Normalized Hash Validation for deduplication, achieving a 100x speedup over AST parsing. Experimental results from generating 1,900 unique architectures across seven benchmarks demonstrate the effectiveness of these contributions, along with a dataset-balanced evaluation methodology, providing practical guidelines for resource-efficient automated design.</div>
<div class="mono" style="margin-top:8px">本研究针对计算机视觉中自动化神经网络设计的挑战，利用大语言模型作为传统神经架构搜索的高效替代方案。其动机在于系统性地研究基于大语言模型的架构生成，重点关注提示工程和验证策略，以克服计算资源限制。方法上引入了少样本架构提示，确定三个支持示例能最佳平衡架构多样性与任务聚焦，并提出了一种快速的白空格归一化哈希验证方法进行去重，相比AST解析实现了100倍加速。在七个基准数据集上生成1,900个独特架构的实验结果表明了这些贡献的有效性，同时提出的数据集平衡评估方法为资源有限的研究者提供了实用的自动化设计指南。</div>
</details>
</div>
<div class="card">
<div class="title">BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Yunpeng Qing, Yixiao Chi, Shuo Chen, Shunyu Liu, Kelu Yao, Sixu Lin, Litao Liu, Changqing Zou</div>
<div class="meta-line">First: 2025-06-06T05:41:33+00:00 · Latest: 2025-12-30T09:13:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05762v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.05762v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in offline Reinforcement Learning (RL) have proven that effective policy learning can benefit from imposing conservative constraints on pre-collected datasets. However, such static datasets often exhibit distribution bias, resulting in limited generalizability. To address this limitation, a straightforward solution is data augmentation (DA), which leverages generative models to enrich data distribution. Despite the promising results, current DA techniques focus solely on reconstructing future trajectories from given states, while ignoring the exploration of history transitions that reach them. This single-direction paradigm inevitably hinders the discovery of diverse behavior patterns, especially those leading to critical states that may have yielded high-reward outcomes. In this work, we introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework for offline RL that models both future and history trajectories from any intermediate states. Specifically, we decompose the trajectory generation task into two independent yet complementary diffusion processes: one generating forward trajectories to predict future dynamics, and the other generating backward trajectories to trace essential history transitions.BiTrajDiff can efficiently leverage critical states as anchors to expand into potentially valuable yet underexplored regions of the state space, thereby facilitating dataset diversity. Extensive experiments on the D4RL benchmark suite demonstrate that BiTrajDiff achieves superior performance compared to other advanced DA methods across various offline RL backbones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiTrajDiff：基于扩散模型的双向轨迹生成用于离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习的最新进展表明，通过对预收集数据集施加保守约束可有效促进策略学习。然而，此类静态数据集常存在分布偏差，导致泛化能力受限。为解决这一问题，数据增强作为一种直接方案，利用生成模型丰富数据分布。尽管现有方法已取得一定成果，但其仅关注从给定状态重构未来轨迹，而忽略了探索到达这些状态的历史转移过程。这种单向范式不可避免地限制了多样化行为模式的发现，尤其是那些可能导向高回报关键状态的模式。本研究提出双向轨迹扩散，一种新颖的离线强化学习数据增强框架，可从任意中间状态同时建模未来与历史轨迹。具体而言，我们将轨迹生成任务分解为两个独立且互补的扩散过程：前向扩散生成未来动态轨迹，后向扩散追溯关键历史转移。该框架能高效利用关键状态作为锚点，拓展至状态空间中潜在有价值但未充分探索的区域，从而提升数据集多样性。在D4RL基准测试上的大量实验表明，相较于其他先进数据增强方法，该框架在不同离线强化学习骨干模型中均表现出更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces BiTrajDiff, a data augmentation framework for offline reinforcement learning motivated by the need to overcome distribution bias in static datasets, which limits policy generalizability. The method employs bidirectional diffusion models to generate both future and historical trajectories from any intermediate state, thereby expanding dataset diversity by exploring critical states and their surrounding transitions. Experimental results on the D4RL benchmark show that BiTrajDiff outperforms existing data augmentation techniques across multiple offline RL algorithms.</div>
<div class="mono" style="margin-top:8px">本文提出了BiTrajDiff，一种用于离线强化学习的数据增强框架，其动机在于解决静态数据集中存在的分布偏差问题，该偏差限制了策略的泛化能力。该方法采用双向扩散模型，从任意中间状态同时生成未来和历史的轨迹，从而通过探索关键状态及其周围转移来增强数据集的多样性。在D4RL基准测试上的实验结果表明，BiTrajDiff在多种离线RL算法中优于现有的数据增强方法。</div>
</details>
</div>
<div class="card">
<div class="title">How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns</div>
<div class="meta-line">Authors: Haoyue Bai, Yiyou Sun, Wenjie Hu, Shi Qiu, Maggie Ziyu Huan, Peiyang Song, Robert Nowak, Dawn Song</div>
<div class="meta-line">First: 2025-12-30T08:16:20+00:00 · Latest: 2025-12-30T08:16:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24063v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24063v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型如何实现泛化及其原因：从认知行为到低层模式的细粒度推理分析</div>
<div class="mono" style="margin-top:8px">大语言模型展现出截然不同的泛化行为：监督微调常导致能力收窄，而强化学习调优则倾向于保持能力。这种差异背后的原因尚不明确，因为先前研究主要依赖粗粒度准确率指标。我们通过引入一个将推理分解为原子核心技能（如计算、事实检索、模拟、枚举和诊断）的新型基准来填补这一空白，为解决&#x27;什么构成大语言模型推理&#x27;这一根本问题提供了具体框架。通过隔离和测量这些核心技能，该基准能更精细地观察特定认知能力在训练后如何涌现、迁移乃至崩溃。结合对分布差异和参数统计等低层统计模式的分析，该框架支持对数学、科学推理及非推理任务中SFT与RL下泛化演变的细粒度研究。我们的元探测框架追踪不同训练阶段的模型行为，发现RL调优模型能保持更稳定的行为特征并抵抗推理技能崩溃，而SFT模型则表现出更剧烈的漂移并过度拟合表层模式。这项工作为大语言模型推理的本质提供了新见解，并为设计促进广泛稳健泛化的训练策略指明了原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates why supervised fine-tuning (SFT) often narrows the generalization capabilities of Large Language Models (LLMs) while reinforcement-learning (RL) tuning tends to preserve them, a divergence not well understood through coarse accuracy metrics alone. To address this, the authors introduce a novel benchmark that decomposes reasoning into atomic core skills—such as calculation, fact retrieval, and simulation—enabling a fine-grained analysis of how these cognitive abilities emerge and transfer during post-training. By combining this with analyses of low-level statistical patterns, the study reveals that RL-tuned models maintain more stable behavioral profiles and resist skill collapse, whereas SFT models exhibit sharper drift and overfitting to surface patterns, offering insights for designing training strategies that promote robust generalization.</div>
<div class="mono" style="margin-top:8px">本文研究了大语言模型（LLMs）中监督微调（SFT）常导致泛化能力变窄而强化学习（RL）调优则倾向于保持该能力的现象，这一差异仅通过粗略的准确率指标难以解释。为此，作者引入了一个新颖的基准测试，将推理分解为计算、事实检索、模拟等原子核心技能，从而能够细粒度地分析这些认知能力在训练后如何涌现和迁移。结合对分布差异和参数统计等低层统计模式的分析，研究发现RL调优的模型保持了更稳定的行为特征并抵抗了技能崩溃，而SFT模型则表现出更明显的漂移和表面模式过拟合，这为设计促进广泛、稳健泛化的训练策略提供了新见解。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Mirror Descent with Temporal Difference Learning: Sample Complexity under Online Markov Data</div>
<div class="meta-line">Authors: Wenye Li, Hongxu Chen, Jiacai Liu, Ke Wei</div>
<div class="meta-line">First: 2025-12-30T07:57:03+00:00 · Latest: 2025-12-30T07:57:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24056v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper studies the policy mirror descent (PMD) method, which is a general policy optimization framework in reinforcement learning and can cover a wide range of policy gradient methods by specifying difference mirror maps. Existing sample complexity analysis for policy mirror descent either focuses on the generative sampling model, or the Markovian sampling model but with the action values being explicitly approximated to certain pre-specified accuracy. In contrast, we consider the sample complexity of policy mirror descent with temporal difference (TD) learning under the Markovian sampling model. Two algorithms called Expected TD-PMD and Approximate TD-PMD have been presented, which are off-policy and mixed policy algorithms respectively. Under a small enough constant policy update step size, the $\tilde{O}(\varepsilon^{-2})$ (a logarithm factor about $\varepsilon$ is hidden in $\tilde{O}(\cdot)$) sample complexity can be established for them to achieve average-time $\varepsilon$-optimality. The sample complexity is further improved to $O(\varepsilon^{-2})$ (without the hidden logarithm factor) to achieve the last-iterate $\varepsilon$-optimality based on adaptive policy update step sizes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>策略镜像下降与时间差分学习：在线马尔可夫数据下的样本复杂度</div>
<div class="mono" style="margin-top:8px">本文研究了策略镜像下降（PMD）方法，这是一种强化学习中的通用策略优化框架，可通过指定不同的镜像映射覆盖广泛的策略梯度方法。现有对策略镜像下降的样本复杂度分析要么聚焦于生成采样模型，要么针对马尔可夫采样模型但需将动作值显式近似至特定预设精度。相比之下，我们探讨了在马尔可夫采样模型下，结合时间差分（TD）学习的策略镜像下降的样本复杂度。提出了两种算法——期望TD-PMD与近似TD-PMD，分别为离策略和混合策略算法。在足够小的恒定策略更新步长下，二者达到平均时间ε最优性需具备$\tilde{O}(\varepsilon^{-2})$样本复杂度（对数因子隐藏于$\tilde{O}(\cdot)$中）。若采用自适应策略更新步长以实现最终迭代ε最优性，样本复杂度可进一步提升至$O(\varepsilon^{-2})$（无隐藏对数因子）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to analyze the sample complexity of policy mirror descent (PMD) methods under more realistic Markovian sampling conditions, where existing analyses often rely on generative models or pre-specified value approximations. The method introduces two algorithms, Expected TD-PMD and Approximate TD-PMD, which integrate temporal difference learning with PMD to handle off-policy and mixed policy scenarios under online Markov data. The main experimental results show that with a sufficiently small constant step size, both algorithms achieve an average-time ε-optimality with a sample complexity of Õ(ε⁻²), and by using adaptive step sizes, the sample complexity improves to O(ε⁻²) for last-iterate ε-optimality, removing logarithmic factors.</div>
<div class="mono" style="margin-top:8px">本文的动机在于需要在更现实的马尔可夫采样条件下分析策略镜像下降方法的样本复杂度，因为现有分析通常依赖于生成模型或预先指定的价值近似。方法上提出了两种算法——期望TD-PMD和近似TD-PMD，它们将时序差分学习与策略镜像下降结合，以处理在线马尔可夫数据下的离策略和混合策略场景。主要实验结果表明，在足够小的恒定步长下，两种算法实现平均时间ε最优性的样本复杂度为Õ(ε⁻²)，而通过采用自适应步长，样本复杂度可提升至O(ε⁻²)以实现最后迭代ε最优性，消除了对数因子。</div>
</details>
</div>
<div class="card">
<div class="title">ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment</div>
<div class="meta-line">Authors: Natchaya Temyingyong, Daman Jain, Neeraj Kumarsahu, Prabhat Kumar, Rachata Phondi, Wachiravit Modecrua, Krittanon Kaewtawee, Krittin Pachtrachai, Touchapon Kraisingkorn</div>
<div class="meta-line">First: 2025-12-30T07:31:34+00:00 · Latest: 2025-12-30T07:31:34+00:00</div>
<div class="meta-line">Comments: 22 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24040v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24040v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ROAD：基于自动化调试的零样本智能体对齐反思优化框架</div>
<div class="mono" style="margin-top:8px">自动提示优化（APO）已成为提升大语言模型（LLM）性能的关键技术，但当前主流方法通常依赖大规模标注黄金标准开发集来计算进化算法或强化学习（RL）的适应度分数。然而在实际软件工程中，智能体开发初始冷启动阶段往往缺乏此类精编数据集，工程师面对的是杂乱的生产日志与动态演变的故障模式。本文提出ROAD（基于自动化调试的反思优化）框架，通过将优化过程重构为动态调试研究而非随机搜索，摆脱对精细数据集的依赖。与传统变异策略不同，ROAD采用专有多智能体架构——包含根因分析器、模式聚合优化器和策略集成教练——将非结构化故障日志转化为鲁棒的结构化决策树协议。我们在标准化学术基准和实际生产知识管理引擎上评估ROAD。实验结果表明：ROAD具有极高样本效率，仅通过三次自动迭代即实现成功率提升5.6%（从73.6%至79.2%），搜索准确率提升3.8%。在零售领域复杂推理任务中，ROAD相较基线提升智能体性能约19%。这些发现表明，模拟人类工程实践中故障分析与修复的循环机制，为部署可靠LLM智能体提供了一种可替代资源密集型RL训练的高数据效率方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for ROAD stems from the impracticality of requiring large, labeled datasets for automatic prompt optimization in real-world agent development, where only messy production logs are available. The method introduces a novel framework that treats optimization as a debugging investigation, employing a multi-agent architecture with an Analyzer, Optimizer, and Coach to convert failure logs into structured Decision Tree Protocols. Experimental results show that ROAD is highly sample-efficient, achieving a 5.6% increase in success rate and a 3.8% boost in search accuracy within three iterations, and it improved agent performance by about 19% on complex reasoning tasks, demonstrating its viability as a data-efficient alternative to resource-intensive training methods.</div>
<div class="mono" style="margin-top:8px">ROAD的动机源于现实世界智能体开发中需要大型标注数据集进行自动提示优化的不切实际性，因为实际中只有杂乱的生产日志可用。该方法提出了一种新颖框架，将优化视为调试调查，采用包含分析器、优化器和教练的多智能体架构，将失败日志转化为结构化决策树协议。实验结果表明，ROAD具有很高的样本效率，在三次自动迭代中实现了成功率提升5.6%和搜索准确率提高3.8%，并在复杂推理任务上将智能体性能提升约19%，证明了其作为资源密集型训练方法的数据高效替代方案的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations</div>
<div class="meta-line">Authors: Xingqi He, Yujie Zhang, Shuyong Gao, Wenjie Li, Lingyi Hong, Mingxi Chen, Kaixun Jiang, Jiyuan Fu, Wenqiang Zhang</div>
<div class="meta-line">First: 2025-12-30T06:50:11+00:00 · Latest: 2025-12-30T06:50:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24023v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RSAgent：通过多轮工具调用实现文本引导分割的推理与执行学习</div>
<div class="mono" style="margin-top:8px">文本引导目标分割需要跨模态推理与像素级定位能力。现有方法多将其视为单次定位任务，模型通过单次前向传播预测像素提示以驱动外部分割器，当初始定位错误时缺乏验证、重聚焦与优化能力。为此，我们提出RSAgent，一种基于多模态大语言模型的智能体，通过多轮工具调用交替执行推理与分割操作。RSAgent查询分割工具箱、观察视觉反馈，并利用历史观测修正空间假设以重新定位目标并迭代优化掩码。我们构建了合成多轮推理分割轨迹的数据流程，采用两阶段框架训练RSAgent：先进行冷启动监督微调，再通过细粒度任务特定奖励进行智能体强化学习。大量实验表明，RSAgent在ReasonSeg测试集上实现66.5%的零样本广义交并比，较Seg-Zero-7B提升9%；在RefCOCOg上达到81.5%的类别交并比，在领域内与跨领域基准测试中均取得最先进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of one-shot grounding in text-guided segmentation, where initial errors cannot be corrected, by proposing RSAgent, an agentic Multimodal Large Language Model that performs multi-turn tool invocations to interleave reasoning and action. The method involves querying a segmentation toolbox, observing visual feedback, and revising spatial hypotheses iteratively, supported by a data pipeline for synthesizing multi-turn trajectories and a two-stage training framework combining supervised fine-tuning and agentic reinforcement learning with task-specific rewards. Experimental results show RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg, improving over prior methods by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对文本引导分割中单次定位无法纠正初始错误的问题，提出了RSAgent，一种通过多轮工具调用来交替进行推理和行动的代理式多模态大语言模型。该方法通过查询分割工具箱、观察视觉反馈并迭代修正空间假设来实现，并构建了用于合成多轮推理轨迹的数据管道，以及结合监督微调和基于任务特定奖励的代理强化学习的两阶段训练框架。实验结果表明，RSAgent在ReasonSeg上实现了66.5% gIoU的零样本性能，比先前方法提升9%，在RefCOCOg上达到81.5% cIoU，在领域内和领域外基准测试中均展示了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Reinforcement Learning Framework for ESP Cheater Simulation</div>
<div class="meta-line">Authors: Inkyu Park, Jeong-Gwan Lee, Taehwan Kwon, Juheon Choi, Seungku Kim, Junsu Kim, Kimin Lee</div>
<div class="meta-line">First: 2025-09-29T04:32:45+00:00 · Latest: 2025-12-30T02:22:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24274v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.24274v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game information such as enemy locations, are difficult to detect because their effects are not directly observable in player behavior. The lack of observable evidence makes it difficult to collect reliably labeled data, which is essential for training effective anti-cheat systems. Furthermore, cheaters often adapt their behavior by limiting or disguising their cheat usage, which further complicates detection and detector development. To address these challenges, we propose a simulation framework for controlled modeling of ESP cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and non-cheaters as reinforcement learning agents with different levels of observability, while detectors classify their behavioral trajectories. Next, we formulate the interaction between the cheater and the detector as an adversarial game, allowing both players to co-adapt over time. To reflect realistic cheater strategies, we introduce a structured cheater model that dynamically switches between cheating and non-cheating behaviors based on detection risk. Experiments demonstrate that our framework successfully simulates adaptive cheater behaviors that strategically balance reward optimization and detection evasion. This work provides a controllable and extensible platform for studying adaptive cheating behaviors and developing effective cheat detectors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对抗性强化学习框架用于ESP作弊者模拟</div>
<div class="mono" style="margin-top:8px">超感官知觉（ESP）作弊通过揭示游戏中隐藏信息（如敌人位置）而难以被检测，因为其效果无法直接从玩家行为中观察到。缺乏可观测证据导致难以收集可靠标注数据，而这对于训练有效的反作弊系统至关重要。此外，作弊者常通过限制或伪装作弊行为来适应环境，进一步增加了检测与检测器开发的复杂性。为应对这些挑战，我们提出一个模拟框架，用于对ESP作弊者、非作弊者及基于轨迹的检测器进行受控建模。我们将作弊者和非作弊者建模为具有不同可观测度的强化学习智能体，而检测器则对其行为轨迹进行分类。随后，我们将作弊者与检测器之间的互动形式化为对抗博弈，使双方能够随时间推移共同适应。为反映真实作弊策略，我们引入结构化作弊者模型，使其能根据检测风险动态切换作弊与非作弊行为。实验表明，该框架成功模拟了能战略性平衡奖励优化与规避检测的自适应作弊行为。本研究为探索自适应作弊行为及开发高效作弊检测器提供了可控且可扩展的平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the difficulty of detecting Extra-Sensory Perception (ESP) cheats in games due to their unobservable effects and the lack of reliably labeled data for training anti-cheat systems, this paper proposes an adversarial reinforcement learning framework to simulate adaptive cheaters and detectors. The method models cheaters and non-cheaters as reinforcement learning agents with varying observability levels, with detectors classifying their behavioral trajectories, and frames their interaction as an adversarial game to enable co-adaptation; it also introduces a structured cheater model that dynamically switches between cheating and non-cheating based on detection risk. Experimental results show that the framework successfully simulates adaptive cheater behaviors that strategically balance reward optimization and detection evasion, providing a controllable platform for studying cheating and developing detectors.</div>
<div class="mono" style="margin-top:8px">本研究针对游戏中超感官知觉（ESP）作弊行为难以检测的问题，由于作弊效果不可直接观察且缺乏可靠标注数据来训练反作弊系统，提出了一种对抗性强化学习框架来模拟自适应作弊者和检测器。方法将作弊者和非作弊者建模为具有不同可观察性的强化学习智能体，检测器对其行为轨迹进行分类，并将两者互动构建为对抗性游戏以实现协同适应；同时引入了一种结构化作弊者模型，能根据检测风险动态切换作弊与非作弊行为。实验结果表明，该框架成功模拟了自适应作弊行为，能策略性地平衡奖励优化与规避检测，为研究作弊行为和开发检测器提供了一个可控且可扩展的平台。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Constraint from Two Direction in Evolutionary Constrained Multi-objective Optimization</div>
<div class="meta-line">Authors: Ruiqing Sun, Dawei Feng, Xing Zhou, Lianghao Li, Sheng Qi, Bo Ding, Yijie Wang, Rui Wang, Huaimin Wang</div>
<div class="meta-line">First: 2025-12-30T02:22:32+00:00 · Latest: 2025-12-30T02:22:32+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23945v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23945v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world Constrained Multi-objective Optimization Problems (CMOPs) often contain multiple constraints, and understanding and utilizing the coupling between these constraints is crucial for solving CMOPs. However, existing Constrained Multi-objective Evolutionary Algorithms (CMOEAs) typically ignore these couplings and treat all constraints as a single aggregate, which lacks interpretability regarding the specific geometric roles of constraints. To address this limitation, we first analyze how different constraints interact and show that the final Constrained Pareto Front (CPF) depends not only on the Pareto fronts of individual constraints but also on the boundaries of infeasible regions. This insight implies that CMOPs with different coupling types must be solved from different search directions. Accordingly, we propose a novel algorithm named Decoupling Constraint from Two Directions (DCF2D). This method periodically detects constraint couplings and spawns an auxiliary population for each relevant constraint with an appropriate search direction. Extensive experiments on seven challenging CMOP benchmark suites and on a collection of real-world CMOPs demonstrate that DCF2D outperforms five state-of-the-art CMOEAs, including existing decoupling-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化约束多目标优化中从两个方向解耦约束</div>
<div class="mono" style="margin-top:8px">现实中的约束多目标优化问题通常包含多个约束，理解并利用这些约束间的耦合关系对求解至关重要。然而，现有的约束多目标进化算法往往忽略这些耦合，将所有约束视为单一聚合体，缺乏对约束具体几何作用的可解释性。为克服此局限，我们首先分析了不同约束如何相互作用，并表明最终约束帕累托前沿不仅取决于单个约束的帕累托前沿，还受不可行区域边界的影响。这一见解意味着具有不同耦合类型的约束多目标优化问题需从不同搜索方向求解。据此，我们提出一种名为“从两个方向解耦约束”的新算法。该方法周期性检测约束耦合，并为每个相关约束生成具有合适搜索方向的辅助种群。在七个高难度约束多目标优化基准测试集及一系列现实问题上的大量实验表明，DCF2D算法优于五种先进约束多目标进化算法，包括现有基于解耦的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the observation that existing constrained multi-objective evolutionary algorithms typically treat all constraints as a single aggregate, ignoring the crucial couplings between multiple constraints in real-world problems, which limits interpretability and performance. To address this, the method proposes DCF2D, a novel algorithm that analyzes constraint interactions and periodically detects coupling types to spawn auxiliary populations with tailored search directions for each relevant constraint. Experimental results on seven benchmark suites and real-world problems demonstrate that DCF2D outperforms five state-of-the-art algorithms, including other decoupling-based methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，现有的约束多目标进化算法通常将所有约束视为单一整体，忽略了现实问题中多个约束间至关重要的耦合关系，这限制了算法的可解释性和性能。为此，本文提出了DCF2D方法，该算法通过分析约束间的相互作用，周期性检测耦合类型，并为每个相关约束生成具有针对性搜索方向的辅助种群。在七个基准测试集和实际问题上的实验结果表明，DCF2D的性能优于五种先进的约束多目标进化算法，包括其他基于解耦的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Stationary Reweighting Yields Local Convergence of Soft Fitted Q-Iteration</div>
<div class="meta-line">Authors: Lars van der Laan, Nathan Kallus</div>
<div class="meta-line">First: 2025-12-30T00:58:35+00:00 · Latest: 2025-12-30T00:58:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23927v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23927v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fitted Q-iteration (FQI) and its entropy-regularized variant, soft FQI, are central tools for value-based model-free offline reinforcement learning, but can behave poorly under function approximation and distribution shift. In the entropy-regularized setting, we show that the soft Bellman operator is locally contractive in the stationary norm of the soft-optimal policy, rather than in the behavior norm used by standard FQI. This geometric mismatch explains the instability of soft Q-iteration with function approximation in the absence of Bellman completeness. To restore contraction, we introduce stationary-reweighted soft FQI, which reweights each regression update using the stationary distribution of the current policy. We prove local linear convergence under function approximation with geometrically damped weight-estimation errors, assuming approximate realizability. Our analysis further suggests that global convergence may be recovered by gradually reducing the softmax temperature, and that this continuation approach can extend to the hardmax limit under a mild margin condition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平稳重加权实现软拟合Q迭代的局部收敛</div>
<div class="mono" style="margin-top:8px">拟合Q迭代（FQI）及其熵正则化变体——软FQI，是基于价值的无模型离线强化学习的核心工具，但在函数逼近和分布偏移下可能表现不佳。在熵正则化设定中，我们证明软贝尔曼算子在软最优策略的平稳范数下是局部压缩的，而非标准FQI所使用的行为范数。这种几何失配解释了在缺乏贝尔曼完备性时，软Q迭代在函数逼近下的不稳定性。为恢复压缩性，我们引入平稳重加权软FQI，该方法利用当前策略的平稳分布对每个回归更新进行重加权。在近似可实现性假设下，我们证明了在函数逼近下具有几何衰减权重估计误差的局部线性收敛性。我们的分析进一步表明，通过逐步降低softmax温度可能恢复全局收敛性，且这种延拓方法在温和边界条件下可扩展至硬极大极限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability of soft fitted Q-iteration (FQI) in offline reinforcement learning under function approximation and distribution shift, motivated by the observation that the soft Bellman operator contracts in the stationary norm of the soft-optimal policy, not the behavior norm used in standard FQI. To restore contraction, the method introduces stationary-reweighted soft FQI, which reweights regression updates using the current policy&#x27;s stationary distribution. Experimental results demonstrate local linear convergence under function approximation with geometrically damped weight-estimation errors, assuming approximate realizability, and suggest that global convergence can be achieved by gradually reducing the softmax temperature, extending to the hardmax limit under a mild margin condition.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习中软拟合Q迭代在函数逼近和分布偏移下的不稳定性问题，其动机是发现软贝尔曼算子在软最优策略的平稳范数下具有压缩性，而非标准FQI使用的行为范数。为恢复压缩性，方法提出了平稳重加权软FQI，通过当前策略的平稳分布对每次回归更新进行重加权。实验结果表明，在近似可实现性假设下，该方法在函数逼近下实现了局部线性收敛，且权重估计误差呈几何衰减，并指出通过逐步降低软最大温度可恢复全局收敛性，在温和边界条件下可扩展至硬最大极限。</div>
</details>
</div>
<div class="card">
<div class="title">Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias</div>
<div class="meta-line">Authors: Xia Chen</div>
<div class="meta-line">First: 2025-12-30T00:34:24+00:00 · Latest: 2025-12-30T00:34:24+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23916v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23916v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional deep learning prioritizes unconstrained optimization, yet biological systems operate under strict metabolic constraints. We propose that these physical constraints shape dynamics to function not as limitations, but as a temporal inductive bias that breeds generalization. Through a phase-space analysis of signal propagation, we reveal a fundamental asymmetry: expansive dynamics amplify noise, whereas proper dissipative dynamics compress phase space that aligns with the network&#x27;s spectral bias, compelling the abstraction of invariant features. This condition can be imposed externally via input encoding, or intrinsically through the network&#x27;s own temporal dynamics. Both pathways require architectures capable of temporal integration and proper constraints to decode induced invariants, whereas static architectures fail to capitalize on temporal structure. Through comprehensive evaluations across supervised classification, unsupervised reconstruction, and zero-shot reinforcement learning, we demonstrate that a critical &quot;transition&quot; regime maximizes generalization capability. These findings establish dynamical constraints as a distinct class of inductive bias, suggesting that robust AI development requires not only scaling and removing limitations, but computationally mastering the temporal characteristics that naturally promote generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束催生泛化：作为归纳偏置的时序动态机制</div>
<div class="mono" style="margin-top:8px">传统深度学习优先考虑无约束优化，而生物系统却在严格的代谢约束下运行。我们提出，这些物理约束塑造的动态机制并非限制，而是作为一种时序归纳偏置促进泛化能力。通过对信号传播的相空间分析，我们揭示了一个基本不对称性：扩张性动态会放大噪声，而恰当的耗散性动态则压缩相空间，使其与网络的光谱偏置对齐，从而强制提取不变特征。这种条件可通过外部输入编码施加，或由网络自身的时序动态内在实现。两种途径都需要具备时序整合能力及适当约束的架构来解码诱导不变性，而静态架构无法利用时序结构。通过在监督分类、无监督重建和零样本强化学习等任务上的综合评估，我们证明临界“过渡”机制能最大化泛化能力。这些发现确立了动态约束作为一类独特的归纳偏置，表明稳健的AI发展不仅需要扩大规模和消除限制，更需在计算层面掌握自然促进泛化的时序特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that biological systems operate under strict metabolic constraints unlike unconstrained deep learning, this paper proposes that such physical constraints act as a temporal inductive bias to improve generalization. The method involves a phase-space analysis showing that proper dissipative dynamics compress phase space and align with spectral bias, forcing abstraction of invariant features, which can be imposed via input encoding or intrinsic network dynamics. Experimental results across supervised classification, unsupervised reconstruction, and zero-shot reinforcement learning demonstrate that a critical transition regime maximizes generalization, establishing dynamical constraints as a distinct inductive bias for robust AI.</div>
<div class="mono" style="margin-top:8px">本文的动机源于生物系统在严格代谢约束下运行，而无约束的深度学习则不然，提出这些物理约束可作为时间归纳偏置以提升泛化能力。方法上通过相空间分析表明，适当的耗散动力学能压缩相空间并与频谱偏置对齐，从而强制抽象不变特征，这可通过输入编码或网络内在动力学实现。在监督分类、无监督重建和零样本强化学习的综合评估中，实验结果显示临界“过渡”机制能最大化泛化能力，确立了动力学约束作为一类独特的归纳偏置，对鲁棒AI发展具有重要意义。</div>
</details>
</div>
<div class="card">
<div class="title">Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</div>
<div class="meta-line">Authors: Guangchen Lan, Huseyin A. Inan, Sahar Abdelnabi, Janardhan Kulkarni, Lukas Wutschitz, Reza Shokri, Christopher G. Brinton, Robert Sim</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-29T21:26:21+00:00 · Latest: 2025-12-29T22:27:54+00:00</div>
<div class="meta-line">Comments: 39th Conference on Neural Information Processing Systems (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04245v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.04245v4">PDF</a> · <a href="https://github.com/EricGLan/CI-RL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls. Our code is available at: https://github.com/EricGLan/CI-RL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于推理与强化学习实现大语言模型的语境完整性</div>
<div class="mono" style="margin-top:8px">随着自主代理代表用户决策时代的到来，如何确保语境完整性（CI）——即在执行特定任务时应共享何种适当信息——成为该领域的核心问题。我们认为CI需要一种推理形式，即代理需对其运作语境进行推理。为验证此观点，我们首先引导大语言模型在决定信息披露时对CI进行显式推理。随后通过开发强化学习框架扩展该方法，使模型进一步内化实现CI所需的推理能力。使用仅含约700个样本但涵盖多元语境与信息披露规范的自动生成合成数据集，我们证明该方法能大幅减少不当信息披露，同时在多种模型规模与架构中保持任务性能。重要的是，这种改进能从合成数据集迁移至已建立的CI基准测试（如含人工标注、评估AI助手在行动与工具调用中隐私泄露的PrivacyLens）。代码已开源：https://github.com/EricGLan/CI-RL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the challenge of ensuring contextual integrity (CI) in large language models (LLMs) as autonomous agents make decisions, which involves determining what information is appropriate to share in a given context. The method first prompts LLMs to explicitly reason about CI for disclosure decisions and then develops a reinforcement learning (RL) framework to instill such reasoning more deeply into the models. Experimental results on a small synthetic dataset of about 700 examples show that this approach substantially reduces inappropriate information disclosure while preserving task performance across various model sizes and families, with improvements transferring effectively to human-annotated benchmarks like PrivacyLens.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型作为自主代理决策时确保上下文完整性的挑战，即确定在特定情境下分享何种信息是合适的。方法上，首先提示模型对信息披露进行显式推理，然后开发一个强化学习框架，将这种推理能力更深入地嵌入模型中。实验结果表明，在一个约700个示例的小型合成数据集上，该方法显著减少了不恰当的信息披露，同时在不同规模和系列的模型中保持了任务性能，并且这些改进能有效迁移到如PrivacyLens等人工标注的基准测试中。</div>
</details>
</div>
<div class="card">
<div class="title">Max-Entropy Reinforcement Learning with Flow Matching and A Case Study on LQR</div>
<div class="meta-line">Authors: Yuyang Zhang, Yang Hu, Bo Dai, Na Li</div>
<div class="meta-line">First: 2025-12-29T21:23:52+00:00 · Latest: 2025-12-29T21:23:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23870v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23870v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Soft actor-critic (SAC) is a popular algorithm for max-entropy reinforcement learning. In practice, the energy-based policies in SAC are often approximated using simple policy classes for efficiency, sacrificing the expressiveness and robustness. In this paper, we propose a variant of the SAC algorithm that parameterizes the policy with flow-based models, leveraging their rich expressiveness. In the algorithm, we evaluate the flow-based policy utilizing the instantaneous change-of-variable technique and update the policy with an online variant of flow matching developed in this paper. This online variant, termed importance sampling flow matching (ISFM), enables policy update with only samples from a user-specified sampling distribution rather than the unknown target distribution. We develop a theoretical analysis of ISFM, characterizing how different choices of sampling distributions affect the learning efficiency. Finally, we conduct a case study of our algorithm on the max-entropy linear quadratic regulator problems, demonstrating that the proposed algorithm learns the optimal action distribution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于流匹配的最大熵强化学习及其在线性二次调节器中的案例研究</div>
<div class="mono" style="margin-top:8px">软演员-评论家（SAC）是最大熵强化学习中常用的算法。实践中，SAC的基于能量的策略常为效率采用简单策略类近似，牺牲了表达能力和鲁棒性。本文提出一种SAC变体，利用流模型的丰富表达能力对策略进行参数化。算法中，我们采用瞬时变量变换技术评估流策略，并通过本文提出的在线流匹配变体更新策略。该在线变体称为重要性采样流匹配（ISFM），仅需从用户指定的采样分布（而非未知目标分布）中采样即可实现策略更新。我们建立了ISFM的理论分析，阐明不同采样分布选择对学习效率的影响。最后，在线性二次调节器的最大熵问题上进行案例研究，证明所提算法能够学习最优动作分布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of simple policy approximations in soft actor-critic (SAC), which sacrifice expressiveness and robustness for efficiency, this paper introduces a SAC variant that employs flow-based models to parameterize policies, leveraging their rich expressiveness. The method evaluates policies using the instantaneous change-of-variable technique and updates them via a novel online flow matching variant called importance sampling flow matching (ISFM), which requires only samples from a user-specified distribution rather than the unknown target distribution. Experimental results from a case study on max-entropy linear quadratic regulator problems demonstrate that the proposed algorithm successfully learns the optimal action distribution, with theoretical analysis providing insights into how sampling distribution choices impact learning efficiency.</div>
<div class="mono" style="margin-top:8px">针对软演员-评论家（SAC）算法中为效率而采用简单策略近似、牺牲表达能力和鲁棒性的问题，本文提出了一种SAC变体，利用基于流的模型参数化策略，以发挥其丰富的表达能力。该方法通过瞬时变量变换技术评估策略，并使用本文开发的在线流匹配变体——重要性采样流匹配（ISFM）进行策略更新，该变体仅需从用户指定的采样分布中获取样本，而非未知的目标分布。在线性二次调节器问题的案例研究中，实验结果表明所提算法能够学习到最优动作分布，理论分析则揭示了不同采样分布选择对学习效率的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information</div>
<div class="meta-line">Authors: İbrahim Oğuz Çetinkaya, Sajad Khodadadian, Taylan G. Topcu</div>
<div class="meta-line">First: 2025-12-23T18:36:07+00:00 · Latest: 2025-12-29T20:24:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20589v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20589v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As systems engineering (SE) objectives evolve from design and operation of monolithic systems to complex System of Systems (SoS), the discipline of Mission Engineering (ME) has emerged which is increasingly being accepted as a new line of thinking for the SE community. Moreover, mission environments are uncertain, dynamic, and mission outcomes are a direct function of how the mission assets will interact with this environment. This proves static architectures brittle and calls for analytically rigorous approaches for ME. To that end, this paper proposes an intelligent mission coordination methodology that integrates digital mission models with Reinforcement Learning (RL), that specifically addresses the need for adaptive task allocation and reconfiguration. More specifically, we are leveraging a Digital Engineering (DE) based infrastructure that is composed of a high-fidelity digital mission model and agent-based simulation; and then we formulate the mission tactics management problem as a Markov Decision Process (MDP), and employ an RL agent trained via Proximal Policy Optimization. By leveraging the simulation as a sandbox, we map the system states to actions, refining the policy based on realized mission outcomes. The utility of the RL-based intelligent mission coordinator is demonstrated through an aerial firefighting case study. Our findings indicate that the RL-based intelligent mission coordinator not only surpasses baseline performance but also significantly reduces the variability in mission performance. Thus, this study serves as a proof of concept demonstrating that DE-enabled mission simulations combined with advanced analytical tools offer a mission-agnostic framework for improving ME practice; which can be extended to more complicated fleet design and selection problems in the future from a mission-first perspective.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用高保真数字模型与强化学习进行任务工程：基于完美信息的空中灭火案例研究</div>
<div class="mono" style="margin-top:8px">随着系统工程目标从单体系统的设计与运行转向复杂系统体系，任务工程作为系统工程领域的新兴思维范式逐渐被接受。任务环境具有不确定性与动态性，任务结果直接取决于任务资产与环境的交互，这使得静态架构显得脆弱，亟需严谨的分析方法。为此，本文提出一种集成数字任务模型与强化学习的智能任务协调方法，特别针对自适应任务分配与重构需求。具体而言，我们依托基于数字工程的基础设施——包含高保真数字任务模型与基于智能体的仿真，将任务战术管理问题建模为马尔可夫决策过程，并采用近端策略优化训练的强化学习智能体。通过将仿真作为沙盒环境，我们将系统状态映射为行动，并根据实际任务结果优化策略。基于强化学习的智能任务协调器在空降灭火案例中验证了其效用：不仅超越基准性能，还显著降低了任务表现的波动性。本研究证明，数字工程驱动的任务仿真结合先进分析工具，可构建一个与任务无关的框架以改进任务工程实践，未来可从任务优先视角扩展至更复杂的编队设计与选择问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the shift in systems engineering towards complex System of Systems and the need for adaptive, analytically rigorous approaches in uncertain mission environments, this paper proposes an intelligent mission coordination methodology. The method integrates high-fidelity digital mission models and agent-based simulation within a Digital Engineering framework, formulates mission tactics as a Markov Decision Process, and employs a Reinforcement Learning agent trained via Proximal Policy Optimization. In an aerial firefighting case study, the experimental results demonstrate that the RL-based coordinator not only surpasses baseline performance but also significantly reduces variability in mission outcomes, serving as a proof of concept for a mission-agnostic framework that can be extended to future fleet design problems.</div>
<div class="mono" style="margin-top:8px">本文的动机源于系统工程向复杂系统体系的演变，以及对不确定任务环境中自适应、分析严谨方法的需求。该方法在数字工程框架内，集成了高保真数字任务模型和基于智能体的仿真，将任务战术制定为马尔可夫决策过程，并采用通过近端策略优化训练的强化学习智能体。在一个空中灭火的案例研究中，实验结果表明，基于强化学习的任务协调器不仅超越了基线性能，还显著降低了任务表现的变异性，这为一个可扩展至未来编队设计问题的、与任务无关的框架提供了概念验证。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
