<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-16 03:46</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260116_0346</div>
    <div class="row"><div class="card">
<div class="title">Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park</div>
<div class="meta-line">First: 2026-01-14T17:57:43+00:00 · Latest: 2026-01-14T17:57:43+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09667v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09667v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作式多智能体测试时强化学习推理框架</div>
<div class="mono" style="margin-top:8px">多智能体系统已发展为多种实际应用中由大语言模型驱动的协作工具，其通过多样性与交叉验证获得鲁棒性。然而，多智能体强化学习训练资源消耗大且不稳定：队友间的协同适应会引发非平稳性，奖励信号通常稀疏且方差高。为此，我们提出\textbf{多智能体测试时强化学习（MATTRL）}框架，该框架在推理阶段将结构化文本经验注入多智能体决策过程。MATTRL构建由专业智能体组成的多专家团队进行多轮讨论，检索并整合测试时经验，最终达成共识进行决策。我们还研究了用于构建轮次级经验池的信用分配机制，并将其重新注入对话流程。在医学、数学和教育领域的多个挑战性基准测试中，MATTRL相较于多智能体基线平均准确率提升3.67%，较可比单智能体基线提升8.67%。消融实验检验了不同信用分配方案，并详细比较了其对训练结果的影响。MATTRL为无需调参的、分布偏移鲁棒的多智能体推理提供了一条稳定、高效且有效的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the resource-intensive and unstable nature of multi-agent reinforcement learning (MARL) training, where co-adaptation leads to non-stationarity and sparse rewards. To overcome this, it introduces Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation during inference, forming a team of specialists for multi-turn discussions, retrieving test-time experiences, and achieving consensus for decisions. Experimental results across medicine, math, and education benchmarks show that MATTRL improves accuracy by an average of 3.67% over multi-agent baselines and 8.67% over single-agent baselines, with ablation studies analyzing credit-assignment schemes to enhance training outcomes, offering a stable and efficient path to robust reasoning without tuning.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体强化学习训练资源密集且不稳定的问题，其中智能体协同适应导致非平稳性和稀疏奖励。为解决此问题，提出了多智能体测试时强化学习框架，该框架在推理时将结构化文本经验注入多智能体审议过程，组建专家团队进行多轮讨论，检索并整合测试时经验，最终达成共识决策。在医学、数学和教育等挑战性基准测试中，实验结果表明该框架相比多智能体基线平均准确率提升3.67%，相比单智能体基线提升8.67%，消融研究分析了信用分配方案对训练结果的影响，为无需调优的鲁棒推理提供了一条稳定高效的路径。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization</div>
<div class="meta-line">Authors: Frank Röder, Jan Benad, Manfred Eppe, Pradeep Kr. Banerjee</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-08-27T22:02:56+00:00 · Latest: 2026-01-14T17:50:26+00:00</div>
<div class="meta-line">Comments: 31 pages, 4 figures, accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20294v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.20294v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI&#x27;s latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向零样本泛化的上下文世界模型中动态对齐的潜在想象</div>
<div class="mono" style="margin-top:8px">现实世界的强化学习需要在不进行昂贵重训练的情况下适应未见过的环境条件。上下文马尔可夫决策过程（cMDP）对此挑战进行建模，但现有方法通常需要显式的上下文变量（如摩擦力、重力），当上下文是潜在或难以测量时，其应用受限。我们提出了动态对齐潜在想象（DALI），这是一个集成在Dreamer架构中的框架，它从智能体-环境交互中推断潜在上下文表示。通过训练一个自监督编码器来预测前向动态，DALI生成可操作的表示，用于调节世界模型和策略，从而桥接感知与控制。我们从理论上证明该编码器对于高效的上下文推断和鲁棒泛化至关重要。DALI的潜在空间实现了反事实一致性：扰动重力编码维度会以物理上合理的方式改变想象的推演轨迹。在具有挑战性的cMDP基准测试中，DALI相比无上下文感知的基线取得了显著提升，在外推任务中通常超越有上下文感知的基线，实现了对未见过的上下文变化的零样本泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of zero-shot generalization in reinforcement learning, where agents must adapt to unseen environmental conditions without retraining, a problem modeled by contextual Markov Decision Processes (cMDPs). Existing methods often rely on explicit context variables, which are impractical when contexts are latent or unmeasurable. To overcome this, the authors propose Dynamics-Aligned Latent Imagination (DALI), a framework integrated into the Dreamer architecture that infers latent context representations from agent-environment interactions through a self-supervised encoder trained to predict forward dynamics. This encoder provides actionable representations that condition both the world model and policy, theoretically proven essential for efficient context inference and robust generalization. Experimentally, DALI demonstrates counterfactual consistency in its latent space—e.g., perturbing gravity-encoding dimensions alters imagined rollouts plausibly—and achieves significant performance gains over context-unaware baselines on cMDP benchmarks, often surpassing context-aware baselines in extrapolation tasks, enabling effective zero-shot generalization to unseen contextual variations.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中的零样本泛化挑战，即智能体必须在无需重新训练的情况下适应未见的环境条件，该问题由上下文马尔可夫决策过程（cMDP）建模。现有方法通常依赖显式的上下文变量，这在上下文为潜在或难以测量时并不实用。为解决此问题，作者提出了动态对齐潜在想象（DALI），这是一个集成于Dreamer架构中的框架，通过一个自监督编码器从智能体-环境交互中推断潜在上下文表示，该编码器经过训练以预测前向动态。这些表示可操作地调节世界模型和策略，理论上被证明对高效上下文推断和鲁棒泛化至关重要。实验上，DALI在其潜在空间中展示了反事实一致性——例如，扰动编码重力的维度会以物理上合理的方式改变想象轨迹——并在cMDP基准测试中相比无上下文基线取得显著性能提升，在外推任务中常超越有上下文基线，实现了对未见上下文变化的有效零样本泛化。</div>
</details>
</div>
<div class="card">
<div class="title">From Prompt to Protocol: Fast Charging Batteries with Large Language Models</div>
<div class="meta-line">Authors: Ge Lei, Ferran Brosa Planella, Sterling G. Baird, Samuel J. Cooper</div>
<div class="meta-line">First: 2026-01-14T16:58:20+00:00 · Latest: 2026-01-14T16:58:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09626v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09626v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which simply writes an explicit function for the current and its scalar parameters. Across our case studies, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search. In a realistic fast charging scenario, both P2O and P2P yield around a 4.2 percent improvement in state of health (capacity retention based health metric under fast charging cycling) over a state-of-the-art multi-step constant current (CC) baseline, with P2P achieving this under matched evaluation budgets (same number of protocol evaluations). These results demonstrate that LLMs can expand the space of protocol functional forms, incorporate language-based constraints, and enable efficient optimization in high cost experimental settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从提示到协议：利用大语言模型实现电池快速充电</div>
<div class="mono" style="margin-top:8px">高效优化电池充电协议具有挑战性，因为每次评估都耗时、昂贵且不可微分。现有方法多通过严格限制协议搜索空间来应对这一难题，但这限制了可探索协议的多样性，阻碍了更高性能解决方案的发现。我们提出了两种基于大语言模型的无梯度闭环方法：提示到优化器（P2O）——利用大语言模型生成小型神经网络协议代码，并通过内部循环进行训练；以及提示到协议（P2P）——直接编写电流及其标量参数的显式函数。在案例研究中，大语言模型引导的P2O在性能上超越了贝叶斯优化、进化算法和随机搜索设计的神经网络。在现实快充场景下，P2O和P2P相比先进的多步恒流基线方案，在健康状态（基于快充循环容量保持的健康指标）上均实现了约4.2%的提升，其中P2P在相同评估预算下达成该成果。这些结果表明，大语言模型能够拓展协议函数形式空间，整合基于语言的约束条件，并在高成本实验环境中实现高效优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to overcome the slow, costly, and non-differentiable nature of battery charging protocol evaluations, which forces existing methods to constrain the search space and limit discovery of better solutions. The method introduces two gradient-free, LLM-driven approaches: Prompt-to-Optimizer (P2O), where an LLM generates code for small neural-network protocols that are then trained, and Prompt-to-Protocol (P2P), where an LLM directly writes an explicit function with scalar parameters. Experimental results show that LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search; in a realistic fast charging scenario, both P2O and P2P achieve about a 4.2% improvement in state of health over a state-of-the-art multi-step constant current baseline, with P2P doing so under matched evaluation budgets.</div>
<div class="mono" style="margin-top:8px">该研究的动机是解决电池充电协议评估过程缓慢、成本高且不可微分的难题，现有方法因过度约束搜索空间而限制了高性能方案的发现。方法上提出了两种无梯度、由大语言模型驱动的闭环方法：提示到优化器（P2O），即利用大语言模型生成小型神经网络协议代码并通过内循环训练；以及提示到协议（P2P），即直接由大语言模型编写带标量参数的显式函数。主要实验结果表明，在大语言模型引导下，P2O在性能上超越了基于贝叶斯优化、进化算法和随机搜索设计的神经网络；在现实快速充电场景中，P2O和P2P均比先进的多步恒流基线在健康状态（基于快速充电循环的容量保持健康指标）上提升了约4.2%，且P2P在相同评估预算下实现了这一改进。</div>
</details>
</div>
<div class="card">
<div class="title">DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing</div>
<div class="meta-line">Authors: Qian Cao, Yahui Liu, Wei Bi, Yi Zhao, Ruihua Song, Xiting Wang, Ruiming Tang, Guorui Zhou, Han Li</div>
<div class="meta-line">First: 2026-01-14T16:30:20+00:00 · Latest: 2026-01-14T16:30:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09609v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DPWriter：基于多样化规划分支的强化学习创意写作框架</div>
<div class="mono" style="margin-top:8px">基于强化学习的大语言模型增强方法常导致输出多样性降低，削弱了其在创意写作等开放式任务中的实用性。现有方法缺乏引导多样化探索的显式机制，往往优先考虑优化效率与性能而牺牲多样性。本文提出一种围绕半结构化长链思维构建的强化学习框架，将生成过程分解为显式规划的中间步骤。我们引入多样化规划分支方法，基于多样性变化在规划阶段策略性地引入分岔，并结合群体感知的多样性奖励机制以激励差异化轨迹。创意写作基准测试表明，该方法在保持生成质量的同时显著提升输出多样性，持续优于现有基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the observation that reinforcement learning (RL) applied to large language models (LLMs) for creative writing often reduces output diversity, as existing methods prioritize optimization efficiency over diversity. To address this, the authors propose DPWriter, an RL framework that structures generation using a semi-structured long Chain-of-Thought (CoT) to decompose writing into planned intermediate steps, incorporating a Diverse Planning Branching method to introduce divergence during planning based on diversity variation and a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks show that this approach significantly enhances output diversity while maintaining generation quality, consistently outperforming existing baselines.</div>
<div class="mono" style="margin-top:8px">本文的动机是观察到强化学习应用于大型语言模型进行创意写作时，常导致输出多样性降低，因为现有方法优先考虑优化效率而非多样性。为解决此问题，作者提出DPWriter框架，该框架使用半结构化长思维链将生成过程分解为明确规划的中间步骤，并引入多样化规划分支方法，基于多样性变化在规划阶段策略性地引入分歧，同时采用群体感知的多样性奖励以鼓励不同轨迹。在创意写作基准测试中的实验结果表明，该方法在保持生成质量的同时显著提高了输出多样性，一致优于现有基线。</div>
</details>
</div>
<div class="card">
<div class="title">Improving CMA-ES Convergence Speed, Efficiency, and Reliability in Noisy Robot Optimization Problems</div>
<div class="meta-line">Authors: Russell M. Martin, Steven H. Collins</div>
<div class="meta-line">First: 2026-01-14T16:12:18+00:00 · Latest: 2026-01-14T16:12:18+00:00</div>
<div class="meta-line">Comments: This is the authors&#x27; final accepted manuscript (post-peer-review, pre-publication). It has been accepted for publication in Evolutionary Computation on 12 Jan 2026. For associated code, see https://github.com/RussellMMartin/AS-CMA-ES</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09594v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09594v1">PDF</a> · <a href="https://github.com/RussellMMartin/AS-CMA-ES">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Experimental robot optimization often requires evaluating each candidate policy for seconds to minutes. The chosen evaluation time influences optimization because of a speed-accuracy tradeoff: shorter evaluations enable faster iteration, but are also more subject to noise. Here, we introduce a supplement to the CMA-ES optimization algorithm, named Adaptive Sampling CMA-ES (AS-CMA), which assigns sampling time to candidates based on predicted sorting difficulty, aiming to achieve consistent precision. We compared AS-CMA to CMA-ES and Bayesian optimization using a range of static sampling times in four simulated cost landscapes. AS-CMA converged on 98% of all runs without adjustment to its tunable parameter, and converged 24-65% faster and with 29-76% lower total cost than each landscape&#x27;s best CMA-ES static sampling time. As compared to Bayesian optimization, AS-CMA converged more efficiently and reliably in complex landscapes, while in simpler landscapes, AS-CMA was less efficient but equally reliable. We deployed AS-CMA in an exoskeleton optimization experiment and found the optimizer&#x27;s behavior was consistent with expectations. These results indicate that AS-CMA can improve optimization efficiency in the presence of noise while minimally affecting optimization setup complexity and tuning requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提升CMA-ES在噪声机器人优化问题中的收敛速度、效率与可靠性</div>
<div class="mono" style="margin-top:8px">机器人实验优化通常需要对每个候选策略进行数秒至数分钟的评估。由于速度-精度权衡的存在，评估时长的选择会影响优化效果：较短评估能加速迭代，但更易受噪声干扰。本文提出一种CMA-ES优化算法的增强方法——自适应采样CMA-ES（AS-CMA），其根据预测的排序难度为候选方案分配采样时间，旨在实现稳定精度。我们在四种模拟代价场景中，将AS-CMA与采用固定采样时长的CMA-ES及贝叶斯优化进行对比。AS-CMA在98%的实验中无需调整可调参数即实现收敛，且收敛速度比各场景最优固定采样CMA-ES快24-65%，总代价降低29-76%。相较于贝叶斯优化，AS-CMA在复杂场景中收敛效率更高、可靠性更强；在简单场景中效率稍低但可靠性相当。通过外骨骼优化实验验证，AS-CMA的表现符合预期。结果表明，AS-CMA能在噪声环境下提升优化效率，同时几乎不增加优化设置的复杂性和调参需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the speed-accuracy trade-off in noisy robot policy evaluation, where shorter trials are faster but noisier, this paper introduces Adaptive Sampling CMA-ES (AS-CMA), a method that dynamically allocates evaluation time to candidates based on predicted ranking difficulty to ensure consistent precision. In experiments across four simulated cost landscapes, AS-CMA achieved 98% convergence without parameter tuning, converging 24-65% faster and with 29-76% lower total cost than the best static-sampling CMA-ES baseline, and demonstrated superior efficiency and reliability to Bayesian optimization in complex landscapes. A real-world exoskeleton optimization confirmed the method&#x27;s expected behavior, showing AS-CMA effectively improves optimization efficiency under noise with minimal setup complexity.</div>
<div class="mono" style="margin-top:8px">针对机器人策略评估中存在的速度-精度权衡问题（即评估时间短则迭代快但噪声大），本文提出了自适应采样CMA-ES（AS-CMA）方法，该方法根据预测的排序难度动态分配候选方案的评估时间，以确保一致的评估精度。在四个模拟成本环境中的实验表明，AS-CMA在无需调整参数的情况下实现了98%的收敛成功率，相比最佳静态采样CMA-ES基线，收敛速度加快24-65%，总成本降低29-76%，并且在复杂环境中比贝叶斯优化更具效率和可靠性。外骨骼机器人实物优化实验验证了该方法的预期性能，表明AS-CMA能以极低的设置复杂度有效提升噪声环境下的优化效率。</div>
</details>
</div>
<div class="card">
<div class="title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
<div class="meta-line">Authors: Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</div>
<div class="meta-line">First: 2026-01-12T21:57:52+00:00 · Latest: 2026-01-14T15:49:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08026v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08026v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FigEx2：面向科学复合图表的视觉条件化面板检测与描述生成框架</div>
<div class="mono" style="margin-top:8px">科学复合图表将多个带标签的面板整合为单一图像，但实际流程中的描述文本常缺失或仅提供图表级摘要，导致面板级理解困难。本文提出FigEx2——一种视觉条件化框架，可直接从复合图表中定位面板并生成面板级描述。为缓解开放式描述中多样化表述的影响，我们引入噪声感知门控融合模块，自适应过滤词元级特征以稳定检测查询空间。此外，采用结合监督学习与强化学习的分阶段优化策略，利用基于CLIP的对齐奖励和基于BERTScore的语义奖励，确保严格的多模态一致性。为提供高质量监督数据，我们构建了面板级定位基准数据集BioSci-Fig-Cap，以及跨物理与化学学科的测试集。实验表明：FigEx2在检测任务上达到0.726 mAP@0.5:0.95的优异性能，在METEOR和BERTScore指标上分别显著超越Qwen3-VL-8B模型0.51和0.24。值得注意的是，FigEx2在未经微调的情况下，对分布外科学领域展现出卓越的零样本迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of understanding scientific compound figures, where captions are often missing or only provide figure-level summaries, hindering panel-level analysis. The authors propose FigEx2, a visual-conditioned framework that localizes panels and generates panel-wise captions directly from the figure. To handle diverse phrasing in captioning, they introduce a noise-aware gated fusion module to filter token-level features and stabilize detection queries, and employ a staged optimization strategy combining supervised learning with reinforcement learning, using CLIP-based alignment and BERTScore rewards for multimodal consistency. They also curate the BioSci-Fig-Cap benchmark for panel-level grounding and cross-disciplinary test suites. Experiments show FigEx2 achieves 0.726 mAP@0.5:0.95 for detection and outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore, with notable zero-shot transferability to out-of-distribution scientific domains.</div>
<div class="mono" style="margin-top:8px">本文针对科学复合图中常因标题缺失或仅提供图级摘要而难以进行面板级理解的问题，提出了FigEx2这一视觉条件框架，可直接从复合图中定位面板并生成面板级标题。为缓解开放式标题生成中多样化表述的影响，引入了噪声感知门控融合模块来自适应过滤词级特征以稳定检测查询空间，并采用结合监督学习和强化学习的分阶段优化策略，利用基于CLIP的对齐和基于BERTScore的语义奖励来确保严格的多模态一致性。此外，构建了用于面板级定位的BioSci-Fig-Cap基准以及跨学科测试集。实验结果表明，FigEx2在检测上达到了0.726 mAP@0.5:0.95的优异性能，在METEOR和BERTScore上分别显著超越Qwen3-VL-8B模型0.51和0.24分，且在未经微调的情况下展现出对分布外科学领域的出色零样本迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks</div>
<div class="meta-line">Authors: Ningzhe Shi, Yiqing Zhou, Ling Liu, Jinglin Shi, Yihao Wu, Haiwei Shi, Hanxiao Yu</div>
<div class="meta-line">First: 2025-08-16T15:29:59+00:00 · Latest: 2026-01-14T15:28:17+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TMC</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12079v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.12079v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrated sensing and communication (ISAC) can enhance artificial intelligence-generated content (AIGC) networks by providing efficient sensing and transmission. Existing AIGC services usually assume that the accuracy of the generated content can be ensured, given accurate input data and prompt, thus only the content generation quality (CGQ) is concerned. However, it is not applicable in ISAC-based AIGC networks, where content generation is based on inaccurate sensed data. Moreover, the AIGC model itself introduces generation errors, which depend on the number of generating steps (i.e., computing resources). To assess the quality of experience of ISAC-based AIGC services, we propose a content accuracy and quality aware service assessment metric (CAQA). Since allocating more resources to sensing and generating improves content accuracy but may reduce communication quality, and vice versa, this sensing-generating (computing)-communication three-dimensional resource tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution space that grows exponentially with the number of users. To solve the CAQA-AIGC problem with low complexity, a linear programming (LP) guided deep reinforcement learning (DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the LP-guided approach and the action filter, LPDRL-F can transform the original three-dimensional solution space to two dimensions, reducing complexity while improving the learning performance of DRL. Simulations show that compared to existing DRL and generative diffusion model (GDM) algorithms without LP, LPDRL-F converges faster and finds better resource allocation solutions, improving AvgCAQA by more than 10%. With LPDRL-F, CAQA-AIGC can achieve an improvement in AvgCAQA of more than 50% compared to existing schemes focusing solely on CGQ.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于线性规划引导深度强化学习的ISAC驱动AIGC网络内容准确性与质量感知资源分配</div>
<div class="mono" style="margin-top:8px">集成感知与通信（ISAC）可通过提供高效感知与传输增强人工智能生成内容（AIGC）网络。现有AIGC服务通常假设在输入数据与提示准确时能确保生成内容准确性，故仅关注内容生成质量（CGQ）。然而这在基于ISAC的AIGC网络中并不适用，因其内容生成基于不精确的感知数据。此外，AIGC模型本身会引入取决于生成步数（即计算资源）的生成误差。为评估基于ISAC的AIGC服务的体验质量，本文提出内容准确性与质量感知服务评估指标（CAQA）。由于为感知与生成分配更多资源可提升内容准确性但可能降低通信质量，反之亦然，必须优化感知-生成（计算）-通信三维资源权衡，以最大化所有AIGC用户的平均CAQA（AvgCAQA）。该问题属于NP难问题，其解空间随用户数呈指数增长。为低复杂度求解CAQA-AIGC问题，提出一种带动作滤波器的线性规划（LP）引导深度强化学习（DRL）算法（LPDRL-F）。通过LP引导方法与动作滤波器，LPDRL-F可将原始三维解空间降至二维，在降低复杂度的同时提升DRL学习性能。仿真表明，相较于现有无LP的DRL与生成扩散模型（GDM）算法，LPDRL-F收敛更快且能获得更优资源分配方案，将AvgCAQA提升超10%。采用LPDRL-F时，CAQA-AIGC相较于仅关注CGQ的现有方案可实现AvgCAQA超50%的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of optimizing resource allocation in integrated sensing and communication (ISAC)-driven artificial intelligence-generated content (AIGC) networks, where content generation relies on inaccurate sensed data and the AIGC model itself introduces errors. The motivation is to enhance user experience by jointly considering content accuracy and quality, leading to the proposal of a content accuracy and quality aware (CAQA) metric and the formulation of an NP-hard optimization problem to maximize average CAQA across users. The method introduces a low-complexity linear programming (LP)-guided deep reinforcement learning algorithm with an action filter (LPDRL-F), which transforms the three-dimensional resource tradeoff space into two dimensions to improve learning efficiency. Experimental simulations demonstrate that LPDRL-F converges faster and achieves over 10% higher average CAQA compared to baseline DRL and generative diffusion model algorithms, and more than 50% improvement over schemes focusing only on content generation quality.</div>
<div class="mono" style="margin-top:8px">本文针对集成感知与通信（ISAC）驱动的人工智能生成内容（AIGC）网络中的资源分配优化问题，其中内容生成依赖于不准确的感知数据且AIGC模型本身会引入误差。研究动机是通过联合考虑内容准确性和质量来提升用户体验，为此提出了内容准确性与质量感知（CAQA）度量指标，并构建了最大化用户平均CAQA的NP难优化问题。方法上，提出了一种低复杂度的线性规划（LP）引导的深度强化学习算法（LPDRL-F），通过动作过滤器将三维资源权衡空间降为二维以提高学习效率。实验仿真表明，与现有的深度强化学习和生成扩散模型算法相比，LPDRL-F收敛更快，平均CAQA提升超过10%，且相较于仅关注内容生成质量的方案，平均CAQA改进超过50%。</div>
</details>
</div>
<div class="card">
<div class="title">What Can RL Bring to VLA Generalization? An Empirical Study</div>
<div class="meta-line">Authors: Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-26T10:19:26+00:00 · Latest: 2026-01-14T14:23:30+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19789v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.19789v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rlvla.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习能为视觉语言动作模型泛化带来什么？一项实证研究</div>
<div class="mono" style="margin-top:8px">大规模视觉语言动作模型在具身人工智能领域展现出巨大潜力，但其主要依赖监督微调的训练方式因分布偏移下的误差累积问题而限制了泛化能力。强化学习通过试错优化任务目标为突破这些限制提供了路径，然而与监督微调相比，其对视觉语言动作模型泛化的具体效益尚缺乏系统认知。为此，本研究构建了评估视觉语言动作模型泛化的综合基准，系统探究了强化学习微调在视觉、语义与执行等多维度的作用。大量实验表明，强化学习微调（尤其是PPO算法）在语义理解和执行鲁棒性方面显著优于监督微调，同时保持相当的视觉鲁棒性。研究发现PPO相比DPO、GRPO等基于大语言模型的方法更适合视觉语言动作模型。我们还提出了高效的视觉语言动作模型PPO训练方案，并验证了其提升模型泛化的实用价值。项目页面详见https://rlvla.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the limited generalization of large Vision-Language-Action (VLA) models under distribution shifts when trained primarily with supervised fine-tuning (SFT), prompting an investigation into whether reinforcement learning (RL) can offer superior generalization benefits. The method involves creating a comprehensive benchmark to evaluate VLA generalization and systematically applying RL fine-tuning, notably with the PPO algorithm, across visual, semantic, and execution dimensions. The main experimental results show that RL fine-tuning, especially with PPO, significantly improves semantic understanding and execution robustness compared to SFT, while maintaining similar visual robustness, and identifies PPO as more effective than LLM-derived RL methods like DPO and GRPO, leading to a practical training recipe for enhanced VLA generalization.</div>
<div class="mono" style="margin-top:8px">本研究动机在于，主要通过监督微调训练的大型视觉-语言-动作模型在分布偏移下泛化能力有限，因此探究强化学习是否能带来更好的泛化优势。方法上，研究建立了一个全面的基准来评估VLA泛化能力，并系统性地应用强化学习微调，特别是PPO算法，覆盖视觉、语义和执行维度。主要实验结果表明，与监督微调相比，强化学习微调（尤其是PPO）显著提升了语义理解和执行鲁棒性，同时保持了相当的视觉鲁棒性，并发现PPO比DPO、GRPO等基于大语言模型的强化学习方法更有效，从而提出了一种实用的训练方案以增强VLA泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering</div>
<div class="meta-line">Authors: Chenliang Zhang, Lin Wang, Yuanyuan Lu, Yusheng Qi, Kexin Wang, Peixu Hou, Wenshi Chen</div>
<div class="meta-line">First: 2025-08-14T04:37:56+00:00 · Latest: 2026-01-14T13:50:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10337v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10337v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper describes the solutions of the Dianping-Trust-Safety team for the META CRAG-MM challenge. The challenge requires building a comprehensive retrieval-augmented generation system capable for multi-modal multi-turn question answering. The competition consists of three tasks: (1) answering questions using structured data retrieved from an image-based mock knowledge graph, (2) synthesizing information from both knowledge graphs and web search results, and (3) handling multi-turn conversations that require context understanding and information aggregation from multiple sources. For Task 1, our solution is based on the vision large language model, enhanced by supervised fine-tuning with knowledge distilled from GPT-4.1. We further applied curriculum learning strategies to guide reinforcement learning, resulting in improved answer accuracy and reduced hallucination. For Task 2 and Task 3, we additionally leveraged web search APIs to incorporate external knowledge, enabling the system to better handle complex queries and multi-turn conversations. Our approach achieved 1st place in Task 1 with a significant lead of 52.38%, and 3rd place in Task 3, demonstrating the effectiveness of the integration of curriculum learning with reinforcement learning in our training pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习的课程学习方法：利用RAG实现多模态问答</div>
<div class="mono" style="margin-top:8px">本文介绍了大众点评-信任与安全团队在META CRAG-MM挑战赛中的解决方案。该挑战要求构建一个能够进行多模态多轮问答的检索增强生成系统。竞赛包含三项任务：(1) 基于图像模拟知识图谱检索结构化数据回答问题，(2) 综合知识图谱与网络搜索结果的信息，(3) 处理需要上下文理解及多源信息聚合的多轮对话。针对任务一，我们采用视觉大语言模型为基础，通过GPT-4.1知识蒸馏的监督微调进行增强，并运用课程学习策略指导强化学习，显著提升答案准确性并减少幻觉生成。对于任务二和三，我们额外引入网络搜索API整合外部知识，使系统能更好处理复杂查询与多轮对话。该方法在任务一中以52.38%的显著优势获得第一名，在任务三中获得第三名，验证了课程学习与强化学习在训练流程中融合的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the META CRAG-MM challenge to build a robust multimodal question-answering system, this paper proposes a curriculum learning strategy to guide reinforcement learning within a retrieval-augmented generation framework. The method employs a vision large language model fine-tuned with GPT-4.1 distilled knowledge and integrates web search APIs for complex queries. Experimental results show the approach secured first place in Task 1 with a 52.38% lead and third place in Task 3, validating its effectiveness in improving answer accuracy and reducing hallucination.</div>
<div class="mono" style="margin-top:8px">本文针对META CRAG-MM挑战中构建多模态问答系统的需求，提出了一种将课程学习策略用于指导强化学习的方法，并基于检索增强生成框架。该方法采用经GPT-4.1知识蒸馏监督微调的视觉大语言模型，并整合网络搜索API处理复杂查询。实验结果表明，该方法在任务一中以52.38%的显著优势获得第一名，在任务三中获得第三名，有效提升了答案准确性并减少了幻觉生成。</div>
</details>
</div>
<div class="card">
<div class="title">SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning</div>
<div class="meta-line">Authors: Caijun Xu, Changyi Xiao, Zhongyuan Peng, Xinrun Wang, Yixin Cao</div>
<div class="meta-line">First: 2026-01-08T10:42:04+00:00 · Latest: 2026-01-14T13:43:32+00:00</div>
<div class="meta-line">Comments: 19 pages,5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04809v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04809v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model&#x27;s capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCALER：面向推理的合成可扩展自适应学习环境</div>
<div class="mono" style="margin-top:8px">强化学习为提升大语言模型的推理能力提供了原则性方法，但其有效性依赖于随模型演化保持信息量的训练信号。实践中，当任务难度与模型能力不匹配，或训练被少数重复问题模式主导时，强化学习进展常会放缓。为协同解决这些问题，我们提出SCALER（面向推理的合成可扩展自适应学习环境），该框架通过自适应环境设计维持有效的学习信号。SCALER引入可扩展的合成流程，将真实编程问题转化为具有可控难度和无限实例生成的、可验证的推理环境，使强化学习训练能突破有限数据集的限制，同时保持强正确性保证。在此基础上，SCALER进一步采用自适应多环境强化学习策略，动态调整实例难度并筛选活跃环境集合，以追踪模型能力边界并保持分布多样性。这种协同适应机制避免了奖励稀疏性，缓解了对狭窄任务模式的过拟合，支持训练过程中的持续改进。大量实验表明，SCALER在多种推理基准测试中持续优于基于数据集的强化学习基线，并展现出更稳定、更长周期的训练动态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the challenge that reinforcement learning (RL) for improving language model reasoning often suffers from diminishing training signals when task difficulty mismatches model capability or when training data lacks diversity. To address this, the method introduces SCALER, a framework that creates a synthetic, scalable learning environment by converting real-world programming problems into verifiable reasoning tasks with controllable difficulty and unlimited instance generation, coupled with an adaptive multi-environment RL strategy that dynamically adjusts difficulty and curates environments to match the model&#x27;s evolving frontier. The main experimental results demonstrate that SCALER consistently outperforms dataset-based RL baselines across various reasoning benchmarks and shows more stable, long-term training dynamics.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，用于提升大语言模型推理能力的强化学习常因任务难度与模型能力不匹配或训练数据缺乏多样性而导致训练信号减弱。为解决此问题，方法提出了SCALER框架，它通过将现实编程问题转化为可验证、难度可控且可无限生成实例的推理环境，构建了一个合成的可扩展学习环境，并结合了一种自适应多环境强化学习策略，动态调整难度并筛选环境以匹配模型的能力前沿。主要实验结果表明，SCALER在多种推理基准测试中持续优于基于数据集的强化学习基线，并展现出更稳定、长期的训练动态。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling for Strategic Multiagent Settings</div>
<div class="meta-line">Authors: Georgios Chalkiadakis, Charilaos Akasiadis, Gerasimos Koresis, Stergios Plataniotis, Leonidas Bakopoulos</div>
<div class="meta-line">First: 2025-11-13T17:06:56+00:00 · Latest: 2026-01-14T13:06:54+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10501v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.10501v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper provides a comprehensive review of mainly GNN, DRL, and PTM methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) ML methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of GNN. Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of RL, and in particular that of multiagent deep reinforcement learning. Single-agent deep RL has been widely used for decision making in demanding game settings. Its application in multiagent settings though is hindered due to, e.g., varying relationships between agents, and non-stationarity of the environment. We describe existing relevant game theoretic solution concepts, and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes probabilistic topic modeling (PTM) in domains other than that of document analysis and classification. Finally, we identify certain open challenges -- specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向战略多智能体场景的图神经网络、深度强化学习与概率主题建模方法综述</div>
<div class="mono" style="margin-top:8px">本文系统综述了图神经网络（GNN）、深度强化学习（DRL）与概率主题建模（PTM）方法在战略多智能体场景中的应用潜力。重点关注：（1）适用于战略对手建模的未知模型结构发现方法；（2）与博弈论概念的融合，以规避现实场景中常失效的假设（如共同先验假设与自利假说）。研究分析了方法处理不确定性与异质性（现实应用中的常见特征）的能力及可扩展性。针对多智能体场景中的关系建模，本文主张采用擅长处理图结构数据的GNN方法。同时综述了多智能体深度强化学习领域，指出单智能体深度RL在复杂博弈决策中的广泛应用及其在多智能体场景中面临的挑战（如智能体间动态关系与环境非平稳性）。研究还探讨了相关博弈论解概念及其公平性、稳定性等特性，并扩展了PTM在非文本分析领域的应用文献。最后提出开放挑战：（1）适应非平稳环境；（2）平衡稳定性与适应性；（3）应对不确定性与异质性；（4）保障可扩展性与求解可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This review paper is motivated by the need to model strategic multiagent interactions without relying on restrictive game theory assumptions like the Common Prior Assumption and Self-Interest Hypothesis. The method involves a comprehensive analysis of three machine learning approaches: Graph Neural Networks (GNNs) are championed for modeling agent relationships on graph structures, Deep Reinforcement Learning (DRL) is reviewed for multiagent decision-making despite challenges like non-stationarity, and Probabilistic Topic Modeling (PTM) is considered for applications beyond text. The main experimental and analytical results highlight these methods&#x27; capabilities in handling key real-world characteristics such as uncertainty, heterogeneity, and scalability, while also identifying open challenges including adapting to non-stationary environments and balancing stability with adaptation.</div>
<div class="mono" style="margin-top:8px">本综述论文的动机在于，需要在战略多智能体交互建模中避免依赖共同先验假设和自利假说等限制性博弈论假设。其方法是对三种机器学习途径进行综合分析：倡导使用图神经网络（GNN）在图结构上建模智能体关系，回顾深度强化学习（DRL）在多智能体决策中的应用（尽管存在非平稳性等挑战），并探讨概率主题模型（PTM）在文本以外的应用。主要实验和分析结果表明，这些方法在处理不确定性、异质性和可扩展性等关键现实世界特征方面具备能力，同时指出了包括适应非平稳环境、平衡稳定性与适应性在内的若干开放挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Optimization with Preference Exploration using a Monotonic Neural Network Ensemble</div>
<div class="meta-line">Authors: Hanyang Wang, Juergen Branke, Matthias Poloczek</div>
<div class="meta-line">First: 2025-01-30T22:50:34+00:00 · Latest: 2026-01-14T12:51:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.18792v5">Abs</a> · <a href="https://arxiv.org/pdf/2501.18792v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world black-box optimization problems have multiple conflicting objectives. Rather than attempting to approximate the entire set of Pareto-optimal solutions, interactive preference learning allows to focus the search on the most relevant subset. However, few previous studies have exploited the fact that utility functions are usually monotonic. In this paper, we address the Bayesian Optimization with Preference Exploration (BOPE) problem and propose using a neural network ensemble as a utility surrogate model. This approach naturally integrates monotonicity and supports pairwise comparison data. Our experiments demonstrate that the proposed method outperforms state-of-the-art approaches and exhibits robustness to noise in utility evaluations. An ablation study highlights the critical role of monotonicity in enhancing performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单调神经网络集成的贝叶斯优化与偏好探索</div>
<div class="mono" style="margin-top:8px">许多现实世界的黑盒优化问题存在多个相互冲突的目标。相较于试图逼近整个帕累托最优解集，交互式偏好学习能够将搜索聚焦于最相关的子集。然而，先前研究很少利用效用函数通常具有单调性这一事实。本文针对贝叶斯优化与偏好探索问题，提出使用神经网络集成作为效用代理模型。该方法自然地整合了单调性并支持成对比较数据。实验表明，所提方法优于现有先进方法，且在效用评估中表现出对噪声的鲁棒性。消融研究进一步揭示了单调性对提升性能的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses Bayesian Optimization with Preference Exploration (BOPE) for multi-objective black-box optimization, motivated by the need to focus search on user-relevant subsets rather than approximating entire Pareto fronts and by the under-exploited property that utility functions are typically monotonic. The method introduces a neural network ensemble as a utility surrogate model, which naturally enforces monotonicity constraints and accommodates pairwise comparison data. Experimental results show that the proposed approach outperforms state-of-the-art methods, demonstrates robustness to noise in utility evaluations, and an ablation study confirms that incorporating monotonicity is crucial for its enhanced performance.</div>
<div class="mono" style="margin-top:8px">本文针对多目标黑盒优化中的贝叶斯偏好探索问题，其动机在于需要将搜索聚焦于用户相关的子集而非近似整个帕累托前沿，且以往研究较少利用效用函数通常具有单调性这一特性。该方法提出使用神经网络集成作为效用代理模型，自然地整合了单调性约束并支持成对比较数据。实验结果表明，所提方法优于现有先进方法，对效用评估中的噪声具有鲁棒性，且消融研究证实了引入单调性对于提升性能的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps</div>
<div class="meta-line">Authors: Siyi Li, Joseph G. Lambourne, Longfei Zhang, Pradeep Kumar Jayaraman, Karl. D. D. Willis</div>
<div class="meta-line">First: 2026-01-14T12:17:34+00:00 · Latest: 2026-01-14T12:17:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09428v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09428v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer&#x27;s input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>如欧几里得般绘制：利用尺规作图步骤教导Transformer模型生成CAD轮廓</div>
<div class="mono" style="margin-top:8px">我们提出一种通过一系列简单几何构造（包括曲线偏移、旋转和相交）生成计算机辅助设计（CAD）轮廓的新方法。这些构造序列以设计师提供的几何图形为起点，逐步构建最终轮廓的点与曲线。我们证明，在设计师输入的几何图形与最终轮廓之间添加构造步骤，能提升生成质量，其效果类似于在语言模型中引入思维链。与参数化CAD模型中的约束类似，构造序列将建模形状的自由度缩减为一小组可由设计师调整的参数值，从而实现以浮点精度评估构造几何的参量化编辑。此外，我们表明对构造序列应用强化学习能在包括未显式优化的多项指标上取得进一步改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve the generation quality and parametric editability of Computer Aided Design (CAD) profiles, this paper introduces a method that generates profiles through sequences of geometric construction steps, such as offsetting and intersections, starting from designer-provided geometry. The approach treats these construction sequences analogously to a chain of thought in language models, reducing shape degrees of freedom to adjustable parameters for precise parametric editing. Experimental results demonstrate that incorporating construction steps enhances generation quality, and applying reinforcement learning to these sequences yields further improvements across multiple metrics, including those not explicitly optimized.</div>
<div class="mono" style="margin-top:8px">本文旨在提升计算机辅助设计（CAD）轮廓的生成质量与参数化可编辑性，提出了一种通过几何构造步骤序列（如偏移和相交）生成轮廓的方法，该方法从设计师提供的几何图形开始逐步构建。该方法将构造序列类比于语言模型中的思维链，将形状自由度减少为可调整参数，从而实现高精度的参数化编辑。实验结果表明，引入构造步骤提高了生成质量，并且对这些序列应用强化学习可在包括未明确优化的指标在内的多个度量上带来进一步改善。</div>
</details>
</div>
<div class="card">
<div class="title">SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling</div>
<div class="meta-line">Authors: Yixian Zhang, Shu&#x27;ang Yu, Tonghe Zhang, Mo Guang, Haojia Hui, Kaiwen Long, Yu Wang, Chao Yu, Wenbo Ding</div>
<div class="meta-line">First: 2025-09-30T04:21:20+00:00 · Latest: 2026-01-14T11:56:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25756v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25756v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which utilizes a decoded velocity. We then develop a practical SAC-based algorithm, enabled by a noise-augmented rollout, that facilitates direct end-to-end training of these policies. Our approach supports both from-scratch and offline-to-online learning and achieves state-of-the-art performance on continuous control and robotic manipulation benchmarks, eliminating the need for common workarounds like policy distillation or surrogate objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAC Flow：基于速度重参数化序列建模的样本高效流策略强化学习</div>
<div class="mono" style="margin-top:8px">由于多步动作采样过程中的梯度病态问题，利用离策略强化学习训练表达能力强的流策略极不稳定。我们将这种不稳定性归因于一个根本关联：流展开在代数上等价于残差循环计算，使其与RNN一样易受梯度消失和爆炸的影响。为此，我们借鉴现代序列模型原理对速度网络进行重参数化，提出了两种稳定架构：Flow-G（采用门控速度）和Flow-T（采用解码速度）。随后，我们开发了一种基于SAC的实用算法，通过噪声增强的展开过程，实现了对这些策略的直接端到端训练。该方法支持从零开始学习及离线到在线学习，在连续控制和机器人操作基准测试中达到了最先进的性能，无需依赖策略蒸馏或代理目标等常见变通方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the instability in training flow-based policies with off-policy reinforcement learning, which stems from gradient issues akin to those in recurrent neural networks during multi-step action sampling. To mitigate this, the authors propose a velocity-reparameterization method, introducing two stable architectures—Flow-G with gated velocity and Flow-T with decoded velocity—and develop a SAC-based algorithm using noise-augmented rollout for end-to-end training. Experimental results demonstrate state-of-the-art performance on continuous control and robotic manipulation benchmarks, enabling both from-scratch and offline-to-online learning without relying on policy distillation or surrogate objectives.</div>
<div class="mono" style="margin-top:8px">该论文针对离线策略强化学习中训练基于流的策略时的不稳定性问题，该问题源于多步动作采样过程中类似于循环神经网络的梯度消失或爆炸。为解决此问题，作者提出了一种速度重参数化方法，引入了两种稳定架构——Flow-G（采用门控速度）和Flow-T（采用解码速度），并开发了一种基于SAC的算法，通过噪声增强的展开实现端到端训练。实验结果表明，该方法在连续控制和机器人操作基准测试中达到了最先进的性能，支持从零开始和离线到在线的学习，且无需依赖策略蒸馏或替代目标等常见变通方案。</div>
</details>
</div>
<div class="card">
<div class="title">GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR</div>
<div class="meta-line">Authors: Jiaying Zhang, Lei Shi, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He</div>
<div class="meta-line">First: 2026-01-14T10:41:34+00:00 · Latest: 2026-01-14T10:41:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09361v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoRA：面向RLVR的几何感知低秩适配方法</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）对推进大规模推理模型至关重要。然而，现有参数高效方法（如PiSSA和MiLoRA）专为监督微调（SFT）设计，未考虑RLVR特有的优化动态与几何结构。直接应用这些方法会导致谱崩溃和优化不稳定，严重限制模型性能。同时，利用更新稀疏性的替代方法因非结构化计算在现代硬件上面临显著效率瓶颈。为解决这些问题，我们提出GeoRA（几何感知低秩适配），该方法利用RL更新子空间的各向异性与可压缩特性。GeoRA通过在几何约束子空间内通过奇异值分解（SVD）提取主方向来初始化适配器，同时冻结残差分量。此方法保留了预训练几何结构，并通过稠密算子实现高效GPU计算。在Qwen和Llama上的实验表明，GeoRA缓解了几何失准导致的优化瓶颈，在关键数学基准测试中持续超越现有低秩基线，达到最先进（SOTA）水平。此外，GeoRA在跨域任务中展现出优异的泛化能力与抗灾难性遗忘鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the limitations of existing parameter-efficient fine-tuning methods like PiSSA and MiLoRA, which are designed for supervised fine-tuning and fail to address the unique optimization dynamics and geometric structures in Reinforcement Learning with Verifiable Rewards (RLVR), leading to spectral collapse and instability. To overcome this, the authors propose GeoRA, a geometry-aware low-rank adaptation method that initializes adapters by applying Singular Value Decomposition within a constrained subspace to extract principal directions while freezing residual components, thereby preserving pre-trained geometry and enabling efficient GPU computation. Experimental results on models such as Qwen and Llama show that GeoRA effectively mitigates optimization bottlenecks, outperforms established low-rank baselines on mathematical benchmarks with state-of-the-art performance, and demonstrates superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于现有参数高效微调方法如PiSSA和MiLoRA专为监督微调设计，无法处理可验证奖励强化学习中的独特优化动态和几何结构，导致谱崩溃和不稳定问题。为此，作者提出了GeoRA，一种几何感知的低秩适应方法，通过在几何约束子空间内应用奇异值分解来提取主方向并初始化适配器，同时冻结残差分量，从而保留预训练几何结构并实现高效的GPU计算。在Qwen和Llama等模型上的实验结果表明，GeoRA有效缓解了由几何失配引起的优化瓶颈，在关键数学基准测试中持续优于现有低秩基线方法，取得了最先进的性能，并在域外任务中展现出更优的泛化能力和对灾难性遗忘的抵抗力。</div>
</details>
</div>
<div class="card">
<div class="title">Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving</div>
<div class="meta-line">Authors: Ioannis Peridis, Dimitrios Troullinos, Georgios Chalkiadakis, Pantelis Giankoulidis, Ioannis Papamichail, Markos Papageorgiou</div>
<div class="meta-line">First: 2026-01-14T10:35:21+00:00 · Latest: 2026-01-14T10:35:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09353v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09353v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles&#x27; policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经网络引导的蒙特卡洛树搜索在无车道自动驾驶中的应用</div>
<div class="mono" style="margin-top:8px">无车道交通环境使车辆能够更好地利用道路横向空间，不受车道保持限制，从而提高交通流量。这为自动驾驶带来了独特且更具挑战性的场景。本研究针对无车道交通中的单智能体自动驾驶，采用蒙特卡洛树搜索规划方法，其关联的马尔可夫决策过程借鉴了现有强化学习框架的方法。此外，MCTS配备了预训练的神经网络来引导选择阶段，该过程结合了神经网络的预测能力，在计算约束下实现更明智的树搜索。实验评估中，我们采用安全性（通过碰撞率）和效率（通过测速）指标，并分析了：（a）无车道环境中车辆各向同性状态信息的影响——车辆因后方快速车辆的存在而产生避让行为；（b）神经网络引导的MCTS变体的性能加速效果；（c）计算资源与求解质量之间的权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of autonomous driving in lane-free traffic environments, which aim to increase road capacity by removing lane restrictions but introduce greater planning complexity. The method employs a Monte-Carlo Tree Search (MCTS) planner for single-agent decision-making, enhanced with a pre-trained neural network to guide action selection, thereby improving search efficiency under computational limits. Experimental results demonstrate that the approach effectively reduces collision rates while maintaining vehicle speed, reveals how isotropic state information induces nudging behaviors in response to faster trailing vehicles, and shows that the neural network guidance accelerates MCTS performance while balancing computation and solution quality.</div>
<div class="mono" style="margin-top:8px">本文针对无车道交通环境中的自动驾驶挑战展开研究，该环境通过取消车道限制来提高道路容量，但增加了规划复杂性。方法采用蒙特卡洛树搜索进行单智能体决策规划，并引入预训练神经网络指导动作选择，从而在计算限制下提升搜索效率。实验结果表明，该方法有效降低了碰撞率并保持了车辆速度，揭示了各向同性状态信息如何促使车辆对后方更快车辆产生避让行为，同时证明了神经网络引导加速了蒙特卡洛树搜索的性能，并在计算资源与求解质量间取得了平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures</div>
<div class="meta-line">Authors: Sofiene Lassoued, Stefan Lier, Andreas Schwung</div>
<div class="meta-line">First: 2026-01-14T08:53:46+00:00 · Latest: 2026-01-14T08:53:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09293v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach follows a model-based paradigm, using Coloured Timed Petri Nets to represent the scheduling environment, and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. To simulate realistic industrial conditions, dynamic job arrivals are modeled using a Gamma distribution, which captures complex temporal patterns such as bursts, clustering, and fluctuating workloads. Machine failures are modeled using a Weibull distribution to represent age-dependent degradation and wear-out dynamics. These stochastic models enable the framework to reflect real-world manufacturing scenarios better. In addition, we study two action-masking strategies: a non-gradient approach that overrides the probabilities of invalid actions, and a gradient-based approach that assigns negative gradients to invalid actions within the policy network. We conduct extensive experiments on dynamic JSSP benchmarks, demonstrating that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization. The results highlight the strength of combining interpretable Petri-net-based models with adaptive reinforcement learning policies, yielding a resilient, scalable, and explainable framework for real-time scheduling in dynamic and uncertain manufacturing environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略强化学习与动作掩码的不确定性动态作业车间调度：处理随机到达与机器故障</div>
<div class="mono" style="margin-top:8px">本文提出一种解决不确定性条件下动态作业车间调度问题的新框架，针对随机工件到达和意外机器故障带来的挑战。该方法采用基于模型的范式，使用着色时间Petri网表示调度环境，并利用可掩码近端策略优化实现动态决策，同时限制智能体在每个决策点仅选择可行动作。为模拟真实工业条件，动态工件到达采用伽马分布建模，以捕捉突发性、聚集性和波动性工作负载等复杂时序特征；机器故障采用威布尔分布建模，以表征与使用时长相关的性能退化与磨损动态。这些随机模型使框架能更准确反映实际制造场景。此外，我们研究了两种动作掩码策略：一种是通过覆盖无效动作概率的非梯度方法，另一种是在策略网络中对无效动作分配负梯度的基于梯度的方法。通过在动态JSSP基准测试上的大量实验，证明本方法在最小化完工时间指标上持续优于传统启发式和基于规则的调度方法。结果表明，将可解释的Petri网模型与自适应强化学习策略相结合，能够为动态不确定制造环境构建具有韧性、可扩展性和可解释性的实时调度框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of Dynamic Job Shop Scheduling under uncertainty, motivated by the need to handle stochastic job arrivals and machine failures in real-world manufacturing. The method employs a model-based framework using Coloured Timed Petri Nets to represent the environment and Maskable Proximal Policy Optimization for decision-making, with action masking to restrict actions to feasible ones; job arrivals and machine failures are modeled with Gamma and Weibull distributions, respectively, to simulate realistic conditions. Experimental results on dynamic benchmarks show that this approach consistently outperforms traditional heuristic and rule-based methods in minimizing makespan, demonstrating the effectiveness of combining interpretable Petri-net models with adaptive reinforcement learning for resilient and explainable scheduling.</div>
<div class="mono" style="margin-top:8px">本文针对动态作业车间调度中的不确定性问题，旨在解决实际制造中随机工件到达和机器故障带来的挑战。方法采用基于模型的框架，使用着色时间Petri网表示调度环境，并利用可掩码近端策略优化进行决策，通过动作掩码限制可行动作；工件到达和机器故障分别用Gamma和Weibull分布建模以模拟真实条件。在动态基准测试上的实验结果表明，该方法在最小化完工时间方面持续优于传统启发式和基于规则的方法，凸显了将可解释Petri网模型与自适应强化学习相结合的优势，为动态不确定制造环境提供了鲁棒、可扩展且可解释的实时调度框架。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction</div>
<div class="meta-line">Authors: Mianzhi Pan, JianFei Li, Peishuo Liu, Botian Wang, Yawen Ouyang, Yiming Rong, Hao Zhou, Jianbing Zhang</div>
<div class="meta-line">First: 2026-01-14T08:45:07+00:00 · Latest: 2026-01-14T08:45:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09285v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs&#x27; high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强大语言模型空间推理能力用于金属有机框架结构预测</div>
<div class="mono" style="margin-top:8px">金属有机框架（MOFs）是一种多孔晶体材料，在碳捕获和药物递送等领域有广泛应用，但其三维结构的精确预测仍面临重大挑战。尽管大语言模型（LLMs）在晶体生成方面展现出潜力，但MOFs的高原子复杂性限制了其在该领域的应用。受深度生成模型中块级范式成功的启发，我们率先将LLMs引入该领域，提出了首个专用于块级MOF结构预测的LLM框架——MOF-LLM。为有效利用LLMs完成这种模块化组装任务，我们的训练范式整合了空间感知持续预训练（CPT）、结构监督微调（SFT）和匹配驱动强化学习（RL）。通过引入显式空间先验，并利用软自适应策略优化（SAPO）提升结构稳定性，该方法显著增强了Qwen-3 8B模型在MOF结构预测中的空间推理能力。综合实验表明，MOF-LLM在保持优异采样效率的同时，性能超越了当前最先进的基于去噪和基于LLM的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of predicting the complex 3D structures of metal-organic frameworks (MOFs), which are crucial for applications like carbon capture, by enhancing the spatial reasoning of large language models (LLMs). The method introduces MOF-LLM, a novel framework that adapts LLMs for block-level structure prediction through a training paradigm combining spatial-aware continual pre-training, structural supervised fine-tuning, and matching-driven reinforcement learning with Soft Adaptive Policy Optimization to incorporate spatial priors and optimize stability. Experimental results show that MOF-LLM outperforms existing denoising-based and LLM-based methods in accuracy and sampling efficiency, demonstrating superior performance in MOF structure prediction.</div>
<div class="mono" style="margin-top:8px">本文针对金属有机框架（MOF）这一在碳捕获等领域应用广泛但结构预测困难的复杂三维材料，通过增强大语言模型的空间推理能力来解决其结构预测难题。方法上提出了MOF-LLM框架，首次将大语言模型适配于块级MOF结构预测，采用融合空间感知持续预训练、结构监督微调和匹配驱动强化学习的训练范式，并引入Soft Adaptive Policy Optimization以整合空间先验并优化结构稳定性。实验结果表明，MOF-LLM在预测准确性和采样效率上均优于当前最先进的去噪基和LLM基方法，展现了卓越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?</div>
<div class="meta-line">Authors: Long Zhang, Yuchen Xia, Bingqing Wei, Zhen Liu, Shiwen Mao, Zhu Han, Mohsen Guizani</div>
<div class="meta-line">First: 2026-01-13T11:05:12+00:00 · Latest: 2026-01-14T08:39:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08434v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08434v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向具身智能驾驶的大型多模态模型：自动驾驶的下一个前沿？</div>
<div class="mono" style="margin-top:8px">大型多模态模型的出现为解决自动驾驶模块化设计在开放场景中的局限性提供了前景广阔的技术路径，此类场景需持续的环境理解与逻辑推理能力。同时，具身人工智能通过闭环交互实现策略优化，赋予系统持续学习能力，从而推动自动驾驶向具身智能驾驶演进。然而，若仅依赖大型多模态模型增强具身智能驾驶而缺乏联合决策，其能力将受到制约。本文提出一种新颖的语义与策略双驱动混合决策框架以应对该挑战，确保持续学习与联合决策。该框架融合大型多模态模型的语义理解与认知表征能力，以及深度强化学习的实时策略优化机制。我们首先阐述具身智能驾驶与大型多模态模型的基础原理，继而探讨该框架催生的新兴机遇，包括潜在优势与典型应用场景。通过车道变换规划任务的实验案例验证了本框架的性能优越性。最后，提出若干赋能具身智能驾驶的未来研究方向以指导后续工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of modular autonomous driving systems in complex open-world scenarios, this paper proposes a novel hybrid decision framework that integrates Large Multimodal Models (LMMs) for semantic understanding with Deep Reinforcement Learning (DRL) for policy optimization to achieve embodied intelligent driving. The method combines LMMs&#x27; cognitive representation and DRL&#x27;s real-time decision-making to enable continuous learning and joint action planning. The main experimental results from a lane-change planning case study demonstrate the performance superiority of this dual-driven framework in completing the driving task.</div>
<div class="mono" style="margin-top:8px">本文针对模块化自动驾驶系统在复杂开放场景中的局限性，提出了一种新颖的混合决策框架，将大型多模态模型（LMM）的语义理解能力与深度强化学习（DRL）的实时策略优化相结合，以实现具身智能驾驶。该方法通过融合LMM的认知表征和DRL的决策，支持持续学习和联合规划。在车道变换规划的案例实验中，该双驱动框架被验证在完成任务方面具有性能优势。</div>
</details>
</div>
<div class="card">
<div class="title">RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering</div>
<div class="meta-line">Authors: Wencheng Ye, Liang Peng, Xiaoyang Yuan, Yi Bin, Pengpeng Zeng, Hengyu Jin, Heng Tao Shen</div>
<div class="meta-line">First: 2026-01-14T08:04:33+00:00 · Latest: 2026-01-14T08:04:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09269v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09269v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RISER：基于潜在推理能力的自适应激活引导编排框架</div>
<div class="mono" style="margin-top:8px">近期针对大语言模型（LLMs）的领域特定推理研究多依赖需要参数更新的训练密集型方法。虽然激活引导已成为参数高效的替代方案，但现有方法采用静态人工干预，难以适应复杂推理的动态特性。为此，我们提出RISER（基于路由器的可引导推理增强框架），这是一种即插即用的干预框架，可在激活空间自适应引导LLM推理。RISER构建可复用推理向量库，并采用轻量级路由器为每个输入动态组合向量。该路由器通过任务级奖励下的强化学习进行优化，以涌现式组合方式激活潜在认知基元。在七个多样化基准测试中，RISER相比基础模型实现零样本准确率平均提升3.4-6.5%，同时以2-3倍更高的标记效率超越思维链式推理，并获得稳健的准确率增益。进一步分析表明，RISER能自主组合多个向量形成可解释的精确控制策略，为实现更可控高效的LLM推理指明方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to overcome the limitations of static, manual activation steering methods in large language models (LLMs), which fail to adapt during complex reasoning tasks, and to provide a more efficient alternative to training-intensive approaches. The proposed method, RISER, is a plug-and-play framework that constructs a library of reusable reasoning vectors and uses a lightweight Router, optimized via reinforcement learning, to dynamically compose these vectors for each input, thereby adaptively steering LLM reasoning in activation space. The main experimental results show that across seven diverse benchmarks, RISER achieves an average zero-shot accuracy improvement of 3.4-6.5% over the base model, surpasses chain-of-thought reasoning with 2-3 times higher token efficiency, and demonstrates robust accuracy gains, while analysis reveals it autonomously combines vectors into interpretable control strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是克服大型语言模型中静态、手动激活引导方法的局限性，这些方法在复杂推理任务中无法自适应调整，且旨在为训练密集型方法提供更高效的替代方案。所提出的方法RISER是一个即插即用框架，它构建了一个可复用的推理向量库，并通过强化学习优化的轻量级路由器动态组合这些向量以适应每个输入，从而在激活空间中自适应地引导模型推理。主要实验结果表明，在七个多样化基准测试中，RISER相比基础模型实现了3.4-6.5%的平均零样本准确率提升，以2-3倍的令牌效率超越思维链式推理，并展现出稳健的准确率增益，同时分析显示其能自主将向量组合为可解释的控制策略。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability</div>
<div class="meta-line">Authors: Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang</div>
<div class="meta-line">First: 2026-01-14T07:52:14+00:00 · Latest: 2026-01-14T07:52:14+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures. Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09261v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09261v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner&#x27;s own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner&#x27;s internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学会信任经验：不可观测反馈可靠性下的学习监控-信任-调节框架</div>
<div class="mono" style="margin-top:8px">在反馈可靠性不可观测的条件下学习，提出了超越优化鲁棒性的独特挑战：系统必须决定是否从经验中学习，而不仅仅是学习如何稳定学习。我们将此设定研究为不可观测可靠性下的认知可识别性（EIUR），其中每个经验具有潜在可信度，可靠与不可靠反馈在局部可能无法区分，且数据由学习者自身不断演化的信念与行动在闭环中生成。在EIUR中，标准鲁棒学习可能稳定收敛，却形成高置信度但系统错误的信念。
我们提出元认知调节作为实用应对方案：一个内省的二级控制环路，从学习者内部动态的内生证据中推断经验可信度。我们将其形式化为模块化的监控-信任-调节（MTR）分解，并通过自我诊断实例化——该方法维护一个缓慢变化的经验信任变量，以软性调节学习更新，无需外部可靠性标签或显式损坏模型。
实证研究表明，在本文探讨的EIUR机制中，自我诊断与认知可识别性的提升相关。在强化学习中，它实现了校准后的怀疑态度及在系统损坏奖励下的恢复能力。在监督学习中，它揭示了一个关键分离现象：性能恢复并不等同于认知恢复。准确率可能回升，而内部信念动态仍被早期误导性数据锁定，这种失败仅能通过内省诊断检测。综上，MTR与自我诊断为不可观测可靠性下自主学习的内部可靠性评估提供了组织化抽象框架与具体设计模板。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of learning when feedback reliability is unobservable, a problem termed Epistemic Identifiability under Unobservable Reliability (EIUR), where standard robust methods can converge to confidently wrong beliefs. To counter this, the authors propose a metacognitive Monitor-Trust-Regulator (MTR) framework that uses introspective self-diagnosis to infer experience credibility from the learner&#x27;s internal dynamics, softly modulating updates without external labels. Experimental results in reinforcement and supervised learning show that self-diagnosis improves epistemic identifiability, enabling recovery from corrupted rewards and revealing that accuracy recovery can mask persistent misleading beliefs, detectable only through introspective diagnostics.</div>
<div class="mono" style="margin-top:8px">本文研究了在反馈可靠性不可观测情况下的学习挑战，即不可观测可靠性的认知可识别性问题，其中标准鲁棒方法可能稳定地收敛到错误信念。作为应对，作者提出了一个元认知的监控-信任-调节器框架，通过内省式自我诊断从学习者的内部动态推断经验可信度，无需外部标签即可软调节更新。在强化学习和监督学习中的实验结果表明，自我诊断提高了认知可识别性，能够从系统损坏的奖励中恢复，并揭示出性能恢复可能掩盖由早期误导数据导致的持续错误信念，这种失败只能通过内省诊断检测到。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models</div>
<div class="meta-line">Authors: Yan Liu, Feng Zhang, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Han Liu, Yangdong Deng</div>
<div class="meta-line">First: 2026-01-14T07:52:05+00:00 · Latest: 2026-01-14T07:52:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09260v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09260v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效路径与密集奖励：大语言模型的概率流推理</div>
<div class="mono" style="margin-top:8px">高质量思维链已展现出激发大语言模型推理能力的强大潜力。然而，当前范式通常将推理过程视为不可分割的序列，缺乏量化逐步信息增益的内在机制。这种粒度缺失体现在两个局限：因缺乏显式指导而冗余探索导致的推理低效，以及因稀疏结果监督或昂贵外部验证器导致的优化困难。本研究提出CoT-Flow框架，将离散推理步骤重构为连续概率流，量化每个步骤对真实答案的贡献。基于此框架，CoT-Flow实现了两种互补方法：流引导解码——采用基于流的贪心解码策略提取信息高效的推理路径；流强化学习——构建无需验证器的密集奖励函数。在挑战性基准测试上的实验表明，CoT-Flow在推理效率与推理性能间实现了更优平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses inefficiencies in chain-of-thought reasoning for large language models, where current methods treat reasoning as an indivisible sequence without quantifying step-wise information gain, leading to redundant exploration and sparse supervision. To overcome this, the authors propose CoT-Flow, a framework that models reasoning steps as a continuous probabilistic flow to measure each step&#x27;s contribution toward the correct answer, enabling flow-guided decoding for efficient path extraction and flow-based reinforcement learning with dense rewards. Experimental results on challenging benchmarks show that CoT-Flow achieves a better balance between inference efficiency and reasoning performance compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型中思维链推理的低效性问题，指出当前方法将推理视为不可分割的序列，缺乏对逐步信息增益的量化，导致冗余探索和稀疏监督。为此，作者提出CoT-Flow框架，将离散推理步骤重新概念化为连续概率流，以量化每个步骤对正确答案的贡献，从而支持基于流的解码以提取高效推理路径，以及基于流的强化学习以构建无需验证器的密集奖励函数。在具有挑战性的基准测试中，实验结果表明CoT-Flow在推理效率和性能之间实现了更优的平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Learning through Ranking Mean Squared Error</div>
<div class="meta-line">Authors: Chaitanya Kharyal, Calarina Muslimani, Matthew E. Taylor</div>
<div class="meta-line">First: 2026-01-14T07:18:12+00:00 · Latest: 2026-01-14T07:18:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09236v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., &quot;bad,&quot; &quot;neutral,&quot; &quot;good&quot;). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher&#x27;s ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于排序均方误差的奖励学习</div>
<div class="mono" style="margin-top:8px">奖励设计仍是强化学习应用于现实问题的关键瓶颈。奖励学习作为一种主流替代方案，通过人类反馈而非人工设定来推导奖励函数。近期研究提出从评分形式的人类反馈中学习奖励函数，取代传统的二元偏好，以提供更丰富且认知负担更低的监督。基于此范式，我们提出一种新的基于评分的强化学习方法——强化学习排序回报回归（R4）。其核心采用新颖的排序均方误差损失函数，将教师提供的评分视为序数目标。该方法通过轨迹-评分配对数据集进行学习，每条轨迹均标注离散评分（如“差”“中”“优”）。在每轮训练中，我们采样一组轨迹并预测其回报，通过可微分排序算子（软排序）进行排名，随后优化软排序结果与教师评分间的均方误差损失。与现有基于评分的方法不同，R4具备形式化保证：在温和假设下，其解集可证明为最小且完备的。通过模拟人类反馈的实验验证，R4在OpenAI Gym与DeepMind Control Suite的机器人运动基准测试中，持续匹配或超越现有基于评分及偏好的强化学习方法，且所需反馈量显著减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the bottleneck of reward design in reinforcement learning by learning reward functions from human feedback in the form of ratings, which provide richer supervision than binary preferences. The method introduces Ranked Return Regression for RL (R4), which uses a novel ranking mean squared error loss that treats ratings as ordinal targets and employs differentiable soft ranking of predicted returns. Experimental results on robotic locomotion benchmarks show that R4 consistently matches or outperforms existing rating and preference-based methods while requiring significantly less feedback, with formal guarantees on solution minimality and completeness.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过从人类评分反馈中学习奖励函数，以解决强化学习中奖励设计的瓶颈问题，评分比二元偏好提供更丰富的监督信息。方法上提出了Ranked Return Regression for RL (R4)，采用新颖的排序均方误差损失，将评分视为序数目标，并使用可微分的软排序处理预测回报。在机器人运动基准测试中的实验结果表明，R4在显著减少反馈需求的同时，持续匹配或优于现有的评分和偏好方法，并具有解集最小性和完备性的形式化保证。</div>
</details>
</div>
<div class="card">
<div class="title">GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization</div>
<div class="meta-line">Authors: Zhengyang Zhao, Lu Ma, Yizhen Jiang, Xiaochen Ma, Zimo Meng, Chengyu Shen, Lexiang Tang, Haoze Sun, Peng Pei, Wentao Zhang</div>
<div class="meta-line">First: 2026-01-14T07:13:57+00:00 · Latest: 2026-01-14T07:13:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09233v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09233v1">PDF</a> · <a href="https://github.com/zzy1127/GIFT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GIFT：通过有限温度吉布斯初始化解锁训练后全局最优性</div>
<div class="mono" style="margin-top:8px">当前大型推理模型（LRM）的训练后范式——监督微调（SFT）后接强化学习（RL）——存在内在的优化失配问题：SFT固有的刚性监督会导致分布坍缩，从而耗尽后续RL所需的探索空间。本文在统一的训练后框架中重新形式化SFT，并提出有限温度吉布斯初始化（GIFT）。我们将标准SFT描述为抑制基础先验的退化零温极限；相反，GIFT将监督作为有限温度能量势引入，建立了一个分布桥梁，确保整个训练后流程的目标一致性。实验表明，当用于RL初始化时，GIFT显著优于标准SFT及其他竞争基线，为实现训练后全局最优性提供了数学原理清晰的路径。代码发布于 https://github.com/zzy1127/GIFT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address the optimization mismatch in the standard post-training pipeline for Large Reasoning Models, where Supervised Fine-Tuning (SFT) causes distributional collapse and hinders subsequent Reinforcement Learning (RL). The method proposed, Gibbs Initialization with Finite Temperature (GIFT), reformulates SFT within a unified framework by incorporating supervision as a finite-temperature energy potential, thereby preserving base priors and ensuring objective consistency. Experimental results show that GIFT significantly outperforms standard SFT and other baselines when used to initialize RL, offering a principled path toward global optimality in post-training.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型推理模型标准后训练流程中的优化不匹配问题，即监督微调会导致分布坍缩，从而阻碍后续的强化学习。所提出的方法——有限温度吉布斯初始化（GIFT）——在一个统一框架中重新表述了监督微调，通过将监督作为有限温度能量势引入，保留了基础先验并确保了整个训练流程的目标一致性。实验结果表明，当用于初始化强化学习时，GIFT显著优于标准监督微调及其他竞争基线，为后训练中实现全局最优性提供了理论依据明确的途径。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning with Exogenous States and Rewards</div>
<div class="meta-line">Authors: George Trimponias, Thomas G. Dietterich</div>
<div class="meta-line">First: 2023-03-22T23:37:28+00:00 · Latest: 2026-01-14T05:15:05+00:00</div>
<div class="meta-line">Comments: Substantial rewrite to improve rigor and clarity in response to referee reports at JMLR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2303.12957v2">Abs</a> · <a href="https://arxiv.org/pdf/2303.12957v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms can be applied during reinforcement learning to discover the exogenous subspace, remove the exogenous reward, and focus reinforcement learning on the endogenous MDP. Experiments on a variety of challenging synthetic MDPs show that these methods, applied online, discover large exogenous state spaces and produce substantial speedups in reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有外生状态与奖励的强化学习</div>
<div class="mono" style="margin-top:8px">外生状态变量和奖励会通过向奖励信号注入不可控的变异而减缓强化学习进程。本文形式化定义了外生状态变量与奖励，并证明若奖励函数可加性分解为内生与外生成分，则马尔可夫决策过程可分解为基于外生奖励的外生马尔可夫奖励过程，以及优化内生奖励的内生马尔可夫决策过程。内生MDP的任何最优策略同样是原MDP的最优策略，但由于内生奖励通常具有更低的方差，内生MDP更易于求解。我们研究了状态空间分解为外生与内生子空间未知而需主动发现的情境。本文提出并证明了在线性组合混合状态下发现状态空间外生与内生子空间的算法正确性。这些算法可在强化学习过程中实时应用，以发现外生子空间、移除外生奖励，并将强化学习聚焦于内生MDP。在多种复杂合成MDP上的实验表明，这些在线应用的方法能有效发现大规模外生状态空间，并显著加速强化学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge that exogenous state variables and rewards pose to reinforcement learning by introducing uncontrolled variation, which slows down learning. The authors formalize these concepts and demonstrate that if the reward function can be additively decomposed into endogenous and exogenous components, the original MDP can be separated into an exogenous Markov Reward Process and an endogenous MDP, where any optimal policy for the latter remains optimal for the full problem but is easier to solve due to reduced reward variance. They propose algorithms to automatically discover the exogenous and endogenous state subspaces when they are linearly mixed, enabling online application during reinforcement learning to remove exogenous rewards and focus on the endogenous MDP. Experimental results on synthetic MDPs confirm that these methods successfully identify large exogenous state spaces and achieve significant speedups in learning.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中外部状态变量和奖励引入不可控变化从而减缓学习速度的问题，提出了解决方案。作者形式化了这些概念，并证明如果奖励函数可加性分解为内生和外部成分，原始马尔可夫决策过程可分解为外部马尔可夫奖励过程和内生MDP，其中内生MDP的最优策略对原问题仍是最优的，但由于奖励方差降低而更易求解。他们提出了在线算法，用于在线性混合情况下自动发现外部和内生状态子空间，从而在强化学习过程中移除外部奖励并聚焦于内生MDP。在合成MDP上的实验结果表明，这些方法能有效识别大规模外部状态空间，并显著加速学习过程。</div>
</details>
</div>
<div class="card">
<div class="title">ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning</div>
<div class="meta-line">Authors: Ruiyang Zhou, Shuozhe Li, Amy Zhang, Liu Leqi</div>
<div class="meta-line">Venue: NeurIPS 2025 Poster</div>
<div class="meta-line">First: 2025-07-03T17:44:55+00:00 · Latest: 2026-01-14T04:45:32+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025 (Poster). Code available at https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02834v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.02834v2">PDF</a> · <a href="https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-improvement via RL often fails on complex reasoning tasks because GRPO-style post-training methods rely on the model&#x27;s initial ability to generate positive samples. Without guided exploration, these approaches merely reinforce what the model already knows (distribution-sharpening) rather than enabling the model to solve problems where it initially generates no correct solutions. To unlock reasoning ability in such settings, the model must explore new reasoning trajectories beyond its current output distribution. Such exploration requires access to sufficiently good positive samples to guide the learning. While expert demonstrations seem like a natural solution, we find that they are often ineffective in RL post-training. Instead, we identify two key properties of effective positive samples: they should (1) be likely under the current policy, and (2) increase the model&#x27;s likelihood of predicting the correct answer. Based on these insights, we propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and modular framework that generates such samples by conditioning on the ground-truth answer. It can be integrated with popular RL training methods like GRPO and DPO. ExPO enables efficient exploration and guides the model to produce reasoning trajectories more aligned with its policy than expert-written CoTs, while ensuring higher quality than its own (incorrect) samples. Experiments show that ExPO improves both learning efficiency and final performance on reasoning benchmarks, surpassing expert-demonstration-based methods in challenging settings such as MATH level-5, where the model initially struggles the most. Code is available at https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExPO：通过自解释引导的强化学习解锁复杂推理能力</div>
<div class="mono" style="margin-top:8px">基于强化学习（RL）的自我改进方法在复杂推理任务上常失效，因为GRPO等后训练方法依赖模型生成正向样本的初始能力。若无引导式探索，这些方法仅强化模型已有知识（分布锐化），而无法解决模型初始无法生成正确解的问题。为在此类场景中解锁推理能力，模型需探索超出当前输出分布的新推理轨迹。此类探索需要足够优质的正向样本来引导学习。虽然专家演示看似是自然解决方案，但我们发现其在RL后训练中往往效果有限。相反，我们提出有效正向样本的两个关键特性：（1）在当前策略下应具有高似然性；（2）能提升模型预测正确答案的似然度。基于此，我们提出$\textbf{自解释策略优化（ExPO）}$——一种通过以真实答案为条件生成此类样本的简洁模块化框架，可与GRPO、DPO等主流RL训练方法集成。相比专家撰写的思维链，ExPO既能实现高效探索，又能引导模型生成更符合其策略的推理轨迹，同时确保比模型自身（错误）样本更高的质量。实验表明，ExPO在推理基准测试中提升了学习效率和最终性能，在MATH 5级等模型初始表现最困难的挑战性场景中超越了基于专家演示的方法。代码发布于https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of self-improvement via reinforcement learning (RL) on complex reasoning tasks, where methods like GRPO often fail because they merely reinforce existing knowledge rather than enabling exploration when the model initially generates no correct solutions. To overcome this, the authors propose Self-Explanation Policy Optimization (ExPO), a modular framework that generates effective positive samples by conditioning on ground-truth answers, ensuring these samples are likely under the current policy and increase the model&#x27;s likelihood of predicting correct answers, which can be integrated with RL methods like GRPO and DPO. Experimental results demonstrate that ExPO enhances learning efficiency and final performance on reasoning benchmarks, outperforming expert-demonstration-based methods in challenging settings such as MATH level-5, where models initially struggle the most.</div>
<div class="mono" style="margin-top:8px">本文针对复杂推理任务中基于强化学习（RL）的自改进方法的局限性，指出如GRPO等方法往往仅强化模型已有知识，而在模型初始无法生成正确解时难以实现探索。为解决这一问题，作者提出了自解释策略优化（ExPO），这是一个模块化框架，通过基于真实答案生成有效的正样本，确保这些样本在当前策略下具有高可能性并能提高模型预测正确答案的似然，可与GRPO和DPO等RL方法结合。实验结果表明，ExPO在推理基准测试中提升了学习效率和最终性能，在如MATH level-5等模型初始表现最差的挑战性场景中，超越了基于专家演示的方法。</div>
</details>
</div>
<div class="card">
<div class="title">SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL</div>
<div class="meta-line">Authors: Lijun Liu, Linwei Chen, Zhishou Zhang, Meng Tian, Hengfu Cui, Ruiyang Li, Zhaocheng Liu, Qiang Ju, Qianxi Li, Hong-Yu Zhou</div>
<div class="meta-line">First: 2026-01-14T04:21:07+00:00 · Latest: 2026-01-14T04:21:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to &quot;diffuse attention&quot; - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to &quot;unfold&quot; complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SkinFlow：基于动态视觉编码与分阶段强化学习的开放式皮肤病诊断高效信息传输框架</div>
<div class="mono" style="margin-top:8px">通用大规模视觉语言模型（LVLMs）虽参数量庞大，但在皮肤病学中常因&#x27;注意力弥散&#x27;问题而表现不佳——即难以从背景噪声中分离细微病理特征。本文挑战了&#x27;参数扩展是提升医学精度的唯一路径&#x27;这一假设，提出SkinFlow框架，将诊断过程视为视觉信息传输效率的优化问题。该方法采用虚拟宽度动态视觉编码器（DVE）在不增加实体参数的前提下&#x27;展开&#x27;复杂病理流形，并结合两阶段强化学习策略：第一阶段对齐显性医学描述，第二阶段在受限语义空间中重建隐性诊断纹理特征。此外，我们提出以临床实践为基础的评估方案，优先考量诊断安全性与层次化关联性，而非僵化的标签匹配。实证结果显著：我们的70亿参数模型在Fitzpatrick17k基准测试中刷新最优性能，相较于通用大模型（如Qwen3VL-235B和GPT-5.2），Top-1准确率提升12.06%，Top-6准确率提升28.57%。这表明优化几何容量与信息流能比单纯扩展参数产生更优越的诊断推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper challenges the reliance on scaling model parameters for medical AI precision, motivated by the &#x27;diffuse attention&#x27; problem where general large vision-language models struggle to focus on subtle skin lesions amidst background noise. It introduces SkinFlow, a framework that optimizes visual information transmission efficiency through a Virtual-Width Dynamic Vision Encoder to unfold complex pathological features without adding parameters, combined with a two-stage reinforcement learning strategy that first aligns explicit medical descriptions and then reconstructs implicit diagnostic textures. Experimental results show that the compact 7B model achieves state-of-the-art performance on the Fitzpatrick17k benchmark, with a +12.06% Top-1 accuracy and +28.57% Top-6 accuracy improvement over much larger general-purpose models, demonstrating that enhanced geometric capacity and information flow outperform mere parameter scaling.</div>
<div class="mono" style="margin-top:8px">本文针对通用大型视觉语言模型在皮肤病诊断中因&#x27;注意力分散&#x27;而难以区分细微病变与背景噪声的问题，挑战了仅靠参数缩放提升医学精度的假设。提出了SkinFlow框架，通过虚拟宽度动态视觉编码器在不增加物理参数的情况下展开复杂病理特征，并结合两阶段强化学习策略，先对齐显性医学描述，再重建隐性诊断纹理，以优化视觉信息传输效率。实验结果表明，该紧凑的7B模型在Fitzpatrick17k基准测试中取得了最先进的性能，Top-1准确率提升12.06%，Top-6准确率提升28.57%，优于参数量大得多的通用模型，证明优化几何能力和信息流比单纯参数缩放更有效。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design</div>
<div class="meta-line">Authors: Lianghong Chen, Dongkyu Eugene Kim, Mike Domaratzki, Pingzhao Hu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-24T04:49:23+00:00 · Latest: 2026-01-14T04:12:09+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21153v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21153v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing de novo 3D molecules with desirable properties remains a fundamental challenge in drug discovery and molecular engineering. While diffusion models have demonstrated remarkable capabilities in generating high-quality 3D molecular structures, they often struggle to effectively control complex multi-objective constraints critical for real-world applications. In this study, we propose an uncertainty-aware Reinforcement Learning (RL) framework to guide the optimization of 3D molecular diffusion models toward multiple property objectives while enhancing the overall quality of the generated molecules. Our method leverages surrogate models with predictive uncertainty estimation to dynamically shape reward functions, facilitating balance across multiple optimization objectives. We comprehensively evaluate our framework across three benchmark datasets and multiple diffusion model architectures, consistently outperforming baselines for molecular quality and property optimization. Additionally, Molecular Dynamics (MD) simulations and ADMET profiling of top generated candidates indicate promising drug-like behavior and binding stability, comparable to known Epidermal Growth Factor Receptor (EGFR) inhibitors. Our results demonstrate the strong potential of RL-guided generative diffusion models for advancing automated molecular design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不确定性感知多目标强化学习引导扩散模型用于三维从头分子设计</div>
<div class="mono" style="margin-top:8px">设计具有理想性质的三维从头分子仍是药物发现和分子工程领域的核心挑战。尽管扩散模型在生成高质量三维分子结构方面展现出卓越能力，但其往往难以有效控制实际应用中至关重要的复杂多目标约束。本研究提出一种不确定性感知强化学习框架，用于引导三维分子扩散模型向多属性目标优化，同时提升生成分子的整体质量。该方法利用具有预测不确定性估计的代理模型动态调整奖励函数，促进多优化目标间的平衡。我们在三个基准数据集和多种扩散模型架构上全面评估了该框架，其在分子质量和属性优化方面均持续优于基线方法。此外，对生成候选分子的分子动力学模拟和ADMET分析表明，其具有与已知表皮生长因子受体抑制剂相当的类药行为和结合稳定性。我们的结果证明了强化学习引导的生成扩散模型在推进自动化分子设计方面的巨大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of designing 3D molecules with specific multi-property constraints, a key need in drug discovery, where standard diffusion models often fall short. The authors propose an uncertainty-aware reinforcement learning framework that guides 3D molecular diffusion models by using surrogate models with uncertainty estimation to dynamically shape rewards, balancing multiple property objectives. Experimental results on three benchmark datasets show the method outperforms baselines in molecular quality and property optimization, with Molecular Dynamics simulations and ADMET profiling of top candidates revealing drug-like behavior and binding stability comparable to known EGFR inhibitors, demonstrating its potential for automated molecular design.</div>
<div class="mono" style="margin-top:8px">本研究针对药物发现中设计满足多重属性约束的3D分子这一关键挑战，而标准扩散模型在此方面常显不足。作者提出了一种不确定性感知的强化学习框架，通过利用具有不确定性估计的代理模型动态调整奖励函数，以引导3D分子扩散模型，从而平衡多个属性优化目标。在三个基准数据集上的实验结果表明，该方法在分子质量和属性优化上均优于基线模型；对生成候选分子的分子动力学模拟和ADMET分析显示，其具有与已知表皮生长因子受体抑制剂相当的类药行为和结合稳定性，证明了其在自动化分子设计中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">LSRIF: Logic-Structured Reinforcement Learning for Instruction Following</div>
<div class="meta-line">Authors: Qingyu Ren, Qianyu He, Jingwen Chang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Han Xia, Zeye Sun, Fei Yu</div>
<div class="meta-line">First: 2026-01-10T05:11:38+00:00 · Latest: 2026-01-14T02:51:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06431v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06431v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LSRIF：面向指令跟随的逻辑结构化强化学习</div>
<div class="mono" style="margin-top:8px">指令跟随对大语言模型至关重要，但现实指令常包含顺序依赖和条件分支等逻辑结构。现有方法通常构建带并行约束的数据集并优化平均奖励，忽略了逻辑依赖关系并产生噪声信号。我们提出逻辑结构化训练框架LSRIF，显式建模指令逻辑。首先构建包含并行、顺序、条件等约束结构的LSRInstruct数据集，随后设计结构感知奖励方法LSRIF：对并行结构采用平均聚合，对顺序结构实施失败惩罚传播，对条件分支采用选择性奖励。实验表明LSRIF在指令跟随（域内/域外）和通用推理方面带来显著提升。分析显示，通过显式逻辑结构学习能驱动注意力层的参数更新，并增强对约束条件与逻辑运算符的词元级注意力聚焦。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of enabling large language models to follow real-world instructions that often contain complex logical structures like sequences and conditional branches, which existing methods overlook by treating constraints as independent and optimizing average rewards, leading to noisy training signals. The authors propose LSRIF, a logic-structured reinforcement learning framework that explicitly models instruction logic by first constructing a dataset, LSRInstruct, with structured constraints (parallel, sequential, conditional) and then designing a structure-aware rewarding method that applies average aggregation, failure-penalty propagation, and selective rewards tailored to each logic type. Experimental results demonstrate that LSRIF significantly improves instruction-following performance both in-domain and out-of-domain, as well as general reasoning capabilities, with analysis showing that explicit logic structure learning induces parameter updates in attention layers and sharpens token-level focus on constraints and logical operators.</div>
<div class="mono" style="margin-top:8px">该论文针对大型语言模型在遵循现实世界指令时面临的挑战，这些指令常包含如顺序依赖和条件分支等逻辑结构，而现有方法通常将约束视为独立并优化平均奖励，忽略了逻辑依赖性，导致训练信号嘈杂。作者提出了LSRIF，一种逻辑结构化的强化学习框架，通过首先构建包含并行、顺序和条件等结构化约束的数据集LSRInstruct，然后设计一种结构感知的奖励方法，针对每种逻辑类型应用平均聚合、失败惩罚传播和选择性奖励，以显式建模指令逻辑。实验结果表明，LSRIF在指令遵循（领域内和领域外）和一般推理能力上均带来显著提升，分析显示，显式的逻辑结构学习促使注意力层参数更新，并增强了模型对约束和逻辑运算符的令牌级关注。</div>
</details>
</div>
<div class="card">
<div class="title">SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache</div>
<div class="meta-line">Authors: Chi-Chih Chang, Siqi Zhu, Zhichen Zeng, Haibin Lin, Jiaxuan You, Mohamed S. Abdelfattah, Ziheng Jiang, Xuehai Qian</div>
<div class="meta-line">First: 2026-01-14T02:34:48+00:00 · Latest: 2026-01-14T02:34:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SRT：基于树结构缓存的推测式轨迹生成加速强化学习</div>
<div class="mono" style="margin-top:8px">本文提出基于树结构缓存的推测式轨迹生成方法（SRT），这是一种无需模型即可加速语言模型同策略强化学习的简易方法，且不牺牲分布准确性。SRT通过将同一提示在不同训练步骤中生成的延续文本存储于按提示组织的树结构缓存中，利用其经验相似性。在生成阶段，当前策略将此树作为草稿模型进行推测解码。为保持缓存新鲜度并提升草稿模型质量，SRT通过持续轨迹在线更新树结构，并利用GPU空闲时段主动执行预跑生成。集成至标准RL流程（如PPO、GRPO和DAPO）及多轮对话场景后，SRT持续降低生成与步骤延迟、减少单令牌推理成本，在轨迹生成阶段实现最高2.08倍的实际时间加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Speculative Rollout with Tree-Structured Cache (SRT), motivated by the need to accelerate on-policy reinforcement learning for language models without compromising distributional correctness. The method leverages empirical similarities in rollouts across training steps by storing past generated continuations in a per-prompt tree cache, which serves as a draft model for speculative decoding during generation; it keeps the cache updated via online rollouts and proactive run-ahead generation during GPU idle times. Experimental results show that SRT, when integrated into RL pipelines like PPO, GRPO, and DAPO, reduces generation latency and inference costs, achieving up to 2.08x wall-clock speedup during rollout.</div>
<div class="mono" style="margin-top:8px">本文提出了基于树结构缓存的推测性展开方法（SRT），其动机是在不牺牲分布正确性的前提下，加速语言模型的在线策略强化学习。该方法利用同一提示在不同训练步骤中展开的相似性，将先前生成的续写存储在每提示的树结构缓存中，作为生成时推测解码的草稿模型；通过在线展开和GPU空闲时的前瞻生成来更新缓存以保持其新鲜度。实验结果表明，SRT集成到PPO、GRPO和DAPO等标准强化学习流程后，能降低生成延迟和推理成本，在展开阶段实现最高2.08倍的时钟时间加速。</div>
</details>
</div>
<div class="card">
<div class="title">TranslateGemma Technical Report</div>
<div class="meta-line">Authors: Mara Finkelstein, Isaac Caswell, Tobias Domhan, Jan-Thorsten Peter, Juraj Juraska, Parker Riley, Daniel Deutsch, Cole Dilanni, Colin Cherry, Eleftheria Briakou, Elizabeth Nielsen, Jiaming Luo, Kat Black, Ryan Mullins, Sweta Agrawal, Wenda Xu, Erin Kats, Stephane Jaskiewicz, Markus Freitag, David Vilar</div>
<div class="meta-line">First: 2026-01-13T22:23:24+00:00 · Latest: 2026-01-13T22:23:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09012v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09012v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TranslateGemma技术报告</div>
<div class="mono" style="margin-top:8px">本文介绍了TranslateGemma——一套基于Gemma 3基础模型的开源机器翻译模型。为增强Gemma 3在翻译任务中的多语言能力，我们采用两阶段微调策略：首先使用通过前沿模型生成的大规模高质量合成平行数据与人工翻译平行数据混合进行监督微调；随后通过强化学习阶段，集成MetricX-QE和AutoMQM等奖励模型优化翻译质量。我们在WMT25测试集的10个语言对上进行了人工评估，并在WMT24++基准的55个语言对上进行了自动评估，验证了TranslateGemma的有效性。自动指标显示，所有规模的模型均较基线Gemma 3模型取得持续显著提升。值得注意的是，较小规模的TranslateGemma模型常能达到与较大基线模型相当的性能，且效率更优。实验还表明TranslateGemma模型保留了强大的多模态能力，在Vistra图像翻译基准上表现提升。开源TranslateGemma模型的发布旨在为研究社区提供强大且适应性强的机器翻译工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated to enhance the inherent multilingual capabilities of the Gemma 3 foundation models for machine translation, this work introduces TranslateGemma, a suite of open translation models. The method employs a two-stage fine-tuning process: first, supervised fine-tuning on a mixture of high-quality synthetic and human-translated parallel data, followed by reinforcement learning optimized with an ensemble of reward models like MetricX-QE and AutoMQM. Experimental results from human evaluation on the WMT25 test set (10 language pairs) and automatic evaluation on WMT24++ (55 pairs) show consistent and substantial gains over baseline Gemma 3 models across all sizes, with smaller models often matching the performance of larger baselines for improved efficiency, while also retaining and enhancing multimodal capabilities on the Vistra image translation benchmark.</div>
<div class="mono" style="margin-top:8px">本研究旨在增强Gemma 3基础模型固有的多语言能力以用于机器翻译任务，由此提出了TranslateGemma这一套开源翻译模型。其方法采用两阶段微调流程：首先使用高质量的大规模合成并行数据与人工翻译并行数据进行混合监督微调，随后通过集成MetricX-QE和AutoMQM等奖励模型进行强化学习以优化翻译质量。在WMT25测试集（10个语言对）上的人工评估和在WMT24++基准（55个语言对）上的自动评估结果表明，相比基线Gemma 3模型，所有规模的TranslateGemma模型均取得了持续且显著的性能提升，其中较小模型常能达到与较大基线模型相当的性能，从而提高了效率；同时，模型在Vistra图像翻译基准上保持了强大的多模态能力并有所增强。</div>
</details>
</div>
<div class="card">
<div class="title">Machine Learning-Driven Creep Law Discovery Across Alloy Compositional Space</div>
<div class="meta-line">Authors: Hongshun Chen, Ryan Zhou, Rujing Zha, Zihan Chen, Wenpan Li, Rowan Rolark, John Patrick Reidy, Jian Cao, Ping Guo, David C. Dunand, Horacio D. Espinosa</div>
<div class="meta-line">First: 2026-01-13T20:29:15+00:00 · Latest: 2026-01-13T20:29:15+00:00</div>
<div class="meta-line">Comments: 27 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08970v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08970v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hihg-temperature creep characterization of structural alloys traditionally relies on serial uniaxial tests, which are highly inefficient for exploring the large search space of alloy compositions and for material discovery. Here, we introduce a machine-learning-assisted, high-throughput framework for creep law identification based on a dimple array bulge instrument (DABI) configuration, which enables parallel creep testing of 25 dimples, each fabricated from a different alloy, in a single experiment. Full-field surface displacements of dimples undergoing time-dependent creep-induced bulging under inert gas pressure are measured by 3D digital image correlation. We train a recurrent neural network (RNN) as a surrogate model, mapping creep parameters and loading conditions to the time-dependent deformation response of DABI. Coupling this surrogate with a particle swarm optimization scheme enables rapid and global inverse identification with sparsity regularization of creep parameters from experiment displacement-time histories. In addition, we propose a phenomenological creep law with a time-dependent stress exponent that captures the sigmoidal primary creep observed in wrought INCONEL 625 and extracts its temperature dependence from DABI test at multiple temperatures. Furthermore, we employ a general creep law combining several conventional forms together with regularized inversion to identify the creep laws for 47 additional Fe-, Ni-, and Co-rich alloys and to automatically select the dominant functional form for each alloy. This workflow combined with DABI experiment provides a quantitative, high-throughput creep characterization platform that is compatible with data mining, composition-property modeling, and nonlinear structural optimization with creep behavior across a large alloy design space.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器学习驱动的合金成分空间蠕变规律发现</div>
<div class="mono" style="margin-top:8px">传统结构合金的高温蠕变表征依赖串行单轴测试，在探索合金成分的大规模搜索空间和材料发现方面效率低下。本文提出一种基于凹坑阵列鼓胀仪（DABI）配置的机器学习辅助高通量蠕变规律识别框架，可在单次实验中并行测试25个不同合金制备的凹坑。通过三维数字图像相关技术，测量惰性气体压力下随时间发生蠕变鼓胀的凹坑全场表面位移。我们训练循环神经网络（RNN）作为代理模型，将蠕变参数和加载条件映射至DABI的时变变形响应。该代理模型与粒子群优化算法结合，可通过实验位移-时间历程实现带稀疏正则化的蠕变参数快速全局反演识别。此外，我们提出具有时变应力指数的唯象蠕变定律，用于描述锻造INCONEL 625中观测到的S型初始蠕变，并通过多温度DABI测试提取其温度依赖性。进一步地，我们采用融合多种传统形式的广义蠕变定律结合正则化反演，识别了47种富铁、镍、钴合金的蠕变规律，并自动为每种合金选择主导函数形式。该工作流与DABI实验相结合，构建了定量化高通量蠕变表征平台，兼容数据挖掘、成分-性能建模以及大范围合金设计空间中考虑蠕变行为的非线性结构优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of traditional serial creep tests for exploring vast alloy compositional spaces, this paper introduces a machine-learning-assisted, high-throughput framework using a dimple array bulge instrument (DABI) to enable parallel creep testing of 25 different alloys simultaneously. The method employs 3D digital image correlation to measure full-field surface displacements and trains a recurrent neural network as a surrogate model, coupled with particle swarm optimization for inverse identification of creep parameters with sparsity regularization. Key experimental results include the derivation of a phenomenological creep law with a time-dependent stress exponent for INCONEL 625, capturing its sigmoidal primary creep and temperature dependence, and the successful identification of creep laws for 47 additional Fe-, Ni-, and Co-rich alloys, automatically selecting dominant functional forms for each, thereby establishing a quantitative platform for high-throughput creep characterization across large alloy design spaces.</div>
<div class="mono" style="margin-top:8px">针对传统串行蠕变测试在探索广阔合金成分空间时效率低下的问题，本文提出了一种机器学习辅助的高通量框架，采用凹坑阵列鼓胀仪器（DABI）配置，可在单次实验中并行测试25种不同合金的蠕变行为。该方法利用三维数字图像相关技术测量全场表面位移，训练循环神经网络作为代理模型，并结合粒子群优化算法进行稀疏正则化的蠕变参数全局反演识别。主要实验结果包括：针对锻造INCONEL 625推导了具有时间依赖性应力指数的唯象蠕变定律，捕捉了其S形初始蠕变和温度依赖性；并成功识别了47种富铁、镍和钴合金的蠕变定律，自动为每种合金选择主导函数形式，从而建立了一个适用于大范围合金设计空间、兼容数据挖掘和性能建模的定量高通量蠕变表征平台。</div>
</details>
</div>
<div class="card">
<div class="title">Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation</div>
<div class="meta-line">Authors: Abdolazim Rezaei, Mehdi Sookhak, Ahmad Patooghy, Shahab S. Band, Amir Mosavi</div>
<div class="meta-line">First: 2025-06-18T20:02:24+00:00 · Latest: 2026-01-13T20:26:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.15854v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.15854v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Intelligent Transportation Systems (ITS) rely on a variety of devices that frequently process privacy-sensitive data. Roadside units are important because they use AI-equipped cameras to detect traffic violations in Connected and Autonomous Vehicles (CAV). However, although the interior of a vehicle is generally considered a private space, the privacy risks associated with captured imagery remain a major concern, as such data can be misused for identity theft, profiling, or unauthorized commercial purposes. Methods like face blurring reduce privacy risks, however individuals&#x27; privacy can still be compromised. This paper introduces a novel privacy-preserving framework that leverages feedback-based reinforcement learning (RL) and vision-language models (VLMs) to protect sensitive visual information captured by AIE cameras. The proposed idea transforms images into textual descriptions using an innovative method while the main scene details are preserved and protects privacy. A hierarchical RL strategy is employed to iteratively refine the generated text, enhancing both semantic accuracy and privacy. Unlike prior captioning-based methods, our model incorporates an iterative reinforcement-learning cycle with external knowledge feedback which progressively refines privacy-aware text. In addition to qualitative textual metric evaluations, the privacy-based metrics demonstrate significant improvements in privacy preservation where SSIM, PSNR, MSE, and SRRA values obtained using the proposed method on two different datasets outperform other methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉到文本转换的网联自动驾驶车辆隐私保护</div>
<div class="mono" style="margin-top:8px">智能交通系统依赖各类设备处理隐私敏感数据。路侧单元通过搭载人工智能的摄像头检测网联自动驾驶车辆的交通违规行为，但车辆内部作为私人空间，其图像数据存在被滥用于身份盗窃、用户画像或商业用途的风险。传统人脸模糊等方法仍存在隐私泄露隐患。本文提出一种融合反馈式强化学习与视觉语言模型的新型隐私保护框架，通过创新方法将图像转换为保留主要场景细节的文本描述。采用分层强化学习策略迭代优化生成文本，提升语义准确性与隐私保护效果。相较于现有基于描述的方法，本模型引入结合外部知识反馈的迭代强化学习循环，逐步生成隐私感知文本。在定性文本指标评估基础上，隐私指标显示该方法在两个数据集上的SSIM、PSNR、MSE和SRRA值均优于其他方法，显著提升了隐私保护性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the privacy risks in Intelligent Transportation Systems where AI-equipped cameras capture sensitive visual data from vehicle interiors, this paper proposes a novel privacy-preserving framework. The method employs a feedback-based reinforcement learning strategy combined with vision-language models to transform captured images into textual descriptions, iteratively refining the text to preserve scene semantics while removing private details. Experimental results on two datasets demonstrate significant improvements in privacy metrics, including SSIM, PSNR, MSE, and SRRA, outperforming prior methods like face blurring and basic captioning approaches.</div>
<div class="mono" style="margin-top:8px">本文针对智能交通系统中配备人工智能的摄像头捕获车辆内部敏感视觉数据所带来的隐私风险，提出了一种新颖的隐私保护框架。该方法采用基于反馈的强化学习策略结合视觉语言模型，将捕获的图像转换为文本描述，并通过迭代优化在保留场景语义的同时移除隐私细节。在两个数据集上的实验结果表明，该方法在SSIM、PSNR、MSE和SRRA等隐私度量指标上显著优于人脸模糊和传统描述生成等方法，有效提升了隐私保护效果。</div>
</details>
</div>
<div class="card">
<div class="title">Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge</div>
<div class="meta-line">Authors: Yao Tang, Li Dong, Yaru Hao, Qingxiu Dong, Furu Wei, Jiatao Gu</div>
<div class="meta-line">First: 2026-01-13T18:48:00+00:00 · Latest: 2026-01-13T18:48:00+00:00</div>
<div class="meta-line">Comments: 21 pages. Code available at https://github.com/GMLR-Penn/Multiplex-Thinking</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08808v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08808v1">PDF</a> · <a href="https://github.com/GMLR-Penn/Multiplex-Thinking">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多路思维：基于词元级分支与合并的推理方法</div>
<div class="mono" style="margin-top:8px">大型语言模型通常通过思维链（CoT）更有效地解决复杂推理任务，但代价是生成长而低带宽的词元序列。相比之下，人类常通过保持对可能后续步骤的概率分布进行软推理。受此启发，我们提出多路思维，一种随机软推理机制：在每个思考步骤中，采样K个候选词元，并将其嵌入聚合为单个连续的多路词元。这既保留了词汇嵌入先验和标准离散生成的采样动态，又能在多路展开上导出可处理的概率分布。因此，多路轨迹可直接通过同策略强化学习（RL）进行优化。重要的是，多路思维具有自适应性：当模型置信度高时，多路词元近乎离散，行为类似标准CoT；当模型不确定时，它能紧凑表示多个可能的后续步骤，且不增加序列长度。在多项高难度数学推理基准测试中，多路思维从Pass@1到Pass@1024均持续优于强离散CoT和RL基线方法，同时生成更短的序列。代码与模型检查点发布于https://github.com/GMLR-Penn/Multiplex-Thinking。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that human reasoning often maintains a distribution over plausible next steps, unlike the long, low-bandwidth token sequences of Chain-of-Thought (CoT) in large language models, this paper introduces Multiplex Thinking, a stochastic soft reasoning mechanism. The method samples K candidate tokens at each thinking step and aggregates their embeddings into a single continuous multiplex token, preserving vocabulary embedding priors and sampling dynamics while enabling tractable probability distributions over rollouts, which allows direct optimization via on-policy reinforcement learning. Experimental results on challenging math reasoning benchmarks show that Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines across metrics from Pass@1 to Pass@1024 while generating shorter sequences.</div>
<div class="mono" style="margin-top:8px">受人类推理常保持对可能后续步骤的分布这一观察启发，本文针对大语言模型中思维链方法产生的长序列、低带宽问题，提出了多重思维，一种随机软推理机制。该方法在每个思维步骤采样K个候选标记，并将其嵌入聚合为单个连续的多重标记，既保留了词汇嵌入先验和采样动态，又实现了对展开轨迹的可处理概率分布，从而可通过策略强化学习直接优化。在具有挑战性的数学推理基准测试中，多重思维在Pass@1到Pass@1024的各项指标上均持续优于强离散思维链和强化学习基线，同时生成更短的序列。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yucheng Wang, Yufei He, Jiaying Wu, Yilun Zhao, See-Kiong Ng, Cynthia Breazeal, Anh Tuan Luu, Hae Won Park, Bryan Hooi</div>
<div class="meta-line">First: 2026-01-13T17:48:43+00:00 · Latest: 2026-01-13T17:48:43+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08763v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08763v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励罕见策略：面向大语言模型创造性问题求解的独特性感知强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为大语言模型后训练的核心范式，尤其在复杂推理任务中，但其常面临探索坍缩问题：策略过早集中于少数主导推理模式，虽提升pass@1指标，却限制了轨迹级多样性及pass@k增益。本文指出该问题源于对局部词元行为的正则化而非解决方案集的多样性调控。为此，我们提出独特性感知强化学习——一种轨迹级优化目标，显式奖励采用罕见高层策略的正确解法。该方法基于大语言模型的评判器，将同一问题的求解轨迹按高层策略聚类（忽略表面差异），并依据聚类规模对策略优势进行逆向加权。由此，正确且新颖的策略将比冗余策略获得更高奖励。在数学、物理和医学推理基准测试中，本方法在大规模采样预算下持续提升pass@$k$指标，在保持pass@1水平的同时显著提高pass@$k$曲线下面积（AUC@$K$），并能维持探索性，大规模发掘更多样化的求解策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work is to address exploration collapse in reinforcement learning (RL) for large language models (LLMs), where policies prematurely converge on a narrow set of reasoning patterns, limiting solution diversity and gains in pass@k metrics. The method introduces Uniqueness-Aware Reinforcement Learning, which rewards correct solutions that employ rare high-level strategies by using an LLM-based judge to cluster rollouts by strategy and reweight policy advantages inversely with cluster size. Experimental results across mathematics, physics, and medical reasoning benchmarks show that this approach consistently improves pass@k and the area under the pass@k curve without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大型语言模型强化学习中的探索崩溃问题，即策略过早集中于少数主导推理模式，限制了解决方案多样性和pass@k指标的提升。方法上提出了独特性感知强化学习，通过基于LLM的评判器将相同问题的解决方案按高层策略聚类，并依据聚类大小反向调整策略优势，从而奖励使用罕见策略的正确解。实验结果表明，在数学、物理和医学推理基准测试中，该方法在不牺牲pass@1的前提下，持续提升了pass@k和pass@k曲线下面积，同时保持了探索性并大规模发现了更多样化的解决策略。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis</div>
<div class="meta-line">Authors: Sara Giordano, Kornikar Sen, Miguel A. Martin-Delgado</div>
<div class="meta-line">First: 2025-07-22T14:39:20+00:00 · Latest: 2026-01-13T17:34:14+00:00</div>
<div class="meta-line">Comments: 35 pages, 7 figures, color figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.16641v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.16641v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A reinforcement learning (RL) framework is introduced for the efficient synthesis of quantum circuits that generate specified target quantum states from a fixed initial state, addressing a central challenge in both the Noisy Intermediate-Scale Quantum (NISQ) era and future fault-tolerant quantum computing. The approach utilizes tabular Q-learning, based on action sequences, within a discretized quantum state space, to effectively manage the exponential growth of the space dimension.The framework introduces a hybrid reward mechanism, combining a static, domain-informed reward that guides the agent toward the target state with customizable dynamic penalties that discourage inefficient circuit structures such as gate congestion and redundant state revisits. This is a circuit-aware reward, in contrast to the current trend of works on this topic, which are primarily fidelity-based. By leveraging sparse matrix representations and state-space discretization, the method enables practical navigation of high-dimensional environments while minimizing computational overhead. Benchmarking on graph-state preparation tasks for up to seven qubits, we demonstrate that the algorithm consistently discovers minimal-depth circuits with optimized gate counts. Moreover, extending the framework to a universal gate set still yields low depth circuits, highlighting the algorithm robustness and adaptability. The results confirm that this RL-driven approach, with our completely circuit-aware method, efficiently explores the complex quantum state space and synthesizes near-optimal quantum circuits, providing a resource-efficient foundation for quantum circuit optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合奖励驱动的强化学习在高效量子电路合成中的应用</div>
<div class="mono" style="margin-top:8px">本文提出一种强化学习框架，用于高效合成从固定初始态生成指定目标量子态的量子电路，以应对含噪声中等规模量子时代及未来容错量子计算的核心挑战。该方法在离散化量子态空间中，基于动作序列采用表格型Q学习，有效应对空间维度的指数增长。框架引入混合奖励机制：结合静态的领域知识奖励引导智能体趋向目标态，以及可定制的动态惩罚机制抑制低效电路结构（如门拥堵和冗余态重访）。这是一种电路感知的奖励机制，区别于当前该领域主要基于保真度的研究趋势。通过利用稀疏矩阵表示和态空间离散化，该方法能在最小化计算开销的同时，实现对高维环境的实际导航。在最多七量子比特的图态制备任务上进行基准测试表明，该算法能持续发现具有优化门数量的最小深度电路。此外，将框架扩展至通用门集仍能获得低深度电路，突显了算法的鲁棒性和适应性。结果证实，这种结合完全电路感知方法的强化学习驱动方案，能高效探索复杂量子态空间并合成近似最优量子电路，为量子电路优化提供了资源高效的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient quantum circuit synthesis in both NISQ and fault-tolerant quantum computing, this paper introduces a reinforcement learning framework using tabular Q-learning in a discretized quantum state space to manage exponential dimensionality. The method employs a hybrid reward mechanism combining a static, domain-informed reward to guide toward target states with dynamic penalties to discourage inefficient circuit structures like gate congestion and redundant revisits, distinguishing it from fidelity-based approaches. Experimental results on graph-state preparation for up to seven qubits show the algorithm consistently discovers minimal-depth circuits with optimized gate counts, and extension to a universal gate set maintains low-depth performance, confirming its robustness and resource efficiency for near-optimal circuit synthesis.</div>
<div class="mono" style="margin-top:8px">本文针对嘈杂中型量子（NISQ）时代和未来容错量子计算中高效合成量子电路的核心挑战，提出了一种强化学习框架，在离散化量子态空间中使用基于动作序列的表格Q学习来管理空间维度的指数增长。该方法采用混合奖励机制，将静态的领域知识奖励与可定制的动态惩罚相结合，以引导智能体朝向目标态并抑制低效电路结构（如门拥堵和状态重复访问），区别于当前主流的保真度驱动方法。在多达七个量子比特的图态制备任务上的实验结果表明，该算法能一致地发现具有优化门数的最小深度电路，且扩展到通用门集后仍能保持低深度性能，验证了其鲁棒性和适应性，为量子电路优化提供了资源高效的基础。</div>
</details>
</div>
<div class="card">
<div class="title">TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback</div>
<div class="meta-line">Authors: Prithwish Jana, Sam Davidson, Bhavana Bhasker, Andrey Kan, Anoop Deoras, Laurent Callot</div>
<div class="meta-line">First: 2026-01-13T17:08:30+00:00 · Latest: 2026-01-13T17:08:30+00:00</div>
<div class="meta-line">Comments: The paper has been published at the 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE 2026), Rio de Janeiro, Brazil, April 12-18, 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08734v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08734v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TerraFormer：基于策略引导验证器反馈微调大语言模型的自动化基础设施即代码生成框架</div>
<div class="mono" style="margin-top:8px">自动化基础设施即代码（IaC）具有挑战性，大语言模型（LLM）常从自然语言生成错误配置。本文提出TerraFormer——一种结合监督微调与验证器引导强化学习的神经符号框架，通过形式化验证工具提供语法、可部署性及策略合规性反馈。我们通过多阶段验证与迭代式LLM自校正，构建了两个高质量自然语言转IaC数据集：TF-Gen（15.2万实例）和TF-Mutn（5.2万实例）。在包含Sonnet 3.7、DeepSeek-R1、GPT-4.1等约50倍参数量模型的17个前沿LLM对比评估中，TerraFormer在IaC-Eval、TF-Gen（测试集）、TF-Mutn（测试集）上分别将基础LLM正确率提升15.94%、11.65%、19.60%，在TF-Gen与TF-Mutn测试集上超越更大规模模型，于IaC-Eval排名第三，并实现最佳实践与安全合规性领先。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of automating Infrastructure-as-Code (IaC) generation from natural language, where large language models (LLMs) often produce incorrect configurations. It introduces TerraFormer, a neuro-symbolic framework that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. Experimental results show that TerraFormer improves correctness over its base LLM by up to 19.60% on curated datasets, outperforms many larger state-of-the-art models, and achieves high best-practices and security compliance.</div>
<div class="mono" style="margin-top:8px">该论文针对从自然语言自动生成基础设施即代码（IaC）的挑战，其中大型语言模型（LLM）常产生错误配置。它提出了TerraFormer，一个神经符号框架，结合了监督微调和验证器引导的强化学习，使用形式化验证工具提供语法、可部署性和策略合规性反馈。实验结果表明，TerraFormer在其基础LLM上提高了高达19.60%的正确性，优于许多更大的先进模型，并实现了最佳实践和高安全合规性。</div>
</details>
</div>
<div class="card">
<div class="title">Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts</div>
<div class="meta-line">Authors: Bert Verbruggen, Arne Vanhoyweghen, Vincent Ginis</div>
<div class="meta-line">First: 2026-01-13T16:53:40+00:00 · Latest: 2026-01-13T16:53:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08726v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08726v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality&#x27;&#x27; depends on the environment&#x27;s statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network&#x27;s function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process&#x27;s intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent&#x27;s exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非遍历情境下深度强化学习的模型无关解决方案</div>
<div class="mono" style="margin-top:8px">强化学习（RL）仍是机器学习的核心优化框架。尽管RL智能体能够收敛至最优解，但“最优性”的定义取决于环境的统计特性。作为大多数RL算法核心的贝尔曼方程，其构建基于未来奖励的期望值。然而，当遍历性被打破时，长期结果取决于特定轨迹而非总体均值。在此类情境下，总体均值与个体智能体经历的时间平均增长相偏离，导致期望值公式产生系统性次优策略。先前研究表明，传统RL架构在非遍历环境中无法恢复真正的最优解。我们将此分析延伸至深度RL实现，并证明其在非遍历动态下同样产生次优策略。在学习过程中引入显式时间依赖性可纠正这一局限：通过使网络函数逼近器融合时序信息，智能体能够估算与过程内在增长率一致的价值函数。这一改进无需改变环境反馈（如奖励转换或修正目标函数），而是源自智能体对时序轨迹的自然感知。我们的研究成果为非遍历系统强化学习方法的研究体系提供了新的贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the failure of traditional and deep reinforcement learning (RL) methods to achieve optimal policies in non-ergodic environments, where ensemble averages diverge from time-averaged growth, making expected-value formulations like the Bellman equation systematically suboptimal. The proposed method introduces explicit time dependence into the learning process by allowing the neural network&#x27;s function approximation to incorporate temporal information, enabling the agent to estimate value functions aligned with the process&#x27;s intrinsic growth rate without altering environmental feedback such as rewards or objectives. Experimental results demonstrate that standard deep RL implementations produce suboptimal policies under non-ergodic dynamics, while the time-dependent approach corrects this limitation, recovering policies consistent with the true optimum.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于传统和深度强化学习方法在非遍历环境中无法获得最优策略，因为其中系综平均与时间平均增长发生偏离，导致基于期望值的贝尔曼方程等方法产生系统性次优策略。所提出的方法通过让神经网络的函数近似纳入时间信息，将显式时间依赖性引入学习过程，使智能体能够估计与过程内在增长率一致的价值函数，而无需改变环境反馈如奖励或目标函数。实验结果表明，标准深度强化学习实现在非遍历动态下会产生次优策略，而时间依赖方法纠正了这一缺陷，恢复了与真实最优一致的政策。</div>
</details>
</div>
<div class="card">
<div class="title">Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following</div>
<div class="meta-line">Authors: Yirong Zeng, Yufei Liu, Xiao Ding, Yutai Hou, Yuxian Wang, Haonan Song, Wu Ning, Dandan Tu, Qixun Zhang, Bibo Cai, Yuxiang He, Ting Liu</div>
<div class="meta-line">First: 2026-01-08T14:00:51+00:00 · Latest: 2026-01-13T16:42:42+00:00</div>
<div class="meta-line">Comments: Under review, 13 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\% in performance while achieving a 58\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>精度优于多样性：高精度奖励泛化至稳健指令跟随</div>
<div class="mono" style="margin-top:8px">在指令跟随任务中，一个普遍观点认为：可验证的硬约束与不可验证的软约束的多样性混合，对于泛化至未见指令至关重要。本研究通过系统性实证检验挑战了这一共识。反直觉的是，我们发现仅使用硬约束训练的模型始终优于混合数据集训练的模型。大量实验表明，奖励精度（而非约束多样性）是实现有效对齐的主要驱动力。大语言模型评判器在检测错误响应时召回率较低，导致严重的奖励破解现象，从而削弱了多样性的优势。注意力机制分析进一步揭示，高精度奖励能发展出可迁移的指令跟随元技能。基于这些发现，我们提出一种以数据为中心的精炼策略，优先保障奖励精度。在五个基准测试中，该方法以13.4%的性能优势超越基线模型，训练时间减少58%，且在指令跟随之外保持强泛化能力。本研究主张范式转变：从盲目追求数据多样性转向聚焦高精度奖励。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the prevailing belief that diverse mixtures of verifiable and unverifiable constraints are essential for generalizing instruction-following models, finding instead that high-precision rewards are the key driver. The method involves systematic empirical comparisons showing models trained solely on hard constraints outperform those on mixed datasets, as low recall in LLM judges for detecting false responses leads to reward hacking that undermines diversity benefits. Experimental results across five benchmarks demonstrate that a proposed data-centric refinement strategy prioritizing reward precision outperforms baselines by 13.4% in performance while reducing training time by 58%, with analysis revealing these rewards develop transferable meta-skills for robust generalization.</div>
<div class="mono" style="margin-top:8px">本文挑战了当前主流观点，即指令跟随模型的泛化需要可验证与不可验证约束的多样混合，发现高精度奖励才是关键驱动因素。研究方法通过系统实证比较表明，仅使用硬约束训练的模型优于混合数据集训练的模型，因为大语言模型法官在检测错误响应时召回率低，导致奖励黑客攻击，从而削弱了多样性的益处。在五个基准测试上的实验结果表明，所提出的以数据为中心、优先考虑奖励精度的优化策略，其性能比基线高出13.4%，同时训练时间减少58%，分析还揭示这些奖励能发展出可迁移的元技能，实现稳健泛化。</div>
</details>
</div>
<div class="card">
<div class="title">The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection</div>
<div class="meta-line">Authors: Md Shafiqul Islam, Shakti Prasad Padhy, Douglas Allaire, Raymundo Arróyave</div>
<div class="meta-line">First: 2026-01-08T20:52:53+00:00 · Latest: 2026-01-13T16:22:49+00:00</div>
<div class="meta-line">Comments: Included a subsection named &quot;Budgetary impact of inline kernel optimization during BO&quot;, and corrected label of a figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05371v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05371v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>核流形：高斯过程模型选择的几何方法</div>
<div class="mono" style="margin-top:8px">高斯过程回归是一种强大的非参数贝叶斯框架，但其性能关键取决于协方差核的选择。选择合适的核函数是模型质量的核心，但仍是概率建模中最具挑战性且计算成本最高的步骤之一。我们提出一种基于核-核几何结构的贝叶斯优化框架，利用高斯过程先验间的期望散度距离高效探索核空间。通过该距离矩阵的多维尺度分析嵌入，将离散核函数库映射至连续欧几里得流形，实现平滑的贝叶斯优化。在此框架中，输入空间由核组合构成，目标函数为对数边际似然，特征化通过多维尺度分析坐标实现。当散度构成有效度量时，该嵌入能保持几何结构并产生稳定的贝叶斯优化景观。我们在合成基准测试、真实世界时间序列数据集以及预测熔池几何形状的增材制造案例研究中验证了该方法，相较于大型语言模型引导搜索等基线方法，取得了更优的预测精度与不确定性校准效果。该框架为核搜索建立了可复用的概率几何体系，对高斯过程建模与深度核学习具有直接应用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the critical challenge of kernel selection in Gaussian Process regression, which heavily influences model performance but is computationally demanding. The authors propose a Bayesian optimization framework that leverages a geometric approach, using divergence-based distances between GP priors to structure kernel space and embedding a discrete kernel library into a continuous Euclidean manifold via multidimensional scaling. This enables smooth Bayesian optimization over kernel compositions by treating MDS coordinates as features and log marginal likelihood as the objective. Experimental results on synthetic benchmarks, real-world time-series data, and an additive manufacturing case study demonstrate that the method achieves superior predictive accuracy and uncertainty calibration compared to baselines, including LLM-guided search, establishing a reusable probabilistic geometry for kernel selection.</div>
<div class="mono" style="margin-top:8px">本文针对高斯过程回归中核函数选择这一关键挑战展开研究，该选择对模型性能影响重大但计算成本高昂。作者提出了一种基于几何方法的贝叶斯优化框架，利用高斯过程先验之间的散度距离构建核空间，并通过多维缩放将离散核函数库嵌入连续欧几里得流形。该方法将多维缩放坐标作为特征、对数边际似然作为目标，实现了对核函数组合的平滑贝叶斯优化。在合成基准测试、真实世界时间序列数据和增材制造案例研究上的实验结果表明，相较于包括大语言模型引导搜索在内的基线方法，该方法在预测精度和不确定性校准方面表现更优，为核函数搜索建立了可复用的概率几何框架。</div>
</details>
</div>
<div class="card">
<div class="title">PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning</div>
<div class="meta-line">Authors: Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei</div>
<div class="meta-line">First: 2026-01-13T16:02:35+00:00 · Latest: 2026-01-13T16:02:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08679v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08679v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PersonaDual：通过自适应推理平衡个性化与客观性</div>
<div class="mono" style="margin-top:8px">随着用户日益期望大语言模型（LLM）与其偏好保持一致，个性化信息变得愈发重要。然而，个性化信息是一把双刃剑：它虽能提升交互体验，却可能损害客观性与事实准确性，尤其当其与问题不匹配时。为缓解此问题，我们提出PersonaDual框架，该框架在单一模型中同时支持通用客观推理与个性化推理，并能根据上下文自适应切换模式。PersonaDual首先通过监督微调（SFT）学习两种推理模式，再借助我们提出的DualGRPO强化学习进行优化以改进模式选择。在客观性与个性化基准测试上的实验表明，PersonaDual在保留个性化优势的同时减少了干扰，实现了近乎无干扰的性能，并能更有效地利用有益的个性化信号来提升客观问题解决能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of balancing personalization with objectivity in LLMs, as user-aligned responses can enhance interaction but risk factual compromise when preferences conflict with queries. To mitigate this, the authors propose PersonaDual, a framework that integrates both general objective and personalized reasoning within a single model, using adaptive mode switching based on context; it is trained via supervised fine-tuning to learn dual reasoning patterns and further optimized with a novel reinforcement learning method called DualGRPO to refine mode selection. Experimental results on objective and personalized benchmarks demonstrate that PersonaDual nearly eliminates interference from unhelpful personalization while effectively leveraging beneficial personalized signals to enhance objective problem-solving, thus preserving personalization benefits without sacrificing factual correctness.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型中个性化与客观性之间的平衡问题展开研究，因为适应用户偏好的回答虽能提升交互体验，但当偏好与查询冲突时可能损害事实准确性。为此，作者提出了PersonaDual框架，该框架在单一模型中整合了通用客观推理和个性化推理，并根据上下文自适应切换模式；其训练首先通过监督微调学习两种推理模式，然后采用新提出的强化学习方法DualGRPO进行优化以改进模式选择。在客观和个性化基准测试上的实验结果表明，PersonaDual几乎消除了有害个性化信息的干扰，同时有效利用有益的个性化信号来提升客观问题解决能力，从而在保持个性化优势的同时不牺牲事实正确性。</div>
</details>
</div>
<div class="card">
<div class="title">From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner&#x27;s Tutorial</div>
<div class="meta-line">Authors: Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar</div>
<div class="meta-line">First: 2026-01-13T15:40:55+00:00 · Latest: 2026-01-13T15:40:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08662v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从经典到量子强化学习及其在量子控制中的应用：初学者教程</div>
<div class="mono" style="margin-top:8px">本教程旨在通过清晰、示例驱动的讲解，使本科生更易理解强化学习（RL）。重点弥合RL理论与实际编程应用之间的差距，解决学生从概念理解转向实践时面临的常见挑战。通过动手示例和易于理解的解释，教程致力于帮助学生掌握在现实场景中自信应用RL技术所需的基础技能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This tutorial is motivated by the need to make reinforcement learning (RL) more accessible to undergraduate students, bridging the gap between theoretical concepts and practical implementation. The method involves providing clear, example-driven explanations and hands-on coding examples to demystify common challenges in applying RL. The main outcome is an educational resource that equips students with foundational skills to confidently use RL techniques in real-world scenarios, emphasizing practical application over abstract theory.</div>
<div class="mono" style="margin-top:8px">本教程旨在降低本科生学习强化学习（RL）的门槛，以弥合理论概念与实际应用之间的差距。其方法是通过清晰、示例驱动的解释和动手编码示例，解决学生在从概念理解转向实现时面临的常见挑战。主要成果是提供了一个教育资源，帮助学生掌握在现实场景中自信应用RL技术的基础技能，强调实践应用而非抽象理论。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Safe Reinforcement Learning using Entropy Regularizer</div>
<div class="meta-line">Authors: Abhijit Mazumdar, Rafal Wisniewski, Manuela L. Bujorianu</div>
<div class="meta-line">First: 2026-01-13T15:23:19+00:00 · Latest: 2026-01-13T15:23:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08646v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08646v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于熵正则化的可证明安全强化学习</div>
<div class="mono" style="margin-top:8px">我们研究了在安全约束下学习马尔可夫决策过程最优策略的问题，并将其建模为可达-规避框架。目标是设计在线强化学习算法，确保在学习阶段以任意高概率满足安全约束。为此，我们首先提出了一种基于不确定性乐观原则的算法。在此基础上，我们提出了利用熵正则化的核心算法。我们对两种算法进行了有限样本分析，并推导了其遗憾界。结果表明，熵正则化的引入改善了遗憾界，并显著控制了基于不确定性乐观原则的安全强化学习算法固有的回合间波动性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of ensuring safety during online reinforcement learning in Markov decision processes with reach-avoid constraints, aiming to guarantee constraint satisfaction with high probability throughout learning. The method introduces two algorithms: an initial one based on the optimism in the face of uncertainty (OFU) principle, followed by a main algorithm that incorporates entropy regularization to enhance performance. Experimental analysis shows that the entropy-regularized algorithm not only improves regret bounds but also significantly reduces the high episode-to-episode variability typically associated with OFU-based safe RL approaches.</div>
<div class="mono" style="margin-top:8px">本文针对具有安全约束的马尔可夫决策过程，旨在在线强化学习过程中以高概率确保安全性，特别是在到达-规避设定下。研究方法提出了两种算法：首先基于不确定性乐观原则设计基础算法，随后引入熵正则化作为主要算法以优化性能。实验结果表明，熵正则化算法的加入不仅改善了遗憾界，还显著控制了基于不确定性乐观原则的安全强化学习算法中固有的回合间波动性。</div>
</details>
</div>
<div class="card">
<div class="title">TROJail: Trajectory-Level Optimization for Multi-Turn Large Language Model Jailbreaks with Process Rewards</div>
<div class="meta-line">Authors: Xiqiao Xiong, Ouxiang Li, Zhuo Liu, Moxin Li, Wentao Shi, Fengbin Zhu, Qifan Wang, Fuli Feng</div>
<div class="meta-line">First: 2025-12-08T17:42:59+00:00 · Latest: 2026-01-13T15:14:32+00:00</div>
<div class="meta-line">Comments: 21 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07761v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07761v2">PDF</a> · <a href="https://github.com/xxiqiao/TROJail">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have seen widespread adoption, yet they remain vulnerable to multi-turn jailbreak attacks, threatening their safe deployment. This has led to the task of training automated multi-turn attackers to probe model safety vulnerabilities. However, existing approaches typically rely on turn-level optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate this task as a multi-turn reinforcement learning problem, directly optimizing the harmfulness of the final-turn response as the outcome reward. To address the sparse supervision of the outcome reward, we introduce TROJail, which employs two process rewards to evaluate the utility of intermediate prompts and integrate them into advantage estimation. These rewards (1) penalize overly harmful prompts that trigger the model&#x27;s refusal mechanism, and (2) encourage steering the semantic relevance of responses toward the targeted harmful content. Experimental results show improved attack success rates across multiple models and benchmarks, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/TROJail. Warning: This paper contains examples of harmful content.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TROJail：基于过程奖励的多轮大语言模型越狱轨迹级优化方法</div>
<div class="mono" style="margin-top:8px">大语言模型已获广泛应用，但仍易受多轮越狱攻击威胁，影响其安全部署。这催生了训练自动化多轮攻击器以探测模型安全漏洞的需求。现有方法通常依赖轮次级优化，难以学习长期攻击策略。为此，我们将该任务构建为多轮强化学习问题，直接以最终轮次回答的危害性作为结果奖励进行优化。针对结果奖励监督稀疏的问题，我们提出TROJail方法，通过两个过程奖励评估中间提示的效用并整合至优势估计：其一对触发模型拒绝机制的过度有害提示进行惩罚，其二引导回答的语义相关性朝向目标有害内容。实验表明，该方法在多个模型和基准测试中显著提升了攻击成功率。代码发布于https://github.com/xxiqiao/TROJail。注：本文包含有害内容示例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of large language models to multi-turn jailbreak attacks, motivating the development of an automated attacker to probe safety weaknesses. The method formulates the task as a multi-turn reinforcement learning problem, introducing TROJail to optimize trajectory-level outcomes with process rewards that penalize overly harmful prompts and steer semantic relevance toward targeted harmful content. Experimental results demonstrate improved attack success rates across multiple models and benchmarks, validating the effectiveness of the approach.</div>
<div class="mono" style="margin-top:8px">本文针对大语言模型在多轮越狱攻击中的脆弱性，旨在开发自动化攻击器以探测安全漏洞。方法将任务构建为多轮强化学习问题，提出TROJail，通过过程奖励优化轨迹级结果，这些奖励惩罚触发模型拒绝机制的过度有害提示，并引导回答的语义相关性朝向目标有害内容。实验结果在多个模型和基准测试中显示出更高的攻击成功率，验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Enabling Population-Level Parallelism in Tree-Based Genetic Programming for GPU Acceleration</div>
<div class="meta-line">Authors: Zhihong Wu, Lishuang Wang, Kebin Sun, Zhuozhao Li, Ran Cheng</div>
<div class="meta-line">First: 2025-01-21T07:42:54+00:00 · Latest: 2026-01-13T14:05:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.17168v6">Abs</a> · <a href="https://arxiv.org/pdf/2501.17168v6">PDF</a> · <a href="https://github.com/EMI-Group/evogp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tree-based Genetic Programming (TGP) is a widely used evolutionary algorithm for tasks such as symbolic regression, classification, and robotic control. Due to the intensive computational demands of running TGP, GPU acceleration is crucial for achieving scalable performance. However, efficient GPU-based execution of TGP remains challenging, primarily due to three core issues: (1) the structural heterogeneity of program individuals, (2) the complexity of integrating multiple levels of parallelism, and (3) the incompatibility between high-performance CUDA execution and flexible Python-based environments. To address these issues, we propose EvoGP, a high-performance framework tailored for GPU acceleration of TGP via population-level parallel execution. First, EvoGP introduces a tensorized representation that encodes variable-sized trees into fixed-shape, memory-aligned arrays, enabling uniform memory access and parallel computation across diverse individuals. Second, EvoGP adopts an adaptive parallelism strategy that dynamically combines intra- and inter-individual parallelism based on dataset size, ensuring high GPU utilization across a broad spectrum of tasks. Third, EvoGP embeds custom CUDA kernels into the PyTorch runtime, achieving seamless integration with Python-based environments such as Gym, MuJoCo, Brax, and Genesis. Experimental results demonstrate that EvoGP achieves a peak throughput exceeding $10^{11}$ GPops/s. Specifcially, this performance represents a speedup of up to $304\times$ over existing GPU-based TGP implementations and $18\times$ over state-of-the-art CPU-based libraries. Furthermore, EvoGP maintains comparable accuracy and exhibits improved scalability across large population sizes. EvoGP is open source and accessible at: https://github.com/EMI-Group/evogp.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实现基于树的遗传编程中种群级并行化以支持GPU加速</div>
<div class="mono" style="margin-top:8px">基于树的遗传编程是一种广泛应用于符号回归、分类和机器人控制等任务的进化算法。由于运行TGP的计算需求密集，GPU加速对于实现可扩展性能至关重要。然而，基于GPU的高效TGP执行仍面临挑战，主要源于三个核心问题：(1) 程序个体的结构异质性，(2) 多层次并行化集成的复杂性，以及(3) 高性能CUDA执行与灵活Python环境之间的不兼容性。为解决这些问题，我们提出了EvoGP——一个通过种群级并行执行为TGP的GPU加速定制的高性能框架。首先，EvoGP引入张量化表示法，将可变大小的树编码为固定形状、内存对齐的数组，实现对不同个体的统一内存访问和并行计算。其次，EvoGP采用自适应并行策略，根据数据集规模动态结合个体内与个体间并行，确保在广泛任务中实现高GPU利用率。第三，EvoGP将定制CUDA内核嵌入PyTorch运行时，实现与Gym、MuJoCo、Brax和Genesis等Python环境的无缝集成。实验结果表明，EvoGP峰值吞吐量超过$10^{11}$ GPops/s，具体性能较现有基于GPU的TGP实现提升高达$304\times$，较先进基于CPU的库提升$18\times$。此外，EvoGP在保持相当精度的同时，展现出在大种群规模下更优的可扩展性。EvoGP已开源，访问地址：https://github.com/EMI-Group/evogp。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of efficiently accelerating tree-based genetic programming (TGP) on GPUs, motivated by the computational intensity of TGP for tasks like symbolic regression and the difficulties posed by structural heterogeneity, multi-level parallelism, and Python-CUDA integration. The proposed method, EvoGP, introduces a tensorized representation for variable-sized trees, an adaptive parallelism strategy, and custom CUDA kernels embedded in PyTorch to enable population-level parallel execution. Experimental results show that EvoGP achieves a peak throughput exceeding 10^11 GPops/s, delivering speedups of up to 304x over existing GPU-based TGP implementations and 18x over state-of-the-art CPU libraries while maintaining comparable accuracy and improved scalability with large populations.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在GPU上高效加速树基遗传编程（TGP）的挑战，其动机在于TGP在符号回归等任务中计算密集，且面临结构异构性、多级并行性以及Python-CUDA集成兼容性等核心问题。所提出的EvoGP方法通过引入张量化表示将变长树编码为固定形状的内存对齐数组，采用基于数据集大小的自适应并行策略，并将自定义CUDA内核嵌入PyTorch运行时，实现了种群级别的并行执行。实验结果表明，EvoGP的峰值吞吐量超过10^11 GPops/s，相比现有GPU-based TGP实现加速高达304倍，相比最先进的CPU库加速18倍，同时在保持相当精度的前提下，对大规模种群表现出更好的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">A Differential Perspective on Distributional Reinforcement Learning</div>
<div class="meta-line">Authors: Juan Sebastian Rojas, Chi-Guhn Lee</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-06-03T19:26:25+00:00 · Latest: 2026-01-13T13:43:59+00:00</div>
<div class="meta-line">Comments: In AAAI Conference on Artificial Intelligence 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.03333v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.03333v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To date, distributional reinforcement learning (distributional RL) methods have exclusively focused on the discounted setting, where an agent aims to optimize a discounted sum of rewards over time. In this work, we extend distributional RL to the average-reward setting, where an agent aims to optimize the reward received per time step. In particular, we utilize a quantile-based approach to develop the first set of algorithms that can successfully learn and/or optimize the long-run per-step reward distribution, as well as the differential return distribution of an average-reward MDP. We derive proven-convergent tabular algorithms for both prediction and control, as well as a broader family of algorithms that have appealing scaling properties. Empirically, we find that these algorithms yield competitive and sometimes superior performance when compared to their non-distributional equivalents, while also capturing rich information about the long-run per-step reward and differential return distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分布强化学习的差分视角</div>
<div class="mono" style="margin-top:8px">迄今为止，分布强化学习方法仅聚焦于折扣设定，即智能体旨在优化随时间折扣的奖励总和。本研究将分布强化学习扩展至平均奖励设定，其中智能体旨在优化每时间步获得的奖励。具体而言，我们采用基于分位数的方法，开发了首套能够成功学习和/或优化长期每步奖励分布以及平均奖励马尔可夫决策过程差分回报分布的算法。我们推导出经证明收敛的表格算法（适用于预测与控制），以及一个具有良好扩展性的更广泛算法族。实证研究表明，与非分布等效方法相比，这些算法不仅具有竞争力甚至有时表现更优，同时还能捕捉关于长期每步奖励和差分回报分布的丰富信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need to extend distributional reinforcement learning, which has been limited to discounted reward settings, to the average-reward setting where the goal is to optimize the long-run per-step reward. The method introduces a quantile-based approach to develop algorithms for learning the long-run per-step reward distribution and the differential return distribution in average-reward Markov decision processes, including proven-convergent tabular algorithms for prediction and control and scalable variants. The main experimental results show that these algorithms achieve competitive or superior performance compared to non-distributional methods while effectively capturing detailed distributional information.</div>
<div class="mono" style="margin-top:8px">本文的动机是将分布强化学习从传统的折扣奖励设置扩展到平均奖励设置，旨在优化长期每步奖励。方法上，采用基于分位数的策略，开发了首个能够学习平均奖励马尔可夫决策过程中长期每步奖励分布和差分回报分布的算法，包括可证明收敛的表格算法及可扩展的变体。主要实验结果表明，这些算法相比非分布方法具有竞争力甚至更优的性能，同时能有效捕捉丰富的分布信息。</div>
</details>
</div>
<div class="card">
<div class="title">VGC-Bench: Towards Mastering Diverse Team Strategies in Competitive Pokémon</div>
<div class="meta-line">Authors: Cameron Angliss, Jiaxun Cui, Jiaheng Hu, Arrasy Rahman, Peter Stone</div>
<div class="meta-line">First: 2025-06-12T03:19:39+00:00 · Latest: 2026-01-13T13:17:41+00:00</div>
<div class="meta-line">Comments: AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10326v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.10326v3">PDF</a> · <a href="https://github.com/cameronangliss/vgc-bench">Code1</a> · <a href="https://huggingface.co/datasets/cameronangliss/vgc-battle-logs">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing AI agents that can robustly adapt to varying strategic landscapes without retraining is a central challenge in multi-agent learning. Pokémon Video Game Championships (VGC) is a domain with a vast space of approximately $10^{139}$ team configurations, far larger than those of other games such as Chess, Go, Poker, StarCraft, or Dota. The combinatorial nature of team building in Pokémon VGC causes optimal strategies to vary substantially depending on both the controlled team and the opponent&#x27;s team, making generalization uniquely challenging. To advance research on this problem, we introduce VGC-Bench: a benchmark that provides critical infrastructure, standardizes evaluation protocols, and supplies a human-play dataset of over 700,000 battle logs and a range of baseline agents based on heuristics, large language models, behavior cloning, and multi-agent reinforcement learning with empirical game-theoretic methods such as self-play, fictitious play, and double oracle. In the restricted setting where an agent is trained and evaluated in a mirror match with a single team configuration, our methods can win against a professional VGC competitor. We repeat this training and evaluation with progressively larger team sets and find that as the number of teams increases, the best-performing algorithm in the single-team setting has worse performance and is more exploitable, but has improved generalization to unseen teams. Our code and dataset are open-sourced at https://github.com/cameronangliss/vgc-bench and https://huggingface.co/datasets/cameronangliss/vgc-battle-logs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VGC-Bench：迈向精通《宝可梦》竞技中的多样化队伍策略</div>
<div class="mono" style="margin-top:8px">开发无需重新训练即可稳健适应多变战略格局的智能体，是多智能体学习的核心挑战。宝可梦电子游戏锦标赛（VGC）的策略空间极为庞大，队伍配置组合数约达10^139种，远超国际象棋、围棋、扑克、《星际争霸》或《Dota》等游戏。宝可梦VGC中队伍构建的组合特性导致最优策略会因己方队伍与对手队伍的差异而发生显著变化，这使泛化能力面临独特挑战。为推进该问题的研究，我们推出VGC-Bench基准平台：它提供关键基础设施、标准化评估协议，并包含超过70万条对战记录的人类对战数据集，以及基于启发式方法、大语言模型、行为克隆、结合自我博弈/虚拟博弈/双重预言等经验博弈论方法的多智能体强化学习基线智能体。在智能体使用单一队伍配置进行镜像对战的限定场景中，我们的方法已能战胜职业VGC选手。我们通过逐步扩大队伍集合进行重复训练与评估，发现随着队伍数量增加：单队伍场景中的最优算法性能下降且更易被针对，但对未见过队伍的泛化能力有所提升。代码与数据集已开源：https://github.com/cameronangliss/vgc-bench 与 https://huggingface.co/datasets/cameronangliss/vgc-battle-logs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces VGC-Bench, a benchmark designed to address the challenge of creating AI agents that can generalize across the vast strategic landscape of competitive Pokémon, where team configurations number approximately 10^139, making adaptation far more complex than in games like Chess or Go. The method involves providing infrastructure, standardized evaluation, a dataset of over 700,000 human battle logs, and baseline agents using heuristics, large language models, behavior cloning, and multi-agent reinforcement learning with game-theoretic methods. Experimental results show that in a restricted mirror-match setting with a single team, the methods can defeat a professional player, but as the number of teams increases, performance declines and exploitability rises, though generalization to unseen teams improves.</div>
<div class="mono" style="margin-top:8px">本文介绍了VGC-Bench基准，旨在解决在竞技宝可梦这一战略空间巨大（约10^139种队伍配置）的领域中，开发能够泛化的AI智能体的挑战，其复杂性远超国际象棋或围棋等游戏。方法包括提供基础设施、标准化评估、超过70万条人类对战日志的数据集，以及基于启发式、大语言模型、行为克隆和采用自我博弈、虚拟博弈等博弈论方法的多智能体强化学习的基线智能体。实验结果表明，在单一队伍的镜像对战限制设置中，方法能击败职业选手，但随着队伍数量增加，性能下降且可被利用性上升，不过对未见队伍的泛化能力有所提升。</div>
</details>
</div>
<div class="card">
<div class="title">Your Group-Relative Advantage Is Biased</div>
<div class="meta-line">Authors: Fengkai Yang, Zherui Chen, Xiaohan Wang, Xiaodong Lu, Jiajun Chai, Guojun Yin, Wei Lin, Shuai Ma, Fuzhen Zhuang, Deqing Wang, Yaodong Yang, Jianxin Li, Yikun Ban</div>
<div class="meta-line">First: 2026-01-13T13:03:15+00:00 · Latest: 2026-01-13T13:03:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08521v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08521v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>群体相对优势估计存在偏差</div>
<div class="mono" style="margin-top:8px">基于验证器奖励的强化学习已成为推理任务上对大语言模型进行后训练的常用方法，其中GRPO及其变体等基于群体的方法获得广泛应用。这些方法依赖群体相对优势估计以避免学习判别器，但其理论性质仍缺乏深入理解。本研究揭示了基于群体强化学习的一个根本问题：群体相对优势估计量相对于真实期望优势存在固有偏差。我们首次通过理论分析证明，该方法会系统性地低估困难提示的优势、高估简单提示的优势，导致探索与利用失衡。为解决此问题，我们提出历史感知自适应难度加权方案，该自适应重加权机制基于动态演化的难度锚点和训练状态调整优势估计。在五个数学推理基准上的理论分析与实验均表明，HA-DW在融入GRPO及其变体后能持续提升性能。我们的结果表明，纠正有偏差的优势估计对于实现稳健高效的RLVR训练至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the widespread use of group-based reinforcement learning from verifier rewards (RLVR) methods like GRPO for post-training large language models on reasoning tasks, despite a poor theoretical understanding of their core group-relative advantage estimator. The method proposed to address this issue is History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that corrects the estimator&#x27;s inherent bias by adjusting advantage estimates based on a dynamically updated difficulty anchor and training history. The main experimental results, validated through theoretical analysis and tests on five mathematical reasoning benchmarks, show that HA-DW consistently enhances performance when integrated into GRPO and its variants, indicating that correcting this bias is crucial for robust and efficient RLVR training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，尽管基于群体的验证器奖励强化学习（RLVR）方法（如GRPO）被广泛用于大型语言模型在推理任务上的后训练，但其核心的群体相对优势估计器的理论性质尚不明确。为解决该估计器固有的偏差问题，本文提出了历史感知自适应难度加权（HA-DW）方法，这是一种通过基于动态更新的难度锚点和训练历史来调整优势估计的自适应重加权方案。主要实验结果通过理论分析和在五个数学推理基准测试上的实验表明，将HA-DW集成到GRPO及其变体中能持续提升性能，证明纠正这种偏差对于实现稳健高效的RLVR训练至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">AUV Trajectory Learning for Underwater Acoustic Energy Transfer and Age Minimization</div>
<div class="meta-line">Authors: Mohamed Afouene Melki, Mohammad Shehab, Mohamed-Slim Alouini</div>
<div class="meta-line">First: 2026-01-13T12:23:53+00:00 · Latest: 2026-01-13T12:23:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08491v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08491v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internet of underwater things (IoUT) is increasingly gathering attention with the aim of monitoring sea life and deep ocean environment, underwater surveillance as well as maintenance of underwater installments. However, conventional IoUT devices, reliant on battery power, face limitations in lifespan and pose environmental hazards upon disposal. This paper introduces a sustainable approach for simultaneous information uplink from the IoUT devices and acoustic energy transfer (AET) to the devices via an autonomous underwater vehicle (AUV), potentially enabling them to operate indefinitely. To tackle the time-sensitivity, we adopt age of information (AoI), and Jain&#x27;s fairness index. We develop two deep-reinforcement learning (DRL) algorithms, offering a high-complexity, high-performance frequency division duplex (FDD) solution and a low-complexity, medium-performance time division duplex (TDD) approach. The results elucidate that the proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向水下声能传输与信息年龄最小化的自主水下航行器轨迹学习</div>
<div class="mono" style="margin-top:8px">水下物联网（IoUT）在海洋生物监测、深海环境观测、水下安防及设施维护等领域日益受到关注。然而，依赖电池供电的传统IoUT设备存在寿命有限且废弃后易造成环境危害的问题。本文提出一种可持续方案：通过自主水下航行器（AUV）同时实现IoUT设备信息上行链路传输与声能传输（AET），有望使设备实现永久运行。针对时间敏感性，我们采用信息年龄（AoI）与Jain公平性指数作为度量标准。开发了两种深度强化学习（DRL）算法：高性能高复杂度的频分双工（FDD）方案与中性能低复杂度的时分双工（TDD）方案。实验结果表明，相较于基线方法，所提FDD与TDD方案能显著降低平均AoI，并提升能量收集效率与数据收集公平性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited lifespan and environmental concerns of battery-powered Internet of Underwater Things (IoUT) devices, this paper proposes a sustainable system where an autonomous underwater vehicle (AUV) simultaneously collects data and delivers acoustic energy to these devices. The method employs deep reinforcement learning (DRL) to design two AUV trajectory strategies: a high-performance frequency division duplex (FDD) approach and a lower-complexity time division duplex (TDD) approach, both optimizing for age of information (AoI) and fairness. Experimental results demonstrate that both proposed solutions significantly reduce average AoI, increase harvested energy, and improve data collection fairness compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">针对依赖电池的水下物联网设备寿命有限且存在环境危害的问题，本文提出了一种可持续方案，利用自主水下航行器同时从设备收集信息并通过声能传输为其充电。研究方法采用深度强化学习，设计了两种航行器轨迹策略：高性能的频分双工方案和较低复杂度的时分双工方案，均以优化信息年龄和公平性为目标。实验结果表明，与基线方法相比，所提出的两种方案均能显著降低平均信息年龄、提升能量收集效率并改善数据收集的公平性。</div>
</details>
</div>
<div class="card">
<div class="title">JudgeRLVR: Judge First, Generate Second for Efficient Reasoning</div>
<div class="meta-line">Authors: Jiangshan Duo, Hanyu Li, Hailin Zhang, Yudong Wang, Sujian Li, Liang Zhao</div>
<div class="meta-line">First: 2026-01-13T11:47:42+00:00 · Latest: 2026-01-13T11:47:42+00:00</div>
<div class="meta-line">Comments: 16 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08468v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JudgeRLVR：先判别后生成的高效推理框架</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习已成为大语言模型推理的标准范式。然而，仅针对最终答案正确性进行优化，常导致模型陷入漫无目的、冗长的探索，依赖穷举试错而非结构化规划来求解。虽然长度惩罚等启发式约束可减少冗余，但常会截断关键推理步骤，形成效率与可验证性间的艰难权衡。本文提出判别能力是高效生成的前提：通过学习区分有效解，模型可内化一种剪枝搜索空间的引导信号。我们提出JudgeRLVR两阶段“先判别后生成”范式：第一阶段训练模型判别具有可验证答案的解决方案；第二阶段用判别器初始化模型，通过经典生成式RLVR进行微调。在相同数学领域训练数据下，相比经典RLVR，JudgeRLVR为Qwen3-30B-A3B模型实现了更优的质量-效率权衡：在领域内数学任务上，平均准确率提升约3.7分，平均生成长度减少42%；在领域外基准测试中，平均准确率提升约4.5分，展现出更强的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of standard Reinforcement Learning with Verifiable Rewards (RLVR) in large language models, where optimization for final-answer correctness often leads to verbose, unstructured exploration. The authors propose JudgeRLVR, a two-stage paradigm that first trains the model to judge solution responses, thereby internalizing discriminative guidance to prune the search space, and then fine-tunes it with vanilla generating RLVR. Experimental results on the Qwen3-30B-A3B model show that JudgeRLVR achieves a better trade-off between quality and efficiency, yielding an average accuracy gain of about +3.7 points with a 42% reduction in generation length on in-domain math tasks, and about +4.5 points improvement on out-of-domain benchmarks, demonstrating enhanced generalization.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型中基于可验证奖励的强化学习（RLVR）效率低下的问题，指出仅优化最终答案正确性常导致模型进行冗长、无结构的探索。作者提出JudgeRLVR这一两阶段范式，首先训练模型评判解决方案响应，从而内化判别性指导以剪枝搜索空间，然后使用标准生成式RLVR进行微调。在Qwen3-30B-A3B模型上的实验结果表明，JudgeRLVR在质量和效率间取得了更好的平衡，在领域内数学任务上平均准确率提升约3.7分且生成长度减少42%，在领域外基准测试上平均准确率提升约4.5分，显示出更强的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation</div>
<div class="meta-line">Authors: Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, Wei Chen</div>
<div class="meta-line">First: 2026-01-13T10:56:39+00:00 · Latest: 2026-01-13T10:56:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08430v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08430v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RubricHub：基于自动化由粗到细生成框架构建的全面且高区分度评分标准数据集</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）在数学等推理密集型领域取得了显著进展，但开放生成任务的优化仍因缺乏真实标注而面临挑战。基于评分标准的评估虽能提供结构化验证代理，但现有方法存在可扩展性瓶颈与标准粗糙问题，导致监督效果受限。为此，我们提出自动化由粗到细评分标准生成框架，通过融合原则引导合成、多模型聚合与难度演进机制，生成能捕捉细微差异的全面高区分度标准。基于此框架，我们构建了RubricHub——一个大规模（约11万条）多领域数据集。通过包含基于评分标准的拒绝采样微调（RuFT）与强化学习（RuRL）的两阶段后训练流程验证其有效性：实验表明RubricHub带来显著性能提升，后训练的Qwen3-14B模型在HealthBench上达到69.3分的最先进水平，超越GPT-5等前沿专有模型。代码与数据即将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of optimizing open-ended text generation in reinforcement learning, where the lack of ground truth and coarse evaluation rubrics limit progress. To overcome this, the authors propose an automated Coarse-to-Fine Rubric Generation framework that combines principle-guided synthesis, multi-model aggregation, and difficulty evolution to create detailed, discriminative criteria. They introduce RubricHub, a large-scale dataset of approximately 110k rubrics across multiple domains, and validate it through a two-stage post-training pipeline involving Rubric-based Rejection Sampling Fine-Tuning and Reinforcement Learning. Experimental results show that models trained with RubricHub achieve state-of-the-art performance, with Qwen3-14B scoring 69.3 on HealthBench, outperforming proprietary models like GPT-5.</div>
<div class="mono" style="margin-top:8px">本文针对强化学习中开放式文本生成的优化挑战，指出缺乏真实标注和粗糙评估标准限制了进展。为解决这一问题，作者提出了一种自动化的从粗到细的评分标准生成框架，结合原则引导合成、多模型聚合和难度演进，以创建细致且区分度高的评估准则。他们引入了RubricHub，这是一个跨多个领域、规模约11万条评分标准的大规模数据集，并通过包含基于评分标准的拒绝采样微调和强化学习的两阶段后训练流程进行验证。实验结果表明，使用RubricHub训练的模型实现了最先进的性能，其中Qwen3-14B在HealthBench上得分69.3，超越了GPT-5等专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering</div>
<div class="meta-line">Authors: Nonghai Zhang, Weitao Ma, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Jingwen Xu</div>
<div class="meta-line">First: 2026-01-13T10:55:08+00:00 · Latest: 2026-01-13T10:55:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08427v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08427v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid&#x27;&#x27; through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>静默裁判：基于潜在几何聚类的自验证强化学习</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）显著提升了大型语言模型（LLM）的推理性能，但其成功严重依赖昂贵的外部验证器或人工规则，这不仅导致高昂的计算成本和训练延迟，还因稀疏奖励而阻碍优化效率。为解决这些问题，我们提出Latent-GRPO框架，直接从潜在空间几何结构推导内在奖励。关键实证分析揭示了一个引人注目的几何特性：正确推理轨迹的终止词表征形成具有高类内相似度的密集聚类，而错误轨迹则作为离群点分散分布。基于此发现，我们提出迭代鲁棒质心估计（IRCE）算法，通过球面投影缓解幅度波动，并借助迭代聚合估计鲁棒的“真值质心”，从而生成密集连续的奖励。在多数据集上的实验结果表明，本方法在保持模型性能的同时，相比基线实现了超过2倍的训练加速。此外，广泛实验结果证明了其强大的泛化能力与鲁棒性。代码即将发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high computational cost and sparse rewards of using external verifiers in Group Relative Policy Optimization (GRPO) for enhancing LLM reasoning. To address this, the authors propose Latent-GRPO, a method that leverages latent geometric clustering by observing that correct reasoning trajectories form dense clusters in representation space while incorrect ones scatter as outliers. They introduce the Iterative Robust Centroid Estimation (IRCE) algorithm to generate dense, continuous rewards via spherical projection and robust centroid estimation. Experimental results across multiple datasets show the method maintains performance while achieving over 2x training speedup compared to baselines, with additional demonstrations of strong generalization and robustness.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，使用外部验证器进行群体相对策略优化（GRPO）以增强大语言模型推理时，存在计算成本高和奖励稀疏的问题。为解决此问题，作者提出Latent-GRPO方法，该方法利用潜在几何聚类，观察到正确推理轨迹在表示空间中形成密集簇而错误轨迹则分散为异常值。他们引入了迭代鲁棒质心估计（IRCE）算法，通过球面投影和鲁棒质心估计来生成密集、连续的奖励。在多个数据集上的实验结果表明，该方法在保持模型性能的同时，相比基线实现了超过2倍的训练加速，并展现出强大的泛化能力和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs</div>
<div class="meta-line">Authors: Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy</div>
<div class="meta-line">First: 2026-01-13T10:17:46+00:00 · Latest: 2026-01-13T10:17:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08403v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08403v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens&#x27; marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&amp;M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>欧文-沙普利策略优化（OSPO）：一种面向生成式搜索大语言模型的原则性强化学习算法</div>
<div class="mono" style="margin-top:8px">大语言模型日益通过强化学习训练以完成个性化推荐任务，但GRPO等标准方法依赖稀疏的序列级奖励，导致信用分配缺口，难以确定驱动成功的关键词元。当模型必须从缺乏真实标签的模糊语言中推断潜在用户意图时——这种推理模式在预训练阶段极少出现——该问题尤为突出。我们提出欧文-沙普利策略优化（OSPO）框架，该框架基于词元对结果的边际贡献重新分配序列级优势。与需要额外计算的价值模型方法不同，OSPO通过沙普利-欧文归因实现基于势函数的奖励塑形，在保持最优策略的同时分配片段级信用，无需参数化价值模型即可直接从任务反馈中学习。通过构建语义连贯单元（描述产品属性的短语或捕捉偏好的句子）的联盟，OSPO能识别驱动性能的响应部分。在亚马逊ESCI和H&amp;M时尚数据集上的实验表明，该方法相对基线模型持续获得提升，并对训练时未见的分布外检索器展现出显著的测试时鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the credit assignment problem in reinforcement learning for large language models used in personalized recommendation, where sparse sequence-level rewards obscure token-level contributions, especially when inferring latent user intent from ambiguous queries. It introduces Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages by computing tokens&#x27; marginal contributions via Shapley-Owen attributions, using potential-based reward shaping to assign credit to semantically coherent segments without requiring parametric value models. Experimental results on Amazon ESCI and H&amp;M Fashion datasets demonstrate that OSPO consistently outperforms baselines and exhibits robust performance with out-of-distribution retrievers at test time.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在个性化推荐任务中强化学习面临的信用分配问题，即稀疏的序列级奖励难以明确各令牌的贡献，尤其是在从模糊查询中推断潜在用户意图时。作者提出了欧文-沙普利策略优化（OSPO）框架，通过沙普利-欧文归因计算令牌的边际贡献，重新分配序列级优势，利用基于势的奖励塑造为语义连贯的片段分配信用，无需参数化价值模型。在亚马逊ESCI和H&amp;M时尚数据集上的实验表明，OSPO持续优于基线方法，并在测试时对训练中未见过的分布外检索器表现出鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Memory in the Age of AI Agents</div>
<div class="meta-line">Authors: Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, Senjie Jin, Jiejun Tan, Yanbin Yin, Jiongnan Liu, Zeyu Zhang, Zhongxiang Sun, Yutao Zhu, Hao Sun, Boci Peng, Zhenrong Cheng, Xuanbo Fan, Jiaxin Guo, Xinlei Yu, Zhenhong Zhou, Zewen Hu, Jiahao Huo, Junhao Wang, Yuwei Niu, Yu Wang, Zhenfei Yin, Xiaobin Hu, Yue Liao, Qiankun Li, Kun Wang, Wangchunshu Zhou, Yixin Liu, Dawei Cheng, Qi Zhang, Tao Gui, Shirui Pan, Yan Zhang, Philip Torr, Zhicheng Dou, Ji-Rong Wen, Xuanjing Huang, Yu-Gang Jiang, Shuicheng Yan</div>
<div class="meta-line">First: 2025-12-15T17:22:34+00:00 · Latest: 2026-01-13T09:33:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13564v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.13564v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人工智能代理时代的记忆</div>
<div class="mono" style="margin-top:8px">记忆已成为并仍将是基于基础模型的智能代理的核心能力。随着代理记忆研究迅速扩展并受到空前关注，该领域也日益呈现碎片化。现有关于代理记忆的研究在动机、实现方式和评估标准上往往存在显著差异，而定义松散的记忆术语泛滥进一步模糊了概念清晰度。传统分类（如长/短期记忆）已不足以涵盖当代代理记忆系统的多样性。本文旨在提供当前代理记忆研究的最新全景。我们首先明确界定代理记忆的范围，并将其与相关概念（如大语言模型记忆、检索增强生成和上下文工程）区分开来。接着，我们从形式、功能和动态三个统一视角审视代理记忆：从形式角度，我们识别出三种主流记忆实现方式——词元级记忆、参数化记忆和潜在记忆；从功能角度，我们提出更细粒度的分类法，区分事实记忆、经验记忆和工作记忆；从动态角度，我们分析记忆如何随时间形成、演化和检索。为支持实际开发，我们汇编了全面的记忆基准测试和开源框架总结。在整合现有成果的基础上，我们阐述了前瞻性研究前沿，包括记忆自动化、强化学习集成、多模态记忆、多代理记忆及可信度问题。我们希望本综述不仅能作为现有工作的参考，更能为将记忆重新定位为未来智能代理设计中的一等原语提供概念基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the growing yet fragmented research landscape surrounding memory in AI agents, where diverse terminologies and approaches obscure a clear understanding. To address this, the authors systematically survey agent memory by delineating its scope from related concepts and analyzing it through three unified lenses: forms (identifying token-level, parametric, and latent memory), functions (proposing a taxonomy of factual, experiential, and working memory), and dynamics (examining memory formation, evolution, and retrieval). The main experimental results include a comprehensive compilation of memory benchmarks and open-source frameworks to support practical development, alongside a forward-looking discussion of emerging research frontiers such as memory automation and multimodal memory.</div>
<div class="mono" style="margin-top:8px">本文的动机在于，人工智能智能体记忆研究领域发展迅速但日益碎片化，术语定义松散且方法各异，阻碍了概念清晰性。为此，作者通过界定智能体记忆与相关概念（如大语言模型记忆、检索增强生成）的区分，从形式、功能和动态三个统一视角系统梳理了该领域：在形式上识别了令牌级、参数化和潜在记忆三种主要实现；在功能上提出了事实记忆、经验记忆和工作记忆的更细粒度分类；在动态上分析了记忆的形成、演化和检索过程。主要实验成果包括汇编了全面的记忆基准测试和开源框架以支持实际开发，并前瞻性地探讨了记忆自动化、多模态记忆等新兴研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition</div>
<div class="meta-line">Authors: Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos</div>
<div class="meta-line">First: 2026-01-13T08:30:43+00:00 · Latest: 2026-01-13T08:30:43+00:00</div>
<div class="meta-line">Comments: 7 pages, 4 figures, submitted to the IFAC World Congress 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08327v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08327v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent&#x27;s policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents&#x27; communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe and stable task execution, confirming the framework&#x27;s effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于通信正则化的异构多智能体强化学习安全协同目标捕获框架</div>
<div class="mono" style="margin-top:8px">本文提出一种去中心化多智能体强化学习框架，使结构异构的智能体团队能在部分可观测、通信受限和动态交互的环境中协同发现并捕获随机分布的目标。各智能体策略采用多智能体近端策略优化算法训练，并配备图注意力网络编码器，该编码器融合模拟距离传感数据与相邻智能体交换的通信嵌入向量，实现基于局部传感和关系信息的上下文感知决策。本工作特别提出统一框架，通过安全滤波器集成基于图的通信机制和轨迹感知安全策略。该架构采用结构化奖励设计，通过促进信息正交性激励有效目标发现与捕获、碰撞规避以及智能体通信向量间的解耦。通过系统消融实验验证了所提奖励函数的有效性。仿真结果展示了安全稳定的任务执行性能，证实了框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for safe and coordinated target acquisition in partially observable, communication-constrained environments with structurally heterogeneous agents, this paper proposes a decentralized multi-agent reinforcement learning framework. The method employs Multi-Agent Proximal Policy Optimization with a Graph Attention Network encoder to integrate local sensing and communicated embeddings, and introduces a unified architecture incorporating graph-based communication and trajectory-aware safety filters. The main experimental results, validated through ablation studies and simulations, demonstrate that the proposed structured reward function—which promotes target acquisition, collision avoidance, and communication de-correlation—enables safe, stable, and effective task execution.</div>
<div class="mono" style="margin-top:8px">本文旨在解决异构多智能体在部分可观测、通信受限的动态环境中安全协调获取随机目标的问题，提出了一种去中心化的多智能体强化学习框架。该方法采用多智能体近端策略优化算法，并利用图注意力网络编码器整合局部感知与邻域通信信息，同时通过一个统一架构集成了基于图的通信和轨迹感知的安全滤波器。主要实验结果表明，所设计的结构化奖励函数能有效促进目标发现与获取、避免碰撞并实现通信向量的解相关，消融研究与仿真验证了该框架可实现安全稳定的任务执行。</div>
</details>
</div>
<div class="card">
<div class="title">AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation</div>
<div class="meta-line">Authors: Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin</div>
<div class="meta-line">First: 2026-01-13T08:22:28+00:00 · Latest: 2026-01-13T08:22:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08323v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08323v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AtomMem：基于原子内存操作的可学习动态智能体记忆</div>
<div class="mono" style="margin-top:8px">为智能体配备记忆对于解决现实世界中的长程问题至关重要。然而，现有的大多数智能体记忆机制依赖于静态且手工设计的工作流程，这限制了其性能与泛化能力，凸显了对更灵活、基于学习的记忆框架的需求。本文提出AtomMem，将记忆管理重构为一个动态决策问题：通过将高层记忆过程解构为基本的原子CRUD（创建、读取、更新、删除）操作，将记忆工作流转化为可学习的决策过程。结合监督微调与强化学习，AtomMem能够学习一种自主的、任务对齐的策略，以协调适应特定任务需求的记忆行为。在三个长上下文基准测试中的实验结果表明，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法。对训练动态的进一步分析表明，这种基于学习的框架使智能体能够发现结构化、任务对齐的记忆管理策略，凸显了其相对于预定义流程的关键优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the limitations of static, hand-crafted memory workflows in agents for long-horizon tasks, which hinder performance and generalization. To address this, the authors propose AtomMem, a learnable memory framework that reframes memory management as a dynamic decision-making problem by decomposing it into fundamental atomic CRUD operations, making the workflow a learnable process. Through a combination of supervised fine-tuning and reinforcement learning, AtomMem learns an autonomous policy to orchestrate memory behaviors. Experimental results on three long-context benchmarks show that the trained AtomMem-8B model consistently outperforms prior static memory methods, with further analysis revealing that the learning-based approach enables the discovery of structured, task-aligned memory management strategies.</div>
<div class="mono" style="margin-top:8px">本文的动机在于解决智能体在长周期任务中依赖静态、手工设计的内存工作流程的局限性，这些限制影响了性能与泛化能力。为此，作者提出了AtomMem，一种可学习的动态内存框架，它将内存管理重构为一个动态决策问题，通过将高级内存过程解构为基本的原子CRUD操作，使工作流程可学习。结合监督微调与强化学习，AtomMem学习了一种自主策略来编排适应特定任务需求的内存行为。在三个长上下文基准测试上的实验结果表明，训练后的AtomMem-8B模型持续优于先前的静态工作流内存方法，进一步分析训练动态显示，这种基于学习的方法使智能体能够发现结构化、与任务对齐的内存管理策略，突显了其相对于预定义流程的关键优势。</div>
</details>
</div>
<div class="card">
<div class="title">ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning</div>
<div class="meta-line">Authors: Kun Liang, Clive Bai, Xin Xu, Chenming Tang, Sanwoo Lee, Weijie Liu, Saiyong Yang, Yunfang Wu</div>
<div class="meta-line">First: 2026-01-13T07:57:48+00:00 · Latest: 2026-01-13T07:57:48+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08310v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08310v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORBIT：面向可控多预算推理的在线策略探索-利用框架</div>
<div class="mono" style="margin-top:8px">近期的大型推理模型通过利用长链思维推理展现出强大性能，但在推理时统一应用过长的推理过程会产生高昂且常非必要的计算成本。为解决此问题，先前研究探索了多种从输入推断适当推理预算的策略。然而，此类方法在最坏情况下并不可靠，因为估计最小所需推理努力本质上具有难度，且它们在训练期间隐式固定了推理成本与准确性的权衡，限制了不同部署场景下的灵活性。基于这些局限，我们提出ORBIT——一种由输入触发的、具有明确分离推理模式的可控多预算推理框架。ORBIT采用多阶段强化学习探索各推理努力水平下的帕累托最优推理行为，随后通过在线策略蒸馏将这些行为融合至单一统一模型。实验表明，ORBIT实现了：（1）跨多模式的可控推理行为；（2）各模式内具有竞争力的推理密度；（3）将前沿策略集成至单一学生模型，同时保持清晰的模式分离与各模式的高性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the high computational cost of uniformly applying long-form Chain-of-Thought reasoning in Large Reasoning Models, and the inflexibility of prior methods that fix the cost-accuracy trade-off during training. The proposed method, ORBIT, introduces a controllable multi-budget reasoning framework that uses multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors for different effort levels, followed by on-policy distillation to unify these behaviors into a single model. Experimental results demonstrate that ORBIT achieves controllable reasoning across multiple modes, maintains competitive reasoning density within each mode, and successfully integrates frontier policies into a unified student model while preserving mode separation and high performance.</div>
<div class="mono" style="margin-top:8px">本文的动机在于大型推理模型统一应用长链思维推理会导致高昂计算成本，而先前方法在训练时固定成本与准确性的权衡，缺乏部署灵活性。所提出的ORBIT方法是一个可控的多预算推理框架，采用多阶段强化学习来发现不同推理努力水平下的帕累托最优行为，并通过策略蒸馏将这些行为融合到单一模型中。实验结果表明，ORBIT实现了跨多种模式的可控推理，在每个模式下保持了有竞争力的推理密度，并将前沿策略成功集成到统一的学生模型中，同时保持了清晰的模式分离和高性能。</div>
</details>
</div>
<div class="card">
<div class="title">GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition</div>
<div class="meta-line">Authors: Jingchao Wang, Yifan He, Haote Yang, Jiang Wu, Lingli Ge, Xingjian Wei, Yinfan Wang, Linye Li, Huijie Ao, Chengjin Liu, Bin Wang, Lijun Wu, Conghui He</div>
<div class="meta-line">First: 2025-06-09T08:47:10+00:00 · Latest: 2026-01-13T07:21:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07553v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.07553v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optical Chemical Structure Recognition (OCSR) is essential for converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown promise, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To address these issues, we introduce GTR-VL, featuring two key innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric \textit{Faithfully Recognize What You&#x27;ve Seen} principle, which aligns abbreviated structures in images with their expanded annotations. For hand-drawn OCSR tasks, where datasets lack graph annotations and only provide final SMILES, we apply reinforcement learning using the GRPO method, introducing reward mechanisms like format reward, graph reward, and SMILES reward. This approach significantly enhances performance in hand-drawn recognition tasks through weak supervision. We developed GTR-1.3M, a large-scale instruction-tuning dataset with corrected annotations, and MolRec-Bench, the first benchmark for fine-grained evaluation of graph-parsing accuracy in OCSR. Our two-stage training scheme involves SFT training for printed images and the GRPO method for transferring capabilities to hand-drawn tasks. Experiments show that GTR-VL outperforms specialist models, chemistry-domain VLMs, and commercial VLMs on both printed and hand-drawn datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GTR-CoT：图遍历作为分子结构识别的视觉思维链</div>
<div class="mono" style="margin-top:8px">光学化学结构识别（OCSR）对于将分子图像转换为机器可读格式至关重要。尽管近期视觉语言模型（VLMs）展现出潜力，但其图像描述方法在处理复杂分子结构和不一致标注时仍存在困难。为解决这些问题，我们提出GTR-VL模型，其具备两项关键创新：（1）采用“图遍历作为视觉思维链”机制，通过顺序预测原子-键合来模拟人类逐步解析分子图的推理过程；（2）遵循以数据为中心的“忠实识别所见结构”原则，实现图像中简写结构与扩展标注的精确对齐。针对手绘OCSR任务（其数据集缺乏图结构标注，仅提供最终SMILES），我们应用基于GRPO方法的强化学习，引入格式奖励、图结构奖励和SMILES奖励机制。这种弱监督方法显著提升了手绘识别任务的性能。我们构建了包含修正标注的大规模指令调优数据集GTR-1.3M，并创建了首个用于细粒度评估OCSR图解析精度的基准测试MolRec-Bench。采用两阶段训练方案：先对印刷图像进行监督微调训练，再通过GRPO方法将能力迁移至手绘任务。实验表明，GTR-VL在印刷和手绘数据集上均优于专业模型、化学领域VLMs及商业VLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing vision-language models in Optical Chemical Structure Recognition (OCSR), which often fail with complex molecular structures and inconsistent annotations, this paper introduces GTR-VL with two key innovations: a Graph Traversal as Visual Chain of Thought mechanism that incrementally parses molecular graphs through sequential atom-bond predictions, and a data-centric principle to align abbreviated image structures with expanded annotations. The method employs a two-stage training scheme, using supervised fine-tuning for printed images and reinforcement learning (GRPO) with rewards for format, graph, and SMILES accuracy to transfer capabilities to hand-drawn tasks where only SMILES annotations are available. Experimental results demonstrate that GTR-VL outperforms specialist models, chemistry-domain VLMs, and commercial VLMs on both printed and hand-drawn datasets, supported by the newly created GTR-1.3M instruction-tuning dataset and MolRec-Bench benchmark for fine-grained evaluation.</div>
<div class="mono" style="margin-top:8px">针对现有视觉语言模型在光学化学结构识别（OCSR）中处理复杂分子结构和不一致标注时的不足，本文提出了GTR-VL，其核心创新包括：模拟人类推理的“图遍历作为视觉思维链”机制，通过顺序预测原子-键来逐步解析分子图；以及数据中心的“忠实识别所见”原则，用于对齐图像中的缩写结构与扩展标注。方法采用两阶段训练方案，对印刷图像使用监督微调，并对仅提供SMILES标注的手绘任务应用强化学习（GRPO），引入格式、图和SMILES奖励机制以实现弱监督下的能力迁移。实验结果表明，GTR-VL在印刷和手绘数据集上均优于专业模型、化学领域视觉语言模型和商业视觉语言模型，并基于新构建的GTR-1.3M指令调优数据集和用于细粒度评估的MolRec-Bench基准验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">RAG-R1: Incentivizing the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism</div>
<div class="meta-line">Authors: Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu</div>
<div class="meta-line">First: 2025-06-30T09:02:45+00:00 · Latest: 2026-01-13T06:28:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02962v6">Abs</a> · <a href="https://arxiv.org/pdf/2507.02962v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs), despite their remarkable capabilities, are prone to generating hallucinated or outdated content due to their static internal knowledge. While Retrieval-Augmented Generation (RAG) integrated with Reinforcement Learning (RL) offers a solution, these methods are fundamentally constrained by a single-query mode, leading to prohibitive latency and inherent brittleness. To overcome these limitations, we introduce RAG-R1, a novel two-stage training framework centered around multi-query parallelism. Our framework enables LLMs to adaptively leverage internal and external knowledge during the reasoning process while transitioning from the single-query mode to multi-query parallelism. This architectural shift bolsters reasoning robustness while significantly reducing inference latency. Extensive experiments on seven question-answering benchmarks confirm the superiority of our method, which outperforms the strongest baseline by up to 13.7% and decreases inference time by 11.1%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAG-R1：通过多查询并行化激励大语言模型的检索与推理能力</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）虽能力卓越，但其静态内部知识易导致生成内容存在幻觉或过时。尽管结合强化学习（RL）的检索增强生成（RAG）提供了解决方案，这些方法本质上受限于单查询模式，导致延迟过高且存在固有脆弱性。为突破这些限制，我们提出了RAG-R1——一种以多查询并行为核心的新型两阶段训练框架。该框架使LLMs在推理过程中能自适应地利用内部与外部知识，同时实现从单查询模式向多查询并行的转变。这一架构性变革增强了推理的鲁棒性，并显著降低了推理延迟。在七个问答基准上的大量实验证实了本方法的优越性：其性能最高超越最强基线13.7%，推理时间降低11.1%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of single-query Retrieval-Augmented Generation (RAG) methods, which often suffer from high latency and brittleness when Large Language Models (LLMs) generate outdated or hallucinated content. The authors propose RAG-R1, a novel two-stage training framework that shifts from single-query to multi-query parallelism, enabling LLMs to adaptively combine internal and external knowledge during reasoning. Experimental results on seven question-answering benchmarks demonstrate that RAG-R1 outperforms the strongest baseline by up to 13.7% in performance while reducing inference time by 11.1%, thereby enhancing both robustness and efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对单查询检索增强生成（RAG）方法存在的延迟高、鲁棒性差等问题，旨在解决大型语言模型（LLM）因静态内部知识易产生过时或幻觉内容的缺陷。研究者提出了RAG-R1，一种新颖的两阶段训练框架，其核心是从单查询模式转向多查询并行，使LLM在推理过程中能自适应地利用内部和外部知识。在七个问答基准上的大量实验表明，该方法性能最高超越最强基线13.7%，同时推理时间降低11.1%，显著提升了模型的鲁棒性和推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks</div>
<div class="meta-line">Authors: Abdikarim Mohamed Ibrahim, Rosdiadee Nordin</div>
<div class="meta-line">First: 2026-01-13T06:23:21+00:00 · Latest: 2026-01-13T06:23:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08254v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08254v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型人工智能模型引导的非地面网络资源分配深度强化学习</div>
<div class="mono" style="margin-top:8px">大型人工智能模型（LAM）已被提出应用于非地面网络（NTN），其强大的泛化能力和减少的任务特定训练需求可带来更优性能。本文提出一种由大型语言模型（LLM）引导的深度强化学习（DRL）智能体。LLM作为高层协调器，生成文本化引导信息以塑造DRL智能体训练过程中的奖励机制。实验结果表明：在吞吐量、公平性和中断概率指标上，LAM-DRL相比启发式方法，在常规天气场景下性能提升40%，在极端天气场景下提升64%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the need to enhance resource allocation in Non-Terrestrial Networks (NTN) by leveraging the generalization capabilities of Large AI Models (LAM) to reduce task-specific training. The method introduces a Deep Reinforcement Learning (DRL) agent guided by a Large Language Model (LLM), where the LLM acts as a high-level coordinator providing textual guidance to shape the DRL agent&#x27;s reward function during training. Experimental results demonstrate that the proposed LAM-DRL approach significantly outperforms traditional DRL, achieving 40% higher performance in nominal weather scenarios and 64% higher in extreme weather scenarios compared to heuristic methods, as measured by throughput, fairness, and outage probability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是利用大型人工智能模型（LAM）的泛化能力来改善非地面网络（NTN）中的资源分配，以减少针对特定任务的训练。方法上提出了一种由大型语言模型（LLM）引导的深度强化学习（DRL）智能体，其中LLM作为高层协调器，在训练过程中生成文本指导以塑造DRL智能体的奖励函数。实验结果表明，所提出的LAM-DRL方法在吞吐量、公平性和中断概率方面显著优于传统DRL，在正常天气场景下性能提升40%，在极端天气场景下提升64%，均相对于启发式方法而言。</div>
</details>
</div>
<div class="card">
<div class="title">Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making</div>
<div class="meta-line">Authors: Liu He</div>
<div class="meta-line">First: 2026-01-13T06:09:24+00:00 · Latest: 2026-01-13T06:09:24+00:00</div>
<div class="meta-line">Comments: 15 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08247v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08247v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将认知偏差融入强化学习以优化金融决策</div>
<div class="mono" style="margin-top:8px">金融市场受人类非理性行为影响，认知偏差导致决策偏离理性。传统强化学习（RL）金融决策模型假设理性主体，可能忽略心理因素的作用。本研究将认知偏差整合到金融交易的RL框架中，假设此类模型能模拟人类交易行为，并在风险调整后收益上优于标准RL代理。我们引入过度自信和损失厌恶等偏差至奖励结构和决策过程，并在模拟及真实交易环境中评估其表现。尽管结果未定或呈负面，本研究为将类人偏差融入RL的挑战提供了见解，为开发稳健的金融AI系统提供了宝贵经验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the influence of human cognitive biases on financial markets, this study integrates psychological factors like overconfidence and loss aversion into reinforcement learning (RL) frameworks for trading, moving beyond traditional rational-agent assumptions. The method involves embedding these biases into reward structures and decision-making processes within RL models. Experimental results from simulated and real-world trading environments show that while the biased models can exhibit human-like behavior, they yield inconclusive or negative performance outcomes, highlighting the challenges of such integration but offering insights for developing more robust financial AI systems.</div>
<div class="mono" style="margin-top:8px">本研究基于金融市场受人类认知偏差影响的动机，将过度自信和损失厌恶等心理因素融入强化学习交易框架，以超越传统理性代理假设。方法上，通过将偏差嵌入强化学习模型的奖励结构和决策过程来实现。在模拟和真实交易环境中的实验结果表明，尽管这些带偏差模型能表现出类人行为，但其性能结果不确定或为负面，这揭示了将人类偏差整合到强化学习中的挑战，同时也为开发更稳健的金融人工智能系统提供了宝贵经验。</div>
</details>
</div>
<div class="card">
<div class="title">The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination</div>
<div class="meta-line">Authors: Haoran Su, Yandong Sun, Congjia Yu</div>
<div class="meta-line">First: 2026-01-13T05:47:18+00:00 · Latest: 2026-01-13T05:47:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08237v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08237v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励工程学的终结：大语言模型如何重塑多智能体协作</div>
<div class="mono" style="margin-top:8px">奖励工程学——即通过人工设计奖励函数以引导智能体达成预期行为——始终是多智能体强化学习领域的核心难题。信用分配模糊性、环境非平稳性以及交互复杂度的组合增长进一步加剧了这一挑战。本文认为，大语言模型的最新进展正推动从手工设计的数值奖励向基于语言的目标描述的范式转变。已有研究表明，大语言模型可直接从自然语言描述合成奖励函数（如EUREKA），并能以最少人工干预在线调整奖励方案（如CARD）。与此同时，新兴的“可验证奖励强化学习”范式为语言介导的监督机制替代传统奖励工程提供了实证依据。我们将这一转型归纳为三个维度：语义化奖励描述、动态奖励适配以及更优的人类意图对齐，同时指出其在计算开销、幻觉鲁棒性以及大规模多智能体系统扩展性方面仍存在开放挑战。最后，我们提出一种新的研究方向：让协作从共享语义表征中自然涌现，而非依赖显式设计的数值信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper argues that the longstanding challenge of manual reward engineering in multi-agent reinforcement learning, plagued by issues like credit assignment and non-stationarity, is being transformed by large language models (LLMs). The proposed shift moves from hand-crafted numerical rewards to using natural language for objective specification, leveraging methods where LLMs synthesize or adapt rewards from descriptions, as evidenced by prior work like EUREKA and CARD, and the Reinforcement Learning from Verifiable Rewards (RLVR) paradigm. The main conceptual transition involves semantic specification, dynamic adaptation, and better human alignment, though challenges in computational cost, hallucination robustness, and scalability for large systems remain, pointing toward a future where coordination stems from shared semantic understanding rather than engineered numerical signals.</div>
<div class="mono" style="margin-top:8px">本文认为，多智能体强化学习中长期存在的手动奖励工程挑战，如信用分配模糊和环境非平稳性，正被大型语言模型（LLMs）所改变。其提出的转变是从手工设计的数值奖励转向使用自然语言进行目标指定，利用LLMs从描述中合成或适配奖励的方法，如EUREKA和CARD等先前工作以及可验证奖励的强化学习（RLVR）范式所证明。这一概念性转变主要涉及语义化指定、动态适配和更好的人类意图对齐，尽管在计算开销、幻觉鲁棒性和大规模系统可扩展性方面仍存在挑战，预示着一个协调源于共享语义表征而非显式工程化数值信号的未来研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition</div>
<div class="meta-line">Authors: Hao Deng, Bo Liu</div>
<div class="meta-line">First: 2026-01-13T05:25:32+00:00 · Latest: 2026-01-13T05:25:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08230v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08230v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Graph Neural Networks (GNNs) excel on graph-structured data, their performance is fundamentally limited by the quality of the observed graph, which often contains noise, missing links, or structural properties misaligned with GNNs&#x27; underlying assumptions. To address this, graph structure learning aims to infer a more optimal topology. Existing methods, however, often incur high computational costs due to complex generative models and iterative joint optimization, limiting their practical utility. In this paper, we propose GADPN, a simple yet effective graph structure learning framework that adaptively refines graph topology via low-rank denoising and generalized structural perturbation. Our approach makes two key contributions: (1) we introduce Bayesian optimization to adaptively determine the optimal denoising strength, tailoring the process to each graph&#x27;s homophily level; and (2) we extend the structural perturbation method to arbitrary graphs via Singular Value Decomposition (SVD), overcoming its original limitation to symmetric structures. Extensive experiments on benchmark datasets demonstrate that GADPN achieves state-of-the-art performance while significantly improving efficiency. It shows particularly strong gains on challenging disassortative graphs, validating its ability to robustly learn enhanced graph structures across diverse network types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GADPN：基于奇异值分解的图自适应去噪与扰动网络</div>
<div class="mono" style="margin-top:8px">尽管图神经网络在图结构数据上表现出色，但其性能根本上受限于观测图的质量，这些图常包含噪声、缺失边或与图神经网络基本假设不符的结构特性。为解决此问题，图结构学习旨在推断更优的拓扑结构。然而，现有方法因复杂的生成模型和迭代联合优化常导致高昂计算成本，限制了实际应用。本文提出GADPN——一种简单而有效的图结构学习框架，通过低秩去噪和广义结构扰动自适应优化图拓扑。本方法有两个关键贡献：(1) 引入贝叶斯优化自适应确定最佳去噪强度，根据各图的同配性水平定制处理流程；(2) 通过奇异值分解将结构扰动方法扩展至任意图结构，突破了原方法仅适用于对称结构的限制。在基准数据集上的大量实验表明，GADPN在显著提升效率的同时实现了最先进的性能，尤其在具有挑战性的异配性图上表现出显著优势，验证了其在不同网络类型中鲁棒学习增强图结构的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper is motivated by the performance limitations of Graph Neural Networks (GNNs) when applied to noisy or incomplete observed graph structures, which existing graph structure learning methods address with high computational costs. The proposed method, GADPN, adaptively refines graph topology through low-rank denoising and generalized structural perturbation, utilizing Bayesian optimization to determine optimal denoising strength per graph&#x27;s homophily and extending structural perturbation via Singular Value Decomposition (SVD) to arbitrary graphs. Experimental results on benchmark datasets show that GADPN achieves state-of-the-art performance with improved efficiency, particularly excelling on disassortative graphs, demonstrating robust graph structure learning across diverse network types.</div>
<div class="mono" style="margin-top:8px">本文的动机在于图神经网络（GNNs）在应用于噪声或结构不完整的观测图时性能受限，而现有的图结构学习方法计算成本高昂。提出的GADPN方法通过低秩去噪和广义结构扰动自适应优化图拓扑，利用贝叶斯优化根据图的同配性确定最佳去噪强度，并通过奇异值分解（SVD）将结构扰动扩展至任意图。在基准数据集上的实验结果表明，GADPN实现了最先进的性能并显著提升了效率，尤其在异配性图上表现突出，验证了其在不同网络类型中鲁棒学习增强图结构的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function</div>
<div class="meta-line">Authors: Hyeongyu Kang, Jaewoo Lee, Woocheol Shin, Kiyoung Om, Jinkyoo Park</div>
<div class="meta-line">First: 2025-12-04T08:21:52+00:00 · Latest: 2026-01-13T04:42:44+00:00</div>
<div class="meta-line">Comments: 36 pages, 21 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04559v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04559v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose Soft Q-based Diffusion Finetuning (SQDF), a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于软Q函数重参数化策略梯度的扩散模型微调方法</div>
<div class="mono" style="margin-top:8px">扩散模型擅长生成高似然样本，但常需与下游目标对齐。现有扩散模型微调方法普遍存在奖励过优化问题，导致高奖励但不自然的样本及多样性下降。为缓解过优化，我们提出基于软Q的扩散微调方法（SQDF），这是一种用于扩散对齐的新型KL正则化强化学习方法，采用训练无关、可微分的软Q函数估计的重参数化策略梯度。SQDF通过三项创新进一步增强：为去噪过程设计折扣因子以实现准确信度分配，集成一致性模型以优化Q函数估计，利用离线策略回放缓冲区提升模式覆盖度并平衡奖励与多样性。实验表明，SQDF在文本-图像对齐任务中既能实现优越的目标奖励，又能保持样本多样性。此外，在在线黑盒优化任务中，SQDF在保持自然度与多样性的同时获得了高样本效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of reward over-optimization in fine-tuning diffusion models for downstream tasks, which often leads to unnatural samples and reduced diversity. To mitigate this, the authors propose Soft Q-based Diffusion Finetuning (SQDF), a KL-regularized reinforcement learning method that uses a reparameterized policy gradient based on a training-free, differentiable estimate of the soft Q-function, enhanced with a discount factor for credit assignment, consistency models for refined Q-estimates, and an off-policy replay buffer to balance reward and diversity. Experimental results show that SQDF achieves higher target rewards while preserving sample diversity in text-to-image alignment and demonstrates high sample efficiency with maintained naturalness and diversity in online black-box optimization.</div>
<div class="mono" style="margin-top:8px">本文针对扩散模型在下游任务微调中存在的奖励过优化问题，该问题常导致生成样本不自然且多样性下降。为缓解此问题，作者提出了基于软Q函数的扩散微调方法（SQDF），这是一种KL正则化的强化学习方法，它利用基于免训练、可微分的软Q函数估计的重参数化策略梯度，并通过折扣因子进行去噪过程的信用分配、结合一致性模型改进Q函数估计、以及使用离策略回放缓冲区来权衡奖励与多样性。实验结果表明，在文本到图像对齐任务中，SQDF在获得更高目标奖励的同时保持了样本多样性，并在在线黑盒优化中实现了高样本效率，同时维持了生成的自然性和多样性。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Multiagent Reinforcement Learning with Collective Influence Estimation</div>
<div class="meta-line">Authors: Zhenglong Luo, Zhiyong Chen, Aoxiang Liu, Ke Pan</div>
<div class="meta-line">First: 2026-01-13T04:24:11+00:00 · Latest: 2026-01-13T04:24:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08210v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.
  To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object&#x27;s states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于集体影响力估计的可扩展多智能体强化学习</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）因其在解决复杂协作任务方面的潜力而备受关注。然而，现有MARL方法通常依赖智能体间频繁交换动作或状态信息以实现有效协调，这在实际机器人系统中难以满足。常见解决方案是引入估计器网络来建模其他智能体行为并预测其动作；然而，此类设计会导致估计器网络的规模和计算成本随智能体数量快速增长，从而限制大规模系统的可扩展性。为应对这些挑战，本文提出一种增强型多智能体学习框架，集成集体影响力估计网络（CIEN）。通过显式建模其他智能体对任务对象的集体影响力，每个智能体仅需根据局部观测和任务对象状态即可推断关键交互信息，实现无需显式动作信息交换的高效协作。该框架有效避免了网络规模随团队规模扩张而增长；此外，新智能体的加入无需修改现有智能体的网络结构，展现出强大的可扩展性。基于软演员-评论家（SAC）算法的多智能体协作任务实验表明，该方法在通信受限环境下实现了稳定高效的协调。进一步地，采用集体影响力建模训练的策略部署于真实机器人平台，实验结果表明其鲁棒性和部署可行性显著提升，同时降低了对通信基础设施的依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the scalability and communication limitations in multiagent reinforcement learning (MARL) by proposing a Collective Influence Estimation Network (CIEN) framework. The motivation stems from the impracticality of frequent action or state exchanges in real-world robotic systems and the computational inefficiency of existing estimator networks that grow with agent count. The method enables each agent to infer collective influences from local observations and task object states, eliminating explicit inter-agent communication and allowing scalable integration of new agents without network modifications. Experimental results on cooperative tasks using Soft Actor-Critic demonstrate stable coordination in communication-limited settings, with real robotic deployment showing enhanced robustness, feasibility, and reduced communication dependency.</div>
<div class="mono" style="margin-top:8px">本文针对多智能体强化学习中的可扩展性和通信限制问题，提出了集体影响估计网络（CIEN）框架。其动机源于现实机器人系统中频繁交换动作或状态信息的不切实际性，以及现有估计器网络随智能体数量增加而计算成本激增的缺陷。该方法使每个智能体能从局部观察和任务对象状态中推断集体影响，无需显式的智能体间通信，并允许在不修改现有网络结构的情况下扩展新智能体，从而提升可扩展性。基于软演员-评论家算法的多智能体协作任务实验表明，该方法在通信受限环境下实现了稳定高效的协调；真实机器人平台部署进一步验证了其增强的鲁棒性、部署可行性及对通信基础设施依赖的降低。</div>
</details>
</div>
<div class="card">
<div class="title">On the Sample Complexity of Differentially Private Policy Optimization</div>
<div class="meta-line">Authors: Yi He, Xingyu Zhou</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-24T00:21:38+00:00 · Latest: 2026-01-13T04:00:52+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21060v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21060v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Policy optimization (PO) is a cornerstone of modern reinforcement learning (RL), with diverse applications spanning robotics, healthcare, and large language model training. The increasing deployment of PO in sensitive domains, however, raises significant privacy concerns. In this paper, we initiate a theoretical study of differentially private policy optimization, focusing explicitly on its sample complexity. We first formalize an appropriate definition of differential privacy (DP) tailored to PO, addressing the inherent challenges arising from on-policy learning dynamics and the subtlety involved in defining the unit of privacy. We then systematically analyze the sample complexity of widely-used PO algorithms, including policy gradient (PG), natural policy gradient (NPG) and more, under DP constraints and various settings, via a unified framework. Our theoretical results demonstrate that privacy costs can often manifest as lower-order terms in the sample complexity, while also highlighting subtle yet important observations in private PO settings. These offer valuable practical insights for privacy-preserving PO algorithms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>差分隐私策略优化的样本复杂度研究</div>
<div class="mono" style="margin-top:8px">策略优化（PO）是现代强化学习（RL）的基石，广泛应用于机器人、医疗和大语言模型训练等领域。然而，PO在敏感领域的日益部署引发了显著的隐私担忧。本文首次对差分隐私策略优化展开理论研究，重点关注其样本复杂度。我们首先形式化了一个适用于PO的差分隐私（DP）定义，以应对在线策略学习动态带来的固有挑战以及隐私单位定义的微妙性。随后，通过统一框架，系统分析了在DP约束和多种设置下广泛使用的PO算法（包括策略梯度（PG）、自然策略梯度（NPG）等）的样本复杂度。理论结果表明，隐私成本通常表现为样本复杂度的低阶项，同时也揭示了私有PO设置中微妙而重要的现象。这些发现为隐私保护PO算法提供了宝贵的实践洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing use of policy optimization in sensitive areas like healthcare and the need to address associated privacy risks, this paper provides a theoretical investigation into the sample complexity of differentially private policy optimization. The method involves formalizing a tailored definition of differential privacy for the policy optimization setting to handle on-policy learning dynamics and privacy unit definition challenges, followed by a unified framework to analyze algorithms such as policy gradient and natural policy gradient under DP constraints. The main experimental results reveal that the privacy cost often appears as a lower-order term in the sample complexity, while also uncovering subtle but important observations that offer practical guidance for designing privacy-preserving algorithms.</div>
<div class="mono" style="margin-top:8px">本文的动机是策略优化在医疗等敏感领域的应用日益增多，亟需解决相关的隐私风险，因此对差分隐私策略优化的样本复杂度进行了理论研究。方法上，针对策略优化中的在线学习动态和隐私单元定义等挑战，形式化了一个定制化的差分隐私定义，并通过一个统一框架分析了策略梯度、自然策略梯度等算法在差分隐私约束下的性能。主要实验结果发现，隐私成本通常表现为样本复杂度中的低阶项，同时揭示了一些微妙而重要的观察，为设计隐私保护的策略优化算法提供了实用的见解。</div>
</details>
</div>
<div class="card">
<div class="title">ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</div>
<div class="meta-line">Authors: Jinpeng Wang, Chao Li, Ting Ye, Mengyuan Zhang, Wei Liu, Jian Luan</div>
<div class="meta-line">First: 2025-11-26T03:10:15+00:00 · Latest: 2026-01-13T03:30:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21005v5">Abs</a> · <a href="https://arxiv.org/pdf/2511.21005v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ICPO：基于内在置信度的群体相对偏好优化方法用于高效强化学习</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）在提升大语言模型（LLM）的推理能力方面展现出显著潜力。然而，现有RLVR方法常受限于奖励粒度粗糙、奖励噪声和探索效率低下等问题，导致训练不稳定和熵崩溃。为应对这一挑战，我们提出了基于内在置信度的群体相对偏好优化方法（ICPO）。其核心思想在于：LLM生成不同响应的概率本质上能直接反映其对推理过程的自我评估。受偏好建模思想启发，ICPO通过比较同一输入提示下多个响应的相对生成概率，计算每个响应的偏好优势分数，并将该分数与可验证奖励结合以指导探索过程。我们发现，偏好优势分数不仅能缓解粗粒度奖励和奖励噪声问题，还能有效抑制过度自信误差，增强被低估的高质量响应的相对优势，并防止模型对特定策略的过拟合。在四个通用领域基准和三个数学基准上的综合实验表明，相较于GRPO，ICPO能持续提升推理性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses limitations in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models, such as coarse-grained rewards, reward noise, and inefficient exploration, which cause unstable training and entropy collapse. To overcome these issues, the authors propose Intrinsic Confidence-Driven Group Relative Preference Optimization (ICPO), a method that leverages the LLM&#x27;s own generation probabilities for different responses under the same prompt to compute a preference advantage score, reflecting the model&#x27;s self-assessment of reasoning; this score is integrated with verifiable rewards to guide exploration. Experimental results across four general-domain and three mathematical benchmarks show that ICPO consistently enhances reasoning performance compared to GRPO by mitigating reward-related problems, reducing overconfident errors, and preventing overfitting to specific strategies.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型的可验证奖励强化学习中存在的奖励粒度粗、奖励噪声和探索效率低等问题，这些问题会导致训练不稳定和熵崩溃。为解决这些挑战，作者提出了内在置信驱动的群体相对偏好优化方法（ICPO），该方法利用同一提示下模型生成不同响应的概率来计算偏好优势分数，以反映模型对推理过程的自我评估，并将该分数与可验证奖励结合以指导探索过程。在四个通用领域基准和三个数学基准上的综合实验表明，ICPO相比GRPO能稳定提升推理能力，有效缓解了奖励相关问题，抑制了过度自信错误，并防止了模型对特定策略的过拟合。</div>
</details>
</div>
<div class="card">
<div class="title">VBO-MI: A Fully Gradient-Based Bayesian Optimization Framework Using Variational Mutual Information Estimation</div>
<div class="meta-line">Authors: Farhad Mirkarimi</div>
<div class="meta-line">First: 2026-01-13T03:07:52+00:00 · Latest: 2026-01-13T03:07:52+00:00</div>
<div class="meta-line">Comments: 31 pages, 8 figures, Code will be released upon acceptance</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world tasks require optimizing expensive black-box functions accessible only through noisy evaluations, a setting commonly addressed with Bayesian optimization (BO). While Bayesian neural networks (BNNs) have recently emerged as scalable alternatives to Gaussian Processes (GPs), traditional BNN-BO frameworks remain burdened by expensive posterior sampling and acquisition function optimization. In this work, we propose {VBO-MI} (Variational Bayesian Optimization with Mutual Information), a fully gradient-based BO framework that leverages recent advances in variational mutual information estimation. To enable end-to-end gradient flow, we employ an actor-critic architecture consisting of an {action-net} to navigate the input space and a {variational critic} to estimate information gain. This formulation effectively eliminates the traditional inner-loop acquisition optimization bottleneck, achieving up to a {$10^2 \times$ reduction in FLOPs} compared to BNN-BO baselines. We evaluate our method on a diverse suite of benchmarks, including high-dimensional synthetic functions and complex real-world tasks such as PDE optimization, the Lunar Lander control problem, and categorical Pest Control. Our experiments demonstrate that VBO-MI consistently provides the same or superior optimization performance and computational scalability over the baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VBO-MI：基于变分互信息估计的全梯度贝叶斯优化框架</div>
<div class="mono" style="margin-top:8px">许多现实任务需要优化仅能通过噪声评估访问的昂贵黑箱函数，这一设定通常采用贝叶斯优化（BO）处理。虽然贝叶斯神经网络（BNN）近期作为高斯过程（GP）的可扩展替代方案出现，但传统BNN-BO框架仍受限于昂贵的后验采样与采集函数优化。本研究提出{VBO-MI}（基于互信息的变分贝叶斯优化），这是一个利用变分互信息估计最新进展的全梯度BO框架。为实现端到端梯度流，我们采用由在输入空间导航的{动作网络}和估计信息增益的{变分评判器}组成的演员-评论家架构。该设计有效消除了传统内循环采集优化瓶颈，相比BNN-BO基线实现高达{$10^2$倍浮点运算量削减}。我们在多样化基准测试中评估本方法，包括高维合成函数与复杂现实任务（如偏微分方程优化、月球着陆器控制问题、分类害虫防治）。实验表明VBO-MI在优化性能与计算可扩展性上始终达到等同或优于基线的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces VBO-MI, a fully gradient-based Bayesian optimization framework designed to overcome the computational inefficiencies of traditional methods, which rely on expensive posterior sampling and acquisition function optimization. The method leverages variational mutual information estimation within an actor-critic architecture, featuring an action-net to explore the input space and a variational critic to estimate information gain, thereby enabling end-to-end gradient flow and eliminating inner-loop optimization bottlenecks. Experimental results on high-dimensional synthetic functions and real-world tasks like PDE optimization and Lunar Lander control show that VBO-MI achieves up to a 100x reduction in FLOPs while matching or surpassing the performance of baseline Bayesian optimization methods.</div>
<div class="mono" style="margin-top:8px">本文提出了VBO-MI，一种完全基于梯度的贝叶斯优化框架，旨在解决传统方法中因昂贵后验采样和采集函数优化带来的计算效率低下问题。该方法利用变分互信息估计，采用行动者-评论家架构，其中行动网络负责探索输入空间，变分评论家用于估计信息增益，从而实现端到端的梯度流动，消除了内部循环优化瓶颈。在高维合成函数和实际任务（如偏微分方程优化和月球着陆器控制）上的实验结果表明，VBO-MI在计算量上最高可减少100倍，同时达到或超越了基线贝叶斯优化方法的性能。</div>
</details>
</div>
<div class="card">
<div class="title">ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms</div>
<div class="meta-line">Authors: Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari</div>
<div class="meta-line">First: 2026-01-13T02:56:06+00:00 · Latest: 2026-01-13T02:56:06+00:00</div>
<div class="meta-line">Comments: 39 pages, 12 figures, 8 tables (including appendix)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08166v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08166v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ZeroDVFS：面向嵌入式平台的零样本大语言模型引导核心与频率分配方法</div>
<div class="mono" style="margin-top:8px">动态电压频率调节（DVFS）与任务核心分配对嵌入式系统的热管理及能效平衡至关重要。现有方法或依赖忽略停滞时间的基于利用率的启发式策略，或需大量离线性能分析以生成静态对照表，无法实现运行时自适应。本文提出一种基于模型的分层多智能体强化学习（MARL）框架，用于多核平台的热感知与能效感知调度。两个协作智能体通过分解指数级动作空间，将后续决策延迟降至358毫秒。首次决策耗时3.5至8.0秒（含一次性LLM特征提取）。精准的环境模型采用回归技术预测热力学特性与性能状态，结合LLM提取的语义特征，可在无需特定负载分析样本的情况下生成合成训练数据，实现训练后平台对新工作负载的零样本部署。本文提出的基于LLM的语义特征提取方法，通过13项代码级特征静态表征OpenMP程序。受Dyna-Q启发的框架融合直接强化学习与基于模型的规划，收敛速度比无模型方法快20倍。在NVIDIA Jetson TX2/Orin NX、RubikPi及Intel Core i7平台上对BOTS与PolybenchC基准测试的实验表明：相比Linux ondemand调控器，能效提升7.09倍，完工时间缩短4.0倍；首次决策延迟比基于对照表的方法快8300倍，满足动态嵌入式系统的实际部署需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing DVFS and core allocation methods, which either rely on simplistic heuristics or require extensive offline profiling, this paper proposes ZeroDVFS, a zero-shot LLM-guided hierarchical multi-agent reinforcement learning framework for thermal- and energy-aware scheduling on embedded platforms. The method employs two collaborative agents to decompose the action space and uses an accurate environment model combined with LLM-extracted semantic features from OpenMP code, enabling zero-shot deployment for new workloads without prior profiling. Experimental results on multiple embedded platforms and benchmarks demonstrate that the framework achieves 7.09x better energy efficiency and 4.0x better makespan than the Linux ondemand governor, with a first-decision latency 8,300x faster than table-based profiling approaches.</div>
<div class="mono" style="margin-top:8px">针对现有动态电压频率缩放和核心分配方法依赖简单启发式规则或需要大量离线性能分析的问题，本文提出了ZeroDVFS，一种基于零样本大语言模型引导的分层多智能体强化学习框架，用于嵌入式平台的热管理和能效感知调度。该方法采用两个协作智能体分解动作空间，并利用精确的环境模型结合从OpenMP代码中提取的语义特征，无需先验性能分析即可实现新工作负载的零样本部署。在多种嵌入式平台和基准测试上的实验结果表明，该框架相比Linux ondemand调度器实现了7.09倍的能效提升和4.0倍的完工时间优化，且首次决策延迟比基于表格的性能分析方法快8,300倍。</div>
</details>
</div>
<div class="card">
<div class="title">Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following</div>
<div class="meta-line">Authors: Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu</div>
<div class="meta-line">First: 2025-10-16T08:24:44+00:00 · Latest: 2026-01-13T02:52:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14420v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.14420v3">PDF</a> · <a href="https://github.com/Rainier-rq/verl-if">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>指令即所需：面向指令跟随的自监督强化学习</div>
<div class="mono" style="margin-top:8px">语言模型在处理现实应用中至关重要的多约束指令时往往表现不佳。现有强化学习方法依赖外部监督，且多约束任务产生的奖励信号稀疏。本文提出一种无标签的自监督强化学习框架，通过直接从指令中提取奖励信号并生成伪标签用于奖励模型训练，从而消除对外部监督的依赖。该方法引入约束分解策略和高效的逐约束二分类机制，以应对稀疏奖励挑战，同时保持计算效率。实验表明，该方法泛化能力强，在3个领域内和5个领域外数据集（包括具有挑战性的智能体与多轮指令跟随任务）上均取得显著提升。数据和代码已公开于 https://github.com/Rainier-rq/verl-if</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of language models struggling to follow multi-constraint instructions, which is critical for real-world applications, and overcomes limitations of existing reinforcement learning methods that rely on external supervision and suffer from sparse rewards. The proposed method is a self-supervised reinforcement learning framework that derives reward signals directly from instructions and generates pseudo-labels for training a reward model, employing constraint decomposition and efficient constraint-wise binary classification to handle sparse rewards efficiently. Experimental results demonstrate that the approach generalizes effectively, achieving significant improvements across three in-domain and five out-of-domain datasets, including in agentic and multi-turn instruction-following tasks.</div>
<div class="mono" style="margin-top:8px">本文针对语言模型在遵循对现实应用至关重要的多约束指令时存在的困难，并克服了现有强化学习方法依赖外部监督和面临稀疏奖励信号的局限。所提出的方法是一种自监督强化学习框架，直接从指令中推导奖励信号并生成伪标签来训练奖励模型，采用约束分解和高效的约束级二元分类策略以高效处理稀疏奖励问题。实验结果表明，该方法具有良好的泛化能力，在三个领域内和五个领域外数据集上均取得了显著改进，包括在具有挑战性的代理和多轮指令遵循任务中。</div>
</details>
</div>
<div class="card">
<div class="title">Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies</div>
<div class="meta-line">Authors: Zeyang Li, Sunbochen Tang, Navid Azizan</div>
<div class="meta-line">First: 2026-01-13T01:58:24+00:00 · Latest: 2026-01-13T01:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逆向流匹配：基于扩散与流策略的在线强化学习统一框架</div>
<div class="mono" style="margin-top:8px">扩散与流策略因其强大的表达能力在在线强化学习中日益受到关注，但高效训练仍是关键挑战。在线强化学习的核心难点在于缺乏目标分布的直接样本，目标实为由Q函数定义的未归一化玻尔兹曼分布。为此，学界针对扩散策略提出了两类方法：噪声期望法（以加权平均噪声为训练目标）和梯度期望法（以Q函数梯度的加权平均为目标）。然而，这两类目标的形式关联及能否整合为更普适的框架尚不明确。本文提出统一框架——逆向流匹配，通过逆向推断视角将训练目标构建为给定噪声中间样本的后验均值估计问题。关键创新在于引入朗之万-斯坦算子构造零均值控制变量，推导出能有效降低重要性采样方差的一般性估计器类别。研究表明，现有噪声期望法与梯度期望法均属该广义类别的特例。这一统一视角带来两大进展：将玻尔兹曼分布的目标优化能力从扩散策略拓展至流策略；支持Q值与Q梯度信息的原理性融合，从而得到最优的最小方差估计器，提升训练效率与稳定性。通过在线强化学习中的流策略训练实例验证，RFM在连续控制基准测试中较扩散策略基线表现出更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of efficiently training expressive diffusion and flow policies in online reinforcement learning, where direct samples from the target Boltzmann distribution are unavailable. The authors propose a unified framework called reverse flow matching (RFM), which formulates the training target as a posterior mean estimation problem and introduces Langevin Stein operators to construct low-variance estimators via control variates. This framework generalizes existing noise-expectation and gradient-expectation methods, enabling their extension to flow policies and the principled combination of Q-value and Q-gradient information for an optimal estimator. Experimental results on continuous-control benchmarks demonstrate that RFM improves performance over diffusion policy baselines.</div>
<div class="mono" style="margin-top:8px">本文针对在线强化学习中扩散和流策略表达能力强但训练效率低的问题，提出了一种统一框架——反向流匹配（RFM）。该方法通过将训练目标构建为后验均值估计问题，并引入朗之万-斯坦因算子来构造低方差估计器，从而在没有目标分布直接样本的情况下有效训练策略。该框架统一了现有的噪声期望和梯度期望方法，将其扩展到流策略，并能结合Q值和Q梯度信息得到最优估计器。在连续控制基准测试上的实验表明，RFM相比扩散策略基线取得了更好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Structure Detection for Contextual Reinforcement Learning</div>
<div class="meta-line">Authors: Tianyue Zhou, Jung-Hoon Cho, Cathy Wu</div>
<div class="meta-line">First: 2026-01-13T01:22:39+00:00 · Latest: 2026-01-13T01:22:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08120v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08120v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contextual Reinforcement Learning (CRL) tackles the problem of solving a set of related Contextual Markov Decision Processes (CMDPs) that vary across different context variables. Traditional approaches--independent training and multi-task learning--struggle with either excessive computational costs or negative transfer. A recently proposed multi-policy approach, Model-Based Transfer Learning (MBTL), has demonstrated effectiveness by strategically selecting a few tasks to train and zero-shot transfer. However, CMDPs encompass a wide range of problems, exhibiting structural properties that vary from problem to problem. As such, different task selection strategies are suitable for different CMDPs. In this work, we introduce Structure Detection MBTL (SD-MBTL), a generic framework that dynamically identifies the underlying generalization structure of CMDP and selects an appropriate MBTL algorithm. For instance, we observe Mountain structure in which generalization performance degrades from the training performance of the target task as the context difference increases. We thus propose M/GP-MBTL, which detects the structure and adaptively switches between a Gaussian Process-based approach and a clustering-based approach. Extensive experiments on synthetic data and CRL benchmarks--covering continuous control, traffic control, and agricultural management--show that M/GP-MBTL surpasses the strongest prior method by 12.49% on the aggregated metric. These results highlight the promise of online structure detection for guiding source task selection in complex CRL environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向上下文强化学习的结构检测</div>
<div class="mono" style="margin-top:8px">上下文强化学习（CRL）旨在解决一组随不同上下文变量变化的上下文马尔可夫决策过程（CMDP）问题。传统方法——独立训练与多任务学习——常面临计算成本过高或负迁移的困境。近期提出的多策略方法——基于模型的迁移学习（MBTL）——通过策略性地选择少量任务进行训练并实现零样本迁移，已展现出显著效果。然而，CMDP涵盖的问题范围广泛，其结构特性因问题而异，因此不同的任务选择策略适用于不同的CMDP。本文提出结构检测MBTL（SD-MBTL），这是一个通用框架，能够动态识别CMDP的潜在泛化结构并选择合适的MBTL算法。例如，我们观察到“山峰结构”——随着上下文差异增大，目标任务的泛化性能会从训练性能逐渐下降。为此，我们提出M/GP-MBTL，该算法能检测结构并自适应地在基于高斯过程的方法与基于聚类的方法间切换。在合成数据及CRL基准测试（涵盖连续控制、交通控制与农业管理）上的大量实验表明，M/GP-MBTL在综合指标上优于先前最优方法12.49%。这些结果凸显了在线结构检测在复杂CRL环境中指导源任务选择的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge in Contextual Reinforcement Learning (CRL) where traditional methods like independent training or multi-task learning suffer from high computational costs or negative transfer, while the multi-policy approach Model-Based Transfer Learning (MBTL) lacks adaptability to diverse structural properties across different Contextual Markov Decision Processes (CMDPs). To overcome this, the authors propose Structure Detection MBTL (SD-MBTL), a framework that dynamically identifies the underlying generalization structure of CMDPs and selects an appropriate MBTL algorithm; specifically, they introduce M/GP-MBTL, which detects structures like the &#x27;Mountain&#x27; pattern and adaptively switches between Gaussian Process-based and clustering-based approaches. Experimental results on synthetic data and CRL benchmarks, including continuous control and traffic control, demonstrate that M/GP-MBTL outperforms the strongest prior method by 12.49% on an aggregated metric, highlighting the effectiveness of online structure detection for guiding source task selection in complex CRL environments.</div>
<div class="mono" style="margin-top:8px">本文针对情境强化学习（CRL）中的挑战，即传统方法如独立训练或多任务学习存在计算成本高或负迁移问题，而多策略方法模型迁移学习（MBTL）难以适应不同情境马尔可夫决策过程（CMDP）的多样化结构特性。为解决此问题，作者提出了结构检测MBTL（SD-MBTL）框架，该框架动态识别CMDP的潜在泛化结构并选择合适的MBTL算法；具体而言，他们引入了M/GP-MBTL方法，可检测如“山峰”结构等模式，并自适应地在基于高斯过程和基于聚类的方法间切换。在合成数据和CRL基准测试（包括连续控制和交通控制）上的实验结果表明，M/GP-MBTL在综合指标上优于先前最强方法12.49%，凸显了在线结构检测在复杂CRL环境中指导源任务选择的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order</div>
<div class="meta-line">Authors: Chengyang Gu, Yuxin Pan, Hui Xiong, Yize Chen</div>
<div class="meta-line">First: 2026-01-13T00:57:45+00:00 · Latest: 2026-01-13T00:57:45+00:00</div>
<div class="meta-line">Comments: Accepted at International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL&#x27;s robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STO-RL：基于LLM引导的子目标时序排序的稀疏奖励离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）能够从预收集的数据集中学习策略，避免了昂贵且风险高的在线交互，但在涉及稀疏奖励的长时程任务中往往表现不佳。现有的目标条件与分层离线RL方法通过分解任务并生成中间奖励来缓解传统离线RL的局限，但通常忽略了子目标间的时序依赖关系，并依赖不精确的奖励塑形，导致策略次优。为解决这些问题，我们提出了STO-RL（基于LLM引导子目标时序排序的离线RL），这是一个利用大语言模型（LLM）生成时序排序的子目标序列及对应状态到子目标阶段映射的离线RL框架。借助此时序结构，STO-RL应用基于势能的奖励塑形，将稀疏的终端奖励转化为密集且时序一致的信号，在促进子目标进展的同时避免次优解。由此产生的带有塑形奖励的增强数据集，能够高效地离线训练高性能策略。在四个离散与连续稀疏奖励基准测试上的评估表明，STO-RL始终优于最先进的离线目标条件与分层RL基线方法，实现了更快的收敛速度、更高的成功率和更短的轨迹。消融研究进一步证实了STO-RL对不完美或有噪声的LLM生成子目标序列的鲁棒性，表明LLM引导的子目标时序结构与理论支撑的奖励塑形相结合，为长时程离线RL提供了一个实用且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of offline reinforcement learning (RL) in long-horizon tasks with sparse rewards, where existing methods often fail to account for temporal dependencies among subgoals and rely on imprecise reward shaping. To overcome this, the authors propose STO-RL, a framework that uses large language models (LLMs) to generate temporally ordered subgoal sequences and state-to-subgoal mappings, then applies potential-based reward shaping to convert sparse terminal rewards into dense, consistent signals for offline policy training. Experimental results on four sparse-reward benchmarks show that STO-RL outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving higher success rates, faster convergence, and shorter trajectories, with ablation studies confirming robustness to imperfect LLM outputs.</div>
<div class="mono" style="margin-top:8px">本文针对离线强化学习在稀疏奖励的长时程任务中的挑战，现有方法常忽略子目标间的时间依赖并依赖不精确的奖励塑造。为此，作者提出STO-RL框架，利用大语言模型生成时序有序的子目标序列和状态到子目标的映射，再通过基于势能的奖励塑造将稀疏终端奖励转化为密集、一致的信号，用于离线策略训练。在四个稀疏奖励基准测试中，STO-RL优于最先进的离线目标条件和分层强化学习方法，实现了更高的成功率、更快的收敛速度和更短的轨迹，消融研究证实其对不完美大语言模型输出的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Large-scale Regional Traffic Signal Control Based on Single-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Qiang Li, Jin Niu, Qin Luo, Lina Yu</div>
<div class="meta-line">First: 2025-03-12T10:51:29+00:00 · Latest: 2026-01-12T23:22:59+00:00</div>
<div class="meta-line">Comments: A critical error in the methodology. The reported congestion control effects were not caused by the proposed signal timing optimization, but by an incorrect traffic volume scaling factor during evaluation. The traffic demand was not properly amplified, resulting in misleading performance gains. Due to the substantial nature of the error, completion of revisions is not feasible in the short term</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.09252v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.09252v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of global urbanization and motorization, traffic congestion has become a significant issue, severely affecting the quality of life, environment, and economy. This paper puts forward a single-agent reinforcement learning (RL)-based regional traffic signal control (TSC) model. Different from multi - agent systems, this model can coordinate traffic signals across a large area, with the goals of alleviating regional traffic congestion and minimizing the total travel time. The TSC environment is precisely defined through specific state space, action space, and reward functions. The state space consists of the current congestion state, which is represented by the queue lengths of each link, and the current signal phase scheme of intersections. The action space is designed to select an intersection first and then adjust its phase split. Two reward functions are meticulously crafted. One focuses on alleviating congestion and the other aims to minimize the total travel time while considering the congestion level. The experiments are carried out with the SUMO traffic simulation software. The performance of the TSC model is evaluated by comparing it with a base case where no signal-timing adjustments are made. The results show that the model can effectively control congestion. For example, the queuing length is significantly reduced in the scenarios tested. Moreover, when the reward is set to both alleviate congestion and minimize the total travel time, the average travel time is remarkably decreased, which indicates that the model can effectively improve traffic conditions. This research provides a new approach for large-scale regional traffic signal control and offers valuable insights for future urban traffic management.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单智能体强化学习的大规模区域交通信号控制</div>
<div class="mono" style="margin-top:8px">在全球城市化与机动化背景下，交通拥堵已成为严重影响生活质量、环境和经济的重要问题。本文提出一种基于单智能体强化学习的区域交通信号控制模型。与多智能体系统不同，该模型能够协调大范围交通信号，以缓解区域交通拥堵、最小化总行程时间为目标。通过明确定义状态空间、动作空间和奖励函数构建了TSC环境：状态空间由各路段排队长度表征的当前拥堵状态及交叉口当前信号相位方案组成；动作空间设计为先选择交叉口再调整其相位配时。研究精心设计了两类奖励函数：一类侧重缓解拥堵，另一类在考虑拥堵水平的同时以最小化总行程时间为目标。实验采用SUMO交通仿真软件进行，通过对比无信号配时调整的基准案例评估模型性能。结果表明，该模型能有效控制拥堵，例如测试场景中排队长度显著缩短；当奖励函数同时兼顾缓解拥堵与最小化行程时间时，平均行程时间显著下降，证明模型能有效改善交通状况。本研究为大规模区域交通信号控制提供了新思路，为未来城市交通管理提供了重要参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to address traffic congestion from urbanization, this paper proposes a single-agent reinforcement learning model for large-scale regional traffic signal control, aiming to coordinate signals to reduce congestion and total travel time. The method defines a state space with queue lengths and signal phases, an action space for selecting intersections and adjusting phase splits, and designs two reward functions targeting congestion alleviation and travel time minimization. Experimental results using SUMO simulation, compared to a no-adjustment baseline, initially indicated significant reductions in queue length and average travel time, suggesting effective traffic improvement; however, a critical methodological error was later identified where incorrect traffic volume scaling during evaluation invalidated these performance gains, making revisions infeasible.</div>
<div class="mono" style="margin-top:8px">本文针对城市化带来的交通拥堵问题，提出了一种基于单智能体强化学习的大规模区域交通信号控制模型，旨在通过协调信号来缓解拥堵并减少总行程时间。方法上，该模型定义了包含队列长度和信号相位的状态空间、用于选择交叉口和调整相位时长的动作空间，并精心设计了两个分别针对缓解拥堵和最小化行程时间的奖励函数。使用SUMO仿真进行的实验，与无调整基准相比，最初结果显示排队长度和平均行程时间显著降低，表明交通状况得到有效改善；但随后发现一个关键的方法论错误，即评估中交通流量缩放因子不正确，导致性能增益存在误导，短期内无法完成修订。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations</div>
<div class="meta-line">Authors: Qiang Li, Jin Niu, Lina Yu</div>
<div class="meta-line">First: 2025-11-01T13:18:50+00:00 · Latest: 2026-01-12T23:22:21+00:00</div>
<div class="meta-line">Comments: A critical error in the methodology. The reported congestion control effects were not caused by the proposed signal timing optimization, but by an incorrect traffic volume scaling factor during evaluation. The traffic demand was not properly amplified, resulting in misleading performance gains. Due to the substantial nature of the error, completion of revisions is not feasible in the short term</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00549v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00549v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model&#x27;s effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>需求波动下区域交通信号控制的鲁棒单智能体强化学习方法</div>
<div class="mono" style="margin-top:8px">交通拥堵主要由交叉口排队引发，严重影响城市生活水平、安全、环境质量与经济效率。交通信号控制（TSC）系统虽具缓解拥堵潜力，但传统优化模型常难以捕捉真实交通的复杂性与动态性。本研究提出一种新颖的单智能体强化学习（RL）框架，用于区域自适应TSC，通过集中决策范式规避多智能体系统的协调复杂性。该模型采用邻接矩阵统一编码路网拓扑、基于探测车辆数据的实时排队状态及当前信号配时参数。借助DreamerV3世界模型的高效学习能力，智能体学习控制策略：其动作依次选择交叉口并调整信号相位配时以调控交通流入/流出，类似于反馈控制系统。奖励设计优先考虑排队消散，直接将拥堵指标（队列长度）与控制动作关联。在SUMO中的仿真实验验证了模型有效性：在具有多层级（10%、20%、30%）起讫点（OD）需求波动的推理场景下，该框架展现出强健的抗波动能力，并显著降低队列长度。本研究为兼容探测车辆技术的智能交通控制建立了新范式。未来研究将通过在训练中纳入随机OD需求波动、探索针对突发事件的区域优化机制，以增强实际适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses traffic congestion by proposing a single-agent reinforcement learning framework for regional adaptive traffic signal control, aiming to overcome the limitations of traditional optimization models and the coordination complexities of multi-agent systems. The method employs a centralized paradigm using an adjacency matrix to encode network topology, real-time queue states from probe data, and signal timing, and leverages the DreamerV3 world model to learn policies that sequentially select intersections and adjust phase splits for inflow/outflow regulation, with a reward focused on queue dissipation. Simulation results in SUMO under multi-level demand fluctuations reportedly showed robust anti-fluctuation capability and reduced queue lengths, though the venue comments reveal a critical methodological error where an incorrect traffic volume scaling factor during evaluation invalidated the claimed performance gains.</div>
<div class="mono" style="margin-top:8px">本文针对交通拥堵问题，提出了一种用于区域自适应交通信号控制的单智能体强化学习框架，旨在克服传统优化模型的局限性和多智能体系统的协调复杂性。该方法采用集中式决策范式，利用邻接矩阵编码网络拓扑、来自探测数据的实时排队状态和信号配时，并借助DreamerV3世界模型学习策略，以顺序选择交叉口并调整相位配时来调节流入/流出，其奖励函数专注于排队消散。在SUMO中的仿真实验报告显示，在多级别需求波动下模型具有鲁棒的抗波动能力和显著降低的排队长度，但会议评论指出存在关键方法错误，即评估中错误的交通量缩放因子导致所宣称的性能提升无效。</div>
</details>
</div>
<div class="card">
<div class="title">Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control</div>
<div class="meta-line">Authors: Qiang Li, Ningjing Zeng, Lina Yu</div>
<div class="meta-line">First: 2025-11-01T13:29:13+00:00 · Latest: 2026-01-12T23:21:57+00:00</div>
<div class="meta-line">Comments: A critical error in the methodology. The reported congestion control effects were not caused by the proposed signal timing optimization, but by an incorrect traffic volume scaling factor during evaluation. The traffic demand was not properly amplified, resulting in misleading performance gains. Due to the substantial nature of the error, completion of revisions is not feasible in the short term</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00551v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00551v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Several studies have employed reinforcement learning (RL) to address the challenges of regional adaptive traffic signal control (ATSC) and achieved promising results. In this field, existing research predominantly adopts multi-agent frameworks. However, the adoption of multi-agent frameworks presents challenges for scalability. Instead, the Traffic signal control (TSC) problem necessitates a single-agent framework. TSC inherently relies on centralized management by a single control center, which can monitor traffic conditions across all roads in the study area and coordinate the control of all intersections. This work proposes a single-agent RL-based regional ATSC model compatible with probe vehicle technology. Key components of the RL design include state, action, and reward function definitions. To facilitate learning and manage congestion, both state and reward functions are defined based on queue length, with action designed to regulate queue dynamics. The queue length definition used in this study differs slightly from conventional definitions but is closely correlated with congestion states. More importantly, it allows for reliable estimation using link travel time data from probe vehicles. With probe vehicle data already covering most urban roads, this feature enhances the proposed method&#x27;s potential for widespread deployment. The method was comprehensively evaluated using the SUMO simulation platform. Experimental results demonstrate that the proposed model effectively mitigates large-scale regional congestion levels via coordinated multi-intersection control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向区域自适应交通信号控制的单智能体强化学习模型</div>
<div class="mono" style="margin-top:8px">多项研究采用强化学习（RL）应对区域自适应交通信号控制（ATSC）的挑战并取得显著成果。该领域现有研究主要采用多智能体框架，但其可扩展性存在局限。交通信号控制（TSC）问题本质上需要单智能体框架，因其依赖单一控制中心的集中管理——该中心可监测研究区域内所有道路的交通状况并协调各交叉口控制。本研究提出一种兼容探测车技术的单智能体RL区域ATSC模型，其RL设计的核心要素包括状态、动作与奖励函数定义。为促进学习并管理拥堵，状态与奖励函数均基于排队长度构建，动作设计则用于调控排队动态。本研究采用的排队长度定义与传统方式略有差异，但与拥堵状态高度相关，更重要的是能通过探测车的路段行程时间数据进行可靠估计。鉴于探测车数据已覆盖大部分城市道路，此特性增强了该方法大规模部署的潜力。基于SUMO仿真平台的综合评估表明，该模型通过多交叉口协同控制，能有效缓解大规模区域拥堵水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the scalability limitations of multi-agent reinforcement learning frameworks for regional adaptive traffic signal control, this paper proposes a single-agent RL model that aligns with centralized traffic management practices. The method defines state and reward functions based on queue length, with actions designed to regulate queue dynamics, and utilizes probe vehicle data for reliable estimation to enhance real-world deployability. Experimental evaluation in SUMO initially suggested the model effectively mitigated large-scale regional congestion through coordinated intersection control, but a critical methodological error was later identified: the reported performance gains were misleading due to an incorrect traffic volume scaling factor during simulation, which improperly reduced traffic demand, making revisions infeasible in the short term.</div>
<div class="mono" style="margin-top:8px">针对区域自适应交通信号控制中多智能体强化学习框架可扩展性不足的问题，本文提出了一种符合集中式管理模式的单智能体强化学习模型。该方法基于排队长度定义状态和奖励函数，通过动作调节队列动态，并利用浮动车数据进行可靠估计以提高实际部署潜力。在SUMO中的实验评估最初表明该模型能通过协调多路口控制有效缓解大规模区域拥堵，但随后发现一个关键方法错误：由于评估时使用了错误的交通量缩放因子，未正确放大交通需求，导致报告的性能提升存在误导，短期内无法完成修订。</div>
</details>
</div>
<div class="card">
<div class="title">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</div>
<div class="meta-line">Authors: Nawazish Alia, Rachael Shawb, Karl Mason</div>
<div class="meta-line">First: 2026-01-12T22:41:26+00:00 · Latest: 2026-01-12T22:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08052v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向奶牛场高效电力负荷调度的预测感知深度强化学习</div>
<div class="mono" style="margin-top:8px">奶牛养殖是高度依赖电网电力的能源密集型产业。随着可再生能源并网比例提升，可持续能源管理对降低电网依赖、支持联合国可持续发展目标7（经济适用的清洁能源）至关重要。然而，可再生能源的间歇性给实时供需平衡带来挑战。智能负荷调度成为在保障可靠性的同时最小化运营成本的关键。强化学习在提升能效与降低成本方面展现出潜力，但现有基于强化学习的调度方法多假设未来电价或发电量完全已知，这在动态环境中不切实际。此外，标准PPO变体依赖固定裁剪或KL散度阈值，易在波动电价下导致训练不稳定。为此，本研究提出一种面向奶牛场高效负荷调度的深度强化学习框架，聚焦实际运行约束下的电池储能与热水系统。所提出的预测感知PPO通过基于小时和月份的残差校准，整合短期需求与可再生能源发电预测；而PID-KL-PPO变体采用比例积分微分控制器自适应调节KL散度，实现稳定策略更新。基于真实奶牛场数据训练，该方法比PPO降低用电成本达1%，比DQN降低4.8%，比SAC降低1.5%。在电池调度中，PPO减少电网购电13.1%，证明了该框架在现代奶牛养殖可持续能源管理中的可扩展性与有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for sustainable energy management in the energy-intensive dairy sector amidst increasing renewable integration, this study proposes a Deep Reinforcement Learning framework for efficient electricity load scheduling. The method introduces a Forecast Aware PPO that incorporates short-term demand and renewable generation forecasts using residual calibration, alongside a PID KL PPO variant that adaptively regulates KL divergence for stable policy updates. Experimental results on real-world dairy farm data show the approach reduces electricity costs by up to 1% compared to standard PPO, 4.8% compared to DQN, and 1.5% compared to SAC, while also cutting grid imports by 13.1% in battery scheduling, demonstrating its effectiveness for cost savings and grid independence.</div>
<div class="mono" style="margin-top:8px">本研究针对乳制品行业能源密集且可再生能源集成度提高的背景下，对可持续能源管理的需求，提出了一种用于高效电力负荷调度的深度强化学习框架。该方法引入了预测感知PPO，利用基于小时和月份的残差校准整合短期需求和可再生能源发电预测，同时采用PID KL PPO变体自适应调节KL散度以实现稳定的策略更新。基于真实乳牛场数据的实验结果表明，该方法相比标准PPO降低电力成本达1%，相比DQN降低4.8%，相比SAC降低1.5%，并在电池调度中减少电网输入13.1%，证明了其在节约成本和增强电网独立性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality</div>
<div class="meta-line">Authors: Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik</div>
<div class="meta-line">First: 2025-05-23T11:30:30+00:00 · Latest: 2026-01-12T21:52:55+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/ZLKong/Awesome-Collection-Token-Reduction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18227v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18227v3">PDF</a> · <a href="https://github.com/ZLKong/Awesome-Collection-Token-Reduction">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input&#x27;s essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate &quot;overthinking&quot; and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, agentic framework design, and broader ML and scientific domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成模型中的令牌缩减应超越效率考量——从视觉、语言到多模态</div>
<div class="mono" style="margin-top:8px">在Transformer架构中，令牌——从原始数据中提取的离散单元——通过将输入分割为固定长度的块而形成。每个令牌随后被映射为嵌入向量，从而实现并行注意力计算，同时保留输入的关键信息。由于Transformer自注意力机制具有二次计算复杂度，令牌缩减主要被用作一种效率优化策略。这在单一视觉和语言领域中尤为明显，它有助于平衡计算成本、内存使用和推理延迟。尽管已有这些进展，本文认为在大规模生成模型时代，令牌缩减应超越其传统以效率为导向的角色。相反，我们将其定位为生成建模中的一项基本原则，对模型架构和更广泛的应用具有关键影响。具体而言，我们认为在视觉、语言和多模态系统中，令牌缩减能够：(i) 促进更深层的多模态整合与对齐，(ii) 缓解“过度思考”和幻觉现象，(iii) 在长输入中保持连贯性，以及(iv) 增强训练稳定性等。我们将令牌重新定义为超越效率度量的工具。基于此，我们展望了未来有前景的研究方向，包括算法设计、强化学习引导的令牌缩减、上下文学习中的令牌优化、智能体框架设计，以及更广泛的机器学习和科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper argues that token reduction, traditionally used to improve computational efficiency in Transformers due to quadratic attention complexity, should be re-evaluated as a core principle in generative modeling beyond mere efficiency. The authors propose that in vision, language, and multimodal systems, token reduction can enhance multimodal integration, reduce hallucinations and overthinking, maintain long-context coherence, and improve training stability. They reframe it as a fundamental design element and outline future directions including algorithm design, reinforcement learning-guided reduction, and applications in in-context learning and agentic frameworks.</div>
<div class="mono" style="margin-top:8px">本文认为，在Transformer模型中，传统上因注意力机制二次计算复杂度而用于提升效率的令牌缩减策略，应被重新定位为生成式建模的核心原则，而不仅仅是效率工具。作者提出，在视觉、语言和多模态系统中，令牌缩减能促进更深层的多模态融合与对齐，减轻“过度思考”和幻觉现象，保持长输入的一致性，并增强训练稳定性。他们将令牌重塑为一个基本设计要素，并展望了未来方向，包括算法设计、强化学习引导的令牌缩减、上下文学习的令牌优化以及智能体框架设计等。</div>
</details>
</div>
<div class="card">
<div class="title">MCP Bridge: A Lightweight, LLM-Agnostic RESTful Proxy for Model Context Protocol Servers</div>
<div class="meta-line">Authors: Arash Ahmadi, Sarah Sharif, Yaser M. Banad</div>
<div class="meta-line">First: 2025-04-11T22:19:48+00:00 · Latest: 2026-01-12T21:39:11+00:00</div>
<div class="meta-line">Comments: 29 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.08999v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.08999v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly augmented with external tools through standardized interfaces like the Model Context Protocol (MCP). However, current MCP implementations face critical limitations: they typically require local process execution through STDIO transports, making them impractical for resource-constrained environments like mobile devices, web browsers, and edge computing. We present MCP Bridge, a lightweight RESTful proxy that connects to multiple MCP servers and exposes their capabilities through a unified API. Unlike existing solutions, MCP Bridge is fully LLM-agnostic, supporting any backend regardless of vendor. The system implements a risk-based execution model with three security levels-standard execution, confirmation workflow, and Docker isolation - while maintaining backward compatibility with standard MCP clients. However, reliable execution within this framework requires models that can strictly adhere to protocol schemas. To this end, we also fine-tuned the Qwen3 4B and 8B model family on the Agent-Ark/Toucan-1.5M dataset using four Reinforcement Learning techniques: Group Relative Policy Optimization (GRPO), Dr. GRPO, Beta Normalization Policy Optimization (BNPO), and Decoupled Alignment Policy Optimization (DAPO). Evaluated on the MCPToolBench++ benchmark, our optimized model achieves an F1 score of 73.0% that outperforms GPT-OSS-120B (62.17%) and remains competitive with the 70B+ parameter baselines. Evaluation demonstrates that MCP Bridge successfully addresses the constraints of direct MCP connections while providing enhanced security controls and cross-platform compatibility, enabling sophisticated LLM-powered applications in previously inaccessible environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MCP Bridge：面向模型上下文协议服务器的轻量级、LLM无关RESTful代理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）正日益通过模型上下文协议（MCP）等标准化接口与外部工具增强集成。然而，现有MCP实现面临关键局限：通常需通过STDIO传输在本地进程执行，难以适用于移动设备、网页浏览器和边缘计算等资源受限环境。本文提出MCP Bridge——一种轻量级RESTful代理，可连接多个MCP服务器并通过统一API暴露其能力。与现有方案不同，MCP Bridge完全与LLM无关，支持任意供应商的后端。该系统采用基于风险的执行模型，提供标准执行、确认工作流和Docker隔离三级安全机制，同时保持与标准MCP客户端的向后兼容性。但该框架的可靠执行需要模型严格遵循协议模式。为此，我们采用四种强化学习技术——组相对策略优化（GRPO）、Dr. GRPO、贝塔归一化策略优化（BNPO）和解耦对齐策略优化（DAPO），在Agent-Ark/Toucan-1.5M数据集上对Qwen3 4B和8B模型系列进行微调。在MCPToolBench++基准测试中，优化后模型取得73.0%的F1分数，优于GPT-OSS-120B（62.17%），并与70B+参数基线模型保持竞争力。评估表明，MCP Bridge成功解决了直接MCP连接的限制，同时提供增强的安全控制与跨平台兼容性，使复杂LLM应用得以在以往无法部署的环境中运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces MCP Bridge, a lightweight RESTful proxy designed to overcome the limitations of current Model Context Protocol (MCP) implementations, which often rely on local STDIO transports and are unsuitable for resource-constrained environments like mobile and edge devices. The method involves creating an LLM-agnostic proxy that connects to multiple MCP servers, exposing their capabilities through a unified API with a risk-based execution model offering three security levels, and it includes fine-tuning the Qwen3 4B and 8B models using four Reinforcement Learning techniques to ensure strict protocol adherence. Experimental results on the MCPToolBench++ benchmark show that the optimized model achieves an F1 score of 73.0%, outperforming GPT-OSS-120B and competing with larger 70B+ parameter models, demonstrating that MCP Bridge successfully enables secure, cross-platform LLM applications in previously inaccessible settings.</div>
<div class="mono" style="margin-top:8px">本文介绍了MCP Bridge，一种轻量级RESTful代理，旨在解决当前模型上下文协议（MCP）实现的局限性，这些实现通常依赖本地STDIO传输，不适合移动和边缘设备等资源受限环境。该方法通过创建一个与LLM无关的代理，连接多个MCP服务器，通过统一API暴露其功能，并采用基于风险的执行模型提供三个安全级别，同时使用四种强化学习技术对Qwen3 4B和8B模型进行微调以确保严格遵循协议。在MCPToolBench++基准测试上的实验结果表明，优化后的模型取得了73.0%的F1分数，优于GPT-OSS-120B，并与更大的70B+参数模型竞争，证明MCP Bridge成功在先前无法访问的环境中实现了安全、跨平台的LLM应用。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety</div>
<div class="meta-line">Authors: Can Jin, Rui Wu, Tong Che, Qixin Zhang, Hongwu Peng, Jiahui Zhao, Zhenting Wang, Wenqi Wei, Ligong Han, Zhao Zhang, Yuan Cao, Ruixiang Tang, Dimitris N. Metaxas</div>
<div class="meta-line">First: 2026-01-12T21:08:46+00:00 · Latest: 2026-01-12T21:08:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08000v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like&#x27;&#x27; safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结合法规与判例的推理：面向大语言模型安全的案例增强审慎对齐方法</div>
<div class="mono" style="margin-top:8px">确保大语言模型（LLMs）在遵循安全原则的同时不拒绝良性请求仍是一大挑战。OpenAI通过引入审慎对齐（DA）方法，让模型对类似代码的详细安全规则进行推理以提升其o系列模型的安全性，但该方法在通常缺乏高级推理能力的开源LLMs中的效果尚未得到充分研究。本研究系统评估了明确指定大量安全规则与通过示例案例演示这两种方式的影响。研究发现，引用显式规则对无害性的提升效果不稳定，且会系统性降低模型的有用性；而基于案例增强的简化规则进行训练，能产生更稳健且泛化的安全行为。通过以案例增强的推理而非复杂的类代码安全规则来引导LLMs，我们避免了模型对狭窄枚举规则的僵化遵循，实现了更广泛的适应性。基于这些发现，我们提出CADA方法——一种利用强化学习对自生成安全推理链进行训练的案例增强审慎对齐方法。CADA有效提升了模型的无害性，增强了对攻击的鲁棒性，减少了过度拒绝行为，并在多样化基准测试中保持了实用性，为在维持有用性的同时提升安全性提供了一种替代纯规则DA的可行方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of aligning large language models (LLMs) with safety principles without causing excessive refusal of benign requests, motivated by the limitations of rule-only deliberative alignment in open-source models. The method introduces CADA, a case-augmented deliberative alignment approach that uses reinforcement learning on self-generated safety reasoning chains, combining illustrative cases with simple codes rather than relying solely on extensive, code-like safety rules. Experimental results show that CADA enhances harmlessness, improves robustness against attacks, reduces over-refusal, and preserves utility across diverse benchmarks, outperforming methods based on explicit safety codes which often degrade helpfulness.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型（LLM）在遵循安全原则时避免过度拒绝良性请求的挑战，其动机在于仅依赖规则进行审慎对齐的方法在开源模型中效果有限。该方法提出了CADA，一种案例增强的审慎对齐方法，它利用强化学习对自生成的安全推理链进行训练，将示例案例与简单规则结合，而非仅依赖冗长的代码式安全规则。实验结果表明，CADA有效提升了无害性，增强了对抗攻击的鲁棒性，减少了过度拒绝，并在多样基准测试中保持了实用性，优于那些依赖显式安全规则、常损害帮助性的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL</div>
<div class="meta-line">Authors: Zhewei Yao, Guoheng Sun, Lukasz Borchmann, Gaurav Nuti, Zheyu Shen, Minghang Deng, Bohan Zhai, Hao Zhang, Ang Li, Yuxiong He</div>
<div class="meta-line">First: 2025-05-22T23:33:47+00:00 · Latest: 2026-01-12T20:32:51+00:00</div>
<div class="meta-line">Comments: 22 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20315v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20315v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Translating natural language into SQL (Test2SQL) is a longstanding challenge at the intersection of natural language understanding and structured data access. While large language models (LLMs) have significantly improved fluency in SQL generation, producing correct and executable SQL--particularly for complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a reinforcement learning (RL) framework and model family designed to generate accurate, executable SQL using a lightweight reward signal based solely on execution correctness. Our approach avoids brittle intermediate supervision and complex reward shaping, promoting stable training and alignment with the end task. Combined with carefully curated data, strong supervised initialization, and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art execution accuracy across six diverse Test2SQL benchmarks, including the top position on the BIRD leaderboard. Notably, our 7B model outperforms prior 70B-class systems, highlighting the framework&#x27;s scalability and efficiency. We further demonstrate inference-time robustness through simple extensions like value retrieval and majority voting. Extensive experiments and ablation studies offer both positive and negative insights, providing practical guidance for future Test2SQL research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Arctic-Text2SQL-R1：简单奖励机制下的强推理文本到SQL转换</div>
<div class="mono" style="margin-top:8px">将自然语言转换为SQL（Text2SQL）是自然语言理解与结构化数据访问交叉领域的长期挑战。尽管大语言模型（LLMs）显著提升了SQL生成的流畅性，但生成正确且可执行的SQL——特别是针对复杂查询——仍是瓶颈。我们提出Arctic-Text2SQL-R1，这是一个基于强化学习（RL）的框架与模型系列，旨在通过仅基于执行正确性的轻量级奖励信号生成准确、可执行的SQL。该方法避免了脆弱的中间监督与复杂的奖励塑形，促进了训练稳定性并与最终任务对齐。结合精心构建的数据、强监督初始化及有效训练实践，Arctic-Text2SQL-R1在六项多样化Text2SQL基准测试中实现了最先进的执行准确率，包括在BIRD排行榜上位列第一。值得注意的是，我们的70亿参数模型性能超越此前700亿参数级系统，凸显了该框架的可扩展性与效率。我们进一步通过值检索和多数投票等简单扩展展示了推理时的鲁棒性。大量实验与消融研究提供了正反两方面的洞见，为未来Text2SQL研究提供了实用指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the persistent challenge in Text-to-SQL of generating correct and executable SQL for complex queries, where large language models often fall short despite improved fluency. The authors introduce Arctic-Text2SQL-R1, a reinforcement learning framework that uses a simple reward signal based solely on execution correctness to train models, avoiding complex reward shaping and intermediate supervision for stable training and task alignment. Experimental results show that the approach achieves state-of-the-art execution accuracy across six benchmarks, including top performance on BIRD, with a 7B model outperforming prior 70B-class systems, demonstrating scalability, efficiency, and robustness through extensions like value retrieval and majority voting.</div>
<div class="mono" style="margin-top:8px">本文针对Text-to-SQL中长期存在的挑战，即生成正确且可执行的复杂查询SQL，尽管大语言模型在流畅性上有所提升，但仍存在瓶颈。作者提出了Arctic-Text2SQL-R1，这是一个强化学习框架，仅基于执行正确性使用轻量级奖励信号来训练模型，避免了复杂的奖励塑造和中间监督，从而促进稳定训练并与最终任务对齐。实验结果表明，该方法在六个不同的Text-to-SQL基准测试中实现了最先进的执行准确率，包括在BIRD排行榜上位居首位，其中7B模型超越了先前70B级别的系统，展示了其可扩展性、效率以及通过值检索和多数投票等扩展的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">DataScribe: An AI-Native, Policy-Aligned Web Platform for Multi-Objective Materials Design and Discovery</div>
<div class="meta-line">Authors: Divyanshu Singh, Doguhan Sarıtürk, Cameron Lea, Md Shafiqul Islam, Raymundo Arroyave, Vahid Attari</div>
<div class="meta-line">First: 2026-01-12T19:59:39+00:00 · Latest: 2026-01-12T19:59:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07966v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07966v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The acceleration of materials discovery requires digital platforms that go beyond data repositories to embed learning, optimization, and decision-making directly into research workflows. We introduce DataScribe, an AI-native, cloud-based materials discovery platform that unifies heterogeneous experimental and computational data through ontology-backed ingestion and machine-actionable knowledge graphs. The platform integrates FAIR-compliant metadata capture, schema and unit harmonization, uncertainty-aware surrogate modeling, and native multi-objective multi-fidelity Bayesian optimization, enabling closed-loop propose-measure-learn workflows across experimental and computational pipelines. DataScribe functions as an application-layer intelligence stack, coupling data governance, optimization, and explainability rather than treating them as downstream add-ons. We validate the platform through case studies in electrochemical materials and high-entropy alloys, demonstrating end-to-end data fusion, real-time optimization, and reproducible exploration of multi-objective trade spaces. By embedding optimization engines, machine learning, and unified access to public and private scientific data directly within the data infrastructure, and by supporting open, free use for academic and non-profit researchers, DataScribe functions as a general-purpose application-layer backbone for laboratories of any scale, including self-driving laboratories and geographically distributed materials acceleration platforms, with built-in support for performance, sustainability, and supply-chain-aware objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DataScribe：面向多目标材料设计与发现的人工智能原生、策略对齐网络平台</div>
<div class="mono" style="margin-top:8px">加速材料发现需要超越数据存储库的数字平台，将学习、优化和决策直接嵌入研究流程。我们推出DataScribe——一个基于云的人工智能原生材料发现平台，通过本体支持的数据摄取和机器可操作知识图谱，统一异构的实验与计算数据。该平台集成符合FAIR原则的元数据采集、模式与单位协调、不确定性感知代理建模，以及原生多目标多保真贝叶斯优化，实现跨实验与计算流程的闭环“提出-测量-学习”工作流。DataScribe作为应用层智能栈，将数据治理、优化和可解释性耦合而非视为下游附加组件。我们通过电化学材料和高熵合金的案例研究验证平台，展示端到端数据融合、实时优化及多目标权衡空间的可重复探索。通过将优化引擎、机器学习及对公共与私有科学数据的统一访问直接嵌入数据基础设施，并支持学术与非营利研究者的开放免费使用，DataScribe成为适用于任意规模实验室（包括自动驾驶实验室和地理分布式材料加速平台）的通用应用层主干，内置对性能、可持续性和供应链感知目标的支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to accelerate materials discovery beyond static data repositories, this paper introduces DataScribe, an AI-native cloud platform that integrates heterogeneous materials data via ontology-backed knowledge graphs and machine-actionable workflows. Its method combines FAIR-compliant metadata capture, schema harmonization, and native multi-objective multi-fidelity Bayesian optimization to enable closed-loop propose-measure-learn cycles. Experimental validation through case studies in electrochemical materials and high-entropy alloys demonstrates the platform&#x27;s capability for end-to-end data fusion, real-time optimization, and reproducible exploration of multi-objective trade spaces, serving as a scalable backbone for self-driving and distributed laboratories.</div>
<div class="mono" style="margin-top:8px">本文旨在加速材料发现，超越静态数据存储库，为此提出了DataScribe——一个基于人工智能的云平台，通过本体支持的知识图谱和机器可操作工作流整合异构材料数据。其方法结合了FAIR合规的元数据捕获、模式协调以及原生多目标多保真度贝叶斯优化，实现了闭环的“提出-测量-学习”循环。通过对电化学材料和高熵合金的案例研究进行实验验证，证明了该平台能够实现端到端数据融合、实时优化以及多目标权衡空间的可重复探索，为自驱动和分布式实验室提供了可扩展的通用应用层支撑。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Methods for Neighborhood Selection in Local Search</div>
<div class="meta-line">Authors: Yannick Molinghen, Augustin Delecluse, Renaud De Landtsheer, Stefano Michelini</div>
<div class="meta-line">First: 2026-01-12T19:25:29+00:00 · Latest: 2026-01-12T19:25:29+00:00</div>
<div class="meta-line">Comments: Accepted at ICORES 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07948v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has recently gained traction as a means to improve combinatorial optimization methods, yet its effectiveness within local search metaheuristics specifically remains comparatively underexamined. In this study, we evaluate a range of reinforcement learning-based neighborhood selection strategies -- multi-armed bandits (upper confidence bound, $ε$-greedy) and deep reinforcement learning methods (proximal policy optimization, double deep $Q$-network) -- and compare them against multiple baselines across three different problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. We show how search-specific characteristics, particularly large variations in cost due to constraint violation penalties, necessitate carefully designed reward functions to provide stable and informative learning signals. Our extensive experiments reveal that algorithm performance varies substantially across problems, although that $ε$-greedy consistently ranks among the best performers. In contrast, the computational overhead of deep reinforcement learning approaches only makes them competitive with a substantially longer runtime. These findings highlight both the promise and the practical limitations of deep reinforcement learning in local search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>局部搜索中邻域选择的强化学习方法研究</div>
<div class="mono" style="margin-top:8px">强化学习作为改进组合优化方法的手段近期受到关注，但其在局部搜索元启发式算法中的具体应用效果仍相对缺乏深入研究。本研究评估了一系列基于强化学习的邻域选择策略——包括多臂老虎机（置信上界、ε-贪婪）和深度强化学习方法（近端策略优化、双深度Q网络），并在三个不同问题（旅行商问题、带时间窗的取送货问题、车辆排序问题）中与多种基线方法进行比较。研究表明，搜索过程的特定特征（尤其是因约束违反惩罚导致的成本大幅波动）需要精心设计奖励函数，以提供稳定且信息丰富的学习信号。大量实验表明，算法性能在不同问题间差异显著，但ε-贪婪策略始终表现优异。相比之下，深度强化学习方法因计算开销较大，仅能在显著延长运行时间的情况下具备竞争力。这些发现既揭示了深度强化学习在局部搜索中的应用潜力，也指出了其实际局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the application of reinforcement learning (RL) for neighborhood selection within local search metaheuristics, motivated by its underexplored potential in this specific combinatorial optimization context. The method evaluates several RL strategies, including multi-armed bandits (Upper Confidence Bound, ε-greedy) and deep RL approaches (Proximal Policy Optimization, Double Deep Q-Network), against baseline methods on three problems: the Traveling Salesman Problem, the Pickup and Delivery Problem with Time Windows, and the Car Sequencing Problem. The main experimental results demonstrate that algorithm performance is highly problem-dependent, with ε-greedy consistently performing well, while deep RL methods incur significant computational overhead, making them competitive only with substantially longer runtimes, thus highlighting both the promise and practical limitations of deep RL in local search.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在局部搜索元启发式中应用强化学习进行邻域选择，其动机在于强化学习在这一特定组合优化场景中的潜力尚未得到充分探索。方法上，评估了多种强化学习策略，包括多臂赌博机（上置信界、ε-贪婪）和深度强化学习方法（近端策略优化、双深度Q网络），并在旅行商问题、带时间窗的取送货问题以及车辆排序问题这三个问题上与基线方法进行了比较。主要实验结果表明，算法性能高度依赖于具体问题，其中ε-贪婪策略表现 consistently 良好，而深度强化学习方法则带来显著的计算开销，仅在运行时间大幅延长时才具有竞争力，从而凸显了深度强化学习在局部搜索中的前景与实际局限性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260115_0345.html">20260115_0345</a>
<a href="archive/20260114_0337.html">20260114_0337</a>
<a href="archive/20260113_0317.html">20260113_0317</a>
<a href="archive/20260112_0312.html">20260112_0312</a>
<a href="archive/20260111_0330.html">20260111_0330</a>
<a href="archive/20260110_0347.html">20260110_0347</a>
<a href="archive/20260109_0343.html">20260109_0343</a>
<a href="archive/20260108_0342.html">20260108_0342</a>
<a href="archive/20260107_0328.html">20260107_0328</a>
<a href="archive/20260106_0317.html">20260106_0317</a>
<a href="archive/20260105_0312.html">20260105_0312</a>
<a href="archive/20260104_0312.html">20260104_0312</a>
<a href="archive/20260103_0318.html">20260103_0318</a>
<a href="archive/20260102_0327.html">20260102_0327</a>
<a href="archive/20260101_0321.html">20260101_0321</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0531.html">20251230_0531</a>
<a href="archive/20251230_0518.html">20251230_0518</a>
<a href="archive/20251230_0505.html">20251230_0505</a>
<a href="archive/20251230_0350.html">20251230_0350</a>
<a href="archive/20251230_0331.html">20251230_0331</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0338.html">20251228_0338</a>
<a href="archive/20251227_0324.html">20251227_0324</a>
<a href="archive/20251226_0323.html">20251226_0323</a>
<a href="archive/20251225_0321.html">20251225_0321</a>
<a href="archive/20251224_0329.html">20251224_0329</a>
<a href="archive/20251223_0325.html">20251223_0325</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
